2023/12/02 03:09:16 - INFO - root -   Num train examples = 566
2023/12/02 03:09:16 - INFO - root -   Num val examples = 143
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/12/02 03:09:16 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/12/02 03:09:17 - INFO - root -   backend = nccl
2023/12/02 03:09:17 - INFO - root -   batch_size = 2
2023/12/02 03:09:17 - INFO - root -   dropout = 0.5
2023/12/02 03:09:17 - INFO - root -   epochs = 300
2023/12/02 03:09:17 - INFO - root -   eval_freq = 5
2023/12/02 03:09:17 - INFO - root -   focal_loss = False
2023/12/02 03:09:17 - INFO - root -   input_size = 128
2023/12/02 03:09:17 - INFO - root -   is_pretrained = False
2023/12/02 03:09:17 - INFO - root -   label_smooth = False
2023/12/02 03:09:17 - INFO - root -   local_rank = -1
2023/12/02 03:09:17 - INFO - root -   lr = 1e-05
2023/12/02 03:09:17 - INFO - root -   lr_decay_rate = 0.1
2023/12/02 03:09:17 - INFO - root -   lr_steps = [50, 100]
2023/12/02 03:09:17 - INFO - root -   lr_type = cosine
2023/12/02 03:09:17 - INFO - root -   model_depth = 34
2023/12/02 03:09:17 - INFO - root -   model_name = resnet50
2023/12/02 03:09:17 - INFO - root -   momentum = 0.9
2023/12/02 03:09:17 - INFO - root -   num_classes = 2
2023/12/02 03:09:17 - INFO - root -   output = ./tcia_ucsf_roi_idh_outputs
2023/12/02 03:09:17 - INFO - root -   print_freq = 20
2023/12/02 03:09:17 - INFO - root -   resume = 
2023/12/02 03:09:17 - INFO - root -   start_epoch = 0
2023/12/02 03:09:17 - INFO - root -   train_list = /media/spgou/FAST/ZYJ/Glioma_easy/dataset/tcia_ucsf_train_patients.txt
2023/12/02 03:09:17 - INFO - root -   tune_from = 
2023/12/02 03:09:17 - INFO - root -   val_list = /media/spgou/FAST/ZYJ/Glioma_easy/dataset/tcia_ucsf_test_patients.txt
2023/12/02 03:09:17 - INFO - root -   warmup_epoch = 20
2023/12/02 03:09:17 - INFO - root -   warmup_multiplier = 100
2023/12/02 03:09:17 - INFO - root -   weight_decay = 0.0005
2023/12/02 03:09:17 - INFO - root -   workers = 16
2023/12/02 03:09:33 - INFO - root -   Epoch: [0/300][0/283], lr: 0.00000010 	 loss = 0.2787(0.2787)
2023/12/02 03:09:46 - INFO - root -   Epoch: [0/300][20/283], lr: 0.00000010 	 loss = 1.0628(0.8538)
2023/12/02 03:10:05 - INFO - root -   Epoch: [0/300][40/283], lr: 0.00000010 	 loss = 0.4697(0.8442)
2023/12/02 03:10:23 - INFO - root -   Epoch: [0/300][60/283], lr: 0.00000010 	 loss = 0.9128(0.8651)
2023/12/02 03:10:31 - INFO - root -   Epoch: [0/300][80/283], lr: 0.00000010 	 loss = 0.7101(0.8473)
2023/12/02 10:20:13 - INFO - root -   Num train examples = 566
2023/12/02 10:20:13 - INFO - root -   Num val examples = 143
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/12/02 10:20:13 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/12/02 10:20:13 - INFO - root -   backend = nccl
2023/12/02 10:20:13 - INFO - root -   batch_size = 2
2023/12/02 10:20:13 - INFO - root -   dropout = 0.5
2023/12/02 10:20:13 - INFO - root -   epochs = 300
2023/12/02 10:20:13 - INFO - root -   eval_freq = 5
2023/12/02 10:20:13 - INFO - root -   focal_loss = False
2023/12/02 10:20:13 - INFO - root -   input_size = 128
2023/12/02 10:20:13 - INFO - root -   is_pretrained = False
2023/12/02 10:20:13 - INFO - root -   label_smooth = False
2023/12/02 10:20:13 - INFO - root -   local_rank = -1
2023/12/02 10:20:13 - INFO - root -   lr = 1e-05
2023/12/02 10:20:13 - INFO - root -   lr_decay_rate = 0.1
2023/12/02 10:20:13 - INFO - root -   lr_steps = [50, 100]
2023/12/02 10:20:13 - INFO - root -   lr_type = cosine
2023/12/02 10:20:13 - INFO - root -   model_depth = 34
2023/12/02 10:20:13 - INFO - root -   model_name = resnet50
2023/12/02 10:20:13 - INFO - root -   momentum = 0.9
2023/12/02 10:20:13 - INFO - root -   num_classes = 2
2023/12/02 10:20:13 - INFO - root -   output = ./tcia_ucsf_roi_idh_outputs
2023/12/02 10:20:13 - INFO - root -   print_freq = 20
2023/12/02 10:20:13 - INFO - root -   resume = 
2023/12/02 10:20:13 - INFO - root -   start_epoch = 0
2023/12/02 10:20:13 - INFO - root -   train_list = /media/spgou/FAST/ZYJ/Glioma_easy/dataset/tcia_ucsf_train_patients.txt
2023/12/02 10:20:13 - INFO - root -   tune_from = 
2023/12/02 10:20:13 - INFO - root -   val_list = /media/spgou/FAST/ZYJ/Glioma_easy/dataset/tcia_ucsf_test_patients.txt
2023/12/02 10:20:13 - INFO - root -   warmup_epoch = 20
2023/12/02 10:20:13 - INFO - root -   warmup_multiplier = 100
2023/12/02 10:20:13 - INFO - root -   weight_decay = 0.0005
2023/12/02 10:20:13 - INFO - root -   workers = 16
2023/12/02 10:20:31 - INFO - root -   Epoch: [0/300][0/283], lr: 0.00000010 	 loss = 0.9312(0.9312)
2023/12/02 10:20:41 - INFO - root -   Epoch: [0/300][20/283], lr: 0.00000010 	 loss = 0.7681(0.7285)
2023/12/02 10:21:01 - INFO - root -   Epoch: [0/300][40/283], lr: 0.00000010 	 loss = 0.8544(0.7722)
2023/12/02 10:21:16 - INFO - root -   Epoch: [0/300][60/283], lr: 0.00000010 	 loss = 0.3301(0.7475)
2023/12/02 10:21:38 - INFO - root -   Epoch: [0/300][80/283], lr: 0.00000010 	 loss = 0.4667(0.7154)
2023/12/02 10:21:52 - INFO - root -   Epoch: [0/300][100/283], lr: 0.00000010 	 loss = 1.8925(0.7143)
2023/12/02 10:22:03 - INFO - root -   Epoch: [0/300][120/283], lr: 0.00000010 	 loss = 0.8202(0.7035)
2023/12/02 10:22:26 - INFO - root -   Epoch: [0/300][140/283], lr: 0.00000010 	 loss = 0.6163(0.6922)
2023/12/02 10:22:36 - INFO - root -   Epoch: [0/300][160/283], lr: 0.00000010 	 loss = 0.5841(0.6753)
2023/12/02 10:23:01 - INFO - root -   Epoch: [0/300][180/283], lr: 0.00000010 	 loss = 0.8937(0.6607)
2023/12/02 10:23:30 - INFO - root -   Epoch: [0/300][200/283], lr: 0.00000010 	 loss = 1.2443(0.6568)
2023/12/02 10:23:38 - INFO - root -   Epoch: [0/300][220/283], lr: 0.00000010 	 loss = 0.4633(0.6513)
2023/12/02 10:23:57 - INFO - root -   Epoch: [0/300][240/283], lr: 0.00000010 	 loss = 0.6512(0.6518)
2023/12/02 10:24:10 - INFO - root -   Epoch: [0/300][260/283], lr: 0.00000010 	 loss = 0.3092(0.6443)
2023/12/02 10:24:24 - INFO - root -   Epoch: [0/300][280/283], lr: 0.00000010 	 loss = 1.6198(0.6411)
2023/12/02 10:24:25 - INFO - root -   Epoch: [0/300] 	 loss = 0.6384
2023/12/02 10:24:25 - INFO - root -   train_accuracy = 0.6820
2023/12/02 10:24:27 - INFO - root -   Epoch: [1/300][0/283], lr: 0.00000010 	 loss = 0.9717(0.9717)
2023/12/02 10:25:11 - INFO - root -   Epoch: [1/300][20/283], lr: 0.00000010 	 loss = 0.2970(0.7835)
2023/12/02 10:25:19 - INFO - root -   Epoch: [1/300][40/283], lr: 0.00000010 	 loss = 0.6613(0.7750)
2023/12/02 10:25:32 - INFO - root -   Epoch: [1/300][60/283], lr: 0.00000010 	 loss = 1.4495(0.7263)
2023/12/02 10:25:46 - INFO - root -   Epoch: [1/300][80/283], lr: 0.00000010 	 loss = 0.9035(0.7318)
2023/12/02 10:26:06 - INFO - root -   Epoch: [1/300][100/283], lr: 0.00000010 	 loss = 0.1869(0.6907)
2023/12/02 10:26:25 - INFO - root -   Epoch: [1/300][120/283], lr: 0.00000010 	 loss = 1.0118(0.6760)
2023/12/02 10:26:37 - INFO - root -   Epoch: [1/300][140/283], lr: 0.00000010 	 loss = 0.1832(0.6575)
2023/12/02 10:26:54 - INFO - root -   Epoch: [1/300][160/283], lr: 0.00000010 	 loss = 0.6789(0.6517)
2023/12/02 10:27:10 - INFO - root -   Epoch: [1/300][180/283], lr: 0.00000010 	 loss = 1.2166(0.6294)
2023/12/02 10:27:32 - INFO - root -   Epoch: [1/300][200/283], lr: 0.00000010 	 loss = 1.0654(0.6371)
2023/12/02 10:27:56 - INFO - root -   Epoch: [1/300][220/283], lr: 0.00000010 	 loss = 0.2968(0.6356)
2023/12/02 10:28:04 - INFO - root -   Epoch: [1/300][240/283], lr: 0.00000010 	 loss = 0.4401(0.6291)
2023/12/02 10:28:12 - INFO - root -   Epoch: [1/300][260/283], lr: 0.00000010 	 loss = 0.4012(0.6231)
2023/12/02 10:28:25 - INFO - root -   Epoch: [1/300][280/283], lr: 0.00000010 	 loss = 1.5173(0.6136)
2023/12/02 10:28:26 - INFO - root -   Epoch: [1/300] 	 loss = 0.6103
2023/12/02 10:28:26 - INFO - root -   train_accuracy = 0.7155
2023/12/02 10:28:41 - INFO - root -   Epoch: [2/300][0/283], lr: 0.00000010 	 loss = 1.4966(1.4966)
2023/12/02 10:28:59 - INFO - root -   Epoch: [2/300][20/283], lr: 0.00000010 	 loss = 0.4259(0.9623)
2023/12/02 10:29:07 - INFO - root -   Epoch: [2/300][40/283], lr: 0.00000010 	 loss = 0.3563(0.8492)
2023/12/02 10:29:36 - INFO - root -   Epoch: [2/300][60/283], lr: 0.00000010 	 loss = 2.4172(0.8307)
2023/12/02 10:30:05 - INFO - root -   Epoch: [2/300][80/283], lr: 0.00000010 	 loss = 1.1622(0.8307)
2023/12/02 10:30:13 - INFO - root -   Epoch: [2/300][100/283], lr: 0.00000010 	 loss = 0.2389(0.7584)
2023/12/02 10:30:25 - INFO - root -   Epoch: [2/300][120/283], lr: 0.00000010 	 loss = 1.2597(0.7086)
2023/12/02 10:30:49 - INFO - root -   Epoch: [2/300][140/283], lr: 0.00000010 	 loss = 0.3640(0.6790)
2023/12/02 10:31:05 - INFO - root -   Epoch: [2/300][160/283], lr: 0.00000010 	 loss = 0.6387(0.6626)
2023/12/02 10:31:24 - INFO - root -   Epoch: [2/300][180/283], lr: 0.00000010 	 loss = 0.6324(0.6416)
2023/12/02 10:31:37 - INFO - root -   Epoch: [2/300][200/283], lr: 0.00000010 	 loss = 0.7902(0.6297)
2023/12/02 10:31:54 - INFO - root -   Epoch: [2/300][220/283], lr: 0.00000010 	 loss = 0.2403(0.6179)
2023/12/02 10:32:02 - INFO - root -   Epoch: [2/300][240/283], lr: 0.00000010 	 loss = 0.3339(0.6174)
2023/12/02 10:32:25 - INFO - root -   Epoch: [2/300][260/283], lr: 0.00000010 	 loss = 0.1523(0.6111)
2023/12/02 10:32:33 - INFO - root -   Epoch: [2/300][280/283], lr: 0.00000010 	 loss = 0.9745(0.6063)
2023/12/02 10:32:34 - INFO - root -   Epoch: [2/300] 	 loss = 0.6045
2023/12/02 10:32:34 - INFO - root -   train_accuracy = 0.7067
2023/12/02 10:32:49 - INFO - root -   Epoch: [3/300][0/283], lr: 0.00000011 	 loss = 0.8815(0.8815)
2023/12/02 10:33:01 - INFO - root -   Epoch: [3/300][20/283], lr: 0.00000011 	 loss = 0.2408(0.6361)
2023/12/02 10:33:16 - INFO - root -   Epoch: [3/300][40/283], lr: 0.00000011 	 loss = 0.4643(0.6090)
2023/12/02 10:33:24 - INFO - root -   Epoch: [3/300][60/283], lr: 0.00000011 	 loss = 0.7990(0.6397)
2023/12/02 10:33:36 - INFO - root -   Epoch: [3/300][80/283], lr: 0.00000011 	 loss = 1.0140(0.6758)
2023/12/02 10:33:57 - INFO - root -   Epoch: [3/300][100/283], lr: 0.00000011 	 loss = 0.5265(0.6474)
2023/12/02 10:34:05 - INFO - root -   Epoch: [3/300][120/283], lr: 0.00000011 	 loss = 0.4702(0.6039)
2023/12/02 10:34:31 - INFO - root -   Epoch: [3/300][140/283], lr: 0.00000011 	 loss = 0.3731(0.5832)
2023/12/02 10:34:46 - INFO - root -   Epoch: [3/300][160/283], lr: 0.00000011 	 loss = 1.2699(0.5733)
2023/12/02 10:35:00 - INFO - root -   Epoch: [3/300][180/283], lr: 0.00000011 	 loss = 0.6514(0.5539)
2023/12/02 10:35:22 - INFO - root -   Epoch: [3/300][200/283], lr: 0.00000011 	 loss = 1.0778(0.5591)
2023/12/02 10:35:32 - INFO - root -   Epoch: [3/300][220/283], lr: 0.00000011 	 loss = 0.5292(0.5562)
2023/12/02 10:35:40 - INFO - root -   Epoch: [3/300][240/283], lr: 0.00000011 	 loss = 0.0847(0.5505)
2023/12/02 10:36:05 - INFO - root -   Epoch: [3/300][260/283], lr: 0.00000011 	 loss = 0.0955(0.5521)
2023/12/02 10:36:13 - INFO - root -   Epoch: [3/300][280/283], lr: 0.00000011 	 loss = 0.5439(0.5489)
2023/12/02 10:36:14 - INFO - root -   Epoch: [3/300] 	 loss = 0.5467
2023/12/02 10:36:14 - INFO - root -   train_accuracy = 0.7438
2023/12/02 10:36:17 - INFO - root -   Epoch: [4/300][0/283], lr: 0.00000011 	 loss = 1.4954(1.4954)
2023/12/02 10:36:51 - INFO - root -   Epoch: [4/300][20/283], lr: 0.00000011 	 loss = 0.1261(0.8619)
2023/12/02 10:36:59 - INFO - root -   Epoch: [4/300][40/283], lr: 0.00000011 	 loss = 0.6901(0.7872)
2023/12/02 10:37:22 - INFO - root -   Epoch: [4/300][60/283], lr: 0.00000011 	 loss = 1.3350(0.7473)
2023/12/02 10:37:39 - INFO - root -   Epoch: [4/300][80/283], lr: 0.00000011 	 loss = 0.6720(0.7283)
2023/12/02 10:37:47 - INFO - root -   Epoch: [4/300][100/283], lr: 0.00000011 	 loss = 0.1813(0.6693)
2023/12/02 10:38:06 - INFO - root -   Epoch: [4/300][120/283], lr: 0.00000011 	 loss = 0.3974(0.6287)
2023/12/02 10:38:20 - INFO - root -   Epoch: [4/300][140/283], lr: 0.00000011 	 loss = 0.2560(0.6069)
2023/12/02 10:38:40 - INFO - root -   Epoch: [4/300][160/283], lr: 0.00000011 	 loss = 0.5341(0.5911)
2023/12/02 10:39:02 - INFO - root -   Epoch: [4/300][180/283], lr: 0.00000011 	 loss = 0.3662(0.5661)
2023/12/02 10:39:21 - INFO - root -   Epoch: [4/300][200/283], lr: 0.00000011 	 loss = 1.1103(0.5651)
2023/12/02 10:39:36 - INFO - root -   Epoch: [4/300][220/283], lr: 0.00000011 	 loss = 0.1604(0.5562)
2023/12/02 10:39:48 - INFO - root -   Epoch: [4/300][240/283], lr: 0.00000011 	 loss = 0.2078(0.5634)
2023/12/02 10:40:06 - INFO - root -   Epoch: [4/300][260/283], lr: 0.00000011 	 loss = 0.2543(0.5749)
2023/12/02 10:40:16 - INFO - root -   Epoch: [4/300][280/283], lr: 0.00000011 	 loss = 0.9254(0.5652)
2023/12/02 10:40:17 - INFO - root -   Epoch: [4/300] 	 loss = 0.5630
2023/12/02 10:40:27 - INFO - root -   precision = 0.7483
2023/12/02 10:40:27 - INFO - root -   eval_loss = 0.4991
2023/12/02 10:40:27 - INFO - root -   eval_acc = 0.7483
2023/12/02 10:40:27 - INFO - root -   train_accuracy = 0.7314
2023/12/02 10:40:46 - INFO - root -   Epoch: [5/300][0/283], lr: 0.00000011 	 loss = 0.7357(0.7357)
2023/12/02 10:41:00 - INFO - root -   Epoch: [5/300][20/283], lr: 0.00000011 	 loss = 0.1202(0.7906)
2023/12/02 10:41:24 - INFO - root -   Epoch: [5/300][40/283], lr: 0.00000011 	 loss = 0.6401(0.6981)
2023/12/02 10:41:36 - INFO - root -   Epoch: [5/300][60/283], lr: 0.00000011 	 loss = 2.3360(0.7547)
2023/12/02 10:41:53 - INFO - root -   Epoch: [5/300][80/283], lr: 0.00000011 	 loss = 0.9678(0.7201)
2023/12/02 10:42:01 - INFO - root -   Epoch: [5/300][100/283], lr: 0.00000011 	 loss = 0.1883(0.6907)
2023/12/02 10:42:21 - INFO - root -   Epoch: [5/300][120/283], lr: 0.00000011 	 loss = 0.9536(0.6590)
2023/12/02 10:42:41 - INFO - root -   Epoch: [5/300][140/283], lr: 0.00000011 	 loss = 0.2627(0.6413)
2023/12/02 10:42:55 - INFO - root -   Epoch: [5/300][160/283], lr: 0.00000011 	 loss = 0.4325(0.6211)
2023/12/02 10:43:03 - INFO - root -   Epoch: [5/300][180/283], lr: 0.00000011 	 loss = 0.9042(0.5902)
2023/12/02 10:43:30 - INFO - root -   Epoch: [5/300][200/283], lr: 0.00000011 	 loss = 0.8388(0.5900)
2023/12/02 10:43:38 - INFO - root -   Epoch: [5/300][220/283], lr: 0.00000011 	 loss = 0.3976(0.5728)
2023/12/02 10:43:52 - INFO - root -   Epoch: [5/300][240/283], lr: 0.00000011 	 loss = 0.3225(0.5666)
2023/12/02 10:44:09 - INFO - root -   Epoch: [5/300][260/283], lr: 0.00000011 	 loss = 0.0537(0.5677)
2023/12/02 10:44:17 - INFO - root -   Epoch: [5/300][280/283], lr: 0.00000011 	 loss = 1.4728(0.5678)
2023/12/02 10:44:18 - INFO - root -   Epoch: [5/300] 	 loss = 0.5657
2023/12/02 10:44:18 - INFO - root -   train_accuracy = 0.7155
2023/12/02 10:44:31 - INFO - root -   Epoch: [6/300][0/283], lr: 0.00000011 	 loss = 0.5014(0.5014)
2023/12/02 10:44:46 - INFO - root -   Epoch: [6/300][20/283], lr: 0.00000011 	 loss = 0.3528(0.7818)
2023/12/02 10:45:00 - INFO - root -   Epoch: [6/300][40/283], lr: 0.00000011 	 loss = 0.3343(0.7095)
2023/12/02 10:45:11 - INFO - root -   Epoch: [6/300][60/283], lr: 0.00000011 	 loss = 1.4407(0.7031)
2023/12/02 10:45:22 - INFO - root -   Epoch: [6/300][80/283], lr: 0.00000011 	 loss = 0.2144(0.7057)
2023/12/02 10:45:35 - INFO - root -   Epoch: [6/300][100/283], lr: 0.00000011 	 loss = 0.3539(0.6601)
2023/12/02 10:46:01 - INFO - root -   Epoch: [6/300][120/283], lr: 0.00000011 	 loss = 0.8995(0.6234)
2023/12/02 10:46:20 - INFO - root -   Epoch: [6/300][140/283], lr: 0.00000011 	 loss = 0.2595(0.5966)
2023/12/02 10:46:28 - INFO - root -   Epoch: [6/300][160/283], lr: 0.00000011 	 loss = 0.2764(0.5670)
2023/12/02 10:46:41 - INFO - root -   Epoch: [6/300][180/283], lr: 0.00000011 	 loss = 0.5244(0.5412)
2023/12/02 10:46:50 - INFO - root -   Epoch: [6/300][200/283], lr: 0.00000011 	 loss = 0.7953(0.5428)
2023/12/02 10:47:17 - INFO - root -   Epoch: [6/300][220/283], lr: 0.00000011 	 loss = 0.2285(0.5323)
2023/12/02 10:47:35 - INFO - root -   Epoch: [6/300][240/283], lr: 0.00000011 	 loss = 0.2141(0.5285)
2023/12/02 10:47:51 - INFO - root -   Epoch: [6/300][260/283], lr: 0.00000011 	 loss = 0.1804(0.5241)
2023/12/02 10:47:59 - INFO - root -   Epoch: [6/300][280/283], lr: 0.00000011 	 loss = 1.5056(0.5201)
2023/12/02 10:48:00 - INFO - root -   Epoch: [6/300] 	 loss = 0.5172
2023/12/02 10:48:00 - INFO - root -   train_accuracy = 0.7686
2023/12/02 10:48:02 - INFO - root -   Epoch: [7/300][0/283], lr: 0.00000011 	 loss = 0.6305(0.6305)
2023/12/02 10:48:37 - INFO - root -   Epoch: [7/300][20/283], lr: 0.00000011 	 loss = 0.3913(0.6889)
2023/12/02 10:48:49 - INFO - root -   Epoch: [7/300][40/283], lr: 0.00000011 	 loss = 0.1722(0.6499)
2023/12/02 10:49:00 - INFO - root -   Epoch: [7/300][60/283], lr: 0.00000011 	 loss = 0.7463(0.6211)
2023/12/02 10:49:21 - INFO - root -   Epoch: [7/300][80/283], lr: 0.00000011 	 loss = 0.3672(0.6116)
2023/12/02 10:49:29 - INFO - root -   Epoch: [7/300][100/283], lr: 0.00000011 	 loss = 0.4051(0.5765)
2023/12/02 10:49:43 - INFO - root -   Epoch: [7/300][120/283], lr: 0.00000011 	 loss = 0.2289(0.5619)
2023/12/02 10:49:57 - INFO - root -   Epoch: [7/300][140/283], lr: 0.00000011 	 loss = 0.1172(0.5433)
2023/12/02 10:50:08 - INFO - root -   Epoch: [7/300][160/283], lr: 0.00000011 	 loss = 1.1860(0.5430)
2023/12/02 10:50:26 - INFO - root -   Epoch: [7/300][180/283], lr: 0.00000011 	 loss = 1.1670(0.5270)
2023/12/02 10:50:45 - INFO - root -   Epoch: [7/300][200/283], lr: 0.00000011 	 loss = 1.0819(0.5289)
2023/12/02 10:50:53 - INFO - root -   Epoch: [7/300][220/283], lr: 0.00000011 	 loss = 0.6267(0.5124)
2023/12/02 10:51:07 - INFO - root -   Epoch: [7/300][240/283], lr: 0.00000011 	 loss = 0.0680(0.5159)
2023/12/02 10:51:29 - INFO - root -   Epoch: [7/300][260/283], lr: 0.00000011 	 loss = 0.1574(0.5110)
2023/12/02 10:51:37 - INFO - root -   Epoch: [7/300][280/283], lr: 0.00000011 	 loss = 1.4939(0.5086)
2023/12/02 10:51:37 - INFO - root -   Epoch: [7/300] 	 loss = 0.5071
2023/12/02 10:51:37 - INFO - root -   train_accuracy = 0.7721
2023/12/02 10:51:53 - INFO - root -   Epoch: [8/300][0/283], lr: 0.00000011 	 loss = 0.4239(0.4239)
2023/12/02 10:52:12 - INFO - root -   Epoch: [8/300][20/283], lr: 0.00000011 	 loss = 0.6272(0.7036)
2023/12/02 10:52:21 - INFO - root -   Epoch: [8/300][40/283], lr: 0.00000011 	 loss = 0.3810(0.6211)
2023/12/02 10:52:35 - INFO - root -   Epoch: [8/300][60/283], lr: 0.00000011 	 loss = 1.0722(0.6291)
2023/12/02 10:53:03 - INFO - root -   Epoch: [8/300][80/283], lr: 0.00000011 	 loss = 1.5190(0.6515)
2023/12/02 10:53:14 - INFO - root -   Epoch: [8/300][100/283], lr: 0.00000011 	 loss = 0.2081(0.5918)
2023/12/02 10:53:23 - INFO - root -   Epoch: [8/300][120/283], lr: 0.00000011 	 loss = 0.1833(0.5966)
2023/12/02 10:53:40 - INFO - root -   Epoch: [8/300][140/283], lr: 0.00000011 	 loss = 0.3052(0.5725)
2023/12/02 10:53:48 - INFO - root -   Epoch: [8/300][160/283], lr: 0.00000011 	 loss = 0.4651(0.5561)
2023/12/02 10:54:15 - INFO - root -   Epoch: [8/300][180/283], lr: 0.00000011 	 loss = 0.5300(0.5358)
2023/12/02 10:54:23 - INFO - root -   Epoch: [8/300][200/283], lr: 0.00000011 	 loss = 0.2221(0.5234)
2023/12/02 10:54:34 - INFO - root -   Epoch: [8/300][220/283], lr: 0.00000011 	 loss = 0.2037(0.5090)
2023/12/02 10:54:59 - INFO - root -   Epoch: [8/300][240/283], lr: 0.00000011 	 loss = 0.2724(0.5111)
2023/12/02 10:55:14 - INFO - root -   Epoch: [8/300][260/283], lr: 0.00000011 	 loss = 0.2270(0.5129)
2023/12/02 10:55:23 - INFO - root -   Epoch: [8/300][280/283], lr: 0.00000011 	 loss = 1.2254(0.5053)
2023/12/02 10:55:24 - INFO - root -   Epoch: [8/300] 	 loss = 0.5034
2023/12/02 10:55:24 - INFO - root -   train_accuracy = 0.7756
2023/12/02 10:55:25 - INFO - root -   Epoch: [9/300][0/283], lr: 0.00000012 	 loss = 0.5008(0.5008)
2023/12/02 10:55:53 - INFO - root -   Epoch: [9/300][20/283], lr: 0.00000012 	 loss = 0.5472(0.5881)
2023/12/02 10:56:07 - INFO - root -   Epoch: [9/300][40/283], lr: 0.00000012 	 loss = 0.4592(0.6634)
2023/12/02 10:56:19 - INFO - root -   Epoch: [9/300][60/283], lr: 0.00000012 	 loss = 3.0032(0.6770)
2023/12/02 10:56:38 - INFO - root -   Epoch: [9/300][80/283], lr: 0.00000012 	 loss = 0.3352(0.6593)
2023/12/02 10:56:50 - INFO - root -   Epoch: [9/300][100/283], lr: 0.00000012 	 loss = 0.2847(0.6002)
2023/12/02 10:57:12 - INFO - root -   Epoch: [9/300][120/283], lr: 0.00000012 	 loss = 1.0075(0.5872)
2023/12/02 10:57:28 - INFO - root -   Epoch: [9/300][140/283], lr: 0.00000012 	 loss = 0.0923(0.5698)
2023/12/02 10:57:46 - INFO - root -   Epoch: [9/300][160/283], lr: 0.00000012 	 loss = 0.2090(0.5626)
2023/12/02 10:58:00 - INFO - root -   Epoch: [9/300][180/283], lr: 0.00000012 	 loss = 0.7707(0.5452)
2023/12/02 10:58:25 - INFO - root -   Epoch: [9/300][200/283], lr: 0.00000012 	 loss = 0.4663(0.5369)
2023/12/02 10:58:33 - INFO - root -   Epoch: [9/300][220/283], lr: 0.00000012 	 loss = 0.1295(0.5173)
2023/12/02 10:58:58 - INFO - root -   Epoch: [9/300][240/283], lr: 0.00000012 	 loss = 0.0976(0.5109)
2023/12/02 10:59:06 - INFO - root -   Epoch: [9/300][260/283], lr: 0.00000012 	 loss = 0.1368(0.5007)
2023/12/02 10:59:14 - INFO - root -   Epoch: [9/300][280/283], lr: 0.00000012 	 loss = 0.6138(0.4932)
2023/12/02 10:59:15 - INFO - root -   Epoch: [9/300] 	 loss = 0.4917
2023/12/02 10:59:24 - INFO - root -   precision = 0.7762
2023/12/02 10:59:24 - INFO - root -   eval_loss = 0.4901
2023/12/02 10:59:24 - INFO - root -   eval_acc = 0.7762
2023/12/02 10:59:25 - INFO - root -   train_accuracy = 0.7686
2023/12/02 10:59:27 - INFO - root -   Epoch: [10/300][0/283], lr: 0.00000012 	 loss = 0.5759(0.5759)
2023/12/02 11:00:03 - INFO - root -   Epoch: [10/300][20/283], lr: 0.00000012 	 loss = 0.2683(0.8110)
2023/12/02 11:00:11 - INFO - root -   Epoch: [10/300][40/283], lr: 0.00000012 	 loss = 0.2236(0.7580)
2023/12/02 11:00:34 - INFO - root -   Epoch: [10/300][60/283], lr: 0.00000012 	 loss = 1.5578(0.7456)
2023/12/02 11:00:49 - INFO - root -   Epoch: [10/300][80/283], lr: 0.00000012 	 loss = 0.5853(0.7106)
2023/12/02 11:01:12 - INFO - root -   Epoch: [10/300][100/283], lr: 0.00000012 	 loss = 0.3503(0.6647)
2023/12/02 11:01:25 - INFO - root -   Epoch: [10/300][120/283], lr: 0.00000012 	 loss = 1.1213(0.6258)
2023/12/02 11:01:41 - INFO - root -   Epoch: [10/300][140/283], lr: 0.00000012 	 loss = 0.1554(0.5962)
2023/12/02 11:01:59 - INFO - root -   Epoch: [10/300][160/283], lr: 0.00000012 	 loss = 0.1332(0.5784)
2023/12/02 11:02:06 - INFO - root -   Epoch: [10/300][180/283], lr: 0.00000012 	 loss = 0.3937(0.5580)
2023/12/02 11:02:26 - INFO - root -   Epoch: [10/300][200/283], lr: 0.00000012 	 loss = 0.3917(0.5554)
2023/12/02 11:02:39 - INFO - root -   Epoch: [10/300][220/283], lr: 0.00000012 	 loss = 0.0764(0.5361)
2023/12/02 11:02:52 - INFO - root -   Epoch: [10/300][240/283], lr: 0.00000012 	 loss = 0.0915(0.5332)
2023/12/02 11:03:13 - INFO - root -   Epoch: [10/300][260/283], lr: 0.00000012 	 loss = 0.5585(0.5353)
2023/12/02 11:03:21 - INFO - root -   Epoch: [10/300][280/283], lr: 0.00000012 	 loss = 0.8676(0.5309)
2023/12/02 11:03:22 - INFO - root -   Epoch: [10/300] 	 loss = 0.5279
2023/12/02 11:03:22 - INFO - root -   train_accuracy = 0.7650
2023/12/02 11:03:24 - INFO - root -   Epoch: [11/300][0/283], lr: 0.00000012 	 loss = 1.2502(1.2502)
2023/12/02 11:03:53 - INFO - root -   Epoch: [11/300][20/283], lr: 0.00000012 	 loss = 0.0921(0.8123)
2023/12/02 11:04:10 - INFO - root -   Epoch: [11/300][40/283], lr: 0.00000012 	 loss = 0.1917(0.7352)
2023/12/02 11:04:23 - INFO - root -   Epoch: [11/300][60/283], lr: 0.00000012 	 loss = 1.0197(0.7111)
2023/12/02 11:04:31 - INFO - root -   Epoch: [11/300][80/283], lr: 0.00000012 	 loss = 0.5951(0.6619)
2023/12/02 11:04:49 - INFO - root -   Epoch: [11/300][100/283], lr: 0.00000012 	 loss = 0.1431(0.6133)
2023/12/02 11:04:57 - INFO - root -   Epoch: [11/300][120/283], lr: 0.00000012 	 loss = 0.4329(0.5893)
2023/12/02 11:05:21 - INFO - root -   Epoch: [11/300][140/283], lr: 0.00000012 	 loss = 0.0842(0.5624)
2023/12/02 11:05:39 - INFO - root -   Epoch: [11/300][160/283], lr: 0.00000012 	 loss = 0.2649(0.5439)
2023/12/02 11:05:55 - INFO - root -   Epoch: [11/300][180/283], lr: 0.00000012 	 loss = 1.5970(0.5322)
2023/12/02 11:06:17 - INFO - root -   Epoch: [11/300][200/283], lr: 0.00000012 	 loss = 0.6270(0.5283)
2023/12/02 11:06:25 - INFO - root -   Epoch: [11/300][220/283], lr: 0.00000012 	 loss = 0.8391(0.5219)
2023/12/02 11:06:43 - INFO - root -   Epoch: [11/300][240/283], lr: 0.00000012 	 loss = 0.1911(0.5216)
2023/12/02 11:06:59 - INFO - root -   Epoch: [11/300][260/283], lr: 0.00000012 	 loss = 0.3075(0.5166)
2023/12/02 11:07:07 - INFO - root -   Epoch: [11/300][280/283], lr: 0.00000012 	 loss = 0.7763(0.5108)
2023/12/02 11:07:08 - INFO - root -   Epoch: [11/300] 	 loss = 0.5092
2023/12/02 11:07:08 - INFO - root -   train_accuracy = 0.7650
2023/12/02 11:07:09 - INFO - root -   Epoch: [12/300][0/283], lr: 0.00000012 	 loss = 0.6797(0.6797)
2023/12/02 11:07:39 - INFO - root -   Epoch: [12/300][20/283], lr: 0.00000012 	 loss = 0.5412(0.6215)
2023/12/02 11:07:51 - INFO - root -   Epoch: [12/300][40/283], lr: 0.00000012 	 loss = 0.0702(0.5843)
2023/12/02 11:08:09 - INFO - root -   Epoch: [12/300][60/283], lr: 0.00000012 	 loss = 0.9701(0.6292)
2023/12/02 11:08:17 - INFO - root -   Epoch: [12/300][80/283], lr: 0.00000012 	 loss = 0.3222(0.6224)
2023/12/02 11:08:32 - INFO - root -   Epoch: [12/300][100/283], lr: 0.00000012 	 loss = 0.1780(0.5836)
2023/12/02 11:08:45 - INFO - root -   Epoch: [12/300][120/283], lr: 0.00000012 	 loss = 0.8145(0.5629)
2023/12/02 11:09:05 - INFO - root -   Epoch: [12/300][140/283], lr: 0.00000012 	 loss = 0.2296(0.5495)
2023/12/02 11:09:21 - INFO - root -   Epoch: [12/300][160/283], lr: 0.00000012 	 loss = 0.1274(0.5249)
2023/12/02 11:09:29 - INFO - root -   Epoch: [12/300][180/283], lr: 0.00000012 	 loss = 0.6604(0.5033)
2023/12/02 11:09:49 - INFO - root -   Epoch: [12/300][200/283], lr: 0.00000012 	 loss = 0.2346(0.5036)
2023/12/02 11:10:05 - INFO - root -   Epoch: [12/300][220/283], lr: 0.00000012 	 loss = 0.2566(0.4931)
2023/12/02 11:10:21 - INFO - root -   Epoch: [12/300][240/283], lr: 0.00000012 	 loss = 0.1623(0.4898)
2023/12/02 11:10:35 - INFO - root -   Epoch: [12/300][260/283], lr: 0.00000012 	 loss = 0.1156(0.4904)
2023/12/02 11:10:43 - INFO - root -   Epoch: [12/300][280/283], lr: 0.00000012 	 loss = 0.2074(0.4816)
2023/12/02 11:10:44 - INFO - root -   Epoch: [12/300] 	 loss = 0.4786
2023/12/02 11:10:44 - INFO - root -   train_accuracy = 0.7898
2023/12/02 11:10:46 - INFO - root -   Epoch: [13/300][0/283], lr: 0.00000012 	 loss = 0.2681(0.2681)
2023/12/02 11:11:19 - INFO - root -   Epoch: [13/300][20/283], lr: 0.00000012 	 loss = 0.2489(0.6050)
2023/12/02 11:11:27 - INFO - root -   Epoch: [13/300][40/283], lr: 0.00000012 	 loss = 0.0794(0.6355)
2023/12/02 11:11:42 - INFO - root -   Epoch: [13/300][60/283], lr: 0.00000012 	 loss = 1.1724(0.6178)
2023/12/02 11:12:00 - INFO - root -   Epoch: [13/300][80/283], lr: 0.00000012 	 loss = 0.8851(0.6007)
2023/12/02 11:12:14 - INFO - root -   Epoch: [13/300][100/283], lr: 0.00000012 	 loss = 0.1952(0.5676)
2023/12/02 11:12:29 - INFO - root -   Epoch: [13/300][120/283], lr: 0.00000012 	 loss = 0.1552(0.5672)
2023/12/02 11:12:39 - INFO - root -   Epoch: [13/300][140/283], lr: 0.00000012 	 loss = 0.1339(0.5367)
2023/12/02 11:13:01 - INFO - root -   Epoch: [13/300][160/283], lr: 0.00000012 	 loss = 0.3581(0.5244)
2023/12/02 11:13:11 - INFO - root -   Epoch: [13/300][180/283], lr: 0.00000012 	 loss = 0.8037(0.5167)
2023/12/02 11:13:35 - INFO - root -   Epoch: [13/300][200/283], lr: 0.00000012 	 loss = 0.8974(0.5161)
2023/12/02 11:13:54 - INFO - root -   Epoch: [13/300][220/283], lr: 0.00000012 	 loss = 0.5727(0.5053)
2023/12/02 11:14:10 - INFO - root -   Epoch: [13/300][240/283], lr: 0.00000012 	 loss = 0.2146(0.5081)
2023/12/02 11:14:27 - INFO - root -   Epoch: [13/300][260/283], lr: 0.00000012 	 loss = 0.1211(0.5040)
2023/12/02 11:14:35 - INFO - root -   Epoch: [13/300][280/283], lr: 0.00000012 	 loss = 0.7020(0.5010)
2023/12/02 11:14:36 - INFO - root -   Epoch: [13/300] 	 loss = 0.4995
2023/12/02 11:14:36 - INFO - root -   train_accuracy = 0.7721
2023/12/02 11:14:52 - INFO - root -   Epoch: [14/300][0/283], lr: 0.00000012 	 loss = 0.3041(0.3041)
2023/12/02 11:15:14 - INFO - root -   Epoch: [14/300][20/283], lr: 0.00000012 	 loss = 0.5515(0.5611)
2023/12/02 11:15:32 - INFO - root -   Epoch: [14/300][40/283], lr: 0.00000012 	 loss = 0.2523(0.6088)
2023/12/02 11:15:39 - INFO - root -   Epoch: [14/300][60/283], lr: 0.00000012 	 loss = 1.7898(0.6322)
2023/12/02 11:16:03 - INFO - root -   Epoch: [14/300][80/283], lr: 0.00000012 	 loss = 0.4291(0.6480)
2023/12/02 11:16:20 - INFO - root -   Epoch: [14/300][100/283], lr: 0.00000012 	 loss = 0.1126(0.5969)
2023/12/02 11:16:36 - INFO - root -   Epoch: [14/300][120/283], lr: 0.00000012 	 loss = 0.5824(0.5908)
2023/12/02 11:16:46 - INFO - root -   Epoch: [14/300][140/283], lr: 0.00000012 	 loss = 0.0558(0.5497)
2023/12/02 11:16:57 - INFO - root -   Epoch: [14/300][160/283], lr: 0.00000012 	 loss = 1.0805(0.5292)
2023/12/02 11:17:10 - INFO - root -   Epoch: [14/300][180/283], lr: 0.00000012 	 loss = 1.9298(0.5168)
2023/12/02 11:17:34 - INFO - root -   Epoch: [14/300][200/283], lr: 0.00000012 	 loss = 0.5925(0.5120)
2023/12/02 11:17:51 - INFO - root -   Epoch: [14/300][220/283], lr: 0.00000012 	 loss = 0.1498(0.5001)
2023/12/02 11:18:10 - INFO - root -   Epoch: [14/300][240/283], lr: 0.00000012 	 loss = 0.3324(0.5093)
2023/12/02 11:18:22 - INFO - root -   Epoch: [14/300][260/283], lr: 0.00000012 	 loss = 0.2640(0.5048)
2023/12/02 11:18:34 - INFO - root -   Epoch: [14/300][280/283], lr: 0.00000012 	 loss = 0.5451(0.4958)
2023/12/02 11:18:35 - INFO - root -   Epoch: [14/300] 	 loss = 0.4930
2023/12/02 11:18:45 - INFO - root -   precision = 0.7902
2023/12/02 11:18:45 - INFO - root -   eval_loss = 0.4510
2023/12/02 11:18:45 - INFO - root -   eval_acc = 0.7902
2023/12/02 11:18:45 - INFO - root -   train_accuracy = 0.7756
2023/12/02 11:18:58 - INFO - root -   Epoch: [15/300][0/283], lr: 0.00000013 	 loss = 0.5680(0.5680)
2023/12/02 11:19:13 - INFO - root -   Epoch: [15/300][20/283], lr: 0.00000013 	 loss = 0.2084(0.5493)
2023/12/02 11:19:22 - INFO - root -   Epoch: [15/300][40/283], lr: 0.00000013 	 loss = 0.2143(0.6720)
2023/12/02 11:19:36 - INFO - root -   Epoch: [15/300][60/283], lr: 0.00000013 	 loss = 1.4948(0.6913)
2023/12/02 11:20:04 - INFO - root -   Epoch: [15/300][80/283], lr: 0.00000013 	 loss = 0.6006(0.6844)
2023/12/02 11:20:11 - INFO - root -   Epoch: [15/300][100/283], lr: 0.00000013 	 loss = 0.5278(0.6502)
2023/12/02 11:20:25 - INFO - root -   Epoch: [15/300][120/283], lr: 0.00000013 	 loss = 0.5101(0.6391)
2023/12/02 11:20:48 - INFO - root -   Epoch: [15/300][140/283], lr: 0.00000013 	 loss = 0.1456(0.5935)
2023/12/02 11:21:02 - INFO - root -   Epoch: [15/300][160/283], lr: 0.00000013 	 loss = 0.7174(0.5693)
2023/12/02 11:21:15 - INFO - root -   Epoch: [15/300][180/283], lr: 0.00000013 	 loss = 1.0661(0.5342)
2023/12/02 11:21:37 - INFO - root -   Epoch: [15/300][200/283], lr: 0.00000013 	 loss = 1.2506(0.5411)
2023/12/02 11:21:56 - INFO - root -   Epoch: [15/300][220/283], lr: 0.00000013 	 loss = 0.0843(0.5284)
2023/12/02 11:22:08 - INFO - root -   Epoch: [15/300][240/283], lr: 0.00000013 	 loss = 0.2040(0.5275)
2023/12/02 11:22:31 - INFO - root -   Epoch: [15/300][260/283], lr: 0.00000013 	 loss = 0.0798(0.5198)
2023/12/02 11:22:39 - INFO - root -   Epoch: [15/300][280/283], lr: 0.00000013 	 loss = 1.9052(0.5103)
2023/12/02 11:22:40 - INFO - root -   Epoch: [15/300] 	 loss = 0.5091
2023/12/02 11:22:40 - INFO - root -   train_accuracy = 0.7845
2023/12/02 11:22:53 - INFO - root -   Epoch: [16/300][0/283], lr: 0.00000013 	 loss = 0.5850(0.5850)
2023/12/02 11:23:07 - INFO - root -   Epoch: [16/300][20/283], lr: 0.00000013 	 loss = 0.2564(0.6004)
2023/12/02 11:23:25 - INFO - root -   Epoch: [16/300][40/283], lr: 0.00000013 	 loss = 0.1416(0.5537)
2023/12/02 11:23:35 - INFO - root -   Epoch: [16/300][60/283], lr: 0.00000013 	 loss = 0.8012(0.5344)
2023/12/02 11:23:57 - INFO - root -   Epoch: [16/300][80/283], lr: 0.00000013 	 loss = 0.3132(0.5386)
2023/12/02 11:24:08 - INFO - root -   Epoch: [16/300][100/283], lr: 0.00000013 	 loss = 0.1373(0.5205)
2023/12/02 11:24:16 - INFO - root -   Epoch: [16/300][120/283], lr: 0.00000013 	 loss = 0.4086(0.5298)
2023/12/02 11:24:24 - INFO - root -   Epoch: [16/300][140/283], lr: 0.00000013 	 loss = 0.2627(0.4956)
2023/12/02 11:24:34 - INFO - root -   Epoch: [16/300][160/283], lr: 0.00000013 	 loss = 0.2332(0.4689)
2023/12/02 11:24:53 - INFO - root -   Epoch: [16/300][180/283], lr: 0.00000013 	 loss = 1.1797(0.4521)
2023/12/02 11:25:10 - INFO - root -   Epoch: [16/300][200/283], lr: 0.00000013 	 loss = 0.5597(0.4608)
2023/12/02 11:25:24 - INFO - root -   Epoch: [16/300][220/283], lr: 0.00000013 	 loss = 0.0920(0.4478)
2023/12/02 11:25:45 - INFO - root -   Epoch: [16/300][240/283], lr: 0.00000013 	 loss = 0.2507(0.4519)
2023/12/02 11:25:56 - INFO - root -   Epoch: [16/300][260/283], lr: 0.00000013 	 loss = 0.0656(0.4456)
2023/12/02 11:26:05 - INFO - root -   Epoch: [16/300][280/283], lr: 0.00000013 	 loss = 1.1594(0.4441)
2023/12/02 11:26:06 - INFO - root -   Epoch: [16/300] 	 loss = 0.4417
2023/12/02 11:26:06 - INFO - root -   train_accuracy = 0.8021
2023/12/02 11:26:24 - INFO - root -   Epoch: [17/300][0/283], lr: 0.00000013 	 loss = 0.4291(0.4291)
2023/12/02 11:26:58 - INFO - root -   Epoch: [17/300][20/283], lr: 0.00000013 	 loss = 0.2959(0.5833)
2023/12/02 11:27:07 - INFO - root -   Epoch: [17/300][40/283], lr: 0.00000013 	 loss = 0.3071(0.5863)
2023/12/02 11:27:17 - INFO - root -   Epoch: [17/300][60/283], lr: 0.00000013 	 loss = 1.3257(0.6062)
2023/12/02 11:27:36 - INFO - root -   Epoch: [17/300][80/283], lr: 0.00000013 	 loss = 0.9648(0.5914)
2023/12/02 11:27:53 - INFO - root -   Epoch: [17/300][100/283], lr: 0.00000013 	 loss = 0.1970(0.5600)
2023/12/02 11:28:08 - INFO - root -   Epoch: [17/300][120/283], lr: 0.00000013 	 loss = 1.3718(0.5450)
2023/12/02 11:28:15 - INFO - root -   Epoch: [17/300][140/283], lr: 0.00000013 	 loss = 0.0343(0.5051)
2023/12/02 11:28:32 - INFO - root -   Epoch: [17/300][160/283], lr: 0.00000013 	 loss = 0.1089(0.4850)
2023/12/02 11:28:40 - INFO - root -   Epoch: [17/300][180/283], lr: 0.00000013 	 loss = 1.5539(0.4831)
2023/12/02 11:29:06 - INFO - root -   Epoch: [17/300][200/283], lr: 0.00000013 	 loss = 1.0785(0.4864)
2023/12/02 11:29:18 - INFO - root -   Epoch: [17/300][220/283], lr: 0.00000013 	 loss = 0.1813(0.4787)
2023/12/02 11:29:43 - INFO - root -   Epoch: [17/300][240/283], lr: 0.00000013 	 loss = 0.0787(0.4696)
2023/12/02 11:29:53 - INFO - root -   Epoch: [17/300][260/283], lr: 0.00000013 	 loss = 0.1268(0.4667)
2023/12/02 11:30:03 - INFO - root -   Epoch: [17/300][280/283], lr: 0.00000013 	 loss = 1.6446(0.4602)
2023/12/02 11:30:04 - INFO - root -   Epoch: [17/300] 	 loss = 0.4573
2023/12/02 11:30:04 - INFO - root -   train_accuracy = 0.7986
2023/12/02 11:30:06 - INFO - root -   Epoch: [18/300][0/283], lr: 0.00000013 	 loss = 0.5125(0.5125)
2023/12/02 11:30:36 - INFO - root -   Epoch: [18/300][20/283], lr: 0.00000013 	 loss = 0.2722(0.6511)
2023/12/02 11:30:44 - INFO - root -   Epoch: [18/300][40/283], lr: 0.00000013 	 loss = 0.1973(0.6865)
2023/12/02 11:31:04 - INFO - root -   Epoch: [18/300][60/283], lr: 0.00000013 	 loss = 1.3386(0.6661)
2023/12/02 11:31:20 - INFO - root -   Epoch: [18/300][80/283], lr: 0.00000013 	 loss = 0.4186(0.6599)
2023/12/02 11:31:28 - INFO - root -   Epoch: [18/300][100/283], lr: 0.00000013 	 loss = 0.3793(0.6153)
2023/12/02 11:31:47 - INFO - root -   Epoch: [18/300][120/283], lr: 0.00000013 	 loss = 0.9219(0.5940)
2023/12/02 11:32:02 - INFO - root -   Epoch: [18/300][140/283], lr: 0.00000013 	 loss = 0.1335(0.5611)
2023/12/02 11:32:24 - INFO - root -   Epoch: [18/300][160/283], lr: 0.00000013 	 loss = 0.9865(0.5376)
2023/12/02 11:32:32 - INFO - root -   Epoch: [18/300][180/283], lr: 0.00000013 	 loss = 0.6075(0.5100)
2023/12/02 11:32:51 - INFO - root -   Epoch: [18/300][200/283], lr: 0.00000013 	 loss = 0.4277(0.5046)
2023/12/02 11:33:12 - INFO - root -   Epoch: [18/300][220/283], lr: 0.00000013 	 loss = 0.0536(0.4916)
2023/12/02 11:33:20 - INFO - root -   Epoch: [18/300][240/283], lr: 0.00000013 	 loss = 0.4958(0.5042)
2023/12/02 11:33:39 - INFO - root -   Epoch: [18/300][260/283], lr: 0.00000013 	 loss = 0.2585(0.5006)
2023/12/02 11:33:47 - INFO - root -   Epoch: [18/300][280/283], lr: 0.00000013 	 loss = 1.3108(0.4889)
2023/12/02 11:33:48 - INFO - root -   Epoch: [18/300] 	 loss = 0.4865
2023/12/02 11:33:48 - INFO - root -   train_accuracy = 0.7703
2023/12/02 11:33:50 - INFO - root -   Epoch: [19/300][0/283], lr: 0.00000013 	 loss = 0.1271(0.1271)
2023/12/02 11:34:25 - INFO - root -   Epoch: [19/300][20/283], lr: 0.00000013 	 loss = 0.7537(0.5046)
2023/12/02 11:34:37 - INFO - root -   Epoch: [19/300][40/283], lr: 0.00000013 	 loss = 0.1637(0.5092)
2023/12/02 11:34:52 - INFO - root -   Epoch: [19/300][60/283], lr: 0.00000013 	 loss = 1.2400(0.5544)
2023/12/02 11:35:09 - INFO - root -   Epoch: [19/300][80/283], lr: 0.00000013 	 loss = 0.9916(0.5582)
2023/12/02 11:35:19 - INFO - root -   Epoch: [19/300][100/283], lr: 0.00000013 	 loss = 0.1811(0.5338)
2023/12/02 11:35:31 - INFO - root -   Epoch: [19/300][120/283], lr: 0.00000013 	 loss = 0.6362(0.5245)
2023/12/02 11:35:49 - INFO - root -   Epoch: [19/300][140/283], lr: 0.00000013 	 loss = 0.3094(0.4982)
2023/12/02 11:36:01 - INFO - root -   Epoch: [19/300][160/283], lr: 0.00000013 	 loss = 0.1527(0.4924)
2023/12/02 11:36:22 - INFO - root -   Epoch: [19/300][180/283], lr: 0.00000013 	 loss = 1.0305(0.4711)
2023/12/02 11:36:43 - INFO - root -   Epoch: [19/300][200/283], lr: 0.00000013 	 loss = 0.4910(0.4766)
2023/12/02 11:36:51 - INFO - root -   Epoch: [19/300][220/283], lr: 0.00000013 	 loss = 0.2511(0.4735)
2023/12/02 11:37:03 - INFO - root -   Epoch: [19/300][240/283], lr: 0.00000013 	 loss = 0.2311(0.4702)
2023/12/02 11:37:19 - INFO - root -   Epoch: [19/300][260/283], lr: 0.00000013 	 loss = 0.2097(0.4564)
2023/12/02 11:37:27 - INFO - root -   Epoch: [19/300][280/283], lr: 0.00000013 	 loss = 1.0872(0.4554)
2023/12/02 11:37:27 - INFO - root -   Epoch: [19/300] 	 loss = 0.4540
2023/12/02 11:37:37 - INFO - root -   precision = 0.8042
2023/12/02 11:37:37 - INFO - root -   eval_loss = 0.4204
2023/12/02 11:37:37 - INFO - root -   eval_acc = 0.8042
2023/12/02 11:37:38 - INFO - root -   train_accuracy = 0.8145
2023/12/02 11:37:41 - INFO - root -   Epoch: [20/300][0/283], lr: 0.00000013 	 loss = 0.4773(0.4773)
2023/12/02 11:38:03 - INFO - root -   Epoch: [20/300][20/283], lr: 0.00000013 	 loss = 0.2437(0.5951)
2023/12/02 11:38:26 - INFO - root -   Epoch: [20/300][40/283], lr: 0.00000013 	 loss = 0.2488(0.6120)
2023/12/02 11:38:34 - INFO - root -   Epoch: [20/300][60/283], lr: 0.00000013 	 loss = 1.2580(0.5942)
2023/12/02 11:38:42 - INFO - root -   Epoch: [20/300][80/283], lr: 0.00000013 	 loss = 0.7529(0.5916)
2023/12/02 11:39:02 - INFO - root -   Epoch: [20/300][100/283], lr: 0.00000013 	 loss = 0.4979(0.5749)
2023/12/02 11:39:22 - INFO - root -   Epoch: [20/300][120/283], lr: 0.00000013 	 loss = 0.5148(0.5948)
2023/12/02 11:39:37 - INFO - root -   Epoch: [20/300][140/283], lr: 0.00000013 	 loss = 0.0295(0.5509)
2023/12/02 11:39:59 - INFO - root -   Epoch: [20/300][160/283], lr: 0.00000013 	 loss = 0.1183(0.5225)
2023/12/02 11:40:07 - INFO - root -   Epoch: [20/300][180/283], lr: 0.00000013 	 loss = 0.3440(0.4971)
2023/12/02 11:40:19 - INFO - root -   Epoch: [20/300][200/283], lr: 0.00000013 	 loss = 0.4080(0.4959)
2023/12/02 11:40:46 - INFO - root -   Epoch: [20/300][220/283], lr: 0.00000013 	 loss = 0.4169(0.4781)
2023/12/02 11:40:59 - INFO - root -   Epoch: [20/300][240/283], lr: 0.00000013 	 loss = 0.6827(0.4689)
2023/12/02 11:41:17 - INFO - root -   Epoch: [20/300][260/283], lr: 0.00000013 	 loss = 0.1964(0.4744)
2023/12/02 11:41:25 - INFO - root -   Epoch: [20/300][280/283], lr: 0.00000013 	 loss = 0.6012(0.4565)
2023/12/02 11:41:26 - INFO - root -   Epoch: [20/300] 	 loss = 0.4547
2023/12/02 11:41:26 - INFO - root -   train_accuracy = 0.7968
2023/12/02 11:41:59 - INFO - root -   Epoch: [21/300][0/283], lr: 0.00000014 	 loss = 1.2973(1.2973)
2023/12/02 11:42:06 - INFO - root -   Epoch: [21/300][20/283], lr: 0.00000014 	 loss = 0.2489(0.6468)
2023/12/02 11:42:31 - INFO - root -   Epoch: [21/300][40/283], lr: 0.00000014 	 loss = 0.2253(0.6049)
2023/12/02 11:42:39 - INFO - root -   Epoch: [21/300][60/283], lr: 0.00000014 	 loss = 1.6411(0.5785)
2023/12/02 11:42:52 - INFO - root -   Epoch: [21/300][80/283], lr: 0.00000014 	 loss = 0.2124(0.5619)
2023/12/02 11:43:15 - INFO - root -   Epoch: [21/300][100/283], lr: 0.00000014 	 loss = 0.5053(0.5360)
2023/12/02 11:43:24 - INFO - root -   Epoch: [21/300][120/283], lr: 0.00000014 	 loss = 0.5271(0.5360)
2023/12/02 11:43:41 - INFO - root -   Epoch: [21/300][140/283], lr: 0.00000014 	 loss = 0.1876(0.5154)
2023/12/02 11:43:59 - INFO - root -   Epoch: [21/300][160/283], lr: 0.00000014 	 loss = 0.3029(0.4953)
2023/12/02 11:44:07 - INFO - root -   Epoch: [21/300][180/283], lr: 0.00000014 	 loss = 1.0415(0.4752)
2023/12/02 11:44:28 - INFO - root -   Epoch: [21/300][200/283], lr: 0.00000014 	 loss = 0.8437(0.4811)
2023/12/02 11:44:41 - INFO - root -   Epoch: [21/300][220/283], lr: 0.00000014 	 loss = 0.0699(0.4683)
2023/12/02 11:44:50 - INFO - root -   Epoch: [21/300][240/283], lr: 0.00000014 	 loss = 0.3053(0.4615)
2023/12/02 11:45:15 - INFO - root -   Epoch: [21/300][260/283], lr: 0.00000014 	 loss = 0.4741(0.4635)
2023/12/02 11:45:23 - INFO - root -   Epoch: [21/300][280/283], lr: 0.00000014 	 loss = 0.5952(0.4505)
2023/12/02 11:45:23 - INFO - root -   Epoch: [21/300] 	 loss = 0.4477
2023/12/02 11:45:23 - INFO - root -   train_accuracy = 0.7915
2023/12/02 11:45:36 - INFO - root -   Epoch: [22/300][0/283], lr: 0.00000014 	 loss = 0.2810(0.2810)
2023/12/02 11:45:44 - INFO - root -   Epoch: [22/300][20/283], lr: 0.00000014 	 loss = 0.1476(0.5025)
2023/12/02 11:46:00 - INFO - root -   Epoch: [22/300][40/283], lr: 0.00000014 	 loss = 0.1654(0.5379)
2023/12/02 11:46:26 - INFO - root -   Epoch: [22/300][60/283], lr: 0.00000014 	 loss = 1.6785(0.5743)
2023/12/02 11:46:39 - INFO - root -   Epoch: [22/300][80/283], lr: 0.00000014 	 loss = 0.9146(0.5843)
2023/12/02 11:47:03 - INFO - root -   Epoch: [22/300][100/283], lr: 0.00000014 	 loss = 0.2833(0.5762)
2023/12/02 11:47:11 - INFO - root -   Epoch: [22/300][120/283], lr: 0.00000014 	 loss = 0.4294(0.5802)
2023/12/02 11:47:29 - INFO - root -   Epoch: [22/300][140/283], lr: 0.00000014 	 loss = 0.2019(0.5420)
2023/12/02 11:47:38 - INFO - root -   Epoch: [22/300][160/283], lr: 0.00000014 	 loss = 0.1760(0.5227)
2023/12/02 11:47:52 - INFO - root -   Epoch: [22/300][180/283], lr: 0.00000014 	 loss = 0.5385(0.5004)
2023/12/02 11:48:01 - INFO - root -   Epoch: [22/300][200/283], lr: 0.00000014 	 loss = 0.2916(0.4963)
2023/12/02 11:48:17 - INFO - root -   Epoch: [22/300][220/283], lr: 0.00000014 	 loss = 0.1673(0.4847)
2023/12/02 11:48:44 - INFO - root -   Epoch: [22/300][240/283], lr: 0.00000014 	 loss = 0.2508(0.4974)
2023/12/02 11:48:56 - INFO - root -   Epoch: [22/300][260/283], lr: 0.00000014 	 loss = 0.1476(0.4897)
2023/12/02 11:49:04 - INFO - root -   Epoch: [22/300][280/283], lr: 0.00000014 	 loss = 0.8604(0.4802)
2023/12/02 11:49:05 - INFO - root -   Epoch: [22/300] 	 loss = 0.4776
2023/12/02 11:49:05 - INFO - root -   train_accuracy = 0.7933
2023/12/02 11:49:06 - INFO - root -   Epoch: [23/300][0/283], lr: 0.00000014 	 loss = 0.0890(0.0890)
2023/12/02 11:49:26 - INFO - root -   Epoch: [23/300][20/283], lr: 0.00000014 	 loss = 0.3032(0.4564)
2023/12/02 11:49:40 - INFO - root -   Epoch: [23/300][40/283], lr: 0.00000014 	 loss = 0.6664(0.4985)
2023/12/02 11:49:58 - INFO - root -   Epoch: [23/300][60/283], lr: 0.00000014 	 loss = 1.1748(0.5271)
2023/12/02 11:50:06 - INFO - root -   Epoch: [23/300][80/283], lr: 0.00000014 	 loss = 0.4957(0.5253)
2023/12/02 11:50:21 - INFO - root -   Epoch: [23/300][100/283], lr: 0.00000014 	 loss = 0.2681(0.5146)
2023/12/02 11:50:29 - INFO - root -   Epoch: [23/300][120/283], lr: 0.00000014 	 loss = 1.8416(0.5188)
2023/12/02 11:50:44 - INFO - root -   Epoch: [23/300][140/283], lr: 0.00000014 	 loss = 0.1002(0.4896)
2023/12/02 11:50:59 - INFO - root -   Epoch: [23/300][160/283], lr: 0.00000014 	 loss = 0.6066(0.4775)
2023/12/02 11:51:21 - INFO - root -   Epoch: [23/300][180/283], lr: 0.00000014 	 loss = 0.9462(0.4594)
2023/12/02 11:51:37 - INFO - root -   Epoch: [23/300][200/283], lr: 0.00000014 	 loss = 0.2221(0.4605)
2023/12/02 11:51:52 - INFO - root -   Epoch: [23/300][220/283], lr: 0.00000014 	 loss = 0.0773(0.4558)
2023/12/02 11:52:00 - INFO - root -   Epoch: [23/300][240/283], lr: 0.00000014 	 loss = 0.0387(0.4523)
2023/12/02 11:52:20 - INFO - root -   Epoch: [23/300][260/283], lr: 0.00000014 	 loss = 0.2966(0.4542)
2023/12/02 11:52:29 - INFO - root -   Epoch: [23/300][280/283], lr: 0.00000014 	 loss = 0.7582(0.4520)
2023/12/02 11:52:30 - INFO - root -   Epoch: [23/300] 	 loss = 0.4493
2023/12/02 11:52:30 - INFO - root -   train_accuracy = 0.8021
2023/12/02 11:52:45 - INFO - root -   Epoch: [24/300][0/283], lr: 0.00000014 	 loss = 0.6273(0.6273)
2023/12/02 11:53:08 - INFO - root -   Epoch: [24/300][20/283], lr: 0.00000014 	 loss = 0.2441(0.3852)
2023/12/02 11:53:20 - INFO - root -   Epoch: [24/300][40/283], lr: 0.00000014 	 loss = 0.3148(0.4871)
2023/12/02 11:53:45 - INFO - root -   Epoch: [24/300][60/283], lr: 0.00000014 	 loss = 1.3583(0.5360)
2023/12/02 11:54:16 - INFO - root -   Epoch: [24/300][80/283], lr: 0.00000014 	 loss = 0.5265(0.5827)
2023/12/02 11:54:24 - INFO - root -   Epoch: [24/300][100/283], lr: 0.00000014 	 loss = 0.2507(0.5644)
2023/12/02 11:54:37 - INFO - root -   Epoch: [24/300][120/283], lr: 0.00000014 	 loss = 0.7818(0.5729)
2023/12/02 11:54:58 - INFO - root -   Epoch: [24/300][140/283], lr: 0.00000014 	 loss = 0.0958(0.5343)
2023/12/02 11:55:22 - INFO - root -   Epoch: [24/300][160/283], lr: 0.00000014 	 loss = 1.7478(0.5216)
2023/12/02 11:55:38 - INFO - root -   Epoch: [24/300][180/283], lr: 0.00000014 	 loss = 0.2242(0.4899)
2023/12/02 11:55:55 - INFO - root -   Epoch: [24/300][200/283], lr: 0.00000014 	 loss = 0.2310(0.4806)
2023/12/02 11:56:14 - INFO - root -   Epoch: [24/300][220/283], lr: 0.00000014 	 loss = 0.1813(0.4747)
2023/12/02 11:56:22 - INFO - root -   Epoch: [24/300][240/283], lr: 0.00000014 	 loss = 0.1208(0.4853)
2023/12/02 11:56:39 - INFO - root -   Epoch: [24/300][260/283], lr: 0.00000014 	 loss = 0.1062(0.4778)
2023/12/02 11:56:48 - INFO - root -   Epoch: [24/300][280/283], lr: 0.00000014 	 loss = 1.3624(0.4685)
2023/12/02 11:56:49 - INFO - root -   Epoch: [24/300] 	 loss = 0.4658
2023/12/02 11:56:59 - INFO - root -   precision = 0.8182
2023/12/02 11:56:59 - INFO - root -   eval_loss = 0.4227
2023/12/02 11:56:59 - INFO - root -   eval_acc = 0.8182
2023/12/02 11:56:59 - INFO - root -   train_accuracy = 0.8057
2023/12/02 11:57:16 - INFO - root -   Epoch: [25/300][0/283], lr: 0.00000014 	 loss = 0.4851(0.4851)
2023/12/02 11:57:39 - INFO - root -   Epoch: [25/300][20/283], lr: 0.00000014 	 loss = 0.9225(0.4731)
2023/12/02 11:57:47 - INFO - root -   Epoch: [25/300][40/283], lr: 0.00000014 	 loss = 0.3524(0.5559)
2023/12/02 11:58:06 - INFO - root -   Epoch: [25/300][60/283], lr: 0.00000014 	 loss = 2.9897(0.5869)
2023/12/02 11:58:16 - INFO - root -   Epoch: [25/300][80/283], lr: 0.00000014 	 loss = 1.1191(0.5792)
2023/12/02 11:58:40 - INFO - root -   Epoch: [25/300][100/283], lr: 0.00000014 	 loss = 0.1112(0.5319)
2023/12/02 11:58:55 - INFO - root -   Epoch: [25/300][120/283], lr: 0.00000014 	 loss = 0.2049(0.5181)
2023/12/02 11:59:06 - INFO - root -   Epoch: [25/300][140/283], lr: 0.00000014 	 loss = 0.0560(0.4857)
2023/12/02 11:59:27 - INFO - root -   Epoch: [25/300][160/283], lr: 0.00000014 	 loss = 1.3524(0.4746)
2023/12/02 11:59:35 - INFO - root -   Epoch: [25/300][180/283], lr: 0.00000014 	 loss = 0.2373(0.4496)
2023/12/02 11:59:56 - INFO - root -   Epoch: [25/300][200/283], lr: 0.00000014 	 loss = 0.3662(0.4457)
2023/12/02 12:00:20 - INFO - root -   Epoch: [25/300][220/283], lr: 0.00000014 	 loss = 0.1311(0.4357)
2023/12/02 12:00:28 - INFO - root -   Epoch: [25/300][240/283], lr: 0.00000014 	 loss = 0.5373(0.4390)
2023/12/02 12:00:48 - INFO - root -   Epoch: [25/300][260/283], lr: 0.00000014 	 loss = 0.0923(0.4401)
2023/12/02 12:00:56 - INFO - root -   Epoch: [25/300][280/283], lr: 0.00000014 	 loss = 0.7033(0.4373)
2023/12/02 12:00:57 - INFO - root -   Epoch: [25/300] 	 loss = 0.4360
2023/12/02 12:00:57 - INFO - root -   train_accuracy = 0.7986
2023/12/02 12:01:13 - INFO - root -   Epoch: [26/300][0/283], lr: 0.00000015 	 loss = 1.1368(1.1368)
2023/12/02 12:01:33 - INFO - root -   Epoch: [26/300][20/283], lr: 0.00000015 	 loss = 0.6010(0.6400)
2023/12/02 12:01:53 - INFO - root -   Epoch: [26/300][40/283], lr: 0.00000015 	 loss = 0.0916(0.5804)
2023/12/02 12:02:01 - INFO - root -   Epoch: [26/300][60/283], lr: 0.00000015 	 loss = 2.8365(0.6071)
2023/12/02 12:02:33 - INFO - root -   Epoch: [26/300][80/283], lr: 0.00000015 	 loss = 1.2269(0.5966)
2023/12/02 12:02:41 - INFO - root -   Epoch: [26/300][100/283], lr: 0.00000015 	 loss = 0.1218(0.5590)
2023/12/02 12:02:59 - INFO - root -   Epoch: [26/300][120/283], lr: 0.00000015 	 loss = 0.1569(0.5662)
2023/12/02 12:03:17 - INFO - root -   Epoch: [26/300][140/283], lr: 0.00000015 	 loss = 0.0724(0.5299)
2023/12/02 12:03:25 - INFO - root -   Epoch: [26/300][160/283], lr: 0.00000015 	 loss = 0.9009(0.5199)
2023/12/02 12:03:46 - INFO - root -   Epoch: [26/300][180/283], lr: 0.00000015 	 loss = 0.4690(0.4986)
2023/12/02 12:03:57 - INFO - root -   Epoch: [26/300][200/283], lr: 0.00000015 	 loss = 0.2283(0.4869)
2023/12/02 12:04:17 - INFO - root -   Epoch: [26/300][220/283], lr: 0.00000015 	 loss = 0.4387(0.4704)
2023/12/02 12:04:25 - INFO - root -   Epoch: [26/300][240/283], lr: 0.00000015 	 loss = 0.2460(0.4685)
2023/12/02 12:04:40 - INFO - root -   Epoch: [26/300][260/283], lr: 0.00000015 	 loss = 0.1491(0.4674)
2023/12/02 12:04:48 - INFO - root -   Epoch: [26/300][280/283], lr: 0.00000015 	 loss = 0.4708(0.4520)
2023/12/02 12:04:49 - INFO - root -   Epoch: [26/300] 	 loss = 0.4494
2023/12/02 12:04:49 - INFO - root -   train_accuracy = 0.8110
2023/12/02 12:05:06 - INFO - root -   Epoch: [27/300][0/283], lr: 0.00000015 	 loss = 0.1581(0.1581)
2023/12/02 12:05:25 - INFO - root -   Epoch: [27/300][20/283], lr: 0.00000015 	 loss = 0.4392(0.4974)
2023/12/02 12:05:33 - INFO - root -   Epoch: [27/300][40/283], lr: 0.00000015 	 loss = 0.6339(0.5333)
2023/12/02 12:05:53 - INFO - root -   Epoch: [27/300][60/283], lr: 0.00000015 	 loss = 0.8164(0.5163)
2023/12/02 12:06:09 - INFO - root -   Epoch: [27/300][80/283], lr: 0.00000015 	 loss = 0.3956(0.5209)
2023/12/02 12:06:18 - INFO - root -   Epoch: [27/300][100/283], lr: 0.00000015 	 loss = 0.2353(0.4989)
2023/12/02 12:06:35 - INFO - root -   Epoch: [27/300][120/283], lr: 0.00000015 	 loss = 0.3616(0.5035)
2023/12/02 12:06:43 - INFO - root -   Epoch: [27/300][140/283], lr: 0.00000015 	 loss = 0.1932(0.4870)
2023/12/02 12:07:04 - INFO - root -   Epoch: [27/300][160/283], lr: 0.00000015 	 loss = 1.1674(0.4636)
2023/12/02 12:07:12 - INFO - root -   Epoch: [27/300][180/283], lr: 0.00000015 	 loss = 0.3155(0.4410)
2023/12/02 12:07:19 - INFO - root -   Epoch: [27/300][200/283], lr: 0.00000015 	 loss = 0.3381(0.4348)
2023/12/02 12:07:33 - INFO - root -   Epoch: [27/300][220/283], lr: 0.00000015 	 loss = 0.2449(0.4300)
2023/12/02 12:08:00 - INFO - root -   Epoch: [27/300][240/283], lr: 0.00000015 	 loss = 0.2021(0.4398)
2023/12/02 12:08:08 - INFO - root -   Epoch: [27/300][260/283], lr: 0.00000015 	 loss = 0.1780(0.4398)
2023/12/02 12:08:16 - INFO - root -   Epoch: [27/300][280/283], lr: 0.00000015 	 loss = 1.5663(0.4337)
2023/12/02 12:08:17 - INFO - root -   Epoch: [27/300] 	 loss = 0.4339
2023/12/02 12:08:17 - INFO - root -   train_accuracy = 0.8233
2023/12/02 12:08:18 - INFO - root -   Epoch: [28/300][0/283], lr: 0.00000015 	 loss = 0.0835(0.0835)
2023/12/02 12:08:42 - INFO - root -   Epoch: [28/300][20/283], lr: 0.00000015 	 loss = 0.0899(0.4301)
2023/12/02 12:08:52 - INFO - root -   Epoch: [28/300][40/283], lr: 0.00000015 	 loss = 0.1176(0.4536)
2023/12/02 12:09:19 - INFO - root -   Epoch: [28/300][60/283], lr: 0.00000015 	 loss = 0.9903(0.5108)
2023/12/02 12:09:40 - INFO - root -   Epoch: [28/300][80/283], lr: 0.00000015 	 loss = 0.5459(0.5157)
2023/12/02 12:09:54 - INFO - root -   Epoch: [28/300][100/283], lr: 0.00000015 	 loss = 0.2553(0.5179)
2023/12/02 12:10:02 - INFO - root -   Epoch: [28/300][120/283], lr: 0.00000015 	 loss = 0.1294(0.5053)
2023/12/02 12:10:21 - INFO - root -   Epoch: [28/300][140/283], lr: 0.00000015 	 loss = 0.0823(0.4784)
2023/12/02 12:10:43 - INFO - root -   Epoch: [28/300][160/283], lr: 0.00000015 	 loss = 0.4802(0.4663)
2023/12/02 12:10:53 - INFO - root -   Epoch: [28/300][180/283], lr: 0.00000015 	 loss = 1.5155(0.4555)
2023/12/02 12:11:06 - INFO - root -   Epoch: [28/300][200/283], lr: 0.00000015 	 loss = 0.5251(0.4586)
2023/12/02 12:11:22 - INFO - root -   Epoch: [28/300][220/283], lr: 0.00000015 	 loss = 0.0777(0.4461)
2023/12/02 12:11:37 - INFO - root -   Epoch: [28/300][240/283], lr: 0.00000015 	 loss = 0.1005(0.4484)
2023/12/02 12:11:45 - INFO - root -   Epoch: [28/300][260/283], lr: 0.00000015 	 loss = 0.1855(0.4448)
2023/12/02 12:11:53 - INFO - root -   Epoch: [28/300][280/283], lr: 0.00000015 	 loss = 0.2724(0.4332)
2023/12/02 12:11:54 - INFO - root -   Epoch: [28/300] 	 loss = 0.4317
2023/12/02 12:11:54 - INFO - root -   train_accuracy = 0.8163
2023/12/02 12:11:56 - INFO - root -   Epoch: [29/300][0/283], lr: 0.00000015 	 loss = 0.1910(0.1910)
2023/12/02 12:12:18 - INFO - root -   Epoch: [29/300][20/283], lr: 0.00000015 	 loss = 0.8976(0.4751)
2023/12/02 12:12:37 - INFO - root -   Epoch: [29/300][40/283], lr: 0.00000015 	 loss = 0.0655(0.5067)
2023/12/02 12:12:49 - INFO - root -   Epoch: [29/300][60/283], lr: 0.00000015 	 loss = 0.6923(0.5227)
2023/12/02 12:13:04 - INFO - root -   Epoch: [29/300][80/283], lr: 0.00000015 	 loss = 0.5007(0.5065)
2023/12/02 12:13:23 - INFO - root -   Epoch: [29/300][100/283], lr: 0.00000015 	 loss = 0.2298(0.5011)
2023/12/02 12:13:45 - INFO - root -   Epoch: [29/300][120/283], lr: 0.00000015 	 loss = 0.2039(0.4906)
2023/12/02 12:14:02 - INFO - root -   Epoch: [29/300][140/283], lr: 0.00000015 	 loss = 0.0455(0.4978)
2023/12/02 12:14:21 - INFO - root -   Epoch: [29/300][160/283], lr: 0.00000015 	 loss = 0.2421(0.4771)
2023/12/02 12:14:29 - INFO - root -   Epoch: [29/300][180/283], lr: 0.00000015 	 loss = 0.4227(0.4535)
2023/12/02 12:14:44 - INFO - root -   Epoch: [29/300][200/283], lr: 0.00000015 	 loss = 1.6415(0.4599)
2023/12/02 12:14:55 - INFO - root -   Epoch: [29/300][220/283], lr: 0.00000015 	 loss = 0.3526(0.4433)
2023/12/02 12:15:17 - INFO - root -   Epoch: [29/300][240/283], lr: 0.00000015 	 loss = 0.3744(0.4562)
2023/12/02 12:15:28 - INFO - root -   Epoch: [29/300][260/283], lr: 0.00000015 	 loss = 0.0728(0.4549)
2023/12/02 12:15:36 - INFO - root -   Epoch: [29/300][280/283], lr: 0.00000015 	 loss = 0.9620(0.4524)
2023/12/02 12:15:37 - INFO - root -   Epoch: [29/300] 	 loss = 0.4507
2023/12/02 12:15:46 - INFO - root -   precision = 0.8182
2023/12/02 12:15:46 - INFO - root -   eval_loss = 0.4157
2023/12/02 12:15:46 - INFO - root -   eval_acc = 0.8182
2023/12/02 12:15:47 - INFO - root -   train_accuracy = 0.8092
2023/12/02 12:16:03 - INFO - root -   Epoch: [30/300][0/283], lr: 0.00000015 	 loss = 0.5727(0.5727)
2023/12/02 12:16:31 - INFO - root -   Epoch: [30/300][20/283], lr: 0.00000015 	 loss = 0.2648(0.5094)
2023/12/02 12:16:39 - INFO - root -   Epoch: [30/300][40/283], lr: 0.00000015 	 loss = 0.3609(0.5300)
2023/12/02 12:16:51 - INFO - root -   Epoch: [30/300][60/283], lr: 0.00000015 	 loss = 1.9691(0.5792)
2023/12/02 12:17:17 - INFO - root -   Epoch: [30/300][80/283], lr: 0.00000015 	 loss = 0.3601(0.5615)
2023/12/02 12:17:27 - INFO - root -   Epoch: [30/300][100/283], lr: 0.00000015 	 loss = 0.2588(0.5416)
2023/12/02 12:17:37 - INFO - root -   Epoch: [30/300][120/283], lr: 0.00000015 	 loss = 0.2950(0.5528)
2023/12/02 12:17:49 - INFO - root -   Epoch: [30/300][140/283], lr: 0.00000015 	 loss = 0.1377(0.5049)
2023/12/02 12:18:07 - INFO - root -   Epoch: [30/300][160/283], lr: 0.00000015 	 loss = 0.1689(0.4831)
2023/12/02 12:18:22 - INFO - root -   Epoch: [30/300][180/283], lr: 0.00000015 	 loss = 1.1965(0.4612)
2023/12/02 12:18:30 - INFO - root -   Epoch: [30/300][200/283], lr: 0.00000015 	 loss = 0.6866(0.4549)
2023/12/02 12:18:50 - INFO - root -   Epoch: [30/300][220/283], lr: 0.00000015 	 loss = 0.0440(0.4317)
2023/12/02 12:18:58 - INFO - root -   Epoch: [30/300][240/283], lr: 0.00000015 	 loss = 0.0457(0.4342)
2023/12/02 12:19:18 - INFO - root -   Epoch: [30/300][260/283], lr: 0.00000015 	 loss = 0.1105(0.4356)
2023/12/02 12:19:26 - INFO - root -   Epoch: [30/300][280/283], lr: 0.00000015 	 loss = 0.5768(0.4327)
2023/12/02 12:19:26 - INFO - root -   Epoch: [30/300] 	 loss = 0.4302
2023/12/02 12:19:26 - INFO - root -   train_accuracy = 0.8233
2023/12/02 12:19:42 - INFO - root -   Epoch: [31/300][0/283], lr: 0.00000015 	 loss = 0.0683(0.0683)
2023/12/02 12:20:01 - INFO - root -   Epoch: [31/300][20/283], lr: 0.00000015 	 loss = 0.9107(0.5789)
2023/12/02 12:20:09 - INFO - root -   Epoch: [31/300][40/283], lr: 0.00000015 	 loss = 0.1639(0.5008)
2023/12/02 12:20:21 - INFO - root -   Epoch: [31/300][60/283], lr: 0.00000015 	 loss = 1.6283(0.4869)
2023/12/02 12:20:34 - INFO - root -   Epoch: [31/300][80/283], lr: 0.00000015 	 loss = 0.2279(0.5046)
2023/12/02 12:20:53 - INFO - root -   Epoch: [31/300][100/283], lr: 0.00000015 	 loss = 0.3234(0.4904)
2023/12/02 12:21:02 - INFO - root -   Epoch: [31/300][120/283], lr: 0.00000015 	 loss = 0.4269(0.5089)
2023/12/02 12:21:10 - INFO - root -   Epoch: [31/300][140/283], lr: 0.00000015 	 loss = 0.0925(0.4832)
2023/12/02 12:21:35 - INFO - root -   Epoch: [31/300][160/283], lr: 0.00000015 	 loss = 0.4093(0.4532)
2023/12/02 12:22:07 - INFO - root -   Epoch: [31/300][180/283], lr: 0.00000015 	 loss = 0.4990(0.4333)
2023/12/02 12:22:15 - INFO - root -   Epoch: [31/300][200/283], lr: 0.00000015 	 loss = 0.2968(0.4251)
2023/12/02 12:22:38 - INFO - root -   Epoch: [31/300][220/283], lr: 0.00000015 	 loss = 0.3338(0.4191)
2023/12/02 12:22:46 - INFO - root -   Epoch: [31/300][240/283], lr: 0.00000015 	 loss = 1.2945(0.4212)
2023/12/02 12:22:59 - INFO - root -   Epoch: [31/300][260/283], lr: 0.00000015 	 loss = 0.1013(0.4198)
2023/12/02 12:23:09 - INFO - root -   Epoch: [31/300][280/283], lr: 0.00000015 	 loss = 0.3134(0.4216)
2023/12/02 12:23:09 - INFO - root -   Epoch: [31/300] 	 loss = 0.4193
2023/12/02 12:23:09 - INFO - root -   train_accuracy = 0.8269
2023/12/02 12:23:24 - INFO - root -   Epoch: [32/300][0/283], lr: 0.00000016 	 loss = 0.1217(0.1217)
2023/12/02 12:23:39 - INFO - root -   Epoch: [32/300][20/283], lr: 0.00000016 	 loss = 0.3423(0.5257)
2023/12/02 12:23:55 - INFO - root -   Epoch: [32/300][40/283], lr: 0.00000016 	 loss = 0.1721(0.6291)
2023/12/02 12:24:17 - INFO - root -   Epoch: [32/300][60/283], lr: 0.00000016 	 loss = 1.9280(0.5608)
2023/12/02 12:24:32 - INFO - root -   Epoch: [32/300][80/283], lr: 0.00000016 	 loss = 0.1782(0.5986)
2023/12/02 12:24:40 - INFO - root -   Epoch: [32/300][100/283], lr: 0.00000016 	 loss = 0.4824(0.5681)
2023/12/02 12:24:56 - INFO - root -   Epoch: [32/300][120/283], lr: 0.00000016 	 loss = 0.1351(0.5602)
2023/12/02 12:25:14 - INFO - root -   Epoch: [32/300][140/283], lr: 0.00000016 	 loss = 0.2524(0.5183)
2023/12/02 12:25:22 - INFO - root -   Epoch: [32/300][160/283], lr: 0.00000016 	 loss = 0.7625(0.4978)
2023/12/02 12:25:43 - INFO - root -   Epoch: [32/300][180/283], lr: 0.00000016 	 loss = 0.6234(0.4648)
2023/12/02 12:25:51 - INFO - root -   Epoch: [32/300][200/283], lr: 0.00000016 	 loss = 0.2715(0.4644)
2023/12/02 12:26:11 - INFO - root -   Epoch: [32/300][220/283], lr: 0.00000016 	 loss = 0.2006(0.4603)
2023/12/02 12:26:26 - INFO - root -   Epoch: [32/300][240/283], lr: 0.00000016 	 loss = 0.0248(0.4686)
2023/12/02 12:26:34 - INFO - root -   Epoch: [32/300][260/283], lr: 0.00000016 	 loss = 0.1560(0.4627)
2023/12/02 12:26:42 - INFO - root -   Epoch: [32/300][280/283], lr: 0.00000016 	 loss = 0.7091(0.4492)
2023/12/02 12:26:43 - INFO - root -   Epoch: [32/300] 	 loss = 0.4478
2023/12/02 12:26:43 - INFO - root -   train_accuracy = 0.7880
2023/12/02 12:26:44 - INFO - root -   Epoch: [33/300][0/283], lr: 0.00000016 	 loss = 0.6074(0.6074)
2023/12/02 12:27:12 - INFO - root -   Epoch: [33/300][20/283], lr: 0.00000016 	 loss = 0.1661(0.4363)
2023/12/02 12:27:28 - INFO - root -   Epoch: [33/300][40/283], lr: 0.00000016 	 loss = 0.1205(0.4236)
2023/12/02 12:27:44 - INFO - root -   Epoch: [33/300][60/283], lr: 0.00000016 	 loss = 2.2764(0.4877)
2023/12/02 12:27:52 - INFO - root -   Epoch: [33/300][80/283], lr: 0.00000016 	 loss = 0.1708(0.4975)
2023/12/02 12:28:13 - INFO - root -   Epoch: [33/300][100/283], lr: 0.00000016 	 loss = 0.1804(0.4637)
2023/12/02 12:28:24 - INFO - root -   Epoch: [33/300][120/283], lr: 0.00000016 	 loss = 0.2473(0.4404)
2023/12/02 12:28:38 - INFO - root -   Epoch: [33/300][140/283], lr: 0.00000016 	 loss = 0.0742(0.4084)
2023/12/02 12:29:01 - INFO - root -   Epoch: [33/300][160/283], lr: 0.00000016 	 loss = 0.0788(0.3971)
2023/12/02 12:29:08 - INFO - root -   Epoch: [33/300][180/283], lr: 0.00000016 	 loss = 0.1334(0.3882)
2023/12/02 12:29:31 - INFO - root -   Epoch: [33/300][200/283], lr: 0.00000016 	 loss = 0.1796(0.3895)
2023/12/02 12:29:47 - INFO - root -   Epoch: [33/300][220/283], lr: 0.00000016 	 loss = 0.0339(0.3782)
2023/12/02 12:30:08 - INFO - root -   Epoch: [33/300][240/283], lr: 0.00000016 	 loss = 0.3991(0.3876)
2023/12/02 12:30:17 - INFO - root -   Epoch: [33/300][260/283], lr: 0.00000016 	 loss = 0.1260(0.3816)
2023/12/02 12:30:25 - INFO - root -   Epoch: [33/300][280/283], lr: 0.00000016 	 loss = 0.4443(0.3718)
2023/12/02 12:30:26 - INFO - root -   Epoch: [33/300] 	 loss = 0.3695
2023/12/02 12:30:26 - INFO - root -   train_accuracy = 0.8569
2023/12/02 12:30:28 - INFO - root -   Epoch: [34/300][0/283], lr: 0.00000016 	 loss = 0.5843(0.5843)
2023/12/02 12:31:00 - INFO - root -   Epoch: [34/300][20/283], lr: 0.00000016 	 loss = 0.8433(0.6560)
2023/12/02 12:31:22 - INFO - root -   Epoch: [34/300][40/283], lr: 0.00000016 	 loss = 0.0342(0.6640)
2023/12/02 12:31:30 - INFO - root -   Epoch: [34/300][60/283], lr: 0.00000016 	 loss = 0.9802(0.6180)
2023/12/02 12:31:46 - INFO - root -   Epoch: [34/300][80/283], lr: 0.00000016 	 loss = 0.1309(0.5824)
2023/12/02 12:31:55 - INFO - root -   Epoch: [34/300][100/283], lr: 0.00000016 	 loss = 0.1335(0.5667)
2023/12/02 12:32:12 - INFO - root -   Epoch: [34/300][120/283], lr: 0.00000016 	 loss = 0.9317(0.5887)
2023/12/02 12:32:33 - INFO - root -   Epoch: [34/300][140/283], lr: 0.00000016 	 loss = 0.0718(0.5543)
2023/12/02 12:32:41 - INFO - root -   Epoch: [34/300][160/283], lr: 0.00000016 	 loss = 0.0479(0.5209)
2023/12/02 12:33:01 - INFO - root -   Epoch: [34/300][180/283], lr: 0.00000016 	 loss = 0.4237(0.4913)
2023/12/02 12:33:09 - INFO - root -   Epoch: [34/300][200/283], lr: 0.00000016 	 loss = 0.1649(0.4876)
2023/12/02 12:33:28 - INFO - root -   Epoch: [34/300][220/283], lr: 0.00000016 	 loss = 0.0678(0.4675)
2023/12/02 12:33:41 - INFO - root -   Epoch: [34/300][240/283], lr: 0.00000016 	 loss = 0.0823(0.4650)
2023/12/02 12:33:59 - INFO - root -   Epoch: [34/300][260/283], lr: 0.00000016 	 loss = 0.1180(0.4636)
2023/12/02 12:34:07 - INFO - root -   Epoch: [34/300][280/283], lr: 0.00000016 	 loss = 0.3105(0.4547)
2023/12/02 12:34:08 - INFO - root -   Epoch: [34/300] 	 loss = 0.4529
2023/12/02 12:34:17 - INFO - root -   precision = 0.8252
2023/12/02 12:34:17 - INFO - root -   eval_loss = 0.4081
2023/12/02 12:34:17 - INFO - root -   eval_acc = 0.8252
2023/12/02 12:34:18 - INFO - root -   train_accuracy = 0.8110
2023/12/02 12:34:20 - INFO - root -   Epoch: [35/300][0/283], lr: 0.00000016 	 loss = 0.5012(0.5012)
2023/12/02 12:34:47 - INFO - root -   Epoch: [35/300][20/283], lr: 0.00000016 	 loss = 0.3387(0.6480)
2023/12/02 12:35:12 - INFO - root -   Epoch: [35/300][40/283], lr: 0.00000016 	 loss = 0.2876(0.5706)
2023/12/02 12:35:20 - INFO - root -   Epoch: [35/300][60/283], lr: 0.00000016 	 loss = 1.3050(0.5491)
2023/12/02 12:35:28 - INFO - root -   Epoch: [35/300][80/283], lr: 0.00000016 	 loss = 0.3561(0.5281)
2023/12/02 12:35:56 - INFO - root -   Epoch: [35/300][100/283], lr: 0.00000016 	 loss = 0.3002(0.5105)
2023/12/02 12:36:16 - INFO - root -   Epoch: [35/300][120/283], lr: 0.00000016 	 loss = 0.5487(0.5305)
2023/12/02 12:36:39 - INFO - root -   Epoch: [35/300][140/283], lr: 0.00000016 	 loss = 0.0764(0.5021)
2023/12/02 12:36:47 - INFO - root -   Epoch: [35/300][160/283], lr: 0.00000016 	 loss = 0.4115(0.4750)
2023/12/02 12:36:55 - INFO - root -   Epoch: [35/300][180/283], lr: 0.00000016 	 loss = 0.3058(0.4593)
2023/12/02 12:37:06 - INFO - root -   Epoch: [35/300][200/283], lr: 0.00000016 	 loss = 0.2682(0.4533)
2023/12/02 12:37:25 - INFO - root -   Epoch: [35/300][220/283], lr: 0.00000016 	 loss = 0.1790(0.4433)
2023/12/02 12:37:32 - INFO - root -   Epoch: [35/300][240/283], lr: 0.00000016 	 loss = 0.0889(0.4492)
2023/12/02 12:37:47 - INFO - root -   Epoch: [35/300][260/283], lr: 0.00000016 	 loss = 0.2634(0.4417)
2023/12/02 12:37:55 - INFO - root -   Epoch: [35/300][280/283], lr: 0.00000016 	 loss = 1.9257(0.4359)
2023/12/02 12:37:56 - INFO - root -   Epoch: [35/300] 	 loss = 0.4343
2023/12/02 12:37:56 - INFO - root -   train_accuracy = 0.8163
2023/12/02 12:38:17 - INFO - root -   Epoch: [36/300][0/283], lr: 0.00000016 	 loss = 0.4394(0.4394)
2023/12/02 12:38:25 - INFO - root -   Epoch: [36/300][20/283], lr: 0.00000016 	 loss = 0.1309(0.4218)
2023/12/02 12:38:35 - INFO - root -   Epoch: [36/300][40/283], lr: 0.00000016 	 loss = 0.2919(0.4826)
2023/12/02 12:39:01 - INFO - root -   Epoch: [36/300][60/283], lr: 0.00000016 	 loss = 2.1484(0.4949)
2023/12/02 12:39:15 - INFO - root -   Epoch: [36/300][80/283], lr: 0.00000016 	 loss = 0.4764(0.4999)
2023/12/02 12:39:27 - INFO - root -   Epoch: [36/300][100/283], lr: 0.00000016 	 loss = 0.3723(0.4865)
2023/12/02 12:39:44 - INFO - root -   Epoch: [36/300][120/283], lr: 0.00000016 	 loss = 0.0990(0.4832)
2023/12/02 12:39:52 - INFO - root -   Epoch: [36/300][140/283], lr: 0.00000016 	 loss = 0.1612(0.4507)
2023/12/02 12:40:00 - INFO - root -   Epoch: [36/300][160/283], lr: 0.00000016 	 loss = 0.1287(0.4198)
2023/12/02 12:40:23 - INFO - root -   Epoch: [36/300][180/283], lr: 0.00000016 	 loss = 0.4582(0.4092)
2023/12/02 12:40:34 - INFO - root -   Epoch: [36/300][200/283], lr: 0.00000016 	 loss = 0.1556(0.4108)
2023/12/02 12:40:44 - INFO - root -   Epoch: [36/300][220/283], lr: 0.00000016 	 loss = 0.5622(0.3994)
2023/12/02 12:41:08 - INFO - root -   Epoch: [36/300][240/283], lr: 0.00000016 	 loss = 0.1895(0.3991)
2023/12/02 12:41:17 - INFO - root -   Epoch: [36/300][260/283], lr: 0.00000016 	 loss = 0.3561(0.4048)
2023/12/02 12:41:25 - INFO - root -   Epoch: [36/300][280/283], lr: 0.00000016 	 loss = 0.6173(0.4036)
2023/12/02 12:41:26 - INFO - root -   Epoch: [36/300] 	 loss = 0.4018
2023/12/02 12:41:26 - INFO - root -   train_accuracy = 0.8428
2023/12/02 12:41:52 - INFO - root -   Epoch: [37/300][0/283], lr: 0.00000016 	 loss = 0.0638(0.0638)
2023/12/02 12:42:09 - INFO - root -   Epoch: [37/300][20/283], lr: 0.00000016 	 loss = 0.1487(0.4924)
2023/12/02 12:42:17 - INFO - root -   Epoch: [37/300][40/283], lr: 0.00000016 	 loss = 0.1782(0.4807)
2023/12/02 12:42:32 - INFO - root -   Epoch: [37/300][60/283], lr: 0.00000016 	 loss = 1.3648(0.5165)
2023/12/02 12:42:53 - INFO - root -   Epoch: [37/300][80/283], lr: 0.00000016 	 loss = 0.6325(0.5260)
2023/12/02 12:43:01 - INFO - root -   Epoch: [37/300][100/283], lr: 0.00000016 	 loss = 0.5105(0.5017)
2023/12/02 12:43:23 - INFO - root -   Epoch: [37/300][120/283], lr: 0.00000016 	 loss = 0.0464(0.5028)
2023/12/02 12:43:35 - INFO - root -   Epoch: [37/300][140/283], lr: 0.00000016 	 loss = 0.0173(0.4608)
2023/12/02 12:43:46 - INFO - root -   Epoch: [37/300][160/283], lr: 0.00000016 	 loss = 0.1029(0.4398)
2023/12/02 12:43:59 - INFO - root -   Epoch: [37/300][180/283], lr: 0.00000016 	 loss = 0.1656(0.4245)
2023/12/02 12:44:07 - INFO - root -   Epoch: [37/300][200/283], lr: 0.00000016 	 loss = 0.1351(0.4068)
2023/12/02 12:44:34 - INFO - root -   Epoch: [37/300][220/283], lr: 0.00000016 	 loss = 0.0779(0.3999)
2023/12/02 12:44:46 - INFO - root -   Epoch: [37/300][240/283], lr: 0.00000016 	 loss = 0.1293(0.3945)
2023/12/02 12:45:01 - INFO - root -   Epoch: [37/300][260/283], lr: 0.00000016 	 loss = 0.1739(0.3961)
2023/12/02 12:45:09 - INFO - root -   Epoch: [37/300][280/283], lr: 0.00000016 	 loss = 1.1667(0.3892)
2023/12/02 12:45:10 - INFO - root -   Epoch: [37/300] 	 loss = 0.3871
2023/12/02 12:45:10 - INFO - root -   train_accuracy = 0.8286
2023/12/02 12:45:28 - INFO - root -   Epoch: [38/300][0/283], lr: 0.00000017 	 loss = 0.2525(0.2525)
2023/12/02 12:45:39 - INFO - root -   Epoch: [38/300][20/283], lr: 0.00000017 	 loss = 1.4645(0.5074)
2023/12/02 12:45:53 - INFO - root -   Epoch: [38/300][40/283], lr: 0.00000017 	 loss = 0.0741(0.5173)
2023/12/02 12:46:09 - INFO - root -   Epoch: [38/300][60/283], lr: 0.00000017 	 loss = 1.0656(0.5468)
2023/12/02 12:46:31 - INFO - root -   Epoch: [38/300][80/283], lr: 0.00000017 	 loss = 0.3934(0.5566)
2023/12/02 12:46:45 - INFO - root -   Epoch: [38/300][100/283], lr: 0.00000017 	 loss = 0.0863(0.5258)
2023/12/02 12:46:59 - INFO - root -   Epoch: [38/300][120/283], lr: 0.00000017 	 loss = 0.6421(0.5127)
2023/12/02 12:47:18 - INFO - root -   Epoch: [38/300][140/283], lr: 0.00000017 	 loss = 0.1289(0.4816)
2023/12/02 12:47:28 - INFO - root -   Epoch: [38/300][160/283], lr: 0.00000017 	 loss = 1.4393(0.4685)
2023/12/02 12:47:43 - INFO - root -   Epoch: [38/300][180/283], lr: 0.00000017 	 loss = 0.5086(0.4420)
2023/12/02 12:47:56 - INFO - root -   Epoch: [38/300][200/283], lr: 0.00000017 	 loss = 0.0679(0.4363)
2023/12/02 12:48:17 - INFO - root -   Epoch: [38/300][220/283], lr: 0.00000017 	 loss = 0.0722(0.4292)
2023/12/02 12:48:28 - INFO - root -   Epoch: [38/300][240/283], lr: 0.00000017 	 loss = 0.1352(0.4442)
2023/12/02 12:48:39 - INFO - root -   Epoch: [38/300][260/283], lr: 0.00000017 	 loss = 0.0585(0.4405)
2023/12/02 12:48:50 - INFO - root -   Epoch: [38/300][280/283], lr: 0.00000017 	 loss = 0.3880(0.4258)
2023/12/02 12:48:51 - INFO - root -   Epoch: [38/300] 	 loss = 0.4239
2023/12/02 12:48:51 - INFO - root -   train_accuracy = 0.8074
2023/12/02 12:49:04 - INFO - root -   Epoch: [39/300][0/283], lr: 0.00000017 	 loss = 1.0632(1.0632)
2023/12/02 12:49:16 - INFO - root -   Epoch: [39/300][20/283], lr: 0.00000017 	 loss = 1.4398(0.5013)
2023/12/02 12:49:24 - INFO - root -   Epoch: [39/300][40/283], lr: 0.00000017 	 loss = 0.0598(0.4867)
2023/12/02 12:49:41 - INFO - root -   Epoch: [39/300][60/283], lr: 0.00000017 	 loss = 1.8172(0.5341)
2023/12/02 12:49:58 - INFO - root -   Epoch: [39/300][80/283], lr: 0.00000017 	 loss = 0.4419(0.5266)
2023/12/02 12:50:19 - INFO - root -   Epoch: [39/300][100/283], lr: 0.00000017 	 loss = 0.4829(0.4989)
2023/12/02 12:50:27 - INFO - root -   Epoch: [39/300][120/283], lr: 0.00000017 	 loss = 0.1444(0.5073)
2023/12/02 12:50:43 - INFO - root -   Epoch: [39/300][140/283], lr: 0.00000017 	 loss = 0.0189(0.4743)
2023/12/02 12:50:52 - INFO - root -   Epoch: [39/300][160/283], lr: 0.00000017 	 loss = 0.0335(0.4482)
2023/12/02 12:51:01 - INFO - root -   Epoch: [39/300][180/283], lr: 0.00000017 	 loss = 0.7366(0.4230)
2023/12/02 12:51:23 - INFO - root -   Epoch: [39/300][200/283], lr: 0.00000017 	 loss = 0.6854(0.4249)
2023/12/02 12:51:37 - INFO - root -   Epoch: [39/300][220/283], lr: 0.00000017 	 loss = 0.3066(0.4197)
2023/12/02 12:51:55 - INFO - root -   Epoch: [39/300][240/283], lr: 0.00000017 	 loss = 0.2430(0.4266)
2023/12/02 12:52:03 - INFO - root -   Epoch: [39/300][260/283], lr: 0.00000017 	 loss = 0.0306(0.4247)
2023/12/02 12:52:11 - INFO - root -   Epoch: [39/300][280/283], lr: 0.00000017 	 loss = 0.4617(0.4232)
2023/12/02 12:52:12 - INFO - root -   Epoch: [39/300] 	 loss = 0.4214
2023/12/02 12:52:21 - INFO - root -   precision = 0.8252
2023/12/02 12:52:21 - INFO - root -   eval_loss = 0.4210
2023/12/02 12:52:21 - INFO - root -   eval_acc = 0.8252
2023/12/02 12:52:22 - INFO - root -   train_accuracy = 0.8127
2023/12/02 12:52:23 - INFO - root -   Epoch: [40/300][0/283], lr: 0.00000017 	 loss = 0.4608(0.4608)
2023/12/02 12:52:52 - INFO - root -   Epoch: [40/300][20/283], lr: 0.00000017 	 loss = 1.2646(0.5951)
2023/12/02 12:53:00 - INFO - root -   Epoch: [40/300][40/283], lr: 0.00000017 	 loss = 0.3402(0.5498)
2023/12/02 12:53:18 - INFO - root -   Epoch: [40/300][60/283], lr: 0.00000017 	 loss = 1.7124(0.5552)
2023/12/02 12:53:39 - INFO - root -   Epoch: [40/300][80/283], lr: 0.00000017 	 loss = 0.2745(0.5130)
2023/12/02 12:53:56 - INFO - root -   Epoch: [40/300][100/283], lr: 0.00000017 	 loss = 0.0640(0.4849)
2023/12/02 12:54:13 - INFO - root -   Epoch: [40/300][120/283], lr: 0.00000017 	 loss = 0.2922(0.5122)
2023/12/02 12:54:25 - INFO - root -   Epoch: [40/300][140/283], lr: 0.00000017 	 loss = 0.0526(0.4739)
2023/12/02 12:54:34 - INFO - root -   Epoch: [40/300][160/283], lr: 0.00000017 	 loss = 0.1148(0.4518)
2023/12/02 12:54:57 - INFO - root -   Epoch: [40/300][180/283], lr: 0.00000017 	 loss = 0.3868(0.4310)
2023/12/02 12:55:05 - INFO - root -   Epoch: [40/300][200/283], lr: 0.00000017 	 loss = 0.2110(0.4227)
2023/12/02 12:55:30 - INFO - root -   Epoch: [40/300][220/283], lr: 0.00000017 	 loss = 0.0864(0.4126)
2023/12/02 12:55:40 - INFO - root -   Epoch: [40/300][240/283], lr: 0.00000017 	 loss = 0.3186(0.4265)
2023/12/02 12:55:59 - INFO - root -   Epoch: [40/300][260/283], lr: 0.00000017 	 loss = 0.1603(0.4200)
2023/12/02 12:56:07 - INFO - root -   Epoch: [40/300][280/283], lr: 0.00000017 	 loss = 1.1156(0.4169)
2023/12/02 12:56:08 - INFO - root -   Epoch: [40/300] 	 loss = 0.4142
2023/12/02 12:56:08 - INFO - root -   train_accuracy = 0.8163
2023/12/02 12:56:21 - INFO - root -   Epoch: [41/300][0/283], lr: 0.00000017 	 loss = 0.5525(0.5525)
2023/12/02 12:56:38 - INFO - root -   Epoch: [41/300][20/283], lr: 0.00000017 	 loss = 0.3846(0.4600)
2023/12/02 12:57:01 - INFO - root -   Epoch: [41/300][40/283], lr: 0.00000017 	 loss = 0.1949(0.5338)
2023/12/02 12:57:09 - INFO - root -   Epoch: [41/300][60/283], lr: 0.00000017 	 loss = 1.9471(0.5682)
2023/12/02 12:57:30 - INFO - root -   Epoch: [41/300][80/283], lr: 0.00000017 	 loss = 0.2549(0.5192)
2023/12/02 12:57:49 - INFO - root -   Epoch: [41/300][100/283], lr: 0.00000017 	 loss = 0.3635(0.4932)
2023/12/02 12:58:08 - INFO - root -   Epoch: [41/300][120/283], lr: 0.00000017 	 loss = 0.0602(0.4925)
2023/12/02 12:58:27 - INFO - root -   Epoch: [41/300][140/283], lr: 0.00000017 	 loss = 0.2317(0.4636)
2023/12/02 12:58:35 - INFO - root -   Epoch: [41/300][160/283], lr: 0.00000017 	 loss = 0.1947(0.4383)
2023/12/02 12:59:01 - INFO - root -   Epoch: [41/300][180/283], lr: 0.00000017 	 loss = 0.2756(0.4200)
2023/12/02 12:59:13 - INFO - root -   Epoch: [41/300][200/283], lr: 0.00000017 	 loss = 0.2478(0.4138)
2023/12/02 12:59:24 - INFO - root -   Epoch: [41/300][220/283], lr: 0.00000017 	 loss = 0.1239(0.3942)
2023/12/02 12:59:38 - INFO - root -   Epoch: [41/300][240/283], lr: 0.00000017 	 loss = 0.1692(0.4083)
2023/12/02 13:00:01 - INFO - root -   Epoch: [41/300][260/283], lr: 0.00000017 	 loss = 0.4255(0.4102)
2023/12/02 13:00:09 - INFO - root -   Epoch: [41/300][280/283], lr: 0.00000017 	 loss = 0.6663(0.3972)
2023/12/02 13:00:10 - INFO - root -   Epoch: [41/300] 	 loss = 0.3951
2023/12/02 13:00:10 - INFO - root -   train_accuracy = 0.8587
2023/12/02 13:00:22 - INFO - root -   Epoch: [42/300][0/283], lr: 0.00000017 	 loss = 1.7933(1.7933)
2023/12/02 13:00:35 - INFO - root -   Epoch: [42/300][20/283], lr: 0.00000017 	 loss = 1.4473(0.6037)
2023/12/02 13:00:49 - INFO - root -   Epoch: [42/300][40/283], lr: 0.00000017 	 loss = 0.1843(0.5616)
2023/12/02 13:01:08 - INFO - root -   Epoch: [42/300][60/283], lr: 0.00000017 	 loss = 2.3763(0.6164)
2023/12/02 13:01:34 - INFO - root -   Epoch: [42/300][80/283], lr: 0.00000017 	 loss = 0.2716(0.5620)
2023/12/02 13:01:42 - INFO - root -   Epoch: [42/300][100/283], lr: 0.00000017 	 loss = 0.3045(0.5284)
2023/12/02 13:01:50 - INFO - root -   Epoch: [42/300][120/283], lr: 0.00000017 	 loss = 0.3353(0.5060)
2023/12/02 13:02:15 - INFO - root -   Epoch: [42/300][140/283], lr: 0.00000017 	 loss = 0.1783(0.4784)
2023/12/02 13:02:36 - INFO - root -   Epoch: [42/300][160/283], lr: 0.00000017 	 loss = 0.4313(0.4674)
2023/12/02 13:02:58 - INFO - root -   Epoch: [42/300][180/283], lr: 0.00000017 	 loss = 0.0701(0.4393)
2023/12/02 13:03:07 - INFO - root -   Epoch: [42/300][200/283], lr: 0.00000017 	 loss = 0.5284(0.4501)
2023/12/02 13:03:32 - INFO - root -   Epoch: [42/300][220/283], lr: 0.00000017 	 loss = 0.1585(0.4371)
2023/12/02 13:03:40 - INFO - root -   Epoch: [42/300][240/283], lr: 0.00000017 	 loss = 0.1442(0.4443)
2023/12/02 13:03:58 - INFO - root -   Epoch: [42/300][260/283], lr: 0.00000017 	 loss = 0.0634(0.4362)
2023/12/02 13:04:06 - INFO - root -   Epoch: [42/300][280/283], lr: 0.00000017 	 loss = 0.5703(0.4242)
2023/12/02 13:04:06 - INFO - root -   Epoch: [42/300] 	 loss = 0.4224
2023/12/02 13:04:06 - INFO - root -   train_accuracy = 0.8251
2023/12/02 13:04:08 - INFO - root -   Epoch: [43/300][0/283], lr: 0.00000018 	 loss = 0.2703(0.2703)
2023/12/02 13:04:46 - INFO - root -   Epoch: [43/300][20/283], lr: 0.00000018 	 loss = 0.3702(0.4204)
2023/12/02 13:04:54 - INFO - root -   Epoch: [43/300][40/283], lr: 0.00000018 	 loss = 0.1301(0.4369)
2023/12/02 13:05:12 - INFO - root -   Epoch: [43/300][60/283], lr: 0.00000018 	 loss = 1.1956(0.4808)
2023/12/02 13:05:34 - INFO - root -   Epoch: [43/300][80/283], lr: 0.00000018 	 loss = 0.4747(0.5118)
2023/12/02 13:05:45 - INFO - root -   Epoch: [43/300][100/283], lr: 0.00000018 	 loss = 0.2146(0.5112)
2023/12/02 13:05:53 - INFO - root -   Epoch: [43/300][120/283], lr: 0.00000018 	 loss = 0.1941(0.4977)
2023/12/02 13:06:12 - INFO - root -   Epoch: [43/300][140/283], lr: 0.00000018 	 loss = 0.2546(0.4791)
2023/12/02 13:06:30 - INFO - root -   Epoch: [43/300][160/283], lr: 0.00000018 	 loss = 0.1469(0.4548)
2023/12/02 13:06:47 - INFO - root -   Epoch: [43/300][180/283], lr: 0.00000018 	 loss = 0.3387(0.4306)
2023/12/02 13:07:03 - INFO - root -   Epoch: [43/300][200/283], lr: 0.00000018 	 loss = 0.9448(0.4315)
2023/12/02 13:07:20 - INFO - root -   Epoch: [43/300][220/283], lr: 0.00000018 	 loss = 0.0520(0.4176)
2023/12/02 13:07:35 - INFO - root -   Epoch: [43/300][240/283], lr: 0.00000018 	 loss = 0.1552(0.4176)
2023/12/02 13:07:43 - INFO - root -   Epoch: [43/300][260/283], lr: 0.00000018 	 loss = 0.0334(0.4195)
2023/12/02 13:07:57 - INFO - root -   Epoch: [43/300][280/283], lr: 0.00000018 	 loss = 1.0939(0.4196)
2023/12/02 13:07:58 - INFO - root -   Epoch: [43/300] 	 loss = 0.4174
2023/12/02 13:07:58 - INFO - root -   train_accuracy = 0.8233
2023/12/02 13:08:14 - INFO - root -   Epoch: [44/300][0/283], lr: 0.00000018 	 loss = 0.1455(0.1455)
2023/12/02 13:08:22 - INFO - root -   Epoch: [44/300][20/283], lr: 0.00000018 	 loss = 0.3639(0.4173)
2023/12/02 13:08:42 - INFO - root -   Epoch: [44/300][40/283], lr: 0.00000018 	 loss = 0.3247(0.4944)
2023/12/02 13:08:53 - INFO - root -   Epoch: [44/300][60/283], lr: 0.00000018 	 loss = 1.5285(0.5280)
2023/12/02 13:09:01 - INFO - root -   Epoch: [44/300][80/283], lr: 0.00000018 	 loss = 0.1438(0.5120)
2023/12/02 13:09:17 - INFO - root -   Epoch: [44/300][100/283], lr: 0.00000018 	 loss = 0.0726(0.4903)
2023/12/02 13:09:25 - INFO - root -   Epoch: [44/300][120/283], lr: 0.00000018 	 loss = 0.1172(0.4871)
2023/12/02 13:09:43 - INFO - root -   Epoch: [44/300][140/283], lr: 0.00000018 	 loss = 0.1180(0.4499)
2023/12/02 13:10:09 - INFO - root -   Epoch: [44/300][160/283], lr: 0.00000018 	 loss = 0.2155(0.4363)
2023/12/02 13:10:17 - INFO - root -   Epoch: [44/300][180/283], lr: 0.00000018 	 loss = 0.4896(0.4148)
2023/12/02 13:10:34 - INFO - root -   Epoch: [44/300][200/283], lr: 0.00000018 	 loss = 0.3155(0.4150)
2023/12/02 13:10:42 - INFO - root -   Epoch: [44/300][220/283], lr: 0.00000018 	 loss = 0.0821(0.3993)
2023/12/02 13:11:09 - INFO - root -   Epoch: [44/300][240/283], lr: 0.00000018 	 loss = 0.0874(0.4041)
2023/12/02 13:11:17 - INFO - root -   Epoch: [44/300][260/283], lr: 0.00000018 	 loss = 0.1706(0.4084)
2023/12/02 13:11:24 - INFO - root -   Epoch: [44/300][280/283], lr: 0.00000018 	 loss = 0.8863(0.4081)
2023/12/02 13:11:25 - INFO - root -   Epoch: [44/300] 	 loss = 0.4060
2023/12/02 13:11:35 - INFO - root -   precision = 0.8322
2023/12/02 13:11:35 - INFO - root -   eval_loss = 0.3940
2023/12/02 13:11:35 - INFO - root -   eval_acc = 0.8322
2023/12/02 13:11:35 - INFO - root -   train_accuracy = 0.8357
2023/12/02 13:11:49 - INFO - root -   Epoch: [45/300][0/283], lr: 0.00000018 	 loss = 0.7812(0.7812)
2023/12/02 13:12:00 - INFO - root -   Epoch: [45/300][20/283], lr: 0.00000018 	 loss = 0.0736(0.4890)
2023/12/02 13:12:19 - INFO - root -   Epoch: [45/300][40/283], lr: 0.00000018 	 loss = 0.1608(0.4568)
2023/12/02 13:12:36 - INFO - root -   Epoch: [45/300][60/283], lr: 0.00000018 	 loss = 1.0354(0.4990)
2023/12/02 13:12:44 - INFO - root -   Epoch: [45/300][80/283], lr: 0.00000018 	 loss = 0.1485(0.4955)
2023/12/02 13:12:52 - INFO - root -   Epoch: [45/300][100/283], lr: 0.00000018 	 loss = 0.2442(0.4797)
2023/12/02 13:13:14 - INFO - root -   Epoch: [45/300][120/283], lr: 0.00000018 	 loss = 0.9823(0.5032)
2023/12/02 13:13:28 - INFO - root -   Epoch: [45/300][140/283], lr: 0.00000018 	 loss = 0.1721(0.4730)
2023/12/02 13:13:51 - INFO - root -   Epoch: [45/300][160/283], lr: 0.00000018 	 loss = 0.0715(0.4428)
2023/12/02 13:14:04 - INFO - root -   Epoch: [45/300][180/283], lr: 0.00000018 	 loss = 0.1522(0.4267)
2023/12/02 13:14:26 - INFO - root -   Epoch: [45/300][200/283], lr: 0.00000018 	 loss = 1.2828(0.4148)
2023/12/02 13:14:39 - INFO - root -   Epoch: [45/300][220/283], lr: 0.00000018 	 loss = 0.2088(0.4015)
2023/12/02 13:14:57 - INFO - root -   Epoch: [45/300][240/283], lr: 0.00000018 	 loss = 0.4707(0.4075)
2023/12/02 13:15:05 - INFO - root -   Epoch: [45/300][260/283], lr: 0.00000018 	 loss = 0.0421(0.4093)
2023/12/02 13:15:23 - INFO - root -   Epoch: [45/300][280/283], lr: 0.00000018 	 loss = 0.2588(0.3995)
2023/12/02 13:15:24 - INFO - root -   Epoch: [45/300] 	 loss = 0.3971
2023/12/02 13:15:24 - INFO - root -   train_accuracy = 0.8428
2023/12/02 13:15:25 - INFO - root -   Epoch: [46/300][0/283], lr: 0.00000018 	 loss = 0.1899(0.1899)
2023/12/02 13:15:56 - INFO - root -   Epoch: [46/300][20/283], lr: 0.00000018 	 loss = 1.2079(0.5079)
2023/12/02 13:16:10 - INFO - root -   Epoch: [46/300][40/283], lr: 0.00000018 	 loss = 0.2804(0.5269)
2023/12/02 13:16:30 - INFO - root -   Epoch: [46/300][60/283], lr: 0.00000018 	 loss = 2.9469(0.5888)
2023/12/02 13:16:38 - INFO - root -   Epoch: [46/300][80/283], lr: 0.00000018 	 loss = 0.1803(0.5409)
2023/12/02 13:16:57 - INFO - root -   Epoch: [46/300][100/283], lr: 0.00000018 	 loss = 0.2430(0.5062)
2023/12/02 13:17:18 - INFO - root -   Epoch: [46/300][120/283], lr: 0.00000018 	 loss = 0.4579(0.5263)
2023/12/02 13:17:26 - INFO - root -   Epoch: [46/300][140/283], lr: 0.00000018 	 loss = 0.0624(0.4855)
2023/12/02 13:17:44 - INFO - root -   Epoch: [46/300][160/283], lr: 0.00000018 	 loss = 0.7276(0.4641)
2023/12/02 13:17:59 - INFO - root -   Epoch: [46/300][180/283], lr: 0.00000018 	 loss = 0.0855(0.4455)
2023/12/02 13:18:22 - INFO - root -   Epoch: [46/300][200/283], lr: 0.00000018 	 loss = 0.6073(0.4448)
2023/12/02 13:18:30 - INFO - root -   Epoch: [46/300][220/283], lr: 0.00000018 	 loss = 0.1747(0.4348)
2023/12/02 13:18:45 - INFO - root -   Epoch: [46/300][240/283], lr: 0.00000018 	 loss = 0.1285(0.4345)
2023/12/02 13:18:58 - INFO - root -   Epoch: [46/300][260/283], lr: 0.00000018 	 loss = 0.3541(0.4402)
2023/12/02 13:19:07 - INFO - root -   Epoch: [46/300][280/283], lr: 0.00000018 	 loss = 0.7819(0.4277)
2023/12/02 13:19:08 - INFO - root -   Epoch: [46/300] 	 loss = 0.4264
2023/12/02 13:19:08 - INFO - root -   train_accuracy = 0.8304
2023/12/02 13:19:10 - INFO - root -   Epoch: [47/300][0/283], lr: 0.00000018 	 loss = 0.1201(0.1201)
2023/12/02 13:19:40 - INFO - root -   Epoch: [47/300][20/283], lr: 0.00000018 	 loss = 0.8510(0.3935)
2023/12/02 13:19:51 - INFO - root -   Epoch: [47/300][40/283], lr: 0.00000018 	 loss = 0.2560(0.4506)
2023/12/02 13:20:02 - INFO - root -   Epoch: [47/300][60/283], lr: 0.00000018 	 loss = 1.8130(0.4936)
2023/12/02 13:20:13 - INFO - root -   Epoch: [47/300][80/283], lr: 0.00000018 	 loss = 0.5766(0.4780)
2023/12/02 13:20:40 - INFO - root -   Epoch: [47/300][100/283], lr: 0.00000018 	 loss = 0.4524(0.4873)
2023/12/02 13:20:47 - INFO - root -   Epoch: [47/300][120/283], lr: 0.00000018 	 loss = 0.2018(0.4985)
2023/12/02 13:20:55 - INFO - root -   Epoch: [47/300][140/283], lr: 0.00000018 	 loss = 0.0329(0.4592)
2023/12/02 13:21:25 - INFO - root -   Epoch: [47/300][160/283], lr: 0.00000018 	 loss = 0.8284(0.4486)
2023/12/02 13:21:38 - INFO - root -   Epoch: [47/300][180/283], lr: 0.00000018 	 loss = 0.3716(0.4324)
2023/12/02 13:21:55 - INFO - root -   Epoch: [47/300][200/283], lr: 0.00000018 	 loss = 0.4024(0.4321)
2023/12/02 13:22:03 - INFO - root -   Epoch: [47/300][220/283], lr: 0.00000018 	 loss = 0.0230(0.4195)
2023/12/02 13:22:26 - INFO - root -   Epoch: [47/300][240/283], lr: 0.00000018 	 loss = 0.5037(0.4237)
2023/12/02 13:22:34 - INFO - root -   Epoch: [47/300][260/283], lr: 0.00000018 	 loss = 0.0383(0.4274)
2023/12/02 13:22:43 - INFO - root -   Epoch: [47/300][280/283], lr: 0.00000018 	 loss = 1.1658(0.4198)
2023/12/02 13:22:44 - INFO - root -   Epoch: [47/300] 	 loss = 0.4173
2023/12/02 13:22:44 - INFO - root -   train_accuracy = 0.7968
2023/12/02 13:22:47 - INFO - root -   Epoch: [48/300][0/283], lr: 0.00000018 	 loss = 0.3388(0.3388)
2023/12/02 13:23:17 - INFO - root -   Epoch: [48/300][20/283], lr: 0.00000018 	 loss = 0.4078(0.5671)
2023/12/02 13:23:40 - INFO - root -   Epoch: [48/300][40/283], lr: 0.00000018 	 loss = 0.1727(0.5608)
2023/12/02 13:23:49 - INFO - root -   Epoch: [48/300][60/283], lr: 0.00000018 	 loss = 1.6310(0.5643)
2023/12/02 13:24:09 - INFO - root -   Epoch: [48/300][80/283], lr: 0.00000018 	 loss = 0.4191(0.5661)
2023/12/02 13:24:20 - INFO - root -   Epoch: [48/300][100/283], lr: 0.00000018 	 loss = 0.3913(0.5457)
2023/12/02 13:24:36 - INFO - root -   Epoch: [48/300][120/283], lr: 0.00000018 	 loss = 0.1567(0.5555)
2023/12/02 13:24:55 - INFO - root -   Epoch: [48/300][140/283], lr: 0.00000018 	 loss = 0.2237(0.5297)
2023/12/02 13:25:08 - INFO - root -   Epoch: [48/300][160/283], lr: 0.00000018 	 loss = 0.1046(0.4988)
2023/12/02 13:25:22 - INFO - root -   Epoch: [48/300][180/283], lr: 0.00000018 	 loss = 0.1646(0.4730)
2023/12/02 13:25:36 - INFO - root -   Epoch: [48/300][200/283], lr: 0.00000018 	 loss = 0.3723(0.4596)
2023/12/02 13:25:44 - INFO - root -   Epoch: [48/300][220/283], lr: 0.00000018 	 loss = 0.0272(0.4417)
2023/12/02 13:25:57 - INFO - root -   Epoch: [48/300][240/283], lr: 0.00000018 	 loss = 0.1057(0.4399)
2023/12/02 13:26:13 - INFO - root -   Epoch: [48/300][260/283], lr: 0.00000018 	 loss = 0.0502(0.4408)
2023/12/02 13:26:21 - INFO - root -   Epoch: [48/300][280/283], lr: 0.00000018 	 loss = 1.5154(0.4380)
2023/12/02 13:26:22 - INFO - root -   Epoch: [48/300] 	 loss = 0.4359
2023/12/02 13:26:22 - INFO - root -   train_accuracy = 0.8004
2023/12/02 13:26:35 - INFO - root -   Epoch: [49/300][0/283], lr: 0.00000019 	 loss = 0.2764(0.2764)
2023/12/02 13:26:54 - INFO - root -   Epoch: [49/300][20/283], lr: 0.00000019 	 loss = 1.2735(0.5714)
2023/12/02 13:27:02 - INFO - root -   Epoch: [49/300][40/283], lr: 0.00000019 	 loss = 0.4919(0.5374)
2023/12/02 13:27:29 - INFO - root -   Epoch: [49/300][60/283], lr: 0.00000019 	 loss = 1.6769(0.5379)
2023/12/02 13:27:39 - INFO - root -   Epoch: [49/300][80/283], lr: 0.00000019 	 loss = 1.0721(0.5352)
2023/12/02 13:27:53 - INFO - root -   Epoch: [49/300][100/283], lr: 0.00000019 	 loss = 0.6173(0.5131)
2023/12/02 13:28:12 - INFO - root -   Epoch: [49/300][120/283], lr: 0.00000019 	 loss = 0.1684(0.5056)
2023/12/02 13:28:19 - INFO - root -   Epoch: [49/300][140/283], lr: 0.00000019 	 loss = 0.0298(0.4766)
2023/12/02 13:28:32 - INFO - root -   Epoch: [49/300][160/283], lr: 0.00000019 	 loss = 1.1358(0.4624)
2023/12/02 13:28:47 - INFO - root -   Epoch: [49/300][180/283], lr: 0.00000019 	 loss = 0.3346(0.4407)
2023/12/02 13:28:59 - INFO - root -   Epoch: [49/300][200/283], lr: 0.00000019 	 loss = 1.1353(0.4364)
2023/12/02 13:29:22 - INFO - root -   Epoch: [49/300][220/283], lr: 0.00000019 	 loss = 0.1638(0.4266)
2023/12/02 13:29:37 - INFO - root -   Epoch: [49/300][240/283], lr: 0.00000019 	 loss = 0.1389(0.4205)
2023/12/02 13:29:58 - INFO - root -   Epoch: [49/300][260/283], lr: 0.00000019 	 loss = 0.0498(0.4223)
2023/12/02 13:30:06 - INFO - root -   Epoch: [49/300][280/283], lr: 0.00000019 	 loss = 0.6422(0.4175)
2023/12/02 13:30:07 - INFO - root -   Epoch: [49/300] 	 loss = 0.4159
2023/12/02 13:30:16 - INFO - root -   precision = 0.8392
2023/12/02 13:30:16 - INFO - root -   eval_loss = 0.4065
2023/12/02 13:30:16 - INFO - root -   eval_acc = 0.8392
2023/12/02 13:30:16 - INFO - root -   train_accuracy = 0.8216
2023/12/02 13:30:36 - INFO - root -   Epoch: [50/300][0/283], lr: 0.00000019 	 loss = 1.1418(1.1418)
2023/12/02 13:30:56 - INFO - root -   Epoch: [50/300][20/283], lr: 0.00000019 	 loss = 0.2173(0.5663)
2023/12/02 13:31:04 - INFO - root -   Epoch: [50/300][40/283], lr: 0.00000019 	 loss = 0.2471(0.5369)
2023/12/02 13:31:22 - INFO - root -   Epoch: [50/300][60/283], lr: 0.00000019 	 loss = 2.2681(0.5818)
2023/12/02 13:31:30 - INFO - root -   Epoch: [50/300][80/283], lr: 0.00000019 	 loss = 0.2512(0.5331)
2023/12/02 13:31:50 - INFO - root -   Epoch: [50/300][100/283], lr: 0.00000019 	 loss = 0.2403(0.4997)
2023/12/02 13:31:58 - INFO - root -   Epoch: [50/300][120/283], lr: 0.00000019 	 loss = 0.6726(0.5234)
2023/12/02 13:32:16 - INFO - root -   Epoch: [50/300][140/283], lr: 0.00000019 	 loss = 0.1540(0.4916)
2023/12/02 13:32:28 - INFO - root -   Epoch: [50/300][160/283], lr: 0.00000019 	 loss = 0.1373(0.4638)
2023/12/02 13:32:45 - INFO - root -   Epoch: [50/300][180/283], lr: 0.00000019 	 loss = 0.2910(0.4374)
2023/12/02 13:33:02 - INFO - root -   Epoch: [50/300][200/283], lr: 0.00000019 	 loss = 0.3744(0.4335)
2023/12/02 13:33:24 - INFO - root -   Epoch: [50/300][220/283], lr: 0.00000019 	 loss = 0.2747(0.4291)
2023/12/02 13:33:33 - INFO - root -   Epoch: [50/300][240/283], lr: 0.00000019 	 loss = 0.0809(0.4257)
2023/12/02 13:33:48 - INFO - root -   Epoch: [50/300][260/283], lr: 0.00000019 	 loss = 0.1075(0.4119)
2023/12/02 13:33:56 - INFO - root -   Epoch: [50/300][280/283], lr: 0.00000019 	 loss = 0.6565(0.4019)
2023/12/02 13:33:56 - INFO - root -   Epoch: [50/300] 	 loss = 0.4000
2023/12/02 13:33:56 - INFO - root -   train_accuracy = 0.8180
2023/12/02 13:34:09 - INFO - root -   Epoch: [51/300][0/283], lr: 0.00000019 	 loss = 0.1815(0.1815)
2023/12/02 13:34:22 - INFO - root -   Epoch: [51/300][20/283], lr: 0.00000019 	 loss = 1.0293(0.3796)
2023/12/02 13:34:38 - INFO - root -   Epoch: [51/300][40/283], lr: 0.00000019 	 loss = 0.1619(0.4202)
2023/12/02 13:35:01 - INFO - root -   Epoch: [51/300][60/283], lr: 0.00000019 	 loss = 1.7971(0.4599)
2023/12/02 13:35:09 - INFO - root -   Epoch: [51/300][80/283], lr: 0.00000019 	 loss = 0.4044(0.4527)
2023/12/02 13:35:25 - INFO - root -   Epoch: [51/300][100/283], lr: 0.00000019 	 loss = 0.2648(0.4171)
2023/12/02 13:35:33 - INFO - root -   Epoch: [51/300][120/283], lr: 0.00000019 	 loss = 0.9046(0.4443)
2023/12/02 13:35:51 - INFO - root -   Epoch: [51/300][140/283], lr: 0.00000019 	 loss = 0.0500(0.4140)
2023/12/02 13:35:59 - INFO - root -   Epoch: [51/300][160/283], lr: 0.00000019 	 loss = 0.0332(0.3865)
2023/12/02 13:36:18 - INFO - root -   Epoch: [51/300][180/283], lr: 0.00000019 	 loss = 0.6158(0.3742)
2023/12/02 13:36:39 - INFO - root -   Epoch: [51/300][200/283], lr: 0.00000019 	 loss = 0.3070(0.3800)
2023/12/02 13:36:51 - INFO - root -   Epoch: [51/300][220/283], lr: 0.00000019 	 loss = 0.2433(0.3605)
2023/12/02 13:36:59 - INFO - root -   Epoch: [51/300][240/283], lr: 0.00000019 	 loss = 0.1383(0.3589)
2023/12/02 13:37:11 - INFO - root -   Epoch: [51/300][260/283], lr: 0.00000019 	 loss = 0.0078(0.3602)
2023/12/02 13:37:19 - INFO - root -   Epoch: [51/300][280/283], lr: 0.00000019 	 loss = 1.4118(0.3605)
2023/12/02 13:37:20 - INFO - root -   Epoch: [51/300] 	 loss = 0.3581
2023/12/02 13:37:20 - INFO - root -   train_accuracy = 0.8569
2023/12/02 13:37:37 - INFO - root -   Epoch: [52/300][0/283], lr: 0.00000019 	 loss = 0.5019(0.5019)
2023/12/02 13:37:55 - INFO - root -   Epoch: [52/300][20/283], lr: 0.00000019 	 loss = 0.3489(0.5648)
2023/12/02 13:38:04 - INFO - root -   Epoch: [52/300][40/283], lr: 0.00000019 	 loss = 0.0964(0.4870)
2023/12/02 13:38:22 - INFO - root -   Epoch: [52/300][60/283], lr: 0.00000019 	 loss = 2.3982(0.5116)
2023/12/02 13:38:32 - INFO - root -   Epoch: [52/300][80/283], lr: 0.00000019 	 loss = 1.5920(0.5163)
2023/12/02 13:38:49 - INFO - root -   Epoch: [52/300][100/283], lr: 0.00000019 	 loss = 0.0630(0.4818)
2023/12/02 13:38:57 - INFO - root -   Epoch: [52/300][120/283], lr: 0.00000019 	 loss = 0.6981(0.4792)
2023/12/02 13:39:22 - INFO - root -   Epoch: [52/300][140/283], lr: 0.00000019 	 loss = 0.1101(0.4445)
2023/12/02 13:39:44 - INFO - root -   Epoch: [52/300][160/283], lr: 0.00000019 	 loss = 0.6933(0.4311)
2023/12/02 13:40:08 - INFO - root -   Epoch: [52/300][180/283], lr: 0.00000019 	 loss = 0.1559(0.4141)
2023/12/02 13:40:21 - INFO - root -   Epoch: [52/300][200/283], lr: 0.00000019 	 loss = 0.0789(0.4217)
2023/12/02 13:40:37 - INFO - root -   Epoch: [52/300][220/283], lr: 0.00000019 	 loss = 0.1648(0.4071)
2023/12/02 13:40:45 - INFO - root -   Epoch: [52/300][240/283], lr: 0.00000019 	 loss = 0.3487(0.4043)
2023/12/02 13:41:01 - INFO - root -   Epoch: [52/300][260/283], lr: 0.00000019 	 loss = 0.0492(0.3967)
2023/12/02 13:41:11 - INFO - root -   Epoch: [52/300][280/283], lr: 0.00000019 	 loss = 1.6998(0.3996)
2023/12/02 13:41:12 - INFO - root -   Epoch: [52/300] 	 loss = 0.3971
2023/12/02 13:41:12 - INFO - root -   train_accuracy = 0.8304
2023/12/02 13:41:13 - INFO - root -   Epoch: [53/300][0/283], lr: 0.00000019 	 loss = 0.4181(0.4181)
2023/12/02 13:41:44 - INFO - root -   Epoch: [53/300][20/283], lr: 0.00000019 	 loss = 1.7075(0.5535)
2023/12/02 13:42:06 - INFO - root -   Epoch: [53/300][40/283], lr: 0.00000019 	 loss = 0.6236(0.5056)
2023/12/02 13:42:24 - INFO - root -   Epoch: [53/300][60/283], lr: 0.00000019 	 loss = 0.6482(0.4827)
2023/12/02 13:42:35 - INFO - root -   Epoch: [53/300][80/283], lr: 0.00000019 	 loss = 0.4869(0.4708)
2023/12/02 13:42:57 - INFO - root -   Epoch: [53/300][100/283], lr: 0.00000019 	 loss = 0.1494(0.4537)
2023/12/02 13:43:17 - INFO - root -   Epoch: [53/300][120/283], lr: 0.00000019 	 loss = 0.4504(0.4764)
2023/12/02 13:43:36 - INFO - root -   Epoch: [53/300][140/283], lr: 0.00000019 	 loss = 0.0504(0.4380)
2023/12/02 13:43:46 - INFO - root -   Epoch: [53/300][160/283], lr: 0.00000019 	 loss = 0.0723(0.4184)
2023/12/02 13:44:01 - INFO - root -   Epoch: [53/300][180/283], lr: 0.00000019 	 loss = 0.1535(0.4082)
2023/12/02 13:44:15 - INFO - root -   Epoch: [53/300][200/283], lr: 0.00000019 	 loss = 0.0589(0.3938)
2023/12/02 13:44:34 - INFO - root -   Epoch: [53/300][220/283], lr: 0.00000019 	 loss = 0.0316(0.3844)
2023/12/02 13:44:45 - INFO - root -   Epoch: [53/300][240/283], lr: 0.00000019 	 loss = 0.1162(0.3875)
2023/12/02 13:44:56 - INFO - root -   Epoch: [53/300][260/283], lr: 0.00000019 	 loss = 0.0935(0.3817)
2023/12/02 13:45:04 - INFO - root -   Epoch: [53/300][280/283], lr: 0.00000019 	 loss = 1.2359(0.3839)
2023/12/02 13:45:05 - INFO - root -   Epoch: [53/300] 	 loss = 0.3838
2023/12/02 13:45:05 - INFO - root -   train_accuracy = 0.8322
2023/12/02 13:45:23 - INFO - root -   Epoch: [54/300][0/283], lr: 0.00000019 	 loss = 0.1290(0.1290)
2023/12/02 13:45:42 - INFO - root -   Epoch: [54/300][20/283], lr: 0.00000019 	 loss = 0.8029(0.5324)
2023/12/02 13:45:56 - INFO - root -   Epoch: [54/300][40/283], lr: 0.00000019 	 loss = 0.0555(0.5216)
2023/12/02 13:46:20 - INFO - root -   Epoch: [54/300][60/283], lr: 0.00000019 	 loss = 0.9069(0.5574)
2023/12/02 13:46:37 - INFO - root -   Epoch: [54/300][80/283], lr: 0.00000019 	 loss = 1.2087(0.5254)
2023/12/02 13:46:46 - INFO - root -   Epoch: [54/300][100/283], lr: 0.00000019 	 loss = 0.0662(0.4764)
2023/12/02 13:47:12 - INFO - root -   Epoch: [54/300][120/283], lr: 0.00000019 	 loss = 0.9947(0.4834)
2023/12/02 13:47:28 - INFO - root -   Epoch: [54/300][140/283], lr: 0.00000019 	 loss = 0.2202(0.4494)
2023/12/02 13:47:40 - INFO - root -   Epoch: [54/300][160/283], lr: 0.00000019 	 loss = 0.0863(0.4379)
2023/12/02 13:47:59 - INFO - root -   Epoch: [54/300][180/283], lr: 0.00000019 	 loss = 0.2628(0.4124)
2023/12/02 13:48:22 - INFO - root -   Epoch: [54/300][200/283], lr: 0.00000019 	 loss = 1.6951(0.4256)
2023/12/02 13:48:37 - INFO - root -   Epoch: [54/300][220/283], lr: 0.00000019 	 loss = 0.1524(0.4146)
2023/12/02 13:48:57 - INFO - root -   Epoch: [54/300][240/283], lr: 0.00000019 	 loss = 0.0978(0.4080)
2023/12/02 13:49:08 - INFO - root -   Epoch: [54/300][260/283], lr: 0.00000019 	 loss = 0.0621(0.4048)
2023/12/02 13:49:16 - INFO - root -   Epoch: [54/300][280/283], lr: 0.00000019 	 loss = 1.2041(0.4021)
2023/12/02 13:49:17 - INFO - root -   Epoch: [54/300] 	 loss = 0.3999
2023/12/02 13:49:27 - INFO - root -   precision = 0.8252
2023/12/02 13:49:27 - INFO - root -   eval_loss = 0.4197
2023/12/02 13:49:27 - INFO - root -   eval_acc = 0.8252
2023/12/02 13:49:27 - INFO - root -   train_accuracy = 0.8339
2023/12/02 13:49:42 - INFO - root -   Epoch: [55/300][0/283], lr: 0.00000020 	 loss = 0.1027(0.1027)
2023/12/02 13:49:57 - INFO - root -   Epoch: [55/300][20/283], lr: 0.00000020 	 loss = 0.6374(0.5869)
2023/12/02 13:50:05 - INFO - root -   Epoch: [55/300][40/283], lr: 0.00000020 	 loss = 0.2446(0.5822)
2023/12/02 13:50:23 - INFO - root -   Epoch: [55/300][60/283], lr: 0.00000020 	 loss = 1.4637(0.5814)
2023/12/02 13:50:41 - INFO - root -   Epoch: [55/300][80/283], lr: 0.00000020 	 loss = 0.5008(0.5747)
2023/12/02 13:50:59 - INFO - root -   Epoch: [55/300][100/283], lr: 0.00000020 	 loss = 0.3774(0.5574)
2023/12/02 13:51:07 - INFO - root -   Epoch: [55/300][120/283], lr: 0.00000020 	 loss = 0.0814(0.5399)
2023/12/02 13:51:31 - INFO - root -   Epoch: [55/300][140/283], lr: 0.00000020 	 loss = 0.0636(0.5045)
2023/12/02 13:51:41 - INFO - root -   Epoch: [55/300][160/283], lr: 0.00000020 	 loss = 0.0528(0.4743)
2023/12/02 13:52:04 - INFO - root -   Epoch: [55/300][180/283], lr: 0.00000020 	 loss = 0.1401(0.4556)
2023/12/02 13:52:21 - INFO - root -   Epoch: [55/300][200/283], lr: 0.00000020 	 loss = 0.2225(0.4446)
2023/12/02 13:52:35 - INFO - root -   Epoch: [55/300][220/283], lr: 0.00000020 	 loss = 0.3701(0.4377)
2023/12/02 13:52:46 - INFO - root -   Epoch: [55/300][240/283], lr: 0.00000020 	 loss = 0.4187(0.4286)
2023/12/02 13:53:00 - INFO - root -   Epoch: [55/300][260/283], lr: 0.00000020 	 loss = 0.0579(0.4259)
2023/12/02 13:53:08 - INFO - root -   Epoch: [55/300][280/283], lr: 0.00000020 	 loss = 1.2713(0.4196)
2023/12/02 13:53:09 - INFO - root -   Epoch: [55/300] 	 loss = 0.4180
2023/12/02 13:53:09 - INFO - root -   train_accuracy = 0.8110
2023/12/02 13:53:23 - INFO - root -   Epoch: [56/300][0/283], lr: 0.00000020 	 loss = 0.4663(0.4663)
2023/12/02 13:53:46 - INFO - root -   Epoch: [56/300][20/283], lr: 0.00000020 	 loss = 0.5784(0.5368)
2023/12/02 13:54:05 - INFO - root -   Epoch: [56/300][40/283], lr: 0.00000020 	 loss = 0.2473(0.5167)
2023/12/02 13:54:13 - INFO - root -   Epoch: [56/300][60/283], lr: 0.00000020 	 loss = 0.8800(0.5427)
2023/12/02 13:54:37 - INFO - root -   Epoch: [56/300][80/283], lr: 0.00000020 	 loss = 0.2941(0.5450)
2023/12/02 13:54:45 - INFO - root -   Epoch: [56/300][100/283], lr: 0.00000020 	 loss = 0.4954(0.5370)
2023/12/02 13:55:02 - INFO - root -   Epoch: [56/300][120/283], lr: 0.00000020 	 loss = 0.9916(0.5313)
2023/12/02 13:55:19 - INFO - root -   Epoch: [56/300][140/283], lr: 0.00000020 	 loss = 0.2826(0.4973)
2023/12/02 13:55:37 - INFO - root -   Epoch: [56/300][160/283], lr: 0.00000020 	 loss = 0.0590(0.4763)
2023/12/02 13:55:45 - INFO - root -   Epoch: [56/300][180/283], lr: 0.00000020 	 loss = 0.2779(0.4528)
2023/12/02 13:56:01 - INFO - root -   Epoch: [56/300][200/283], lr: 0.00000020 	 loss = 0.6410(0.4599)
2023/12/02 13:56:29 - INFO - root -   Epoch: [56/300][220/283], lr: 0.00000020 	 loss = 0.1802(0.4489)
2023/12/02 13:56:37 - INFO - root -   Epoch: [56/300][240/283], lr: 0.00000020 	 loss = 0.1189(0.4425)
2023/12/02 13:56:48 - INFO - root -   Epoch: [56/300][260/283], lr: 0.00000020 	 loss = 0.1264(0.4331)
2023/12/02 13:56:56 - INFO - root -   Epoch: [56/300][280/283], lr: 0.00000020 	 loss = 1.9088(0.4297)
2023/12/02 13:56:57 - INFO - root -   Epoch: [56/300] 	 loss = 0.4271
2023/12/02 13:56:57 - INFO - root -   train_accuracy = 0.8357
2023/12/02 13:56:59 - INFO - root -   Epoch: [57/300][0/283], lr: 0.00000020 	 loss = 0.0605(0.0605)
2023/12/02 13:57:29 - INFO - root -   Epoch: [57/300][20/283], lr: 0.00000020 	 loss = 0.2197(0.5400)
2023/12/02 13:57:37 - INFO - root -   Epoch: [57/300][40/283], lr: 0.00000020 	 loss = 0.0948(0.5659)
2023/12/02 13:57:48 - INFO - root -   Epoch: [57/300][60/283], lr: 0.00000020 	 loss = 1.1629(0.5466)
2023/12/02 13:58:02 - INFO - root -   Epoch: [57/300][80/283], lr: 0.00000020 	 loss = 0.6496(0.5432)
2023/12/02 13:58:16 - INFO - root -   Epoch: [57/300][100/283], lr: 0.00000020 	 loss = 0.1697(0.5196)
2023/12/02 13:58:32 - INFO - root -   Epoch: [57/300][120/283], lr: 0.00000020 	 loss = 0.2095(0.5263)
2023/12/02 13:58:53 - INFO - root -   Epoch: [57/300][140/283], lr: 0.00000020 	 loss = 0.1616(0.4898)
2023/12/02 13:59:03 - INFO - root -   Epoch: [57/300][160/283], lr: 0.00000020 	 loss = 0.4026(0.4597)
2023/12/02 13:59:21 - INFO - root -   Epoch: [57/300][180/283], lr: 0.00000020 	 loss = 0.2202(0.4515)
2023/12/02 13:59:29 - INFO - root -   Epoch: [57/300][200/283], lr: 0.00000020 	 loss = 0.3192(0.4385)
2023/12/02 13:59:48 - INFO - root -   Epoch: [57/300][220/283], lr: 0.00000020 	 loss = 0.6677(0.4231)
2023/12/02 13:59:56 - INFO - root -   Epoch: [57/300][240/283], lr: 0.00000020 	 loss = 0.0685(0.4264)
2023/12/02 14:00:27 - INFO - root -   Epoch: [57/300][260/283], lr: 0.00000020 	 loss = 0.2616(0.4186)
2023/12/02 14:00:35 - INFO - root -   Epoch: [57/300][280/283], lr: 0.00000020 	 loss = 1.0245(0.4063)
2023/12/02 14:00:36 - INFO - root -   Epoch: [57/300] 	 loss = 0.4042
2023/12/02 14:00:36 - INFO - root -   train_accuracy = 0.8269
2023/12/02 14:00:52 - INFO - root -   Epoch: [58/300][0/283], lr: 0.00000020 	 loss = 0.1868(0.1868)
2023/12/02 14:01:04 - INFO - root -   Epoch: [58/300][20/283], lr: 0.00000020 	 loss = 0.9303(0.4262)
2023/12/02 14:01:15 - INFO - root -   Epoch: [58/300][40/283], lr: 0.00000020 	 loss = 0.3144(0.4199)
2023/12/02 14:01:33 - INFO - root -   Epoch: [58/300][60/283], lr: 0.00000020 	 loss = 0.7974(0.4674)
2023/12/02 14:01:49 - INFO - root -   Epoch: [58/300][80/283], lr: 0.00000020 	 loss = 1.2804(0.4897)
2023/12/02 14:01:57 - INFO - root -   Epoch: [58/300][100/283], lr: 0.00000020 	 loss = 0.1179(0.4556)
2023/12/02 14:02:15 - INFO - root -   Epoch: [58/300][120/283], lr: 0.00000020 	 loss = 1.4833(0.4621)
2023/12/02 14:02:25 - INFO - root -   Epoch: [58/300][140/283], lr: 0.00000020 	 loss = 0.1741(0.4251)
2023/12/02 14:02:49 - INFO - root -   Epoch: [58/300][160/283], lr: 0.00000020 	 loss = 0.1680(0.4180)
2023/12/02 14:03:00 - INFO - root -   Epoch: [58/300][180/283], lr: 0.00000020 	 loss = 0.6839(0.4119)
2023/12/02 14:03:18 - INFO - root -   Epoch: [58/300][200/283], lr: 0.00000020 	 loss = 0.7928(0.4126)
2023/12/02 14:03:29 - INFO - root -   Epoch: [58/300][220/283], lr: 0.00000020 	 loss = 0.0539(0.4008)
2023/12/02 14:03:54 - INFO - root -   Epoch: [58/300][240/283], lr: 0.00000020 	 loss = 0.0804(0.3981)
2023/12/02 14:04:01 - INFO - root -   Epoch: [58/300][260/283], lr: 0.00000020 	 loss = 0.1462(0.3878)
2023/12/02 14:04:09 - INFO - root -   Epoch: [58/300][280/283], lr: 0.00000020 	 loss = 0.4717(0.3909)
2023/12/02 14:04:10 - INFO - root -   Epoch: [58/300] 	 loss = 0.3891
2023/12/02 14:04:10 - INFO - root -   train_accuracy = 0.8198
2023/12/02 14:04:29 - INFO - root -   Epoch: [59/300][0/283], lr: 0.00000020 	 loss = 0.2762(0.2762)
2023/12/02 14:04:50 - INFO - root -   Epoch: [59/300][20/283], lr: 0.00000020 	 loss = 0.7538(0.5901)
2023/12/02 14:05:10 - INFO - root -   Epoch: [59/300][40/283], lr: 0.00000020 	 loss = 0.2253(0.6427)
2023/12/02 14:05:30 - INFO - root -   Epoch: [59/300][60/283], lr: 0.00000020 	 loss = 2.4465(0.6427)
2023/12/02 14:05:49 - INFO - root -   Epoch: [59/300][80/283], lr: 0.00000020 	 loss = 0.5426(0.5846)
2023/12/02 14:06:01 - INFO - root -   Epoch: [59/300][100/283], lr: 0.00000020 	 loss = 0.3632(0.5291)
2023/12/02 14:06:22 - INFO - root -   Epoch: [59/300][120/283], lr: 0.00000020 	 loss = 0.1543(0.5260)
2023/12/02 14:06:34 - INFO - root -   Epoch: [59/300][140/283], lr: 0.00000020 	 loss = 0.0707(0.4948)
2023/12/02 14:06:44 - INFO - root -   Epoch: [59/300][160/283], lr: 0.00000020 	 loss = 0.0477(0.4656)
2023/12/02 14:07:09 - INFO - root -   Epoch: [59/300][180/283], lr: 0.00000020 	 loss = 0.1163(0.4411)
2023/12/02 14:07:19 - INFO - root -   Epoch: [59/300][200/283], lr: 0.00000020 	 loss = 0.8463(0.4255)
2023/12/02 14:07:49 - INFO - root -   Epoch: [59/300][220/283], lr: 0.00000020 	 loss = 0.1703(0.4178)
2023/12/02 14:07:57 - INFO - root -   Epoch: [59/300][240/283], lr: 0.00000020 	 loss = 0.4576(0.4325)
2023/12/02 14:08:15 - INFO - root -   Epoch: [59/300][260/283], lr: 0.00000020 	 loss = 0.2700(0.4287)
2023/12/02 14:08:23 - INFO - root -   Epoch: [59/300][280/283], lr: 0.00000020 	 loss = 1.8568(0.4247)
2023/12/02 14:08:24 - INFO - root -   Epoch: [59/300] 	 loss = 0.4239
2023/12/02 14:08:34 - INFO - root -   precision = 0.8322
2023/12/02 14:08:34 - INFO - root -   eval_loss = 0.4077
2023/12/02 14:08:34 - INFO - root -   eval_acc = 0.8322
2023/12/02 14:08:34 - INFO - root -   train_accuracy = 0.8286
2023/12/02 14:08:57 - INFO - root -   Epoch: [60/300][0/283], lr: 0.00000020 	 loss = 0.6304(0.6304)
2023/12/02 14:09:04 - INFO - root -   Epoch: [60/300][20/283], lr: 0.00000020 	 loss = 0.6754(0.4966)
2023/12/02 14:09:28 - INFO - root -   Epoch: [60/300][40/283], lr: 0.00000020 	 loss = 0.3698(0.5459)
2023/12/02 14:09:46 - INFO - root -   Epoch: [60/300][60/283], lr: 0.00000020 	 loss = 0.5434(0.5562)
2023/12/02 14:09:54 - INFO - root -   Epoch: [60/300][80/283], lr: 0.00000020 	 loss = 0.7638(0.5277)
2023/12/02 14:10:18 - INFO - root -   Epoch: [60/300][100/283], lr: 0.00000020 	 loss = 0.0743(0.4998)
2023/12/02 14:10:30 - INFO - root -   Epoch: [60/300][120/283], lr: 0.00000020 	 loss = 0.5227(0.4933)
2023/12/02 14:10:38 - INFO - root -   Epoch: [60/300][140/283], lr: 0.00000020 	 loss = 0.0362(0.4430)
2023/12/02 14:10:55 - INFO - root -   Epoch: [60/300][160/283], lr: 0.00000020 	 loss = 0.5817(0.4147)
2023/12/02 14:11:12 - INFO - root -   Epoch: [60/300][180/283], lr: 0.00000020 	 loss = 0.2032(0.3984)
2023/12/02 14:11:21 - INFO - root -   Epoch: [60/300][200/283], lr: 0.00000020 	 loss = 0.2567(0.3964)
2023/12/02 14:11:36 - INFO - root -   Epoch: [60/300][220/283], lr: 0.00000020 	 loss = 0.0681(0.3847)
2023/12/02 14:11:49 - INFO - root -   Epoch: [60/300][240/283], lr: 0.00000020 	 loss = 0.0519(0.3915)
2023/12/02 14:11:58 - INFO - root -   Epoch: [60/300][260/283], lr: 0.00000020 	 loss = 0.0649(0.3934)
2023/12/02 14:12:05 - INFO - root -   Epoch: [60/300][280/283], lr: 0.00000020 	 loss = 1.6085(0.3895)
2023/12/02 14:12:06 - INFO - root -   Epoch: [60/300] 	 loss = 0.3876
2023/12/02 14:12:06 - INFO - root -   train_accuracy = 0.8269
2023/12/02 14:12:09 - INFO - root -   Epoch: [61/300][0/283], lr: 0.00000021 	 loss = 0.1935(0.1935)
2023/12/02 14:12:34 - INFO - root -   Epoch: [61/300][20/283], lr: 0.00000021 	 loss = 0.3894(0.5101)
2023/12/02 14:12:44 - INFO - root -   Epoch: [61/300][40/283], lr: 0.00000021 	 loss = 0.1530(0.4930)
2023/12/02 14:13:04 - INFO - root -   Epoch: [61/300][60/283], lr: 0.00000021 	 loss = 1.8614(0.4716)
2023/12/02 14:13:15 - INFO - root -   Epoch: [61/300][80/283], lr: 0.00000021 	 loss = 0.2867(0.4677)
2023/12/02 14:13:38 - INFO - root -   Epoch: [61/300][100/283], lr: 0.00000021 	 loss = 0.1645(0.4365)
2023/12/02 14:14:03 - INFO - root -   Epoch: [61/300][120/283], lr: 0.00000021 	 loss = 0.6033(0.4584)
2023/12/02 14:14:16 - INFO - root -   Epoch: [61/300][140/283], lr: 0.00000021 	 loss = 0.2391(0.4258)
2023/12/02 14:14:30 - INFO - root -   Epoch: [61/300][160/283], lr: 0.00000021 	 loss = 0.0808(0.4218)
2023/12/02 14:14:46 - INFO - root -   Epoch: [61/300][180/283], lr: 0.00000021 	 loss = 0.4222(0.4070)
2023/12/02 14:14:57 - INFO - root -   Epoch: [61/300][200/283], lr: 0.00000021 	 loss = 0.0915(0.4026)
2023/12/02 14:15:22 - INFO - root -   Epoch: [61/300][220/283], lr: 0.00000021 	 loss = 0.0474(0.3908)
2023/12/02 14:15:38 - INFO - root -   Epoch: [61/300][240/283], lr: 0.00000021 	 loss = 0.0975(0.3931)
2023/12/02 14:15:46 - INFO - root -   Epoch: [61/300][260/283], lr: 0.00000021 	 loss = 0.0311(0.3862)
2023/12/02 14:15:56 - INFO - root -   Epoch: [61/300][280/283], lr: 0.00000021 	 loss = 1.4163(0.3809)
2023/12/02 14:15:57 - INFO - root -   Epoch: [61/300] 	 loss = 0.3786
2023/12/02 14:15:57 - INFO - root -   train_accuracy = 0.8640
2023/12/02 14:15:59 - INFO - root -   Epoch: [62/300][0/283], lr: 0.00000021 	 loss = 0.1115(0.1115)
2023/12/02 14:16:30 - INFO - root -   Epoch: [62/300][20/283], lr: 0.00000021 	 loss = 0.4753(0.4403)
2023/12/02 14:16:38 - INFO - root -   Epoch: [62/300][40/283], lr: 0.00000021 	 loss = 0.2301(0.4922)
2023/12/02 14:16:54 - INFO - root -   Epoch: [62/300][60/283], lr: 0.00000021 	 loss = 1.6667(0.5200)
2023/12/02 14:17:07 - INFO - root -   Epoch: [62/300][80/283], lr: 0.00000021 	 loss = 0.2983(0.4580)
2023/12/02 14:17:15 - INFO - root -   Epoch: [62/300][100/283], lr: 0.00000021 	 loss = 0.0797(0.4541)
2023/12/02 14:17:35 - INFO - root -   Epoch: [62/300][120/283], lr: 0.00000021 	 loss = 0.1410(0.4592)
2023/12/02 14:17:52 - INFO - root -   Epoch: [62/300][140/283], lr: 0.00000021 	 loss = 0.0293(0.4168)
2023/12/02 14:18:08 - INFO - root -   Epoch: [62/300][160/283], lr: 0.00000021 	 loss = 0.0246(0.4140)
2023/12/02 14:18:39 - INFO - root -   Epoch: [62/300][180/283], lr: 0.00000021 	 loss = 0.1361(0.3967)
2023/12/02 14:18:50 - INFO - root -   Epoch: [62/300][200/283], lr: 0.00000021 	 loss = 0.1875(0.3961)
2023/12/02 14:19:17 - INFO - root -   Epoch: [62/300][220/283], lr: 0.00000021 	 loss = 0.0670(0.3751)
2023/12/02 14:19:25 - INFO - root -   Epoch: [62/300][240/283], lr: 0.00000021 	 loss = 0.0552(0.3733)
2023/12/02 14:19:39 - INFO - root -   Epoch: [62/300][260/283], lr: 0.00000021 	 loss = 0.1582(0.3696)
2023/12/02 14:19:49 - INFO - root -   Epoch: [62/300][280/283], lr: 0.00000021 	 loss = 0.4884(0.3703)
2023/12/02 14:19:49 - INFO - root -   Epoch: [62/300] 	 loss = 0.3683
2023/12/02 14:19:49 - INFO - root -   train_accuracy = 0.8481
2023/12/02 14:19:51 - INFO - root -   Epoch: [63/300][0/283], lr: 0.00000021 	 loss = 0.0456(0.0456)
2023/12/02 14:20:22 - INFO - root -   Epoch: [63/300][20/283], lr: 0.00000021 	 loss = 0.6508(0.4612)
2023/12/02 14:20:34 - INFO - root -   Epoch: [63/300][40/283], lr: 0.00000021 	 loss = 0.5277(0.4582)
2023/12/02 14:20:48 - INFO - root -   Epoch: [63/300][60/283], lr: 0.00000021 	 loss = 1.2741(0.4591)
2023/12/02 14:21:08 - INFO - root -   Epoch: [63/300][80/283], lr: 0.00000021 	 loss = 0.4481(0.4697)
2023/12/02 14:21:31 - INFO - root -   Epoch: [63/300][100/283], lr: 0.00000021 	 loss = 0.3488(0.4287)
2023/12/02 14:21:41 - INFO - root -   Epoch: [63/300][120/283], lr: 0.00000021 	 loss = 0.3839(0.4502)
2023/12/02 14:21:57 - INFO - root -   Epoch: [63/300][140/283], lr: 0.00000021 	 loss = 0.0434(0.4172)
2023/12/02 14:22:12 - INFO - root -   Epoch: [63/300][160/283], lr: 0.00000021 	 loss = 0.7137(0.4005)
2023/12/02 14:22:25 - INFO - root -   Epoch: [63/300][180/283], lr: 0.00000021 	 loss = 1.0190(0.3985)
2023/12/02 14:22:49 - INFO - root -   Epoch: [63/300][200/283], lr: 0.00000021 	 loss = 0.0547(0.3887)
2023/12/02 14:23:11 - INFO - root -   Epoch: [63/300][220/283], lr: 0.00000021 	 loss = 0.1094(0.3791)
2023/12/02 14:23:19 - INFO - root -   Epoch: [63/300][240/283], lr: 0.00000021 	 loss = 0.0772(0.3801)
2023/12/02 14:23:41 - INFO - root -   Epoch: [63/300][260/283], lr: 0.00000021 	 loss = 0.1861(0.3721)
2023/12/02 14:23:49 - INFO - root -   Epoch: [63/300][280/283], lr: 0.00000021 	 loss = 1.1307(0.3671)
2023/12/02 14:23:50 - INFO - root -   Epoch: [63/300] 	 loss = 0.3653
2023/12/02 14:23:50 - INFO - root -   train_accuracy = 0.8357
2023/12/02 14:24:03 - INFO - root -   Epoch: [64/300][0/283], lr: 0.00000021 	 loss = 0.5571(0.5571)
2023/12/02 14:24:29 - INFO - root -   Epoch: [64/300][20/283], lr: 0.00000021 	 loss = 0.6583(0.5514)
2023/12/02 14:24:37 - INFO - root -   Epoch: [64/300][40/283], lr: 0.00000021 	 loss = 0.1646(0.5215)
2023/12/02 14:24:53 - INFO - root -   Epoch: [64/300][60/283], lr: 0.00000021 	 loss = 1.5975(0.5339)
2023/12/02 14:25:12 - INFO - root -   Epoch: [64/300][80/283], lr: 0.00000021 	 loss = 0.7555(0.5045)
2023/12/02 14:25:23 - INFO - root -   Epoch: [64/300][100/283], lr: 0.00000021 	 loss = 0.0867(0.4675)
2023/12/02 14:25:44 - INFO - root -   Epoch: [64/300][120/283], lr: 0.00000021 	 loss = 0.7236(0.4542)
2023/12/02 14:25:52 - INFO - root -   Epoch: [64/300][140/283], lr: 0.00000021 	 loss = 0.0255(0.4365)
2023/12/02 14:26:09 - INFO - root -   Epoch: [64/300][160/283], lr: 0.00000021 	 loss = 0.1278(0.4105)
2023/12/02 14:26:20 - INFO - root -   Epoch: [64/300][180/283], lr: 0.00000021 	 loss = 0.0884(0.3949)
2023/12/02 14:26:35 - INFO - root -   Epoch: [64/300][200/283], lr: 0.00000021 	 loss = 0.1453(0.3815)
2023/12/02 14:26:45 - INFO - root -   Epoch: [64/300][220/283], lr: 0.00000021 	 loss = 0.0698(0.3674)
2023/12/02 14:27:09 - INFO - root -   Epoch: [64/300][240/283], lr: 0.00000021 	 loss = 0.1419(0.3651)
2023/12/02 14:27:17 - INFO - root -   Epoch: [64/300][260/283], lr: 0.00000021 	 loss = 0.0320(0.3654)
2023/12/02 14:27:29 - INFO - root -   Epoch: [64/300][280/283], lr: 0.00000021 	 loss = 1.1449(0.3571)
2023/12/02 14:27:30 - INFO - root -   Epoch: [64/300] 	 loss = 0.3553
2023/12/02 14:27:40 - INFO - root -   precision = 0.8182
2023/12/02 14:27:40 - INFO - root -   eval_loss = 0.4413
2023/12/02 14:27:40 - INFO - root -   eval_acc = 0.8182
2023/12/02 14:27:40 - INFO - root -   train_accuracy = 0.8640
2023/12/02 14:27:43 - INFO - root -   Epoch: [65/300][0/283], lr: 0.00000021 	 loss = 0.5511(0.5511)
2023/12/02 14:28:03 - INFO - root -   Epoch: [65/300][20/283], lr: 0.00000021 	 loss = 1.5708(0.5079)
2023/12/02 14:28:23 - INFO - root -   Epoch: [65/300][40/283], lr: 0.00000021 	 loss = 0.1008(0.5246)
2023/12/02 14:28:35 - INFO - root -   Epoch: [65/300][60/283], lr: 0.00000021 	 loss = 1.2911(0.4782)
2023/12/02 14:28:56 - INFO - root -   Epoch: [65/300][80/283], lr: 0.00000021 	 loss = 0.2401(0.4760)
2023/12/02 14:29:07 - INFO - root -   Epoch: [65/300][100/283], lr: 0.00000021 	 loss = 0.2215(0.4613)
2023/12/02 14:29:19 - INFO - root -   Epoch: [65/300][120/283], lr: 0.00000021 	 loss = 0.2362(0.4660)
2023/12/02 14:29:38 - INFO - root -   Epoch: [65/300][140/283], lr: 0.00000021 	 loss = 0.0553(0.4315)
2023/12/02 14:29:47 - INFO - root -   Epoch: [65/300][160/283], lr: 0.00000021 	 loss = 0.0566(0.4009)
2023/12/02 14:30:03 - INFO - root -   Epoch: [65/300][180/283], lr: 0.00000021 	 loss = 0.2154(0.3860)
2023/12/02 14:30:19 - INFO - root -   Epoch: [65/300][200/283], lr: 0.00000021 	 loss = 0.0721(0.3743)
2023/12/02 14:30:34 - INFO - root -   Epoch: [65/300][220/283], lr: 0.00000021 	 loss = 0.1216(0.3643)
2023/12/02 14:30:51 - INFO - root -   Epoch: [65/300][240/283], lr: 0.00000021 	 loss = 0.5016(0.3647)
2023/12/02 14:30:59 - INFO - root -   Epoch: [65/300][260/283], lr: 0.00000021 	 loss = 0.1222(0.3684)
2023/12/02 14:31:08 - INFO - root -   Epoch: [65/300][280/283], lr: 0.00000021 	 loss = 1.3008(0.3712)
2023/12/02 14:31:09 - INFO - root -   Epoch: [65/300] 	 loss = 0.3693
2023/12/02 14:31:09 - INFO - root -   train_accuracy = 0.8693
2023/12/02 14:31:24 - INFO - root -   Epoch: [66/300][0/283], lr: 0.00000022 	 loss = 0.4825(0.4825)
2023/12/02 14:31:39 - INFO - root -   Epoch: [66/300][20/283], lr: 0.00000022 	 loss = 0.4362(0.3496)
2023/12/02 14:31:53 - INFO - root -   Epoch: [66/300][40/283], lr: 0.00000022 	 loss = 0.3037(0.4861)
2023/12/02 14:32:12 - INFO - root -   Epoch: [66/300][60/283], lr: 0.00000022 	 loss = 0.5948(0.4976)
2023/12/02 14:32:27 - INFO - root -   Epoch: [66/300][80/283], lr: 0.00000022 	 loss = 1.1350(0.4560)
2023/12/02 14:32:35 - INFO - root -   Epoch: [66/300][100/283], lr: 0.00000022 	 loss = 0.1576(0.4569)
2023/12/02 14:32:55 - INFO - root -   Epoch: [66/300][120/283], lr: 0.00000022 	 loss = 0.7142(0.4771)
2023/12/02 14:33:15 - INFO - root -   Epoch: [66/300][140/283], lr: 0.00000022 	 loss = 0.0195(0.4413)
2023/12/02 14:33:23 - INFO - root -   Epoch: [66/300][160/283], lr: 0.00000022 	 loss = 0.1364(0.4206)
2023/12/02 14:33:31 - INFO - root -   Epoch: [66/300][180/283], lr: 0.00000022 	 loss = 0.1970(0.4018)
2023/12/02 14:34:04 - INFO - root -   Epoch: [66/300][200/283], lr: 0.00000022 	 loss = 0.7710(0.4048)
2023/12/02 14:34:12 - INFO - root -   Epoch: [66/300][220/283], lr: 0.00000022 	 loss = 0.0848(0.3864)
2023/12/02 14:34:25 - INFO - root -   Epoch: [66/300][240/283], lr: 0.00000022 	 loss = 0.1016(0.3873)
2023/12/02 14:34:45 - INFO - root -   Epoch: [66/300][260/283], lr: 0.00000022 	 loss = 0.0468(0.3860)
2023/12/02 14:34:53 - INFO - root -   Epoch: [66/300][280/283], lr: 0.00000022 	 loss = 0.7425(0.3801)
2023/12/02 14:34:53 - INFO - root -   Epoch: [66/300] 	 loss = 0.3784
2023/12/02 14:34:53 - INFO - root -   train_accuracy = 0.8322
2023/12/02 14:35:17 - INFO - root -   Epoch: [67/300][0/283], lr: 0.00000022 	 loss = 0.1943(0.1943)
2023/12/02 14:35:31 - INFO - root -   Epoch: [67/300][20/283], lr: 0.00000022 	 loss = 0.9983(0.5232)
2023/12/02 14:35:39 - INFO - root -   Epoch: [67/300][40/283], lr: 0.00000022 	 loss = 0.4185(0.4748)
2023/12/02 14:35:51 - INFO - root -   Epoch: [67/300][60/283], lr: 0.00000022 	 loss = 1.8430(0.4802)
2023/12/02 14:36:06 - INFO - root -   Epoch: [67/300][80/283], lr: 0.00000022 	 loss = 0.2143(0.4552)
2023/12/02 14:36:15 - INFO - root -   Epoch: [67/300][100/283], lr: 0.00000022 	 loss = 0.1066(0.4228)
2023/12/02 14:36:24 - INFO - root -   Epoch: [67/300][120/283], lr: 0.00000022 	 loss = 0.6909(0.4569)
2023/12/02 14:36:52 - INFO - root -   Epoch: [67/300][140/283], lr: 0.00000022 	 loss = 0.0371(0.4428)
2023/12/02 14:37:08 - INFO - root -   Epoch: [67/300][160/283], lr: 0.00000022 	 loss = 0.2418(0.4269)
2023/12/02 14:37:31 - INFO - root -   Epoch: [67/300][180/283], lr: 0.00000022 	 loss = 0.4871(0.4093)
2023/12/02 14:37:45 - INFO - root -   Epoch: [67/300][200/283], lr: 0.00000022 	 loss = 0.1209(0.4171)
2023/12/02 14:38:01 - INFO - root -   Epoch: [67/300][220/283], lr: 0.00000022 	 loss = 0.5139(0.4138)
2023/12/02 14:38:22 - INFO - root -   Epoch: [67/300][240/283], lr: 0.00000022 	 loss = 0.1501(0.4205)
2023/12/02 14:38:34 - INFO - root -   Epoch: [67/300][260/283], lr: 0.00000022 	 loss = 0.3501(0.4164)
2023/12/02 14:38:42 - INFO - root -   Epoch: [67/300][280/283], lr: 0.00000022 	 loss = 0.8783(0.4021)
2023/12/02 14:38:43 - INFO - root -   Epoch: [67/300] 	 loss = 0.3995
2023/12/02 14:38:43 - INFO - root -   train_accuracy = 0.8375
2023/12/02 14:38:59 - INFO - root -   Epoch: [68/300][0/283], lr: 0.00000022 	 loss = 0.0150(0.0150)
2023/12/02 14:39:16 - INFO - root -   Epoch: [68/300][20/283], lr: 0.00000022 	 loss = 0.2006(0.6129)
2023/12/02 14:39:28 - INFO - root -   Epoch: [68/300][40/283], lr: 0.00000022 	 loss = 0.0724(0.5627)
2023/12/02 14:39:36 - INFO - root -   Epoch: [68/300][60/283], lr: 0.00000022 	 loss = 0.5151(0.5175)
2023/12/02 14:40:00 - INFO - root -   Epoch: [68/300][80/283], lr: 0.00000022 	 loss = 0.7831(0.4978)
2023/12/02 14:40:15 - INFO - root -   Epoch: [68/300][100/283], lr: 0.00000022 	 loss = 0.3268(0.4929)
2023/12/02 14:40:31 - INFO - root -   Epoch: [68/300][120/283], lr: 0.00000022 	 loss = 0.2261(0.4996)
2023/12/02 14:40:39 - INFO - root -   Epoch: [68/300][140/283], lr: 0.00000022 	 loss = 0.0347(0.4615)
2023/12/02 14:40:57 - INFO - root -   Epoch: [68/300][160/283], lr: 0.00000022 	 loss = 0.0432(0.4323)
2023/12/02 14:41:19 - INFO - root -   Epoch: [68/300][180/283], lr: 0.00000022 	 loss = 0.2525(0.4088)
2023/12/02 14:41:32 - INFO - root -   Epoch: [68/300][200/283], lr: 0.00000022 	 loss = 0.5490(0.4102)
2023/12/02 14:41:46 - INFO - root -   Epoch: [68/300][220/283], lr: 0.00000022 	 loss = 0.1713(0.3982)
2023/12/02 14:42:00 - INFO - root -   Epoch: [68/300][240/283], lr: 0.00000022 	 loss = 0.0583(0.4035)
2023/12/02 14:42:20 - INFO - root -   Epoch: [68/300][260/283], lr: 0.00000022 	 loss = 0.2206(0.4042)
2023/12/02 14:42:28 - INFO - root -   Epoch: [68/300][280/283], lr: 0.00000022 	 loss = 1.1608(0.3937)
2023/12/02 14:42:29 - INFO - root -   Epoch: [68/300] 	 loss = 0.3913
2023/12/02 14:42:29 - INFO - root -   train_accuracy = 0.8392
2023/12/02 14:42:41 - INFO - root -   Epoch: [69/300][0/283], lr: 0.00000022 	 loss = 2.1650(2.1650)
2023/12/02 14:42:52 - INFO - root -   Epoch: [69/300][20/283], lr: 0.00000022 	 loss = 0.5108(0.4048)
2023/12/02 14:43:16 - INFO - root -   Epoch: [69/300][40/283], lr: 0.00000022 	 loss = 0.1100(0.3712)
2023/12/02 14:43:24 - INFO - root -   Epoch: [69/300][60/283], lr: 0.00000022 	 loss = 0.9375(0.4170)
2023/12/02 14:43:35 - INFO - root -   Epoch: [69/300][80/283], lr: 0.00000022 	 loss = 0.5812(0.4323)
2023/12/02 14:43:50 - INFO - root -   Epoch: [69/300][100/283], lr: 0.00000022 	 loss = 0.1388(0.4358)
2023/12/02 14:44:06 - INFO - root -   Epoch: [69/300][120/283], lr: 0.00000022 	 loss = 0.9165(0.4549)
2023/12/02 14:44:26 - INFO - root -   Epoch: [69/300][140/283], lr: 0.00000022 	 loss = 0.0184(0.4246)
2023/12/02 14:44:34 - INFO - root -   Epoch: [69/300][160/283], lr: 0.00000022 	 loss = 0.0863(0.3999)
2023/12/02 14:44:53 - INFO - root -   Epoch: [69/300][180/283], lr: 0.00000022 	 loss = 0.2038(0.3773)
2023/12/02 14:45:01 - INFO - root -   Epoch: [69/300][200/283], lr: 0.00000022 	 loss = 0.1268(0.3763)
2023/12/02 14:45:13 - INFO - root -   Epoch: [69/300][220/283], lr: 0.00000022 	 loss = 0.1653(0.3644)
2023/12/02 14:45:24 - INFO - root -   Epoch: [69/300][240/283], lr: 0.00000022 	 loss = 0.0801(0.3575)
2023/12/02 14:45:40 - INFO - root -   Epoch: [69/300][260/283], lr: 0.00000022 	 loss = 0.0474(0.3608)
2023/12/02 14:45:48 - INFO - root -   Epoch: [69/300][280/283], lr: 0.00000022 	 loss = 0.5324(0.3486)
2023/12/02 14:45:49 - INFO - root -   Epoch: [69/300] 	 loss = 0.3464
2023/12/02 14:45:58 - INFO - root -   precision = 0.8531
2023/12/02 14:45:58 - INFO - root -   eval_loss = 0.4037
2023/12/02 14:45:58 - INFO - root -   eval_acc = 0.8531
2023/12/02 14:45:59 - INFO - root -   train_accuracy = 0.8604
2023/12/02 14:46:00 - INFO - root -   Epoch: [70/300][0/283], lr: 0.00000022 	 loss = 0.0596(0.0596)
2023/12/02 14:46:31 - INFO - root -   Epoch: [70/300][20/283], lr: 0.00000022 	 loss = 0.6615(0.4450)
2023/12/02 14:46:46 - INFO - root -   Epoch: [70/300][40/283], lr: 0.00000022 	 loss = 0.0569(0.5153)
2023/12/02 14:47:07 - INFO - root -   Epoch: [70/300][60/283], lr: 0.00000022 	 loss = 0.9086(0.5047)
2023/12/02 14:47:15 - INFO - root -   Epoch: [70/300][80/283], lr: 0.00000022 	 loss = 0.3640(0.5161)
2023/12/02 14:47:40 - INFO - root -   Epoch: [70/300][100/283], lr: 0.00000022 	 loss = 0.1615(0.4646)
2023/12/02 14:48:02 - INFO - root -   Epoch: [70/300][120/283], lr: 0.00000022 	 loss = 0.9153(0.5049)
2023/12/02 14:48:10 - INFO - root -   Epoch: [70/300][140/283], lr: 0.00000022 	 loss = 0.0207(0.4684)
2023/12/02 14:48:31 - INFO - root -   Epoch: [70/300][160/283], lr: 0.00000022 	 loss = 0.1960(0.4547)
2023/12/02 14:48:43 - INFO - root -   Epoch: [70/300][180/283], lr: 0.00000022 	 loss = 0.3871(0.4300)
2023/12/02 14:48:58 - INFO - root -   Epoch: [70/300][200/283], lr: 0.00000022 	 loss = 0.1688(0.4242)
2023/12/02 14:49:10 - INFO - root -   Epoch: [70/300][220/283], lr: 0.00000022 	 loss = 0.2156(0.4008)
2023/12/02 14:49:26 - INFO - root -   Epoch: [70/300][240/283], lr: 0.00000022 	 loss = 1.1019(0.4051)
2023/12/02 14:49:43 - INFO - root -   Epoch: [70/300][260/283], lr: 0.00000022 	 loss = 0.0499(0.4026)
2023/12/02 14:49:51 - INFO - root -   Epoch: [70/300][280/283], lr: 0.00000022 	 loss = 1.0067(0.3992)
2023/12/02 14:49:52 - INFO - root -   Epoch: [70/300] 	 loss = 0.3967
2023/12/02 14:49:52 - INFO - root -   train_accuracy = 0.8445
2023/12/02 14:50:07 - INFO - root -   Epoch: [71/300][0/283], lr: 0.00000022 	 loss = 0.0804(0.0804)
2023/12/02 14:50:24 - INFO - root -   Epoch: [71/300][20/283], lr: 0.00000022 	 loss = 0.4070(0.4668)
2023/12/02 14:50:48 - INFO - root -   Epoch: [71/300][40/283], lr: 0.00000022 	 loss = 0.4110(0.4301)
2023/12/02 14:50:56 - INFO - root -   Epoch: [71/300][60/283], lr: 0.00000022 	 loss = 0.8729(0.4971)
2023/12/02 14:51:12 - INFO - root -   Epoch: [71/300][80/283], lr: 0.00000022 	 loss = 1.0343(0.4884)
2023/12/02 14:51:29 - INFO - root -   Epoch: [71/300][100/283], lr: 0.00000022 	 loss = 0.4203(0.4703)
2023/12/02 14:51:36 - INFO - root -   Epoch: [71/300][120/283], lr: 0.00000022 	 loss = 0.3709(0.4760)
2023/12/02 14:51:57 - INFO - root -   Epoch: [71/300][140/283], lr: 0.00000022 	 loss = 0.1953(0.4431)
2023/12/02 14:52:05 - INFO - root -   Epoch: [71/300][160/283], lr: 0.00000022 	 loss = 0.9501(0.4298)
2023/12/02 14:52:19 - INFO - root -   Epoch: [71/300][180/283], lr: 0.00000022 	 loss = 0.2662(0.4083)
2023/12/02 14:52:31 - INFO - root -   Epoch: [71/300][200/283], lr: 0.00000022 	 loss = 0.1828(0.4015)
2023/12/02 14:52:43 - INFO - root -   Epoch: [71/300][220/283], lr: 0.00000022 	 loss = 0.0613(0.3907)
2023/12/02 14:53:01 - INFO - root -   Epoch: [71/300][240/283], lr: 0.00000022 	 loss = 0.1499(0.3948)
2023/12/02 14:53:17 - INFO - root -   Epoch: [71/300][260/283], lr: 0.00000022 	 loss = 0.0327(0.3853)
2023/12/02 14:53:25 - INFO - root -   Epoch: [71/300][280/283], lr: 0.00000022 	 loss = 0.3663(0.3729)
2023/12/02 14:53:26 - INFO - root -   Epoch: [71/300] 	 loss = 0.3705
2023/12/02 14:53:26 - INFO - root -   train_accuracy = 0.8339
2023/12/02 14:53:29 - INFO - root -   Epoch: [72/300][0/283], lr: 0.00000023 	 loss = 0.0367(0.0367)
2023/12/02 14:53:56 - INFO - root -   Epoch: [72/300][20/283], lr: 0.00000023 	 loss = 0.7405(0.4571)
2023/12/02 14:54:07 - INFO - root -   Epoch: [72/300][40/283], lr: 0.00000023 	 loss = 0.2979(0.4736)
2023/12/02 14:54:30 - INFO - root -   Epoch: [72/300][60/283], lr: 0.00000023 	 loss = 0.5328(0.4923)
2023/12/02 14:54:38 - INFO - root -   Epoch: [72/300][80/283], lr: 0.00000023 	 loss = 0.3132(0.4615)
2023/12/02 14:54:54 - INFO - root -   Epoch: [72/300][100/283], lr: 0.00000023 	 loss = 0.2212(0.4485)
2023/12/02 14:55:12 - INFO - root -   Epoch: [72/300][120/283], lr: 0.00000023 	 loss = 0.3908(0.4563)
2023/12/02 14:55:20 - INFO - root -   Epoch: [72/300][140/283], lr: 0.00000023 	 loss = 0.1255(0.4242)
2023/12/02 14:55:39 - INFO - root -   Epoch: [72/300][160/283], lr: 0.00000023 	 loss = 0.0163(0.3970)
2023/12/02 14:56:04 - INFO - root -   Epoch: [72/300][180/283], lr: 0.00000023 	 loss = 0.6061(0.3838)
2023/12/02 14:56:12 - INFO - root -   Epoch: [72/300][200/283], lr: 0.00000023 	 loss = 0.9648(0.3841)
2023/12/02 14:56:32 - INFO - root -   Epoch: [72/300][220/283], lr: 0.00000023 	 loss = 0.0832(0.3698)
2023/12/02 14:56:45 - INFO - root -   Epoch: [72/300][240/283], lr: 0.00000023 	 loss = 0.0659(0.3853)
2023/12/02 14:56:53 - INFO - root -   Epoch: [72/300][260/283], lr: 0.00000023 	 loss = 0.2461(0.3753)
2023/12/02 14:57:01 - INFO - root -   Epoch: [72/300][280/283], lr: 0.00000023 	 loss = 0.8449(0.3712)
2023/12/02 14:57:02 - INFO - root -   Epoch: [72/300] 	 loss = 0.3696
2023/12/02 14:57:02 - INFO - root -   train_accuracy = 0.8498
2023/12/02 14:57:19 - INFO - root -   Epoch: [73/300][0/283], lr: 0.00000023 	 loss = 0.0912(0.0912)
2023/12/02 14:57:36 - INFO - root -   Epoch: [73/300][20/283], lr: 0.00000023 	 loss = 0.3418(0.3806)
2023/12/02 14:57:44 - INFO - root -   Epoch: [73/300][40/283], lr: 0.00000023 	 loss = 0.4395(0.3954)
2023/12/02 14:57:52 - INFO - root -   Epoch: [73/300][60/283], lr: 0.00000023 	 loss = 1.7087(0.4211)
2023/12/02 14:58:21 - INFO - root -   Epoch: [73/300][80/283], lr: 0.00000023 	 loss = 0.5986(0.4971)
2023/12/02 14:58:50 - INFO - root -   Epoch: [73/300][100/283], lr: 0.00000023 	 loss = 0.4185(0.4677)
2023/12/02 14:58:58 - INFO - root -   Epoch: [73/300][120/283], lr: 0.00000023 	 loss = 0.2959(0.4881)
2023/12/02 14:59:15 - INFO - root -   Epoch: [73/300][140/283], lr: 0.00000023 	 loss = 0.0870(0.4543)
2023/12/02 14:59:34 - INFO - root -   Epoch: [73/300][160/283], lr: 0.00000023 	 loss = 0.0358(0.4241)
2023/12/02 15:00:04 - INFO - root -   Epoch: [73/300][180/283], lr: 0.00000023 	 loss = 0.3314(0.4020)
2023/12/02 15:00:12 - INFO - root -   Epoch: [73/300][200/283], lr: 0.00000023 	 loss = 0.0452(0.4008)
2023/12/02 15:00:26 - INFO - root -   Epoch: [73/300][220/283], lr: 0.00000023 	 loss = 0.0388(0.3874)
2023/12/02 15:00:34 - INFO - root -   Epoch: [73/300][240/283], lr: 0.00000023 	 loss = 0.0843(0.3880)
2023/12/02 15:00:49 - INFO - root -   Epoch: [73/300][260/283], lr: 0.00000023 	 loss = 0.1776(0.3839)
2023/12/02 15:00:57 - INFO - root -   Epoch: [73/300][280/283], lr: 0.00000023 	 loss = 1.9257(0.3804)
2023/12/02 15:00:58 - INFO - root -   Epoch: [73/300] 	 loss = 0.3787
2023/12/02 15:00:58 - INFO - root -   train_accuracy = 0.8551
2023/12/02 15:01:12 - INFO - root -   Epoch: [74/300][0/283], lr: 0.00000023 	 loss = 0.4190(0.4190)
2023/12/02 15:01:27 - INFO - root -   Epoch: [74/300][20/283], lr: 0.00000023 	 loss = 0.3376(0.4395)
2023/12/02 15:01:35 - INFO - root -   Epoch: [74/300][40/283], lr: 0.00000023 	 loss = 0.2701(0.4828)
2023/12/02 15:02:03 - INFO - root -   Epoch: [74/300][60/283], lr: 0.00000023 	 loss = 1.7722(0.4545)
2023/12/02 15:02:11 - INFO - root -   Epoch: [74/300][80/283], lr: 0.00000023 	 loss = 0.2799(0.4657)
2023/12/02 15:02:33 - INFO - root -   Epoch: [74/300][100/283], lr: 0.00000023 	 loss = 0.3603(0.4662)
2023/12/02 15:02:41 - INFO - root -   Epoch: [74/300][120/283], lr: 0.00000023 	 loss = 0.3177(0.4739)
2023/12/02 15:02:58 - INFO - root -   Epoch: [74/300][140/283], lr: 0.00000023 	 loss = 0.0377(0.4510)
2023/12/02 15:03:19 - INFO - root -   Epoch: [74/300][160/283], lr: 0.00000023 	 loss = 0.0719(0.4328)
2023/12/02 15:03:30 - INFO - root -   Epoch: [74/300][180/283], lr: 0.00000023 	 loss = 0.1342(0.4131)
2023/12/02 15:03:46 - INFO - root -   Epoch: [74/300][200/283], lr: 0.00000023 	 loss = 0.2774(0.4098)
2023/12/02 15:04:06 - INFO - root -   Epoch: [74/300][220/283], lr: 0.00000023 	 loss = 0.1635(0.4015)
2023/12/02 15:04:22 - INFO - root -   Epoch: [74/300][240/283], lr: 0.00000023 	 loss = 0.1545(0.4047)
2023/12/02 15:04:31 - INFO - root -   Epoch: [74/300][260/283], lr: 0.00000023 	 loss = 0.1411(0.3986)
2023/12/02 15:04:39 - INFO - root -   Epoch: [74/300][280/283], lr: 0.00000023 	 loss = 0.7817(0.3849)
2023/12/02 15:04:39 - INFO - root -   Epoch: [74/300] 	 loss = 0.3825
2023/12/02 15:04:49 - INFO - root -   precision = 0.8252
2023/12/02 15:04:49 - INFO - root -   eval_loss = 0.4074
2023/12/02 15:04:49 - INFO - root -   eval_acc = 0.8252
2023/12/02 15:04:49 - INFO - root -   train_accuracy = 0.8445
2023/12/02 15:04:51 - INFO - root -   Epoch: [75/300][0/283], lr: 0.00000023 	 loss = 0.0619(0.0619)
2023/12/02 15:05:29 - INFO - root -   Epoch: [75/300][20/283], lr: 0.00000023 	 loss = 0.2926(0.4840)
2023/12/02 15:05:45 - INFO - root -   Epoch: [75/300][40/283], lr: 0.00000023 	 loss = 0.1727(0.4360)
2023/12/02 15:06:05 - INFO - root -   Epoch: [75/300][60/283], lr: 0.00000023 	 loss = 0.9872(0.4866)
2023/12/02 15:06:24 - INFO - root -   Epoch: [75/300][80/283], lr: 0.00000023 	 loss = 0.6410(0.4861)
2023/12/02 15:06:44 - INFO - root -   Epoch: [75/300][100/283], lr: 0.00000023 	 loss = 0.4915(0.5085)
2023/12/02 15:07:02 - INFO - root -   Epoch: [75/300][120/283], lr: 0.00000023 	 loss = 0.4459(0.4887)
2023/12/02 15:07:10 - INFO - root -   Epoch: [75/300][140/283], lr: 0.00000023 	 loss = 0.3497(0.4524)
2023/12/02 15:07:38 - INFO - root -   Epoch: [75/300][160/283], lr: 0.00000023 	 loss = 0.2063(0.4318)
2023/12/02 15:07:46 - INFO - root -   Epoch: [75/300][180/283], lr: 0.00000023 	 loss = 0.5168(0.4272)
2023/12/02 15:08:01 - INFO - root -   Epoch: [75/300][200/283], lr: 0.00000023 	 loss = 0.9830(0.4179)
2023/12/02 15:08:23 - INFO - root -   Epoch: [75/300][220/283], lr: 0.00000023 	 loss = 0.1659(0.4069)
2023/12/02 15:08:31 - INFO - root -   Epoch: [75/300][240/283], lr: 0.00000023 	 loss = 0.0757(0.4175)
2023/12/02 15:08:39 - INFO - root -   Epoch: [75/300][260/283], lr: 0.00000023 	 loss = 0.0274(0.4107)
2023/12/02 15:08:47 - INFO - root -   Epoch: [75/300][280/283], lr: 0.00000023 	 loss = 0.3712(0.4044)
2023/12/02 15:08:48 - INFO - root -   Epoch: [75/300] 	 loss = 0.4034
2023/12/02 15:08:48 - INFO - root -   train_accuracy = 0.8357
2023/12/02 15:08:51 - INFO - root -   Epoch: [76/300][0/283], lr: 0.00000023 	 loss = 0.0202(0.0202)
2023/12/02 15:09:23 - INFO - root -   Epoch: [76/300][20/283], lr: 0.00000023 	 loss = 0.6180(0.4313)
2023/12/02 15:09:43 - INFO - root -   Epoch: [76/300][40/283], lr: 0.00000023 	 loss = 0.2730(0.4078)
2023/12/02 15:09:54 - INFO - root -   Epoch: [76/300][60/283], lr: 0.00000023 	 loss = 1.1445(0.3843)
2023/12/02 15:10:16 - INFO - root -   Epoch: [76/300][80/283], lr: 0.00000023 	 loss = 0.6317(0.3828)
2023/12/02 15:10:30 - INFO - root -   Epoch: [76/300][100/283], lr: 0.00000023 	 loss = 0.2977(0.3822)
2023/12/02 15:10:38 - INFO - root -   Epoch: [76/300][120/283], lr: 0.00000023 	 loss = 0.3543(0.4145)
2023/12/02 15:10:57 - INFO - root -   Epoch: [76/300][140/283], lr: 0.00000023 	 loss = 0.0370(0.3963)
2023/12/02 15:11:06 - INFO - root -   Epoch: [76/300][160/283], lr: 0.00000023 	 loss = 0.0920(0.3818)
2023/12/02 15:11:22 - INFO - root -   Epoch: [76/300][180/283], lr: 0.00000023 	 loss = 0.1523(0.3759)
2023/12/02 15:11:30 - INFO - root -   Epoch: [76/300][200/283], lr: 0.00000023 	 loss = 0.2003(0.3643)
2023/12/02 15:11:48 - INFO - root -   Epoch: [76/300][220/283], lr: 0.00000023 	 loss = 0.0656(0.3602)
2023/12/02 15:12:15 - INFO - root -   Epoch: [76/300][240/283], lr: 0.00000023 	 loss = 0.1695(0.3678)
2023/12/02 15:12:32 - INFO - root -   Epoch: [76/300][260/283], lr: 0.00000023 	 loss = 0.4366(0.3792)
2023/12/02 15:12:40 - INFO - root -   Epoch: [76/300][280/283], lr: 0.00000023 	 loss = 0.4364(0.3796)
2023/12/02 15:12:41 - INFO - root -   Epoch: [76/300] 	 loss = 0.3773
2023/12/02 15:12:41 - INFO - root -   train_accuracy = 0.8569
2023/12/02 15:13:03 - INFO - root -   Epoch: [77/300][0/283], lr: 0.00000023 	 loss = 0.1764(0.1764)
2023/12/02 15:13:12 - INFO - root -   Epoch: [77/300][20/283], lr: 0.00000023 	 loss = 0.5281(0.4040)
2023/12/02 15:13:40 - INFO - root -   Epoch: [77/300][40/283], lr: 0.00000023 	 loss = 0.1763(0.4333)
2023/12/02 15:13:48 - INFO - root -   Epoch: [77/300][60/283], lr: 0.00000023 	 loss = 1.8719(0.4961)
2023/12/02 15:14:07 - INFO - root -   Epoch: [77/300][80/283], lr: 0.00000023 	 loss = 0.3984(0.4834)
2023/12/02 15:14:25 - INFO - root -   Epoch: [77/300][100/283], lr: 0.00000023 	 loss = 0.7415(0.4879)
2023/12/02 15:14:33 - INFO - root -   Epoch: [77/300][120/283], lr: 0.00000023 	 loss = 0.7127(0.4909)
2023/12/02 15:14:56 - INFO - root -   Epoch: [77/300][140/283], lr: 0.00000023 	 loss = 0.1414(0.4509)
2023/12/02 15:15:08 - INFO - root -   Epoch: [77/300][160/283], lr: 0.00000023 	 loss = 0.6983(0.4325)
2023/12/02 15:15:28 - INFO - root -   Epoch: [77/300][180/283], lr: 0.00000023 	 loss = 0.3997(0.4178)
2023/12/02 15:15:41 - INFO - root -   Epoch: [77/300][200/283], lr: 0.00000023 	 loss = 1.1059(0.4138)
2023/12/02 15:16:02 - INFO - root -   Epoch: [77/300][220/283], lr: 0.00000023 	 loss = 0.0201(0.4021)
2023/12/02 15:16:17 - INFO - root -   Epoch: [77/300][240/283], lr: 0.00000023 	 loss = 0.2232(0.3936)
2023/12/02 15:16:25 - INFO - root -   Epoch: [77/300][260/283], lr: 0.00000023 	 loss = 0.1694(0.3966)
2023/12/02 15:16:33 - INFO - root -   Epoch: [77/300][280/283], lr: 0.00000023 	 loss = 1.0392(0.3893)
2023/12/02 15:16:34 - INFO - root -   Epoch: [77/300] 	 loss = 0.3869
2023/12/02 15:16:34 - INFO - root -   train_accuracy = 0.8286
2023/12/02 15:16:35 - INFO - root -   Epoch: [78/300][0/283], lr: 0.00000024 	 loss = 0.0770(0.0770)
2023/12/02 15:17:05 - INFO - root -   Epoch: [78/300][20/283], lr: 0.00000024 	 loss = 0.5551(0.4459)
2023/12/02 15:17:17 - INFO - root -   Epoch: [78/300][40/283], lr: 0.00000024 	 loss = 0.1218(0.4388)
2023/12/02 15:17:31 - INFO - root -   Epoch: [78/300][60/283], lr: 0.00000024 	 loss = 2.0445(0.4081)
2023/12/02 15:17:44 - INFO - root -   Epoch: [78/300][80/283], lr: 0.00000024 	 loss = 0.2448(0.3878)
2023/12/02 15:18:01 - INFO - root -   Epoch: [78/300][100/283], lr: 0.00000024 	 loss = 0.2281(0.3649)
2023/12/02 15:18:15 - INFO - root -   Epoch: [78/300][120/283], lr: 0.00000024 	 loss = 0.3093(0.3820)
2023/12/02 15:18:38 - INFO - root -   Epoch: [78/300][140/283], lr: 0.00000024 	 loss = 0.0390(0.3468)
2023/12/02 15:18:48 - INFO - root -   Epoch: [78/300][160/283], lr: 0.00000024 	 loss = 0.0329(0.3395)
2023/12/02 15:19:07 - INFO - root -   Epoch: [78/300][180/283], lr: 0.00000024 	 loss = 0.1203(0.3353)
2023/12/02 15:19:28 - INFO - root -   Epoch: [78/300][200/283], lr: 0.00000024 	 loss = 0.0245(0.3315)
2023/12/02 15:19:39 - INFO - root -   Epoch: [78/300][220/283], lr: 0.00000024 	 loss = 0.0226(0.3248)
2023/12/02 15:19:54 - INFO - root -   Epoch: [78/300][240/283], lr: 0.00000024 	 loss = 0.1915(0.3262)
2023/12/02 15:20:02 - INFO - root -   Epoch: [78/300][260/283], lr: 0.00000024 	 loss = 0.0382(0.3229)
2023/12/02 15:20:10 - INFO - root -   Epoch: [78/300][280/283], lr: 0.00000024 	 loss = 0.4792(0.3215)
2023/12/02 15:20:11 - INFO - root -   Epoch: [78/300] 	 loss = 0.3205
2023/12/02 15:20:11 - INFO - root -   train_accuracy = 0.8622
2023/12/02 15:20:14 - INFO - root -   Epoch: [79/300][0/283], lr: 0.00000024 	 loss = 0.0647(0.0647)
2023/12/02 15:20:39 - INFO - root -   Epoch: [79/300][20/283], lr: 0.00000024 	 loss = 1.9406(0.4851)
2023/12/02 15:20:58 - INFO - root -   Epoch: [79/300][40/283], lr: 0.00000024 	 loss = 0.1386(0.4917)
2023/12/02 15:21:06 - INFO - root -   Epoch: [79/300][60/283], lr: 0.00000024 	 loss = 1.6864(0.4979)
2023/12/02 15:21:14 - INFO - root -   Epoch: [79/300][80/283], lr: 0.00000024 	 loss = 0.5369(0.4626)
2023/12/02 15:21:32 - INFO - root -   Epoch: [79/300][100/283], lr: 0.00000024 	 loss = 0.0877(0.4510)
2023/12/02 15:21:54 - INFO - root -   Epoch: [79/300][120/283], lr: 0.00000024 	 loss = 0.7786(0.4643)
2023/12/02 15:22:04 - INFO - root -   Epoch: [79/300][140/283], lr: 0.00000024 	 loss = 0.0202(0.4275)
2023/12/02 15:22:12 - INFO - root -   Epoch: [79/300][160/283], lr: 0.00000024 	 loss = 0.2917(0.4055)
2023/12/02 15:22:36 - INFO - root -   Epoch: [79/300][180/283], lr: 0.00000024 	 loss = 1.1899(0.3884)
2023/12/02 15:22:52 - INFO - root -   Epoch: [79/300][200/283], lr: 0.00000024 	 loss = 0.1824(0.3847)
2023/12/02 15:23:03 - INFO - root -   Epoch: [79/300][220/283], lr: 0.00000024 	 loss = 0.0239(0.3683)
2023/12/02 15:23:18 - INFO - root -   Epoch: [79/300][240/283], lr: 0.00000024 	 loss = 0.1096(0.3717)
2023/12/02 15:23:35 - INFO - root -   Epoch: [79/300][260/283], lr: 0.00000024 	 loss = 0.1334(0.3735)
2023/12/02 15:23:42 - INFO - root -   Epoch: [79/300][280/283], lr: 0.00000024 	 loss = 1.2299(0.3641)
2023/12/02 15:23:43 - INFO - root -   Epoch: [79/300] 	 loss = 0.3626
2023/12/02 15:23:53 - INFO - root -   precision = 0.8112
2023/12/02 15:23:53 - INFO - root -   eval_loss = 0.4203
2023/12/02 15:23:53 - INFO - root -   eval_acc = 0.8112
2023/12/02 15:23:53 - INFO - root -   train_accuracy = 0.8622
2023/12/02 15:24:09 - INFO - root -   Epoch: [80/300][0/283], lr: 0.00000024 	 loss = 0.1366(0.1366)
2023/12/02 15:24:37 - INFO - root -   Epoch: [80/300][20/283], lr: 0.00000024 	 loss = 1.5947(0.5811)
2023/12/02 15:24:45 - INFO - root -   Epoch: [80/300][40/283], lr: 0.00000024 	 loss = 0.1759(0.5013)
2023/12/02 15:25:04 - INFO - root -   Epoch: [80/300][60/283], lr: 0.00000024 	 loss = 2.0100(0.5323)
2023/12/02 15:25:22 - INFO - root -   Epoch: [80/300][80/283], lr: 0.00000024 	 loss = 1.2588(0.5026)
2023/12/02 15:25:37 - INFO - root -   Epoch: [80/300][100/283], lr: 0.00000024 	 loss = 0.0733(0.4783)
2023/12/02 15:25:45 - INFO - root -   Epoch: [80/300][120/283], lr: 0.00000024 	 loss = 0.5010(0.4943)
2023/12/02 15:26:10 - INFO - root -   Epoch: [80/300][140/283], lr: 0.00000024 	 loss = 0.2908(0.4649)
2023/12/02 15:26:32 - INFO - root -   Epoch: [80/300][160/283], lr: 0.00000024 	 loss = 0.4069(0.4321)
2023/12/02 15:26:41 - INFO - root -   Epoch: [80/300][180/283], lr: 0.00000024 	 loss = 0.1367(0.4187)
2023/12/02 15:26:57 - INFO - root -   Epoch: [80/300][200/283], lr: 0.00000024 	 loss = 0.1759(0.4153)
2023/12/02 15:27:11 - INFO - root -   Epoch: [80/300][220/283], lr: 0.00000024 	 loss = 0.0935(0.4033)
2023/12/02 15:27:28 - INFO - root -   Epoch: [80/300][240/283], lr: 0.00000024 	 loss = 0.3675(0.4048)
2023/12/02 15:27:44 - INFO - root -   Epoch: [80/300][260/283], lr: 0.00000024 	 loss = 0.1085(0.3997)
2023/12/02 15:27:52 - INFO - root -   Epoch: [80/300][280/283], lr: 0.00000024 	 loss = 0.2368(0.3961)
2023/12/02 15:27:53 - INFO - root -   Epoch: [80/300] 	 loss = 0.3950
2023/12/02 15:27:53 - INFO - root -   train_accuracy = 0.8392
2023/12/02 15:27:54 - INFO - root -   Epoch: [81/300][0/283], lr: 0.00000024 	 loss = 0.0455(0.0455)
2023/12/02 15:28:20 - INFO - root -   Epoch: [81/300][20/283], lr: 0.00000024 	 loss = 1.1285(0.3781)
2023/12/02 15:28:32 - INFO - root -   Epoch: [81/300][40/283], lr: 0.00000024 	 loss = 0.1753(0.4573)
2023/12/02 15:28:48 - INFO - root -   Epoch: [81/300][60/283], lr: 0.00000024 	 loss = 1.6809(0.4680)
2023/12/02 15:29:02 - INFO - root -   Epoch: [81/300][80/283], lr: 0.00000024 	 loss = 0.7387(0.4863)
2023/12/02 15:29:19 - INFO - root -   Epoch: [81/300][100/283], lr: 0.00000024 	 loss = 0.0929(0.4669)
2023/12/02 15:29:28 - INFO - root -   Epoch: [81/300][120/283], lr: 0.00000024 	 loss = 0.3111(0.4927)
2023/12/02 15:29:51 - INFO - root -   Epoch: [81/300][140/283], lr: 0.00000024 	 loss = 0.1715(0.4501)
2023/12/02 15:29:59 - INFO - root -   Epoch: [81/300][160/283], lr: 0.00000024 	 loss = 0.0824(0.4159)
2023/12/02 15:30:23 - INFO - root -   Epoch: [81/300][180/283], lr: 0.00000024 	 loss = 0.4373(0.3986)
2023/12/02 15:30:36 - INFO - root -   Epoch: [81/300][200/283], lr: 0.00000024 	 loss = 0.2237(0.3843)
2023/12/02 15:30:55 - INFO - root -   Epoch: [81/300][220/283], lr: 0.00000024 	 loss = 0.4760(0.3766)
2023/12/02 15:31:02 - INFO - root -   Epoch: [81/300][240/283], lr: 0.00000024 	 loss = 0.1633(0.3778)
2023/12/02 15:31:10 - INFO - root -   Epoch: [81/300][260/283], lr: 0.00000024 	 loss = 0.2472(0.3736)
2023/12/02 15:31:21 - INFO - root -   Epoch: [81/300][280/283], lr: 0.00000024 	 loss = 0.5867(0.3674)
2023/12/02 15:31:22 - INFO - root -   Epoch: [81/300] 	 loss = 0.3657
2023/12/02 15:31:22 - INFO - root -   train_accuracy = 0.8357
2023/12/02 15:31:23 - INFO - root -   Epoch: [82/300][0/283], lr: 0.00000024 	 loss = 0.0319(0.0319)
2023/12/02 15:31:50 - INFO - root -   Epoch: [82/300][20/283], lr: 0.00000024 	 loss = 0.7500(0.3744)
2023/12/02 15:32:14 - INFO - root -   Epoch: [82/300][40/283], lr: 0.00000024 	 loss = 0.1700(0.4507)
2023/12/02 15:32:32 - INFO - root -   Epoch: [82/300][60/283], lr: 0.00000024 	 loss = 0.4963(0.4796)
2023/12/02 15:32:40 - INFO - root -   Epoch: [82/300][80/283], lr: 0.00000024 	 loss = 0.7945(0.4658)
2023/12/02 15:32:57 - INFO - root -   Epoch: [82/300][100/283], lr: 0.00000024 	 loss = 0.1911(0.4329)
2023/12/02 15:33:09 - INFO - root -   Epoch: [82/300][120/283], lr: 0.00000024 	 loss = 0.3201(0.4714)
2023/12/02 15:33:27 - INFO - root -   Epoch: [82/300][140/283], lr: 0.00000024 	 loss = 0.0138(0.4349)
2023/12/02 15:33:50 - INFO - root -   Epoch: [82/300][160/283], lr: 0.00000024 	 loss = 0.1112(0.4112)
2023/12/02 15:33:58 - INFO - root -   Epoch: [82/300][180/283], lr: 0.00000024 	 loss = 0.6192(0.3890)
2023/12/02 15:34:08 - INFO - root -   Epoch: [82/300][200/283], lr: 0.00000024 	 loss = 0.1912(0.3762)
2023/12/02 15:34:27 - INFO - root -   Epoch: [82/300][220/283], lr: 0.00000024 	 loss = 0.1046(0.3653)
2023/12/02 15:34:41 - INFO - root -   Epoch: [82/300][240/283], lr: 0.00000024 	 loss = 0.3249(0.3622)
2023/12/02 15:35:03 - INFO - root -   Epoch: [82/300][260/283], lr: 0.00000024 	 loss = 0.1791(0.3622)
2023/12/02 15:35:11 - INFO - root -   Epoch: [82/300][280/283], lr: 0.00000024 	 loss = 1.0823(0.3601)
2023/12/02 15:35:12 - INFO - root -   Epoch: [82/300] 	 loss = 0.3586
2023/12/02 15:35:12 - INFO - root -   train_accuracy = 0.8551
2023/12/02 15:35:14 - INFO - root -   Epoch: [83/300][0/283], lr: 0.00000025 	 loss = 0.0720(0.0720)
2023/12/02 15:35:44 - INFO - root -   Epoch: [83/300][20/283], lr: 0.00000025 	 loss = 1.4807(0.5401)
2023/12/02 15:35:59 - INFO - root -   Epoch: [83/300][40/283], lr: 0.00000025 	 loss = 0.0984(0.4873)
2023/12/02 15:36:19 - INFO - root -   Epoch: [83/300][60/283], lr: 0.00000025 	 loss = 1.9124(0.5498)
2023/12/02 15:36:28 - INFO - root -   Epoch: [83/300][80/283], lr: 0.00000025 	 loss = 0.1128(0.5157)
2023/12/02 15:36:40 - INFO - root -   Epoch: [83/300][100/283], lr: 0.00000025 	 loss = 0.0688(0.4957)
2023/12/02 15:36:58 - INFO - root -   Epoch: [83/300][120/283], lr: 0.00000025 	 loss = 0.4060(0.5055)
2023/12/02 15:37:13 - INFO - root -   Epoch: [83/300][140/283], lr: 0.00000025 	 loss = 0.1083(0.4554)
2023/12/02 15:37:21 - INFO - root -   Epoch: [83/300][160/283], lr: 0.00000025 	 loss = 0.1455(0.4350)
2023/12/02 15:37:44 - INFO - root -   Epoch: [83/300][180/283], lr: 0.00000025 	 loss = 0.3450(0.4077)
2023/12/02 15:37:52 - INFO - root -   Epoch: [83/300][200/283], lr: 0.00000025 	 loss = 0.2137(0.3995)
2023/12/02 15:38:10 - INFO - root -   Epoch: [83/300][220/283], lr: 0.00000025 	 loss = 0.0514(0.4058)
2023/12/02 15:38:25 - INFO - root -   Epoch: [83/300][240/283], lr: 0.00000025 	 loss = 0.2748(0.4076)
2023/12/02 15:38:33 - INFO - root -   Epoch: [83/300][260/283], lr: 0.00000025 	 loss = 0.0064(0.3990)
2023/12/02 15:38:50 - INFO - root -   Epoch: [83/300][280/283], lr: 0.00000025 	 loss = 0.7081(0.3947)
2023/12/02 15:38:51 - INFO - root -   Epoch: [83/300] 	 loss = 0.3930
2023/12/02 15:38:51 - INFO - root -   train_accuracy = 0.8286
2023/12/02 15:38:53 - INFO - root -   Epoch: [84/300][0/283], lr: 0.00000025 	 loss = 0.0623(0.0623)
2023/12/02 15:39:18 - INFO - root -   Epoch: [84/300][20/283], lr: 0.00000025 	 loss = 1.6209(0.5581)
2023/12/02 15:39:41 - INFO - root -   Epoch: [84/300][40/283], lr: 0.00000025 	 loss = 0.3272(0.5207)
2023/12/02 15:39:49 - INFO - root -   Epoch: [84/300][60/283], lr: 0.00000025 	 loss = 1.8636(0.6029)
2023/12/02 15:40:04 - INFO - root -   Epoch: [84/300][80/283], lr: 0.00000025 	 loss = 0.8262(0.5614)
2023/12/02 15:40:22 - INFO - root -   Epoch: [84/300][100/283], lr: 0.00000025 	 loss = 0.1247(0.5176)
2023/12/02 15:40:30 - INFO - root -   Epoch: [84/300][120/283], lr: 0.00000025 	 loss = 0.1276(0.5085)
2023/12/02 15:40:44 - INFO - root -   Epoch: [84/300][140/283], lr: 0.00000025 	 loss = 0.1212(0.4684)
2023/12/02 15:40:56 - INFO - root -   Epoch: [84/300][160/283], lr: 0.00000025 	 loss = 0.1271(0.4324)
2023/12/02 15:41:06 - INFO - root -   Epoch: [84/300][180/283], lr: 0.00000025 	 loss = 0.2524(0.4038)
2023/12/02 15:41:21 - INFO - root -   Epoch: [84/300][200/283], lr: 0.00000025 	 loss = 0.1515(0.3988)
2023/12/02 15:41:36 - INFO - root -   Epoch: [84/300][220/283], lr: 0.00000025 	 loss = 0.2824(0.3834)
2023/12/02 15:41:49 - INFO - root -   Epoch: [84/300][240/283], lr: 0.00000025 	 loss = 0.0487(0.3763)
2023/12/02 15:42:03 - INFO - root -   Epoch: [84/300][260/283], lr: 0.00000025 	 loss = 0.1990(0.3710)
2023/12/02 15:42:11 - INFO - root -   Epoch: [84/300][280/283], lr: 0.00000025 	 loss = 0.7725(0.3626)
2023/12/02 15:42:12 - INFO - root -   Epoch: [84/300] 	 loss = 0.3601
2023/12/02 15:42:22 - INFO - root -   precision = 0.8531
2023/12/02 15:42:22 - INFO - root -   eval_loss = 0.4104
2023/12/02 15:42:22 - INFO - root -   eval_acc = 0.8531
2023/12/02 15:42:22 - INFO - root -   train_accuracy = 0.8445
2023/12/02 15:42:23 - INFO - root -   Epoch: [85/300][0/283], lr: 0.00000025 	 loss = 0.0562(0.0562)
2023/12/02 15:42:54 - INFO - root -   Epoch: [85/300][20/283], lr: 0.00000025 	 loss = 1.5904(0.5156)
2023/12/02 15:43:07 - INFO - root -   Epoch: [85/300][40/283], lr: 0.00000025 	 loss = 0.4268(0.4308)
2023/12/02 15:43:25 - INFO - root -   Epoch: [85/300][60/283], lr: 0.00000025 	 loss = 1.0610(0.4591)
2023/12/02 15:43:38 - INFO - root -   Epoch: [85/300][80/283], lr: 0.00000025 	 loss = 0.2350(0.4410)
2023/12/02 15:43:56 - INFO - root -   Epoch: [85/300][100/283], lr: 0.00000025 	 loss = 0.1036(0.4205)
2023/12/02 15:44:09 - INFO - root -   Epoch: [85/300][120/283], lr: 0.00000025 	 loss = 0.1633(0.4158)
2023/12/02 15:44:27 - INFO - root -   Epoch: [85/300][140/283], lr: 0.00000025 	 loss = 0.1417(0.3822)
2023/12/02 15:44:35 - INFO - root -   Epoch: [85/300][160/283], lr: 0.00000025 	 loss = 0.0296(0.3673)
2023/12/02 15:44:43 - INFO - root -   Epoch: [85/300][180/283], lr: 0.00000025 	 loss = 0.1352(0.3490)
2023/12/02 15:44:58 - INFO - root -   Epoch: [85/300][200/283], lr: 0.00000025 	 loss = 0.1729(0.3490)
2023/12/02 15:45:16 - INFO - root -   Epoch: [85/300][220/283], lr: 0.00000025 	 loss = 0.2026(0.3313)
2023/12/02 15:45:33 - INFO - root -   Epoch: [85/300][240/283], lr: 0.00000025 	 loss = 0.0794(0.3449)
2023/12/02 15:45:41 - INFO - root -   Epoch: [85/300][260/283], lr: 0.00000025 	 loss = 0.0412(0.3382)
2023/12/02 15:45:49 - INFO - root -   Epoch: [85/300][280/283], lr: 0.00000025 	 loss = 1.1805(0.3332)
2023/12/02 15:45:50 - INFO - root -   Epoch: [85/300] 	 loss = 0.3318
2023/12/02 15:45:50 - INFO - root -   train_accuracy = 0.8640
2023/12/02 15:46:11 - INFO - root -   Epoch: [86/300][0/283], lr: 0.00000025 	 loss = 1.0097(1.0097)
2023/12/02 15:46:21 - INFO - root -   Epoch: [86/300][20/283], lr: 0.00000025 	 loss = 0.7742(0.4690)
2023/12/02 15:46:37 - INFO - root -   Epoch: [86/300][40/283], lr: 0.00000025 	 loss = 0.3728(0.5388)
2023/12/02 15:47:06 - INFO - root -   Epoch: [86/300][60/283], lr: 0.00000025 	 loss = 0.8680(0.4821)
2023/12/02 15:47:14 - INFO - root -   Epoch: [86/300][80/283], lr: 0.00000025 	 loss = 0.2756(0.4749)
2023/12/02 15:47:34 - INFO - root -   Epoch: [86/300][100/283], lr: 0.00000025 	 loss = 0.1373(0.4213)
2023/12/02 15:47:42 - INFO - root -   Epoch: [86/300][120/283], lr: 0.00000025 	 loss = 0.6366(0.4364)
2023/12/02 15:47:59 - INFO - root -   Epoch: [86/300][140/283], lr: 0.00000025 	 loss = 0.0438(0.3972)
2023/12/02 15:48:09 - INFO - root -   Epoch: [86/300][160/283], lr: 0.00000025 	 loss = 0.0353(0.3849)
2023/12/02 15:48:26 - INFO - root -   Epoch: [86/300][180/283], lr: 0.00000025 	 loss = 0.2213(0.3673)
2023/12/02 15:48:39 - INFO - root -   Epoch: [86/300][200/283], lr: 0.00000025 	 loss = 0.0989(0.3578)
2023/12/02 15:48:55 - INFO - root -   Epoch: [86/300][220/283], lr: 0.00000025 	 loss = 0.1376(0.3473)
2023/12/02 15:49:03 - INFO - root -   Epoch: [86/300][240/283], lr: 0.00000025 	 loss = 0.2478(0.3444)
2023/12/02 15:49:19 - INFO - root -   Epoch: [86/300][260/283], lr: 0.00000025 	 loss = 0.0719(0.3394)
2023/12/02 15:49:28 - INFO - root -   Epoch: [86/300][280/283], lr: 0.00000025 	 loss = 1.8465(0.3445)
2023/12/02 15:49:29 - INFO - root -   Epoch: [86/300] 	 loss = 0.3460
2023/12/02 15:49:29 - INFO - root -   train_accuracy = 0.8587
2023/12/02 15:49:44 - INFO - root -   Epoch: [87/300][0/283], lr: 0.00000025 	 loss = 0.2315(0.2315)
2023/12/02 15:50:04 - INFO - root -   Epoch: [87/300][20/283], lr: 0.00000025 	 loss = 0.9392(0.5592)
2023/12/02 15:50:27 - INFO - root -   Epoch: [87/300][40/283], lr: 0.00000025 	 loss = 0.1229(0.5085)
2023/12/02 15:50:38 - INFO - root -   Epoch: [87/300][60/283], lr: 0.00000025 	 loss = 1.9550(0.5599)
2023/12/02 15:50:50 - INFO - root -   Epoch: [87/300][80/283], lr: 0.00000025 	 loss = 0.2633(0.5374)
2023/12/02 15:51:12 - INFO - root -   Epoch: [87/300][100/283], lr: 0.00000025 	 loss = 0.0871(0.5005)
2023/12/02 15:51:24 - INFO - root -   Epoch: [87/300][120/283], lr: 0.00000025 	 loss = 0.2537(0.5051)
2023/12/02 15:51:42 - INFO - root -   Epoch: [87/300][140/283], lr: 0.00000025 	 loss = 0.0209(0.4731)
2023/12/02 15:52:04 - INFO - root -   Epoch: [87/300][160/283], lr: 0.00000025 	 loss = 0.0848(0.4571)
2023/12/02 15:52:12 - INFO - root -   Epoch: [87/300][180/283], lr: 0.00000025 	 loss = 1.2060(0.4384)
2023/12/02 15:52:31 - INFO - root -   Epoch: [87/300][200/283], lr: 0.00000025 	 loss = 0.2820(0.4231)
2023/12/02 15:52:48 - INFO - root -   Epoch: [87/300][220/283], lr: 0.00000025 	 loss = 0.0834(0.4048)
2023/12/02 15:52:56 - INFO - root -   Epoch: [87/300][240/283], lr: 0.00000025 	 loss = 0.1427(0.4005)
2023/12/02 15:53:14 - INFO - root -   Epoch: [87/300][260/283], lr: 0.00000025 	 loss = 0.0787(0.3917)
2023/12/02 15:53:22 - INFO - root -   Epoch: [87/300][280/283], lr: 0.00000025 	 loss = 0.0875(0.3760)
2023/12/02 15:53:23 - INFO - root -   Epoch: [87/300] 	 loss = 0.3736
2023/12/02 15:53:23 - INFO - root -   train_accuracy = 0.8534
2023/12/02 15:53:44 - INFO - root -   Epoch: [88/300][0/283], lr: 0.00000025 	 loss = 0.7360(0.7360)
2023/12/02 15:53:52 - INFO - root -   Epoch: [88/300][20/283], lr: 0.00000025 	 loss = 0.5730(0.3161)
2023/12/02 15:54:12 - INFO - root -   Epoch: [88/300][40/283], lr: 0.00000025 	 loss = 0.0961(0.4358)
2023/12/02 15:54:20 - INFO - root -   Epoch: [88/300][60/283], lr: 0.00000025 	 loss = 1.7441(0.4628)
2023/12/02 15:54:33 - INFO - root -   Epoch: [88/300][80/283], lr: 0.00000025 	 loss = 0.1113(0.4265)
2023/12/02 15:55:01 - INFO - root -   Epoch: [88/300][100/283], lr: 0.00000025 	 loss = 0.1844(0.3867)
2023/12/02 15:55:09 - INFO - root -   Epoch: [88/300][120/283], lr: 0.00000025 	 loss = 0.6980(0.4132)
2023/12/02 15:55:17 - INFO - root -   Epoch: [88/300][140/283], lr: 0.00000025 	 loss = 0.0319(0.3913)
2023/12/02 15:55:29 - INFO - root -   Epoch: [88/300][160/283], lr: 0.00000025 	 loss = 0.0573(0.3649)
2023/12/02 15:55:52 - INFO - root -   Epoch: [88/300][180/283], lr: 0.00000025 	 loss = 0.0454(0.3486)
2023/12/02 15:56:08 - INFO - root -   Epoch: [88/300][200/283], lr: 0.00000025 	 loss = 0.0961(0.3435)
2023/12/02 15:56:16 - INFO - root -   Epoch: [88/300][220/283], lr: 0.00000025 	 loss = 0.0224(0.3325)
2023/12/02 15:56:41 - INFO - root -   Epoch: [88/300][240/283], lr: 0.00000025 	 loss = 0.0974(0.3398)
2023/12/02 15:56:48 - INFO - root -   Epoch: [88/300][260/283], lr: 0.00000025 	 loss = 0.0495(0.3305)
2023/12/02 15:57:01 - INFO - root -   Epoch: [88/300][280/283], lr: 0.00000025 	 loss = 0.6090(0.3269)
2023/12/02 15:57:02 - INFO - root -   Epoch: [88/300] 	 loss = 0.3259
2023/12/02 15:57:02 - INFO - root -   train_accuracy = 0.8746
2023/12/02 15:57:03 - INFO - root -   Epoch: [89/300][0/283], lr: 0.00000026 	 loss = 0.0312(0.0312)
2023/12/02 15:57:40 - INFO - root -   Epoch: [89/300][20/283], lr: 0.00000026 	 loss = 0.1973(0.4142)
2023/12/02 15:57:48 - INFO - root -   Epoch: [89/300][40/283], lr: 0.00000026 	 loss = 0.1975(0.3961)
2023/12/02 15:58:07 - INFO - root -   Epoch: [89/300][60/283], lr: 0.00000026 	 loss = 1.0819(0.4355)
2023/12/02 15:58:17 - INFO - root -   Epoch: [89/300][80/283], lr: 0.00000026 	 loss = 0.1895(0.4052)
2023/12/02 15:58:37 - INFO - root -   Epoch: [89/300][100/283], lr: 0.00000026 	 loss = 0.1898(0.3865)
2023/12/02 15:58:53 - INFO - root -   Epoch: [89/300][120/283], lr: 0.00000026 	 loss = 0.4095(0.3845)
2023/12/02 15:59:10 - INFO - root -   Epoch: [89/300][140/283], lr: 0.00000026 	 loss = 0.0157(0.3598)
2023/12/02 15:59:30 - INFO - root -   Epoch: [89/300][160/283], lr: 0.00000026 	 loss = 0.9580(0.3456)
2023/12/02 15:59:38 - INFO - root -   Epoch: [89/300][180/283], lr: 0.00000026 	 loss = 0.4639(0.3355)
2023/12/02 16:00:04 - INFO - root -   Epoch: [89/300][200/283], lr: 0.00000026 	 loss = 0.0358(0.3338)
2023/12/02 16:00:19 - INFO - root -   Epoch: [89/300][220/283], lr: 0.00000026 	 loss = 0.2275(0.3267)
2023/12/02 16:00:38 - INFO - root -   Epoch: [89/300][240/283], lr: 0.00000026 	 loss = 0.1160(0.3295)
2023/12/02 16:01:00 - INFO - root -   Epoch: [89/300][260/283], lr: 0.00000026 	 loss = 0.0433(0.3314)
2023/12/02 16:01:08 - INFO - root -   Epoch: [89/300][280/283], lr: 0.00000026 	 loss = 0.4160(0.3234)
2023/12/02 16:01:09 - INFO - root -   Epoch: [89/300] 	 loss = 0.3212
2023/12/02 16:01:19 - INFO - root -   precision = 0.8671
2023/12/02 16:01:19 - INFO - root -   eval_loss = 0.4138
2023/12/02 16:01:19 - INFO - root -   eval_acc = 0.8671
2023/12/02 16:01:19 - INFO - root -   train_accuracy = 0.8569
2023/12/02 16:01:20 - INFO - root -   Epoch: [90/300][0/283], lr: 0.00000026 	 loss = 0.0378(0.0378)
2023/12/02 16:01:54 - INFO - root -   Epoch: [90/300][20/283], lr: 0.00000026 	 loss = 1.2248(0.5801)
2023/12/02 16:02:07 - INFO - root -   Epoch: [90/300][40/283], lr: 0.00000026 	 loss = 0.1455(0.5291)
2023/12/02 16:02:26 - INFO - root -   Epoch: [90/300][60/283], lr: 0.00000026 	 loss = 1.0304(0.5141)
2023/12/02 16:02:34 - INFO - root -   Epoch: [90/300][80/283], lr: 0.00000026 	 loss = 0.2524(0.5051)
2023/12/02 16:02:58 - INFO - root -   Epoch: [90/300][100/283], lr: 0.00000026 	 loss = 0.1576(0.5073)
2023/12/02 16:03:15 - INFO - root -   Epoch: [90/300][120/283], lr: 0.00000026 	 loss = 0.1992(0.5013)
2023/12/02 16:03:26 - INFO - root -   Epoch: [90/300][140/283], lr: 0.00000026 	 loss = 0.0484(0.4562)
2023/12/02 16:03:40 - INFO - root -   Epoch: [90/300][160/283], lr: 0.00000026 	 loss = 0.0634(0.4258)
2023/12/02 16:03:57 - INFO - root -   Epoch: [90/300][180/283], lr: 0.00000026 	 loss = 0.4119(0.4110)
2023/12/02 16:04:16 - INFO - root -   Epoch: [90/300][200/283], lr: 0.00000026 	 loss = 0.1233(0.3941)
2023/12/02 16:04:23 - INFO - root -   Epoch: [90/300][220/283], lr: 0.00000026 	 loss = 0.0459(0.3797)
2023/12/02 16:04:51 - INFO - root -   Epoch: [90/300][240/283], lr: 0.00000026 	 loss = 0.3738(0.3962)
2023/12/02 16:04:59 - INFO - root -   Epoch: [90/300][260/283], lr: 0.00000026 	 loss = 0.0590(0.3932)
2023/12/02 16:05:07 - INFO - root -   Epoch: [90/300][280/283], lr: 0.00000026 	 loss = 0.6336(0.3837)
2023/12/02 16:05:08 - INFO - root -   Epoch: [90/300] 	 loss = 0.3814
2023/12/02 16:05:08 - INFO - root -   train_accuracy = 0.8445
2023/12/02 16:05:09 - INFO - root -   Epoch: [91/300][0/283], lr: 0.00000026 	 loss = 0.0248(0.0248)
2023/12/02 16:05:34 - INFO - root -   Epoch: [91/300][20/283], lr: 0.00000026 	 loss = 0.9034(0.3990)
2023/12/02 16:05:47 - INFO - root -   Epoch: [91/300][40/283], lr: 0.00000026 	 loss = 0.0960(0.3976)
2023/12/02 16:06:04 - INFO - root -   Epoch: [91/300][60/283], lr: 0.00000026 	 loss = 0.7540(0.3858)
2023/12/02 16:06:15 - INFO - root -   Epoch: [91/300][80/283], lr: 0.00000026 	 loss = 0.6295(0.3858)
2023/12/02 16:06:28 - INFO - root -   Epoch: [91/300][100/283], lr: 0.00000026 	 loss = 0.0698(0.3853)
2023/12/02 16:06:53 - INFO - root -   Epoch: [91/300][120/283], lr: 0.00000026 	 loss = 0.2122(0.4182)
2023/12/02 16:07:01 - INFO - root -   Epoch: [91/300][140/283], lr: 0.00000026 	 loss = 0.0102(0.3861)
2023/12/02 16:07:09 - INFO - root -   Epoch: [91/300][160/283], lr: 0.00000026 	 loss = 0.1034(0.3693)
2023/12/02 16:07:29 - INFO - root -   Epoch: [91/300][180/283], lr: 0.00000026 	 loss = 0.6661(0.3630)
2023/12/02 16:07:42 - INFO - root -   Epoch: [91/300][200/283], lr: 0.00000026 	 loss = 0.1436(0.3444)
2023/12/02 16:08:01 - INFO - root -   Epoch: [91/300][220/283], lr: 0.00000026 	 loss = 0.0344(0.3328)
2023/12/02 16:08:10 - INFO - root -   Epoch: [91/300][240/283], lr: 0.00000026 	 loss = 0.0371(0.3452)
2023/12/02 16:08:25 - INFO - root -   Epoch: [91/300][260/283], lr: 0.00000026 	 loss = 0.0892(0.3420)
2023/12/02 16:08:33 - INFO - root -   Epoch: [91/300][280/283], lr: 0.00000026 	 loss = 0.2932(0.3389)
2023/12/02 16:08:34 - INFO - root -   Epoch: [91/300] 	 loss = 0.3366
2023/12/02 16:08:34 - INFO - root -   train_accuracy = 0.8622
2023/12/02 16:08:37 - INFO - root -   Epoch: [92/300][0/283], lr: 0.00000026 	 loss = 0.1163(0.1163)
2023/12/02 16:09:05 - INFO - root -   Epoch: [92/300][20/283], lr: 0.00000026 	 loss = 0.6967(0.3054)
2023/12/02 16:09:24 - INFO - root -   Epoch: [92/300][40/283], lr: 0.00000026 	 loss = 0.5252(0.4438)
2023/12/02 16:09:32 - INFO - root -   Epoch: [92/300][60/283], lr: 0.00000026 	 loss = 2.3274(0.4502)
2023/12/02 16:10:02 - INFO - root -   Epoch: [92/300][80/283], lr: 0.00000026 	 loss = 0.6027(0.4714)
2023/12/02 16:10:15 - INFO - root -   Epoch: [92/300][100/283], lr: 0.00000026 	 loss = 0.2520(0.4842)
2023/12/02 16:10:23 - INFO - root -   Epoch: [92/300][120/283], lr: 0.00000026 	 loss = 0.0754(0.4708)
2023/12/02 16:10:41 - INFO - root -   Epoch: [92/300][140/283], lr: 0.00000026 	 loss = 0.1683(0.4291)
2023/12/02 16:10:57 - INFO - root -   Epoch: [92/300][160/283], lr: 0.00000026 	 loss = 0.7075(0.4071)
2023/12/02 16:11:16 - INFO - root -   Epoch: [92/300][180/283], lr: 0.00000026 	 loss = 0.5469(0.3924)
2023/12/02 16:11:40 - INFO - root -   Epoch: [92/300][200/283], lr: 0.00000026 	 loss = 0.5372(0.3833)
2023/12/02 16:11:53 - INFO - root -   Epoch: [92/300][220/283], lr: 0.00000026 	 loss = 0.0477(0.3769)
2023/12/02 16:12:01 - INFO - root -   Epoch: [92/300][240/283], lr: 0.00000026 	 loss = 0.1116(0.3795)
2023/12/02 16:12:17 - INFO - root -   Epoch: [92/300][260/283], lr: 0.00000026 	 loss = 0.0175(0.3752)
2023/12/02 16:12:26 - INFO - root -   Epoch: [92/300][280/283], lr: 0.00000026 	 loss = 0.7906(0.3666)
2023/12/02 16:12:27 - INFO - root -   Epoch: [92/300] 	 loss = 0.3646
2023/12/02 16:12:27 - INFO - root -   train_accuracy = 0.8357
2023/12/02 16:12:39 - INFO - root -   Epoch: [93/300][0/283], lr: 0.00000026 	 loss = 0.3608(0.3608)
2023/12/02 16:12:56 - INFO - root -   Epoch: [93/300][20/283], lr: 0.00000026 	 loss = 1.2633(0.3985)
2023/12/02 16:13:03 - INFO - root -   Epoch: [93/300][40/283], lr: 0.00000026 	 loss = 0.2282(0.3492)
2023/12/02 16:13:25 - INFO - root -   Epoch: [93/300][60/283], lr: 0.00000026 	 loss = 1.4644(0.4201)
2023/12/02 16:13:39 - INFO - root -   Epoch: [93/300][80/283], lr: 0.00000026 	 loss = 0.3470(0.4395)
2023/12/02 16:13:47 - INFO - root -   Epoch: [93/300][100/283], lr: 0.00000026 	 loss = 0.0565(0.4192)
2023/12/02 16:14:13 - INFO - root -   Epoch: [93/300][120/283], lr: 0.00000026 	 loss = 0.8243(0.4241)
2023/12/02 16:14:31 - INFO - root -   Epoch: [93/300][140/283], lr: 0.00000026 	 loss = 0.0122(0.4070)
2023/12/02 16:14:43 - INFO - root -   Epoch: [93/300][160/283], lr: 0.00000026 	 loss = 0.0091(0.3842)
2023/12/02 16:15:01 - INFO - root -   Epoch: [93/300][180/283], lr: 0.00000026 	 loss = 0.7605(0.3726)
2023/12/02 16:15:19 - INFO - root -   Epoch: [93/300][200/283], lr: 0.00000026 	 loss = 0.2145(0.3675)
2023/12/02 16:15:33 - INFO - root -   Epoch: [93/300][220/283], lr: 0.00000026 	 loss = 0.0141(0.3582)
2023/12/02 16:15:49 - INFO - root -   Epoch: [93/300][240/283], lr: 0.00000026 	 loss = 0.1705(0.3597)
2023/12/02 16:15:57 - INFO - root -   Epoch: [93/300][260/283], lr: 0.00000026 	 loss = 0.6605(0.3584)
2023/12/02 16:16:05 - INFO - root -   Epoch: [93/300][280/283], lr: 0.00000026 	 loss = 0.5407(0.3564)
2023/12/02 16:16:06 - INFO - root -   Epoch: [93/300] 	 loss = 0.3542
2023/12/02 16:16:06 - INFO - root -   train_accuracy = 0.8551
2023/12/02 16:16:20 - INFO - root -   Epoch: [94/300][0/283], lr: 0.00000026 	 loss = 0.8569(0.8569)
2023/12/02 16:16:35 - INFO - root -   Epoch: [94/300][20/283], lr: 0.00000026 	 loss = 0.2873(0.4037)
2023/12/02 16:16:46 - INFO - root -   Epoch: [94/300][40/283], lr: 0.00000026 	 loss = 0.2874(0.3996)
2023/12/02 16:17:06 - INFO - root -   Epoch: [94/300][60/283], lr: 0.00000026 	 loss = 1.2884(0.4032)
2023/12/02 16:17:24 - INFO - root -   Epoch: [94/300][80/283], lr: 0.00000026 	 loss = 0.5381(0.4397)
2023/12/02 16:17:32 - INFO - root -   Epoch: [94/300][100/283], lr: 0.00000026 	 loss = 0.1771(0.4343)
2023/12/02 16:17:53 - INFO - root -   Epoch: [94/300][120/283], lr: 0.00000026 	 loss = 0.0223(0.4623)
2023/12/02 16:18:01 - INFO - root -   Epoch: [94/300][140/283], lr: 0.00000026 	 loss = 0.1390(0.4348)
2023/12/02 16:18:16 - INFO - root -   Epoch: [94/300][160/283], lr: 0.00000026 	 loss = 0.0868(0.4180)
2023/12/02 16:18:28 - INFO - root -   Epoch: [94/300][180/283], lr: 0.00000026 	 loss = 1.0353(0.4063)
2023/12/02 16:18:43 - INFO - root -   Epoch: [94/300][200/283], lr: 0.00000026 	 loss = 0.6651(0.3927)
2023/12/02 16:18:56 - INFO - root -   Epoch: [94/300][220/283], lr: 0.00000026 	 loss = 0.5369(0.3877)
2023/12/02 16:19:15 - INFO - root -   Epoch: [94/300][240/283], lr: 0.00000026 	 loss = 0.1212(0.3805)
2023/12/02 16:19:22 - INFO - root -   Epoch: [94/300][260/283], lr: 0.00000026 	 loss = 0.0156(0.3752)
2023/12/02 16:19:30 - INFO - root -   Epoch: [94/300][280/283], lr: 0.00000026 	 loss = 1.4586(0.3690)
2023/12/02 16:19:31 - INFO - root -   Epoch: [94/300] 	 loss = 0.3666
2023/12/02 16:19:41 - INFO - root -   precision = 0.8462
2023/12/02 16:19:41 - INFO - root -   eval_loss = 0.4061
2023/12/02 16:19:41 - INFO - root -   eval_acc = 0.8462
2023/12/02 16:19:41 - INFO - root -   train_accuracy = 0.8569
2023/12/02 16:19:44 - INFO - root -   Epoch: [95/300][0/283], lr: 0.00000027 	 loss = 0.1540(0.1540)
2023/12/02 16:20:14 - INFO - root -   Epoch: [95/300][20/283], lr: 0.00000027 	 loss = 0.8504(0.4842)
2023/12/02 16:20:31 - INFO - root -   Epoch: [95/300][40/283], lr: 0.00000027 	 loss = 0.4893(0.4421)
2023/12/02 16:20:43 - INFO - root -   Epoch: [95/300][60/283], lr: 0.00000027 	 loss = 1.1589(0.4941)
2023/12/02 16:21:04 - INFO - root -   Epoch: [95/300][80/283], lr: 0.00000027 	 loss = 0.5512(0.4638)
2023/12/02 16:21:12 - INFO - root -   Epoch: [95/300][100/283], lr: 0.00000027 	 loss = 0.0658(0.4518)
2023/12/02 16:21:35 - INFO - root -   Epoch: [95/300][120/283], lr: 0.00000027 	 loss = 0.7109(0.4659)
2023/12/02 16:21:59 - INFO - root -   Epoch: [95/300][140/283], lr: 0.00000027 	 loss = 0.0581(0.4349)
2023/12/02 16:22:07 - INFO - root -   Epoch: [95/300][160/283], lr: 0.00000027 	 loss = 0.0909(0.4206)
2023/12/02 16:22:23 - INFO - root -   Epoch: [95/300][180/283], lr: 0.00000027 	 loss = 0.1253(0.3947)
2023/12/02 16:22:44 - INFO - root -   Epoch: [95/300][200/283], lr: 0.00000027 	 loss = 0.0751(0.3879)
2023/12/02 16:22:52 - INFO - root -   Epoch: [95/300][220/283], lr: 0.00000027 	 loss = 0.0736(0.3735)
2023/12/02 16:23:06 - INFO - root -   Epoch: [95/300][240/283], lr: 0.00000027 	 loss = 0.2005(0.3750)
2023/12/02 16:23:20 - INFO - root -   Epoch: [95/300][260/283], lr: 0.00000027 	 loss = 0.0867(0.3643)
2023/12/02 16:23:28 - INFO - root -   Epoch: [95/300][280/283], lr: 0.00000027 	 loss = 0.6554(0.3621)
2023/12/02 16:23:29 - INFO - root -   Epoch: [95/300] 	 loss = 0.3596
2023/12/02 16:23:29 - INFO - root -   train_accuracy = 0.8481
2023/12/02 16:23:30 - INFO - root -   Epoch: [96/300][0/283], lr: 0.00000027 	 loss = 0.3081(0.3081)
2023/12/02 16:24:03 - INFO - root -   Epoch: [96/300][20/283], lr: 0.00000027 	 loss = 2.1035(0.5288)
2023/12/02 16:24:11 - INFO - root -   Epoch: [96/300][40/283], lr: 0.00000027 	 loss = 0.3512(0.5054)
2023/12/02 16:24:36 - INFO - root -   Epoch: [96/300][60/283], lr: 0.00000027 	 loss = 0.7406(0.4903)
2023/12/02 16:24:44 - INFO - root -   Epoch: [96/300][80/283], lr: 0.00000027 	 loss = 0.2180(0.4403)
2023/12/02 16:24:55 - INFO - root -   Epoch: [96/300][100/283], lr: 0.00000027 	 loss = 0.3173(0.4199)
2023/12/02 16:25:14 - INFO - root -   Epoch: [96/300][120/283], lr: 0.00000027 	 loss = 0.1940(0.4243)
2023/12/02 16:25:22 - INFO - root -   Epoch: [96/300][140/283], lr: 0.00000027 	 loss = 0.0139(0.3861)
2023/12/02 16:25:43 - INFO - root -   Epoch: [96/300][160/283], lr: 0.00000027 	 loss = 0.7726(0.3666)
2023/12/02 16:25:51 - INFO - root -   Epoch: [96/300][180/283], lr: 0.00000027 	 loss = 0.7795(0.3490)
2023/12/02 16:26:09 - INFO - root -   Epoch: [96/300][200/283], lr: 0.00000027 	 loss = 0.5004(0.3458)
2023/12/02 16:26:21 - INFO - root -   Epoch: [96/300][220/283], lr: 0.00000027 	 loss = 0.2245(0.3402)
2023/12/02 16:26:38 - INFO - root -   Epoch: [96/300][240/283], lr: 0.00000027 	 loss = 0.0200(0.3341)
2023/12/02 16:26:46 - INFO - root -   Epoch: [96/300][260/283], lr: 0.00000027 	 loss = 0.4026(0.3309)
2023/12/02 16:26:56 - INFO - root -   Epoch: [96/300][280/283], lr: 0.00000027 	 loss = 0.8673(0.3325)
2023/12/02 16:26:57 - INFO - root -   Epoch: [96/300] 	 loss = 0.3304
2023/12/02 16:26:57 - INFO - root -   train_accuracy = 0.8640
2023/12/02 16:27:13 - INFO - root -   Epoch: [97/300][0/283], lr: 0.00000027 	 loss = 0.1924(0.1924)
2023/12/02 16:27:40 - INFO - root -   Epoch: [97/300][20/283], lr: 0.00000027 	 loss = 0.8232(0.5121)
2023/12/02 16:27:48 - INFO - root -   Epoch: [97/300][40/283], lr: 0.00000027 	 loss = 0.2718(0.4360)
2023/12/02 16:28:16 - INFO - root -   Epoch: [97/300][60/283], lr: 0.00000027 	 loss = 1.6710(0.4990)
2023/12/02 16:28:24 - INFO - root -   Epoch: [97/300][80/283], lr: 0.00000027 	 loss = 0.0671(0.5001)
2023/12/02 16:28:51 - INFO - root -   Epoch: [97/300][100/283], lr: 0.00000027 	 loss = 0.4149(0.4759)
2023/12/02 16:28:59 - INFO - root -   Epoch: [97/300][120/283], lr: 0.00000027 	 loss = 0.5621(0.4906)
2023/12/02 16:29:06 - INFO - root -   Epoch: [97/300][140/283], lr: 0.00000027 	 loss = 0.0393(0.4543)
2023/12/02 16:29:15 - INFO - root -   Epoch: [97/300][160/283], lr: 0.00000027 	 loss = 0.2511(0.4152)
2023/12/02 16:29:42 - INFO - root -   Epoch: [97/300][180/283], lr: 0.00000027 	 loss = 0.3674(0.3969)
2023/12/02 16:29:50 - INFO - root -   Epoch: [97/300][200/283], lr: 0.00000027 	 loss = 0.9025(0.3881)
2023/12/02 16:30:15 - INFO - root -   Epoch: [97/300][220/283], lr: 0.00000027 	 loss = 0.0240(0.3748)
2023/12/02 16:30:23 - INFO - root -   Epoch: [97/300][240/283], lr: 0.00000027 	 loss = 0.3374(0.3803)
2023/12/02 16:30:43 - INFO - root -   Epoch: [97/300][260/283], lr: 0.00000027 	 loss = 0.0922(0.3701)
2023/12/02 16:30:51 - INFO - root -   Epoch: [97/300][280/283], lr: 0.00000027 	 loss = 0.3915(0.3580)
2023/12/02 16:30:52 - INFO - root -   Epoch: [97/300] 	 loss = 0.3562
2023/12/02 16:30:52 - INFO - root -   train_accuracy = 0.8569
2023/12/02 16:30:54 - INFO - root -   Epoch: [98/300][0/283], lr: 0.00000027 	 loss = 0.0131(0.0131)
2023/12/02 16:31:20 - INFO - root -   Epoch: [98/300][20/283], lr: 0.00000027 	 loss = 0.5439(0.5225)
2023/12/02 16:31:31 - INFO - root -   Epoch: [98/300][40/283], lr: 0.00000027 	 loss = 0.0869(0.4675)
2023/12/02 16:31:40 - INFO - root -   Epoch: [98/300][60/283], lr: 0.00000027 	 loss = 0.7138(0.4398)
2023/12/02 16:32:01 - INFO - root -   Epoch: [98/300][80/283], lr: 0.00000027 	 loss = 0.6001(0.4363)
2023/12/02 16:32:09 - INFO - root -   Epoch: [98/300][100/283], lr: 0.00000027 	 loss = 0.1861(0.4192)
2023/12/02 16:32:25 - INFO - root -   Epoch: [98/300][120/283], lr: 0.00000027 	 loss = 0.3043(0.4160)
2023/12/02 16:32:33 - INFO - root -   Epoch: [98/300][140/283], lr: 0.00000027 	 loss = 0.0253(0.3849)
2023/12/02 16:32:46 - INFO - root -   Epoch: [98/300][160/283], lr: 0.00000027 	 loss = 0.1305(0.3517)
2023/12/02 16:33:02 - INFO - root -   Epoch: [98/300][180/283], lr: 0.00000027 	 loss = 0.0705(0.3349)
2023/12/02 16:33:11 - INFO - root -   Epoch: [98/300][200/283], lr: 0.00000027 	 loss = 0.2905(0.3270)
2023/12/02 16:33:28 - INFO - root -   Epoch: [98/300][220/283], lr: 0.00000027 	 loss = 0.1833(0.3222)
2023/12/02 16:33:40 - INFO - root -   Epoch: [98/300][240/283], lr: 0.00000027 	 loss = 0.1343(0.3339)
2023/12/02 16:33:59 - INFO - root -   Epoch: [98/300][260/283], lr: 0.00000027 	 loss = 0.1415(0.3232)
2023/12/02 16:34:09 - INFO - root -   Epoch: [98/300][280/283], lr: 0.00000027 	 loss = 0.2921(0.3132)
2023/12/02 16:34:10 - INFO - root -   Epoch: [98/300] 	 loss = 0.3115
2023/12/02 16:34:10 - INFO - root -   train_accuracy = 0.8869
2023/12/02 16:34:13 - INFO - root -   Epoch: [99/300][0/283], lr: 0.00000027 	 loss = 0.0203(0.0203)
2023/12/02 16:34:30 - INFO - root -   Epoch: [99/300][20/283], lr: 0.00000027 	 loss = 1.3902(0.3577)
2023/12/02 16:34:46 - INFO - root -   Epoch: [99/300][40/283], lr: 0.00000027 	 loss = 0.5333(0.4477)
2023/12/02 16:34:59 - INFO - root -   Epoch: [99/300][60/283], lr: 0.00000027 	 loss = 1.6874(0.5010)
2023/12/02 16:35:15 - INFO - root -   Epoch: [99/300][80/283], lr: 0.00000027 	 loss = 1.0685(0.4943)
2023/12/02 16:35:23 - INFO - root -   Epoch: [99/300][100/283], lr: 0.00000027 	 loss = 0.0704(0.4523)
2023/12/02 16:35:46 - INFO - root -   Epoch: [99/300][120/283], lr: 0.00000027 	 loss = 0.1554(0.4627)
2023/12/02 16:35:54 - INFO - root -   Epoch: [99/300][140/283], lr: 0.00000027 	 loss = 0.0788(0.4305)
2023/12/02 16:36:17 - INFO - root -   Epoch: [99/300][160/283], lr: 0.00000027 	 loss = 0.1626(0.4144)
2023/12/02 16:36:33 - INFO - root -   Epoch: [99/300][180/283], lr: 0.00000027 	 loss = 0.3115(0.3880)
2023/12/02 16:36:47 - INFO - root -   Epoch: [99/300][200/283], lr: 0.00000027 	 loss = 1.0353(0.3849)
2023/12/02 16:36:55 - INFO - root -   Epoch: [99/300][220/283], lr: 0.00000027 	 loss = 0.0302(0.3711)
2023/12/02 16:37:17 - INFO - root -   Epoch: [99/300][240/283], lr: 0.00000027 	 loss = 0.2248(0.3676)
2023/12/02 16:37:38 - INFO - root -   Epoch: [99/300][260/283], lr: 0.00000027 	 loss = 0.0231(0.3647)
2023/12/02 16:37:46 - INFO - root -   Epoch: [99/300][280/283], lr: 0.00000027 	 loss = 1.0572(0.3576)
2023/12/02 16:37:47 - INFO - root -   Epoch: [99/300] 	 loss = 0.3553
2023/12/02 16:37:56 - INFO - root -   precision = 0.8462
2023/12/02 16:37:56 - INFO - root -   eval_loss = 0.4101
2023/12/02 16:37:56 - INFO - root -   eval_acc = 0.8462
2023/12/02 16:37:57 - INFO - root -   train_accuracy = 0.8498
2023/12/02 16:38:25 - INFO - root -   Epoch: [100/300][0/283], lr: 0.00000027 	 loss = 0.8686(0.8686)
2023/12/02 16:38:47 - INFO - root -   Epoch: [100/300][20/283], lr: 0.00000027 	 loss = 0.3969(0.4178)
2023/12/02 16:38:59 - INFO - root -   Epoch: [100/300][40/283], lr: 0.00000027 	 loss = 0.4560(0.4636)
2023/12/02 16:39:09 - INFO - root -   Epoch: [100/300][60/283], lr: 0.00000027 	 loss = 1.4365(0.4708)
2023/12/02 16:39:27 - INFO - root -   Epoch: [100/300][80/283], lr: 0.00000027 	 loss = 0.5621(0.4593)
2023/12/02 16:39:42 - INFO - root -   Epoch: [100/300][100/283], lr: 0.00000027 	 loss = 0.1655(0.4467)
2023/12/02 16:39:58 - INFO - root -   Epoch: [100/300][120/283], lr: 0.00000027 	 loss = 0.2775(0.4395)
2023/12/02 16:40:17 - INFO - root -   Epoch: [100/300][140/283], lr: 0.00000027 	 loss = 0.0156(0.4005)
2023/12/02 16:40:34 - INFO - root -   Epoch: [100/300][160/283], lr: 0.00000027 	 loss = 0.1091(0.3828)
2023/12/02 16:40:46 - INFO - root -   Epoch: [100/300][180/283], lr: 0.00000027 	 loss = 0.0414(0.3778)
2023/12/02 16:41:00 - INFO - root -   Epoch: [100/300][200/283], lr: 0.00000027 	 loss = 0.1808(0.3662)
2023/12/02 16:41:28 - INFO - root -   Epoch: [100/300][220/283], lr: 0.00000027 	 loss = 0.0941(0.3555)
2023/12/02 16:41:48 - INFO - root -   Epoch: [100/300][240/283], lr: 0.00000027 	 loss = 0.3756(0.3678)
2023/12/02 16:42:06 - INFO - root -   Epoch: [100/300][260/283], lr: 0.00000027 	 loss = 0.0556(0.3668)
2023/12/02 16:42:14 - INFO - root -   Epoch: [100/300][280/283], lr: 0.00000027 	 loss = 0.9493(0.3598)
2023/12/02 16:42:15 - INFO - root -   Epoch: [100/300] 	 loss = 0.3580
2023/12/02 16:42:15 - INFO - root -   train_accuracy = 0.8640
2023/12/02 16:42:17 - INFO - root -   Epoch: [101/300][0/283], lr: 0.00000028 	 loss = 0.0176(0.0176)
2023/12/02 16:42:53 - INFO - root -   Epoch: [101/300][20/283], lr: 0.00000028 	 loss = 1.3576(0.3895)
2023/12/02 16:43:02 - INFO - root -   Epoch: [101/300][40/283], lr: 0.00000028 	 loss = 0.0981(0.4027)
2023/12/02 16:43:13 - INFO - root -   Epoch: [101/300][60/283], lr: 0.00000028 	 loss = 0.8753(0.3912)
2023/12/02 16:43:35 - INFO - root -   Epoch: [101/300][80/283], lr: 0.00000028 	 loss = 0.0426(0.3888)
2023/12/02 16:43:48 - INFO - root -   Epoch: [101/300][100/283], lr: 0.00000028 	 loss = 0.2845(0.3879)
2023/12/02 16:44:07 - INFO - root -   Epoch: [101/300][120/283], lr: 0.00000028 	 loss = 0.3918(0.3833)
2023/12/02 16:44:15 - INFO - root -   Epoch: [101/300][140/283], lr: 0.00000028 	 loss = 0.0385(0.3509)
2023/12/02 16:44:32 - INFO - root -   Epoch: [101/300][160/283], lr: 0.00000028 	 loss = 0.0333(0.3441)
2023/12/02 16:44:42 - INFO - root -   Epoch: [101/300][180/283], lr: 0.00000028 	 loss = 0.1914(0.3278)
2023/12/02 16:44:55 - INFO - root -   Epoch: [101/300][200/283], lr: 0.00000028 	 loss = 0.0964(0.3221)
2023/12/02 16:45:06 - INFO - root -   Epoch: [101/300][220/283], lr: 0.00000028 	 loss = 0.0100(0.3136)
2023/12/02 16:45:21 - INFO - root -   Epoch: [101/300][240/283], lr: 0.00000028 	 loss = 0.6502(0.3201)
2023/12/02 16:45:34 - INFO - root -   Epoch: [101/300][260/283], lr: 0.00000028 	 loss = 0.1213(0.3160)
2023/12/02 16:45:42 - INFO - root -   Epoch: [101/300][280/283], lr: 0.00000028 	 loss = 0.2699(0.3216)
2023/12/02 16:45:43 - INFO - root -   Epoch: [101/300] 	 loss = 0.3200
2023/12/02 16:45:43 - INFO - root -   train_accuracy = 0.8640
2023/12/02 16:45:58 - INFO - root -   Epoch: [102/300][0/283], lr: 0.00000028 	 loss = 0.0861(0.0861)
2023/12/02 16:46:20 - INFO - root -   Epoch: [102/300][20/283], lr: 0.00000028 	 loss = 0.3143(0.4394)
2023/12/02 16:46:28 - INFO - root -   Epoch: [102/300][40/283], lr: 0.00000028 	 loss = 0.1193(0.4055)
2023/12/02 16:46:59 - INFO - root -   Epoch: [102/300][60/283], lr: 0.00000028 	 loss = 0.7626(0.4048)
2023/12/02 16:47:07 - INFO - root -   Epoch: [102/300][80/283], lr: 0.00000028 	 loss = 0.0399(0.4344)
2023/12/02 16:47:16 - INFO - root -   Epoch: [102/300][100/283], lr: 0.00000028 	 loss = 0.0832(0.4075)
2023/12/02 16:47:40 - INFO - root -   Epoch: [102/300][120/283], lr: 0.00000028 	 loss = 0.5894(0.4105)
2023/12/02 16:47:54 - INFO - root -   Epoch: [102/300][140/283], lr: 0.00000028 	 loss = 0.0156(0.3857)
2023/12/02 16:48:13 - INFO - root -   Epoch: [102/300][160/283], lr: 0.00000028 	 loss = 0.1021(0.3647)
2023/12/02 16:48:25 - INFO - root -   Epoch: [102/300][180/283], lr: 0.00000028 	 loss = 0.0386(0.3520)
2023/12/02 16:48:43 - INFO - root -   Epoch: [102/300][200/283], lr: 0.00000028 	 loss = 0.9400(0.3475)
2023/12/02 16:48:54 - INFO - root -   Epoch: [102/300][220/283], lr: 0.00000028 	 loss = 0.0117(0.3395)
2023/12/02 16:49:03 - INFO - root -   Epoch: [102/300][240/283], lr: 0.00000028 	 loss = 0.5367(0.3394)
2023/12/02 16:49:15 - INFO - root -   Epoch: [102/300][260/283], lr: 0.00000028 	 loss = 0.0180(0.3340)
2023/12/02 16:49:23 - INFO - root -   Epoch: [102/300][280/283], lr: 0.00000028 	 loss = 1.1001(0.3351)
2023/12/02 16:49:24 - INFO - root -   Epoch: [102/300] 	 loss = 0.3334
2023/12/02 16:49:24 - INFO - root -   train_accuracy = 0.8587
2023/12/02 16:49:25 - INFO - root -   Epoch: [103/300][0/283], lr: 0.00000028 	 loss = 0.1203(0.1203)
2023/12/02 16:49:51 - INFO - root -   Epoch: [103/300][20/283], lr: 0.00000028 	 loss = 0.2644(0.4994)
2023/12/02 16:50:07 - INFO - root -   Epoch: [103/300][40/283], lr: 0.00000028 	 loss = 0.3790(0.4683)
2023/12/02 16:50:15 - INFO - root -   Epoch: [103/300][60/283], lr: 0.00000028 	 loss = 0.4088(0.4518)
2023/12/02 16:50:32 - INFO - root -   Epoch: [103/300][80/283], lr: 0.00000028 	 loss = 1.0159(0.4683)
2023/12/02 16:50:54 - INFO - root -   Epoch: [103/300][100/283], lr: 0.00000028 	 loss = 0.4674(0.4302)
2023/12/02 16:51:02 - INFO - root -   Epoch: [103/300][120/283], lr: 0.00000028 	 loss = 0.6853(0.4428)
2023/12/02 16:51:13 - INFO - root -   Epoch: [103/300][140/283], lr: 0.00000028 	 loss = 0.0567(0.4112)
2023/12/02 16:51:23 - INFO - root -   Epoch: [103/300][160/283], lr: 0.00000028 	 loss = 0.0768(0.3941)
2023/12/02 16:51:36 - INFO - root -   Epoch: [103/300][180/283], lr: 0.00000028 	 loss = 0.0537(0.3728)
2023/12/02 16:51:55 - INFO - root -   Epoch: [103/300][200/283], lr: 0.00000028 	 loss = 0.9462(0.3692)
2023/12/02 16:52:15 - INFO - root -   Epoch: [103/300][220/283], lr: 0.00000028 	 loss = 0.0549(0.3710)
2023/12/02 16:52:35 - INFO - root -   Epoch: [103/300][240/283], lr: 0.00000028 	 loss = 0.1917(0.3662)
2023/12/02 16:52:46 - INFO - root -   Epoch: [103/300][260/283], lr: 0.00000028 	 loss = 0.2828(0.3579)
2023/12/02 16:53:01 - INFO - root -   Epoch: [103/300][280/283], lr: 0.00000028 	 loss = 0.4684(0.3456)
2023/12/02 16:53:02 - INFO - root -   Epoch: [103/300] 	 loss = 0.3435
2023/12/02 16:53:02 - INFO - root -   train_accuracy = 0.8587
2023/12/02 16:53:16 - INFO - root -   Epoch: [104/300][0/283], lr: 0.00000028 	 loss = 0.7010(0.7010)
2023/12/02 16:53:31 - INFO - root -   Epoch: [104/300][20/283], lr: 0.00000028 	 loss = 0.7707(0.4593)
2023/12/02 16:53:39 - INFO - root -   Epoch: [104/300][40/283], lr: 0.00000028 	 loss = 0.2050(0.4145)
2023/12/02 16:54:02 - INFO - root -   Epoch: [104/300][60/283], lr: 0.00000028 	 loss = 2.2629(0.4975)
2023/12/02 16:54:18 - INFO - root -   Epoch: [104/300][80/283], lr: 0.00000028 	 loss = 0.1548(0.4666)
2023/12/02 16:54:26 - INFO - root -   Epoch: [104/300][100/283], lr: 0.00000028 	 loss = 0.1562(0.4438)
2023/12/02 16:54:43 - INFO - root -   Epoch: [104/300][120/283], lr: 0.00000028 	 loss = 0.2340(0.4666)
2023/12/02 16:54:56 - INFO - root -   Epoch: [104/300][140/283], lr: 0.00000028 	 loss = 0.0112(0.4276)
2023/12/02 16:55:20 - INFO - root -   Epoch: [104/300][160/283], lr: 0.00000028 	 loss = 0.2606(0.4010)
2023/12/02 16:55:43 - INFO - root -   Epoch: [104/300][180/283], lr: 0.00000028 	 loss = 1.1996(0.3955)
2023/12/02 16:55:52 - INFO - root -   Epoch: [104/300][200/283], lr: 0.00000028 	 loss = 0.1703(0.3776)
2023/12/02 16:56:15 - INFO - root -   Epoch: [104/300][220/283], lr: 0.00000028 	 loss = 0.5370(0.3820)
2023/12/02 16:56:23 - INFO - root -   Epoch: [104/300][240/283], lr: 0.00000028 	 loss = 0.1815(0.3907)
2023/12/02 16:56:39 - INFO - root -   Epoch: [104/300][260/283], lr: 0.00000028 	 loss = 0.1443(0.3765)
2023/12/02 16:56:47 - INFO - root -   Epoch: [104/300][280/283], lr: 0.00000028 	 loss = 0.0882(0.3694)
2023/12/02 16:56:48 - INFO - root -   Epoch: [104/300] 	 loss = 0.3670
2023/12/02 16:56:57 - INFO - root -   precision = 0.8531
2023/12/02 16:56:57 - INFO - root -   eval_loss = 0.3917
2023/12/02 16:56:57 - INFO - root -   eval_acc = 0.8531
2023/12/02 16:56:58 - INFO - root -   train_accuracy = 0.8375
2023/12/02 16:56:59 - INFO - root -   Epoch: [105/300][0/283], lr: 0.00000028 	 loss = 0.2728(0.2728)
2023/12/02 16:57:27 - INFO - root -   Epoch: [105/300][20/283], lr: 0.00000028 	 loss = 0.4995(0.3137)
2023/12/02 16:57:40 - INFO - root -   Epoch: [105/300][40/283], lr: 0.00000028 	 loss = 0.1784(0.4812)
2023/12/02 16:57:50 - INFO - root -   Epoch: [105/300][60/283], lr: 0.00000028 	 loss = 0.5119(0.4491)
2023/12/02 16:58:10 - INFO - root -   Epoch: [105/300][80/283], lr: 0.00000028 	 loss = 0.0616(0.4360)
2023/12/02 16:58:21 - INFO - root -   Epoch: [105/300][100/283], lr: 0.00000028 	 loss = 0.0569(0.4190)
2023/12/02 16:58:34 - INFO - root -   Epoch: [105/300][120/283], lr: 0.00000028 	 loss = 1.1128(0.4248)
2023/12/02 16:58:52 - INFO - root -   Epoch: [105/300][140/283], lr: 0.00000028 	 loss = 0.1242(0.3859)
2023/12/02 16:59:03 - INFO - root -   Epoch: [105/300][160/283], lr: 0.00000028 	 loss = 1.1694(0.3739)
2023/12/02 16:59:15 - INFO - root -   Epoch: [105/300][180/283], lr: 0.00000028 	 loss = 0.0926(0.3580)
2023/12/02 16:59:29 - INFO - root -   Epoch: [105/300][200/283], lr: 0.00000028 	 loss = 0.2148(0.3439)
2023/12/02 16:59:47 - INFO - root -   Epoch: [105/300][220/283], lr: 0.00000028 	 loss = 0.1377(0.3419)
2023/12/02 17:00:12 - INFO - root -   Epoch: [105/300][240/283], lr: 0.00000028 	 loss = 0.0478(0.3605)
2023/12/02 17:00:32 - INFO - root -   Epoch: [105/300][260/283], lr: 0.00000028 	 loss = 0.1569(0.3537)
2023/12/02 17:00:40 - INFO - root -   Epoch: [105/300][280/283], lr: 0.00000028 	 loss = 0.5564(0.3479)
2023/12/02 17:00:41 - INFO - root -   Epoch: [105/300] 	 loss = 0.3456
2023/12/02 17:00:41 - INFO - root -   train_accuracy = 0.8534
2023/12/02 17:00:56 - INFO - root -   Epoch: [106/300][0/283], lr: 0.00000029 	 loss = 0.6386(0.6386)
2023/12/02 17:01:10 - INFO - root -   Epoch: [106/300][20/283], lr: 0.00000029 	 loss = 0.4624(0.3475)
2023/12/02 17:01:41 - INFO - root -   Epoch: [106/300][40/283], lr: 0.00000029 	 loss = 0.0429(0.4397)
2023/12/02 17:01:56 - INFO - root -   Epoch: [106/300][60/283], lr: 0.00000029 	 loss = 1.2949(0.4462)
2023/12/02 17:02:04 - INFO - root -   Epoch: [106/300][80/283], lr: 0.00000029 	 loss = 0.0781(0.4483)
2023/12/02 17:02:22 - INFO - root -   Epoch: [106/300][100/283], lr: 0.00000029 	 loss = 0.1589(0.4198)
2023/12/02 17:02:32 - INFO - root -   Epoch: [106/300][120/283], lr: 0.00000029 	 loss = 0.9833(0.4413)
2023/12/02 17:02:51 - INFO - root -   Epoch: [106/300][140/283], lr: 0.00000029 	 loss = 0.0061(0.4110)
2023/12/02 17:03:01 - INFO - root -   Epoch: [106/300][160/283], lr: 0.00000029 	 loss = 0.0679(0.3819)
2023/12/02 17:03:22 - INFO - root -   Epoch: [106/300][180/283], lr: 0.00000029 	 loss = 0.1648(0.3688)
2023/12/02 17:03:40 - INFO - root -   Epoch: [106/300][200/283], lr: 0.00000029 	 loss = 0.1612(0.3590)
2023/12/02 17:03:50 - INFO - root -   Epoch: [106/300][220/283], lr: 0.00000029 	 loss = 0.1299(0.3502)
2023/12/02 17:04:04 - INFO - root -   Epoch: [106/300][240/283], lr: 0.00000029 	 loss = 0.1295(0.3491)
2023/12/02 17:04:12 - INFO - root -   Epoch: [106/300][260/283], lr: 0.00000029 	 loss = 0.0503(0.3431)
2023/12/02 17:04:22 - INFO - root -   Epoch: [106/300][280/283], lr: 0.00000029 	 loss = 0.2798(0.3358)
2023/12/02 17:04:23 - INFO - root -   Epoch: [106/300] 	 loss = 0.3341
2023/12/02 17:04:23 - INFO - root -   train_accuracy = 0.8498
2023/12/02 17:04:44 - INFO - root -   Epoch: [107/300][0/283], lr: 0.00000029 	 loss = 1.3780(1.3780)
2023/12/02 17:04:55 - INFO - root -   Epoch: [107/300][20/283], lr: 0.00000029 	 loss = 0.6295(0.4097)
2023/12/02 17:05:05 - INFO - root -   Epoch: [107/300][40/283], lr: 0.00000029 	 loss = 0.2235(0.3825)
2023/12/02 17:05:21 - INFO - root -   Epoch: [107/300][60/283], lr: 0.00000029 	 loss = 1.8474(0.3882)
2023/12/02 17:05:35 - INFO - root -   Epoch: [107/300][80/283], lr: 0.00000029 	 loss = 0.0759(0.3885)
2023/12/02 17:05:43 - INFO - root -   Epoch: [107/300][100/283], lr: 0.00000029 	 loss = 0.0106(0.3785)
2023/12/02 17:06:03 - INFO - root -   Epoch: [107/300][120/283], lr: 0.00000029 	 loss = 0.7757(0.3988)
2023/12/02 17:06:17 - INFO - root -   Epoch: [107/300][140/283], lr: 0.00000029 	 loss = 0.0170(0.3687)
2023/12/02 17:06:34 - INFO - root -   Epoch: [107/300][160/283], lr: 0.00000029 	 loss = 0.1132(0.3583)
2023/12/02 17:06:42 - INFO - root -   Epoch: [107/300][180/283], lr: 0.00000029 	 loss = 0.1409(0.3448)
2023/12/02 17:06:53 - INFO - root -   Epoch: [107/300][200/283], lr: 0.00000029 	 loss = 1.0618(0.3435)
2023/12/02 17:07:15 - INFO - root -   Epoch: [107/300][220/283], lr: 0.00000029 	 loss = 0.0802(0.3322)
2023/12/02 17:07:28 - INFO - root -   Epoch: [107/300][240/283], lr: 0.00000029 	 loss = 0.0464(0.3287)
2023/12/02 17:07:46 - INFO - root -   Epoch: [107/300][260/283], lr: 0.00000029 	 loss = 0.0220(0.3241)
2023/12/02 17:07:55 - INFO - root -   Epoch: [107/300][280/283], lr: 0.00000029 	 loss = 1.3749(0.3249)
2023/12/02 17:07:56 - INFO - root -   Epoch: [107/300] 	 loss = 0.3231
2023/12/02 17:07:56 - INFO - root -   train_accuracy = 0.8657
2023/12/02 17:07:57 - INFO - root -   Epoch: [108/300][0/283], lr: 0.00000029 	 loss = 0.0606(0.0606)
2023/12/02 17:08:28 - INFO - root -   Epoch: [108/300][20/283], lr: 0.00000029 	 loss = 0.8778(0.4313)
2023/12/02 17:08:36 - INFO - root -   Epoch: [108/300][40/283], lr: 0.00000029 	 loss = 0.1433(0.4387)
2023/12/02 17:08:54 - INFO - root -   Epoch: [108/300][60/283], lr: 0.00000029 	 loss = 1.2292(0.4167)
2023/12/02 17:09:03 - INFO - root -   Epoch: [108/300][80/283], lr: 0.00000029 	 loss = 0.0350(0.4015)
2023/12/02 17:09:26 - INFO - root -   Epoch: [108/300][100/283], lr: 0.00000029 	 loss = 0.0384(0.3739)
2023/12/02 17:09:38 - INFO - root -   Epoch: [108/300][120/283], lr: 0.00000029 	 loss = 0.7034(0.3995)
2023/12/02 17:09:57 - INFO - root -   Epoch: [108/300][140/283], lr: 0.00000029 	 loss = 0.0098(0.3664)
2023/12/02 17:10:13 - INFO - root -   Epoch: [108/300][160/283], lr: 0.00000029 	 loss = 1.0168(0.3539)
2023/12/02 17:10:27 - INFO - root -   Epoch: [108/300][180/283], lr: 0.00000029 	 loss = 1.0276(0.3581)
2023/12/02 17:10:48 - INFO - root -   Epoch: [108/300][200/283], lr: 0.00000029 	 loss = 0.1709(0.3427)
2023/12/02 17:10:57 - INFO - root -   Epoch: [108/300][220/283], lr: 0.00000029 	 loss = 0.0066(0.3381)
2023/12/02 17:11:07 - INFO - root -   Epoch: [108/300][240/283], lr: 0.00000029 	 loss = 0.0809(0.3377)
2023/12/02 17:11:15 - INFO - root -   Epoch: [108/300][260/283], lr: 0.00000029 	 loss = 0.2034(0.3291)
2023/12/02 17:11:29 - INFO - root -   Epoch: [108/300][280/283], lr: 0.00000029 	 loss = 0.9205(0.3247)
2023/12/02 17:11:30 - INFO - root -   Epoch: [108/300] 	 loss = 0.3230
2023/12/02 17:11:30 - INFO - root -   train_accuracy = 0.8728
2023/12/02 17:11:32 - INFO - root -   Epoch: [109/300][0/283], lr: 0.00000029 	 loss = 0.1318(0.1318)
2023/12/02 17:12:13 - INFO - root -   Epoch: [109/300][20/283], lr: 0.00000029 	 loss = 0.3387(0.5079)
2023/12/02 17:12:21 - INFO - root -   Epoch: [109/300][40/283], lr: 0.00000029 	 loss = 0.4467(0.4252)
2023/12/02 17:12:44 - INFO - root -   Epoch: [109/300][60/283], lr: 0.00000029 	 loss = 1.0207(0.4516)
2023/12/02 17:12:54 - INFO - root -   Epoch: [109/300][80/283], lr: 0.00000029 	 loss = 0.1195(0.4106)
2023/12/02 17:13:19 - INFO - root -   Epoch: [109/300][100/283], lr: 0.00000029 	 loss = 0.2339(0.3743)
2023/12/02 17:13:33 - INFO - root -   Epoch: [109/300][120/283], lr: 0.00000029 	 loss = 0.3073(0.3902)
2023/12/02 17:13:55 - INFO - root -   Epoch: [109/300][140/283], lr: 0.00000029 	 loss = 0.2549(0.3704)
2023/12/02 17:14:08 - INFO - root -   Epoch: [109/300][160/283], lr: 0.00000029 	 loss = 0.0275(0.3457)
2023/12/02 17:14:16 - INFO - root -   Epoch: [109/300][180/283], lr: 0.00000029 	 loss = 0.6926(0.3333)
2023/12/02 17:14:30 - INFO - root -   Epoch: [109/300][200/283], lr: 0.00000029 	 loss = 0.0861(0.3219)
2023/12/02 17:14:45 - INFO - root -   Epoch: [109/300][220/283], lr: 0.00000029 	 loss = 0.0094(0.3095)
2023/12/02 17:15:05 - INFO - root -   Epoch: [109/300][240/283], lr: 0.00000029 	 loss = 0.2449(0.3193)
2023/12/02 17:15:13 - INFO - root -   Epoch: [109/300][260/283], lr: 0.00000029 	 loss = 0.0583(0.3265)
2023/12/02 17:15:27 - INFO - root -   Epoch: [109/300][280/283], lr: 0.00000029 	 loss = 0.2160(0.3153)
2023/12/02 17:15:28 - INFO - root -   Epoch: [109/300] 	 loss = 0.3148
2023/12/02 17:15:37 - INFO - root -   precision = 0.8531
2023/12/02 17:15:37 - INFO - root -   eval_loss = 0.3979
2023/12/02 17:15:37 - INFO - root -   eval_acc = 0.8531
2023/12/02 17:15:38 - INFO - root -   train_accuracy = 0.8746
2023/12/02 17:15:56 - INFO - root -   Epoch: [110/300][0/283], lr: 0.00000029 	 loss = 0.6811(0.6811)
2023/12/02 17:16:04 - INFO - root -   Epoch: [110/300][20/283], lr: 0.00000029 	 loss = 0.1202(0.3434)
2023/12/02 17:16:12 - INFO - root -   Epoch: [110/300][40/283], lr: 0.00000029 	 loss = 0.6004(0.3358)
2023/12/02 17:16:28 - INFO - root -   Epoch: [110/300][60/283], lr: 0.00000029 	 loss = 1.3134(0.3826)
2023/12/02 17:16:45 - INFO - root -   Epoch: [110/300][80/283], lr: 0.00000029 	 loss = 0.2948(0.3849)
2023/12/02 17:17:07 - INFO - root -   Epoch: [110/300][100/283], lr: 0.00000029 	 loss = 0.3871(0.3863)
2023/12/02 17:17:18 - INFO - root -   Epoch: [110/300][120/283], lr: 0.00000029 	 loss = 0.8822(0.3927)
2023/12/02 17:17:40 - INFO - root -   Epoch: [110/300][140/283], lr: 0.00000029 	 loss = 0.4372(0.3763)
2023/12/02 17:17:54 - INFO - root -   Epoch: [110/300][160/283], lr: 0.00000029 	 loss = 0.0281(0.3655)
2023/12/02 17:18:03 - INFO - root -   Epoch: [110/300][180/283], lr: 0.00000029 	 loss = 0.0560(0.3513)
2023/12/02 17:18:26 - INFO - root -   Epoch: [110/300][200/283], lr: 0.00000029 	 loss = 0.1695(0.3407)
2023/12/02 17:18:43 - INFO - root -   Epoch: [110/300][220/283], lr: 0.00000029 	 loss = 0.1458(0.3288)
2023/12/02 17:18:51 - INFO - root -   Epoch: [110/300][240/283], lr: 0.00000029 	 loss = 0.1323(0.3289)
2023/12/02 17:19:04 - INFO - root -   Epoch: [110/300][260/283], lr: 0.00000029 	 loss = 0.0227(0.3263)
2023/12/02 17:19:14 - INFO - root -   Epoch: [110/300][280/283], lr: 0.00000029 	 loss = 0.4481(0.3328)
2023/12/02 17:19:15 - INFO - root -   Epoch: [110/300] 	 loss = 0.3311
2023/12/02 17:19:15 - INFO - root -   train_accuracy = 0.8622
2023/12/02 17:19:16 - INFO - root -   Epoch: [111/300][0/283], lr: 0.00000029 	 loss = 0.0233(0.0233)
2023/12/02 17:19:50 - INFO - root -   Epoch: [111/300][20/283], lr: 0.00000029 	 loss = 0.5337(0.4781)
2023/12/02 17:19:58 - INFO - root -   Epoch: [111/300][40/283], lr: 0.00000029 	 loss = 0.2799(0.4514)
2023/12/02 17:20:17 - INFO - root -   Epoch: [111/300][60/283], lr: 0.00000029 	 loss = 1.6403(0.4648)
2023/12/02 17:20:39 - INFO - root -   Epoch: [111/300][80/283], lr: 0.00000029 	 loss = 0.2170(0.4769)
2023/12/02 17:20:47 - INFO - root -   Epoch: [111/300][100/283], lr: 0.00000029 	 loss = 0.0666(0.4239)
2023/12/02 17:20:59 - INFO - root -   Epoch: [111/300][120/283], lr: 0.00000029 	 loss = 0.5617(0.4206)
2023/12/02 17:21:21 - INFO - root -   Epoch: [111/300][140/283], lr: 0.00000029 	 loss = 0.0242(0.3947)
2023/12/02 17:21:29 - INFO - root -   Epoch: [111/300][160/283], lr: 0.00000029 	 loss = 0.0323(0.3721)
2023/12/02 17:21:42 - INFO - root -   Epoch: [111/300][180/283], lr: 0.00000029 	 loss = 0.0415(0.3463)
2023/12/02 17:22:00 - INFO - root -   Epoch: [111/300][200/283], lr: 0.00000029 	 loss = 0.0714(0.3360)
2023/12/02 17:22:08 - INFO - root -   Epoch: [111/300][220/283], lr: 0.00000029 	 loss = 0.3497(0.3267)
2023/12/02 17:22:27 - INFO - root -   Epoch: [111/300][240/283], lr: 0.00000029 	 loss = 0.0852(0.3223)
2023/12/02 17:22:44 - INFO - root -   Epoch: [111/300][260/283], lr: 0.00000029 	 loss = 0.0169(0.3271)
2023/12/02 17:22:52 - INFO - root -   Epoch: [111/300][280/283], lr: 0.00000029 	 loss = 0.2671(0.3229)
2023/12/02 17:22:53 - INFO - root -   Epoch: [111/300] 	 loss = 0.3217
2023/12/02 17:22:53 - INFO - root -   train_accuracy = 0.8781
2023/12/02 17:22:54 - INFO - root -   Epoch: [112/300][0/283], lr: 0.00000030 	 loss = 0.0739(0.0739)
2023/12/02 17:23:33 - INFO - root -   Epoch: [112/300][20/283], lr: 0.00000030 	 loss = 0.9658(0.4456)
2023/12/02 17:23:42 - INFO - root -   Epoch: [112/300][40/283], lr: 0.00000030 	 loss = 0.1377(0.4152)
2023/12/02 17:23:56 - INFO - root -   Epoch: [112/300][60/283], lr: 0.00000030 	 loss = 0.4192(0.4290)
2023/12/02 17:24:16 - INFO - root -   Epoch: [112/300][80/283], lr: 0.00000030 	 loss = 0.2120(0.3844)
2023/12/02 17:24:24 - INFO - root -   Epoch: [112/300][100/283], lr: 0.00000030 	 loss = 0.0521(0.3896)
2023/12/02 17:24:36 - INFO - root -   Epoch: [112/300][120/283], lr: 0.00000030 	 loss = 0.1594(0.4083)
2023/12/02 17:24:47 - INFO - root -   Epoch: [112/300][140/283], lr: 0.00000030 	 loss = 0.1145(0.3829)
2023/12/02 17:25:01 - INFO - root -   Epoch: [112/300][160/283], lr: 0.00000030 	 loss = 0.1057(0.3636)
2023/12/02 17:25:25 - INFO - root -   Epoch: [112/300][180/283], lr: 0.00000030 	 loss = 0.2437(0.3455)
2023/12/02 17:25:46 - INFO - root -   Epoch: [112/300][200/283], lr: 0.00000030 	 loss = 0.7305(0.3436)
2023/12/02 17:26:04 - INFO - root -   Epoch: [112/300][220/283], lr: 0.00000030 	 loss = 0.0192(0.3296)
2023/12/02 17:26:12 - INFO - root -   Epoch: [112/300][240/283], lr: 0.00000030 	 loss = 0.0974(0.3235)
2023/12/02 17:26:28 - INFO - root -   Epoch: [112/300][260/283], lr: 0.00000030 	 loss = 0.0203(0.3230)
2023/12/02 17:26:36 - INFO - root -   Epoch: [112/300][280/283], lr: 0.00000030 	 loss = 0.3450(0.3116)
2023/12/02 17:26:37 - INFO - root -   Epoch: [112/300] 	 loss = 0.3096
2023/12/02 17:26:37 - INFO - root -   train_accuracy = 0.8852
2023/12/02 17:26:54 - INFO - root -   Epoch: [113/300][0/283], lr: 0.00000030 	 loss = 0.6212(0.6212)
2023/12/02 17:27:10 - INFO - root -   Epoch: [113/300][20/283], lr: 0.00000030 	 loss = 1.6413(0.4734)
2023/12/02 17:27:28 - INFO - root -   Epoch: [113/300][40/283], lr: 0.00000030 	 loss = 0.1949(0.4623)
2023/12/02 17:27:36 - INFO - root -   Epoch: [113/300][60/283], lr: 0.00000030 	 loss = 1.0004(0.4156)
2023/12/02 17:28:03 - INFO - root -   Epoch: [113/300][80/283], lr: 0.00000030 	 loss = 0.0575(0.3991)
2023/12/02 17:28:15 - INFO - root -   Epoch: [113/300][100/283], lr: 0.00000030 	 loss = 0.1785(0.3660)
2023/12/02 17:28:30 - INFO - root -   Epoch: [113/300][120/283], lr: 0.00000030 	 loss = 0.0685(0.3649)
2023/12/02 17:28:41 - INFO - root -   Epoch: [113/300][140/283], lr: 0.00000030 	 loss = 0.0089(0.3505)
2023/12/02 17:28:57 - INFO - root -   Epoch: [113/300][160/283], lr: 0.00000030 	 loss = 0.0170(0.3220)
2023/12/02 17:29:06 - INFO - root -   Epoch: [113/300][180/283], lr: 0.00000030 	 loss = 0.0503(0.3106)
2023/12/02 17:29:25 - INFO - root -   Epoch: [113/300][200/283], lr: 0.00000030 	 loss = 0.2371(0.3051)
2023/12/02 17:29:45 - INFO - root -   Epoch: [113/300][220/283], lr: 0.00000030 	 loss = 0.2109(0.2997)
2023/12/02 17:29:54 - INFO - root -   Epoch: [113/300][240/283], lr: 0.00000030 	 loss = 0.1625(0.3086)
2023/12/02 17:30:27 - INFO - root -   Epoch: [113/300][260/283], lr: 0.00000030 	 loss = 0.2399(0.3132)
2023/12/02 17:30:36 - INFO - root -   Epoch: [113/300][280/283], lr: 0.00000030 	 loss = 0.6736(0.3098)
2023/12/02 17:30:37 - INFO - root -   Epoch: [113/300] 	 loss = 0.3082
2023/12/02 17:30:37 - INFO - root -   train_accuracy = 0.8640
2023/12/02 17:30:54 - INFO - root -   Epoch: [114/300][0/283], lr: 0.00000030 	 loss = 1.0717(1.0717)
2023/12/02 17:31:17 - INFO - root -   Epoch: [114/300][20/283], lr: 0.00000030 	 loss = 0.6340(0.4344)
2023/12/02 17:31:30 - INFO - root -   Epoch: [114/300][40/283], lr: 0.00000030 	 loss = 0.1333(0.4273)
2023/12/02 17:31:53 - INFO - root -   Epoch: [114/300][60/283], lr: 0.00000030 	 loss = 1.8486(0.4245)
2023/12/02 17:32:01 - INFO - root -   Epoch: [114/300][80/283], lr: 0.00000030 	 loss = 0.7036(0.4138)
2023/12/02 17:32:20 - INFO - root -   Epoch: [114/300][100/283], lr: 0.00000030 	 loss = 0.1146(0.3770)
2023/12/02 17:32:31 - INFO - root -   Epoch: [114/300][120/283], lr: 0.00000030 	 loss = 0.9078(0.4066)
2023/12/02 17:32:47 - INFO - root -   Epoch: [114/300][140/283], lr: 0.00000030 	 loss = 0.0315(0.3703)
2023/12/02 17:32:55 - INFO - root -   Epoch: [114/300][160/283], lr: 0.00000030 	 loss = 0.7725(0.3586)
2023/12/02 17:33:03 - INFO - root -   Epoch: [114/300][180/283], lr: 0.00000030 	 loss = 0.4007(0.3417)
2023/12/02 17:33:29 - INFO - root -   Epoch: [114/300][200/283], lr: 0.00000030 	 loss = 0.1547(0.3425)
2023/12/02 17:33:43 - INFO - root -   Epoch: [114/300][220/283], lr: 0.00000030 	 loss = 0.3604(0.3355)
2023/12/02 17:33:54 - INFO - root -   Epoch: [114/300][240/283], lr: 0.00000030 	 loss = 0.0705(0.3331)
2023/12/02 17:34:04 - INFO - root -   Epoch: [114/300][260/283], lr: 0.00000030 	 loss = 0.0120(0.3315)
2023/12/02 17:34:12 - INFO - root -   Epoch: [114/300][280/283], lr: 0.00000030 	 loss = 0.4398(0.3303)
2023/12/02 17:34:13 - INFO - root -   Epoch: [114/300] 	 loss = 0.3281
2023/12/02 17:34:23 - INFO - root -   precision = 0.8462
2023/12/02 17:34:23 - INFO - root -   eval_loss = 0.4360
2023/12/02 17:34:23 - INFO - root -   eval_acc = 0.8462
2023/12/02 17:34:23 - INFO - root -   train_accuracy = 0.8675
2023/12/02 17:34:36 - INFO - root -   Epoch: [115/300][0/283], lr: 0.00000030 	 loss = 0.2734(0.2734)
2023/12/02 17:34:58 - INFO - root -   Epoch: [115/300][20/283], lr: 0.00000030 	 loss = 0.5125(0.3291)
2023/12/02 17:35:15 - INFO - root -   Epoch: [115/300][40/283], lr: 0.00000030 	 loss = 0.3408(0.3375)
2023/12/02 17:35:36 - INFO - root -   Epoch: [115/300][60/283], lr: 0.00000030 	 loss = 1.7430(0.3710)
2023/12/02 17:35:44 - INFO - root -   Epoch: [115/300][80/283], lr: 0.00000030 	 loss = 0.2669(0.3719)
2023/12/02 17:36:05 - INFO - root -   Epoch: [115/300][100/283], lr: 0.00000030 	 loss = 0.3191(0.3547)
2023/12/02 17:36:27 - INFO - root -   Epoch: [115/300][120/283], lr: 0.00000030 	 loss = 0.5510(0.3879)
2023/12/02 17:36:35 - INFO - root -   Epoch: [115/300][140/283], lr: 0.00000030 	 loss = 0.0184(0.3580)
2023/12/02 17:36:43 - INFO - root -   Epoch: [115/300][160/283], lr: 0.00000030 	 loss = 0.1663(0.3332)
2023/12/02 17:36:58 - INFO - root -   Epoch: [115/300][180/283], lr: 0.00000030 	 loss = 0.8844(0.3231)
2023/12/02 17:37:13 - INFO - root -   Epoch: [115/300][200/283], lr: 0.00000030 	 loss = 0.0531(0.3153)
2023/12/02 17:37:22 - INFO - root -   Epoch: [115/300][220/283], lr: 0.00000030 	 loss = 0.1467(0.3023)
2023/12/02 17:37:37 - INFO - root -   Epoch: [115/300][240/283], lr: 0.00000030 	 loss = 0.1139(0.3055)
2023/12/02 17:37:53 - INFO - root -   Epoch: [115/300][260/283], lr: 0.00000030 	 loss = 0.0175(0.3071)
2023/12/02 17:38:03 - INFO - root -   Epoch: [115/300][280/283], lr: 0.00000030 	 loss = 1.0377(0.3079)
2023/12/02 17:38:04 - INFO - root -   Epoch: [115/300] 	 loss = 0.3063
2023/12/02 17:38:04 - INFO - root -   train_accuracy = 0.8763
2023/12/02 17:38:19 - INFO - root -   Epoch: [116/300][0/283], lr: 0.00000030 	 loss = 0.0667(0.0667)
2023/12/02 17:38:39 - INFO - root -   Epoch: [116/300][20/283], lr: 0.00000030 	 loss = 0.7391(0.2883)
2023/12/02 17:38:47 - INFO - root -   Epoch: [116/300][40/283], lr: 0.00000030 	 loss = 0.0479(0.3202)
2023/12/02 17:39:08 - INFO - root -   Epoch: [116/300][60/283], lr: 0.00000030 	 loss = 1.6322(0.3761)
2023/12/02 17:39:16 - INFO - root -   Epoch: [116/300][80/283], lr: 0.00000030 	 loss = 0.1789(0.3461)
2023/12/02 17:39:24 - INFO - root -   Epoch: [116/300][100/283], lr: 0.00000030 	 loss = 0.0220(0.3327)
2023/12/02 17:39:38 - INFO - root -   Epoch: [116/300][120/283], lr: 0.00000030 	 loss = 0.0811(0.3625)
2023/12/02 17:39:56 - INFO - root -   Epoch: [116/300][140/283], lr: 0.00000030 	 loss = 0.0282(0.3499)
2023/12/02 17:40:12 - INFO - root -   Epoch: [116/300][160/283], lr: 0.00000030 	 loss = 0.3070(0.3360)
2023/12/02 17:40:31 - INFO - root -   Epoch: [116/300][180/283], lr: 0.00000030 	 loss = 0.0831(0.3179)
2023/12/02 17:40:39 - INFO - root -   Epoch: [116/300][200/283], lr: 0.00000030 	 loss = 0.2296(0.3126)
2023/12/02 17:40:48 - INFO - root -   Epoch: [116/300][220/283], lr: 0.00000030 	 loss = 0.0094(0.2985)
2023/12/02 17:41:10 - INFO - root -   Epoch: [116/300][240/283], lr: 0.00000030 	 loss = 0.1018(0.2967)
2023/12/02 17:41:20 - INFO - root -   Epoch: [116/300][260/283], lr: 0.00000030 	 loss = 0.0046(0.2987)
2023/12/02 17:41:28 - INFO - root -   Epoch: [116/300][280/283], lr: 0.00000030 	 loss = 0.9697(0.2964)
2023/12/02 17:41:29 - INFO - root -   Epoch: [116/300] 	 loss = 0.2945
2023/12/02 17:41:29 - INFO - root -   train_accuracy = 0.8746
2023/12/02 17:41:32 - INFO - root -   Epoch: [117/300][0/283], lr: 0.00000030 	 loss = 0.0426(0.0426)
2023/12/02 17:42:04 - INFO - root -   Epoch: [117/300][20/283], lr: 0.00000030 	 loss = 1.2148(0.4963)
2023/12/02 17:42:18 - INFO - root -   Epoch: [117/300][40/283], lr: 0.00000030 	 loss = 0.2832(0.4803)
2023/12/02 17:42:40 - INFO - root -   Epoch: [117/300][60/283], lr: 0.00000030 	 loss = 1.9726(0.4461)
2023/12/02 17:42:48 - INFO - root -   Epoch: [117/300][80/283], lr: 0.00000030 	 loss = 0.4122(0.4010)
2023/12/02 17:43:16 - INFO - root -   Epoch: [117/300][100/283], lr: 0.00000030 	 loss = 0.0125(0.4043)
2023/12/02 17:43:31 - INFO - root -   Epoch: [117/300][120/283], lr: 0.00000030 	 loss = 0.4315(0.4390)
2023/12/02 17:44:03 - INFO - root -   Epoch: [117/300][140/283], lr: 0.00000030 	 loss = 0.0270(0.4136)
2023/12/02 17:44:14 - INFO - root -   Epoch: [117/300][160/283], lr: 0.00000030 	 loss = 0.0636(0.3987)
2023/12/02 17:44:31 - INFO - root -   Epoch: [117/300][180/283], lr: 0.00000030 	 loss = 0.2188(0.3863)
2023/12/02 17:44:39 - INFO - root -   Epoch: [117/300][200/283], lr: 0.00000030 	 loss = 0.3477(0.3708)
2023/12/02 17:44:47 - INFO - root -   Epoch: [117/300][220/283], lr: 0.00000030 	 loss = 0.0309(0.3563)
2023/12/02 17:45:12 - INFO - root -   Epoch: [117/300][240/283], lr: 0.00000030 	 loss = 0.2991(0.3571)
2023/12/02 17:45:20 - INFO - root -   Epoch: [117/300][260/283], lr: 0.00000030 	 loss = 0.1092(0.3504)
2023/12/02 17:45:28 - INFO - root -   Epoch: [117/300][280/283], lr: 0.00000030 	 loss = 1.4314(0.3446)
2023/12/02 17:45:29 - INFO - root -   Epoch: [117/300] 	 loss = 0.3430
2023/12/02 17:45:29 - INFO - root -   train_accuracy = 0.8746
2023/12/02 17:45:44 - INFO - root -   Epoch: [118/300][0/283], lr: 0.00000031 	 loss = 0.0322(0.0322)
2023/12/02 17:46:00 - INFO - root -   Epoch: [118/300][20/283], lr: 0.00000031 	 loss = 0.3780(0.3872)
2023/12/02 17:46:12 - INFO - root -   Epoch: [118/300][40/283], lr: 0.00000031 	 loss = 0.2165(0.4324)
2023/12/02 17:46:27 - INFO - root -   Epoch: [118/300][60/283], lr: 0.00000031 	 loss = 1.5734(0.4438)
2023/12/02 17:46:35 - INFO - root -   Epoch: [118/300][80/283], lr: 0.00000031 	 loss = 0.6001(0.4230)
2023/12/02 17:46:53 - INFO - root -   Epoch: [118/300][100/283], lr: 0.00000031 	 loss = 0.0803(0.3756)
2023/12/02 17:47:08 - INFO - root -   Epoch: [118/300][120/283], lr: 0.00000031 	 loss = 0.3389(0.3696)
2023/12/02 17:47:24 - INFO - root -   Epoch: [118/300][140/283], lr: 0.00000031 	 loss = 0.0605(0.3441)
2023/12/02 17:47:44 - INFO - root -   Epoch: [118/300][160/283], lr: 0.00000031 	 loss = 0.0496(0.3319)
2023/12/02 17:47:55 - INFO - root -   Epoch: [118/300][180/283], lr: 0.00000031 	 loss = 0.4033(0.3198)
2023/12/02 17:48:18 - INFO - root -   Epoch: [118/300][200/283], lr: 0.00000031 	 loss = 0.1700(0.3096)
2023/12/02 17:48:32 - INFO - root -   Epoch: [118/300][220/283], lr: 0.00000031 	 loss = 0.1067(0.3030)
2023/12/02 17:48:45 - INFO - root -   Epoch: [118/300][240/283], lr: 0.00000031 	 loss = 0.0327(0.3136)
2023/12/02 17:49:11 - INFO - root -   Epoch: [118/300][260/283], lr: 0.00000031 	 loss = 0.0465(0.3186)
2023/12/02 17:49:19 - INFO - root -   Epoch: [118/300][280/283], lr: 0.00000031 	 loss = 0.4603(0.3184)
2023/12/02 17:49:20 - INFO - root -   Epoch: [118/300] 	 loss = 0.3163
2023/12/02 17:49:20 - INFO - root -   train_accuracy = 0.8834
2023/12/02 17:49:34 - INFO - root -   Epoch: [119/300][0/283], lr: 0.00000031 	 loss = 0.6667(0.6667)
2023/12/02 17:49:55 - INFO - root -   Epoch: [119/300][20/283], lr: 0.00000031 	 loss = 0.2054(0.5256)
2023/12/02 17:50:10 - INFO - root -   Epoch: [119/300][40/283], lr: 0.00000031 	 loss = 0.2440(0.4110)
2023/12/02 17:50:32 - INFO - root -   Epoch: [119/300][60/283], lr: 0.00000031 	 loss = 1.9130(0.4225)
2023/12/02 17:50:46 - INFO - root -   Epoch: [119/300][80/283], lr: 0.00000031 	 loss = 0.3560(0.4015)
2023/12/02 17:50:59 - INFO - root -   Epoch: [119/300][100/283], lr: 0.00000031 	 loss = 0.0142(0.3757)
2023/12/02 17:51:07 - INFO - root -   Epoch: [119/300][120/283], lr: 0.00000031 	 loss = 0.1794(0.4060)
2023/12/02 17:51:30 - INFO - root -   Epoch: [119/300][140/283], lr: 0.00000031 	 loss = 0.1406(0.3871)
2023/12/02 17:51:40 - INFO - root -   Epoch: [119/300][160/283], lr: 0.00000031 	 loss = 0.0363(0.3794)
2023/12/02 17:51:48 - INFO - root -   Epoch: [119/300][180/283], lr: 0.00000031 	 loss = 0.3835(0.3707)
2023/12/02 17:52:14 - INFO - root -   Epoch: [119/300][200/283], lr: 0.00000031 	 loss = 0.2997(0.3623)
2023/12/02 17:52:23 - INFO - root -   Epoch: [119/300][220/283], lr: 0.00000031 	 loss = 0.0250(0.3445)
2023/12/02 17:52:41 - INFO - root -   Epoch: [119/300][240/283], lr: 0.00000031 	 loss = 0.1693(0.3469)
2023/12/02 17:52:49 - INFO - root -   Epoch: [119/300][260/283], lr: 0.00000031 	 loss = 0.0682(0.3423)
2023/12/02 17:53:00 - INFO - root -   Epoch: [119/300][280/283], lr: 0.00000031 	 loss = 1.2224(0.3413)
2023/12/02 17:53:01 - INFO - root -   Epoch: [119/300] 	 loss = 0.3398
2023/12/02 17:53:10 - INFO - root -   precision = 0.8392
2023/12/02 17:53:10 - INFO - root -   eval_loss = 0.4366
2023/12/02 17:53:10 - INFO - root -   eval_acc = 0.8392
2023/12/02 17:53:10 - INFO - root -   train_accuracy = 0.8622
2023/12/02 17:53:25 - INFO - root -   Epoch: [120/300][0/283], lr: 0.00000031 	 loss = 0.1342(0.1342)
2023/12/02 17:53:39 - INFO - root -   Epoch: [120/300][20/283], lr: 0.00000031 	 loss = 0.7831(0.3642)
2023/12/02 17:53:55 - INFO - root -   Epoch: [120/300][40/283], lr: 0.00000031 	 loss = 0.1383(0.3258)
2023/12/02 17:54:03 - INFO - root -   Epoch: [120/300][60/283], lr: 0.00000031 	 loss = 1.3441(0.3980)
2023/12/02 17:54:19 - INFO - root -   Epoch: [120/300][80/283], lr: 0.00000031 	 loss = 0.9246(0.3814)
2023/12/02 17:54:34 - INFO - root -   Epoch: [120/300][100/283], lr: 0.00000031 	 loss = 0.7508(0.3879)
2023/12/02 17:54:47 - INFO - root -   Epoch: [120/300][120/283], lr: 0.00000031 	 loss = 0.9756(0.4094)
2023/12/02 17:55:05 - INFO - root -   Epoch: [120/300][140/283], lr: 0.00000031 	 loss = 0.1144(0.3772)
2023/12/02 17:55:12 - INFO - root -   Epoch: [120/300][160/283], lr: 0.00000031 	 loss = 0.0262(0.3514)
2023/12/02 17:55:27 - INFO - root -   Epoch: [120/300][180/283], lr: 0.00000031 	 loss = 0.1828(0.3400)
2023/12/02 17:55:40 - INFO - root -   Epoch: [120/300][200/283], lr: 0.00000031 	 loss = 0.4588(0.3259)
2023/12/02 17:56:00 - INFO - root -   Epoch: [120/300][220/283], lr: 0.00000031 	 loss = 0.1289(0.3175)
2023/12/02 17:56:10 - INFO - root -   Epoch: [120/300][240/283], lr: 0.00000031 	 loss = 0.3159(0.3173)
2023/12/02 17:56:33 - INFO - root -   Epoch: [120/300][260/283], lr: 0.00000031 	 loss = 0.1713(0.3131)
2023/12/02 17:56:40 - INFO - root -   Epoch: [120/300][280/283], lr: 0.00000031 	 loss = 0.3941(0.3058)
2023/12/02 17:56:41 - INFO - root -   Epoch: [120/300] 	 loss = 0.3043
2023/12/02 17:56:41 - INFO - root -   train_accuracy = 0.8728
2023/12/02 17:56:44 - INFO - root -   Epoch: [121/300][0/283], lr: 0.00000031 	 loss = 0.0330(0.0330)
2023/12/02 17:57:07 - INFO - root -   Epoch: [121/300][20/283], lr: 0.00000031 	 loss = 0.5748(0.3081)
2023/12/02 17:57:15 - INFO - root -   Epoch: [121/300][40/283], lr: 0.00000031 	 loss = 0.0259(0.3956)
2023/12/02 17:57:39 - INFO - root -   Epoch: [121/300][60/283], lr: 0.00000031 	 loss = 2.1188(0.4339)
2023/12/02 17:57:52 - INFO - root -   Epoch: [121/300][80/283], lr: 0.00000031 	 loss = 0.5200(0.4236)
2023/12/02 17:58:14 - INFO - root -   Epoch: [121/300][100/283], lr: 0.00000031 	 loss = 0.2966(0.4052)
2023/12/02 17:58:22 - INFO - root -   Epoch: [121/300][120/283], lr: 0.00000031 	 loss = 0.1676(0.4198)
2023/12/02 17:58:40 - INFO - root -   Epoch: [121/300][140/283], lr: 0.00000031 	 loss = 0.0072(0.3937)
2023/12/02 17:58:57 - INFO - root -   Epoch: [121/300][160/283], lr: 0.00000031 	 loss = 0.3671(0.3771)
2023/12/02 17:59:13 - INFO - root -   Epoch: [121/300][180/283], lr: 0.00000031 	 loss = 1.8954(0.3650)
2023/12/02 17:59:25 - INFO - root -   Epoch: [121/300][200/283], lr: 0.00000031 	 loss = 0.0916(0.3598)
2023/12/02 17:59:48 - INFO - root -   Epoch: [121/300][220/283], lr: 0.00000031 	 loss = 0.0231(0.3621)
2023/12/02 17:59:56 - INFO - root -   Epoch: [121/300][240/283], lr: 0.00000031 	 loss = 0.0508(0.3690)
2023/12/02 18:00:17 - INFO - root -   Epoch: [121/300][260/283], lr: 0.00000031 	 loss = 0.0716(0.3584)
2023/12/02 18:00:25 - INFO - root -   Epoch: [121/300][280/283], lr: 0.00000031 	 loss = 0.7601(0.3506)
2023/12/02 18:00:26 - INFO - root -   Epoch: [121/300] 	 loss = 0.3491
2023/12/02 18:00:26 - INFO - root -   train_accuracy = 0.8657
2023/12/02 18:00:29 - INFO - root -   Epoch: [122/300][0/283], lr: 0.00000031 	 loss = 0.0357(0.0357)
2023/12/02 18:00:55 - INFO - root -   Epoch: [122/300][20/283], lr: 0.00000031 	 loss = 0.5948(0.3248)
2023/12/02 18:01:13 - INFO - root -   Epoch: [122/300][40/283], lr: 0.00000031 	 loss = 0.0509(0.3788)
2023/12/02 18:01:22 - INFO - root -   Epoch: [122/300][60/283], lr: 0.00000031 	 loss = 0.4647(0.3768)
2023/12/02 18:01:41 - INFO - root -   Epoch: [122/300][80/283], lr: 0.00000031 	 loss = 0.5368(0.3999)
2023/12/02 18:01:56 - INFO - root -   Epoch: [122/300][100/283], lr: 0.00000031 	 loss = 0.1502(0.3720)
2023/12/02 18:02:21 - INFO - root -   Epoch: [122/300][120/283], lr: 0.00000031 	 loss = 0.3804(0.4002)
2023/12/02 18:02:29 - INFO - root -   Epoch: [122/300][140/283], lr: 0.00000031 	 loss = 0.1631(0.3623)
2023/12/02 18:02:41 - INFO - root -   Epoch: [122/300][160/283], lr: 0.00000031 	 loss = 0.0114(0.3344)
2023/12/02 18:03:05 - INFO - root -   Epoch: [122/300][180/283], lr: 0.00000031 	 loss = 0.0439(0.3219)
2023/12/02 18:03:22 - INFO - root -   Epoch: [122/300][200/283], lr: 0.00000031 	 loss = 0.0692(0.3184)
2023/12/02 18:03:34 - INFO - root -   Epoch: [122/300][220/283], lr: 0.00000031 	 loss = 0.1214(0.3180)
2023/12/02 18:03:57 - INFO - root -   Epoch: [122/300][240/283], lr: 0.00000031 	 loss = 0.1067(0.3304)
2023/12/02 18:04:06 - INFO - root -   Epoch: [122/300][260/283], lr: 0.00000031 	 loss = 0.1444(0.3273)
2023/12/02 18:04:14 - INFO - root -   Epoch: [122/300][280/283], lr: 0.00000031 	 loss = 0.5878(0.3197)
2023/12/02 18:04:15 - INFO - root -   Epoch: [122/300] 	 loss = 0.3177
2023/12/02 18:04:15 - INFO - root -   train_accuracy = 0.8657
2023/12/02 18:04:17 - INFO - root -   Epoch: [123/300][0/283], lr: 0.00000032 	 loss = 0.0839(0.0839)
2023/12/02 18:04:43 - INFO - root -   Epoch: [123/300][20/283], lr: 0.00000032 	 loss = 0.1928(0.2854)
2023/12/02 18:04:51 - INFO - root -   Epoch: [123/300][40/283], lr: 0.00000032 	 loss = 0.2802(0.3209)
2023/12/02 18:05:19 - INFO - root -   Epoch: [123/300][60/283], lr: 0.00000032 	 loss = 0.6853(0.3446)
2023/12/02 18:05:34 - INFO - root -   Epoch: [123/300][80/283], lr: 0.00000032 	 loss = 0.0700(0.3307)
2023/12/02 18:05:54 - INFO - root -   Epoch: [123/300][100/283], lr: 0.00000032 	 loss = 0.1964(0.3364)
2023/12/02 18:06:07 - INFO - root -   Epoch: [123/300][120/283], lr: 0.00000032 	 loss = 0.3351(0.3381)
2023/12/02 18:06:29 - INFO - root -   Epoch: [123/300][140/283], lr: 0.00000032 	 loss = 0.0663(0.3381)
2023/12/02 18:06:40 - INFO - root -   Epoch: [123/300][160/283], lr: 0.00000032 	 loss = 0.0373(0.3157)
2023/12/02 18:07:09 - INFO - root -   Epoch: [123/300][180/283], lr: 0.00000032 	 loss = 1.0866(0.3050)
2023/12/02 18:07:22 - INFO - root -   Epoch: [123/300][200/283], lr: 0.00000032 	 loss = 0.0991(0.3115)
2023/12/02 18:07:30 - INFO - root -   Epoch: [123/300][220/283], lr: 0.00000032 	 loss = 0.0420(0.3081)
2023/12/02 18:07:51 - INFO - root -   Epoch: [123/300][240/283], lr: 0.00000032 	 loss = 0.0650(0.3161)
2023/12/02 18:08:04 - INFO - root -   Epoch: [123/300][260/283], lr: 0.00000032 	 loss = 0.0183(0.3164)
2023/12/02 18:08:12 - INFO - root -   Epoch: [123/300][280/283], lr: 0.00000032 	 loss = 1.0519(0.3088)
2023/12/02 18:08:13 - INFO - root -   Epoch: [123/300] 	 loss = 0.3072
2023/12/02 18:08:13 - INFO - root -   train_accuracy = 0.8887
2023/12/02 18:08:15 - INFO - root -   Epoch: [124/300][0/283], lr: 0.00000032 	 loss = 0.0546(0.0546)
2023/12/02 18:08:51 - INFO - root -   Epoch: [124/300][20/283], lr: 0.00000032 	 loss = 0.8141(0.4321)
2023/12/02 18:08:59 - INFO - root -   Epoch: [124/300][40/283], lr: 0.00000032 	 loss = 0.1405(0.3861)
2023/12/02 18:09:16 - INFO - root -   Epoch: [124/300][60/283], lr: 0.00000032 	 loss = 0.9596(0.4170)
2023/12/02 18:09:30 - INFO - root -   Epoch: [124/300][80/283], lr: 0.00000032 	 loss = 0.2482(0.4047)
2023/12/02 18:09:45 - INFO - root -   Epoch: [124/300][100/283], lr: 0.00000032 	 loss = 0.0558(0.3804)
2023/12/02 18:09:59 - INFO - root -   Epoch: [124/300][120/283], lr: 0.00000032 	 loss = 0.0587(0.3901)
2023/12/02 18:10:21 - INFO - root -   Epoch: [124/300][140/283], lr: 0.00000032 	 loss = 0.0496(0.3680)
2023/12/02 18:10:31 - INFO - root -   Epoch: [124/300][160/283], lr: 0.00000032 	 loss = 0.1672(0.3383)
2023/12/02 18:10:47 - INFO - root -   Epoch: [124/300][180/283], lr: 0.00000032 	 loss = 0.1634(0.3213)
2023/12/02 18:10:58 - INFO - root -   Epoch: [124/300][200/283], lr: 0.00000032 	 loss = 0.0578(0.3128)
2023/12/02 18:11:25 - INFO - root -   Epoch: [124/300][220/283], lr: 0.00000032 	 loss = 0.1270(0.3029)
2023/12/02 18:11:33 - INFO - root -   Epoch: [124/300][240/283], lr: 0.00000032 	 loss = 0.0302(0.3127)
2023/12/02 18:11:49 - INFO - root -   Epoch: [124/300][260/283], lr: 0.00000032 	 loss = 0.0119(0.3094)
2023/12/02 18:11:57 - INFO - root -   Epoch: [124/300][280/283], lr: 0.00000032 	 loss = 0.4540(0.3059)
2023/12/02 18:11:57 - INFO - root -   Epoch: [124/300] 	 loss = 0.3050
2023/12/02 18:12:07 - INFO - root -   precision = 0.8531
2023/12/02 18:12:07 - INFO - root -   eval_loss = 0.4051
2023/12/02 18:12:07 - INFO - root -   eval_acc = 0.8531
2023/12/02 18:12:07 - INFO - root -   train_accuracy = 0.8763
2023/12/02 18:12:25 - INFO - root -   Epoch: [125/300][0/283], lr: 0.00000032 	 loss = 0.2938(0.2938)
2023/12/02 18:12:47 - INFO - root -   Epoch: [125/300][20/283], lr: 0.00000032 	 loss = 0.0919(0.4114)
2023/12/02 18:12:55 - INFO - root -   Epoch: [125/300][40/283], lr: 0.00000032 	 loss = 0.3264(0.4089)
2023/12/02 18:13:27 - INFO - root -   Epoch: [125/300][60/283], lr: 0.00000032 	 loss = 1.7686(0.4761)
2023/12/02 18:13:35 - INFO - root -   Epoch: [125/300][80/283], lr: 0.00000032 	 loss = 0.4484(0.4603)
2023/12/02 18:13:48 - INFO - root -   Epoch: [125/300][100/283], lr: 0.00000032 	 loss = 0.0767(0.4300)
2023/12/02 18:13:58 - INFO - root -   Epoch: [125/300][120/283], lr: 0.00000032 	 loss = 0.9173(0.4467)
2023/12/02 18:14:15 - INFO - root -   Epoch: [125/300][140/283], lr: 0.00000032 	 loss = 0.0704(0.4137)
2023/12/02 18:14:36 - INFO - root -   Epoch: [125/300][160/283], lr: 0.00000032 	 loss = 0.1278(0.3870)
2023/12/02 18:14:54 - INFO - root -   Epoch: [125/300][180/283], lr: 0.00000032 	 loss = 0.2265(0.3696)
2023/12/02 18:15:02 - INFO - root -   Epoch: [125/300][200/283], lr: 0.00000032 	 loss = 0.2022(0.3648)
2023/12/02 18:15:10 - INFO - root -   Epoch: [125/300][220/283], lr: 0.00000032 	 loss = 0.0683(0.3562)
2023/12/02 18:15:37 - INFO - root -   Epoch: [125/300][240/283], lr: 0.00000032 	 loss = 0.1367(0.3561)
2023/12/02 18:15:52 - INFO - root -   Epoch: [125/300][260/283], lr: 0.00000032 	 loss = 0.0148(0.3583)
2023/12/02 18:16:00 - INFO - root -   Epoch: [125/300][280/283], lr: 0.00000032 	 loss = 0.4002(0.3447)
2023/12/02 18:16:00 - INFO - root -   Epoch: [125/300] 	 loss = 0.3426
2023/12/02 18:16:00 - INFO - root -   train_accuracy = 0.8534
2023/12/02 18:16:15 - INFO - root -   Epoch: [126/300][0/283], lr: 0.00000032 	 loss = 0.0983(0.0983)
2023/12/02 18:16:33 - INFO - root -   Epoch: [126/300][20/283], lr: 0.00000032 	 loss = 0.8407(0.4590)
2023/12/02 18:16:50 - INFO - root -   Epoch: [126/300][40/283], lr: 0.00000032 	 loss = 0.1742(0.4865)
2023/12/02 18:16:58 - INFO - root -   Epoch: [126/300][60/283], lr: 0.00000032 	 loss = 1.1460(0.4937)
2023/12/02 18:17:16 - INFO - root -   Epoch: [126/300][80/283], lr: 0.00000032 	 loss = 0.0705(0.4448)
2023/12/02 18:17:40 - INFO - root -   Epoch: [126/300][100/283], lr: 0.00000032 	 loss = 0.1792(0.4117)
2023/12/02 18:17:48 - INFO - root -   Epoch: [126/300][120/283], lr: 0.00000032 	 loss = 1.3600(0.4176)
2023/12/02 18:18:06 - INFO - root -   Epoch: [126/300][140/283], lr: 0.00000032 	 loss = 0.0403(0.3769)
2023/12/02 18:18:31 - INFO - root -   Epoch: [126/300][160/283], lr: 0.00000032 	 loss = 0.0247(0.3522)
2023/12/02 18:18:39 - INFO - root -   Epoch: [126/300][180/283], lr: 0.00000032 	 loss = 0.1698(0.3398)
2023/12/02 18:18:47 - INFO - root -   Epoch: [126/300][200/283], lr: 0.00000032 	 loss = 0.1865(0.3334)
2023/12/02 18:19:00 - INFO - root -   Epoch: [126/300][220/283], lr: 0.00000032 	 loss = 0.0526(0.3237)
2023/12/02 18:19:10 - INFO - root -   Epoch: [126/300][240/283], lr: 0.00000032 	 loss = 0.2915(0.3179)
2023/12/02 18:19:27 - INFO - root -   Epoch: [126/300][260/283], lr: 0.00000032 	 loss = 0.0483(0.3192)
2023/12/02 18:19:36 - INFO - root -   Epoch: [126/300][280/283], lr: 0.00000032 	 loss = 0.1273(0.3064)
2023/12/02 18:19:37 - INFO - root -   Epoch: [126/300] 	 loss = 0.3046
2023/12/02 18:19:37 - INFO - root -   train_accuracy = 0.8799
2023/12/02 18:19:39 - INFO - root -   Epoch: [127/300][0/283], lr: 0.00000032 	 loss = 0.0149(0.0149)
2023/12/02 18:20:06 - INFO - root -   Epoch: [127/300][20/283], lr: 0.00000032 	 loss = 0.2372(0.3345)
2023/12/02 18:20:20 - INFO - root -   Epoch: [127/300][40/283], lr: 0.00000032 	 loss = 0.1732(0.3976)
2023/12/02 18:20:35 - INFO - root -   Epoch: [127/300][60/283], lr: 0.00000032 	 loss = 0.8770(0.4323)
2023/12/02 18:20:48 - INFO - root -   Epoch: [127/300][80/283], lr: 0.00000032 	 loss = 0.0577(0.4276)
2023/12/02 18:21:05 - INFO - root -   Epoch: [127/300][100/283], lr: 0.00000032 	 loss = 0.1366(0.4048)
2023/12/02 18:21:16 - INFO - root -   Epoch: [127/300][120/283], lr: 0.00000032 	 loss = 0.4446(0.3923)
2023/12/02 18:21:37 - INFO - root -   Epoch: [127/300][140/283], lr: 0.00000032 	 loss = 0.0057(0.3760)
2023/12/02 18:22:00 - INFO - root -   Epoch: [127/300][160/283], lr: 0.00000032 	 loss = 0.0765(0.3577)
2023/12/02 18:22:08 - INFO - root -   Epoch: [127/300][180/283], lr: 0.00000032 	 loss = 0.2242(0.3394)
2023/12/02 18:22:31 - INFO - root -   Epoch: [127/300][200/283], lr: 0.00000032 	 loss = 0.0993(0.3339)
2023/12/02 18:22:47 - INFO - root -   Epoch: [127/300][220/283], lr: 0.00000032 	 loss = 0.1164(0.3207)
2023/12/02 18:22:55 - INFO - root -   Epoch: [127/300][240/283], lr: 0.00000032 	 loss = 0.0428(0.3159)
2023/12/02 18:23:13 - INFO - root -   Epoch: [127/300][260/283], lr: 0.00000032 	 loss = 0.1519(0.3119)
2023/12/02 18:23:24 - INFO - root -   Epoch: [127/300][280/283], lr: 0.00000032 	 loss = 0.4780(0.3054)
2023/12/02 18:23:25 - INFO - root -   Epoch: [127/300] 	 loss = 0.3038
2023/12/02 18:23:25 - INFO - root -   train_accuracy = 0.8799
2023/12/02 18:23:43 - INFO - root -   Epoch: [128/300][0/283], lr: 0.00000032 	 loss = 1.1676(1.1676)
2023/12/02 18:23:54 - INFO - root -   Epoch: [128/300][20/283], lr: 0.00000032 	 loss = 0.8132(0.4496)
2023/12/02 18:24:06 - INFO - root -   Epoch: [128/300][40/283], lr: 0.00000032 	 loss = 0.0295(0.4297)
2023/12/02 18:24:16 - INFO - root -   Epoch: [128/300][60/283], lr: 0.00000032 	 loss = 1.5115(0.4548)
2023/12/02 18:24:37 - INFO - root -   Epoch: [128/300][80/283], lr: 0.00000032 	 loss = 0.1442(0.4135)
2023/12/02 18:24:51 - INFO - root -   Epoch: [128/300][100/283], lr: 0.00000032 	 loss = 0.0689(0.3840)
2023/12/02 18:24:59 - INFO - root -   Epoch: [128/300][120/283], lr: 0.00000032 	 loss = 0.6449(0.3867)
2023/12/02 18:25:21 - INFO - root -   Epoch: [128/300][140/283], lr: 0.00000032 	 loss = 0.0374(0.3778)
2023/12/02 18:25:42 - INFO - root -   Epoch: [128/300][160/283], lr: 0.00000032 	 loss = 0.1053(0.3539)
2023/12/02 18:25:58 - INFO - root -   Epoch: [128/300][180/283], lr: 0.00000032 	 loss = 0.9861(0.3519)
2023/12/02 18:26:06 - INFO - root -   Epoch: [128/300][200/283], lr: 0.00000032 	 loss = 1.1140(0.3476)
2023/12/02 18:26:18 - INFO - root -   Epoch: [128/300][220/283], lr: 0.00000032 	 loss = 0.0603(0.3316)
2023/12/02 18:26:35 - INFO - root -   Epoch: [128/300][240/283], lr: 0.00000032 	 loss = 0.2007(0.3393)
2023/12/02 18:26:53 - INFO - root -   Epoch: [128/300][260/283], lr: 0.00000032 	 loss = 0.0431(0.3333)
2023/12/02 18:27:01 - INFO - root -   Epoch: [128/300][280/283], lr: 0.00000032 	 loss = 0.1215(0.3286)
2023/12/02 18:27:01 - INFO - root -   Epoch: [128/300] 	 loss = 0.3267
2023/12/02 18:27:01 - INFO - root -   train_accuracy = 0.8587
2023/12/02 18:27:03 - INFO - root -   Epoch: [129/300][0/283], lr: 0.00000033 	 loss = 0.1050(0.1050)
2023/12/02 18:27:32 - INFO - root -   Epoch: [129/300][20/283], lr: 0.00000033 	 loss = 0.3133(0.3140)
2023/12/02 18:27:42 - INFO - root -   Epoch: [129/300][40/283], lr: 0.00000033 	 loss = 0.2029(0.3472)
2023/12/02 18:27:58 - INFO - root -   Epoch: [129/300][60/283], lr: 0.00000033 	 loss = 1.4786(0.4107)
2023/12/02 18:28:13 - INFO - root -   Epoch: [129/300][80/283], lr: 0.00000033 	 loss = 0.6006(0.4041)
2023/12/02 18:28:24 - INFO - root -   Epoch: [129/300][100/283], lr: 0.00000033 	 loss = 0.0761(0.3945)
2023/12/02 18:28:37 - INFO - root -   Epoch: [129/300][120/283], lr: 0.00000033 	 loss = 0.5154(0.4225)
2023/12/02 18:29:04 - INFO - root -   Epoch: [129/300][140/283], lr: 0.00000033 	 loss = 0.0521(0.3973)
2023/12/02 18:29:12 - INFO - root -   Epoch: [129/300][160/283], lr: 0.00000033 	 loss = 0.7957(0.3852)
2023/12/02 18:29:20 - INFO - root -   Epoch: [129/300][180/283], lr: 0.00000033 	 loss = 0.0353(0.3598)
2023/12/02 18:29:47 - INFO - root -   Epoch: [129/300][200/283], lr: 0.00000033 	 loss = 0.1234(0.3510)
2023/12/02 18:29:55 - INFO - root -   Epoch: [129/300][220/283], lr: 0.00000033 	 loss = 0.4248(0.3450)
2023/12/02 18:30:16 - INFO - root -   Epoch: [129/300][240/283], lr: 0.00000033 	 loss = 0.2513(0.3458)
2023/12/02 18:30:30 - INFO - root -   Epoch: [129/300][260/283], lr: 0.00000033 	 loss = 0.0548(0.3406)
2023/12/02 18:30:38 - INFO - root -   Epoch: [129/300][280/283], lr: 0.00000033 	 loss = 0.5076(0.3409)
2023/12/02 18:30:38 - INFO - root -   Epoch: [129/300] 	 loss = 0.3390
2023/12/02 18:30:48 - INFO - root -   precision = 0.8462
2023/12/02 18:30:48 - INFO - root -   eval_loss = 0.4143
2023/12/02 18:30:48 - INFO - root -   eval_acc = 0.8462
2023/12/02 18:30:48 - INFO - root -   train_accuracy = 0.8640
2023/12/02 18:31:05 - INFO - root -   Epoch: [130/300][0/283], lr: 0.00000033 	 loss = 0.0774(0.0774)
2023/12/02 18:31:25 - INFO - root -   Epoch: [130/300][20/283], lr: 0.00000033 	 loss = 0.6794(0.3569)
2023/12/02 18:31:40 - INFO - root -   Epoch: [130/300][40/283], lr: 0.00000033 	 loss = 0.1199(0.3005)
2023/12/02 18:31:56 - INFO - root -   Epoch: [130/300][60/283], lr: 0.00000033 	 loss = 1.1924(0.3501)
2023/12/02 18:32:10 - INFO - root -   Epoch: [130/300][80/283], lr: 0.00000033 	 loss = 0.2191(0.3734)
2023/12/02 18:32:29 - INFO - root -   Epoch: [130/300][100/283], lr: 0.00000033 	 loss = 0.0246(0.3373)
2023/12/02 18:32:45 - INFO - root -   Epoch: [130/300][120/283], lr: 0.00000033 	 loss = 0.1562(0.3604)
2023/12/02 18:32:58 - INFO - root -   Epoch: [130/300][140/283], lr: 0.00000033 	 loss = 0.0185(0.3492)
2023/12/02 18:33:16 - INFO - root -   Epoch: [130/300][160/283], lr: 0.00000033 	 loss = 0.2228(0.3353)
2023/12/02 18:33:27 - INFO - root -   Epoch: [130/300][180/283], lr: 0.00000033 	 loss = 0.2113(0.3296)
2023/12/02 18:33:51 - INFO - root -   Epoch: [130/300][200/283], lr: 0.00000033 	 loss = 0.7577(0.3306)
2023/12/02 18:34:11 - INFO - root -   Epoch: [130/300][220/283], lr: 0.00000033 	 loss = 0.0144(0.3185)
2023/12/02 18:34:24 - INFO - root -   Epoch: [130/300][240/283], lr: 0.00000033 	 loss = 0.2673(0.3189)
2023/12/02 18:34:49 - INFO - root -   Epoch: [130/300][260/283], lr: 0.00000033 	 loss = 0.0159(0.3121)
2023/12/02 18:34:57 - INFO - root -   Epoch: [130/300][280/283], lr: 0.00000033 	 loss = 1.1062(0.3113)
2023/12/02 18:34:58 - INFO - root -   Epoch: [130/300] 	 loss = 0.3095
2023/12/02 18:34:58 - INFO - root -   train_accuracy = 0.8852
2023/12/02 18:35:13 - INFO - root -   Epoch: [131/300][0/283], lr: 0.00000033 	 loss = 1.1077(1.1077)
2023/12/02 18:35:21 - INFO - root -   Epoch: [131/300][20/283], lr: 0.00000033 	 loss = 1.5304(0.5665)
2023/12/02 18:35:29 - INFO - root -   Epoch: [131/300][40/283], lr: 0.00000033 	 loss = 0.1026(0.5012)
2023/12/02 18:35:37 - INFO - root -   Epoch: [131/300][60/283], lr: 0.00000033 	 loss = 1.3862(0.4767)
2023/12/02 18:36:00 - INFO - root -   Epoch: [131/300][80/283], lr: 0.00000033 	 loss = 0.0325(0.4486)
2023/12/02 18:36:14 - INFO - root -   Epoch: [131/300][100/283], lr: 0.00000033 	 loss = 0.3271(0.4124)
2023/12/02 18:36:29 - INFO - root -   Epoch: [131/300][120/283], lr: 0.00000033 	 loss = 0.6527(0.4167)
2023/12/02 18:36:40 - INFO - root -   Epoch: [131/300][140/283], lr: 0.00000033 	 loss = 0.1742(0.3795)
2023/12/02 18:37:00 - INFO - root -   Epoch: [131/300][160/283], lr: 0.00000033 	 loss = 0.0991(0.3625)
2023/12/02 18:37:16 - INFO - root -   Epoch: [131/300][180/283], lr: 0.00000033 	 loss = 0.9793(0.3658)
2023/12/02 18:37:36 - INFO - root -   Epoch: [131/300][200/283], lr: 0.00000033 	 loss = 0.0640(0.3534)
2023/12/02 18:37:54 - INFO - root -   Epoch: [131/300][220/283], lr: 0.00000033 	 loss = 0.0306(0.3493)
2023/12/02 18:38:08 - INFO - root -   Epoch: [131/300][240/283], lr: 0.00000033 	 loss = 0.2006(0.3431)
2023/12/02 18:38:18 - INFO - root -   Epoch: [131/300][260/283], lr: 0.00000033 	 loss = 0.0564(0.3417)
2023/12/02 18:38:29 - INFO - root -   Epoch: [131/300][280/283], lr: 0.00000033 	 loss = 0.3938(0.3363)
2023/12/02 18:38:30 - INFO - root -   Epoch: [131/300] 	 loss = 0.3347
2023/12/02 18:38:30 - INFO - root -   train_accuracy = 0.8604
2023/12/02 18:38:48 - INFO - root -   Epoch: [132/300][0/283], lr: 0.00000033 	 loss = 1.2207(1.2207)
2023/12/02 18:38:56 - INFO - root -   Epoch: [132/300][20/283], lr: 0.00000033 	 loss = 0.2861(0.5397)
2023/12/02 18:39:04 - INFO - root -   Epoch: [132/300][40/283], lr: 0.00000033 	 loss = 0.0465(0.4649)
2023/12/02 18:39:18 - INFO - root -   Epoch: [132/300][60/283], lr: 0.00000033 	 loss = 1.4067(0.4397)
2023/12/02 18:39:35 - INFO - root -   Epoch: [132/300][80/283], lr: 0.00000033 	 loss = 0.1410(0.4291)
2023/12/02 18:39:55 - INFO - root -   Epoch: [132/300][100/283], lr: 0.00000033 	 loss = 0.1526(0.4054)
2023/12/02 18:40:03 - INFO - root -   Epoch: [132/300][120/283], lr: 0.00000033 	 loss = 0.6237(0.4307)
2023/12/02 18:40:24 - INFO - root -   Epoch: [132/300][140/283], lr: 0.00000033 	 loss = 0.1740(0.3921)
2023/12/02 18:40:46 - INFO - root -   Epoch: [132/300][160/283], lr: 0.00000033 	 loss = 0.0521(0.3817)
2023/12/02 18:40:55 - INFO - root -   Epoch: [132/300][180/283], lr: 0.00000033 	 loss = 0.5775(0.3586)
2023/12/02 18:41:06 - INFO - root -   Epoch: [132/300][200/283], lr: 0.00000033 	 loss = 0.2190(0.3457)
2023/12/02 18:41:17 - INFO - root -   Epoch: [132/300][220/283], lr: 0.00000033 	 loss = 0.0395(0.3275)
2023/12/02 18:41:35 - INFO - root -   Epoch: [132/300][240/283], lr: 0.00000033 	 loss = 0.3820(0.3230)
2023/12/02 18:41:43 - INFO - root -   Epoch: [132/300][260/283], lr: 0.00000033 	 loss = 0.0098(0.3251)
2023/12/02 18:41:54 - INFO - root -   Epoch: [132/300][280/283], lr: 0.00000033 	 loss = 0.1544(0.3231)
2023/12/02 18:41:55 - INFO - root -   Epoch: [132/300] 	 loss = 0.3212
2023/12/02 18:41:55 - INFO - root -   train_accuracy = 0.8693
2023/12/02 18:41:56 - INFO - root -   Epoch: [133/300][0/283], lr: 0.00000033 	 loss = 0.0409(0.0409)
2023/12/02 18:42:29 - INFO - root -   Epoch: [133/300][20/283], lr: 0.00000033 	 loss = 0.6804(0.5833)
2023/12/02 18:42:45 - INFO - root -   Epoch: [133/300][40/283], lr: 0.00000033 	 loss = 0.0724(0.5102)
2023/12/02 18:42:53 - INFO - root -   Epoch: [133/300][60/283], lr: 0.00000033 	 loss = 0.8773(0.4951)
2023/12/02 18:43:06 - INFO - root -   Epoch: [133/300][80/283], lr: 0.00000033 	 loss = 0.0698(0.4402)
2023/12/02 18:43:16 - INFO - root -   Epoch: [133/300][100/283], lr: 0.00000033 	 loss = 0.1787(0.4346)
2023/12/02 18:43:30 - INFO - root -   Epoch: [133/300][120/283], lr: 0.00000033 	 loss = 0.3955(0.4375)
2023/12/02 18:43:49 - INFO - root -   Epoch: [133/300][140/283], lr: 0.00000033 	 loss = 0.0448(0.3982)
2023/12/02 18:44:02 - INFO - root -   Epoch: [133/300][160/283], lr: 0.00000033 	 loss = 0.2882(0.3825)
2023/12/02 18:44:17 - INFO - root -   Epoch: [133/300][180/283], lr: 0.00000033 	 loss = 0.1392(0.3568)
2023/12/02 18:44:29 - INFO - root -   Epoch: [133/300][200/283], lr: 0.00000033 	 loss = 0.1973(0.3511)
2023/12/02 18:44:52 - INFO - root -   Epoch: [133/300][220/283], lr: 0.00000033 	 loss = 0.0139(0.3514)
2023/12/02 18:45:06 - INFO - root -   Epoch: [133/300][240/283], lr: 0.00000033 	 loss = 0.0361(0.3438)
2023/12/02 18:45:15 - INFO - root -   Epoch: [133/300][260/283], lr: 0.00000033 	 loss = 0.1116(0.3445)
2023/12/02 18:45:23 - INFO - root -   Epoch: [133/300][280/283], lr: 0.00000033 	 loss = 0.1261(0.3389)
2023/12/02 18:45:24 - INFO - root -   Epoch: [133/300] 	 loss = 0.3369
2023/12/02 18:45:24 - INFO - root -   train_accuracy = 0.8463
2023/12/02 18:45:38 - INFO - root -   Epoch: [134/300][0/283], lr: 0.00000033 	 loss = 0.3054(0.3054)
2023/12/02 18:46:01 - INFO - root -   Epoch: [134/300][20/283], lr: 0.00000033 	 loss = 0.7602(0.3680)
2023/12/02 18:46:28 - INFO - root -   Epoch: [134/300][40/283], lr: 0.00000033 	 loss = 0.2359(0.3651)
2023/12/02 18:46:36 - INFO - root -   Epoch: [134/300][60/283], lr: 0.00000033 	 loss = 1.5005(0.4308)
2023/12/02 18:47:02 - INFO - root -   Epoch: [134/300][80/283], lr: 0.00000033 	 loss = 0.2074(0.4154)
2023/12/02 18:47:11 - INFO - root -   Epoch: [134/300][100/283], lr: 0.00000033 	 loss = 0.0760(0.3803)
2023/12/02 18:47:19 - INFO - root -   Epoch: [134/300][120/283], lr: 0.00000033 	 loss = 1.3362(0.3960)
2023/12/02 18:47:38 - INFO - root -   Epoch: [134/300][140/283], lr: 0.00000033 	 loss = 0.1684(0.3732)
2023/12/02 18:47:46 - INFO - root -   Epoch: [134/300][160/283], lr: 0.00000033 	 loss = 0.1133(0.3510)
2023/12/02 18:48:05 - INFO - root -   Epoch: [134/300][180/283], lr: 0.00000033 	 loss = 0.2000(0.3416)
2023/12/02 18:48:14 - INFO - root -   Epoch: [134/300][200/283], lr: 0.00000033 	 loss = 0.1081(0.3331)
2023/12/02 18:48:33 - INFO - root -   Epoch: [134/300][220/283], lr: 0.00000033 	 loss = 0.0188(0.3154)
2023/12/02 18:48:41 - INFO - root -   Epoch: [134/300][240/283], lr: 0.00000033 	 loss = 0.0298(0.3107)
2023/12/02 18:48:49 - INFO - root -   Epoch: [134/300][260/283], lr: 0.00000033 	 loss = 0.1092(0.3056)
2023/12/02 18:49:04 - INFO - root -   Epoch: [134/300][280/283], lr: 0.00000033 	 loss = 0.4127(0.3014)
2023/12/02 18:49:05 - INFO - root -   Epoch: [134/300] 	 loss = 0.2997
2023/12/02 18:49:15 - INFO - root -   precision = 0.8252
2023/12/02 18:49:15 - INFO - root -   eval_loss = 0.4431
2023/12/02 18:49:15 - INFO - root -   eval_acc = 0.8252
2023/12/02 18:49:15 - INFO - root -   train_accuracy = 0.8640
2023/12/02 18:49:31 - INFO - root -   Epoch: [135/300][0/283], lr: 0.00000034 	 loss = 0.2636(0.2636)
2023/12/02 18:49:43 - INFO - root -   Epoch: [135/300][20/283], lr: 0.00000034 	 loss = 0.3371(0.2557)
2023/12/02 18:49:54 - INFO - root -   Epoch: [135/300][40/283], lr: 0.00000034 	 loss = 0.0714(0.3364)
2023/12/02 18:50:10 - INFO - root -   Epoch: [135/300][60/283], lr: 0.00000034 	 loss = 0.5274(0.3873)
2023/12/02 18:50:23 - INFO - root -   Epoch: [135/300][80/283], lr: 0.00000034 	 loss = 0.0348(0.3759)
2023/12/02 18:50:40 - INFO - root -   Epoch: [135/300][100/283], lr: 0.00000034 	 loss = 0.1795(0.3528)
2023/12/02 18:50:56 - INFO - root -   Epoch: [135/300][120/283], lr: 0.00000034 	 loss = 1.5325(0.3892)
2023/12/02 18:51:17 - INFO - root -   Epoch: [135/300][140/283], lr: 0.00000034 	 loss = 0.0385(0.3679)
2023/12/02 18:51:36 - INFO - root -   Epoch: [135/300][160/283], lr: 0.00000034 	 loss = 0.0632(0.3536)
2023/12/02 18:51:44 - INFO - root -   Epoch: [135/300][180/283], lr: 0.00000034 	 loss = 0.1120(0.3362)
2023/12/02 18:52:03 - INFO - root -   Epoch: [135/300][200/283], lr: 0.00000034 	 loss = 0.0338(0.3260)
2023/12/02 18:52:11 - INFO - root -   Epoch: [135/300][220/283], lr: 0.00000034 	 loss = 0.0189(0.3231)
2023/12/02 18:52:27 - INFO - root -   Epoch: [135/300][240/283], lr: 0.00000034 	 loss = 0.0764(0.3255)
2023/12/02 18:52:38 - INFO - root -   Epoch: [135/300][260/283], lr: 0.00000034 	 loss = 0.1553(0.3184)
2023/12/02 18:52:46 - INFO - root -   Epoch: [135/300][280/283], lr: 0.00000034 	 loss = 1.7322(0.3099)
2023/12/02 18:52:46 - INFO - root -   Epoch: [135/300] 	 loss = 0.3085
2023/12/02 18:52:46 - INFO - root -   train_accuracy = 0.8781
2023/12/02 18:52:49 - INFO - root -   Epoch: [136/300][0/283], lr: 0.00000034 	 loss = 0.0636(0.0636)
2023/12/02 18:53:25 - INFO - root -   Epoch: [136/300][20/283], lr: 0.00000034 	 loss = 0.1265(0.4907)
2023/12/02 18:53:38 - INFO - root -   Epoch: [136/300][40/283], lr: 0.00000034 	 loss = 0.1224(0.4106)
2023/12/02 18:53:53 - INFO - root -   Epoch: [136/300][60/283], lr: 0.00000034 	 loss = 3.3837(0.4259)
2023/12/02 18:54:01 - INFO - root -   Epoch: [136/300][80/283], lr: 0.00000034 	 loss = 0.7045(0.4108)
2023/12/02 18:54:19 - INFO - root -   Epoch: [136/300][100/283], lr: 0.00000034 	 loss = 0.2367(0.3802)
2023/12/02 18:54:31 - INFO - root -   Epoch: [136/300][120/283], lr: 0.00000034 	 loss = 0.6232(0.3725)
2023/12/02 18:54:42 - INFO - root -   Epoch: [136/300][140/283], lr: 0.00000034 	 loss = 0.0110(0.3442)
2023/12/02 18:55:01 - INFO - root -   Epoch: [136/300][160/283], lr: 0.00000034 	 loss = 0.0456(0.3228)
2023/12/02 18:55:21 - INFO - root -   Epoch: [136/300][180/283], lr: 0.00000034 	 loss = 0.0342(0.3092)
2023/12/02 18:55:29 - INFO - root -   Epoch: [136/300][200/283], lr: 0.00000034 	 loss = 0.3337(0.2969)
2023/12/02 18:55:47 - INFO - root -   Epoch: [136/300][220/283], lr: 0.00000034 	 loss = 0.1980(0.2938)
2023/12/02 18:56:00 - INFO - root -   Epoch: [136/300][240/283], lr: 0.00000034 	 loss = 0.1108(0.3007)
2023/12/02 18:56:10 - INFO - root -   Epoch: [136/300][260/283], lr: 0.00000034 	 loss = 0.0763(0.2950)
2023/12/02 18:56:18 - INFO - root -   Epoch: [136/300][280/283], lr: 0.00000034 	 loss = 0.9895(0.2918)
2023/12/02 18:56:19 - INFO - root -   Epoch: [136/300] 	 loss = 0.2899
2023/12/02 18:56:19 - INFO - root -   train_accuracy = 0.8958
2023/12/02 18:56:31 - INFO - root -   Epoch: [137/300][0/283], lr: 0.00000034 	 loss = 0.1221(0.1221)
2023/12/02 18:56:43 - INFO - root -   Epoch: [137/300][20/283], lr: 0.00000034 	 loss = 0.1357(0.4217)
2023/12/02 18:56:56 - INFO - root -   Epoch: [137/300][40/283], lr: 0.00000034 	 loss = 0.0728(0.3936)
2023/12/02 18:57:04 - INFO - root -   Epoch: [137/300][60/283], lr: 0.00000034 	 loss = 1.5927(0.4131)
2023/12/02 18:57:32 - INFO - root -   Epoch: [137/300][80/283], lr: 0.00000034 	 loss = 1.1340(0.4058)
2023/12/02 18:57:40 - INFO - root -   Epoch: [137/300][100/283], lr: 0.00000034 	 loss = 0.1587(0.3701)
2023/12/02 18:57:57 - INFO - root -   Epoch: [137/300][120/283], lr: 0.00000034 	 loss = 1.5304(0.4041)
2023/12/02 18:58:06 - INFO - root -   Epoch: [137/300][140/283], lr: 0.00000034 	 loss = 0.0527(0.3765)
2023/12/02 18:58:29 - INFO - root -   Epoch: [137/300][160/283], lr: 0.00000034 	 loss = 0.0200(0.3513)
2023/12/02 18:58:37 - INFO - root -   Epoch: [137/300][180/283], lr: 0.00000034 	 loss = 0.6245(0.3439)
2023/12/02 18:58:57 - INFO - root -   Epoch: [137/300][200/283], lr: 0.00000034 	 loss = 0.1597(0.3338)
2023/12/02 18:59:05 - INFO - root -   Epoch: [137/300][220/283], lr: 0.00000034 	 loss = 0.0153(0.3196)
2023/12/02 18:59:27 - INFO - root -   Epoch: [137/300][240/283], lr: 0.00000034 	 loss = 0.1168(0.3174)
2023/12/02 18:59:35 - INFO - root -   Epoch: [137/300][260/283], lr: 0.00000034 	 loss = 0.0043(0.3096)
2023/12/02 18:59:43 - INFO - root -   Epoch: [137/300][280/283], lr: 0.00000034 	 loss = 0.1218(0.3035)
2023/12/02 18:59:44 - INFO - root -   Epoch: [137/300] 	 loss = 0.3017
2023/12/02 18:59:44 - INFO - root -   train_accuracy = 0.8799
2023/12/02 19:00:00 - INFO - root -   Epoch: [138/300][0/283], lr: 0.00000034 	 loss = 0.1411(0.1411)
2023/12/02 19:00:17 - INFO - root -   Epoch: [138/300][20/283], lr: 0.00000034 	 loss = 0.6922(0.4307)
2023/12/02 19:00:36 - INFO - root -   Epoch: [138/300][40/283], lr: 0.00000034 	 loss = 0.3700(0.3442)
2023/12/02 19:00:53 - INFO - root -   Epoch: [138/300][60/283], lr: 0.00000034 	 loss = 2.4777(0.3430)
2023/12/02 19:01:06 - INFO - root -   Epoch: [138/300][80/283], lr: 0.00000034 	 loss = 0.6697(0.3556)
2023/12/02 19:01:17 - INFO - root -   Epoch: [138/300][100/283], lr: 0.00000034 	 loss = 0.2634(0.3590)
2023/12/02 19:01:40 - INFO - root -   Epoch: [138/300][120/283], lr: 0.00000034 	 loss = 0.2971(0.3842)
2023/12/02 19:01:51 - INFO - root -   Epoch: [138/300][140/283], lr: 0.00000034 	 loss = 0.0087(0.3675)
2023/12/02 19:01:59 - INFO - root -   Epoch: [138/300][160/283], lr: 0.00000034 	 loss = 0.7341(0.3425)
2023/12/02 19:02:16 - INFO - root -   Epoch: [138/300][180/283], lr: 0.00000034 	 loss = 0.0811(0.3256)
2023/12/02 19:02:30 - INFO - root -   Epoch: [138/300][200/283], lr: 0.00000034 	 loss = 0.3949(0.3213)
2023/12/02 19:02:40 - INFO - root -   Epoch: [138/300][220/283], lr: 0.00000034 	 loss = 0.0033(0.3164)
2023/12/02 19:02:57 - INFO - root -   Epoch: [138/300][240/283], lr: 0.00000034 	 loss = 0.1566(0.3229)
2023/12/02 19:03:07 - INFO - root -   Epoch: [138/300][260/283], lr: 0.00000034 	 loss = 0.0290(0.3173)
2023/12/02 19:03:14 - INFO - root -   Epoch: [138/300][280/283], lr: 0.00000034 	 loss = 0.1717(0.3140)
2023/12/02 19:03:15 - INFO - root -   Epoch: [138/300] 	 loss = 0.3130
2023/12/02 19:03:15 - INFO - root -   train_accuracy = 0.8763
2023/12/02 19:03:31 - INFO - root -   Epoch: [139/300][0/283], lr: 0.00000034 	 loss = 0.1199(0.1199)
2023/12/02 19:03:47 - INFO - root -   Epoch: [139/300][20/283], lr: 0.00000034 	 loss = 0.2812(0.3043)
2023/12/02 19:04:03 - INFO - root -   Epoch: [139/300][40/283], lr: 0.00000034 	 loss = 0.1583(0.3627)
2023/12/02 19:04:12 - INFO - root -   Epoch: [139/300][60/283], lr: 0.00000034 	 loss = 0.7983(0.3737)
2023/12/02 19:04:33 - INFO - root -   Epoch: [139/300][80/283], lr: 0.00000034 	 loss = 0.3839(0.3986)
2023/12/02 19:04:47 - INFO - root -   Epoch: [139/300][100/283], lr: 0.00000034 	 loss = 0.0636(0.3641)
2023/12/02 19:05:02 - INFO - root -   Epoch: [139/300][120/283], lr: 0.00000034 	 loss = 1.1132(0.3907)
2023/12/02 19:05:19 - INFO - root -   Epoch: [139/300][140/283], lr: 0.00000034 	 loss = 0.0869(0.3536)
2023/12/02 19:05:36 - INFO - root -   Epoch: [139/300][160/283], lr: 0.00000034 	 loss = 0.1932(0.3399)
2023/12/02 19:05:44 - INFO - root -   Epoch: [139/300][180/283], lr: 0.00000034 	 loss = 0.0456(0.3249)
2023/12/02 19:06:03 - INFO - root -   Epoch: [139/300][200/283], lr: 0.00000034 	 loss = 0.0536(0.3133)
2023/12/02 19:06:11 - INFO - root -   Epoch: [139/300][220/283], lr: 0.00000034 	 loss = 0.0054(0.2993)
2023/12/02 19:06:23 - INFO - root -   Epoch: [139/300][240/283], lr: 0.00000034 	 loss = 0.0452(0.3113)
2023/12/02 19:06:43 - INFO - root -   Epoch: [139/300][260/283], lr: 0.00000034 	 loss = 0.1570(0.3035)
2023/12/02 19:06:51 - INFO - root -   Epoch: [139/300][280/283], lr: 0.00000034 	 loss = 0.6174(0.2999)
2023/12/02 19:06:51 - INFO - root -   Epoch: [139/300] 	 loss = 0.2994
2023/12/02 19:07:01 - INFO - root -   precision = 0.8462
2023/12/02 19:07:01 - INFO - root -   eval_loss = 0.4238
2023/12/02 19:07:01 - INFO - root -   eval_acc = 0.8462
2023/12/02 19:07:01 - INFO - root -   train_accuracy = 0.8852
2023/12/02 19:07:18 - INFO - root -   Epoch: [140/300][0/283], lr: 0.00000034 	 loss = 0.3566(0.3566)
2023/12/02 19:07:41 - INFO - root -   Epoch: [140/300][20/283], lr: 0.00000034 	 loss = 0.3815(0.3514)
2023/12/02 19:07:53 - INFO - root -   Epoch: [140/300][40/283], lr: 0.00000034 	 loss = 0.1710(0.3484)
2023/12/02 19:08:09 - INFO - root -   Epoch: [140/300][60/283], lr: 0.00000034 	 loss = 1.4347(0.3813)
2023/12/02 19:08:29 - INFO - root -   Epoch: [140/300][80/283], lr: 0.00000034 	 loss = 0.1760(0.3692)
2023/12/02 19:08:37 - INFO - root -   Epoch: [140/300][100/283], lr: 0.00000034 	 loss = 0.0349(0.3715)
2023/12/02 19:08:45 - INFO - root -   Epoch: [140/300][120/283], lr: 0.00000034 	 loss = 1.0092(0.3741)
2023/12/02 19:08:56 - INFO - root -   Epoch: [140/300][140/283], lr: 0.00000034 	 loss = 0.2441(0.3468)
2023/12/02 19:09:16 - INFO - root -   Epoch: [140/300][160/283], lr: 0.00000034 	 loss = 0.1072(0.3254)
2023/12/02 19:09:34 - INFO - root -   Epoch: [140/300][180/283], lr: 0.00000034 	 loss = 0.1495(0.3086)
2023/12/02 19:09:42 - INFO - root -   Epoch: [140/300][200/283], lr: 0.00000034 	 loss = 0.1236(0.3013)
2023/12/02 19:09:56 - INFO - root -   Epoch: [140/300][220/283], lr: 0.00000034 	 loss = 0.0065(0.2854)
2023/12/02 19:10:28 - INFO - root -   Epoch: [140/300][240/283], lr: 0.00000034 	 loss = 0.1204(0.2841)
2023/12/02 19:10:36 - INFO - root -   Epoch: [140/300][260/283], lr: 0.00000034 	 loss = 0.0206(0.2852)
2023/12/02 19:10:44 - INFO - root -   Epoch: [140/300][280/283], lr: 0.00000034 	 loss = 0.3011(0.2795)
2023/12/02 19:10:45 - INFO - root -   Epoch: [140/300] 	 loss = 0.2777
2023/12/02 19:10:45 - INFO - root -   train_accuracy = 0.8869
2023/12/02 19:10:47 - INFO - root -   Epoch: [141/300][0/283], lr: 0.00000035 	 loss = 0.3382(0.3382)
2023/12/02 19:11:17 - INFO - root -   Epoch: [141/300][20/283], lr: 0.00000035 	 loss = 1.3080(0.4895)
2023/12/02 19:11:32 - INFO - root -   Epoch: [141/300][40/283], lr: 0.00000035 	 loss = 0.1052(0.4200)
2023/12/02 19:11:48 - INFO - root -   Epoch: [141/300][60/283], lr: 0.00000035 	 loss = 0.3757(0.3942)
2023/12/02 19:12:16 - INFO - root -   Epoch: [141/300][80/283], lr: 0.00000035 	 loss = 0.5607(0.4082)
2023/12/02 19:12:24 - INFO - root -   Epoch: [141/300][100/283], lr: 0.00000035 	 loss = 0.0882(0.3887)
2023/12/02 19:12:44 - INFO - root -   Epoch: [141/300][120/283], lr: 0.00000035 	 loss = 0.7787(0.4183)
2023/12/02 19:12:59 - INFO - root -   Epoch: [141/300][140/283], lr: 0.00000035 	 loss = 0.0207(0.3897)
2023/12/02 19:13:11 - INFO - root -   Epoch: [141/300][160/283], lr: 0.00000035 	 loss = 0.8913(0.3601)
2023/12/02 19:13:23 - INFO - root -   Epoch: [141/300][180/283], lr: 0.00000035 	 loss = 0.4763(0.3461)
2023/12/02 19:13:35 - INFO - root -   Epoch: [141/300][200/283], lr: 0.00000035 	 loss = 0.1271(0.3381)
2023/12/02 19:13:49 - INFO - root -   Epoch: [141/300][220/283], lr: 0.00000035 	 loss = 0.2575(0.3273)
2023/12/02 19:14:10 - INFO - root -   Epoch: [141/300][240/283], lr: 0.00000035 	 loss = 0.2940(0.3282)
2023/12/02 19:14:20 - INFO - root -   Epoch: [141/300][260/283], lr: 0.00000035 	 loss = 0.0199(0.3216)
2023/12/02 19:14:28 - INFO - root -   Epoch: [141/300][280/283], lr: 0.00000035 	 loss = 0.1214(0.3153)
2023/12/02 19:14:29 - INFO - root -   Epoch: [141/300] 	 loss = 0.3135
2023/12/02 19:14:29 - INFO - root -   train_accuracy = 0.8675
2023/12/02 19:14:43 - INFO - root -   Epoch: [142/300][0/283], lr: 0.00000035 	 loss = 0.1724(0.1724)
2023/12/02 19:14:59 - INFO - root -   Epoch: [142/300][20/283], lr: 0.00000035 	 loss = 0.2911(0.4564)
2023/12/02 19:15:07 - INFO - root -   Epoch: [142/300][40/283], lr: 0.00000035 	 loss = 0.2606(0.3516)
2023/12/02 19:15:29 - INFO - root -   Epoch: [142/300][60/283], lr: 0.00000035 	 loss = 0.7576(0.3907)
2023/12/02 19:15:39 - INFO - root -   Epoch: [142/300][80/283], lr: 0.00000035 	 loss = 0.2110(0.3783)
2023/12/02 19:16:05 - INFO - root -   Epoch: [142/300][100/283], lr: 0.00000035 	 loss = 0.0369(0.3542)
2023/12/02 19:16:17 - INFO - root -   Epoch: [142/300][120/283], lr: 0.00000035 	 loss = 0.7727(0.3758)
2023/12/02 19:16:25 - INFO - root -   Epoch: [142/300][140/283], lr: 0.00000035 	 loss = 0.0325(0.3431)
2023/12/02 19:16:34 - INFO - root -   Epoch: [142/300][160/283], lr: 0.00000035 	 loss = 0.4131(0.3213)
2023/12/02 19:16:59 - INFO - root -   Epoch: [142/300][180/283], lr: 0.00000035 	 loss = 1.2843(0.3076)
2023/12/02 19:17:07 - INFO - root -   Epoch: [142/300][200/283], lr: 0.00000035 	 loss = 1.3113(0.3125)
2023/12/02 19:17:21 - INFO - root -   Epoch: [142/300][220/283], lr: 0.00000035 	 loss = 0.0497(0.2956)
2023/12/02 19:17:39 - INFO - root -   Epoch: [142/300][240/283], lr: 0.00000035 	 loss = 0.0688(0.2917)
2023/12/02 19:17:55 - INFO - root -   Epoch: [142/300][260/283], lr: 0.00000035 	 loss = 0.1737(0.2898)
2023/12/02 19:18:03 - INFO - root -   Epoch: [142/300][280/283], lr: 0.00000035 	 loss = 1.4146(0.2869)
2023/12/02 19:18:04 - INFO - root -   Epoch: [142/300] 	 loss = 0.2869
2023/12/02 19:18:04 - INFO - root -   train_accuracy = 0.8958
2023/12/02 19:18:06 - INFO - root -   Epoch: [143/300][0/283], lr: 0.00000035 	 loss = 0.0321(0.0321)
2023/12/02 19:18:33 - INFO - root -   Epoch: [143/300][20/283], lr: 0.00000035 	 loss = 0.2241(0.3913)
2023/12/02 19:18:49 - INFO - root -   Epoch: [143/300][40/283], lr: 0.00000035 	 loss = 0.2945(0.3609)
2023/12/02 19:19:04 - INFO - root -   Epoch: [143/300][60/283], lr: 0.00000035 	 loss = 0.8550(0.4324)
2023/12/02 19:19:21 - INFO - root -   Epoch: [143/300][80/283], lr: 0.00000035 	 loss = 0.0251(0.4194)
2023/12/02 19:19:30 - INFO - root -   Epoch: [143/300][100/283], lr: 0.00000035 	 loss = 0.1416(0.4170)
2023/12/02 19:19:46 - INFO - root -   Epoch: [143/300][120/283], lr: 0.00000035 	 loss = 0.1851(0.4318)
2023/12/02 19:20:09 - INFO - root -   Epoch: [143/300][140/283], lr: 0.00000035 	 loss = 0.1075(0.3857)
2023/12/02 19:20:27 - INFO - root -   Epoch: [143/300][160/283], lr: 0.00000035 	 loss = 0.0331(0.3718)
2023/12/02 19:20:42 - INFO - root -   Epoch: [143/300][180/283], lr: 0.00000035 	 loss = 0.0631(0.3484)
2023/12/02 19:21:05 - INFO - root -   Epoch: [143/300][200/283], lr: 0.00000035 	 loss = 0.9418(0.3353)
2023/12/02 19:21:17 - INFO - root -   Epoch: [143/300][220/283], lr: 0.00000035 	 loss = 0.0499(0.3272)
2023/12/02 19:21:25 - INFO - root -   Epoch: [143/300][240/283], lr: 0.00000035 	 loss = 0.0499(0.3310)
2023/12/02 19:21:33 - INFO - root -   Epoch: [143/300][260/283], lr: 0.00000035 	 loss = 0.0258(0.3270)
2023/12/02 19:21:44 - INFO - root -   Epoch: [143/300][280/283], lr: 0.00000035 	 loss = 0.7808(0.3240)
2023/12/02 19:21:45 - INFO - root -   Epoch: [143/300] 	 loss = 0.3225
2023/12/02 19:21:45 - INFO - root -   train_accuracy = 0.8587
2023/12/02 19:22:01 - INFO - root -   Epoch: [144/300][0/283], lr: 0.00000035 	 loss = 0.5593(0.5593)
2023/12/02 19:22:15 - INFO - root -   Epoch: [144/300][20/283], lr: 0.00000035 	 loss = 0.3832(0.3217)
2023/12/02 19:22:42 - INFO - root -   Epoch: [144/300][40/283], lr: 0.00000035 	 loss = 0.1603(0.3255)
2023/12/02 19:22:50 - INFO - root -   Epoch: [144/300][60/283], lr: 0.00000035 	 loss = 1.2159(0.3849)
2023/12/02 19:23:06 - INFO - root -   Epoch: [144/300][80/283], lr: 0.00000035 	 loss = 0.1999(0.3657)
2023/12/02 19:23:14 - INFO - root -   Epoch: [144/300][100/283], lr: 0.00000035 	 loss = 0.1132(0.3602)
2023/12/02 19:23:36 - INFO - root -   Epoch: [144/300][120/283], lr: 0.00000035 	 loss = 1.5288(0.3874)
2023/12/02 19:23:49 - INFO - root -   Epoch: [144/300][140/283], lr: 0.00000035 	 loss = 0.1226(0.3525)
2023/12/02 19:24:03 - INFO - root -   Epoch: [144/300][160/283], lr: 0.00000035 	 loss = 0.8015(0.3316)
2023/12/02 19:24:14 - INFO - root -   Epoch: [144/300][180/283], lr: 0.00000035 	 loss = 0.0243(0.3150)
2023/12/02 19:24:41 - INFO - root -   Epoch: [144/300][200/283], lr: 0.00000035 	 loss = 0.1405(0.3261)
2023/12/02 19:24:54 - INFO - root -   Epoch: [144/300][220/283], lr: 0.00000035 	 loss = 0.0102(0.3120)
2023/12/02 19:25:08 - INFO - root -   Epoch: [144/300][240/283], lr: 0.00000035 	 loss = 0.0098(0.3153)
2023/12/02 19:25:21 - INFO - root -   Epoch: [144/300][260/283], lr: 0.00000035 	 loss = 0.0083(0.3119)
2023/12/02 19:25:29 - INFO - root -   Epoch: [144/300][280/283], lr: 0.00000035 	 loss = 0.8524(0.3078)
2023/12/02 19:25:30 - INFO - root -   Epoch: [144/300] 	 loss = 0.3063
2023/12/02 19:25:40 - INFO - root -   precision = 0.8252
2023/12/02 19:25:40 - INFO - root -   eval_loss = 0.4724
2023/12/02 19:25:40 - INFO - root -   eval_acc = 0.8252
2023/12/02 19:25:40 - INFO - root -   train_accuracy = 0.8887
2023/12/02 19:25:56 - INFO - root -   Epoch: [145/300][0/283], lr: 0.00000035 	 loss = 1.0156(1.0156)
2023/12/02 19:26:04 - INFO - root -   Epoch: [145/300][20/283], lr: 0.00000035 	 loss = 0.2522(0.2821)
2023/12/02 19:26:21 - INFO - root -   Epoch: [145/300][40/283], lr: 0.00000035 	 loss = 0.1267(0.3690)
2023/12/02 19:26:29 - INFO - root -   Epoch: [145/300][60/283], lr: 0.00000035 	 loss = 1.5309(0.3687)
2023/12/02 19:26:37 - INFO - root -   Epoch: [145/300][80/283], lr: 0.00000035 	 loss = 0.0320(0.3462)
2023/12/02 19:26:56 - INFO - root -   Epoch: [145/300][100/283], lr: 0.00000035 	 loss = 0.1608(0.3254)
2023/12/02 19:27:04 - INFO - root -   Epoch: [145/300][120/283], lr: 0.00000035 	 loss = 1.5501(0.3455)
2023/12/02 19:27:13 - INFO - root -   Epoch: [145/300][140/283], lr: 0.00000035 	 loss = 0.1814(0.3228)
2023/12/02 19:27:25 - INFO - root -   Epoch: [145/300][160/283], lr: 0.00000035 	 loss = 0.8102(0.3098)
2023/12/02 19:27:47 - INFO - root -   Epoch: [145/300][180/283], lr: 0.00000035 	 loss = 0.2994(0.2983)
2023/12/02 19:27:56 - INFO - root -   Epoch: [145/300][200/283], lr: 0.00000035 	 loss = 0.0391(0.2861)
2023/12/02 19:28:10 - INFO - root -   Epoch: [145/300][220/283], lr: 0.00000035 	 loss = 0.2804(0.2916)
2023/12/02 19:28:27 - INFO - root -   Epoch: [145/300][240/283], lr: 0.00000035 	 loss = 0.0735(0.2967)
2023/12/02 19:28:45 - INFO - root -   Epoch: [145/300][260/283], lr: 0.00000035 	 loss = 0.2825(0.2962)
2023/12/02 19:28:53 - INFO - root -   Epoch: [145/300][280/283], lr: 0.00000035 	 loss = 0.7027(0.2881)
2023/12/02 19:28:54 - INFO - root -   Epoch: [145/300] 	 loss = 0.2867
2023/12/02 19:28:54 - INFO - root -   train_accuracy = 0.8869
2023/12/02 19:29:09 - INFO - root -   Epoch: [146/300][0/283], lr: 0.00000036 	 loss = 0.0176(0.0176)
2023/12/02 19:29:34 - INFO - root -   Epoch: [146/300][20/283], lr: 0.00000036 	 loss = 0.3417(0.2213)
2023/12/02 19:29:42 - INFO - root -   Epoch: [146/300][40/283], lr: 0.00000036 	 loss = 0.0909(0.2693)
2023/12/02 19:29:57 - INFO - root -   Epoch: [146/300][60/283], lr: 0.00000036 	 loss = 1.4616(0.3240)
2023/12/02 19:30:05 - INFO - root -   Epoch: [146/300][80/283], lr: 0.00000036 	 loss = 0.3322(0.3538)
2023/12/02 19:30:18 - INFO - root -   Epoch: [146/300][100/283], lr: 0.00000036 	 loss = 0.2093(0.3218)
2023/12/02 19:30:38 - INFO - root -   Epoch: [146/300][120/283], lr: 0.00000036 	 loss = 1.8085(0.3629)
2023/12/02 19:30:56 - INFO - root -   Epoch: [146/300][140/283], lr: 0.00000036 	 loss = 0.0314(0.3357)
2023/12/02 19:31:04 - INFO - root -   Epoch: [146/300][160/283], lr: 0.00000036 	 loss = 0.0547(0.3138)
2023/12/02 19:31:23 - INFO - root -   Epoch: [146/300][180/283], lr: 0.00000036 	 loss = 0.1867(0.2952)
2023/12/02 19:31:40 - INFO - root -   Epoch: [146/300][200/283], lr: 0.00000036 	 loss = 0.7383(0.2865)
2023/12/02 19:31:48 - INFO - root -   Epoch: [146/300][220/283], lr: 0.00000036 	 loss = 0.0046(0.2808)
2023/12/02 19:32:07 - INFO - root -   Epoch: [146/300][240/283], lr: 0.00000036 	 loss = 0.2275(0.2846)
2023/12/02 19:32:22 - INFO - root -   Epoch: [146/300][260/283], lr: 0.00000036 	 loss = 0.1912(0.2820)
2023/12/02 19:32:30 - INFO - root -   Epoch: [146/300][280/283], lr: 0.00000036 	 loss = 0.7007(0.2741)
2023/12/02 19:32:31 - INFO - root -   Epoch: [146/300] 	 loss = 0.2733
2023/12/02 19:32:31 - INFO - root -   train_accuracy = 0.9011
2023/12/02 19:32:32 - INFO - root -   Epoch: [147/300][0/283], lr: 0.00000036 	 loss = 0.0165(0.0165)
2023/12/02 19:33:06 - INFO - root -   Epoch: [147/300][20/283], lr: 0.00000036 	 loss = 0.2669(0.3041)
2023/12/02 19:33:22 - INFO - root -   Epoch: [147/300][40/283], lr: 0.00000036 	 loss = 0.0290(0.3246)
2023/12/02 19:33:46 - INFO - root -   Epoch: [147/300][60/283], lr: 0.00000036 	 loss = 1.2161(0.3800)
2023/12/02 19:34:02 - INFO - root -   Epoch: [147/300][80/283], lr: 0.00000036 	 loss = 0.1620(0.3892)
2023/12/02 19:34:10 - INFO - root -   Epoch: [147/300][100/283], lr: 0.00000036 	 loss = 0.0759(0.3866)
2023/12/02 19:34:26 - INFO - root -   Epoch: [147/300][120/283], lr: 0.00000036 	 loss = 0.2402(0.4040)
2023/12/02 19:34:45 - INFO - root -   Epoch: [147/300][140/283], lr: 0.00000036 	 loss = 0.0791(0.3632)
2023/12/02 19:35:15 - INFO - root -   Epoch: [147/300][160/283], lr: 0.00000036 	 loss = 0.3294(0.3568)
2023/12/02 19:35:23 - INFO - root -   Epoch: [147/300][180/283], lr: 0.00000036 	 loss = 0.7557(0.3462)
2023/12/02 19:35:30 - INFO - root -   Epoch: [147/300][200/283], lr: 0.00000036 	 loss = 1.0620(0.3376)
2023/12/02 19:35:49 - INFO - root -   Epoch: [147/300][220/283], lr: 0.00000036 	 loss = 0.0057(0.3385)
2023/12/02 19:35:57 - INFO - root -   Epoch: [147/300][240/283], lr: 0.00000036 	 loss = 0.1628(0.3378)
2023/12/02 19:36:12 - INFO - root -   Epoch: [147/300][260/283], lr: 0.00000036 	 loss = 0.0108(0.3256)
2023/12/02 19:36:20 - INFO - root -   Epoch: [147/300][280/283], lr: 0.00000036 	 loss = 0.4581(0.3193)
2023/12/02 19:36:21 - INFO - root -   Epoch: [147/300] 	 loss = 0.3173
2023/12/02 19:36:21 - INFO - root -   train_accuracy = 0.8746
2023/12/02 19:36:34 - INFO - root -   Epoch: [148/300][0/283], lr: 0.00000036 	 loss = 0.1858(0.1858)
2023/12/02 19:36:48 - INFO - root -   Epoch: [148/300][20/283], lr: 0.00000036 	 loss = 1.7256(0.3762)
2023/12/02 19:37:03 - INFO - root -   Epoch: [148/300][40/283], lr: 0.00000036 	 loss = 0.1765(0.4364)
2023/12/02 19:37:11 - INFO - root -   Epoch: [148/300][60/283], lr: 0.00000036 	 loss = 1.7135(0.4134)
2023/12/02 19:37:30 - INFO - root -   Epoch: [148/300][80/283], lr: 0.00000036 	 loss = 0.0709(0.3898)
2023/12/02 19:37:44 - INFO - root -   Epoch: [148/300][100/283], lr: 0.00000036 	 loss = 0.0845(0.3627)
2023/12/02 19:38:03 - INFO - root -   Epoch: [148/300][120/283], lr: 0.00000036 	 loss = 0.2531(0.3778)
2023/12/02 19:38:19 - INFO - root -   Epoch: [148/300][140/283], lr: 0.00000036 	 loss = 0.0079(0.3445)
2023/12/02 19:38:27 - INFO - root -   Epoch: [148/300][160/283], lr: 0.00000036 	 loss = 0.0745(0.3284)
2023/12/02 19:38:48 - INFO - root -   Epoch: [148/300][180/283], lr: 0.00000036 	 loss = 1.1718(0.3148)
2023/12/02 19:39:17 - INFO - root -   Epoch: [148/300][200/283], lr: 0.00000036 	 loss = 1.0847(0.3151)
2023/12/02 19:39:25 - INFO - root -   Epoch: [148/300][220/283], lr: 0.00000036 	 loss = 0.0666(0.3017)
2023/12/02 19:39:48 - INFO - root -   Epoch: [148/300][240/283], lr: 0.00000036 	 loss = 0.0985(0.2969)
2023/12/02 19:40:02 - INFO - root -   Epoch: [148/300][260/283], lr: 0.00000036 	 loss = 0.1428(0.2985)
2023/12/02 19:40:13 - INFO - root -   Epoch: [148/300][280/283], lr: 0.00000036 	 loss = 0.9510(0.3032)
2023/12/02 19:40:14 - INFO - root -   Epoch: [148/300] 	 loss = 0.3018
2023/12/02 19:40:14 - INFO - root -   train_accuracy = 0.8799
2023/12/02 19:40:15 - INFO - root -   Epoch: [149/300][0/283], lr: 0.00000036 	 loss = 0.0244(0.0244)
2023/12/02 19:40:39 - INFO - root -   Epoch: [149/300][20/283], lr: 0.00000036 	 loss = 0.2065(0.1642)
2023/12/02 19:40:47 - INFO - root -   Epoch: [149/300][40/283], lr: 0.00000036 	 loss = 0.1014(0.2414)
2023/12/02 19:41:08 - INFO - root -   Epoch: [149/300][60/283], lr: 0.00000036 	 loss = 0.3449(0.2986)
2023/12/02 19:41:28 - INFO - root -   Epoch: [149/300][80/283], lr: 0.00000036 	 loss = 0.0197(0.3222)
2023/12/02 19:41:44 - INFO - root -   Epoch: [149/300][100/283], lr: 0.00000036 	 loss = 0.2711(0.3176)
2023/12/02 19:41:52 - INFO - root -   Epoch: [149/300][120/283], lr: 0.00000036 	 loss = 0.5288(0.3592)
2023/12/02 19:42:12 - INFO - root -   Epoch: [149/300][140/283], lr: 0.00000036 	 loss = 0.0248(0.3297)
2023/12/02 19:42:26 - INFO - root -   Epoch: [149/300][160/283], lr: 0.00000036 	 loss = 0.0725(0.3223)
2023/12/02 19:42:45 - INFO - root -   Epoch: [149/300][180/283], lr: 0.00000036 	 loss = 0.0729(0.3190)
2023/12/02 19:42:53 - INFO - root -   Epoch: [149/300][200/283], lr: 0.00000036 	 loss = 0.3374(0.3077)
2023/12/02 19:43:08 - INFO - root -   Epoch: [149/300][220/283], lr: 0.00000036 	 loss = 0.0949(0.2944)
2023/12/02 19:43:36 - INFO - root -   Epoch: [149/300][240/283], lr: 0.00000036 	 loss = 0.1660(0.2968)
2023/12/02 19:43:44 - INFO - root -   Epoch: [149/300][260/283], lr: 0.00000036 	 loss = 0.0172(0.2933)
2023/12/02 19:43:55 - INFO - root -   Epoch: [149/300][280/283], lr: 0.00000036 	 loss = 0.8300(0.2867)
2023/12/02 19:43:56 - INFO - root -   Epoch: [149/300] 	 loss = 0.2855
2023/12/02 19:44:06 - INFO - root -   precision = 0.8252
2023/12/02 19:44:06 - INFO - root -   eval_loss = 0.4596
2023/12/02 19:44:06 - INFO - root -   eval_acc = 0.8252
2023/12/02 19:44:06 - INFO - root -   train_accuracy = 0.8746
2023/12/02 19:44:10 - INFO - root -   Epoch: [150/300][0/283], lr: 0.00000036 	 loss = 0.0227(0.0227)
2023/12/02 19:44:35 - INFO - root -   Epoch: [150/300][20/283], lr: 0.00000036 	 loss = 0.5420(0.3584)
2023/12/02 19:44:50 - INFO - root -   Epoch: [150/300][40/283], lr: 0.00000036 	 loss = 0.3900(0.4191)
2023/12/02 19:44:58 - INFO - root -   Epoch: [150/300][60/283], lr: 0.00000036 	 loss = 1.4844(0.3964)
2023/12/02 19:45:16 - INFO - root -   Epoch: [150/300][80/283], lr: 0.00000036 	 loss = 0.2671(0.3920)
2023/12/02 19:45:26 - INFO - root -   Epoch: [150/300][100/283], lr: 0.00000036 	 loss = 0.0134(0.3771)
2023/12/02 19:45:47 - INFO - root -   Epoch: [150/300][120/283], lr: 0.00000036 	 loss = 0.5157(0.3648)
2023/12/02 19:45:55 - INFO - root -   Epoch: [150/300][140/283], lr: 0.00000036 	 loss = 0.0255(0.3462)
2023/12/02 19:46:12 - INFO - root -   Epoch: [150/300][160/283], lr: 0.00000036 	 loss = 0.5974(0.3270)
2023/12/02 19:46:31 - INFO - root -   Epoch: [150/300][180/283], lr: 0.00000036 	 loss = 0.1926(0.3234)
2023/12/02 19:46:45 - INFO - root -   Epoch: [150/300][200/283], lr: 0.00000036 	 loss = 0.0762(0.3120)
2023/12/02 19:46:57 - INFO - root -   Epoch: [150/300][220/283], lr: 0.00000036 	 loss = 0.1077(0.3024)
2023/12/02 19:47:15 - INFO - root -   Epoch: [150/300][240/283], lr: 0.00000036 	 loss = 0.0290(0.3023)
2023/12/02 19:47:23 - INFO - root -   Epoch: [150/300][260/283], lr: 0.00000036 	 loss = 0.1098(0.2955)
2023/12/02 19:47:31 - INFO - root -   Epoch: [150/300][280/283], lr: 0.00000036 	 loss = 0.6409(0.2866)
2023/12/02 19:47:32 - INFO - root -   Epoch: [150/300] 	 loss = 0.2850
2023/12/02 19:47:32 - INFO - root -   train_accuracy = 0.8834
2023/12/02 19:47:33 - INFO - root -   Epoch: [151/300][0/283], lr: 0.00000036 	 loss = 0.0543(0.0543)
2023/12/02 19:48:10 - INFO - root -   Epoch: [151/300][20/283], lr: 0.00000036 	 loss = 0.2909(0.2558)
2023/12/02 19:48:18 - INFO - root -   Epoch: [151/300][40/283], lr: 0.00000036 	 loss = 0.1235(0.3187)
2023/12/02 19:48:38 - INFO - root -   Epoch: [151/300][60/283], lr: 0.00000036 	 loss = 0.9501(0.3562)
2023/12/02 19:48:58 - INFO - root -   Epoch: [151/300][80/283], lr: 0.00000036 	 loss = 0.9690(0.3737)
2023/12/02 19:49:19 - INFO - root -   Epoch: [151/300][100/283], lr: 0.00000036 	 loss = 0.0740(0.3436)
2023/12/02 19:49:27 - INFO - root -   Epoch: [151/300][120/283], lr: 0.00000036 	 loss = 0.4330(0.3592)
2023/12/02 19:49:42 - INFO - root -   Epoch: [151/300][140/283], lr: 0.00000036 	 loss = 0.0545(0.3429)
2023/12/02 19:49:55 - INFO - root -   Epoch: [151/300][160/283], lr: 0.00000036 	 loss = 0.0806(0.3137)
2023/12/02 19:50:06 - INFO - root -   Epoch: [151/300][180/283], lr: 0.00000036 	 loss = 0.0699(0.3007)
2023/12/02 19:50:19 - INFO - root -   Epoch: [151/300][200/283], lr: 0.00000036 	 loss = 0.7754(0.2877)
2023/12/02 19:50:38 - INFO - root -   Epoch: [151/300][220/283], lr: 0.00000036 	 loss = 0.0031(0.2787)
2023/12/02 19:50:45 - INFO - root -   Epoch: [151/300][240/283], lr: 0.00000036 	 loss = 0.0315(0.2680)
2023/12/02 19:51:05 - INFO - root -   Epoch: [151/300][260/283], lr: 0.00000036 	 loss = 0.2299(0.2693)
2023/12/02 19:51:14 - INFO - root -   Epoch: [151/300][280/283], lr: 0.00000036 	 loss = 0.1748(0.2626)
2023/12/02 19:51:15 - INFO - root -   Epoch: [151/300] 	 loss = 0.2612
2023/12/02 19:51:15 - INFO - root -   train_accuracy = 0.9011
2023/12/02 19:51:17 - INFO - root -   Epoch: [152/300][0/283], lr: 0.00000037 	 loss = 0.0153(0.0153)
2023/12/02 19:51:35 - INFO - root -   Epoch: [152/300][20/283], lr: 0.00000037 	 loss = 0.0704(0.2077)
2023/12/02 19:51:53 - INFO - root -   Epoch: [152/300][40/283], lr: 0.00000037 	 loss = 0.0421(0.2606)
2023/12/02 19:52:21 - INFO - root -   Epoch: [152/300][60/283], lr: 0.00000037 	 loss = 1.5889(0.3316)
2023/12/02 19:52:34 - INFO - root -   Epoch: [152/300][80/283], lr: 0.00000037 	 loss = 0.8115(0.3950)
2023/12/02 19:52:45 - INFO - root -   Epoch: [152/300][100/283], lr: 0.00000037 	 loss = 0.0160(0.3728)
2023/12/02 19:52:53 - INFO - root -   Epoch: [152/300][120/283], lr: 0.00000037 	 loss = 1.7325(0.4059)
2023/12/02 19:53:13 - INFO - root -   Epoch: [152/300][140/283], lr: 0.00000037 	 loss = 0.0098(0.3737)
2023/12/02 19:53:35 - INFO - root -   Epoch: [152/300][160/283], lr: 0.00000037 	 loss = 0.0953(0.3578)
2023/12/02 19:54:02 - INFO - root -   Epoch: [152/300][180/283], lr: 0.00000037 	 loss = 0.8511(0.3440)
2023/12/02 19:54:11 - INFO - root -   Epoch: [152/300][200/283], lr: 0.00000037 	 loss = 0.0935(0.3321)
2023/12/02 19:54:26 - INFO - root -   Epoch: [152/300][220/283], lr: 0.00000037 	 loss = 0.0109(0.3326)
2023/12/02 19:54:44 - INFO - root -   Epoch: [152/300][240/283], lr: 0.00000037 	 loss = 0.0757(0.3278)
2023/12/02 19:54:57 - INFO - root -   Epoch: [152/300][260/283], lr: 0.00000037 	 loss = 0.4415(0.3220)
2023/12/02 19:55:05 - INFO - root -   Epoch: [152/300][280/283], lr: 0.00000037 	 loss = 1.1776(0.3198)
2023/12/02 19:55:06 - INFO - root -   Epoch: [152/300] 	 loss = 0.3177
2023/12/02 19:55:06 - INFO - root -   train_accuracy = 0.8604
2023/12/02 19:55:07 - INFO - root -   Epoch: [153/300][0/283], lr: 0.00000037 	 loss = 0.0202(0.0202)
2023/12/02 19:55:30 - INFO - root -   Epoch: [153/300][20/283], lr: 0.00000037 	 loss = 0.2629(0.3595)
2023/12/02 19:55:47 - INFO - root -   Epoch: [153/300][40/283], lr: 0.00000037 	 loss = 0.1169(0.3145)
2023/12/02 19:55:59 - INFO - root -   Epoch: [153/300][60/283], lr: 0.00000037 	 loss = 2.0033(0.3283)
2023/12/02 19:56:10 - INFO - root -   Epoch: [153/300][80/283], lr: 0.00000037 	 loss = 0.2447(0.3316)
2023/12/02 19:56:32 - INFO - root -   Epoch: [153/300][100/283], lr: 0.00000037 	 loss = 0.0817(0.3383)
2023/12/02 19:56:40 - INFO - root -   Epoch: [153/300][120/283], lr: 0.00000037 	 loss = 0.6827(0.3696)
2023/12/02 19:56:48 - INFO - root -   Epoch: [153/300][140/283], lr: 0.00000037 	 loss = 0.0493(0.3527)
2023/12/02 19:57:11 - INFO - root -   Epoch: [153/300][160/283], lr: 0.00000037 	 loss = 0.5249(0.3411)
2023/12/02 19:57:25 - INFO - root -   Epoch: [153/300][180/283], lr: 0.00000037 	 loss = 0.1668(0.3306)
2023/12/02 19:57:39 - INFO - root -   Epoch: [153/300][200/283], lr: 0.00000037 	 loss = 0.0531(0.3147)
2023/12/02 19:57:53 - INFO - root -   Epoch: [153/300][220/283], lr: 0.00000037 	 loss = 0.6106(0.3069)
2023/12/02 19:58:11 - INFO - root -   Epoch: [153/300][240/283], lr: 0.00000037 	 loss = 0.2347(0.3101)
2023/12/02 19:58:27 - INFO - root -   Epoch: [153/300][260/283], lr: 0.00000037 	 loss = 0.2071(0.3078)
2023/12/02 19:58:35 - INFO - root -   Epoch: [153/300][280/283], lr: 0.00000037 	 loss = 0.4410(0.3019)
2023/12/02 19:58:36 - INFO - root -   Epoch: [153/300] 	 loss = 0.3001
2023/12/02 19:58:36 - INFO - root -   train_accuracy = 0.8834
2023/12/02 19:58:39 - INFO - root -   Epoch: [154/300][0/283], lr: 0.00000037 	 loss = 0.4433(0.4433)
2023/12/02 19:59:05 - INFO - root -   Epoch: [154/300][20/283], lr: 0.00000037 	 loss = 0.9714(0.2771)
2023/12/02 19:59:18 - INFO - root -   Epoch: [154/300][40/283], lr: 0.00000037 	 loss = 0.0360(0.3256)
2023/12/02 19:59:31 - INFO - root -   Epoch: [154/300][60/283], lr: 0.00000037 	 loss = 0.8136(0.4094)
2023/12/02 19:59:39 - INFO - root -   Epoch: [154/300][80/283], lr: 0.00000037 	 loss = 0.6807(0.3745)
2023/12/02 19:59:48 - INFO - root -   Epoch: [154/300][100/283], lr: 0.00000037 	 loss = 0.0556(0.3559)
2023/12/02 20:00:04 - INFO - root -   Epoch: [154/300][120/283], lr: 0.00000037 	 loss = 0.9656(0.3734)
2023/12/02 20:00:22 - INFO - root -   Epoch: [154/300][140/283], lr: 0.00000037 	 loss = 0.0769(0.3500)
2023/12/02 20:00:47 - INFO - root -   Epoch: [154/300][160/283], lr: 0.00000037 	 loss = 1.1973(0.3293)
2023/12/02 20:00:55 - INFO - root -   Epoch: [154/300][180/283], lr: 0.00000037 	 loss = 0.2921(0.3157)
2023/12/02 20:01:21 - INFO - root -   Epoch: [154/300][200/283], lr: 0.00000037 	 loss = 0.1596(0.3152)
2023/12/02 20:01:29 - INFO - root -   Epoch: [154/300][220/283], lr: 0.00000037 	 loss = 0.0143(0.3053)
2023/12/02 20:01:55 - INFO - root -   Epoch: [154/300][240/283], lr: 0.00000037 	 loss = 0.0979(0.3014)
2023/12/02 20:02:08 - INFO - root -   Epoch: [154/300][260/283], lr: 0.00000037 	 loss = 0.0567(0.2967)
2023/12/02 20:02:16 - INFO - root -   Epoch: [154/300][280/283], lr: 0.00000037 	 loss = 0.7925(0.2922)
2023/12/02 20:02:17 - INFO - root -   Epoch: [154/300] 	 loss = 0.2902
2023/12/02 20:02:26 - INFO - root -   precision = 0.8531
2023/12/02 20:02:26 - INFO - root -   eval_loss = 0.4411
2023/12/02 20:02:26 - INFO - root -   eval_acc = 0.8531
2023/12/02 20:02:27 - INFO - root -   train_accuracy = 0.8975
2023/12/02 20:02:42 - INFO - root -   Epoch: [155/300][0/283], lr: 0.00000037 	 loss = 0.6171(0.6171)
2023/12/02 20:02:59 - INFO - root -   Epoch: [155/300][20/283], lr: 0.00000037 	 loss = 1.0064(0.3924)
2023/12/02 20:03:08 - INFO - root -   Epoch: [155/300][40/283], lr: 0.00000037 	 loss = 0.1513(0.3890)
2023/12/02 20:03:24 - INFO - root -   Epoch: [155/300][60/283], lr: 0.00000037 	 loss = 0.4572(0.3920)
2023/12/02 20:03:42 - INFO - root -   Epoch: [155/300][80/283], lr: 0.00000037 	 loss = 0.0618(0.3677)
2023/12/02 20:03:54 - INFO - root -   Epoch: [155/300][100/283], lr: 0.00000037 	 loss = 0.0095(0.3323)
2023/12/02 20:04:08 - INFO - root -   Epoch: [155/300][120/283], lr: 0.00000037 	 loss = 2.4739(0.3638)
2023/12/02 20:04:19 - INFO - root -   Epoch: [155/300][140/283], lr: 0.00000037 	 loss = 0.0161(0.3341)
2023/12/02 20:04:37 - INFO - root -   Epoch: [155/300][160/283], lr: 0.00000037 	 loss = 0.0722(0.3023)
2023/12/02 20:04:51 - INFO - root -   Epoch: [155/300][180/283], lr: 0.00000037 	 loss = 1.6859(0.2901)
2023/12/02 20:05:02 - INFO - root -   Epoch: [155/300][200/283], lr: 0.00000037 	 loss = 0.1218(0.2790)
2023/12/02 20:05:09 - INFO - root -   Epoch: [155/300][220/283], lr: 0.00000037 	 loss = 0.0042(0.2647)
2023/12/02 20:05:25 - INFO - root -   Epoch: [155/300][240/283], lr: 0.00000037 	 loss = 0.0204(0.2629)
2023/12/02 20:05:42 - INFO - root -   Epoch: [155/300][260/283], lr: 0.00000037 	 loss = 0.0283(0.2616)
2023/12/02 20:05:50 - INFO - root -   Epoch: [155/300][280/283], lr: 0.00000037 	 loss = 0.9746(0.2654)
2023/12/02 20:05:51 - INFO - root -   Epoch: [155/300] 	 loss = 0.2637
2023/12/02 20:05:51 - INFO - root -   train_accuracy = 0.9064
2023/12/02 20:05:54 - INFO - root -   Epoch: [156/300][0/283], lr: 0.00000037 	 loss = 0.0255(0.0255)
2023/12/02 20:06:23 - INFO - root -   Epoch: [156/300][20/283], lr: 0.00000037 	 loss = 0.3270(0.2963)
2023/12/02 20:06:31 - INFO - root -   Epoch: [156/300][40/283], lr: 0.00000037 	 loss = 0.0550(0.2674)
2023/12/02 20:06:46 - INFO - root -   Epoch: [156/300][60/283], lr: 0.00000037 	 loss = 1.2531(0.2904)
2023/12/02 20:06:56 - INFO - root -   Epoch: [156/300][80/283], lr: 0.00000037 	 loss = 0.1063(0.2795)
2023/12/02 20:07:19 - INFO - root -   Epoch: [156/300][100/283], lr: 0.00000037 	 loss = 0.1606(0.2879)
2023/12/02 20:07:27 - INFO - root -   Epoch: [156/300][120/283], lr: 0.00000037 	 loss = 0.3029(0.3072)
2023/12/02 20:07:41 - INFO - root -   Epoch: [156/300][140/283], lr: 0.00000037 	 loss = 0.0543(0.2734)
2023/12/02 20:07:51 - INFO - root -   Epoch: [156/300][160/283], lr: 0.00000037 	 loss = 1.0052(0.2783)
2023/12/02 20:08:11 - INFO - root -   Epoch: [156/300][180/283], lr: 0.00000037 	 loss = 0.2131(0.2686)
2023/12/02 20:08:19 - INFO - root -   Epoch: [156/300][200/283], lr: 0.00000037 	 loss = 0.1013(0.2610)
2023/12/02 20:08:37 - INFO - root -   Epoch: [156/300][220/283], lr: 0.00000037 	 loss = 0.0030(0.2532)
2023/12/02 20:08:53 - INFO - root -   Epoch: [156/300][240/283], lr: 0.00000037 	 loss = 0.1397(0.2697)
2023/12/02 20:09:05 - INFO - root -   Epoch: [156/300][260/283], lr: 0.00000037 	 loss = 0.0200(0.2687)
2023/12/02 20:09:13 - INFO - root -   Epoch: [156/300][280/283], lr: 0.00000037 	 loss = 0.6174(0.2762)
2023/12/02 20:09:14 - INFO - root -   Epoch: [156/300] 	 loss = 0.2746
2023/12/02 20:09:14 - INFO - root -   train_accuracy = 0.8922
2023/12/02 20:09:15 - INFO - root -   Epoch: [157/300][0/283], lr: 0.00000037 	 loss = 0.0118(0.0118)
2023/12/02 20:09:45 - INFO - root -   Epoch: [157/300][20/283], lr: 0.00000037 	 loss = 0.1097(0.2834)
2023/12/02 20:09:57 - INFO - root -   Epoch: [157/300][40/283], lr: 0.00000037 	 loss = 0.0520(0.2670)
2023/12/02 20:10:05 - INFO - root -   Epoch: [157/300][60/283], lr: 0.00000037 	 loss = 1.1108(0.2894)
2023/12/02 20:10:33 - INFO - root -   Epoch: [157/300][80/283], lr: 0.00000037 	 loss = 0.0476(0.3037)
2023/12/02 20:10:40 - INFO - root -   Epoch: [157/300][100/283], lr: 0.00000037 	 loss = 0.0362(0.2901)
2023/12/02 20:10:54 - INFO - root -   Epoch: [157/300][120/283], lr: 0.00000037 	 loss = 0.4423(0.3214)
2023/12/02 20:11:18 - INFO - root -   Epoch: [157/300][140/283], lr: 0.00000037 	 loss = 0.1095(0.3082)
2023/12/02 20:11:26 - INFO - root -   Epoch: [157/300][160/283], lr: 0.00000037 	 loss = 0.1912(0.2826)
2023/12/02 20:11:48 - INFO - root -   Epoch: [157/300][180/283], lr: 0.00000037 	 loss = 0.1113(0.2830)
2023/12/02 20:12:05 - INFO - root -   Epoch: [157/300][200/283], lr: 0.00000037 	 loss = 1.3301(0.2873)
2023/12/02 20:12:24 - INFO - root -   Epoch: [157/300][220/283], lr: 0.00000037 	 loss = 0.1621(0.2733)
2023/12/02 20:12:45 - INFO - root -   Epoch: [157/300][240/283], lr: 0.00000037 	 loss = 0.0439(0.2737)
2023/12/02 20:12:53 - INFO - root -   Epoch: [157/300][260/283], lr: 0.00000037 	 loss = 0.0067(0.2674)
2023/12/02 20:13:03 - INFO - root -   Epoch: [157/300][280/283], lr: 0.00000037 	 loss = 0.9773(0.2636)
2023/12/02 20:13:04 - INFO - root -   Epoch: [157/300] 	 loss = 0.2619
2023/12/02 20:13:04 - INFO - root -   train_accuracy = 0.8940
2023/12/02 20:13:20 - INFO - root -   Epoch: [158/300][0/283], lr: 0.00000038 	 loss = 0.7888(0.7888)
2023/12/02 20:13:40 - INFO - root -   Epoch: [158/300][20/283], lr: 0.00000038 	 loss = 0.2756(0.2248)
2023/12/02 20:13:50 - INFO - root -   Epoch: [158/300][40/283], lr: 0.00000038 	 loss = 0.1681(0.3840)
2023/12/02 20:14:05 - INFO - root -   Epoch: [158/300][60/283], lr: 0.00000038 	 loss = 1.5870(0.4031)
2023/12/02 20:14:13 - INFO - root -   Epoch: [158/300][80/283], lr: 0.00000038 	 loss = 0.1744(0.3969)
2023/12/02 20:14:28 - INFO - root -   Epoch: [158/300][100/283], lr: 0.00000038 	 loss = 0.0196(0.3582)
2023/12/02 20:14:40 - INFO - root -   Epoch: [158/300][120/283], lr: 0.00000038 	 loss = 1.8105(0.3929)
2023/12/02 20:14:52 - INFO - root -   Epoch: [158/300][140/283], lr: 0.00000038 	 loss = 0.1735(0.3555)
2023/12/02 20:15:11 - INFO - root -   Epoch: [158/300][160/283], lr: 0.00000038 	 loss = 0.0155(0.3344)
2023/12/02 20:15:27 - INFO - root -   Epoch: [158/300][180/283], lr: 0.00000038 	 loss = 0.1498(0.3263)
2023/12/02 20:15:43 - INFO - root -   Epoch: [158/300][200/283], lr: 0.00000038 	 loss = 0.0469(0.3136)
2023/12/02 20:15:57 - INFO - root -   Epoch: [158/300][220/283], lr: 0.00000038 	 loss = 0.0623(0.3129)
2023/12/02 20:16:13 - INFO - root -   Epoch: [158/300][240/283], lr: 0.00000038 	 loss = 0.0528(0.3075)
2023/12/02 20:16:26 - INFO - root -   Epoch: [158/300][260/283], lr: 0.00000038 	 loss = 0.0078(0.3035)
2023/12/02 20:16:34 - INFO - root -   Epoch: [158/300][280/283], lr: 0.00000038 	 loss = 0.5928(0.2975)
2023/12/02 20:16:35 - INFO - root -   Epoch: [158/300] 	 loss = 0.2958
2023/12/02 20:16:35 - INFO - root -   train_accuracy = 0.8922
2023/12/02 20:16:37 - INFO - root -   Epoch: [159/300][0/283], lr: 0.00000038 	 loss = 0.0050(0.0050)
2023/12/02 20:17:08 - INFO - root -   Epoch: [159/300][20/283], lr: 0.00000038 	 loss = 0.2309(0.3079)
2023/12/02 20:17:15 - INFO - root -   Epoch: [159/300][40/283], lr: 0.00000038 	 loss = 0.0701(0.3045)
2023/12/02 20:17:33 - INFO - root -   Epoch: [159/300][60/283], lr: 0.00000038 	 loss = 1.0734(0.3361)
2023/12/02 20:17:48 - INFO - root -   Epoch: [159/300][80/283], lr: 0.00000038 	 loss = 0.5463(0.3836)
2023/12/02 20:18:03 - INFO - root -   Epoch: [159/300][100/283], lr: 0.00000038 	 loss = 0.0073(0.3528)
2023/12/02 20:18:21 - INFO - root -   Epoch: [159/300][120/283], lr: 0.00000038 	 loss = 0.3617(0.3828)
2023/12/02 20:18:32 - INFO - root -   Epoch: [159/300][140/283], lr: 0.00000038 	 loss = 0.0103(0.3483)
2023/12/02 20:18:58 - INFO - root -   Epoch: [159/300][160/283], lr: 0.00000038 	 loss = 0.0506(0.3404)
2023/12/02 20:19:06 - INFO - root -   Epoch: [159/300][180/283], lr: 0.00000038 	 loss = 0.1261(0.3315)
2023/12/02 20:19:34 - INFO - root -   Epoch: [159/300][200/283], lr: 0.00000038 	 loss = 0.8830(0.3266)
2023/12/02 20:19:42 - INFO - root -   Epoch: [159/300][220/283], lr: 0.00000038 	 loss = 0.1929(0.3160)
2023/12/02 20:20:01 - INFO - root -   Epoch: [159/300][240/283], lr: 0.00000038 	 loss = 0.0304(0.3118)
2023/12/02 20:20:15 - INFO - root -   Epoch: [159/300][260/283], lr: 0.00000038 	 loss = 0.1095(0.3056)
2023/12/02 20:20:24 - INFO - root -   Epoch: [159/300][280/283], lr: 0.00000038 	 loss = 0.2255(0.2998)
2023/12/02 20:20:25 - INFO - root -   Epoch: [159/300] 	 loss = 0.2978
2023/12/02 20:20:35 - INFO - root -   precision = 0.8671
2023/12/02 20:20:35 - INFO - root -   eval_loss = 0.4625
2023/12/02 20:20:35 - INFO - root -   eval_acc = 0.8671
2023/12/02 20:20:35 - INFO - root -   train_accuracy = 0.8746
2023/12/02 20:20:48 - INFO - root -   Epoch: [160/300][0/283], lr: 0.00000038 	 loss = 0.6629(0.6629)
2023/12/02 20:21:14 - INFO - root -   Epoch: [160/300][20/283], lr: 0.00000038 	 loss = 1.0044(0.3687)
2023/12/02 20:21:22 - INFO - root -   Epoch: [160/300][40/283], lr: 0.00000038 	 loss = 0.0496(0.3783)
2023/12/02 20:21:32 - INFO - root -   Epoch: [160/300][60/283], lr: 0.00000038 	 loss = 1.5661(0.3897)
2023/12/02 20:21:48 - INFO - root -   Epoch: [160/300][80/283], lr: 0.00000038 	 loss = 0.0754(0.3794)
2023/12/02 20:22:05 - INFO - root -   Epoch: [160/300][100/283], lr: 0.00000038 	 loss = 0.1156(0.3607)
2023/12/02 20:22:13 - INFO - root -   Epoch: [160/300][120/283], lr: 0.00000038 	 loss = 0.7768(0.3864)
2023/12/02 20:22:29 - INFO - root -   Epoch: [160/300][140/283], lr: 0.00000038 	 loss = 0.0295(0.3559)
2023/12/02 20:22:37 - INFO - root -   Epoch: [160/300][160/283], lr: 0.00000038 	 loss = 0.1094(0.3400)
2023/12/02 20:23:06 - INFO - root -   Epoch: [160/300][180/283], lr: 0.00000038 	 loss = 0.2042(0.3327)
2023/12/02 20:23:14 - INFO - root -   Epoch: [160/300][200/283], lr: 0.00000038 	 loss = 0.0759(0.3189)
2023/12/02 20:23:36 - INFO - root -   Epoch: [160/300][220/283], lr: 0.00000038 	 loss = 0.0061(0.3052)
2023/12/02 20:23:52 - INFO - root -   Epoch: [160/300][240/283], lr: 0.00000038 	 loss = 0.1255(0.3128)
2023/12/02 20:24:04 - INFO - root -   Epoch: [160/300][260/283], lr: 0.00000038 	 loss = 0.0778(0.3133)
2023/12/02 20:24:12 - INFO - root -   Epoch: [160/300][280/283], lr: 0.00000038 	 loss = 0.2366(0.3081)
2023/12/02 20:24:13 - INFO - root -   Epoch: [160/300] 	 loss = 0.3067
2023/12/02 20:24:13 - INFO - root -   train_accuracy = 0.8816
2023/12/02 20:24:28 - INFO - root -   Epoch: [161/300][0/283], lr: 0.00000038 	 loss = 0.3580(0.3580)
2023/12/02 20:24:37 - INFO - root -   Epoch: [161/300][20/283], lr: 0.00000038 	 loss = 0.4050(0.2857)
2023/12/02 20:24:56 - INFO - root -   Epoch: [161/300][40/283], lr: 0.00000038 	 loss = 0.0532(0.3457)
2023/12/02 20:25:15 - INFO - root -   Epoch: [161/300][60/283], lr: 0.00000038 	 loss = 1.5326(0.3744)
2023/12/02 20:25:33 - INFO - root -   Epoch: [161/300][80/283], lr: 0.00000038 	 loss = 0.0343(0.3829)
2023/12/02 20:25:44 - INFO - root -   Epoch: [161/300][100/283], lr: 0.00000038 	 loss = 0.0194(0.3473)
2023/12/02 20:25:58 - INFO - root -   Epoch: [161/300][120/283], lr: 0.00000038 	 loss = 0.2029(0.3639)
2023/12/02 20:26:17 - INFO - root -   Epoch: [161/300][140/283], lr: 0.00000038 	 loss = 0.0262(0.3314)
2023/12/02 20:26:25 - INFO - root -   Epoch: [161/300][160/283], lr: 0.00000038 	 loss = 0.2459(0.3095)
2023/12/02 20:26:50 - INFO - root -   Epoch: [161/300][180/283], lr: 0.00000038 	 loss = 0.1454(0.2906)
2023/12/02 20:27:06 - INFO - root -   Epoch: [161/300][200/283], lr: 0.00000038 	 loss = 0.0207(0.2976)
2023/12/02 20:27:22 - INFO - root -   Epoch: [161/300][220/283], lr: 0.00000038 	 loss = 0.0140(0.2913)
2023/12/02 20:27:31 - INFO - root -   Epoch: [161/300][240/283], lr: 0.00000038 	 loss = 0.1113(0.2882)
2023/12/02 20:27:49 - INFO - root -   Epoch: [161/300][260/283], lr: 0.00000038 	 loss = 0.0079(0.2794)
2023/12/02 20:27:58 - INFO - root -   Epoch: [161/300][280/283], lr: 0.00000038 	 loss = 0.3277(0.2795)
2023/12/02 20:27:58 - INFO - root -   Epoch: [161/300] 	 loss = 0.2780
2023/12/02 20:27:58 - INFO - root -   train_accuracy = 0.9011
2023/12/02 20:28:24 - INFO - root -   Epoch: [162/300][0/283], lr: 0.00000038 	 loss = 0.4252(0.4252)
2023/12/02 20:28:32 - INFO - root -   Epoch: [162/300][20/283], lr: 0.00000038 	 loss = 0.1302(0.3139)
2023/12/02 20:28:48 - INFO - root -   Epoch: [162/300][40/283], lr: 0.00000038 	 loss = 0.0208(0.3130)
2023/12/02 20:29:08 - INFO - root -   Epoch: [162/300][60/283], lr: 0.00000038 	 loss = 1.5723(0.3722)
2023/12/02 20:29:16 - INFO - root -   Epoch: [162/300][80/283], lr: 0.00000038 	 loss = 0.0117(0.3773)
2023/12/02 20:29:41 - INFO - root -   Epoch: [162/300][100/283], lr: 0.00000038 	 loss = 0.0201(0.3574)
2023/12/02 20:29:54 - INFO - root -   Epoch: [162/300][120/283], lr: 0.00000038 	 loss = 0.4177(0.3795)
2023/12/02 20:30:12 - INFO - root -   Epoch: [162/300][140/283], lr: 0.00000038 	 loss = 0.0530(0.3716)
2023/12/02 20:30:30 - INFO - root -   Epoch: [162/300][160/283], lr: 0.00000038 	 loss = 0.0594(0.3446)
2023/12/02 20:30:38 - INFO - root -   Epoch: [162/300][180/283], lr: 0.00000038 	 loss = 0.7239(0.3296)
2023/12/02 20:30:52 - INFO - root -   Epoch: [162/300][200/283], lr: 0.00000038 	 loss = 0.0362(0.3241)
2023/12/02 20:31:09 - INFO - root -   Epoch: [162/300][220/283], lr: 0.00000038 	 loss = 0.0718(0.3295)
2023/12/02 20:31:27 - INFO - root -   Epoch: [162/300][240/283], lr: 0.00000038 	 loss = 0.4134(0.3261)
2023/12/02 20:31:35 - INFO - root -   Epoch: [162/300][260/283], lr: 0.00000038 	 loss = 0.0053(0.3154)
2023/12/02 20:31:43 - INFO - root -   Epoch: [162/300][280/283], lr: 0.00000038 	 loss = 0.3211(0.3050)
2023/12/02 20:31:44 - INFO - root -   Epoch: [162/300] 	 loss = 0.3034
2023/12/02 20:31:44 - INFO - root -   train_accuracy = 0.8834
2023/12/02 20:31:59 - INFO - root -   Epoch: [163/300][0/283], lr: 0.00000039 	 loss = 0.1617(0.1617)
2023/12/02 20:32:11 - INFO - root -   Epoch: [163/300][20/283], lr: 0.00000039 	 loss = 0.1019(0.3500)
2023/12/02 20:32:22 - INFO - root -   Epoch: [163/300][40/283], lr: 0.00000039 	 loss = 0.0659(0.3925)
2023/12/02 20:32:37 - INFO - root -   Epoch: [163/300][60/283], lr: 0.00000039 	 loss = 1.4199(0.3805)
2023/12/02 20:32:50 - INFO - root -   Epoch: [163/300][80/283], lr: 0.00000039 	 loss = 0.0686(0.3551)
2023/12/02 20:33:05 - INFO - root -   Epoch: [163/300][100/283], lr: 0.00000039 	 loss = 0.4139(0.3511)
2023/12/02 20:33:30 - INFO - root -   Epoch: [163/300][120/283], lr: 0.00000039 	 loss = 1.0382(0.3759)
2023/12/02 20:33:39 - INFO - root -   Epoch: [163/300][140/283], lr: 0.00000039 	 loss = 0.0112(0.3570)
2023/12/02 20:33:48 - INFO - root -   Epoch: [163/300][160/283], lr: 0.00000039 	 loss = 0.7851(0.3282)
2023/12/02 20:34:23 - INFO - root -   Epoch: [163/300][180/283], lr: 0.00000039 	 loss = 0.1692(0.3112)
2023/12/02 20:34:38 - INFO - root -   Epoch: [163/300][200/283], lr: 0.00000039 	 loss = 0.6280(0.3123)
2023/12/02 20:34:46 - INFO - root -   Epoch: [163/300][220/283], lr: 0.00000039 	 loss = 0.0572(0.2966)
2023/12/02 20:34:59 - INFO - root -   Epoch: [163/300][240/283], lr: 0.00000039 	 loss = 0.1648(0.2946)
2023/12/02 20:35:21 - INFO - root -   Epoch: [163/300][260/283], lr: 0.00000039 	 loss = 0.0054(0.2940)
2023/12/02 20:35:29 - INFO - root -   Epoch: [163/300][280/283], lr: 0.00000039 	 loss = 0.3275(0.2881)
2023/12/02 20:35:30 - INFO - root -   Epoch: [163/300] 	 loss = 0.2874
2023/12/02 20:35:30 - INFO - root -   train_accuracy = 0.8834
2023/12/02 20:35:33 - INFO - root -   Epoch: [164/300][0/283], lr: 0.00000039 	 loss = 0.0434(0.0434)
2023/12/02 20:36:03 - INFO - root -   Epoch: [164/300][20/283], lr: 0.00000039 	 loss = 0.2520(0.3266)
2023/12/02 20:36:11 - INFO - root -   Epoch: [164/300][40/283], lr: 0.00000039 	 loss = 0.1617(0.3761)
2023/12/02 20:36:25 - INFO - root -   Epoch: [164/300][60/283], lr: 0.00000039 	 loss = 0.3694(0.3619)
2023/12/02 20:36:41 - INFO - root -   Epoch: [164/300][80/283], lr: 0.00000039 	 loss = 0.0700(0.3607)
2023/12/02 20:37:01 - INFO - root -   Epoch: [164/300][100/283], lr: 0.00000039 	 loss = 0.3380(0.3346)
2023/12/02 20:37:21 - INFO - root -   Epoch: [164/300][120/283], lr: 0.00000039 	 loss = 0.1210(0.3636)
2023/12/02 20:37:40 - INFO - root -   Epoch: [164/300][140/283], lr: 0.00000039 	 loss = 0.0096(0.3502)
2023/12/02 20:37:48 - INFO - root -   Epoch: [164/300][160/283], lr: 0.00000039 	 loss = 0.0187(0.3298)
2023/12/02 20:38:12 - INFO - root -   Epoch: [164/300][180/283], lr: 0.00000039 	 loss = 0.4897(0.3246)
2023/12/02 20:38:35 - INFO - root -   Epoch: [164/300][200/283], lr: 0.00000039 	 loss = 0.0532(0.3183)
2023/12/02 20:38:43 - INFO - root -   Epoch: [164/300][220/283], lr: 0.00000039 	 loss = 0.1243(0.3091)
2023/12/02 20:38:56 - INFO - root -   Epoch: [164/300][240/283], lr: 0.00000039 	 loss = 0.1271(0.3168)
2023/12/02 20:39:14 - INFO - root -   Epoch: [164/300][260/283], lr: 0.00000039 	 loss = 0.0103(0.3107)
2023/12/02 20:39:22 - INFO - root -   Epoch: [164/300][280/283], lr: 0.00000039 	 loss = 1.3470(0.3054)
2023/12/02 20:39:23 - INFO - root -   Epoch: [164/300] 	 loss = 0.3044
2023/12/02 20:39:33 - INFO - root -   precision = 0.8531
2023/12/02 20:39:33 - INFO - root -   eval_loss = 0.4368
2023/12/02 20:39:33 - INFO - root -   eval_acc = 0.8531
2023/12/02 20:39:33 - INFO - root -   train_accuracy = 0.8640
2023/12/02 20:39:48 - INFO - root -   Epoch: [165/300][0/283], lr: 0.00000039 	 loss = 0.5183(0.5183)
2023/12/02 20:40:02 - INFO - root -   Epoch: [165/300][20/283], lr: 0.00000039 	 loss = 0.8935(0.3568)
2023/12/02 20:40:10 - INFO - root -   Epoch: [165/300][40/283], lr: 0.00000039 	 loss = 0.0182(0.3368)
2023/12/02 20:40:30 - INFO - root -   Epoch: [165/300][60/283], lr: 0.00000039 	 loss = 0.5200(0.4038)
2023/12/02 20:40:58 - INFO - root -   Epoch: [165/300][80/283], lr: 0.00000039 	 loss = 0.2592(0.3918)
2023/12/02 20:41:06 - INFO - root -   Epoch: [165/300][100/283], lr: 0.00000039 	 loss = 0.1845(0.3809)
2023/12/02 20:41:14 - INFO - root -   Epoch: [165/300][120/283], lr: 0.00000039 	 loss = 1.0188(0.4153)
2023/12/02 20:41:35 - INFO - root -   Epoch: [165/300][140/283], lr: 0.00000039 	 loss = 0.0423(0.4011)
2023/12/02 20:41:43 - INFO - root -   Epoch: [165/300][160/283], lr: 0.00000039 	 loss = 0.0282(0.3628)
2023/12/02 20:42:02 - INFO - root -   Epoch: [165/300][180/283], lr: 0.00000039 	 loss = 0.2287(0.3452)
2023/12/02 20:42:20 - INFO - root -   Epoch: [165/300][200/283], lr: 0.00000039 	 loss = 0.0249(0.3467)
2023/12/02 20:42:28 - INFO - root -   Epoch: [165/300][220/283], lr: 0.00000039 	 loss = 0.0238(0.3453)
2023/12/02 20:42:48 - INFO - root -   Epoch: [165/300][240/283], lr: 0.00000039 	 loss = 0.1065(0.3385)
2023/12/02 20:43:07 - INFO - root -   Epoch: [165/300][260/283], lr: 0.00000039 	 loss = 0.0839(0.3394)
2023/12/02 20:43:15 - INFO - root -   Epoch: [165/300][280/283], lr: 0.00000039 	 loss = 0.2169(0.3289)
2023/12/02 20:43:16 - INFO - root -   Epoch: [165/300] 	 loss = 0.3269
2023/12/02 20:43:16 - INFO - root -   train_accuracy = 0.8728
2023/12/02 20:43:18 - INFO - root -   Epoch: [166/300][0/283], lr: 0.00000039 	 loss = 0.0178(0.0178)
2023/12/02 20:43:54 - INFO - root -   Epoch: [166/300][20/283], lr: 0.00000039 	 loss = 0.1461(0.2927)
2023/12/02 20:44:08 - INFO - root -   Epoch: [166/300][40/283], lr: 0.00000039 	 loss = 0.4113(0.3579)
2023/12/02 20:44:33 - INFO - root -   Epoch: [166/300][60/283], lr: 0.00000039 	 loss = 1.3157(0.3396)
2023/12/02 20:44:49 - INFO - root -   Epoch: [166/300][80/283], lr: 0.00000039 	 loss = 0.7596(0.3601)
2023/12/02 20:44:57 - INFO - root -   Epoch: [166/300][100/283], lr: 0.00000039 	 loss = 0.0250(0.3564)
2023/12/02 20:45:20 - INFO - root -   Epoch: [166/300][120/283], lr: 0.00000039 	 loss = 0.1604(0.3842)
2023/12/02 20:45:37 - INFO - root -   Epoch: [166/300][140/283], lr: 0.00000039 	 loss = 0.2123(0.3575)
2023/12/02 20:45:57 - INFO - root -   Epoch: [166/300][160/283], lr: 0.00000039 	 loss = 0.0482(0.3357)
2023/12/02 20:46:16 - INFO - root -   Epoch: [166/300][180/283], lr: 0.00000039 	 loss = 0.4988(0.3209)
2023/12/02 20:46:26 - INFO - root -   Epoch: [166/300][200/283], lr: 0.00000039 	 loss = 0.1658(0.3182)
2023/12/02 20:46:39 - INFO - root -   Epoch: [166/300][220/283], lr: 0.00000039 	 loss = 0.0035(0.3066)
2023/12/02 20:47:08 - INFO - root -   Epoch: [166/300][240/283], lr: 0.00000039 	 loss = 0.0181(0.3067)
2023/12/02 20:47:24 - INFO - root -   Epoch: [166/300][260/283], lr: 0.00000039 	 loss = 0.0705(0.3015)
2023/12/02 20:47:34 - INFO - root -   Epoch: [166/300][280/283], lr: 0.00000039 	 loss = 0.0316(0.3008)
2023/12/02 20:47:34 - INFO - root -   Epoch: [166/300] 	 loss = 0.2996
2023/12/02 20:47:34 - INFO - root -   train_accuracy = 0.8622
2023/12/02 20:47:49 - INFO - root -   Epoch: [167/300][0/283], lr: 0.00000039 	 loss = 0.2691(0.2691)
2023/12/02 20:48:03 - INFO - root -   Epoch: [167/300][20/283], lr: 0.00000039 	 loss = 0.0666(0.2213)
2023/12/02 20:48:26 - INFO - root -   Epoch: [167/300][40/283], lr: 0.00000039 	 loss = 0.0394(0.2875)
2023/12/02 20:48:41 - INFO - root -   Epoch: [167/300][60/283], lr: 0.00000039 	 loss = 1.4692(0.2868)
2023/12/02 20:48:53 - INFO - root -   Epoch: [167/300][80/283], lr: 0.00000039 	 loss = 0.2905(0.3097)
2023/12/02 20:49:18 - INFO - root -   Epoch: [167/300][100/283], lr: 0.00000039 	 loss = 0.0926(0.3126)
2023/12/02 20:49:28 - INFO - root -   Epoch: [167/300][120/283], lr: 0.00000039 	 loss = 0.6737(0.3423)
2023/12/02 20:49:44 - INFO - root -   Epoch: [167/300][140/283], lr: 0.00000039 	 loss = 0.0062(0.3102)
2023/12/02 20:50:05 - INFO - root -   Epoch: [167/300][160/283], lr: 0.00000039 	 loss = 0.7220(0.3005)
2023/12/02 20:50:13 - INFO - root -   Epoch: [167/300][180/283], lr: 0.00000039 	 loss = 0.2351(0.2805)
2023/12/02 20:50:21 - INFO - root -   Epoch: [167/300][200/283], lr: 0.00000039 	 loss = 0.0296(0.2757)
2023/12/02 20:50:37 - INFO - root -   Epoch: [167/300][220/283], lr: 0.00000039 	 loss = 0.0355(0.2637)
2023/12/02 20:50:55 - INFO - root -   Epoch: [167/300][240/283], lr: 0.00000039 	 loss = 0.0330(0.2680)
2023/12/02 20:51:13 - INFO - root -   Epoch: [167/300][260/283], lr: 0.00000039 	 loss = 0.1231(0.2711)
2023/12/02 20:51:21 - INFO - root -   Epoch: [167/300][280/283], lr: 0.00000039 	 loss = 1.0835(0.2691)
2023/12/02 20:51:22 - INFO - root -   Epoch: [167/300] 	 loss = 0.2674
2023/12/02 20:51:22 - INFO - root -   train_accuracy = 0.8975
2023/12/02 20:51:24 - INFO - root -   Epoch: [168/300][0/283], lr: 0.00000039 	 loss = 0.0065(0.0065)
2023/12/02 20:51:51 - INFO - root -   Epoch: [168/300][20/283], lr: 0.00000039 	 loss = 0.0882(0.3399)
2023/12/02 20:52:05 - INFO - root -   Epoch: [168/300][40/283], lr: 0.00000039 	 loss = 0.2849(0.3514)
2023/12/02 20:52:26 - INFO - root -   Epoch: [168/300][60/283], lr: 0.00000039 	 loss = 1.5341(0.3406)
2023/12/02 20:52:36 - INFO - root -   Epoch: [168/300][80/283], lr: 0.00000039 	 loss = 0.5281(0.3698)
2023/12/02 20:52:52 - INFO - root -   Epoch: [168/300][100/283], lr: 0.00000039 	 loss = 0.0246(0.3321)
2023/12/02 20:53:09 - INFO - root -   Epoch: [168/300][120/283], lr: 0.00000039 	 loss = 1.3092(0.3707)
2023/12/02 20:53:17 - INFO - root -   Epoch: [168/300][140/283], lr: 0.00000039 	 loss = 0.0605(0.3407)
2023/12/02 20:53:41 - INFO - root -   Epoch: [168/300][160/283], lr: 0.00000039 	 loss = 1.0405(0.3372)
2023/12/02 20:53:49 - INFO - root -   Epoch: [168/300][180/283], lr: 0.00000039 	 loss = 0.5442(0.3182)
2023/12/02 20:54:10 - INFO - root -   Epoch: [168/300][200/283], lr: 0.00000039 	 loss = 0.1248(0.3072)
2023/12/02 20:54:18 - INFO - root -   Epoch: [168/300][220/283], lr: 0.00000039 	 loss = 0.0214(0.2912)
2023/12/02 20:54:29 - INFO - root -   Epoch: [168/300][240/283], lr: 0.00000039 	 loss = 0.0179(0.2844)
2023/12/02 20:54:50 - INFO - root -   Epoch: [168/300][260/283], lr: 0.00000039 	 loss = 0.0061(0.2878)
2023/12/02 20:54:58 - INFO - root -   Epoch: [168/300][280/283], lr: 0.00000039 	 loss = 0.9150(0.2858)
2023/12/02 20:54:59 - INFO - root -   Epoch: [168/300] 	 loss = 0.2843
2023/12/02 20:54:59 - INFO - root -   train_accuracy = 0.8887
2023/12/02 20:55:00 - INFO - root -   Epoch: [169/300][0/283], lr: 0.00000040 	 loss = 0.0096(0.0096)
2023/12/02 20:55:25 - INFO - root -   Epoch: [169/300][20/283], lr: 0.00000040 	 loss = 0.6378(0.2335)
2023/12/02 20:55:47 - INFO - root -   Epoch: [169/300][40/283], lr: 0.00000040 	 loss = 0.1546(0.2408)
2023/12/02 20:55:56 - INFO - root -   Epoch: [169/300][60/283], lr: 0.00000040 	 loss = 1.5349(0.3169)
2023/12/02 20:56:18 - INFO - root -   Epoch: [169/300][80/283], lr: 0.00000040 	 loss = 0.0268(0.3518)
2023/12/02 20:56:26 - INFO - root -   Epoch: [169/300][100/283], lr: 0.00000040 	 loss = 0.0637(0.3293)
2023/12/02 20:56:39 - INFO - root -   Epoch: [169/300][120/283], lr: 0.00000040 	 loss = 0.4626(0.3525)
2023/12/02 20:56:57 - INFO - root -   Epoch: [169/300][140/283], lr: 0.00000040 	 loss = 0.0138(0.3262)
2023/12/02 20:57:05 - INFO - root -   Epoch: [169/300][160/283], lr: 0.00000040 	 loss = 0.3243(0.3050)
2023/12/02 20:57:25 - INFO - root -   Epoch: [169/300][180/283], lr: 0.00000040 	 loss = 0.2303(0.2955)
2023/12/02 20:57:37 - INFO - root -   Epoch: [169/300][200/283], lr: 0.00000040 	 loss = 0.0472(0.2794)
2023/12/02 20:57:46 - INFO - root -   Epoch: [169/300][220/283], lr: 0.00000040 	 loss = 0.0099(0.2612)
2023/12/02 20:57:54 - INFO - root -   Epoch: [169/300][240/283], lr: 0.00000040 	 loss = 0.0244(0.2589)
2023/12/02 20:58:13 - INFO - root -   Epoch: [169/300][260/283], lr: 0.00000040 	 loss = 0.0798(0.2537)
2023/12/02 20:58:20 - INFO - root -   Epoch: [169/300][280/283], lr: 0.00000040 	 loss = 0.1911(0.2537)
2023/12/02 20:58:21 - INFO - root -   Epoch: [169/300] 	 loss = 0.2522
2023/12/02 20:58:31 - INFO - root -   precision = 0.8531
2023/12/02 20:58:31 - INFO - root -   eval_loss = 0.4373
2023/12/02 20:58:31 - INFO - root -   eval_acc = 0.8531
2023/12/02 20:58:31 - INFO - root -   train_accuracy = 0.9064
2023/12/02 20:59:00 - INFO - root -   Epoch: [170/300][0/283], lr: 0.00000040 	 loss = 0.7141(0.7141)
2023/12/02 20:59:14 - INFO - root -   Epoch: [170/300][20/283], lr: 0.00000040 	 loss = 0.2465(0.3915)
2023/12/02 20:59:22 - INFO - root -   Epoch: [170/300][40/283], lr: 0.00000040 	 loss = 0.0349(0.3293)
2023/12/02 20:59:40 - INFO - root -   Epoch: [170/300][60/283], lr: 0.00000040 	 loss = 0.6748(0.3213)
2023/12/02 20:59:48 - INFO - root -   Epoch: [170/300][80/283], lr: 0.00000040 	 loss = 0.0280(0.3005)
2023/12/02 21:00:07 - INFO - root -   Epoch: [170/300][100/283], lr: 0.00000040 	 loss = 0.1093(0.2836)
2023/12/02 21:00:19 - INFO - root -   Epoch: [170/300][120/283], lr: 0.00000040 	 loss = 1.2906(0.3117)
2023/12/02 21:00:27 - INFO - root -   Epoch: [170/300][140/283], lr: 0.00000040 	 loss = 0.0182(0.3019)
2023/12/02 21:00:35 - INFO - root -   Epoch: [170/300][160/283], lr: 0.00000040 	 loss = 0.0699(0.2789)
2023/12/02 21:00:54 - INFO - root -   Epoch: [170/300][180/283], lr: 0.00000040 	 loss = 0.0443(0.2667)
2023/12/02 21:01:02 - INFO - root -   Epoch: [170/300][200/283], lr: 0.00000040 	 loss = 0.0812(0.2605)
2023/12/02 21:01:30 - INFO - root -   Epoch: [170/300][220/283], lr: 0.00000040 	 loss = 0.0117(0.2597)
2023/12/02 21:01:44 - INFO - root -   Epoch: [170/300][240/283], lr: 0.00000040 	 loss = 0.0906(0.2686)
2023/12/02 21:02:01 - INFO - root -   Epoch: [170/300][260/283], lr: 0.00000040 	 loss = 0.0110(0.2583)
2023/12/02 21:02:09 - INFO - root -   Epoch: [170/300][280/283], lr: 0.00000040 	 loss = 0.6402(0.2602)
2023/12/02 21:02:10 - INFO - root -   Epoch: [170/300] 	 loss = 0.2594
2023/12/02 21:02:10 - INFO - root -   train_accuracy = 0.9046
2023/12/02 21:02:13 - INFO - root -   Epoch: [171/300][0/283], lr: 0.00000040 	 loss = 0.0139(0.0139)
2023/12/02 21:02:36 - INFO - root -   Epoch: [171/300][20/283], lr: 0.00000040 	 loss = 0.0937(0.3012)
2023/12/02 21:02:50 - INFO - root -   Epoch: [171/300][40/283], lr: 0.00000040 	 loss = 0.0477(0.2736)
2023/12/02 21:03:04 - INFO - root -   Epoch: [171/300][60/283], lr: 0.00000040 	 loss = 0.6466(0.2860)
2023/12/02 21:03:12 - INFO - root -   Epoch: [171/300][80/283], lr: 0.00000040 	 loss = 0.0110(0.3149)
2023/12/02 21:03:30 - INFO - root -   Epoch: [171/300][100/283], lr: 0.00000040 	 loss = 0.4316(0.2937)
2023/12/02 21:03:41 - INFO - root -   Epoch: [171/300][120/283], lr: 0.00000040 	 loss = 0.7833(0.3288)
2023/12/02 21:04:04 - INFO - root -   Epoch: [171/300][140/283], lr: 0.00000040 	 loss = 0.3490(0.2999)
2023/12/02 21:04:12 - INFO - root -   Epoch: [171/300][160/283], lr: 0.00000040 	 loss = 0.0287(0.2730)
2023/12/02 21:04:30 - INFO - root -   Epoch: [171/300][180/283], lr: 0.00000040 	 loss = 0.6541(0.2697)
2023/12/02 21:04:38 - INFO - root -   Epoch: [171/300][200/283], lr: 0.00000040 	 loss = 0.8568(0.2564)
2023/12/02 21:05:09 - INFO - root -   Epoch: [171/300][220/283], lr: 0.00000040 	 loss = 0.1603(0.2558)
2023/12/02 21:05:17 - INFO - root -   Epoch: [171/300][240/283], lr: 0.00000040 	 loss = 0.1610(0.2790)
2023/12/02 21:05:33 - INFO - root -   Epoch: [171/300][260/283], lr: 0.00000040 	 loss = 0.0337(0.2738)
2023/12/02 21:05:41 - INFO - root -   Epoch: [171/300][280/283], lr: 0.00000040 	 loss = 0.0154(0.2713)
2023/12/02 21:05:42 - INFO - root -   Epoch: [171/300] 	 loss = 0.2698
2023/12/02 21:05:42 - INFO - root -   train_accuracy = 0.8940
2023/12/02 21:05:44 - INFO - root -   Epoch: [172/300][0/283], lr: 0.00000040 	 loss = 0.0200(0.0200)
2023/12/02 21:06:07 - INFO - root -   Epoch: [172/300][20/283], lr: 0.00000040 	 loss = 0.4860(0.2480)
2023/12/02 21:06:15 - INFO - root -   Epoch: [172/300][40/283], lr: 0.00000040 	 loss = 0.0634(0.2683)
2023/12/02 21:06:32 - INFO - root -   Epoch: [172/300][60/283], lr: 0.00000040 	 loss = 0.6769(0.2803)
2023/12/02 21:06:48 - INFO - root -   Epoch: [172/300][80/283], lr: 0.00000040 	 loss = 0.0975(0.2703)
2023/12/02 21:07:04 - INFO - root -   Epoch: [172/300][100/283], lr: 0.00000040 	 loss = 0.0865(0.2498)
2023/12/02 21:07:19 - INFO - root -   Epoch: [172/300][120/283], lr: 0.00000040 	 loss = 0.0611(0.2906)
2023/12/02 21:07:38 - INFO - root -   Epoch: [172/300][140/283], lr: 0.00000040 	 loss = 0.1104(0.2900)
2023/12/02 21:07:46 - INFO - root -   Epoch: [172/300][160/283], lr: 0.00000040 	 loss = 1.1993(0.2795)
2023/12/02 21:08:09 - INFO - root -   Epoch: [172/300][180/283], lr: 0.00000040 	 loss = 0.4830(0.2774)
2023/12/02 21:08:24 - INFO - root -   Epoch: [172/300][200/283], lr: 0.00000040 	 loss = 0.0587(0.2707)
2023/12/02 21:08:34 - INFO - root -   Epoch: [172/300][220/283], lr: 0.00000040 	 loss = 0.0068(0.2600)
2023/12/02 21:08:52 - INFO - root -   Epoch: [172/300][240/283], lr: 0.00000040 	 loss = 0.0166(0.2687)
2023/12/02 21:09:09 - INFO - root -   Epoch: [172/300][260/283], lr: 0.00000040 	 loss = 0.0540(0.2640)
2023/12/02 21:09:17 - INFO - root -   Epoch: [172/300][280/283], lr: 0.00000040 	 loss = 0.5955(0.2592)
2023/12/02 21:09:18 - INFO - root -   Epoch: [172/300] 	 loss = 0.2577
2023/12/02 21:09:18 - INFO - root -   train_accuracy = 0.8975
2023/12/02 21:09:20 - INFO - root -   Epoch: [173/300][0/283], lr: 0.00000040 	 loss = 0.2268(0.2268)
2023/12/02 21:09:51 - INFO - root -   Epoch: [173/300][20/283], lr: 0.00000040 	 loss = 0.1121(0.3807)
2023/12/02 21:09:58 - INFO - root -   Epoch: [173/300][40/283], lr: 0.00000040 	 loss = 0.0323(0.4123)
2023/12/02 21:10:25 - INFO - root -   Epoch: [173/300][60/283], lr: 0.00000040 	 loss = 2.1344(0.4711)
2023/12/02 21:10:36 - INFO - root -   Epoch: [173/300][80/283], lr: 0.00000040 	 loss = 0.0415(0.4645)
2023/12/02 21:10:44 - INFO - root -   Epoch: [173/300][100/283], lr: 0.00000040 	 loss = 0.0189(0.4148)
2023/12/02 21:11:08 - INFO - root -   Epoch: [173/300][120/283], lr: 0.00000040 	 loss = 0.3228(0.4266)
2023/12/02 21:11:16 - INFO - root -   Epoch: [173/300][140/283], lr: 0.00000040 	 loss = 0.1828(0.3890)
2023/12/02 21:11:44 - INFO - root -   Epoch: [173/300][160/283], lr: 0.00000040 	 loss = 0.0600(0.3599)
2023/12/02 21:11:52 - INFO - root -   Epoch: [173/300][180/283], lr: 0.00000040 	 loss = 0.2379(0.3522)
2023/12/02 21:12:00 - INFO - root -   Epoch: [173/300][200/283], lr: 0.00000040 	 loss = 0.0607(0.3475)
2023/12/02 21:12:11 - INFO - root -   Epoch: [173/300][220/283], lr: 0.00000040 	 loss = 0.1852(0.3400)
2023/12/02 21:12:29 - INFO - root -   Epoch: [173/300][240/283], lr: 0.00000040 	 loss = 0.2122(0.3294)
2023/12/02 21:12:47 - INFO - root -   Epoch: [173/300][260/283], lr: 0.00000040 	 loss = 0.0083(0.3233)
2023/12/02 21:12:55 - INFO - root -   Epoch: [173/300][280/283], lr: 0.00000040 	 loss = 0.0693(0.3102)
2023/12/02 21:12:56 - INFO - root -   Epoch: [173/300] 	 loss = 0.3082
2023/12/02 21:12:56 - INFO - root -   train_accuracy = 0.8746
2023/12/02 21:13:00 - INFO - root -   Epoch: [174/300][0/283], lr: 0.00000040 	 loss = 0.0449(0.0449)
2023/12/02 21:13:36 - INFO - root -   Epoch: [174/300][20/283], lr: 0.00000040 	 loss = 1.0174(0.4157)
2023/12/02 21:13:44 - INFO - root -   Epoch: [174/300][40/283], lr: 0.00000040 	 loss = 0.3455(0.3643)
2023/12/02 21:13:51 - INFO - root -   Epoch: [174/300][60/283], lr: 0.00000040 	 loss = 2.6813(0.3811)
2023/12/02 21:13:59 - INFO - root -   Epoch: [174/300][80/283], lr: 0.00000040 	 loss = 0.2433(0.3795)
2023/12/02 21:14:26 - INFO - root -   Epoch: [174/300][100/283], lr: 0.00000040 	 loss = 0.0966(0.3624)
2023/12/02 21:14:34 - INFO - root -   Epoch: [174/300][120/283], lr: 0.00000040 	 loss = 0.0640(0.3817)
2023/12/02 21:15:01 - INFO - root -   Epoch: [174/300][140/283], lr: 0.00000040 	 loss = 0.1251(0.3543)
2023/12/02 21:15:14 - INFO - root -   Epoch: [174/300][160/283], lr: 0.00000040 	 loss = 0.0384(0.3351)
2023/12/02 21:15:32 - INFO - root -   Epoch: [174/300][180/283], lr: 0.00000040 	 loss = 0.1501(0.3262)
2023/12/02 21:15:59 - INFO - root -   Epoch: [174/300][200/283], lr: 0.00000040 	 loss = 0.0576(0.3231)
2023/12/02 21:16:07 - INFO - root -   Epoch: [174/300][220/283], lr: 0.00000040 	 loss = 0.2254(0.3084)
2023/12/02 21:16:19 - INFO - root -   Epoch: [174/300][240/283], lr: 0.00000040 	 loss = 0.0651(0.3105)
2023/12/02 21:16:39 - INFO - root -   Epoch: [174/300][260/283], lr: 0.00000040 	 loss = 0.1195(0.3038)
2023/12/02 21:16:47 - INFO - root -   Epoch: [174/300][280/283], lr: 0.00000040 	 loss = 0.9278(0.2967)
2023/12/02 21:16:48 - INFO - root -   Epoch: [174/300] 	 loss = 0.2950
2023/12/02 21:16:57 - INFO - root -   precision = 0.8392
2023/12/02 21:16:57 - INFO - root -   eval_loss = 0.4538
2023/12/02 21:16:57 - INFO - root -   eval_acc = 0.8392
2023/12/02 21:16:58 - INFO - root -   train_accuracy = 0.8852
2023/12/02 21:17:11 - INFO - root -   Epoch: [175/300][0/283], lr: 0.00000041 	 loss = 0.1070(0.1070)
2023/12/02 21:17:29 - INFO - root -   Epoch: [175/300][20/283], lr: 0.00000041 	 loss = 0.3496(0.2578)
2023/12/02 21:17:37 - INFO - root -   Epoch: [175/300][40/283], lr: 0.00000041 	 loss = 0.0892(0.3326)
2023/12/02 21:17:55 - INFO - root -   Epoch: [175/300][60/283], lr: 0.00000041 	 loss = 1.6948(0.3786)
2023/12/02 21:18:07 - INFO - root -   Epoch: [175/300][80/283], lr: 0.00000041 	 loss = 0.0163(0.3659)
2023/12/02 21:18:31 - INFO - root -   Epoch: [175/300][100/283], lr: 0.00000041 	 loss = 0.3996(0.3488)
2023/12/02 21:18:51 - INFO - root -   Epoch: [175/300][120/283], lr: 0.00000041 	 loss = 0.0978(0.3492)
2023/12/02 21:18:59 - INFO - root -   Epoch: [175/300][140/283], lr: 0.00000041 	 loss = 0.0140(0.3163)
2023/12/02 21:19:11 - INFO - root -   Epoch: [175/300][160/283], lr: 0.00000041 	 loss = 0.8662(0.2987)
2023/12/02 21:19:26 - INFO - root -   Epoch: [175/300][180/283], lr: 0.00000041 	 loss = 0.4062(0.2871)
2023/12/02 21:19:51 - INFO - root -   Epoch: [175/300][200/283], lr: 0.00000041 	 loss = 0.3876(0.2748)
2023/12/02 21:19:59 - INFO - root -   Epoch: [175/300][220/283], lr: 0.00000041 	 loss = 0.1070(0.2620)
2023/12/02 21:20:17 - INFO - root -   Epoch: [175/300][240/283], lr: 0.00000041 	 loss = 0.2155(0.2593)
2023/12/02 21:20:25 - INFO - root -   Epoch: [175/300][260/283], lr: 0.00000041 	 loss = 0.1283(0.2573)
2023/12/02 21:20:33 - INFO - root -   Epoch: [175/300][280/283], lr: 0.00000041 	 loss = 0.0327(0.2494)
2023/12/02 21:20:34 - INFO - root -   Epoch: [175/300] 	 loss = 0.2490
2023/12/02 21:20:34 - INFO - root -   train_accuracy = 0.8993
2023/12/02 21:21:03 - INFO - root -   Epoch: [176/300][0/283], lr: 0.00000041 	 loss = 1.3417(1.3417)
2023/12/02 21:21:21 - INFO - root -   Epoch: [176/300][20/283], lr: 0.00000041 	 loss = 1.1282(0.3903)
2023/12/02 21:21:32 - INFO - root -   Epoch: [176/300][40/283], lr: 0.00000041 	 loss = 0.0704(0.3428)
2023/12/02 21:21:55 - INFO - root -   Epoch: [176/300][60/283], lr: 0.00000041 	 loss = 1.7023(0.3867)
2023/12/02 21:22:12 - INFO - root -   Epoch: [176/300][80/283], lr: 0.00000041 	 loss = 0.5492(0.3715)
2023/12/02 21:22:23 - INFO - root -   Epoch: [176/300][100/283], lr: 0.00000041 	 loss = 0.2595(0.3368)
2023/12/02 21:22:45 - INFO - root -   Epoch: [176/300][120/283], lr: 0.00000041 	 loss = 0.7060(0.3664)
2023/12/02 21:22:53 - INFO - root -   Epoch: [176/300][140/283], lr: 0.00000041 	 loss = 0.0194(0.3254)
2023/12/02 21:23:19 - INFO - root -   Epoch: [176/300][160/283], lr: 0.00000041 	 loss = 0.7521(0.2995)
2023/12/02 21:23:28 - INFO - root -   Epoch: [176/300][180/283], lr: 0.00000041 	 loss = 0.0555(0.2866)
2023/12/02 21:23:48 - INFO - root -   Epoch: [176/300][200/283], lr: 0.00000041 	 loss = 0.2233(0.2746)
2023/12/02 21:24:01 - INFO - root -   Epoch: [176/300][220/283], lr: 0.00000041 	 loss = 0.0102(0.2723)
2023/12/02 21:24:09 - INFO - root -   Epoch: [176/300][240/283], lr: 0.00000041 	 loss = 0.1048(0.2699)
2023/12/02 21:24:26 - INFO - root -   Epoch: [176/300][260/283], lr: 0.00000041 	 loss = 0.0610(0.2751)
2023/12/02 21:24:34 - INFO - root -   Epoch: [176/300][280/283], lr: 0.00000041 	 loss = 0.2646(0.2758)
2023/12/02 21:24:35 - INFO - root -   Epoch: [176/300] 	 loss = 0.2742
2023/12/02 21:24:35 - INFO - root -   train_accuracy = 0.8958
2023/12/02 21:24:50 - INFO - root -   Epoch: [177/300][0/283], lr: 0.00000041 	 loss = 0.0315(0.0315)
2023/12/02 21:25:12 - INFO - root -   Epoch: [177/300][20/283], lr: 0.00000041 	 loss = 0.3400(0.3399)
2023/12/02 21:25:20 - INFO - root -   Epoch: [177/300][40/283], lr: 0.00000041 	 loss = 0.3863(0.2731)
2023/12/02 21:25:40 - INFO - root -   Epoch: [177/300][60/283], lr: 0.00000041 	 loss = 1.0988(0.3243)
2023/12/02 21:25:51 - INFO - root -   Epoch: [177/300][80/283], lr: 0.00000041 	 loss = 0.0380(0.3480)
2023/12/02 21:26:13 - INFO - root -   Epoch: [177/300][100/283], lr: 0.00000041 	 loss = 0.1101(0.3069)
2023/12/02 21:26:40 - INFO - root -   Epoch: [177/300][120/283], lr: 0.00000041 	 loss = 1.2296(0.3513)
2023/12/02 21:26:48 - INFO - root -   Epoch: [177/300][140/283], lr: 0.00000041 	 loss = 0.0246(0.3210)
2023/12/02 21:27:02 - INFO - root -   Epoch: [177/300][160/283], lr: 0.00000041 	 loss = 0.0217(0.2904)
2023/12/02 21:27:14 - INFO - root -   Epoch: [177/300][180/283], lr: 0.00000041 	 loss = 0.1412(0.2767)
2023/12/02 21:27:38 - INFO - root -   Epoch: [177/300][200/283], lr: 0.00000041 	 loss = 0.0429(0.2723)
2023/12/02 21:28:01 - INFO - root -   Epoch: [177/300][220/283], lr: 0.00000041 	 loss = 0.0565(0.2695)
2023/12/02 21:28:31 - INFO - root -   Epoch: [177/300][240/283], lr: 0.00000041 	 loss = 0.3271(0.2802)
2023/12/02 21:28:39 - INFO - root -   Epoch: [177/300][260/283], lr: 0.00000041 	 loss = 0.0085(0.2735)
2023/12/02 21:28:47 - INFO - root -   Epoch: [177/300][280/283], lr: 0.00000041 	 loss = 1.0614(0.2738)
2023/12/02 21:28:48 - INFO - root -   Epoch: [177/300] 	 loss = 0.2731
2023/12/02 21:28:48 - INFO - root -   train_accuracy = 0.8975
2023/12/02 21:28:49 - INFO - root -   Epoch: [178/300][0/283], lr: 0.00000041 	 loss = 0.0026(0.0026)
2023/12/02 21:29:18 - INFO - root -   Epoch: [178/300][20/283], lr: 0.00000041 	 loss = 0.2245(0.4354)
2023/12/02 21:29:34 - INFO - root -   Epoch: [178/300][40/283], lr: 0.00000041 	 loss = 0.2938(0.3751)
2023/12/02 21:29:42 - INFO - root -   Epoch: [178/300][60/283], lr: 0.00000041 	 loss = 0.7522(0.3619)
2023/12/02 21:30:05 - INFO - root -   Epoch: [178/300][80/283], lr: 0.00000041 	 loss = 0.3496(0.3454)
2023/12/02 21:30:22 - INFO - root -   Epoch: [178/300][100/283], lr: 0.00000041 	 loss = 0.2613(0.3289)
2023/12/02 21:30:31 - INFO - root -   Epoch: [178/300][120/283], lr: 0.00000041 	 loss = 0.0650(0.3373)
2023/12/02 21:30:39 - INFO - root -   Epoch: [178/300][140/283], lr: 0.00000041 	 loss = 0.0195(0.3080)
2023/12/02 21:30:56 - INFO - root -   Epoch: [178/300][160/283], lr: 0.00000041 	 loss = 0.0392(0.2879)
2023/12/02 21:31:04 - INFO - root -   Epoch: [178/300][180/283], lr: 0.00000041 	 loss = 0.1825(0.2729)
2023/12/02 21:31:18 - INFO - root -   Epoch: [178/300][200/283], lr: 0.00000041 	 loss = 0.6503(0.2679)
2023/12/02 21:31:36 - INFO - root -   Epoch: [178/300][220/283], lr: 0.00000041 	 loss = 0.0916(0.2627)
2023/12/02 21:31:44 - INFO - root -   Epoch: [178/300][240/283], lr: 0.00000041 	 loss = 0.1838(0.2536)
2023/12/02 21:32:06 - INFO - root -   Epoch: [178/300][260/283], lr: 0.00000041 	 loss = 0.0206(0.2501)
2023/12/02 21:32:14 - INFO - root -   Epoch: [178/300][280/283], lr: 0.00000041 	 loss = 0.0576(0.2429)
2023/12/02 21:32:15 - INFO - root -   Epoch: [178/300] 	 loss = 0.2414
2023/12/02 21:32:15 - INFO - root -   train_accuracy = 0.9099
2023/12/02 21:32:31 - INFO - root -   Epoch: [179/300][0/283], lr: 0.00000041 	 loss = 1.1361(1.1361)
2023/12/02 21:32:48 - INFO - root -   Epoch: [179/300][20/283], lr: 0.00000041 	 loss = 1.7925(0.4967)
2023/12/02 21:33:06 - INFO - root -   Epoch: [179/300][40/283], lr: 0.00000041 	 loss = 0.2031(0.3750)
2023/12/02 21:33:14 - INFO - root -   Epoch: [179/300][60/283], lr: 0.00000041 	 loss = 1.5729(0.3504)
2023/12/02 21:33:24 - INFO - root -   Epoch: [179/300][80/283], lr: 0.00000041 	 loss = 0.4872(0.3473)
2023/12/02 21:33:58 - INFO - root -   Epoch: [179/300][100/283], lr: 0.00000041 	 loss = 0.1122(0.3256)
2023/12/02 21:34:13 - INFO - root -   Epoch: [179/300][120/283], lr: 0.00000041 	 loss = 0.3156(0.3287)
2023/12/02 21:34:28 - INFO - root -   Epoch: [179/300][140/283], lr: 0.00000041 	 loss = 0.1361(0.3169)
2023/12/02 21:34:42 - INFO - root -   Epoch: [179/300][160/283], lr: 0.00000041 	 loss = 0.2420(0.2889)
2023/12/02 21:35:01 - INFO - root -   Epoch: [179/300][180/283], lr: 0.00000041 	 loss = 0.1223(0.2803)
2023/12/02 21:35:09 - INFO - root -   Epoch: [179/300][200/283], lr: 0.00000041 	 loss = 0.1005(0.2664)
2023/12/02 21:35:17 - INFO - root -   Epoch: [179/300][220/283], lr: 0.00000041 	 loss = 0.0414(0.2546)
2023/12/02 21:35:40 - INFO - root -   Epoch: [179/300][240/283], lr: 0.00000041 	 loss = 0.1210(0.2517)
2023/12/02 21:35:48 - INFO - root -   Epoch: [179/300][260/283], lr: 0.00000041 	 loss = 0.0029(0.2570)
2023/12/02 21:35:57 - INFO - root -   Epoch: [179/300][280/283], lr: 0.00000041 	 loss = 0.7897(0.2575)
2023/12/02 21:35:58 - INFO - root -   Epoch: [179/300] 	 loss = 0.2558
2023/12/02 21:36:07 - INFO - root -   precision = 0.8252
2023/12/02 21:36:07 - INFO - root -   eval_loss = 0.4727
2023/12/02 21:36:07 - INFO - root -   eval_acc = 0.8252
2023/12/02 21:36:08 - INFO - root -   train_accuracy = 0.8993
2023/12/02 21:36:09 - INFO - root -   Epoch: [180/300][0/283], lr: 0.00000041 	 loss = 0.2637(0.2637)
2023/12/02 21:36:45 - INFO - root -   Epoch: [180/300][20/283], lr: 0.00000041 	 loss = 0.3168(0.3105)
2023/12/02 21:36:53 - INFO - root -   Epoch: [180/300][40/283], lr: 0.00000041 	 loss = 0.3720(0.2958)
2023/12/02 21:37:13 - INFO - root -   Epoch: [180/300][60/283], lr: 0.00000041 	 loss = 0.5849(0.2910)
2023/12/02 21:37:34 - INFO - root -   Epoch: [180/300][80/283], lr: 0.00000041 	 loss = 0.0084(0.3202)
2023/12/02 21:37:42 - INFO - root -   Epoch: [180/300][100/283], lr: 0.00000041 	 loss = 0.1462(0.2944)
2023/12/02 21:38:04 - INFO - root -   Epoch: [180/300][120/283], lr: 0.00000041 	 loss = 0.0244(0.3170)
2023/12/02 21:38:12 - INFO - root -   Epoch: [180/300][140/283], lr: 0.00000041 	 loss = 0.0195(0.2939)
2023/12/02 21:38:37 - INFO - root -   Epoch: [180/300][160/283], lr: 0.00000041 	 loss = 0.0388(0.2763)
2023/12/02 21:39:01 - INFO - root -   Epoch: [180/300][180/283], lr: 0.00000041 	 loss = 0.5495(0.2803)
2023/12/02 21:39:09 - INFO - root -   Epoch: [180/300][200/283], lr: 0.00000041 	 loss = 1.0701(0.2838)
2023/12/02 21:39:17 - INFO - root -   Epoch: [180/300][220/283], lr: 0.00000041 	 loss = 0.0052(0.2825)
2023/12/02 21:39:31 - INFO - root -   Epoch: [180/300][240/283], lr: 0.00000041 	 loss = 0.1788(0.2788)
2023/12/02 21:39:43 - INFO - root -   Epoch: [180/300][260/283], lr: 0.00000041 	 loss = 0.0128(0.2737)
2023/12/02 21:39:51 - INFO - root -   Epoch: [180/300][280/283], lr: 0.00000041 	 loss = 0.6484(0.2689)
2023/12/02 21:39:52 - INFO - root -   Epoch: [180/300] 	 loss = 0.2671
2023/12/02 21:39:52 - INFO - root -   train_accuracy = 0.8869
2023/12/02 21:40:08 - INFO - root -   Epoch: [181/300][0/283], lr: 0.00000042 	 loss = 0.7248(0.7248)
2023/12/02 21:40:16 - INFO - root -   Epoch: [181/300][20/283], lr: 0.00000042 	 loss = 0.1430(0.3120)
2023/12/02 21:40:31 - INFO - root -   Epoch: [181/300][40/283], lr: 0.00000042 	 loss = 0.0313(0.3143)
2023/12/02 21:40:50 - INFO - root -   Epoch: [181/300][60/283], lr: 0.00000042 	 loss = 1.4956(0.3556)
2023/12/02 21:41:06 - INFO - root -   Epoch: [181/300][80/283], lr: 0.00000042 	 loss = 0.1033(0.3401)
2023/12/02 21:41:28 - INFO - root -   Epoch: [181/300][100/283], lr: 0.00000042 	 loss = 0.1809(0.3288)
2023/12/02 21:41:36 - INFO - root -   Epoch: [181/300][120/283], lr: 0.00000042 	 loss = 2.0446(0.3528)
2023/12/02 21:41:50 - INFO - root -   Epoch: [181/300][140/283], lr: 0.00000042 	 loss = 0.0096(0.3314)
2023/12/02 21:42:04 - INFO - root -   Epoch: [181/300][160/283], lr: 0.00000042 	 loss = 0.0396(0.3053)
2023/12/02 21:42:12 - INFO - root -   Epoch: [181/300][180/283], lr: 0.00000042 	 loss = 0.1042(0.2932)
2023/12/02 21:42:27 - INFO - root -   Epoch: [181/300][200/283], lr: 0.00000042 	 loss = 0.0746(0.2789)
2023/12/02 21:42:42 - INFO - root -   Epoch: [181/300][220/283], lr: 0.00000042 	 loss = 0.0183(0.2657)
2023/12/02 21:43:03 - INFO - root -   Epoch: [181/300][240/283], lr: 0.00000042 	 loss = 0.0676(0.2662)
2023/12/02 21:43:17 - INFO - root -   Epoch: [181/300][260/283], lr: 0.00000042 	 loss = 0.0319(0.2576)
2023/12/02 21:43:25 - INFO - root -   Epoch: [181/300][280/283], lr: 0.00000042 	 loss = 0.0336(0.2514)
2023/12/02 21:43:26 - INFO - root -   Epoch: [181/300] 	 loss = 0.2501
2023/12/02 21:43:26 - INFO - root -   train_accuracy = 0.8993
2023/12/02 21:43:27 - INFO - root -   Epoch: [182/300][0/283], lr: 0.00000042 	 loss = 0.0402(0.0402)
2023/12/02 21:44:02 - INFO - root -   Epoch: [182/300][20/283], lr: 0.00000042 	 loss = 0.3696(0.3959)
2023/12/02 21:44:10 - INFO - root -   Epoch: [182/300][40/283], lr: 0.00000042 	 loss = 0.0241(0.3316)
2023/12/02 21:44:28 - INFO - root -   Epoch: [182/300][60/283], lr: 0.00000042 	 loss = 0.6180(0.3718)
2023/12/02 21:44:43 - INFO - root -   Epoch: [182/300][80/283], lr: 0.00000042 	 loss = 0.0564(0.3412)
2023/12/02 21:44:55 - INFO - root -   Epoch: [182/300][100/283], lr: 0.00000042 	 loss = 0.1773(0.3225)
2023/12/02 21:45:14 - INFO - root -   Epoch: [182/300][120/283], lr: 0.00000042 	 loss = 1.0552(0.3446)
2023/12/02 21:45:31 - INFO - root -   Epoch: [182/300][140/283], lr: 0.00000042 	 loss = 0.0905(0.3278)
2023/12/02 21:45:39 - INFO - root -   Epoch: [182/300][160/283], lr: 0.00000042 	 loss = 1.4493(0.3139)
2023/12/02 21:46:05 - INFO - root -   Epoch: [182/300][180/283], lr: 0.00000042 	 loss = 0.4746(0.3039)
2023/12/02 21:46:19 - INFO - root -   Epoch: [182/300][200/283], lr: 0.00000042 	 loss = 0.0581(0.2980)
2023/12/02 21:46:39 - INFO - root -   Epoch: [182/300][220/283], lr: 0.00000042 	 loss = 0.1688(0.2853)
2023/12/02 21:46:47 - INFO - root -   Epoch: [182/300][240/283], lr: 0.00000042 	 loss = 0.1998(0.2800)
2023/12/02 21:47:05 - INFO - root -   Epoch: [182/300][260/283], lr: 0.00000042 	 loss = 0.0779(0.2756)
2023/12/02 21:47:13 - INFO - root -   Epoch: [182/300][280/283], lr: 0.00000042 	 loss = 0.2095(0.2754)
2023/12/02 21:47:14 - INFO - root -   Epoch: [182/300] 	 loss = 0.2734
2023/12/02 21:47:14 - INFO - root -   train_accuracy = 0.8975
2023/12/02 21:47:28 - INFO - root -   Epoch: [183/300][0/283], lr: 0.00000042 	 loss = 0.1098(0.1098)
2023/12/02 21:47:50 - INFO - root -   Epoch: [183/300][20/283], lr: 0.00000042 	 loss = 1.0194(0.4149)
2023/12/02 21:48:01 - INFO - root -   Epoch: [183/300][40/283], lr: 0.00000042 	 loss = 0.0515(0.3207)
2023/12/02 21:48:29 - INFO - root -   Epoch: [183/300][60/283], lr: 0.00000042 	 loss = 0.6671(0.3667)
2023/12/02 21:48:38 - INFO - root -   Epoch: [183/300][80/283], lr: 0.00000042 	 loss = 0.0399(0.3644)
2023/12/02 21:49:00 - INFO - root -   Epoch: [183/300][100/283], lr: 0.00000042 	 loss = 0.1844(0.3386)
2023/12/02 21:49:17 - INFO - root -   Epoch: [183/300][120/283], lr: 0.00000042 	 loss = 1.4774(0.3343)
2023/12/02 21:49:29 - INFO - root -   Epoch: [183/300][140/283], lr: 0.00000042 	 loss = 0.0047(0.3053)
2023/12/02 21:49:45 - INFO - root -   Epoch: [183/300][160/283], lr: 0.00000042 	 loss = 0.0341(0.2904)
2023/12/02 21:50:05 - INFO - root -   Epoch: [183/300][180/283], lr: 0.00000042 	 loss = 1.0270(0.2825)
2023/12/02 21:50:13 - INFO - root -   Epoch: [183/300][200/283], lr: 0.00000042 	 loss = 0.0218(0.2642)
2023/12/02 21:50:26 - INFO - root -   Epoch: [183/300][220/283], lr: 0.00000042 	 loss = 0.1048(0.2556)
2023/12/02 21:50:43 - INFO - root -   Epoch: [183/300][240/283], lr: 0.00000042 	 loss = 0.0224(0.2485)
2023/12/02 21:51:01 - INFO - root -   Epoch: [183/300][260/283], lr: 0.00000042 	 loss = 0.0070(0.2469)
2023/12/02 21:51:09 - INFO - root -   Epoch: [183/300][280/283], lr: 0.00000042 	 loss = 0.2376(0.2458)
2023/12/02 21:51:10 - INFO - root -   Epoch: [183/300] 	 loss = 0.2445
2023/12/02 21:51:10 - INFO - root -   train_accuracy = 0.9028
2023/12/02 21:51:23 - INFO - root -   Epoch: [184/300][0/283], lr: 0.00000042 	 loss = 0.0133(0.0133)
2023/12/02 21:51:45 - INFO - root -   Epoch: [184/300][20/283], lr: 0.00000042 	 loss = 0.3189(0.3670)
2023/12/02 21:52:03 - INFO - root -   Epoch: [184/300][40/283], lr: 0.00000042 	 loss = 0.2711(0.3780)
2023/12/02 21:52:22 - INFO - root -   Epoch: [184/300][60/283], lr: 0.00000042 	 loss = 2.1258(0.3420)
2023/12/02 21:52:33 - INFO - root -   Epoch: [184/300][80/283], lr: 0.00000042 	 loss = 0.0515(0.3432)
2023/12/02 21:52:47 - INFO - root -   Epoch: [184/300][100/283], lr: 0.00000042 	 loss = 0.0572(0.3047)
2023/12/02 21:53:04 - INFO - root -   Epoch: [184/300][120/283], lr: 0.00000042 	 loss = 1.5144(0.3176)
2023/12/02 21:53:30 - INFO - root -   Epoch: [184/300][140/283], lr: 0.00000042 	 loss = 0.0109(0.3073)
2023/12/02 21:53:38 - INFO - root -   Epoch: [184/300][160/283], lr: 0.00000042 	 loss = 0.2475(0.3004)
2023/12/02 21:53:52 - INFO - root -   Epoch: [184/300][180/283], lr: 0.00000042 	 loss = 1.1793(0.3007)
2023/12/02 21:54:00 - INFO - root -   Epoch: [184/300][200/283], lr: 0.00000042 	 loss = 0.1389(0.2849)
2023/12/02 21:54:14 - INFO - root -   Epoch: [184/300][220/283], lr: 0.00000042 	 loss = 0.0683(0.2708)
2023/12/02 21:54:35 - INFO - root -   Epoch: [184/300][240/283], lr: 0.00000042 	 loss = 0.0110(0.2708)
2023/12/02 21:54:43 - INFO - root -   Epoch: [184/300][260/283], lr: 0.00000042 	 loss = 0.0199(0.2685)
2023/12/02 21:54:51 - INFO - root -   Epoch: [184/300][280/283], lr: 0.00000042 	 loss = 0.9992(0.2662)
2023/12/02 21:54:52 - INFO - root -   Epoch: [184/300] 	 loss = 0.2655
2023/12/02 21:55:01 - INFO - root -   precision = 0.8531
2023/12/02 21:55:01 - INFO - root -   eval_loss = 0.4606
2023/12/02 21:55:01 - INFO - root -   eval_acc = 0.8531
2023/12/02 21:55:02 - INFO - root -   train_accuracy = 0.9064
2023/12/02 21:55:03 - INFO - root -   Epoch: [185/300][0/283], lr: 0.00000042 	 loss = 0.0097(0.0097)
2023/12/02 21:55:32 - INFO - root -   Epoch: [185/300][20/283], lr: 0.00000042 	 loss = 0.1078(0.1686)
2023/12/02 21:55:40 - INFO - root -   Epoch: [185/300][40/283], lr: 0.00000042 	 loss = 0.0375(0.2667)
2023/12/02 21:55:52 - INFO - root -   Epoch: [185/300][60/283], lr: 0.00000042 	 loss = 1.1066(0.3020)
2023/12/02 21:56:00 - INFO - root -   Epoch: [185/300][80/283], lr: 0.00000042 	 loss = 0.2290(0.2713)
2023/12/02 21:56:22 - INFO - root -   Epoch: [185/300][100/283], lr: 0.00000042 	 loss = 0.1309(0.2679)
2023/12/02 21:56:36 - INFO - root -   Epoch: [185/300][120/283], lr: 0.00000042 	 loss = 1.1112(0.3156)
2023/12/02 21:56:56 - INFO - root -   Epoch: [185/300][140/283], lr: 0.00000042 	 loss = 0.2009(0.3117)
2023/12/02 21:57:14 - INFO - root -   Epoch: [185/300][160/283], lr: 0.00000042 	 loss = 0.0136(0.3069)
2023/12/02 21:57:21 - INFO - root -   Epoch: [185/300][180/283], lr: 0.00000042 	 loss = 0.0663(0.2968)
2023/12/02 21:57:42 - INFO - root -   Epoch: [185/300][200/283], lr: 0.00000042 	 loss = 0.1835(0.2870)
2023/12/02 21:57:50 - INFO - root -   Epoch: [185/300][220/283], lr: 0.00000042 	 loss = 0.0082(0.2701)
2023/12/02 21:58:14 - INFO - root -   Epoch: [185/300][240/283], lr: 0.00000042 	 loss = 0.0327(0.2678)
2023/12/02 21:58:32 - INFO - root -   Epoch: [185/300][260/283], lr: 0.00000042 	 loss = 0.4682(0.2785)
2023/12/02 21:58:40 - INFO - root -   Epoch: [185/300][280/283], lr: 0.00000042 	 loss = 0.4005(0.2742)
2023/12/02 21:58:41 - INFO - root -   Epoch: [185/300] 	 loss = 0.2739
2023/12/02 21:58:41 - INFO - root -   train_accuracy = 0.8887
2023/12/02 21:58:42 - INFO - root -   Epoch: [186/300][0/283], lr: 0.00000043 	 loss = 0.0336(0.0336)
2023/12/02 21:59:12 - INFO - root -   Epoch: [186/300][20/283], lr: 0.00000043 	 loss = 0.1846(0.2891)
2023/12/02 21:59:23 - INFO - root -   Epoch: [186/300][40/283], lr: 0.00000043 	 loss = 0.2713(0.3334)
2023/12/02 21:59:43 - INFO - root -   Epoch: [186/300][60/283], lr: 0.00000043 	 loss = 0.5903(0.3082)
2023/12/02 21:59:53 - INFO - root -   Epoch: [186/300][80/283], lr: 0.00000043 	 loss = 0.1592(0.2751)
2023/12/02 22:00:09 - INFO - root -   Epoch: [186/300][100/283], lr: 0.00000043 	 loss = 0.0066(0.2724)
2023/12/02 22:00:17 - INFO - root -   Epoch: [186/300][120/283], lr: 0.00000043 	 loss = 0.1048(0.2812)
2023/12/02 22:00:39 - INFO - root -   Epoch: [186/300][140/283], lr: 0.00000043 	 loss = 0.0063(0.2829)
2023/12/02 22:01:02 - INFO - root -   Epoch: [186/300][160/283], lr: 0.00000043 	 loss = 0.0230(0.2685)
2023/12/02 22:01:17 - INFO - root -   Epoch: [186/300][180/283], lr: 0.00000043 	 loss = 0.0233(0.2522)
2023/12/02 22:01:43 - INFO - root -   Epoch: [186/300][200/283], lr: 0.00000043 	 loss = 0.0490(0.2467)
2023/12/02 22:01:51 - INFO - root -   Epoch: [186/300][220/283], lr: 0.00000043 	 loss = 0.0021(0.2431)
2023/12/02 22:02:08 - INFO - root -   Epoch: [186/300][240/283], lr: 0.00000043 	 loss = 0.0494(0.2545)
2023/12/02 22:02:16 - INFO - root -   Epoch: [186/300][260/283], lr: 0.00000043 	 loss = 0.0168(0.2519)
2023/12/02 22:02:24 - INFO - root -   Epoch: [186/300][280/283], lr: 0.00000043 	 loss = 1.2174(0.2478)
2023/12/02 22:02:25 - INFO - root -   Epoch: [186/300] 	 loss = 0.2463
2023/12/02 22:02:25 - INFO - root -   train_accuracy = 0.8975
2023/12/02 22:02:28 - INFO - root -   Epoch: [187/300][0/283], lr: 0.00000043 	 loss = 0.0202(0.0202)
2023/12/02 22:02:54 - INFO - root -   Epoch: [187/300][20/283], lr: 0.00000043 	 loss = 0.3033(0.2524)
2023/12/02 22:03:01 - INFO - root -   Epoch: [187/300][40/283], lr: 0.00000043 	 loss = 0.0994(0.2451)
2023/12/02 22:03:09 - INFO - root -   Epoch: [187/300][60/283], lr: 0.00000043 	 loss = 0.4941(0.2944)
2023/12/02 22:03:27 - INFO - root -   Epoch: [187/300][80/283], lr: 0.00000043 	 loss = 0.5128(0.2989)
2023/12/02 22:03:51 - INFO - root -   Epoch: [187/300][100/283], lr: 0.00000043 	 loss = 0.0058(0.3278)
2023/12/02 22:04:16 - INFO - root -   Epoch: [187/300][120/283], lr: 0.00000043 	 loss = 0.9778(0.3340)
2023/12/02 22:04:26 - INFO - root -   Epoch: [187/300][140/283], lr: 0.00000043 	 loss = 0.0495(0.3158)
2023/12/02 22:04:44 - INFO - root -   Epoch: [187/300][160/283], lr: 0.00000043 	 loss = 0.0290(0.3051)
2023/12/02 22:04:55 - INFO - root -   Epoch: [187/300][180/283], lr: 0.00000043 	 loss = 0.0602(0.2913)
2023/12/02 22:05:18 - INFO - root -   Epoch: [187/300][200/283], lr: 0.00000043 	 loss = 0.1369(0.2960)
2023/12/02 22:05:34 - INFO - root -   Epoch: [187/300][220/283], lr: 0.00000043 	 loss = 0.0118(0.2842)
2023/12/02 22:05:42 - INFO - root -   Epoch: [187/300][240/283], lr: 0.00000043 	 loss = 0.0905(0.2837)
2023/12/02 22:06:09 - INFO - root -   Epoch: [187/300][260/283], lr: 0.00000043 	 loss = 0.0030(0.2872)
2023/12/02 22:06:17 - INFO - root -   Epoch: [187/300][280/283], lr: 0.00000043 	 loss = 1.1512(0.2836)
2023/12/02 22:06:18 - INFO - root -   Epoch: [187/300] 	 loss = 0.2820
2023/12/02 22:06:18 - INFO - root -   train_accuracy = 0.8551
2023/12/02 22:06:34 - INFO - root -   Epoch: [188/300][0/283], lr: 0.00000043 	 loss = 0.0655(0.0655)
2023/12/02 22:06:57 - INFO - root -   Epoch: [188/300][20/283], lr: 0.00000043 	 loss = 1.6863(0.4588)
2023/12/02 22:07:16 - INFO - root -   Epoch: [188/300][40/283], lr: 0.00000043 	 loss = 0.3127(0.4064)
2023/12/02 22:07:32 - INFO - root -   Epoch: [188/300][60/283], lr: 0.00000043 	 loss = 0.7181(0.4027)
2023/12/02 22:07:40 - INFO - root -   Epoch: [188/300][80/283], lr: 0.00000043 	 loss = 0.0538(0.3691)
2023/12/02 22:07:51 - INFO - root -   Epoch: [188/300][100/283], lr: 0.00000043 	 loss = 0.2775(0.3373)
2023/12/02 22:07:59 - INFO - root -   Epoch: [188/300][120/283], lr: 0.00000043 	 loss = 0.6756(0.3522)
2023/12/02 22:08:07 - INFO - root -   Epoch: [188/300][140/283], lr: 0.00000043 	 loss = 0.1497(0.3157)
2023/12/02 22:08:29 - INFO - root -   Epoch: [188/300][160/283], lr: 0.00000043 	 loss = 0.0502(0.3021)
2023/12/02 22:08:50 - INFO - root -   Epoch: [188/300][180/283], lr: 0.00000043 	 loss = 0.6023(0.2893)
2023/12/02 22:08:58 - INFO - root -   Epoch: [188/300][200/283], lr: 0.00000043 	 loss = 0.1086(0.2855)
2023/12/02 22:09:17 - INFO - root -   Epoch: [188/300][220/283], lr: 0.00000043 	 loss = 0.1684(0.2737)
2023/12/02 22:09:34 - INFO - root -   Epoch: [188/300][240/283], lr: 0.00000043 	 loss = 0.0473(0.2805)
2023/12/02 22:09:42 - INFO - root -   Epoch: [188/300][260/283], lr: 0.00000043 	 loss = 0.0426(0.2702)
2023/12/02 22:09:51 - INFO - root -   Epoch: [188/300][280/283], lr: 0.00000043 	 loss = 0.2761(0.2619)
2023/12/02 22:09:52 - INFO - root -   Epoch: [188/300] 	 loss = 0.2603
2023/12/02 22:09:52 - INFO - root -   train_accuracy = 0.8922
2023/12/02 22:10:07 - INFO - root -   Epoch: [189/300][0/283], lr: 0.00000043 	 loss = 0.0949(0.0949)
2023/12/02 22:10:31 - INFO - root -   Epoch: [189/300][20/283], lr: 0.00000043 	 loss = 0.2274(0.4392)
2023/12/02 22:10:38 - INFO - root -   Epoch: [189/300][40/283], lr: 0.00000043 	 loss = 0.2125(0.3535)
2023/12/02 22:10:54 - INFO - root -   Epoch: [189/300][60/283], lr: 0.00000043 	 loss = 2.6927(0.3756)
2023/12/02 22:11:15 - INFO - root -   Epoch: [189/300][80/283], lr: 0.00000043 	 loss = 0.0178(0.3480)
2023/12/02 22:11:23 - INFO - root -   Epoch: [189/300][100/283], lr: 0.00000043 	 loss = 0.0787(0.3242)
2023/12/02 22:11:38 - INFO - root -   Epoch: [189/300][120/283], lr: 0.00000043 	 loss = 0.2330(0.3328)
2023/12/02 22:11:53 - INFO - root -   Epoch: [189/300][140/283], lr: 0.00000043 	 loss = 0.1174(0.3197)
2023/12/02 22:12:05 - INFO - root -   Epoch: [189/300][160/283], lr: 0.00000043 	 loss = 0.0817(0.2966)
2023/12/02 22:12:21 - INFO - root -   Epoch: [189/300][180/283], lr: 0.00000043 	 loss = 0.3632(0.2856)
2023/12/02 22:12:32 - INFO - root -   Epoch: [189/300][200/283], lr: 0.00000043 	 loss = 0.1230(0.2766)
2023/12/02 22:12:43 - INFO - root -   Epoch: [189/300][220/283], lr: 0.00000043 	 loss = 0.0010(0.2645)
2023/12/02 22:12:59 - INFO - root -   Epoch: [189/300][240/283], lr: 0.00000043 	 loss = 0.0993(0.2577)
2023/12/02 22:13:11 - INFO - root -   Epoch: [189/300][260/283], lr: 0.00000043 	 loss = 0.0030(0.2495)
2023/12/02 22:13:19 - INFO - root -   Epoch: [189/300][280/283], lr: 0.00000043 	 loss = 0.0618(0.2419)
2023/12/02 22:13:19 - INFO - root -   Epoch: [189/300] 	 loss = 0.2402
2023/12/02 22:13:29 - INFO - root -   precision = 0.8252
2023/12/02 22:13:29 - INFO - root -   eval_loss = 0.5454
2023/12/02 22:13:29 - INFO - root -   eval_acc = 0.8252
2023/12/02 22:13:29 - INFO - root -   train_accuracy = 0.9152
2023/12/02 22:13:54 - INFO - root -   Epoch: [190/300][0/283], lr: 0.00000043 	 loss = 0.4602(0.4602)
2023/12/02 22:14:02 - INFO - root -   Epoch: [190/300][20/283], lr: 0.00000043 	 loss = 0.2371(0.4047)
2023/12/02 22:14:23 - INFO - root -   Epoch: [190/300][40/283], lr: 0.00000043 	 loss = 0.1560(0.3695)
2023/12/02 22:14:31 - INFO - root -   Epoch: [190/300][60/283], lr: 0.00000043 	 loss = 0.5993(0.3491)
2023/12/02 22:14:46 - INFO - root -   Epoch: [190/300][80/283], lr: 0.00000043 	 loss = 0.0159(0.3298)
2023/12/02 22:15:03 - INFO - root -   Epoch: [190/300][100/283], lr: 0.00000043 	 loss = 0.0784(0.3199)
2023/12/02 22:15:18 - INFO - root -   Epoch: [190/300][120/283], lr: 0.00000043 	 loss = 0.0866(0.3153)
2023/12/02 22:15:34 - INFO - root -   Epoch: [190/300][140/283], lr: 0.00000043 	 loss = 0.0086(0.2924)
2023/12/02 22:15:49 - INFO - root -   Epoch: [190/300][160/283], lr: 0.00000043 	 loss = 0.4806(0.2807)
2023/12/02 22:16:10 - INFO - root -   Epoch: [190/300][180/283], lr: 0.00000043 	 loss = 0.0261(0.2798)
2023/12/02 22:16:18 - INFO - root -   Epoch: [190/300][200/283], lr: 0.00000043 	 loss = 0.0457(0.2725)
2023/12/02 22:16:41 - INFO - root -   Epoch: [190/300][220/283], lr: 0.00000043 	 loss = 0.1188(0.2589)
2023/12/02 22:17:08 - INFO - root -   Epoch: [190/300][240/283], lr: 0.00000043 	 loss = 0.0724(0.2524)
2023/12/02 22:17:16 - INFO - root -   Epoch: [190/300][260/283], lr: 0.00000043 	 loss = 0.0024(0.2523)
2023/12/02 22:17:23 - INFO - root -   Epoch: [190/300][280/283], lr: 0.00000043 	 loss = 0.0586(0.2500)
2023/12/02 22:17:24 - INFO - root -   Epoch: [190/300] 	 loss = 0.2482
2023/12/02 22:17:24 - INFO - root -   train_accuracy = 0.8940
2023/12/02 22:17:26 - INFO - root -   Epoch: [191/300][0/283], lr: 0.00000043 	 loss = 0.0096(0.0096)
2023/12/02 22:17:54 - INFO - root -   Epoch: [191/300][20/283], lr: 0.00000043 	 loss = 0.0284(0.1858)
2023/12/02 22:18:09 - INFO - root -   Epoch: [191/300][40/283], lr: 0.00000043 	 loss = 0.2582(0.2466)
2023/12/02 22:18:31 - INFO - root -   Epoch: [191/300][60/283], lr: 0.00000043 	 loss = 0.5949(0.2910)
2023/12/02 22:18:39 - INFO - root -   Epoch: [191/300][80/283], lr: 0.00000043 	 loss = 1.0343(0.2952)
2023/12/02 22:18:51 - INFO - root -   Epoch: [191/300][100/283], lr: 0.00000043 	 loss = 0.1148(0.2799)
2023/12/02 22:19:05 - INFO - root -   Epoch: [191/300][120/283], lr: 0.00000043 	 loss = 0.2265(0.3139)
2023/12/02 22:19:23 - INFO - root -   Epoch: [191/300][140/283], lr: 0.00000043 	 loss = 0.0421(0.3002)
2023/12/02 22:19:40 - INFO - root -   Epoch: [191/300][160/283], lr: 0.00000043 	 loss = 0.0129(0.2873)
2023/12/02 22:19:51 - INFO - root -   Epoch: [191/300][180/283], lr: 0.00000043 	 loss = 0.1079(0.2739)
2023/12/02 22:20:11 - INFO - root -   Epoch: [191/300][200/283], lr: 0.00000043 	 loss = 0.0085(0.2648)
2023/12/02 22:20:22 - INFO - root -   Epoch: [191/300][220/283], lr: 0.00000043 	 loss = 0.0014(0.2599)
2023/12/02 22:20:48 - INFO - root -   Epoch: [191/300][240/283], lr: 0.00000043 	 loss = 0.3688(0.2714)
2023/12/02 22:20:56 - INFO - root -   Epoch: [191/300][260/283], lr: 0.00000043 	 loss = 0.3097(0.2741)
2023/12/02 22:21:04 - INFO - root -   Epoch: [191/300][280/283], lr: 0.00000043 	 loss = 0.1414(0.2650)
2023/12/02 22:21:05 - INFO - root -   Epoch: [191/300] 	 loss = 0.2632
2023/12/02 22:21:05 - INFO - root -   train_accuracy = 0.8905
2023/12/02 22:21:19 - INFO - root -   Epoch: [192/300][0/283], lr: 0.00000044 	 loss = 0.2862(0.2862)
2023/12/02 22:21:54 - INFO - root -   Epoch: [192/300][20/283], lr: 0.00000044 	 loss = 0.2281(0.4054)
2023/12/02 22:22:05 - INFO - root -   Epoch: [192/300][40/283], lr: 0.00000044 	 loss = 0.2377(0.3884)
2023/12/02 22:22:16 - INFO - root -   Epoch: [192/300][60/283], lr: 0.00000044 	 loss = 1.5898(0.3778)
2023/12/02 22:22:36 - INFO - root -   Epoch: [192/300][80/283], lr: 0.00000044 	 loss = 0.3798(0.3811)
2023/12/02 22:22:50 - INFO - root -   Epoch: [192/300][100/283], lr: 0.00000044 	 loss = 0.0117(0.3402)
2023/12/02 22:22:58 - INFO - root -   Epoch: [192/300][120/283], lr: 0.00000044 	 loss = 0.0561(0.3814)
2023/12/02 22:23:22 - INFO - root -   Epoch: [192/300][140/283], lr: 0.00000044 	 loss = 0.1704(0.3464)
2023/12/02 22:23:31 - INFO - root -   Epoch: [192/300][160/283], lr: 0.00000044 	 loss = 0.7457(0.3361)
2023/12/02 22:23:43 - INFO - root -   Epoch: [192/300][180/283], lr: 0.00000044 	 loss = 0.0162(0.3105)
2023/12/02 22:24:07 - INFO - root -   Epoch: [192/300][200/283], lr: 0.00000044 	 loss = 0.0212(0.2964)
2023/12/02 22:24:18 - INFO - root -   Epoch: [192/300][220/283], lr: 0.00000044 	 loss = 0.0079(0.2831)
2023/12/02 22:24:26 - INFO - root -   Epoch: [192/300][240/283], lr: 0.00000044 	 loss = 0.0156(0.2872)
2023/12/02 22:24:45 - INFO - root -   Epoch: [192/300][260/283], lr: 0.00000044 	 loss = 0.0090(0.2769)
2023/12/02 22:24:53 - INFO - root -   Epoch: [192/300][280/283], lr: 0.00000044 	 loss = 0.8559(0.2734)
2023/12/02 22:24:54 - INFO - root -   Epoch: [192/300] 	 loss = 0.2719
2023/12/02 22:24:54 - INFO - root -   train_accuracy = 0.8922
2023/12/02 22:25:13 - INFO - root -   Epoch: [193/300][0/283], lr: 0.00000044 	 loss = 1.1854(1.1854)
2023/12/02 22:25:33 - INFO - root -   Epoch: [193/300][20/283], lr: 0.00000044 	 loss = 0.1150(0.2850)
2023/12/02 22:25:41 - INFO - root -   Epoch: [193/300][40/283], lr: 0.00000044 	 loss = 0.1934(0.3400)
2023/12/02 22:25:49 - INFO - root -   Epoch: [193/300][60/283], lr: 0.00000044 	 loss = 0.7242(0.3099)
2023/12/02 22:26:02 - INFO - root -   Epoch: [193/300][80/283], lr: 0.00000044 	 loss = 0.0132(0.2767)
2023/12/02 22:26:10 - INFO - root -   Epoch: [193/300][100/283], lr: 0.00000044 	 loss = 0.0384(0.2750)
2023/12/02 22:26:18 - INFO - root -   Epoch: [193/300][120/283], lr: 0.00000044 	 loss = 0.0310(0.2942)
2023/12/02 22:26:40 - INFO - root -   Epoch: [193/300][140/283], lr: 0.00000044 	 loss = 0.1597(0.2712)
2023/12/02 22:26:54 - INFO - root -   Epoch: [193/300][160/283], lr: 0.00000044 	 loss = 0.1019(0.2672)
2023/12/02 22:27:11 - INFO - root -   Epoch: [193/300][180/283], lr: 0.00000044 	 loss = 0.3288(0.2679)
2023/12/02 22:27:19 - INFO - root -   Epoch: [193/300][200/283], lr: 0.00000044 	 loss = 0.0174(0.2571)
2023/12/02 22:27:43 - INFO - root -   Epoch: [193/300][220/283], lr: 0.00000044 	 loss = 0.0049(0.2511)
2023/12/02 22:27:58 - INFO - root -   Epoch: [193/300][240/283], lr: 0.00000044 	 loss = 0.0645(0.2579)
2023/12/02 22:28:17 - INFO - root -   Epoch: [193/300][260/283], lr: 0.00000044 	 loss = 0.0988(0.2561)
2023/12/02 22:28:25 - INFO - root -   Epoch: [193/300][280/283], lr: 0.00000044 	 loss = 0.6329(0.2498)
2023/12/02 22:28:26 - INFO - root -   Epoch: [193/300] 	 loss = 0.2489
2023/12/02 22:28:26 - INFO - root -   train_accuracy = 0.9011
2023/12/02 22:28:28 - INFO - root -   Epoch: [194/300][0/283], lr: 0.00000044 	 loss = 0.0173(0.0173)
2023/12/02 22:29:09 - INFO - root -   Epoch: [194/300][20/283], lr: 0.00000044 	 loss = 0.2563(0.3423)
2023/12/02 22:29:17 - INFO - root -   Epoch: [194/300][40/283], lr: 0.00000044 	 loss = 0.1481(0.3357)
2023/12/02 22:29:45 - INFO - root -   Epoch: [194/300][60/283], lr: 0.00000044 	 loss = 1.1871(0.3465)
2023/12/02 22:29:53 - INFO - root -   Epoch: [194/300][80/283], lr: 0.00000044 	 loss = 0.6712(0.3308)
2023/12/02 22:30:10 - INFO - root -   Epoch: [194/300][100/283], lr: 0.00000044 	 loss = 0.4670(0.2999)
2023/12/02 22:30:22 - INFO - root -   Epoch: [194/300][120/283], lr: 0.00000044 	 loss = 0.7334(0.3276)
2023/12/02 22:30:35 - INFO - root -   Epoch: [194/300][140/283], lr: 0.00000044 	 loss = 0.2673(0.3340)
2023/12/02 22:30:50 - INFO - root -   Epoch: [194/300][160/283], lr: 0.00000044 	 loss = 0.0412(0.3198)
2023/12/02 22:31:11 - INFO - root -   Epoch: [194/300][180/283], lr: 0.00000044 	 loss = 0.4041(0.3064)
2023/12/02 22:31:28 - INFO - root -   Epoch: [194/300][200/283], lr: 0.00000044 	 loss = 0.0400(0.3167)
2023/12/02 22:31:46 - INFO - root -   Epoch: [194/300][220/283], lr: 0.00000044 	 loss = 0.0072(0.3054)
2023/12/02 22:32:08 - INFO - root -   Epoch: [194/300][240/283], lr: 0.00000044 	 loss = 0.0270(0.2962)
2023/12/02 22:32:16 - INFO - root -   Epoch: [194/300][260/283], lr: 0.00000044 	 loss = 0.2073(0.2903)
2023/12/02 22:32:24 - INFO - root -   Epoch: [194/300][280/283], lr: 0.00000044 	 loss = 0.0180(0.2842)
2023/12/02 22:32:24 - INFO - root -   Epoch: [194/300] 	 loss = 0.2826
2023/12/02 22:32:34 - INFO - root -   precision = 0.8322
2023/12/02 22:32:34 - INFO - root -   eval_loss = 0.4655
2023/12/02 22:32:34 - INFO - root -   eval_acc = 0.8322
2023/12/02 22:32:34 - INFO - root -   train_accuracy = 0.8852
2023/12/02 22:32:48 - INFO - root -   Epoch: [195/300][0/283], lr: 0.00000044 	 loss = 0.0234(0.0234)
2023/12/02 22:33:04 - INFO - root -   Epoch: [195/300][20/283], lr: 0.00000044 	 loss = 0.4433(0.2103)
2023/12/02 22:33:17 - INFO - root -   Epoch: [195/300][40/283], lr: 0.00000044 	 loss = 0.0210(0.3173)
2023/12/02 22:33:35 - INFO - root -   Epoch: [195/300][60/283], lr: 0.00000044 	 loss = 1.2023(0.3090)
2023/12/02 22:33:44 - INFO - root -   Epoch: [195/300][80/283], lr: 0.00000044 	 loss = 0.0611(0.3145)
2023/12/02 22:34:06 - INFO - root -   Epoch: [195/300][100/283], lr: 0.00000044 	 loss = 0.0075(0.3205)
2023/12/02 22:34:16 - INFO - root -   Epoch: [195/300][120/283], lr: 0.00000044 	 loss = 0.3961(0.3403)
2023/12/02 22:34:31 - INFO - root -   Epoch: [195/300][140/283], lr: 0.00000044 	 loss = 0.0538(0.3038)
2023/12/02 22:34:41 - INFO - root -   Epoch: [195/300][160/283], lr: 0.00000044 	 loss = 0.0149(0.2834)
2023/12/02 22:35:06 - INFO - root -   Epoch: [195/300][180/283], lr: 0.00000044 	 loss = 0.0939(0.2720)
2023/12/02 22:35:27 - INFO - root -   Epoch: [195/300][200/283], lr: 0.00000044 	 loss = 0.0688(0.2670)
2023/12/02 22:35:34 - INFO - root -   Epoch: [195/300][220/283], lr: 0.00000044 	 loss = 0.0022(0.2518)
2023/12/02 22:35:45 - INFO - root -   Epoch: [195/300][240/283], lr: 0.00000044 	 loss = 0.0029(0.2495)
2023/12/02 22:36:02 - INFO - root -   Epoch: [195/300][260/283], lr: 0.00000044 	 loss = 0.0051(0.2498)
2023/12/02 22:36:10 - INFO - root -   Epoch: [195/300][280/283], lr: 0.00000044 	 loss = 0.0214(0.2452)
2023/12/02 22:36:11 - INFO - root -   Epoch: [195/300] 	 loss = 0.2441
2023/12/02 22:36:11 - INFO - root -   train_accuracy = 0.9046
2023/12/02 22:36:14 - INFO - root -   Epoch: [196/300][0/283], lr: 0.00000044 	 loss = 0.0177(0.0177)
2023/12/02 22:36:41 - INFO - root -   Epoch: [196/300][20/283], lr: 0.00000044 	 loss = 0.4607(0.3954)
2023/12/02 22:36:55 - INFO - root -   Epoch: [196/300][40/283], lr: 0.00000044 	 loss = 0.0183(0.2895)
2023/12/02 22:37:10 - INFO - root -   Epoch: [196/300][60/283], lr: 0.00000044 	 loss = 2.1127(0.2982)
2023/12/02 22:37:18 - INFO - root -   Epoch: [196/300][80/283], lr: 0.00000044 	 loss = 0.0433(0.2717)
2023/12/02 22:37:35 - INFO - root -   Epoch: [196/300][100/283], lr: 0.00000044 	 loss = 0.0527(0.2407)
2023/12/02 22:37:46 - INFO - root -   Epoch: [196/300][120/283], lr: 0.00000044 	 loss = 1.2541(0.2665)
2023/12/02 22:37:59 - INFO - root -   Epoch: [196/300][140/283], lr: 0.00000044 	 loss = 0.0733(0.2595)
2023/12/02 22:38:13 - INFO - root -   Epoch: [196/300][160/283], lr: 0.00000044 	 loss = 0.0419(0.2652)
2023/12/02 22:38:26 - INFO - root -   Epoch: [196/300][180/283], lr: 0.00000044 	 loss = 0.0494(0.2525)
2023/12/02 22:38:45 - INFO - root -   Epoch: [196/300][200/283], lr: 0.00000044 	 loss = 0.0358(0.2436)
2023/12/02 22:38:57 - INFO - root -   Epoch: [196/300][220/283], lr: 0.00000044 	 loss = 0.0077(0.2364)
2023/12/02 22:39:14 - INFO - root -   Epoch: [196/300][240/283], lr: 0.00000044 	 loss = 0.1833(0.2321)
2023/12/02 22:39:30 - INFO - root -   Epoch: [196/300][260/283], lr: 0.00000044 	 loss = 0.0800(0.2311)
2023/12/02 22:39:38 - INFO - root -   Epoch: [196/300][280/283], lr: 0.00000044 	 loss = 0.0122(0.2228)
2023/12/02 22:39:39 - INFO - root -   Epoch: [196/300] 	 loss = 0.2214
2023/12/02 22:39:39 - INFO - root -   train_accuracy = 0.9258
2023/12/02 22:40:00 - INFO - root -   Epoch: [197/300][0/283], lr: 0.00000044 	 loss = 0.5601(0.5601)
2023/12/02 22:40:08 - INFO - root -   Epoch: [197/300][20/283], lr: 0.00000044 	 loss = 0.4075(0.2411)
2023/12/02 22:40:33 - INFO - root -   Epoch: [197/300][40/283], lr: 0.00000044 	 loss = 0.2065(0.2905)
2023/12/02 22:40:49 - INFO - root -   Epoch: [197/300][60/283], lr: 0.00000044 	 loss = 0.2624(0.2690)
2023/12/02 22:40:57 - INFO - root -   Epoch: [197/300][80/283], lr: 0.00000044 	 loss = 0.0134(0.2575)
2023/12/02 22:41:23 - INFO - root -   Epoch: [197/300][100/283], lr: 0.00000044 	 loss = 0.0240(0.2549)
2023/12/02 22:41:33 - INFO - root -   Epoch: [197/300][120/283], lr: 0.00000044 	 loss = 0.0167(0.2694)
2023/12/02 22:41:49 - INFO - root -   Epoch: [197/300][140/283], lr: 0.00000044 	 loss = 0.0067(0.2565)
2023/12/02 22:42:06 - INFO - root -   Epoch: [197/300][160/283], lr: 0.00000044 	 loss = 0.0559(0.2432)
2023/12/02 22:42:14 - INFO - root -   Epoch: [197/300][180/283], lr: 0.00000044 	 loss = 0.1434(0.2337)
2023/12/02 22:42:27 - INFO - root -   Epoch: [197/300][200/283], lr: 0.00000044 	 loss = 0.2184(0.2318)
2023/12/02 22:42:40 - INFO - root -   Epoch: [197/300][220/283], lr: 0.00000044 	 loss = 0.0116(0.2207)
2023/12/02 22:43:04 - INFO - root -   Epoch: [197/300][240/283], lr: 0.00000044 	 loss = 0.0067(0.2334)
2023/12/02 22:43:26 - INFO - root -   Epoch: [197/300][260/283], lr: 0.00000044 	 loss = 0.0029(0.2364)
2023/12/02 22:43:34 - INFO - root -   Epoch: [197/300][280/283], lr: 0.00000044 	 loss = 0.8781(0.2352)
2023/12/02 22:43:35 - INFO - root -   Epoch: [197/300] 	 loss = 0.2338
2023/12/02 22:43:35 - INFO - root -   train_accuracy = 0.9117
2023/12/02 22:43:36 - INFO - root -   Epoch: [198/300][0/283], lr: 0.00000045 	 loss = 0.0210(0.0210)
2023/12/02 22:44:00 - INFO - root -   Epoch: [198/300][20/283], lr: 0.00000045 	 loss = 0.4196(0.2169)
2023/12/02 22:44:21 - INFO - root -   Epoch: [198/300][40/283], lr: 0.00000045 	 loss = 0.0184(0.2464)
2023/12/02 22:44:33 - INFO - root -   Epoch: [198/300][60/283], lr: 0.00000045 	 loss = 0.4755(0.2682)
2023/12/02 22:44:46 - INFO - root -   Epoch: [198/300][80/283], lr: 0.00000045 	 loss = 0.0293(0.2581)
2023/12/02 22:45:00 - INFO - root -   Epoch: [198/300][100/283], lr: 0.00000045 	 loss = 0.5850(0.2587)
2023/12/02 22:45:15 - INFO - root -   Epoch: [198/300][120/283], lr: 0.00000045 	 loss = 0.4212(0.2893)
2023/12/02 22:45:28 - INFO - root -   Epoch: [198/300][140/283], lr: 0.00000045 	 loss = 0.0064(0.2780)
2023/12/02 22:45:45 - INFO - root -   Epoch: [198/300][160/283], lr: 0.00000045 	 loss = 0.1951(0.2594)
2023/12/02 22:45:53 - INFO - root -   Epoch: [198/300][180/283], lr: 0.00000045 	 loss = 0.6885(0.2467)
2023/12/02 22:46:11 - INFO - root -   Epoch: [198/300][200/283], lr: 0.00000045 	 loss = 0.0257(0.2396)
2023/12/02 22:46:25 - INFO - root -   Epoch: [198/300][220/283], lr: 0.00000045 	 loss = 0.0040(0.2321)
2023/12/02 22:46:39 - INFO - root -   Epoch: [198/300][240/283], lr: 0.00000045 	 loss = 0.0473(0.2348)
2023/12/02 22:46:55 - INFO - root -   Epoch: [198/300][260/283], lr: 0.00000045 	 loss = 0.1371(0.2304)
2023/12/02 22:47:05 - INFO - root -   Epoch: [198/300][280/283], lr: 0.00000045 	 loss = 0.1562(0.2309)
2023/12/02 22:47:06 - INFO - root -   Epoch: [198/300] 	 loss = 0.2299
2023/12/02 22:47:06 - INFO - root -   train_accuracy = 0.9117
2023/12/02 22:47:30 - INFO - root -   Epoch: [199/300][0/283], lr: 0.00000045 	 loss = 0.5656(0.5656)
2023/12/02 22:47:47 - INFO - root -   Epoch: [199/300][20/283], lr: 0.00000045 	 loss = 0.7551(0.4066)
2023/12/02 22:47:55 - INFO - root -   Epoch: [199/300][40/283], lr: 0.00000045 	 loss = 0.4137(0.3125)
2023/12/02 22:48:05 - INFO - root -   Epoch: [199/300][60/283], lr: 0.00000045 	 loss = 1.4703(0.2920)
2023/12/02 22:48:31 - INFO - root -   Epoch: [199/300][80/283], lr: 0.00000045 	 loss = 0.0630(0.2878)
2023/12/02 22:48:48 - INFO - root -   Epoch: [199/300][100/283], lr: 0.00000045 	 loss = 0.0086(0.2834)
2023/12/02 22:48:58 - INFO - root -   Epoch: [199/300][120/283], lr: 0.00000045 	 loss = 0.0962(0.3271)
2023/12/02 22:49:12 - INFO - root -   Epoch: [199/300][140/283], lr: 0.00000045 	 loss = 0.1208(0.2940)
2023/12/02 22:49:20 - INFO - root -   Epoch: [199/300][160/283], lr: 0.00000045 	 loss = 0.1707(0.2651)
2023/12/02 22:49:37 - INFO - root -   Epoch: [199/300][180/283], lr: 0.00000045 	 loss = 0.0289(0.2564)
2023/12/02 22:49:47 - INFO - root -   Epoch: [199/300][200/283], lr: 0.00000045 	 loss = 0.2733(0.2609)
2023/12/02 22:50:04 - INFO - root -   Epoch: [199/300][220/283], lr: 0.00000045 	 loss = 0.0073(0.2550)
2023/12/02 22:50:13 - INFO - root -   Epoch: [199/300][240/283], lr: 0.00000045 	 loss = 0.1481(0.2521)
2023/12/02 22:50:29 - INFO - root -   Epoch: [199/300][260/283], lr: 0.00000045 	 loss = 0.0280(0.2431)
2023/12/02 22:50:37 - INFO - root -   Epoch: [199/300][280/283], lr: 0.00000045 	 loss = 0.1826(0.2362)
2023/12/02 22:50:38 - INFO - root -   Epoch: [199/300] 	 loss = 0.2348
2023/12/02 22:50:47 - INFO - root -   precision = 0.8601
2023/12/02 22:50:47 - INFO - root -   eval_loss = 0.5224
2023/12/02 22:50:47 - INFO - root -   eval_acc = 0.8601
2023/12/02 22:50:48 - INFO - root -   train_accuracy = 0.9152
2023/12/02 22:50:49 - INFO - root -   Epoch: [200/300][0/283], lr: 0.00000045 	 loss = 0.0368(0.0368)
2023/12/02 22:51:23 - INFO - root -   Epoch: [200/300][20/283], lr: 0.00000045 	 loss = 0.0447(0.2492)
2023/12/02 22:51:38 - INFO - root -   Epoch: [200/300][40/283], lr: 0.00000045 	 loss = 0.0614(0.3098)
2023/12/02 22:52:02 - INFO - root -   Epoch: [200/300][60/283], lr: 0.00000045 	 loss = 0.9667(0.3582)
2023/12/02 22:52:21 - INFO - root -   Epoch: [200/300][80/283], lr: 0.00000045 	 loss = 0.0061(0.3183)
2023/12/02 22:52:29 - INFO - root -   Epoch: [200/300][100/283], lr: 0.00000045 	 loss = 0.1677(0.3027)
2023/12/02 22:52:42 - INFO - root -   Epoch: [200/300][120/283], lr: 0.00000045 	 loss = 2.5697(0.3230)
2023/12/02 22:52:57 - INFO - root -   Epoch: [200/300][140/283], lr: 0.00000045 	 loss = 0.0143(0.3010)
2023/12/02 22:53:24 - INFO - root -   Epoch: [200/300][160/283], lr: 0.00000045 	 loss = 0.7160(0.2941)
2023/12/02 22:53:32 - INFO - root -   Epoch: [200/300][180/283], lr: 0.00000045 	 loss = 0.0738(0.2786)
2023/12/02 22:53:55 - INFO - root -   Epoch: [200/300][200/283], lr: 0.00000045 	 loss = 0.0854(0.2835)
2023/12/02 22:54:05 - INFO - root -   Epoch: [200/300][220/283], lr: 0.00000045 	 loss = 0.0343(0.2733)
2023/12/02 22:54:35 - INFO - root -   Epoch: [200/300][240/283], lr: 0.00000045 	 loss = 0.0126(0.2885)
2023/12/02 22:54:43 - INFO - root -   Epoch: [200/300][260/283], lr: 0.00000045 	 loss = 0.5118(0.2871)
2023/12/02 22:54:51 - INFO - root -   Epoch: [200/300][280/283], lr: 0.00000045 	 loss = 0.0210(0.2817)
2023/12/02 22:54:52 - INFO - root -   Epoch: [200/300] 	 loss = 0.2803
2023/12/02 22:54:52 - INFO - root -   train_accuracy = 0.8728
2023/12/02 22:55:19 - INFO - root -   Epoch: [201/300][0/283], lr: 0.00000045 	 loss = 0.6167(0.6167)
2023/12/02 22:55:27 - INFO - root -   Epoch: [201/300][20/283], lr: 0.00000045 	 loss = 0.1234(0.3291)
2023/12/02 22:55:35 - INFO - root -   Epoch: [201/300][40/283], lr: 0.00000045 	 loss = 0.2151(0.2472)
2023/12/02 22:55:51 - INFO - root -   Epoch: [201/300][60/283], lr: 0.00000045 	 loss = 1.3363(0.2663)
2023/12/02 22:56:09 - INFO - root -   Epoch: [201/300][80/283], lr: 0.00000045 	 loss = 0.2483(0.2799)
2023/12/02 22:56:18 - INFO - root -   Epoch: [201/300][100/283], lr: 0.00000045 	 loss = 0.0373(0.2635)
2023/12/02 22:56:37 - INFO - root -   Epoch: [201/300][120/283], lr: 0.00000045 	 loss = 2.1613(0.3207)
2023/12/02 22:56:45 - INFO - root -   Epoch: [201/300][140/283], lr: 0.00000045 	 loss = 0.1452(0.3014)
2023/12/02 22:57:18 - INFO - root -   Epoch: [201/300][160/283], lr: 0.00000045 	 loss = 0.0143(0.2853)
2023/12/02 22:57:26 - INFO - root -   Epoch: [201/300][180/283], lr: 0.00000045 	 loss = 0.0254(0.2719)
2023/12/02 22:57:50 - INFO - root -   Epoch: [201/300][200/283], lr: 0.00000045 	 loss = 0.4450(0.2716)
2023/12/02 22:57:59 - INFO - root -   Epoch: [201/300][220/283], lr: 0.00000045 	 loss = 0.1845(0.2607)
2023/12/02 22:58:07 - INFO - root -   Epoch: [201/300][240/283], lr: 0.00000045 	 loss = 0.0328(0.2559)
2023/12/02 22:58:20 - INFO - root -   Epoch: [201/300][260/283], lr: 0.00000045 	 loss = 0.3141(0.2443)
2023/12/02 22:58:28 - INFO - root -   Epoch: [201/300][280/283], lr: 0.00000045 	 loss = 0.7698(0.2436)
2023/12/02 22:58:29 - INFO - root -   Epoch: [201/300] 	 loss = 0.2427
2023/12/02 22:58:29 - INFO - root -   train_accuracy = 0.8958
2023/12/02 22:58:30 - INFO - root -   Epoch: [202/300][0/283], lr: 0.00000045 	 loss = 0.0164(0.0164)
2023/12/02 22:59:06 - INFO - root -   Epoch: [202/300][20/283], lr: 0.00000045 	 loss = 0.4510(0.4496)
2023/12/02 22:59:14 - INFO - root -   Epoch: [202/300][40/283], lr: 0.00000045 	 loss = 0.2351(0.3824)
2023/12/02 22:59:32 - INFO - root -   Epoch: [202/300][60/283], lr: 0.00000045 	 loss = 1.1286(0.3485)
2023/12/02 22:59:45 - INFO - root -   Epoch: [202/300][80/283], lr: 0.00000045 	 loss = 0.0074(0.3085)
2023/12/02 23:00:02 - INFO - root -   Epoch: [202/300][100/283], lr: 0.00000045 	 loss = 0.0588(0.2849)
2023/12/02 23:00:18 - INFO - root -   Epoch: [202/300][120/283], lr: 0.00000045 	 loss = 0.0152(0.3272)
2023/12/02 23:00:26 - INFO - root -   Epoch: [202/300][140/283], lr: 0.00000045 	 loss = 0.0282(0.2941)
2023/12/02 23:00:46 - INFO - root -   Epoch: [202/300][160/283], lr: 0.00000045 	 loss = 0.0514(0.2862)
2023/12/02 23:00:54 - INFO - root -   Epoch: [202/300][180/283], lr: 0.00000045 	 loss = 0.9024(0.2844)
2023/12/02 23:01:20 - INFO - root -   Epoch: [202/300][200/283], lr: 0.00000045 	 loss = 0.4157(0.2773)
2023/12/02 23:01:30 - INFO - root -   Epoch: [202/300][220/283], lr: 0.00000045 	 loss = 0.0115(0.2709)
2023/12/02 23:01:38 - INFO - root -   Epoch: [202/300][240/283], lr: 0.00000045 	 loss = 0.0083(0.2647)
2023/12/02 23:01:56 - INFO - root -   Epoch: [202/300][260/283], lr: 0.00000045 	 loss = 0.1502(0.2590)
2023/12/02 23:02:06 - INFO - root -   Epoch: [202/300][280/283], lr: 0.00000045 	 loss = 0.1930(0.2604)
2023/12/02 23:02:07 - INFO - root -   Epoch: [202/300] 	 loss = 0.2590
2023/12/02 23:02:07 - INFO - root -   train_accuracy = 0.8940
2023/12/02 23:02:22 - INFO - root -   Epoch: [203/300][0/283], lr: 0.00000046 	 loss = 0.1472(0.1472)
2023/12/02 23:02:34 - INFO - root -   Epoch: [203/300][20/283], lr: 0.00000046 	 loss = 0.3359(0.3088)
2023/12/02 23:02:51 - INFO - root -   Epoch: [203/300][40/283], lr: 0.00000046 	 loss = 0.0539(0.2012)
2023/12/02 23:03:08 - INFO - root -   Epoch: [203/300][60/283], lr: 0.00000046 	 loss = 1.8696(0.2898)
2023/12/02 23:03:20 - INFO - root -   Epoch: [203/300][80/283], lr: 0.00000046 	 loss = 0.4725(0.2954)
2023/12/02 23:03:43 - INFO - root -   Epoch: [203/300][100/283], lr: 0.00000046 	 loss = 0.0128(0.3110)
2023/12/02 23:03:58 - INFO - root -   Epoch: [203/300][120/283], lr: 0.00000046 	 loss = 0.0848(0.3274)
2023/12/02 23:04:06 - INFO - root -   Epoch: [203/300][140/283], lr: 0.00000046 	 loss = 0.0044(0.2994)
2023/12/02 23:04:26 - INFO - root -   Epoch: [203/300][160/283], lr: 0.00000046 	 loss = 1.0767(0.2974)
2023/12/02 23:04:38 - INFO - root -   Epoch: [203/300][180/283], lr: 0.00000046 	 loss = 0.3647(0.2803)
2023/12/02 23:04:52 - INFO - root -   Epoch: [203/300][200/283], lr: 0.00000046 	 loss = 0.1035(0.2707)
2023/12/02 23:05:00 - INFO - root -   Epoch: [203/300][220/283], lr: 0.00000046 	 loss = 0.0498(0.2649)
2023/12/02 23:05:08 - INFO - root -   Epoch: [203/300][240/283], lr: 0.00000046 	 loss = 0.0197(0.2572)
2023/12/02 23:05:22 - INFO - root -   Epoch: [203/300][260/283], lr: 0.00000046 	 loss = 0.0133(0.2553)
2023/12/02 23:05:31 - INFO - root -   Epoch: [203/300][280/283], lr: 0.00000046 	 loss = 0.7642(0.2536)
2023/12/02 23:05:32 - INFO - root -   Epoch: [203/300] 	 loss = 0.2531
2023/12/02 23:05:32 - INFO - root -   train_accuracy = 0.8905
2023/12/02 23:05:34 - INFO - root -   Epoch: [204/300][0/283], lr: 0.00000046 	 loss = 0.0040(0.0040)
2023/12/02 23:06:07 - INFO - root -   Epoch: [204/300][20/283], lr: 0.00000046 	 loss = 0.0728(0.4038)
2023/12/02 23:06:22 - INFO - root -   Epoch: [204/300][40/283], lr: 0.00000046 	 loss = 0.0941(0.3096)
2023/12/02 23:06:37 - INFO - root -   Epoch: [204/300][60/283], lr: 0.00000046 	 loss = 1.7566(0.3482)
2023/12/02 23:06:49 - INFO - root -   Epoch: [204/300][80/283], lr: 0.00000046 	 loss = 0.0141(0.3077)
2023/12/02 23:07:16 - INFO - root -   Epoch: [204/300][100/283], lr: 0.00000046 	 loss = 0.1304(0.2869)
2023/12/02 23:07:29 - INFO - root -   Epoch: [204/300][120/283], lr: 0.00000046 	 loss = 0.9371(0.3176)
2023/12/02 23:07:46 - INFO - root -   Epoch: [204/300][140/283], lr: 0.00000046 	 loss = 0.0110(0.3005)
2023/12/02 23:07:56 - INFO - root -   Epoch: [204/300][160/283], lr: 0.00000046 	 loss = 0.1514(0.2917)
2023/12/02 23:08:20 - INFO - root -   Epoch: [204/300][180/283], lr: 0.00000046 	 loss = 0.6080(0.2880)
2023/12/02 23:08:36 - INFO - root -   Epoch: [204/300][200/283], lr: 0.00000046 	 loss = 0.0372(0.2841)
2023/12/02 23:08:48 - INFO - root -   Epoch: [204/300][220/283], lr: 0.00000046 	 loss = 0.1714(0.2767)
2023/12/02 23:09:07 - INFO - root -   Epoch: [204/300][240/283], lr: 0.00000046 	 loss = 0.2544(0.2808)
2023/12/02 23:09:22 - INFO - root -   Epoch: [204/300][260/283], lr: 0.00000046 	 loss = 0.0542(0.2761)
2023/12/02 23:09:30 - INFO - root -   Epoch: [204/300][280/283], lr: 0.00000046 	 loss = 0.1976(0.2684)
2023/12/02 23:09:30 - INFO - root -   Epoch: [204/300] 	 loss = 0.2673
2023/12/02 23:09:40 - INFO - root -   precision = 0.8182
2023/12/02 23:09:40 - INFO - root -   eval_loss = 0.5292
2023/12/02 23:09:40 - INFO - root -   eval_acc = 0.8182
2023/12/02 23:09:40 - INFO - root -   train_accuracy = 0.8799
2023/12/02 23:09:56 - INFO - root -   Epoch: [205/300][0/283], lr: 0.00000046 	 loss = 0.0321(0.0321)
2023/12/02 23:10:23 - INFO - root -   Epoch: [205/300][20/283], lr: 0.00000046 	 loss = 1.7194(0.2608)
2023/12/02 23:10:32 - INFO - root -   Epoch: [205/300][40/283], lr: 0.00000046 	 loss = 0.0158(0.2440)
2023/12/02 23:10:44 - INFO - root -   Epoch: [205/300][60/283], lr: 0.00000046 	 loss = 1.5636(0.2943)
2023/12/02 23:11:06 - INFO - root -   Epoch: [205/300][80/283], lr: 0.00000046 	 loss = 0.1473(0.2904)
2023/12/02 23:11:18 - INFO - root -   Epoch: [205/300][100/283], lr: 0.00000046 	 loss = 0.0863(0.2848)
2023/12/02 23:11:32 - INFO - root -   Epoch: [205/300][120/283], lr: 0.00000046 	 loss = 0.0109(0.3052)
2023/12/02 23:11:49 - INFO - root -   Epoch: [205/300][140/283], lr: 0.00000046 	 loss = 0.0092(0.2813)
2023/12/02 23:12:03 - INFO - root -   Epoch: [205/300][160/283], lr: 0.00000046 	 loss = 0.0092(0.2636)
2023/12/02 23:12:15 - INFO - root -   Epoch: [205/300][180/283], lr: 0.00000046 	 loss = 0.0668(0.2589)
2023/12/02 23:12:35 - INFO - root -   Epoch: [205/300][200/283], lr: 0.00000046 	 loss = 0.0915(0.2514)
2023/12/02 23:12:43 - INFO - root -   Epoch: [205/300][220/283], lr: 0.00000046 	 loss = 0.2640(0.2418)
2023/12/02 23:12:55 - INFO - root -   Epoch: [205/300][240/283], lr: 0.00000046 	 loss = 0.0940(0.2397)
2023/12/02 23:13:03 - INFO - root -   Epoch: [205/300][260/283], lr: 0.00000046 	 loss = 0.0039(0.2430)
2023/12/02 23:13:11 - INFO - root -   Epoch: [205/300][280/283], lr: 0.00000046 	 loss = 0.2455(0.2355)
2023/12/02 23:13:12 - INFO - root -   Epoch: [205/300] 	 loss = 0.2343
2023/12/02 23:13:12 - INFO - root -   train_accuracy = 0.9081
2023/12/02 23:13:25 - INFO - root -   Epoch: [206/300][0/283], lr: 0.00000046 	 loss = 0.4744(0.4744)
2023/12/02 23:13:41 - INFO - root -   Epoch: [206/300][20/283], lr: 0.00000046 	 loss = 0.7154(0.3488)
2023/12/02 23:14:03 - INFO - root -   Epoch: [206/300][40/283], lr: 0.00000046 	 loss = 0.0149(0.3315)
2023/12/02 23:14:11 - INFO - root -   Epoch: [206/300][60/283], lr: 0.00000046 	 loss = 0.3044(0.3349)
2023/12/02 23:14:31 - INFO - root -   Epoch: [206/300][80/283], lr: 0.00000046 	 loss = 0.0240(0.3311)
2023/12/02 23:14:49 - INFO - root -   Epoch: [206/300][100/283], lr: 0.00000046 	 loss = 0.4041(0.3095)
2023/12/02 23:15:12 - INFO - root -   Epoch: [206/300][120/283], lr: 0.00000046 	 loss = 0.1438(0.3275)
2023/12/02 23:15:25 - INFO - root -   Epoch: [206/300][140/283], lr: 0.00000046 	 loss = 0.0015(0.3136)
2023/12/02 23:15:44 - INFO - root -   Epoch: [206/300][160/283], lr: 0.00000046 	 loss = 0.7028(0.3111)
2023/12/02 23:16:02 - INFO - root -   Epoch: [206/300][180/283], lr: 0.00000046 	 loss = 0.0809(0.3108)
2023/12/02 23:16:13 - INFO - root -   Epoch: [206/300][200/283], lr: 0.00000046 	 loss = 0.0307(0.2933)
2023/12/02 23:16:30 - INFO - root -   Epoch: [206/300][220/283], lr: 0.00000046 	 loss = 0.0551(0.2766)
2023/12/02 23:16:37 - INFO - root -   Epoch: [206/300][240/283], lr: 0.00000046 	 loss = 0.0098(0.2765)
2023/12/02 23:16:45 - INFO - root -   Epoch: [206/300][260/283], lr: 0.00000046 	 loss = 0.3183(0.2671)
2023/12/02 23:16:54 - INFO - root -   Epoch: [206/300][280/283], lr: 0.00000046 	 loss = 0.0053(0.2581)
2023/12/02 23:16:55 - INFO - root -   Epoch: [206/300] 	 loss = 0.2571
2023/12/02 23:16:55 - INFO - root -   train_accuracy = 0.8834
2023/12/02 23:16:57 - INFO - root -   Epoch: [207/300][0/283], lr: 0.00000046 	 loss = 0.0106(0.0106)
2023/12/02 23:17:26 - INFO - root -   Epoch: [207/300][20/283], lr: 0.00000046 	 loss = 0.2840(0.2382)
2023/12/02 23:17:34 - INFO - root -   Epoch: [207/300][40/283], lr: 0.00000046 	 loss = 0.0392(0.2472)
2023/12/02 23:18:04 - INFO - root -   Epoch: [207/300][60/283], lr: 0.00000046 	 loss = 2.5546(0.2849)
2023/12/02 23:18:12 - INFO - root -   Epoch: [207/300][80/283], lr: 0.00000046 	 loss = 0.7684(0.2846)
2023/12/02 23:18:22 - INFO - root -   Epoch: [207/300][100/283], lr: 0.00000046 	 loss = 0.0079(0.2550)
2023/12/02 23:18:38 - INFO - root -   Epoch: [207/300][120/283], lr: 0.00000046 	 loss = 0.2183(0.2774)
2023/12/02 23:18:46 - INFO - root -   Epoch: [207/300][140/283], lr: 0.00000046 	 loss = 0.1111(0.2780)
2023/12/02 23:19:13 - INFO - root -   Epoch: [207/300][160/283], lr: 0.00000046 	 loss = 0.1066(0.2675)
2023/12/02 23:19:21 - INFO - root -   Epoch: [207/300][180/283], lr: 0.00000046 	 loss = 0.2711(0.2575)
2023/12/02 23:19:38 - INFO - root -   Epoch: [207/300][200/283], lr: 0.00000046 	 loss = 0.6941(0.2543)
2023/12/02 23:19:53 - INFO - root -   Epoch: [207/300][220/283], lr: 0.00000046 	 loss = 0.2291(0.2456)
2023/12/02 23:20:11 - INFO - root -   Epoch: [207/300][240/283], lr: 0.00000046 	 loss = 0.0049(0.2360)
2023/12/02 23:20:19 - INFO - root -   Epoch: [207/300][260/283], lr: 0.00000046 	 loss = 0.0050(0.2312)
2023/12/02 23:20:27 - INFO - root -   Epoch: [207/300][280/283], lr: 0.00000046 	 loss = 0.4809(0.2345)
2023/12/02 23:20:28 - INFO - root -   Epoch: [207/300] 	 loss = 0.2337
2023/12/02 23:20:28 - INFO - root -   train_accuracy = 0.9099
2023/12/02 23:20:41 - INFO - root -   Epoch: [208/300][0/283], lr: 0.00000046 	 loss = 0.6335(0.6335)
2023/12/02 23:21:00 - INFO - root -   Epoch: [208/300][20/283], lr: 0.00000046 	 loss = 0.4990(0.2561)
2023/12/02 23:21:18 - INFO - root -   Epoch: [208/300][40/283], lr: 0.00000046 	 loss = 0.0740(0.2771)
2023/12/02 23:21:26 - INFO - root -   Epoch: [208/300][60/283], lr: 0.00000046 	 loss = 0.5948(0.2598)
2023/12/02 23:21:41 - INFO - root -   Epoch: [208/300][80/283], lr: 0.00000046 	 loss = 0.5696(0.2353)
2023/12/02 23:21:49 - INFO - root -   Epoch: [208/300][100/283], lr: 0.00000046 	 loss = 0.0079(0.2292)
2023/12/02 23:22:03 - INFO - root -   Epoch: [208/300][120/283], lr: 0.00000046 	 loss = 0.0337(0.2617)
2023/12/02 23:22:28 - INFO - root -   Epoch: [208/300][140/283], lr: 0.00000046 	 loss = 0.1228(0.2479)
2023/12/02 23:22:38 - INFO - root -   Epoch: [208/300][160/283], lr: 0.00000046 	 loss = 0.0426(0.2535)
2023/12/02 23:23:01 - INFO - root -   Epoch: [208/300][180/283], lr: 0.00000046 	 loss = 0.2101(0.2432)
2023/12/02 23:23:09 - INFO - root -   Epoch: [208/300][200/283], lr: 0.00000046 	 loss = 2.0230(0.2420)
2023/12/02 23:23:22 - INFO - root -   Epoch: [208/300][220/283], lr: 0.00000046 	 loss = 0.1226(0.2483)
2023/12/02 23:23:37 - INFO - root -   Epoch: [208/300][240/283], lr: 0.00000046 	 loss = 0.0337(0.2444)
2023/12/02 23:23:57 - INFO - root -   Epoch: [208/300][260/283], lr: 0.00000046 	 loss = 0.3115(0.2408)
2023/12/02 23:24:05 - INFO - root -   Epoch: [208/300][280/283], lr: 0.00000046 	 loss = 0.0057(0.2351)
2023/12/02 23:24:06 - INFO - root -   Epoch: [208/300] 	 loss = 0.2345
2023/12/02 23:24:06 - INFO - root -   train_accuracy = 0.9099
2023/12/02 23:24:08 - INFO - root -   Epoch: [209/300][0/283], lr: 0.00000047 	 loss = 0.0073(0.0073)
2023/12/02 23:24:30 - INFO - root -   Epoch: [209/300][20/283], lr: 0.00000047 	 loss = 0.1337(0.2622)
2023/12/02 23:24:38 - INFO - root -   Epoch: [209/300][40/283], lr: 0.00000047 	 loss = 0.0260(0.3143)
2023/12/02 23:24:53 - INFO - root -   Epoch: [209/300][60/283], lr: 0.00000047 	 loss = 0.5341(0.2813)
2023/12/02 23:25:16 - INFO - root -   Epoch: [209/300][80/283], lr: 0.00000047 	 loss = 0.1043(0.2728)
2023/12/02 23:25:24 - INFO - root -   Epoch: [209/300][100/283], lr: 0.00000047 	 loss = 0.1543(0.2556)
2023/12/02 23:25:35 - INFO - root -   Epoch: [209/300][120/283], lr: 0.00000047 	 loss = 0.1237(0.2704)
2023/12/02 23:25:57 - INFO - root -   Epoch: [209/300][140/283], lr: 0.00000047 	 loss = 0.1305(0.2544)
2023/12/02 23:26:05 - INFO - root -   Epoch: [209/300][160/283], lr: 0.00000047 	 loss = 0.0143(0.2433)
2023/12/02 23:26:30 - INFO - root -   Epoch: [209/300][180/283], lr: 0.00000047 	 loss = 0.1799(0.2355)
2023/12/02 23:26:38 - INFO - root -   Epoch: [209/300][200/283], lr: 0.00000047 	 loss = 0.0334(0.2243)
2023/12/02 23:26:55 - INFO - root -   Epoch: [209/300][220/283], lr: 0.00000047 	 loss = 0.0046(0.2174)
2023/12/02 23:27:03 - INFO - root -   Epoch: [209/300][240/283], lr: 0.00000047 	 loss = 0.0154(0.2138)
2023/12/02 23:27:15 - INFO - root -   Epoch: [209/300][260/283], lr: 0.00000047 	 loss = 0.0011(0.2093)
2023/12/02 23:27:27 - INFO - root -   Epoch: [209/300][280/283], lr: 0.00000047 	 loss = 0.7283(0.2137)
2023/12/02 23:27:28 - INFO - root -   Epoch: [209/300] 	 loss = 0.2135
2023/12/02 23:27:38 - INFO - root -   precision = 0.8392
2023/12/02 23:27:38 - INFO - root -   eval_loss = 0.5231
2023/12/02 23:27:38 - INFO - root -   eval_acc = 0.8392
2023/12/02 23:27:38 - INFO - root -   train_accuracy = 0.9099
2023/12/02 23:27:40 - INFO - root -   Epoch: [210/300][0/283], lr: 0.00000047 	 loss = 0.0172(0.0172)
2023/12/02 23:28:08 - INFO - root -   Epoch: [210/300][20/283], lr: 0.00000047 	 loss = 0.1085(0.3205)
2023/12/02 23:28:25 - INFO - root -   Epoch: [210/300][40/283], lr: 0.00000047 	 loss = 0.3004(0.3173)
2023/12/02 23:28:33 - INFO - root -   Epoch: [210/300][60/283], lr: 0.00000047 	 loss = 1.4943(0.3328)
2023/12/02 23:28:54 - INFO - root -   Epoch: [210/300][80/283], lr: 0.00000047 	 loss = 0.0090(0.3314)
2023/12/02 23:29:14 - INFO - root -   Epoch: [210/300][100/283], lr: 0.00000047 	 loss = 0.2686(0.3071)
2023/12/02 23:29:27 - INFO - root -   Epoch: [210/300][120/283], lr: 0.00000047 	 loss = 0.5978(0.3128)
2023/12/02 23:29:42 - INFO - root -   Epoch: [210/300][140/283], lr: 0.00000047 	 loss = 0.0222(0.3082)
2023/12/02 23:29:58 - INFO - root -   Epoch: [210/300][160/283], lr: 0.00000047 	 loss = 0.3197(0.3055)
2023/12/02 23:30:21 - INFO - root -   Epoch: [210/300][180/283], lr: 0.00000047 	 loss = 0.9559(0.3031)
2023/12/02 23:30:29 - INFO - root -   Epoch: [210/300][200/283], lr: 0.00000047 	 loss = 0.0865(0.2874)
2023/12/02 23:30:40 - INFO - root -   Epoch: [210/300][220/283], lr: 0.00000047 	 loss = 0.0143(0.2679)
2023/12/02 23:30:47 - INFO - root -   Epoch: [210/300][240/283], lr: 0.00000047 	 loss = 0.0812(0.2591)
2023/12/02 23:31:02 - INFO - root -   Epoch: [210/300][260/283], lr: 0.00000047 	 loss = 0.0177(0.2661)
2023/12/02 23:31:12 - INFO - root -   Epoch: [210/300][280/283], lr: 0.00000047 	 loss = 0.0795(0.2620)
2023/12/02 23:31:13 - INFO - root -   Epoch: [210/300] 	 loss = 0.2612
2023/12/02 23:31:13 - INFO - root -   train_accuracy = 0.8975
2023/12/02 23:31:27 - INFO - root -   Epoch: [211/300][0/283], lr: 0.00000047 	 loss = 0.0743(0.0743)
2023/12/02 23:31:45 - INFO - root -   Epoch: [211/300][20/283], lr: 0.00000047 	 loss = 0.4908(0.3860)
2023/12/02 23:32:04 - INFO - root -   Epoch: [211/300][40/283], lr: 0.00000047 	 loss = 0.0275(0.3118)
2023/12/02 23:32:13 - INFO - root -   Epoch: [211/300][60/283], lr: 0.00000047 	 loss = 0.4821(0.3555)
2023/12/02 23:32:41 - INFO - root -   Epoch: [211/300][80/283], lr: 0.00000047 	 loss = 0.4628(0.3555)
2023/12/02 23:32:49 - INFO - root -   Epoch: [211/300][100/283], lr: 0.00000047 	 loss = 0.1954(0.3163)
2023/12/02 23:33:06 - INFO - root -   Epoch: [211/300][120/283], lr: 0.00000047 	 loss = 0.7283(0.3386)
2023/12/02 23:33:30 - INFO - root -   Epoch: [211/300][140/283], lr: 0.00000047 	 loss = 0.0687(0.3477)
2023/12/02 23:33:48 - INFO - root -   Epoch: [211/300][160/283], lr: 0.00000047 	 loss = 1.5265(0.3344)
2023/12/02 23:34:06 - INFO - root -   Epoch: [211/300][180/283], lr: 0.00000047 	 loss = 0.1451(0.3235)
2023/12/02 23:34:15 - INFO - root -   Epoch: [211/300][200/283], lr: 0.00000047 	 loss = 1.1598(0.3216)
2023/12/02 23:34:31 - INFO - root -   Epoch: [211/300][220/283], lr: 0.00000047 	 loss = 0.1917(0.3080)
2023/12/02 23:34:39 - INFO - root -   Epoch: [211/300][240/283], lr: 0.00000047 	 loss = 0.0364(0.2997)
2023/12/02 23:35:03 - INFO - root -   Epoch: [211/300][260/283], lr: 0.00000047 	 loss = 0.1364(0.2965)
2023/12/02 23:35:11 - INFO - root -   Epoch: [211/300][280/283], lr: 0.00000047 	 loss = 0.9561(0.2930)
2023/12/02 23:35:11 - INFO - root -   Epoch: [211/300] 	 loss = 0.2910
2023/12/02 23:35:11 - INFO - root -   train_accuracy = 0.8852
2023/12/02 23:35:26 - INFO - root -   Epoch: [212/300][0/283], lr: 0.00000047 	 loss = 0.9917(0.9917)
2023/12/02 23:35:37 - INFO - root -   Epoch: [212/300][20/283], lr: 0.00000047 	 loss = 0.3798(0.3680)
2023/12/02 23:35:56 - INFO - root -   Epoch: [212/300][40/283], lr: 0.00000047 	 loss = 0.6305(0.3397)
2023/12/02 23:36:05 - INFO - root -   Epoch: [212/300][60/283], lr: 0.00000047 	 loss = 0.4988(0.3411)
2023/12/02 23:36:28 - INFO - root -   Epoch: [212/300][80/283], lr: 0.00000047 	 loss = 0.7126(0.3677)
2023/12/02 23:36:43 - INFO - root -   Epoch: [212/300][100/283], lr: 0.00000047 	 loss = 0.0313(0.3328)
2023/12/02 23:36:59 - INFO - root -   Epoch: [212/300][120/283], lr: 0.00000047 	 loss = 0.3988(0.3371)
2023/12/02 23:37:22 - INFO - root -   Epoch: [212/300][140/283], lr: 0.00000047 	 loss = 0.1253(0.3215)
2023/12/02 23:37:41 - INFO - root -   Epoch: [212/300][160/283], lr: 0.00000047 	 loss = 0.0071(0.2997)
2023/12/02 23:38:01 - INFO - root -   Epoch: [212/300][180/283], lr: 0.00000047 	 loss = 0.0751(0.3049)
2023/12/02 23:38:23 - INFO - root -   Epoch: [212/300][200/283], lr: 0.00000047 	 loss = 0.0155(0.2827)
2023/12/02 23:38:34 - INFO - root -   Epoch: [212/300][220/283], lr: 0.00000047 	 loss = 0.0648(0.2781)
2023/12/02 23:38:44 - INFO - root -   Epoch: [212/300][240/283], lr: 0.00000047 	 loss = 0.3171(0.2798)
2023/12/02 23:39:01 - INFO - root -   Epoch: [212/300][260/283], lr: 0.00000047 	 loss = 0.0058(0.2822)
2023/12/02 23:39:09 - INFO - root -   Epoch: [212/300][280/283], lr: 0.00000047 	 loss = 0.2292(0.2764)
2023/12/02 23:39:10 - INFO - root -   Epoch: [212/300] 	 loss = 0.2758
2023/12/02 23:39:10 - INFO - root -   train_accuracy = 0.8905
2023/12/02 23:39:11 - INFO - root -   Epoch: [213/300][0/283], lr: 0.00000047 	 loss = 1.0914(1.0914)
2023/12/02 23:39:41 - INFO - root -   Epoch: [213/300][20/283], lr: 0.00000047 	 loss = 1.4269(0.4788)
2023/12/02 23:39:49 - INFO - root -   Epoch: [213/300][40/283], lr: 0.00000047 	 loss = 0.0141(0.3728)
2023/12/02 23:40:10 - INFO - root -   Epoch: [213/300][60/283], lr: 0.00000047 	 loss = 0.7702(0.3390)
2023/12/02 23:40:22 - INFO - root -   Epoch: [213/300][80/283], lr: 0.00000047 	 loss = 0.6220(0.3475)
2023/12/02 23:40:39 - INFO - root -   Epoch: [213/300][100/283], lr: 0.00000047 	 loss = 0.2538(0.3168)
2023/12/02 23:41:00 - INFO - root -   Epoch: [213/300][120/283], lr: 0.00000047 	 loss = 0.7010(0.3288)
2023/12/02 23:41:08 - INFO - root -   Epoch: [213/300][140/283], lr: 0.00000047 	 loss = 0.0106(0.3055)
2023/12/02 23:41:22 - INFO - root -   Epoch: [213/300][160/283], lr: 0.00000047 	 loss = 0.0374(0.2995)
2023/12/02 23:41:37 - INFO - root -   Epoch: [213/300][180/283], lr: 0.00000047 	 loss = 0.8254(0.2938)
2023/12/02 23:41:51 - INFO - root -   Epoch: [213/300][200/283], lr: 0.00000047 	 loss = 0.0254(0.2810)
2023/12/02 23:42:06 - INFO - root -   Epoch: [213/300][220/283], lr: 0.00000047 	 loss = 0.1907(0.2701)
2023/12/02 23:42:24 - INFO - root -   Epoch: [213/300][240/283], lr: 0.00000047 	 loss = 0.0416(0.2686)
2023/12/02 23:42:37 - INFO - root -   Epoch: [213/300][260/283], lr: 0.00000047 	 loss = 0.0043(0.2626)
2023/12/02 23:42:45 - INFO - root -   Epoch: [213/300][280/283], lr: 0.00000047 	 loss = 0.0837(0.2563)
2023/12/02 23:42:46 - INFO - root -   Epoch: [213/300] 	 loss = 0.2550
2023/12/02 23:42:46 - INFO - root -   train_accuracy = 0.8869
2023/12/02 23:42:48 - INFO - root -   Epoch: [214/300][0/283], lr: 0.00000047 	 loss = 0.0071(0.0071)
2023/12/02 23:43:16 - INFO - root -   Epoch: [214/300][20/283], lr: 0.00000047 	 loss = 0.1715(0.2385)
2023/12/02 23:43:28 - INFO - root -   Epoch: [214/300][40/283], lr: 0.00000047 	 loss = 0.0178(0.2409)
2023/12/02 23:43:56 - INFO - root -   Epoch: [214/300][60/283], lr: 0.00000047 	 loss = 0.7852(0.2735)
2023/12/02 23:44:10 - INFO - root -   Epoch: [214/300][80/283], lr: 0.00000047 	 loss = 0.9778(0.3060)
2023/12/02 23:44:20 - INFO - root -   Epoch: [214/300][100/283], lr: 0.00000047 	 loss = 0.2313(0.2803)
2023/12/02 23:44:41 - INFO - root -   Epoch: [214/300][120/283], lr: 0.00000047 	 loss = 0.0433(0.3027)
2023/12/02 23:44:52 - INFO - root -   Epoch: [214/300][140/283], lr: 0.00000047 	 loss = 0.0113(0.2798)
2023/12/02 23:45:07 - INFO - root -   Epoch: [214/300][160/283], lr: 0.00000047 	 loss = 0.1443(0.2666)
2023/12/02 23:45:28 - INFO - root -   Epoch: [214/300][180/283], lr: 0.00000047 	 loss = 0.0848(0.2594)
2023/12/02 23:45:39 - INFO - root -   Epoch: [214/300][200/283], lr: 0.00000047 	 loss = 0.8425(0.2578)
2023/12/02 23:45:54 - INFO - root -   Epoch: [214/300][220/283], lr: 0.00000047 	 loss = 0.0081(0.2511)
2023/12/02 23:46:12 - INFO - root -   Epoch: [214/300][240/283], lr: 0.00000047 	 loss = 0.1696(0.2541)
2023/12/02 23:46:25 - INFO - root -   Epoch: [214/300][260/283], lr: 0.00000047 	 loss = 0.1485(0.2536)
2023/12/02 23:46:33 - INFO - root -   Epoch: [214/300][280/283], lr: 0.00000047 	 loss = 0.2134(0.2438)
2023/12/02 23:46:34 - INFO - root -   Epoch: [214/300] 	 loss = 0.2424
2023/12/02 23:46:44 - INFO - root -   precision = 0.8601
2023/12/02 23:46:44 - INFO - root -   eval_loss = 0.5105
2023/12/02 23:46:44 - INFO - root -   eval_acc = 0.8601
2023/12/02 23:46:44 - INFO - root -   train_accuracy = 0.9081
2023/12/02 23:46:45 - INFO - root -   Epoch: [215/300][0/283], lr: 0.00000048 	 loss = 0.1046(0.1046)
2023/12/02 23:47:18 - INFO - root -   Epoch: [215/300][20/283], lr: 0.00000048 	 loss = 0.2649(0.3820)
2023/12/02 23:47:42 - INFO - root -   Epoch: [215/300][40/283], lr: 0.00000048 	 loss = 0.2642(0.3326)
2023/12/02 23:47:50 - INFO - root -   Epoch: [215/300][60/283], lr: 0.00000048 	 loss = 0.9564(0.3322)
2023/12/02 23:48:09 - INFO - root -   Epoch: [215/300][80/283], lr: 0.00000048 	 loss = 0.0250(0.2873)
2023/12/02 23:48:21 - INFO - root -   Epoch: [215/300][100/283], lr: 0.00000048 	 loss = 0.0063(0.2499)
2023/12/02 23:48:40 - INFO - root -   Epoch: [215/300][120/283], lr: 0.00000048 	 loss = 0.8179(0.2971)
2023/12/02 23:48:53 - INFO - root -   Epoch: [215/300][140/283], lr: 0.00000048 	 loss = 0.0754(0.2902)
2023/12/02 23:49:13 - INFO - root -   Epoch: [215/300][160/283], lr: 0.00000048 	 loss = 0.0802(0.2931)
2023/12/02 23:49:26 - INFO - root -   Epoch: [215/300][180/283], lr: 0.00000048 	 loss = 0.3086(0.2849)
2023/12/02 23:49:39 - INFO - root -   Epoch: [215/300][200/283], lr: 0.00000048 	 loss = 0.0434(0.2765)
2023/12/02 23:49:57 - INFO - root -   Epoch: [215/300][220/283], lr: 0.00000048 	 loss = 0.0846(0.2683)
2023/12/02 23:50:05 - INFO - root -   Epoch: [215/300][240/283], lr: 0.00000048 	 loss = 0.0024(0.2610)
2023/12/02 23:50:28 - INFO - root -   Epoch: [215/300][260/283], lr: 0.00000048 	 loss = 0.0490(0.2530)
2023/12/02 23:50:36 - INFO - root -   Epoch: [215/300][280/283], lr: 0.00000048 	 loss = 0.0145(0.2449)
2023/12/02 23:50:37 - INFO - root -   Epoch: [215/300] 	 loss = 0.2442
2023/12/02 23:50:37 - INFO - root -   train_accuracy = 0.9028
2023/12/02 23:50:54 - INFO - root -   Epoch: [216/300][0/283], lr: 0.00000048 	 loss = 0.1528(0.1528)
2023/12/02 23:51:22 - INFO - root -   Epoch: [216/300][20/283], lr: 0.00000048 	 loss = 0.1843(0.4599)
2023/12/02 23:51:35 - INFO - root -   Epoch: [216/300][40/283], lr: 0.00000048 	 loss = 0.0136(0.4143)
2023/12/02 23:51:46 - INFO - root -   Epoch: [216/300][60/283], lr: 0.00000048 	 loss = 0.5658(0.3796)
2023/12/02 23:52:17 - INFO - root -   Epoch: [216/300][80/283], lr: 0.00000048 	 loss = 0.0235(0.3612)
2023/12/02 23:52:31 - INFO - root -   Epoch: [216/300][100/283], lr: 0.00000048 	 loss = 0.0808(0.3281)
2023/12/02 23:52:38 - INFO - root -   Epoch: [216/300][120/283], lr: 0.00000048 	 loss = 0.0239(0.3415)
2023/12/02 23:52:58 - INFO - root -   Epoch: [216/300][140/283], lr: 0.00000048 	 loss = 0.1861(0.3056)
2023/12/02 23:53:07 - INFO - root -   Epoch: [216/300][160/283], lr: 0.00000048 	 loss = 0.0058(0.2778)
2023/12/02 23:53:22 - INFO - root -   Epoch: [216/300][180/283], lr: 0.00000048 	 loss = 0.6337(0.2632)
2023/12/02 23:53:45 - INFO - root -   Epoch: [216/300][200/283], lr: 0.00000048 	 loss = 0.0125(0.2579)
2023/12/02 23:54:01 - INFO - root -   Epoch: [216/300][220/283], lr: 0.00000048 	 loss = 0.4748(0.2605)
2023/12/02 23:54:09 - INFO - root -   Epoch: [216/300][240/283], lr: 0.00000048 	 loss = 0.0190(0.2608)
2023/12/02 23:54:29 - INFO - root -   Epoch: [216/300][260/283], lr: 0.00000048 	 loss = 0.0024(0.2517)
2023/12/02 23:54:37 - INFO - root -   Epoch: [216/300][280/283], lr: 0.00000048 	 loss = 0.7661(0.2436)
2023/12/02 23:54:38 - INFO - root -   Epoch: [216/300] 	 loss = 0.2428
2023/12/02 23:54:38 - INFO - root -   train_accuracy = 0.8940
2023/12/02 23:54:55 - INFO - root -   Epoch: [217/300][0/283], lr: 0.00000048 	 loss = 0.0830(0.0830)
2023/12/02 23:55:16 - INFO - root -   Epoch: [217/300][20/283], lr: 0.00000048 	 loss = 0.0515(0.2549)
2023/12/02 23:55:33 - INFO - root -   Epoch: [217/300][40/283], lr: 0.00000048 	 loss = 0.0205(0.3091)
2023/12/02 23:55:49 - INFO - root -   Epoch: [217/300][60/283], lr: 0.00000048 	 loss = 0.8580(0.2837)
2023/12/02 23:56:02 - INFO - root -   Epoch: [217/300][80/283], lr: 0.00000048 	 loss = 0.0496(0.2959)
2023/12/02 23:56:29 - INFO - root -   Epoch: [217/300][100/283], lr: 0.00000048 	 loss = 0.2443(0.2963)
2023/12/02 23:56:56 - INFO - root -   Epoch: [217/300][120/283], lr: 0.00000048 	 loss = 0.7945(0.3275)
2023/12/02 23:57:14 - INFO - root -   Epoch: [217/300][140/283], lr: 0.00000048 	 loss = 0.0155(0.3101)
2023/12/02 23:57:41 - INFO - root -   Epoch: [217/300][160/283], lr: 0.00000048 	 loss = 0.1437(0.3044)
2023/12/02 23:57:49 - INFO - root -   Epoch: [217/300][180/283], lr: 0.00000048 	 loss = 0.0956(0.2990)
2023/12/02 23:58:04 - INFO - root -   Epoch: [217/300][200/283], lr: 0.00000048 	 loss = 0.8727(0.2907)
2023/12/02 23:58:20 - INFO - root -   Epoch: [217/300][220/283], lr: 0.00000048 	 loss = 0.0016(0.2799)
2023/12/02 23:58:30 - INFO - root -   Epoch: [217/300][240/283], lr: 0.00000048 	 loss = 0.1127(0.2726)
2023/12/02 23:58:53 - INFO - root -   Epoch: [217/300][260/283], lr: 0.00000048 	 loss = 0.0015(0.2676)
2023/12/02 23:59:01 - INFO - root -   Epoch: [217/300][280/283], lr: 0.00000048 	 loss = 0.0211(0.2656)
2023/12/02 23:59:02 - INFO - root -   Epoch: [217/300] 	 loss = 0.2647
2023/12/02 23:59:02 - INFO - root -   train_accuracy = 0.8975
2023/12/02 23:59:16 - INFO - root -   Epoch: [218/300][0/283], lr: 0.00000048 	 loss = 1.2182(1.2182)
2023/12/02 23:59:38 - INFO - root -   Epoch: [218/300][20/283], lr: 0.00000048 	 loss = 0.0625(0.4659)
2023/12/02 23:59:51 - INFO - root -   Epoch: [218/300][40/283], lr: 0.00000048 	 loss = 0.3314(0.3760)
2023/12/03 00:00:05 - INFO - root -   Epoch: [218/300][60/283], lr: 0.00000048 	 loss = 2.0204(0.3887)
2023/12/03 00:00:19 - INFO - root -   Epoch: [218/300][80/283], lr: 0.00000048 	 loss = 0.3468(0.3764)
2023/12/03 00:00:36 - INFO - root -   Epoch: [218/300][100/283], lr: 0.00000048 	 loss = 0.0115(0.3594)
2023/12/03 00:00:45 - INFO - root -   Epoch: [218/300][120/283], lr: 0.00000048 	 loss = 1.2470(0.3623)
2023/12/03 00:01:05 - INFO - root -   Epoch: [218/300][140/283], lr: 0.00000048 	 loss = 0.0034(0.3221)
2023/12/03 00:01:18 - INFO - root -   Epoch: [218/300][160/283], lr: 0.00000048 	 loss = 0.0121(0.2919)
2023/12/03 00:01:26 - INFO - root -   Epoch: [218/300][180/283], lr: 0.00000048 	 loss = 0.0407(0.2727)
2023/12/03 00:01:49 - INFO - root -   Epoch: [218/300][200/283], lr: 0.00000048 	 loss = 0.1059(0.2672)
2023/12/03 00:02:05 - INFO - root -   Epoch: [218/300][220/283], lr: 0.00000048 	 loss = 0.0148(0.2628)
2023/12/03 00:02:13 - INFO - root -   Epoch: [218/300][240/283], lr: 0.00000048 	 loss = 0.0812(0.2628)
2023/12/03 00:02:37 - INFO - root -   Epoch: [218/300][260/283], lr: 0.00000048 	 loss = 0.0008(0.2629)
2023/12/03 00:02:45 - INFO - root -   Epoch: [218/300][280/283], lr: 0.00000048 	 loss = 0.1401(0.2606)
2023/12/03 00:02:46 - INFO - root -   Epoch: [218/300] 	 loss = 0.2592
2023/12/03 00:02:46 - INFO - root -   train_accuracy = 0.8905
2023/12/03 00:02:47 - INFO - root -   Epoch: [219/300][0/283], lr: 0.00000048 	 loss = 0.0026(0.0026)
2023/12/03 00:03:20 - INFO - root -   Epoch: [219/300][20/283], lr: 0.00000048 	 loss = 0.1595(0.3708)
2023/12/03 00:03:37 - INFO - root -   Epoch: [219/300][40/283], lr: 0.00000048 	 loss = 0.2161(0.3316)
2023/12/03 00:03:53 - INFO - root -   Epoch: [219/300][60/283], lr: 0.00000048 	 loss = 0.7849(0.3887)
2023/12/03 00:04:01 - INFO - root -   Epoch: [219/300][80/283], lr: 0.00000048 	 loss = 0.0116(0.3407)
2023/12/03 00:04:30 - INFO - root -   Epoch: [219/300][100/283], lr: 0.00000048 	 loss = 0.0086(0.3280)
2023/12/03 00:04:39 - INFO - root -   Epoch: [219/300][120/283], lr: 0.00000048 	 loss = 0.3283(0.3369)
2023/12/03 00:04:53 - INFO - root -   Epoch: [219/300][140/283], lr: 0.00000048 	 loss = 0.0038(0.3138)
2023/12/03 00:05:03 - INFO - root -   Epoch: [219/300][160/283], lr: 0.00000048 	 loss = 0.4208(0.3011)
2023/12/03 00:05:24 - INFO - root -   Epoch: [219/300][180/283], lr: 0.00000048 	 loss = 0.2353(0.2821)
2023/12/03 00:05:32 - INFO - root -   Epoch: [219/300][200/283], lr: 0.00000048 	 loss = 0.1765(0.2733)
2023/12/03 00:05:55 - INFO - root -   Epoch: [219/300][220/283], lr: 0.00000048 	 loss = 0.3035(0.2616)
2023/12/03 00:06:17 - INFO - root -   Epoch: [219/300][240/283], lr: 0.00000048 	 loss = 0.4643(0.2634)
2023/12/03 00:06:30 - INFO - root -   Epoch: [219/300][260/283], lr: 0.00000048 	 loss = 0.1325(0.2592)
2023/12/03 00:06:41 - INFO - root -   Epoch: [219/300][280/283], lr: 0.00000048 	 loss = 0.6389(0.2574)
2023/12/03 00:06:42 - INFO - root -   Epoch: [219/300] 	 loss = 0.2561
2023/12/03 00:06:52 - INFO - root -   precision = 0.8462
2023/12/03 00:06:52 - INFO - root -   eval_loss = 0.4928
2023/12/03 00:06:52 - INFO - root -   eval_acc = 0.8462
2023/12/03 00:06:52 - INFO - root -   train_accuracy = 0.8799
2023/12/03 00:06:54 - INFO - root -   Epoch: [220/300][0/283], lr: 0.00000048 	 loss = 0.1204(0.1204)
2023/12/03 00:07:29 - INFO - root -   Epoch: [220/300][20/283], lr: 0.00000048 	 loss = 0.9152(0.2934)
2023/12/03 00:07:37 - INFO - root -   Epoch: [220/300][40/283], lr: 0.00000048 	 loss = 0.2075(0.2651)
2023/12/03 00:07:54 - INFO - root -   Epoch: [220/300][60/283], lr: 0.00000048 	 loss = 0.9825(0.2785)
2023/12/03 00:08:02 - INFO - root -   Epoch: [220/300][80/283], lr: 0.00000048 	 loss = 0.0093(0.2718)
2023/12/03 00:08:14 - INFO - root -   Epoch: [220/300][100/283], lr: 0.00000048 	 loss = 0.1065(0.2748)
2023/12/03 00:08:39 - INFO - root -   Epoch: [220/300][120/283], lr: 0.00000048 	 loss = 0.4445(0.2809)
2023/12/03 00:08:57 - INFO - root -   Epoch: [220/300][140/283], lr: 0.00000048 	 loss = 0.1432(0.2751)
2023/12/03 00:09:11 - INFO - root -   Epoch: [220/300][160/283], lr: 0.00000048 	 loss = 0.1023(0.2830)
2023/12/03 00:09:22 - INFO - root -   Epoch: [220/300][180/283], lr: 0.00000048 	 loss = 0.0096(0.2674)
2023/12/03 00:09:38 - INFO - root -   Epoch: [220/300][200/283], lr: 0.00000048 	 loss = 0.1162(0.2567)
2023/12/03 00:09:52 - INFO - root -   Epoch: [220/300][220/283], lr: 0.00000048 	 loss = 0.1516(0.2420)
2023/12/03 00:10:15 - INFO - root -   Epoch: [220/300][240/283], lr: 0.00000048 	 loss = 0.0026(0.2560)
2023/12/03 00:10:25 - INFO - root -   Epoch: [220/300][260/283], lr: 0.00000048 	 loss = 0.0203(0.2493)
2023/12/03 00:10:36 - INFO - root -   Epoch: [220/300][280/283], lr: 0.00000048 	 loss = 0.7684(0.2450)
2023/12/03 00:10:37 - INFO - root -   Epoch: [220/300] 	 loss = 0.2439
2023/12/03 00:10:37 - INFO - root -   train_accuracy = 0.9081
2023/12/03 00:10:39 - INFO - root -   Epoch: [221/300][0/283], lr: 0.00000049 	 loss = 0.0059(0.0059)
2023/12/03 00:11:10 - INFO - root -   Epoch: [221/300][20/283], lr: 0.00000049 	 loss = 0.3910(0.4427)
2023/12/03 00:11:33 - INFO - root -   Epoch: [221/300][40/283], lr: 0.00000049 	 loss = 0.0969(0.3593)
2023/12/03 00:11:52 - INFO - root -   Epoch: [221/300][60/283], lr: 0.00000049 	 loss = 0.4376(0.3654)
2023/12/03 00:12:00 - INFO - root -   Epoch: [221/300][80/283], lr: 0.00000049 	 loss = 0.0994(0.3158)
2023/12/03 00:12:08 - INFO - root -   Epoch: [221/300][100/283], lr: 0.00000049 	 loss = 0.2104(0.2989)
2023/12/03 00:12:25 - INFO - root -   Epoch: [221/300][120/283], lr: 0.00000049 	 loss = 0.7929(0.3240)
2023/12/03 00:12:42 - INFO - root -   Epoch: [221/300][140/283], lr: 0.00000049 	 loss = 0.2091(0.3112)
2023/12/03 00:12:54 - INFO - root -   Epoch: [221/300][160/283], lr: 0.00000049 	 loss = 0.6492(0.2896)
2023/12/03 00:13:06 - INFO - root -   Epoch: [221/300][180/283], lr: 0.00000049 	 loss = 0.3434(0.2758)
2023/12/03 00:13:13 - INFO - root -   Epoch: [221/300][200/283], lr: 0.00000049 	 loss = 1.0213(0.2664)
2023/12/03 00:13:35 - INFO - root -   Epoch: [221/300][220/283], lr: 0.00000049 	 loss = 0.0317(0.2532)
2023/12/03 00:13:45 - INFO - root -   Epoch: [221/300][240/283], lr: 0.00000049 	 loss = 0.0038(0.2648)
2023/12/03 00:13:57 - INFO - root -   Epoch: [221/300][260/283], lr: 0.00000049 	 loss = 0.0021(0.2592)
2023/12/03 00:14:08 - INFO - root -   Epoch: [221/300][280/283], lr: 0.00000049 	 loss = 0.6660(0.2591)
2023/12/03 00:14:09 - INFO - root -   Epoch: [221/300] 	 loss = 0.2579
2023/12/03 00:14:09 - INFO - root -   train_accuracy = 0.8905
2023/12/03 00:14:11 - INFO - root -   Epoch: [222/300][0/283], lr: 0.00000049 	 loss = 0.0091(0.0091)
2023/12/03 00:14:41 - INFO - root -   Epoch: [222/300][20/283], lr: 0.00000049 	 loss = 0.5242(0.1617)
2023/12/03 00:15:07 - INFO - root -   Epoch: [222/300][40/283], lr: 0.00000049 	 loss = 0.0188(0.2404)
2023/12/03 00:15:19 - INFO - root -   Epoch: [222/300][60/283], lr: 0.00000049 	 loss = 0.9455(0.3316)
2023/12/03 00:15:27 - INFO - root -   Epoch: [222/300][80/283], lr: 0.00000049 	 loss = 1.1852(0.3194)
2023/12/03 00:15:47 - INFO - root -   Epoch: [222/300][100/283], lr: 0.00000049 	 loss = 0.0877(0.3078)
2023/12/03 00:16:11 - INFO - root -   Epoch: [222/300][120/283], lr: 0.00000049 	 loss = 1.6586(0.3306)
2023/12/03 00:16:21 - INFO - root -   Epoch: [222/300][140/283], lr: 0.00000049 	 loss = 0.0183(0.3154)
2023/12/03 00:16:43 - INFO - root -   Epoch: [222/300][160/283], lr: 0.00000049 	 loss = 0.1466(0.2937)
2023/12/03 00:16:55 - INFO - root -   Epoch: [222/300][180/283], lr: 0.00000049 	 loss = 0.0374(0.2840)
2023/12/03 00:17:10 - INFO - root -   Epoch: [222/300][200/283], lr: 0.00000049 	 loss = 0.5593(0.2702)
2023/12/03 00:17:24 - INFO - root -   Epoch: [222/300][220/283], lr: 0.00000049 	 loss = 0.0064(0.2582)
2023/12/03 00:17:41 - INFO - root -   Epoch: [222/300][240/283], lr: 0.00000049 	 loss = 0.2834(0.2587)
2023/12/03 00:18:03 - INFO - root -   Epoch: [222/300][260/283], lr: 0.00000049 	 loss = 0.2231(0.2538)
2023/12/03 00:18:11 - INFO - root -   Epoch: [222/300][280/283], lr: 0.00000049 	 loss = 0.5119(0.2455)
2023/12/03 00:18:12 - INFO - root -   Epoch: [222/300] 	 loss = 0.2443
2023/12/03 00:18:12 - INFO - root -   train_accuracy = 0.9081
2023/12/03 00:18:26 - INFO - root -   Epoch: [223/300][0/283], lr: 0.00000049 	 loss = 0.0145(0.0145)
2023/12/03 00:18:45 - INFO - root -   Epoch: [223/300][20/283], lr: 0.00000049 	 loss = 0.2878(0.2908)
2023/12/03 00:18:53 - INFO - root -   Epoch: [223/300][40/283], lr: 0.00000049 	 loss = 0.0188(0.2279)
2023/12/03 00:19:06 - INFO - root -   Epoch: [223/300][60/283], lr: 0.00000049 	 loss = 0.9241(0.2642)
2023/12/03 00:19:26 - INFO - root -   Epoch: [223/300][80/283], lr: 0.00000049 	 loss = 0.8914(0.2812)
2023/12/03 00:19:41 - INFO - root -   Epoch: [223/300][100/283], lr: 0.00000049 	 loss = 0.0695(0.2768)
2023/12/03 00:19:55 - INFO - root -   Epoch: [223/300][120/283], lr: 0.00000049 	 loss = 0.4335(0.2949)
2023/12/03 00:20:14 - INFO - root -   Epoch: [223/300][140/283], lr: 0.00000049 	 loss = 0.0875(0.2853)
2023/12/03 00:20:22 - INFO - root -   Epoch: [223/300][160/283], lr: 0.00000049 	 loss = 0.0059(0.2622)
2023/12/03 00:20:35 - INFO - root -   Epoch: [223/300][180/283], lr: 0.00000049 	 loss = 0.0360(0.2524)
2023/12/03 00:20:49 - INFO - root -   Epoch: [223/300][200/283], lr: 0.00000049 	 loss = 0.0697(0.2564)
2023/12/03 00:21:14 - INFO - root -   Epoch: [223/300][220/283], lr: 0.00000049 	 loss = 0.3555(0.2543)
2023/12/03 00:21:22 - INFO - root -   Epoch: [223/300][240/283], lr: 0.00000049 	 loss = 0.0124(0.2533)
2023/12/03 00:21:30 - INFO - root -   Epoch: [223/300][260/283], lr: 0.00000049 	 loss = 0.0614(0.2415)
2023/12/03 00:21:44 - INFO - root -   Epoch: [223/300][280/283], lr: 0.00000049 	 loss = 0.0834(0.2346)
2023/12/03 00:21:45 - INFO - root -   Epoch: [223/300] 	 loss = 0.2334
2023/12/03 00:21:45 - INFO - root -   train_accuracy = 0.9028
2023/12/03 00:21:47 - INFO - root -   Epoch: [224/300][0/283], lr: 0.00000049 	 loss = 0.1050(0.1050)
2023/12/03 00:22:08 - INFO - root -   Epoch: [224/300][20/283], lr: 0.00000049 	 loss = 0.1760(0.2231)
2023/12/03 00:22:27 - INFO - root -   Epoch: [224/300][40/283], lr: 0.00000049 	 loss = 0.2984(0.2008)
2023/12/03 00:22:45 - INFO - root -   Epoch: [224/300][60/283], lr: 0.00000049 	 loss = 0.5420(0.2583)
2023/12/03 00:22:58 - INFO - root -   Epoch: [224/300][80/283], lr: 0.00000049 	 loss = 0.0303(0.2803)
2023/12/03 00:23:22 - INFO - root -   Epoch: [224/300][100/283], lr: 0.00000049 	 loss = 0.0201(0.3044)
2023/12/03 00:23:34 - INFO - root -   Epoch: [224/300][120/283], lr: 0.00000049 	 loss = 0.0502(0.3081)
2023/12/03 00:23:50 - INFO - root -   Epoch: [224/300][140/283], lr: 0.00000049 	 loss = 0.2957(0.2845)
2023/12/03 00:24:03 - INFO - root -   Epoch: [224/300][160/283], lr: 0.00000049 	 loss = 0.0443(0.2750)
2023/12/03 00:24:26 - INFO - root -   Epoch: [224/300][180/283], lr: 0.00000049 	 loss = 0.4461(0.2749)
2023/12/03 00:24:34 - INFO - root -   Epoch: [224/300][200/283], lr: 0.00000049 	 loss = 0.1324(0.2634)
2023/12/03 00:24:43 - INFO - root -   Epoch: [224/300][220/283], lr: 0.00000049 	 loss = 0.0086(0.2579)
2023/12/03 00:25:19 - INFO - root -   Epoch: [224/300][240/283], lr: 0.00000049 	 loss = 0.1111(0.2609)
2023/12/03 00:25:27 - INFO - root -   Epoch: [224/300][260/283], lr: 0.00000049 	 loss = 0.0212(0.2595)
2023/12/03 00:25:35 - INFO - root -   Epoch: [224/300][280/283], lr: 0.00000049 	 loss = 0.0101(0.2510)
2023/12/03 00:25:36 - INFO - root -   Epoch: [224/300] 	 loss = 0.2494
2023/12/03 00:25:46 - INFO - root -   precision = 0.8531
2023/12/03 00:25:46 - INFO - root -   eval_loss = 0.4406
2023/12/03 00:25:46 - INFO - root -   eval_acc = 0.8531
2023/12/03 00:25:46 - INFO - root -   train_accuracy = 0.9117
2023/12/03 00:26:02 - INFO - root -   Epoch: [225/300][0/283], lr: 0.00000049 	 loss = 0.2671(0.2671)
2023/12/03 00:26:18 - INFO - root -   Epoch: [225/300][20/283], lr: 0.00000049 	 loss = 1.1694(0.3340)
2023/12/03 00:26:39 - INFO - root -   Epoch: [225/300][40/283], lr: 0.00000049 	 loss = 0.7063(0.3036)
2023/12/03 00:26:47 - INFO - root -   Epoch: [225/300][60/283], lr: 0.00000049 	 loss = 0.9623(0.3771)
2023/12/03 00:27:00 - INFO - root -   Epoch: [225/300][80/283], lr: 0.00000049 	 loss = 0.2260(0.3186)
2023/12/03 00:27:11 - INFO - root -   Epoch: [225/300][100/283], lr: 0.00000049 	 loss = 0.0191(0.3068)
2023/12/03 00:27:27 - INFO - root -   Epoch: [225/300][120/283], lr: 0.00000049 	 loss = 2.1476(0.3343)
2023/12/03 00:27:42 - INFO - root -   Epoch: [225/300][140/283], lr: 0.00000049 	 loss = 0.0165(0.3233)
2023/12/03 00:27:50 - INFO - root -   Epoch: [225/300][160/283], lr: 0.00000049 	 loss = 0.0073(0.2959)
2023/12/03 00:28:02 - INFO - root -   Epoch: [225/300][180/283], lr: 0.00000049 	 loss = 0.7078(0.2786)
2023/12/03 00:28:26 - INFO - root -   Epoch: [225/300][200/283], lr: 0.00000049 	 loss = 0.1818(0.2679)
2023/12/03 00:28:34 - INFO - root -   Epoch: [225/300][220/283], lr: 0.00000049 	 loss = 0.0193(0.2484)
2023/12/03 00:28:48 - INFO - root -   Epoch: [225/300][240/283], lr: 0.00000049 	 loss = 0.0199(0.2442)
2023/12/03 00:29:08 - INFO - root -   Epoch: [225/300][260/283], lr: 0.00000049 	 loss = 0.0376(0.2524)
2023/12/03 00:29:16 - INFO - root -   Epoch: [225/300][280/283], lr: 0.00000049 	 loss = 0.0389(0.2443)
2023/12/03 00:29:17 - INFO - root -   Epoch: [225/300] 	 loss = 0.2448
2023/12/03 00:29:17 - INFO - root -   train_accuracy = 0.9117
2023/12/03 00:29:18 - INFO - root -   Epoch: [226/300][0/283], lr: 0.00000050 	 loss = 0.0119(0.0119)
2023/12/03 00:29:53 - INFO - root -   Epoch: [226/300][20/283], lr: 0.00000050 	 loss = 1.5130(0.3711)
2023/12/03 00:30:01 - INFO - root -   Epoch: [226/300][40/283], lr: 0.00000050 	 loss = 0.1628(0.3837)
2023/12/03 00:30:30 - INFO - root -   Epoch: [226/300][60/283], lr: 0.00000050 	 loss = 1.3836(0.4044)
2023/12/03 00:30:38 - INFO - root -   Epoch: [226/300][80/283], lr: 0.00000050 	 loss = 0.0445(0.3493)
2023/12/03 00:30:54 - INFO - root -   Epoch: [226/300][100/283], lr: 0.00000050 	 loss = 0.3293(0.3357)
2023/12/03 00:31:10 - INFO - root -   Epoch: [226/300][120/283], lr: 0.00000050 	 loss = 1.2530(0.3484)
2023/12/03 00:31:18 - INFO - root -   Epoch: [226/300][140/283], lr: 0.00000050 	 loss = 0.0034(0.3155)
2023/12/03 00:31:36 - INFO - root -   Epoch: [226/300][160/283], lr: 0.00000050 	 loss = 0.0161(0.2894)
2023/12/03 00:31:44 - INFO - root -   Epoch: [226/300][180/283], lr: 0.00000050 	 loss = 0.6185(0.2777)
2023/12/03 00:32:01 - INFO - root -   Epoch: [226/300][200/283], lr: 0.00000050 	 loss = 0.6627(0.2701)
2023/12/03 00:32:20 - INFO - root -   Epoch: [226/300][220/283], lr: 0.00000050 	 loss = 0.1007(0.2566)
2023/12/03 00:32:35 - INFO - root -   Epoch: [226/300][240/283], lr: 0.00000050 	 loss = 0.1125(0.2553)
2023/12/03 00:32:58 - INFO - root -   Epoch: [226/300][260/283], lr: 0.00000050 	 loss = 0.0225(0.2536)
2023/12/03 00:33:07 - INFO - root -   Epoch: [226/300][280/283], lr: 0.00000050 	 loss = 0.6673(0.2474)
2023/12/03 00:33:08 - INFO - root -   Epoch: [226/300] 	 loss = 0.2465
2023/12/03 00:33:08 - INFO - root -   train_accuracy = 0.8922
2023/12/03 00:33:09 - INFO - root -   Epoch: [227/300][0/283], lr: 0.00000050 	 loss = 0.0728(0.0728)
2023/12/03 00:33:39 - INFO - root -   Epoch: [227/300][20/283], lr: 0.00000050 	 loss = 0.0200(0.3936)
2023/12/03 00:33:48 - INFO - root -   Epoch: [227/300][40/283], lr: 0.00000050 	 loss = 0.0262(0.2622)
2023/12/03 00:34:08 - INFO - root -   Epoch: [227/300][60/283], lr: 0.00000050 	 loss = 0.8645(0.2727)
2023/12/03 00:34:29 - INFO - root -   Epoch: [227/300][80/283], lr: 0.00000050 	 loss = 0.0633(0.2970)
2023/12/03 00:34:42 - INFO - root -   Epoch: [227/300][100/283], lr: 0.00000050 	 loss = 0.0333(0.2881)
2023/12/03 00:34:50 - INFO - root -   Epoch: [227/300][120/283], lr: 0.00000050 	 loss = 0.6488(0.3146)
2023/12/03 00:35:05 - INFO - root -   Epoch: [227/300][140/283], lr: 0.00000050 	 loss = 0.0105(0.2932)
2023/12/03 00:35:15 - INFO - root -   Epoch: [227/300][160/283], lr: 0.00000050 	 loss = 0.1561(0.2686)
2023/12/03 00:35:36 - INFO - root -   Epoch: [227/300][180/283], lr: 0.00000050 	 loss = 0.0769(0.2513)
2023/12/03 00:35:54 - INFO - root -   Epoch: [227/300][200/283], lr: 0.00000050 	 loss = 0.3320(0.2477)
2023/12/03 00:36:10 - INFO - root -   Epoch: [227/300][220/283], lr: 0.00000050 	 loss = 0.1004(0.2511)
2023/12/03 00:36:20 - INFO - root -   Epoch: [227/300][240/283], lr: 0.00000050 	 loss = 0.2078(0.2459)
2023/12/03 00:36:43 - INFO - root -   Epoch: [227/300][260/283], lr: 0.00000050 	 loss = 0.0120(0.2444)
2023/12/03 00:36:51 - INFO - root -   Epoch: [227/300][280/283], lr: 0.00000050 	 loss = 0.0480(0.2327)
2023/12/03 00:36:52 - INFO - root -   Epoch: [227/300] 	 loss = 0.2322
2023/12/03 00:36:52 - INFO - root -   train_accuracy = 0.8975
2023/12/03 00:37:06 - INFO - root -   Epoch: [228/300][0/283], lr: 0.00000050 	 loss = 0.1079(0.1079)
2023/12/03 00:37:24 - INFO - root -   Epoch: [228/300][20/283], lr: 0.00000050 	 loss = 1.0576(0.2521)
2023/12/03 00:37:37 - INFO - root -   Epoch: [228/300][40/283], lr: 0.00000050 	 loss = 0.0064(0.2566)
2023/12/03 00:37:45 - INFO - root -   Epoch: [228/300][60/283], lr: 0.00000050 	 loss = 0.7837(0.2837)
2023/12/03 00:38:02 - INFO - root -   Epoch: [228/300][80/283], lr: 0.00000050 	 loss = 0.2602(0.2749)
2023/12/03 00:38:11 - INFO - root -   Epoch: [228/300][100/283], lr: 0.00000050 	 loss = 0.0190(0.2564)
2023/12/03 00:38:33 - INFO - root -   Epoch: [228/300][120/283], lr: 0.00000050 	 loss = 0.2916(0.2640)
2023/12/03 00:38:55 - INFO - root -   Epoch: [228/300][140/283], lr: 0.00000050 	 loss = 0.0033(0.2423)
2023/12/03 00:39:07 - INFO - root -   Epoch: [228/300][160/283], lr: 0.00000050 	 loss = 0.0269(0.2383)
2023/12/03 00:39:27 - INFO - root -   Epoch: [228/300][180/283], lr: 0.00000050 	 loss = 0.0311(0.2280)
2023/12/03 00:39:38 - INFO - root -   Epoch: [228/300][200/283], lr: 0.00000050 	 loss = 0.0067(0.2203)
2023/12/03 00:40:03 - INFO - root -   Epoch: [228/300][220/283], lr: 0.00000050 	 loss = 0.0342(0.2170)
2023/12/03 00:40:11 - INFO - root -   Epoch: [228/300][240/283], lr: 0.00000050 	 loss = 0.0042(0.2270)
2023/12/03 00:40:29 - INFO - root -   Epoch: [228/300][260/283], lr: 0.00000050 	 loss = 0.0067(0.2329)
2023/12/03 00:40:45 - INFO - root -   Epoch: [228/300][280/283], lr: 0.00000050 	 loss = 0.8605(0.2372)
2023/12/03 00:40:46 - INFO - root -   Epoch: [228/300] 	 loss = 0.2360
2023/12/03 00:40:46 - INFO - root -   train_accuracy = 0.9064
2023/12/03 00:41:12 - INFO - root -   Epoch: [229/300][0/283], lr: 0.00000050 	 loss = 0.2423(0.2423)
2023/12/03 00:41:28 - INFO - root -   Epoch: [229/300][20/283], lr: 0.00000050 	 loss = 0.2110(0.2834)
2023/12/03 00:41:42 - INFO - root -   Epoch: [229/300][40/283], lr: 0.00000050 	 loss = 0.0250(0.3277)
2023/12/03 00:41:53 - INFO - root -   Epoch: [229/300][60/283], lr: 0.00000050 	 loss = 1.3680(0.3555)
2023/12/03 00:42:15 - INFO - root -   Epoch: [229/300][80/283], lr: 0.00000050 	 loss = 0.2890(0.3343)
2023/12/03 00:42:22 - INFO - root -   Epoch: [229/300][100/283], lr: 0.00000050 	 loss = 0.0084(0.2965)
2023/12/03 00:42:46 - INFO - root -   Epoch: [229/300][120/283], lr: 0.00000050 	 loss = 0.1973(0.3050)
2023/12/03 00:43:04 - INFO - root -   Epoch: [229/300][140/283], lr: 0.00000050 	 loss = 0.0333(0.2869)
2023/12/03 00:43:23 - INFO - root -   Epoch: [229/300][160/283], lr: 0.00000050 	 loss = 0.0104(0.2748)
2023/12/03 00:43:37 - INFO - root -   Epoch: [229/300][180/283], lr: 0.00000050 	 loss = 0.1579(0.2598)
2023/12/03 00:43:53 - INFO - root -   Epoch: [229/300][200/283], lr: 0.00000050 	 loss = 0.1187(0.2584)
2023/12/03 00:44:13 - INFO - root -   Epoch: [229/300][220/283], lr: 0.00000050 	 loss = 0.0036(0.2516)
2023/12/03 00:44:21 - INFO - root -   Epoch: [229/300][240/283], lr: 0.00000050 	 loss = 0.1781(0.2469)
2023/12/03 00:44:37 - INFO - root -   Epoch: [229/300][260/283], lr: 0.00000050 	 loss = 0.0015(0.2532)
2023/12/03 00:44:45 - INFO - root -   Epoch: [229/300][280/283], lr: 0.00000050 	 loss = 0.2296(0.2511)
2023/12/03 00:44:46 - INFO - root -   Epoch: [229/300] 	 loss = 0.2495
2023/12/03 00:44:55 - INFO - root -   precision = 0.8671
2023/12/03 00:44:55 - INFO - root -   eval_loss = 0.4410
2023/12/03 00:44:55 - INFO - root -   eval_acc = 0.8671
2023/12/03 00:44:56 - INFO - root -   train_accuracy = 0.9011
2023/12/03 00:44:57 - INFO - root -   Epoch: [230/300][0/283], lr: 0.00000050 	 loss = 0.0328(0.0328)
2023/12/03 00:45:32 - INFO - root -   Epoch: [230/300][20/283], lr: 0.00000050 	 loss = 0.3017(0.2182)
2023/12/03 00:45:42 - INFO - root -   Epoch: [230/300][40/283], lr: 0.00000050 	 loss = 0.3160(0.2805)
2023/12/03 00:45:59 - INFO - root -   Epoch: [230/300][60/283], lr: 0.00000050 	 loss = 0.9059(0.2606)
2023/12/03 00:46:20 - INFO - root -   Epoch: [230/300][80/283], lr: 0.00000050 	 loss = 0.0308(0.2642)
2023/12/03 00:46:33 - INFO - root -   Epoch: [230/300][100/283], lr: 0.00000050 	 loss = 0.1953(0.2492)
2023/12/03 00:46:46 - INFO - root -   Epoch: [230/300][120/283], lr: 0.00000050 	 loss = 0.2627(0.2809)
2023/12/03 00:47:08 - INFO - root -   Epoch: [230/300][140/283], lr: 0.00000050 	 loss = 0.2847(0.2625)
2023/12/03 00:47:16 - INFO - root -   Epoch: [230/300][160/283], lr: 0.00000050 	 loss = 0.9287(0.2535)
2023/12/03 00:47:24 - INFO - root -   Epoch: [230/300][180/283], lr: 0.00000050 	 loss = 0.0235(0.2371)
2023/12/03 00:47:49 - INFO - root -   Epoch: [230/300][200/283], lr: 0.00000050 	 loss = 0.0806(0.2281)
2023/12/03 00:47:57 - INFO - root -   Epoch: [230/300][220/283], lr: 0.00000050 	 loss = 0.0050(0.2321)
2023/12/03 00:48:10 - INFO - root -   Epoch: [230/300][240/283], lr: 0.00000050 	 loss = 0.0047(0.2248)
2023/12/03 00:48:20 - INFO - root -   Epoch: [230/300][260/283], lr: 0.00000050 	 loss = 0.0231(0.2196)
2023/12/03 00:48:28 - INFO - root -   Epoch: [230/300][280/283], lr: 0.00000050 	 loss = 0.0169(0.2161)
2023/12/03 00:48:29 - INFO - root -   Epoch: [230/300] 	 loss = 0.2148
2023/12/03 00:48:29 - INFO - root -   train_accuracy = 0.9064
2023/12/03 00:48:31 - INFO - root -   Epoch: [231/300][0/283], lr: 0.00000050 	 loss = 0.0144(0.0144)
2023/12/03 00:49:04 - INFO - root -   Epoch: [231/300][20/283], lr: 0.00000050 	 loss = 0.1150(0.2694)
2023/12/03 00:49:12 - INFO - root -   Epoch: [231/300][40/283], lr: 0.00000050 	 loss = 0.3275(0.3316)
2023/12/03 00:49:39 - INFO - root -   Epoch: [231/300][60/283], lr: 0.00000050 	 loss = 1.6628(0.3499)
2023/12/03 00:49:51 - INFO - root -   Epoch: [231/300][80/283], lr: 0.00000050 	 loss = 0.0236(0.3225)
2023/12/03 00:50:06 - INFO - root -   Epoch: [231/300][100/283], lr: 0.00000050 	 loss = 0.0208(0.2919)
2023/12/03 00:50:20 - INFO - root -   Epoch: [231/300][120/283], lr: 0.00000050 	 loss = 0.7177(0.3047)
2023/12/03 00:50:41 - INFO - root -   Epoch: [231/300][140/283], lr: 0.00000050 	 loss = 0.0032(0.2758)
2023/12/03 00:50:58 - INFO - root -   Epoch: [231/300][160/283], lr: 0.00000050 	 loss = 0.0094(0.2716)
2023/12/03 00:51:18 - INFO - root -   Epoch: [231/300][180/283], lr: 0.00000050 	 loss = 0.0743(0.2550)
2023/12/03 00:51:35 - INFO - root -   Epoch: [231/300][200/283], lr: 0.00000050 	 loss = 1.2205(0.2468)
2023/12/03 00:51:43 - INFO - root -   Epoch: [231/300][220/283], lr: 0.00000050 	 loss = 0.0148(0.2423)
2023/12/03 00:51:59 - INFO - root -   Epoch: [231/300][240/283], lr: 0.00000050 	 loss = 0.0024(0.2377)
2023/12/03 00:52:08 - INFO - root -   Epoch: [231/300][260/283], lr: 0.00000050 	 loss = 0.1776(0.2335)
2023/12/03 00:52:17 - INFO - root -   Epoch: [231/300][280/283], lr: 0.00000050 	 loss = 0.7126(0.2309)
2023/12/03 00:52:17 - INFO - root -   Epoch: [231/300] 	 loss = 0.2300
2023/12/03 00:52:17 - INFO - root -   train_accuracy = 0.9117
2023/12/03 00:52:32 - INFO - root -   Epoch: [232/300][0/283], lr: 0.00000051 	 loss = 0.7189(0.7189)
2023/12/03 00:52:51 - INFO - root -   Epoch: [232/300][20/283], lr: 0.00000051 	 loss = 1.2168(0.3714)
2023/12/03 00:53:07 - INFO - root -   Epoch: [232/300][40/283], lr: 0.00000051 	 loss = 0.2295(0.3890)
2023/12/03 00:53:16 - INFO - root -   Epoch: [232/300][60/283], lr: 0.00000051 	 loss = 1.4203(0.4118)
2023/12/03 00:53:24 - INFO - root -   Epoch: [232/300][80/283], lr: 0.00000051 	 loss = 0.0283(0.3967)
2023/12/03 00:53:34 - INFO - root -   Epoch: [232/300][100/283], lr: 0.00000051 	 loss = 0.0547(0.3452)
2023/12/03 00:53:50 - INFO - root -   Epoch: [232/300][120/283], lr: 0.00000051 	 loss = 0.0094(0.3542)
2023/12/03 00:53:58 - INFO - root -   Epoch: [232/300][140/283], lr: 0.00000051 	 loss = 0.1721(0.3264)
2023/12/03 00:54:08 - INFO - root -   Epoch: [232/300][160/283], lr: 0.00000051 	 loss = 0.0281(0.2964)
2023/12/03 00:54:30 - INFO - root -   Epoch: [232/300][180/283], lr: 0.00000051 	 loss = 0.2160(0.2835)
2023/12/03 00:54:45 - INFO - root -   Epoch: [232/300][200/283], lr: 0.00000051 	 loss = 0.1433(0.2745)
2023/12/03 00:55:12 - INFO - root -   Epoch: [232/300][220/283], lr: 0.00000051 	 loss = 0.1331(0.2691)
2023/12/03 00:55:25 - INFO - root -   Epoch: [232/300][240/283], lr: 0.00000051 	 loss = 0.1871(0.2741)
2023/12/03 00:55:33 - INFO - root -   Epoch: [232/300][260/283], lr: 0.00000051 	 loss = 0.0031(0.2685)
2023/12/03 00:55:44 - INFO - root -   Epoch: [232/300][280/283], lr: 0.00000051 	 loss = 0.5028(0.2599)
2023/12/03 00:55:45 - INFO - root -   Epoch: [232/300] 	 loss = 0.2582
2023/12/03 00:55:45 - INFO - root -   train_accuracy = 0.8958
2023/12/03 00:55:47 - INFO - root -   Epoch: [233/300][0/283], lr: 0.00000051 	 loss = 0.0096(0.0096)
2023/12/03 00:56:20 - INFO - root -   Epoch: [233/300][20/283], lr: 0.00000051 	 loss = 0.3278(0.2515)
2023/12/03 00:56:34 - INFO - root -   Epoch: [233/300][40/283], lr: 0.00000051 	 loss = 0.1008(0.3085)
2023/12/03 00:56:47 - INFO - root -   Epoch: [233/300][60/283], lr: 0.00000051 	 loss = 1.0494(0.3016)
2023/12/03 00:57:02 - INFO - root -   Epoch: [233/300][80/283], lr: 0.00000051 	 loss = 0.0281(0.2978)
2023/12/03 00:57:10 - INFO - root -   Epoch: [233/300][100/283], lr: 0.00000051 	 loss = 0.0584(0.2712)
2023/12/03 00:57:29 - INFO - root -   Epoch: [233/300][120/283], lr: 0.00000051 	 loss = 0.2875(0.3034)
2023/12/03 00:57:41 - INFO - root -   Epoch: [233/300][140/283], lr: 0.00000051 	 loss = 0.2303(0.2927)
2023/12/03 00:57:49 - INFO - root -   Epoch: [233/300][160/283], lr: 0.00000051 	 loss = 0.0822(0.2883)
2023/12/03 00:58:11 - INFO - root -   Epoch: [233/300][180/283], lr: 0.00000051 	 loss = 0.0335(0.2736)
2023/12/03 00:58:27 - INFO - root -   Epoch: [233/300][200/283], lr: 0.00000051 	 loss = 0.0950(0.2676)
2023/12/03 00:58:43 - INFO - root -   Epoch: [233/300][220/283], lr: 0.00000051 	 loss = 0.2282(0.2592)
2023/12/03 00:58:51 - INFO - root -   Epoch: [233/300][240/283], lr: 0.00000051 	 loss = 0.0457(0.2581)
2023/12/03 00:59:02 - INFO - root -   Epoch: [233/300][260/283], lr: 0.00000051 	 loss = 0.0096(0.2539)
2023/12/03 00:59:13 - INFO - root -   Epoch: [233/300][280/283], lr: 0.00000051 	 loss = 0.0025(0.2533)
2023/12/03 00:59:14 - INFO - root -   Epoch: [233/300] 	 loss = 0.2523
2023/12/03 00:59:14 - INFO - root -   train_accuracy = 0.8869
2023/12/03 00:59:16 - INFO - root -   Epoch: [234/300][0/283], lr: 0.00000051 	 loss = 0.0063(0.0063)
2023/12/03 00:59:45 - INFO - root -   Epoch: [234/300][20/283], lr: 0.00000051 	 loss = 0.6702(0.2764)
2023/12/03 01:00:00 - INFO - root -   Epoch: [234/300][40/283], lr: 0.00000051 	 loss = 0.3561(0.3457)
2023/12/03 01:00:18 - INFO - root -   Epoch: [234/300][60/283], lr: 0.00000051 	 loss = 1.5018(0.3894)
2023/12/03 01:00:34 - INFO - root -   Epoch: [234/300][80/283], lr: 0.00000051 	 loss = 0.0149(0.3593)
2023/12/03 01:00:46 - INFO - root -   Epoch: [234/300][100/283], lr: 0.00000051 	 loss = 0.2936(0.3769)
2023/12/03 01:01:07 - INFO - root -   Epoch: [234/300][120/283], lr: 0.00000051 	 loss = 1.8391(0.4046)
2023/12/03 01:01:22 - INFO - root -   Epoch: [234/300][140/283], lr: 0.00000051 	 loss = 0.1513(0.3853)
2023/12/03 01:01:41 - INFO - root -   Epoch: [234/300][160/283], lr: 0.00000051 	 loss = 0.0169(0.3515)
2023/12/03 01:01:49 - INFO - root -   Epoch: [234/300][180/283], lr: 0.00000051 	 loss = 0.7310(0.3255)
2023/12/03 01:02:08 - INFO - root -   Epoch: [234/300][200/283], lr: 0.00000051 	 loss = 0.1466(0.3115)
2023/12/03 01:02:22 - INFO - root -   Epoch: [234/300][220/283], lr: 0.00000051 	 loss = 0.0108(0.2948)
2023/12/03 01:02:39 - INFO - root -   Epoch: [234/300][240/283], lr: 0.00000051 	 loss = 0.0061(0.3051)
2023/12/03 01:03:02 - INFO - root -   Epoch: [234/300][260/283], lr: 0.00000051 	 loss = 0.0056(0.3049)
2023/12/03 01:03:10 - INFO - root -   Epoch: [234/300][280/283], lr: 0.00000051 	 loss = 0.0622(0.3005)
2023/12/03 01:03:11 - INFO - root -   Epoch: [234/300] 	 loss = 0.2985
2023/12/03 01:03:21 - INFO - root -   precision = 0.8531
2023/12/03 01:03:21 - INFO - root -   eval_loss = 0.4387
2023/12/03 01:03:21 - INFO - root -   eval_acc = 0.8531
2023/12/03 01:03:21 - INFO - root -   train_accuracy = 0.8746
2023/12/03 01:03:40 - INFO - root -   Epoch: [235/300][0/283], lr: 0.00000051 	 loss = 0.5782(0.5782)
2023/12/03 01:03:56 - INFO - root -   Epoch: [235/300][20/283], lr: 0.00000051 	 loss = 0.4539(0.3387)
2023/12/03 01:04:03 - INFO - root -   Epoch: [235/300][40/283], lr: 0.00000051 	 loss = 0.0935(0.2608)
2023/12/03 01:04:21 - INFO - root -   Epoch: [235/300][60/283], lr: 0.00000051 	 loss = 1.8169(0.2871)
2023/12/03 01:04:36 - INFO - root -   Epoch: [235/300][80/283], lr: 0.00000051 	 loss = 0.4634(0.2685)
2023/12/03 01:04:56 - INFO - root -   Epoch: [235/300][100/283], lr: 0.00000051 	 loss = 0.2855(0.2541)
2023/12/03 01:05:11 - INFO - root -   Epoch: [235/300][120/283], lr: 0.00000051 	 loss = 0.5337(0.2766)
2023/12/03 01:05:33 - INFO - root -   Epoch: [235/300][140/283], lr: 0.00000051 	 loss = 0.3616(0.2760)
2023/12/03 01:05:55 - INFO - root -   Epoch: [235/300][160/283], lr: 0.00000051 	 loss = 0.5545(0.2739)
2023/12/03 01:06:06 - INFO - root -   Epoch: [235/300][180/283], lr: 0.00000051 	 loss = 0.1353(0.2685)
2023/12/03 01:06:24 - INFO - root -   Epoch: [235/300][200/283], lr: 0.00000051 	 loss = 0.0457(0.2606)
2023/12/03 01:06:45 - INFO - root -   Epoch: [235/300][220/283], lr: 0.00000051 	 loss = 0.0025(0.2571)
2023/12/03 01:07:02 - INFO - root -   Epoch: [235/300][240/283], lr: 0.00000051 	 loss = 0.1003(0.2643)
2023/12/03 01:07:13 - INFO - root -   Epoch: [235/300][260/283], lr: 0.00000051 	 loss = 0.2502(0.2596)
2023/12/03 01:07:21 - INFO - root -   Epoch: [235/300][280/283], lr: 0.00000051 	 loss = 0.5820(0.2597)
2023/12/03 01:07:22 - INFO - root -   Epoch: [235/300] 	 loss = 0.2582
2023/12/03 01:07:22 - INFO - root -   train_accuracy = 0.8763
2023/12/03 01:07:42 - INFO - root -   Epoch: [236/300][0/283], lr: 0.00000051 	 loss = 1.2555(1.2555)
2023/12/03 01:07:51 - INFO - root -   Epoch: [236/300][20/283], lr: 0.00000051 	 loss = 0.1423(0.3079)
2023/12/03 01:08:04 - INFO - root -   Epoch: [236/300][40/283], lr: 0.00000051 	 loss = 0.0272(0.3049)
2023/12/03 01:08:32 - INFO - root -   Epoch: [236/300][60/283], lr: 0.00000051 	 loss = 1.1441(0.3153)
2023/12/03 01:08:42 - INFO - root -   Epoch: [236/300][80/283], lr: 0.00000051 	 loss = 0.0103(0.3002)
2023/12/03 01:08:56 - INFO - root -   Epoch: [236/300][100/283], lr: 0.00000051 	 loss = 0.0453(0.2826)
2023/12/03 01:09:21 - INFO - root -   Epoch: [236/300][120/283], lr: 0.00000051 	 loss = 0.8172(0.3223)
2023/12/03 01:09:29 - INFO - root -   Epoch: [236/300][140/283], lr: 0.00000051 	 loss = 0.0716(0.3125)
2023/12/03 01:09:51 - INFO - root -   Epoch: [236/300][160/283], lr: 0.00000051 	 loss = 0.0061(0.3007)
2023/12/03 01:09:59 - INFO - root -   Epoch: [236/300][180/283], lr: 0.00000051 	 loss = 0.0389(0.2836)
2023/12/03 01:10:20 - INFO - root -   Epoch: [236/300][200/283], lr: 0.00000051 	 loss = 0.0248(0.2832)
2023/12/03 01:10:34 - INFO - root -   Epoch: [236/300][220/283], lr: 0.00000051 	 loss = 0.0095(0.2690)
2023/12/03 01:10:49 - INFO - root -   Epoch: [236/300][240/283], lr: 0.00000051 	 loss = 0.0178(0.2564)
2023/12/03 01:11:08 - INFO - root -   Epoch: [236/300][260/283], lr: 0.00000051 	 loss = 0.0563(0.2463)
2023/12/03 01:11:16 - INFO - root -   Epoch: [236/300][280/283], lr: 0.00000051 	 loss = 0.1492(0.2463)
2023/12/03 01:11:17 - INFO - root -   Epoch: [236/300] 	 loss = 0.2453
2023/12/03 01:11:17 - INFO - root -   train_accuracy = 0.9011
2023/12/03 01:11:19 - INFO - root -   Epoch: [237/300][0/283], lr: 0.00000051 	 loss = 0.8765(0.8765)
2023/12/03 01:11:47 - INFO - root -   Epoch: [237/300][20/283], lr: 0.00000051 	 loss = 0.8477(0.3245)
2023/12/03 01:12:03 - INFO - root -   Epoch: [237/300][40/283], lr: 0.00000051 	 loss = 0.0101(0.2830)
2023/12/03 01:12:26 - INFO - root -   Epoch: [237/300][60/283], lr: 0.00000051 	 loss = 0.0952(0.3211)
2023/12/03 01:12:34 - INFO - root -   Epoch: [237/300][80/283], lr: 0.00000051 	 loss = 0.0168(0.3088)
2023/12/03 01:12:49 - INFO - root -   Epoch: [237/300][100/283], lr: 0.00000051 	 loss = 0.2123(0.3108)
2023/12/03 01:12:57 - INFO - root -   Epoch: [237/300][120/283], lr: 0.00000051 	 loss = 0.0649(0.3173)
2023/12/03 01:13:23 - INFO - root -   Epoch: [237/300][140/283], lr: 0.00000051 	 loss = 0.2323(0.2936)
2023/12/03 01:13:31 - INFO - root -   Epoch: [237/300][160/283], lr: 0.00000051 	 loss = 0.1497(0.2864)
2023/12/03 01:13:54 - INFO - root -   Epoch: [237/300][180/283], lr: 0.00000051 	 loss = 0.0677(0.2778)
2023/12/03 01:14:05 - INFO - root -   Epoch: [237/300][200/283], lr: 0.00000051 	 loss = 0.4630(0.2730)
2023/12/03 01:14:19 - INFO - root -   Epoch: [237/300][220/283], lr: 0.00000051 	 loss = 0.0265(0.2611)
2023/12/03 01:14:39 - INFO - root -   Epoch: [237/300][240/283], lr: 0.00000051 	 loss = 0.3192(0.2506)
2023/12/03 01:14:47 - INFO - root -   Epoch: [237/300][260/283], lr: 0.00000051 	 loss = 0.0025(0.2482)
2023/12/03 01:14:57 - INFO - root -   Epoch: [237/300][280/283], lr: 0.00000051 	 loss = 0.8380(0.2426)
2023/12/03 01:14:58 - INFO - root -   Epoch: [237/300] 	 loss = 0.2412
2023/12/03 01:14:58 - INFO - root -   train_accuracy = 0.8975
2023/12/03 01:15:00 - INFO - root -   Epoch: [238/300][0/283], lr: 0.00000052 	 loss = 0.0073(0.0073)
2023/12/03 01:15:22 - INFO - root -   Epoch: [238/300][20/283], lr: 0.00000052 	 loss = 0.3496(0.1843)
2023/12/03 01:15:41 - INFO - root -   Epoch: [238/300][40/283], lr: 0.00000052 	 loss = 0.1110(0.1512)
2023/12/03 01:16:07 - INFO - root -   Epoch: [238/300][60/283], lr: 0.00000052 	 loss = 0.8660(0.2160)
2023/12/03 01:16:31 - INFO - root -   Epoch: [238/300][80/283], lr: 0.00000052 	 loss = 0.2924(0.2289)
2023/12/03 01:16:39 - INFO - root -   Epoch: [238/300][100/283], lr: 0.00000052 	 loss = 0.2270(0.2168)
2023/12/03 01:16:47 - INFO - root -   Epoch: [238/300][120/283], lr: 0.00000052 	 loss = 0.3665(0.2577)
2023/12/03 01:16:55 - INFO - root -   Epoch: [238/300][140/283], lr: 0.00000052 	 loss = 0.0451(0.2401)
2023/12/03 01:17:12 - INFO - root -   Epoch: [238/300][160/283], lr: 0.00000052 	 loss = 1.0007(0.2318)
2023/12/03 01:17:20 - INFO - root -   Epoch: [238/300][180/283], lr: 0.00000052 	 loss = 0.0630(0.2197)
2023/12/03 01:17:36 - INFO - root -   Epoch: [238/300][200/283], lr: 0.00000052 	 loss = 0.0771(0.2291)
2023/12/03 01:17:53 - INFO - root -   Epoch: [238/300][220/283], lr: 0.00000052 	 loss = 0.0019(0.2150)
2023/12/03 01:18:03 - INFO - root -   Epoch: [238/300][240/283], lr: 0.00000052 	 loss = 0.0129(0.2051)
2023/12/03 01:18:23 - INFO - root -   Epoch: [238/300][260/283], lr: 0.00000052 	 loss = 0.0026(0.2080)
2023/12/03 01:18:31 - INFO - root -   Epoch: [238/300][280/283], lr: 0.00000052 	 loss = 0.0059(0.1989)
2023/12/03 01:18:32 - INFO - root -   Epoch: [238/300] 	 loss = 0.1984
2023/12/03 01:18:32 - INFO - root -   train_accuracy = 0.9329
2023/12/03 01:18:33 - INFO - root -   Epoch: [239/300][0/283], lr: 0.00000052 	 loss = 0.2338(0.2338)
2023/12/03 01:19:11 - INFO - root -   Epoch: [239/300][20/283], lr: 0.00000052 	 loss = 0.2327(0.4543)
2023/12/03 01:19:19 - INFO - root -   Epoch: [239/300][40/283], lr: 0.00000052 	 loss = 0.1047(0.4001)
2023/12/03 01:19:27 - INFO - root -   Epoch: [239/300][60/283], lr: 0.00000052 	 loss = 0.3331(0.3877)
2023/12/03 01:19:53 - INFO - root -   Epoch: [239/300][80/283], lr: 0.00000052 	 loss = 0.3256(0.3553)
2023/12/03 01:20:07 - INFO - root -   Epoch: [239/300][100/283], lr: 0.00000052 	 loss = 0.1501(0.3271)
2023/12/03 01:20:27 - INFO - root -   Epoch: [239/300][120/283], lr: 0.00000052 	 loss = 0.2107(0.3225)
2023/12/03 01:20:39 - INFO - root -   Epoch: [239/300][140/283], lr: 0.00000052 	 loss = 0.0080(0.2951)
2023/12/03 01:21:08 - INFO - root -   Epoch: [239/300][160/283], lr: 0.00000052 	 loss = 0.9924(0.2790)
2023/12/03 01:21:16 - INFO - root -   Epoch: [239/300][180/283], lr: 0.00000052 	 loss = 0.1139(0.2698)
2023/12/03 01:21:39 - INFO - root -   Epoch: [239/300][200/283], lr: 0.00000052 	 loss = 0.9574(0.2701)
2023/12/03 01:21:49 - INFO - root -   Epoch: [239/300][220/283], lr: 0.00000052 	 loss = 0.0035(0.2555)
2023/12/03 01:22:11 - INFO - root -   Epoch: [239/300][240/283], lr: 0.00000052 	 loss = 0.0039(0.2476)
2023/12/03 01:22:21 - INFO - root -   Epoch: [239/300][260/283], lr: 0.00000052 	 loss = 0.0039(0.2568)
2023/12/03 01:22:29 - INFO - root -   Epoch: [239/300][280/283], lr: 0.00000052 	 loss = 0.7962(0.2527)
2023/12/03 01:22:30 - INFO - root -   Epoch: [239/300] 	 loss = 0.2517
2023/12/03 01:22:40 - INFO - root -   precision = 0.8531
2023/12/03 01:22:40 - INFO - root -   eval_loss = 0.4728
2023/12/03 01:22:40 - INFO - root -   eval_acc = 0.8531
2023/12/03 01:22:40 - INFO - root -   train_accuracy = 0.8975
2023/12/03 01:22:41 - INFO - root -   Epoch: [240/300][0/283], lr: 0.00000052 	 loss = 0.0185(0.0185)
2023/12/03 01:23:04 - INFO - root -   Epoch: [240/300][20/283], lr: 0.00000052 	 loss = 0.2309(0.1258)
2023/12/03 01:23:19 - INFO - root -   Epoch: [240/300][40/283], lr: 0.00000052 	 loss = 0.0053(0.1619)
2023/12/03 01:23:38 - INFO - root -   Epoch: [240/300][60/283], lr: 0.00000052 	 loss = 1.2766(0.1755)
2023/12/03 01:23:59 - INFO - root -   Epoch: [240/300][80/283], lr: 0.00000052 	 loss = 0.0391(0.1937)
2023/12/03 01:24:17 - INFO - root -   Epoch: [240/300][100/283], lr: 0.00000052 	 loss = 0.0135(0.1863)
2023/12/03 01:24:40 - INFO - root -   Epoch: [240/300][120/283], lr: 0.00000052 	 loss = 0.1924(0.2274)
2023/12/03 01:24:48 - INFO - root -   Epoch: [240/300][140/283], lr: 0.00000052 	 loss = 0.0230(0.2161)
2023/12/03 01:25:05 - INFO - root -   Epoch: [240/300][160/283], lr: 0.00000052 	 loss = 0.1053(0.2135)
2023/12/03 01:25:22 - INFO - root -   Epoch: [240/300][180/283], lr: 0.00000052 	 loss = 0.0876(0.1987)
2023/12/03 01:25:41 - INFO - root -   Epoch: [240/300][200/283], lr: 0.00000052 	 loss = 0.9685(0.2026)
2023/12/03 01:25:54 - INFO - root -   Epoch: [240/300][220/283], lr: 0.00000052 	 loss = 0.1110(0.1962)
2023/12/03 01:26:11 - INFO - root -   Epoch: [240/300][240/283], lr: 0.00000052 	 loss = 0.1274(0.2054)
2023/12/03 01:26:22 - INFO - root -   Epoch: [240/300][260/283], lr: 0.00000052 	 loss = 0.0098(0.1947)
2023/12/03 01:26:30 - INFO - root -   Epoch: [240/300][280/283], lr: 0.00000052 	 loss = 0.0811(0.1938)
2023/12/03 01:26:31 - INFO - root -   Epoch: [240/300] 	 loss = 0.1933
2023/12/03 01:26:31 - INFO - root -   train_accuracy = 0.9329
2023/12/03 01:26:53 - INFO - root -   Epoch: [241/300][0/283], lr: 0.00000052 	 loss = 0.1408(0.1408)
2023/12/03 01:27:01 - INFO - root -   Epoch: [241/300][20/283], lr: 0.00000052 	 loss = 0.1228(0.1678)
2023/12/03 01:27:09 - INFO - root -   Epoch: [241/300][40/283], lr: 0.00000052 	 loss = 0.0248(0.1717)
2023/12/03 01:27:26 - INFO - root -   Epoch: [241/300][60/283], lr: 0.00000052 	 loss = 1.5718(0.2276)
2023/12/03 01:27:41 - INFO - root -   Epoch: [241/300][80/283], lr: 0.00000052 	 loss = 0.1349(0.2469)
2023/12/03 01:27:57 - INFO - root -   Epoch: [241/300][100/283], lr: 0.00000052 	 loss = 0.0047(0.2190)
2023/12/03 01:28:24 - INFO - root -   Epoch: [241/300][120/283], lr: 0.00000052 	 loss = 0.0112(0.2174)
2023/12/03 01:28:32 - INFO - root -   Epoch: [241/300][140/283], lr: 0.00000052 	 loss = 0.1185(0.2219)
2023/12/03 01:28:47 - INFO - root -   Epoch: [241/300][160/283], lr: 0.00000052 	 loss = 0.0684(0.2104)
2023/12/03 01:29:01 - INFO - root -   Epoch: [241/300][180/283], lr: 0.00000052 	 loss = 0.1022(0.2075)
2023/12/03 01:29:15 - INFO - root -   Epoch: [241/300][200/283], lr: 0.00000052 	 loss = 0.0127(0.2006)
2023/12/03 01:29:29 - INFO - root -   Epoch: [241/300][220/283], lr: 0.00000052 	 loss = 0.0294(0.2004)
2023/12/03 01:29:37 - INFO - root -   Epoch: [241/300][240/283], lr: 0.00000052 	 loss = 0.0032(0.2004)
2023/12/03 01:29:56 - INFO - root -   Epoch: [241/300][260/283], lr: 0.00000052 	 loss = 0.0021(0.2050)
2023/12/03 01:30:04 - INFO - root -   Epoch: [241/300][280/283], lr: 0.00000052 	 loss = 0.0039(0.2078)
2023/12/03 01:30:05 - INFO - root -   Epoch: [241/300] 	 loss = 0.2073
2023/12/03 01:30:05 - INFO - root -   train_accuracy = 0.9099
2023/12/03 01:30:30 - INFO - root -   Epoch: [242/300][0/283], lr: 0.00000052 	 loss = 0.2779(0.2779)
2023/12/03 01:30:38 - INFO - root -   Epoch: [242/300][20/283], lr: 0.00000052 	 loss = 0.5581(0.2712)
2023/12/03 01:30:53 - INFO - root -   Epoch: [242/300][40/283], lr: 0.00000052 	 loss = 0.1490(0.2835)
2023/12/03 01:31:18 - INFO - root -   Epoch: [242/300][60/283], lr: 0.00000052 	 loss = 0.6600(0.3251)
2023/12/03 01:31:36 - INFO - root -   Epoch: [242/300][80/283], lr: 0.00000052 	 loss = 0.6034(0.3255)
2023/12/03 01:31:44 - INFO - root -   Epoch: [242/300][100/283], lr: 0.00000052 	 loss = 0.0362(0.2889)
2023/12/03 01:31:58 - INFO - root -   Epoch: [242/300][120/283], lr: 0.00000052 	 loss = 0.1068(0.3117)
2023/12/03 01:32:14 - INFO - root -   Epoch: [242/300][140/283], lr: 0.00000052 	 loss = 0.0180(0.2803)
2023/12/03 01:32:29 - INFO - root -   Epoch: [242/300][160/283], lr: 0.00000052 	 loss = 0.4780(0.2712)
2023/12/03 01:32:53 - INFO - root -   Epoch: [242/300][180/283], lr: 0.00000052 	 loss = 1.3605(0.2737)
2023/12/03 01:33:09 - INFO - root -   Epoch: [242/300][200/283], lr: 0.00000052 	 loss = 0.0924(0.2667)
2023/12/03 01:33:30 - INFO - root -   Epoch: [242/300][220/283], lr: 0.00000052 	 loss = 0.0004(0.2694)
2023/12/03 01:33:39 - INFO - root -   Epoch: [242/300][240/283], lr: 0.00000052 	 loss = 0.4599(0.2579)
2023/12/03 01:33:56 - INFO - root -   Epoch: [242/300][260/283], lr: 0.00000052 	 loss = 0.0034(0.2536)
2023/12/03 01:34:04 - INFO - root -   Epoch: [242/300][280/283], lr: 0.00000052 	 loss = 0.1475(0.2408)
2023/12/03 01:34:05 - INFO - root -   Epoch: [242/300] 	 loss = 0.2392
2023/12/03 01:34:05 - INFO - root -   train_accuracy = 0.9081
2023/12/03 01:34:22 - INFO - root -   Epoch: [243/300][0/283], lr: 0.00000053 	 loss = 1.1245(1.1245)
2023/12/03 01:34:50 - INFO - root -   Epoch: [243/300][20/283], lr: 0.00000053 	 loss = 0.1615(0.2188)
2023/12/03 01:34:58 - INFO - root -   Epoch: [243/300][40/283], lr: 0.00000053 	 loss = 0.0562(0.2433)
2023/12/03 01:35:06 - INFO - root -   Epoch: [243/300][60/283], lr: 0.00000053 	 loss = 2.6183(0.2821)
2023/12/03 01:35:29 - INFO - root -   Epoch: [243/300][80/283], lr: 0.00000053 	 loss = 0.0189(0.2407)
2023/12/03 01:35:44 - INFO - root -   Epoch: [243/300][100/283], lr: 0.00000053 	 loss = 0.0542(0.2193)
2023/12/03 01:36:00 - INFO - root -   Epoch: [243/300][120/283], lr: 0.00000053 	 loss = 0.0275(0.2652)
2023/12/03 01:36:08 - INFO - root -   Epoch: [243/300][140/283], lr: 0.00000053 	 loss = 0.1421(0.2516)
2023/12/03 01:36:17 - INFO - root -   Epoch: [243/300][160/283], lr: 0.00000053 	 loss = 0.0025(0.2472)
2023/12/03 01:36:34 - INFO - root -   Epoch: [243/300][180/283], lr: 0.00000053 	 loss = 1.0780(0.2373)
2023/12/03 01:36:54 - INFO - root -   Epoch: [243/300][200/283], lr: 0.00000053 	 loss = 0.7709(0.2353)
2023/12/03 01:37:02 - INFO - root -   Epoch: [243/300][220/283], lr: 0.00000053 	 loss = 0.0258(0.2272)
2023/12/03 01:37:12 - INFO - root -   Epoch: [243/300][240/283], lr: 0.00000053 	 loss = 0.0092(0.2211)
2023/12/03 01:37:29 - INFO - root -   Epoch: [243/300][260/283], lr: 0.00000053 	 loss = 0.3897(0.2180)
2023/12/03 01:37:37 - INFO - root -   Epoch: [243/300][280/283], lr: 0.00000053 	 loss = 0.0064(0.2077)
2023/12/03 01:37:38 - INFO - root -   Epoch: [243/300] 	 loss = 0.2074
2023/12/03 01:37:38 - INFO - root -   train_accuracy = 0.9258
2023/12/03 01:37:52 - INFO - root -   Epoch: [244/300][0/283], lr: 0.00000053 	 loss = 0.9090(0.9090)
2023/12/03 01:38:05 - INFO - root -   Epoch: [244/300][20/283], lr: 0.00000053 	 loss = 0.3631(0.2596)
2023/12/03 01:38:22 - INFO - root -   Epoch: [244/300][40/283], lr: 0.00000053 	 loss = 0.0174(0.3919)
2023/12/03 01:38:30 - INFO - root -   Epoch: [244/300][60/283], lr: 0.00000053 	 loss = 1.4216(0.4061)
2023/12/03 01:39:04 - INFO - root -   Epoch: [244/300][80/283], lr: 0.00000053 	 loss = 0.0482(0.3721)
2023/12/03 01:39:25 - INFO - root -   Epoch: [244/300][100/283], lr: 0.00000053 	 loss = 0.4700(0.3384)
2023/12/03 01:39:33 - INFO - root -   Epoch: [244/300][120/283], lr: 0.00000053 	 loss = 2.6941(0.3557)
2023/12/03 01:39:51 - INFO - root -   Epoch: [244/300][140/283], lr: 0.00000053 	 loss = 0.1165(0.3276)
2023/12/03 01:40:23 - INFO - root -   Epoch: [244/300][160/283], lr: 0.00000053 	 loss = 0.8590(0.3025)
2023/12/03 01:40:31 - INFO - root -   Epoch: [244/300][180/283], lr: 0.00000053 	 loss = 0.0351(0.2893)
2023/12/03 01:40:39 - INFO - root -   Epoch: [244/300][200/283], lr: 0.00000053 	 loss = 0.0087(0.2835)
2023/12/03 01:40:58 - INFO - root -   Epoch: [244/300][220/283], lr: 0.00000053 	 loss = 0.0897(0.2742)
2023/12/03 01:41:08 - INFO - root -   Epoch: [244/300][240/283], lr: 0.00000053 	 loss = 0.0056(0.2632)
2023/12/03 01:41:31 - INFO - root -   Epoch: [244/300][260/283], lr: 0.00000053 	 loss = 0.0023(0.2579)
2023/12/03 01:41:39 - INFO - root -   Epoch: [244/300][280/283], lr: 0.00000053 	 loss = 0.2048(0.2596)
2023/12/03 01:41:40 - INFO - root -   Epoch: [244/300] 	 loss = 0.2580
2023/12/03 01:41:50 - INFO - root -   precision = 0.8601
2023/12/03 01:41:50 - INFO - root -   eval_loss = 0.5134
2023/12/03 01:41:50 - INFO - root -   eval_acc = 0.8601
2023/12/03 01:41:50 - INFO - root -   train_accuracy = 0.9081
2023/12/03 01:42:03 - INFO - root -   Epoch: [245/300][0/283], lr: 0.00000053 	 loss = 0.0299(0.0299)
2023/12/03 01:42:17 - INFO - root -   Epoch: [245/300][20/283], lr: 0.00000053 	 loss = 0.2159(0.4117)
2023/12/03 01:42:25 - INFO - root -   Epoch: [245/300][40/283], lr: 0.00000053 	 loss = 0.0189(0.2972)
2023/12/03 01:42:41 - INFO - root -   Epoch: [245/300][60/283], lr: 0.00000053 	 loss = 2.4772(0.3050)
2023/12/03 01:43:00 - INFO - root -   Epoch: [245/300][80/283], lr: 0.00000053 	 loss = 0.2130(0.2920)
2023/12/03 01:43:09 - INFO - root -   Epoch: [245/300][100/283], lr: 0.00000053 	 loss = 0.1909(0.2682)
2023/12/03 01:43:25 - INFO - root -   Epoch: [245/300][120/283], lr: 0.00000053 	 loss = 0.3794(0.2686)
2023/12/03 01:43:33 - INFO - root -   Epoch: [245/300][140/283], lr: 0.00000053 	 loss = 0.0226(0.2477)
2023/12/03 01:43:41 - INFO - root -   Epoch: [245/300][160/283], lr: 0.00000053 	 loss = 0.0196(0.2582)
2023/12/03 01:44:03 - INFO - root -   Epoch: [245/300][180/283], lr: 0.00000053 	 loss = 0.2000(0.2469)
2023/12/03 01:44:21 - INFO - root -   Epoch: [245/300][200/283], lr: 0.00000053 	 loss = 0.6631(0.2445)
2023/12/03 01:44:39 - INFO - root -   Epoch: [245/300][220/283], lr: 0.00000053 	 loss = 0.1205(0.2384)
2023/12/03 01:44:47 - INFO - root -   Epoch: [245/300][240/283], lr: 0.00000053 	 loss = 0.0540(0.2307)
2023/12/03 01:45:11 - INFO - root -   Epoch: [245/300][260/283], lr: 0.00000053 	 loss = 0.0026(0.2292)
2023/12/03 01:45:19 - INFO - root -   Epoch: [245/300][280/283], lr: 0.00000053 	 loss = 0.0123(0.2289)
2023/12/03 01:45:20 - INFO - root -   Epoch: [245/300] 	 loss = 0.2291
2023/12/03 01:45:20 - INFO - root -   train_accuracy = 0.9081
2023/12/03 01:45:38 - INFO - root -   Epoch: [246/300][0/283], lr: 0.00000053 	 loss = 1.4285(1.4285)
2023/12/03 01:46:05 - INFO - root -   Epoch: [246/300][20/283], lr: 0.00000053 	 loss = 0.4459(0.4163)
2023/12/03 01:46:13 - INFO - root -   Epoch: [246/300][40/283], lr: 0.00000053 	 loss = 0.0210(0.3157)
2023/12/03 01:46:26 - INFO - root -   Epoch: [246/300][60/283], lr: 0.00000053 	 loss = 0.5821(0.2715)
2023/12/03 01:46:41 - INFO - root -   Epoch: [246/300][80/283], lr: 0.00000053 	 loss = 0.0071(0.2407)
2023/12/03 01:46:57 - INFO - root -   Epoch: [246/300][100/283], lr: 0.00000053 	 loss = 0.0012(0.2556)
2023/12/03 01:47:19 - INFO - root -   Epoch: [246/300][120/283], lr: 0.00000053 	 loss = 1.0897(0.3017)
2023/12/03 01:47:29 - INFO - root -   Epoch: [246/300][140/283], lr: 0.00000053 	 loss = 0.0093(0.2861)
2023/12/03 01:47:45 - INFO - root -   Epoch: [246/300][160/283], lr: 0.00000053 	 loss = 0.0067(0.2670)
2023/12/03 01:48:04 - INFO - root -   Epoch: [246/300][180/283], lr: 0.00000053 	 loss = 0.1191(0.2476)
2023/12/03 01:48:16 - INFO - root -   Epoch: [246/300][200/283], lr: 0.00000053 	 loss = 0.0160(0.2501)
2023/12/03 01:48:30 - INFO - root -   Epoch: [246/300][220/283], lr: 0.00000053 	 loss = 0.1696(0.2391)
2023/12/03 01:48:38 - INFO - root -   Epoch: [246/300][240/283], lr: 0.00000053 	 loss = 0.0009(0.2290)
2023/12/03 01:48:58 - INFO - root -   Epoch: [246/300][260/283], lr: 0.00000053 	 loss = 0.0041(0.2238)
2023/12/03 01:49:06 - INFO - root -   Epoch: [246/300][280/283], lr: 0.00000053 	 loss = 2.4667(0.2215)
2023/12/03 01:49:07 - INFO - root -   Epoch: [246/300] 	 loss = 0.2203
2023/12/03 01:49:07 - INFO - root -   train_accuracy = 0.9117
2023/12/03 01:49:10 - INFO - root -   Epoch: [247/300][0/283], lr: 0.00000053 	 loss = 0.0703(0.0703)
2023/12/03 01:49:41 - INFO - root -   Epoch: [247/300][20/283], lr: 0.00000053 	 loss = 0.2703(0.4009)
2023/12/03 01:49:49 - INFO - root -   Epoch: [247/300][40/283], lr: 0.00000053 	 loss = 0.2520(0.3175)
2023/12/03 01:49:57 - INFO - root -   Epoch: [247/300][60/283], lr: 0.00000053 	 loss = 0.1312(0.2926)
2023/12/03 01:50:16 - INFO - root -   Epoch: [247/300][80/283], lr: 0.00000053 	 loss = 0.0084(0.2757)
2023/12/03 01:50:30 - INFO - root -   Epoch: [247/300][100/283], lr: 0.00000053 	 loss = 0.2564(0.2546)
2023/12/03 01:50:49 - INFO - root -   Epoch: [247/300][120/283], lr: 0.00000053 	 loss = 0.1104(0.2730)
2023/12/03 01:51:03 - INFO - root -   Epoch: [247/300][140/283], lr: 0.00000053 	 loss = 0.0290(0.2614)
2023/12/03 01:51:30 - INFO - root -   Epoch: [247/300][160/283], lr: 0.00000053 	 loss = 0.9107(0.2556)
2023/12/03 01:51:49 - INFO - root -   Epoch: [247/300][180/283], lr: 0.00000053 	 loss = 0.0487(0.2459)
2023/12/03 01:52:03 - INFO - root -   Epoch: [247/300][200/283], lr: 0.00000053 	 loss = 0.0961(0.2406)
2023/12/03 01:52:11 - INFO - root -   Epoch: [247/300][220/283], lr: 0.00000053 	 loss = 0.0013(0.2312)
2023/12/03 01:52:28 - INFO - root -   Epoch: [247/300][240/283], lr: 0.00000053 	 loss = 0.0087(0.2327)
2023/12/03 01:52:36 - INFO - root -   Epoch: [247/300][260/283], lr: 0.00000053 	 loss = 0.0065(0.2302)
2023/12/03 01:52:48 - INFO - root -   Epoch: [247/300][280/283], lr: 0.00000053 	 loss = 1.6231(0.2253)
2023/12/03 01:52:49 - INFO - root -   Epoch: [247/300] 	 loss = 0.2243
2023/12/03 01:52:49 - INFO - root -   train_accuracy = 0.9187
2023/12/03 01:52:50 - INFO - root -   Epoch: [248/300][0/283], lr: 0.00000053 	 loss = 0.0178(0.0178)
2023/12/03 01:53:24 - INFO - root -   Epoch: [248/300][20/283], lr: 0.00000053 	 loss = 0.2473(0.4358)
2023/12/03 01:53:37 - INFO - root -   Epoch: [248/300][40/283], lr: 0.00000053 	 loss = 0.0091(0.3416)
2023/12/03 01:53:56 - INFO - root -   Epoch: [248/300][60/283], lr: 0.00000053 	 loss = 0.4077(0.3109)
2023/12/03 01:54:07 - INFO - root -   Epoch: [248/300][80/283], lr: 0.00000053 	 loss = 0.0076(0.3211)
2023/12/03 01:54:18 - INFO - root -   Epoch: [248/300][100/283], lr: 0.00000053 	 loss = 0.0037(0.3016)
2023/12/03 01:54:36 - INFO - root -   Epoch: [248/300][120/283], lr: 0.00000053 	 loss = 0.2833(0.3169)
2023/12/03 01:54:48 - INFO - root -   Epoch: [248/300][140/283], lr: 0.00000053 	 loss = 0.0020(0.2936)
2023/12/03 01:54:58 - INFO - root -   Epoch: [248/300][160/283], lr: 0.00000053 	 loss = 0.0087(0.2877)
2023/12/03 01:55:13 - INFO - root -   Epoch: [248/300][180/283], lr: 0.00000053 	 loss = 0.0207(0.2692)
2023/12/03 01:55:30 - INFO - root -   Epoch: [248/300][200/283], lr: 0.00000053 	 loss = 0.1721(0.2575)
2023/12/03 01:55:47 - INFO - root -   Epoch: [248/300][220/283], lr: 0.00000053 	 loss = 0.0221(0.2410)
2023/12/03 01:55:55 - INFO - root -   Epoch: [248/300][240/283], lr: 0.00000053 	 loss = 0.0032(0.2363)
2023/12/03 01:56:03 - INFO - root -   Epoch: [248/300][260/283], lr: 0.00000053 	 loss = 0.0969(0.2316)
2023/12/03 01:56:11 - INFO - root -   Epoch: [248/300][280/283], lr: 0.00000053 	 loss = 0.3992(0.2312)
2023/12/03 01:56:12 - INFO - root -   Epoch: [248/300] 	 loss = 0.2316
2023/12/03 01:56:12 - INFO - root -   train_accuracy = 0.9064
2023/12/03 01:56:25 - INFO - root -   Epoch: [249/300][0/283], lr: 0.00000054 	 loss = 0.0248(0.0248)
2023/12/03 01:56:47 - INFO - root -   Epoch: [249/300][20/283], lr: 0.00000054 	 loss = 0.3104(0.2582)
2023/12/03 01:57:03 - INFO - root -   Epoch: [249/300][40/283], lr: 0.00000054 	 loss = 0.1780(0.2653)
2023/12/03 01:57:22 - INFO - root -   Epoch: [249/300][60/283], lr: 0.00000054 	 loss = 0.1342(0.2410)
2023/12/03 01:57:41 - INFO - root -   Epoch: [249/300][80/283], lr: 0.00000054 	 loss = 0.0127(0.2750)
2023/12/03 01:57:49 - INFO - root -   Epoch: [249/300][100/283], lr: 0.00000054 	 loss = 0.0025(0.2829)
2023/12/03 01:58:04 - INFO - root -   Epoch: [249/300][120/283], lr: 0.00000054 	 loss = 0.0180(0.2869)
2023/12/03 01:58:12 - INFO - root -   Epoch: [249/300][140/283], lr: 0.00000054 	 loss = 0.0098(0.2704)
2023/12/03 01:58:39 - INFO - root -   Epoch: [249/300][160/283], lr: 0.00000054 	 loss = 0.0103(0.2546)
2023/12/03 01:59:04 - INFO - root -   Epoch: [249/300][180/283], lr: 0.00000054 	 loss = 0.5279(0.2457)
2023/12/03 01:59:22 - INFO - root -   Epoch: [249/300][200/283], lr: 0.00000054 	 loss = 0.3966(0.2459)
2023/12/03 01:59:43 - INFO - root -   Epoch: [249/300][220/283], lr: 0.00000054 	 loss = 0.0035(0.2438)
2023/12/03 01:59:51 - INFO - root -   Epoch: [249/300][240/283], lr: 0.00000054 	 loss = 0.1870(0.2389)
2023/12/03 02:00:07 - INFO - root -   Epoch: [249/300][260/283], lr: 0.00000054 	 loss = 0.1200(0.2379)
2023/12/03 02:00:15 - INFO - root -   Epoch: [249/300][280/283], lr: 0.00000054 	 loss = 0.0463(0.2280)
2023/12/03 02:00:16 - INFO - root -   Epoch: [249/300] 	 loss = 0.2267
2023/12/03 02:00:26 - INFO - root -   precision = 0.8462
2023/12/03 02:00:26 - INFO - root -   eval_loss = 0.4973
2023/12/03 02:00:26 - INFO - root -   eval_acc = 0.8462
2023/12/03 02:00:26 - INFO - root -   train_accuracy = 0.9046
2023/12/03 02:00:46 - INFO - root -   Epoch: [250/300][0/283], lr: 0.00000054 	 loss = 0.1499(0.1499)
2023/12/03 02:00:58 - INFO - root -   Epoch: [250/300][20/283], lr: 0.00000054 	 loss = 0.1858(0.4497)
2023/12/03 02:01:12 - INFO - root -   Epoch: [250/300][40/283], lr: 0.00000054 	 loss = 0.0027(0.3173)
2023/12/03 02:01:23 - INFO - root -   Epoch: [250/300][60/283], lr: 0.00000054 	 loss = 1.0341(0.3110)
2023/12/03 02:01:48 - INFO - root -   Epoch: [250/300][80/283], lr: 0.00000054 	 loss = 0.3087(0.2981)
2023/12/03 02:02:11 - INFO - root -   Epoch: [250/300][100/283], lr: 0.00000054 	 loss = 0.1402(0.2873)
2023/12/03 02:02:20 - INFO - root -   Epoch: [250/300][120/283], lr: 0.00000054 	 loss = 1.9824(0.3009)
2023/12/03 02:02:39 - INFO - root -   Epoch: [250/300][140/283], lr: 0.00000054 	 loss = 0.0084(0.2785)
2023/12/03 02:03:01 - INFO - root -   Epoch: [250/300][160/283], lr: 0.00000054 	 loss = 0.0061(0.2760)
2023/12/03 02:03:09 - INFO - root -   Epoch: [250/300][180/283], lr: 0.00000054 	 loss = 0.3763(0.2543)
2023/12/03 02:03:26 - INFO - root -   Epoch: [250/300][200/283], lr: 0.00000054 	 loss = 0.1574(0.2352)
2023/12/03 02:03:40 - INFO - root -   Epoch: [250/300][220/283], lr: 0.00000054 	 loss = 0.0621(0.2342)
2023/12/03 02:03:57 - INFO - root -   Epoch: [250/300][240/283], lr: 0.00000054 	 loss = 0.0271(0.2455)
2023/12/03 02:04:19 - INFO - root -   Epoch: [250/300][260/283], lr: 0.00000054 	 loss = 0.2905(0.2444)
2023/12/03 02:04:26 - INFO - root -   Epoch: [250/300][280/283], lr: 0.00000054 	 loss = 0.7260(0.2438)
2023/12/03 02:04:27 - INFO - root -   Epoch: [250/300] 	 loss = 0.2432
2023/12/03 02:04:27 - INFO - root -   train_accuracy = 0.8975
2023/12/03 02:04:29 - INFO - root -   Epoch: [251/300][0/283], lr: 0.00000054 	 loss = 0.2999(0.2999)
2023/12/03 02:04:59 - INFO - root -   Epoch: [251/300][20/283], lr: 0.00000054 	 loss = 0.3880(0.3490)
2023/12/03 02:05:20 - INFO - root -   Epoch: [251/300][40/283], lr: 0.00000054 	 loss = 0.0240(0.4260)
2023/12/03 02:05:28 - INFO - root -   Epoch: [251/300][60/283], lr: 0.00000054 	 loss = 0.0973(0.4159)
2023/12/03 02:05:47 - INFO - root -   Epoch: [251/300][80/283], lr: 0.00000054 	 loss = 0.0098(0.3492)
2023/12/03 02:05:55 - INFO - root -   Epoch: [251/300][100/283], lr: 0.00000054 	 loss = 0.0098(0.3274)
2023/12/03 02:06:13 - INFO - root -   Epoch: [251/300][120/283], lr: 0.00000054 	 loss = 0.4310(0.3423)
2023/12/03 02:06:25 - INFO - root -   Epoch: [251/300][140/283], lr: 0.00000054 	 loss = 0.0083(0.3218)
2023/12/03 02:06:34 - INFO - root -   Epoch: [251/300][160/283], lr: 0.00000054 	 loss = 0.0059(0.2965)
2023/12/03 02:06:52 - INFO - root -   Epoch: [251/300][180/283], lr: 0.00000054 	 loss = 0.6772(0.2816)
2023/12/03 02:07:04 - INFO - root -   Epoch: [251/300][200/283], lr: 0.00000054 	 loss = 0.0404(0.2703)
2023/12/03 02:07:20 - INFO - root -   Epoch: [251/300][220/283], lr: 0.00000054 	 loss = 0.0031(0.2592)
2023/12/03 02:07:29 - INFO - root -   Epoch: [251/300][240/283], lr: 0.00000054 	 loss = 0.0011(0.2614)
2023/12/03 02:07:40 - INFO - root -   Epoch: [251/300][260/283], lr: 0.00000054 	 loss = 0.0052(0.2524)
2023/12/03 02:07:50 - INFO - root -   Epoch: [251/300][280/283], lr: 0.00000054 	 loss = 0.1744(0.2459)
2023/12/03 02:07:51 - INFO - root -   Epoch: [251/300] 	 loss = 0.2451
2023/12/03 02:07:51 - INFO - root -   train_accuracy = 0.9046
2023/12/03 02:08:09 - INFO - root -   Epoch: [252/300][0/283], lr: 0.00000054 	 loss = 0.2117(0.2117)
2023/12/03 02:08:29 - INFO - root -   Epoch: [252/300][20/283], lr: 0.00000054 	 loss = 0.0172(0.2500)
2023/12/03 02:08:56 - INFO - root -   Epoch: [252/300][40/283], lr: 0.00000054 	 loss = 0.0426(0.2660)
2023/12/03 02:09:14 - INFO - root -   Epoch: [252/300][60/283], lr: 0.00000054 	 loss = 0.6187(0.2895)
2023/12/03 02:09:32 - INFO - root -   Epoch: [252/300][80/283], lr: 0.00000054 	 loss = 0.0047(0.2894)
2023/12/03 02:09:46 - INFO - root -   Epoch: [252/300][100/283], lr: 0.00000054 	 loss = 0.0134(0.3065)
2023/12/03 02:09:59 - INFO - root -   Epoch: [252/300][120/283], lr: 0.00000054 	 loss = 0.0407(0.3622)
2023/12/03 02:10:19 - INFO - root -   Epoch: [252/300][140/283], lr: 0.00000054 	 loss = 0.0152(0.3393)
2023/12/03 02:10:27 - INFO - root -   Epoch: [252/300][160/283], lr: 0.00000054 	 loss = 0.0064(0.3111)
2023/12/03 02:10:41 - INFO - root -   Epoch: [252/300][180/283], lr: 0.00000054 	 loss = 0.4283(0.2979)
2023/12/03 02:11:00 - INFO - root -   Epoch: [252/300][200/283], lr: 0.00000054 	 loss = 0.8776(0.2845)
2023/12/03 02:11:08 - INFO - root -   Epoch: [252/300][220/283], lr: 0.00000054 	 loss = 0.0125(0.2765)
2023/12/03 02:11:33 - INFO - root -   Epoch: [252/300][240/283], lr: 0.00000054 	 loss = 0.1115(0.2823)
2023/12/03 02:11:46 - INFO - root -   Epoch: [252/300][260/283], lr: 0.00000054 	 loss = 0.0106(0.2734)
2023/12/03 02:11:53 - INFO - root -   Epoch: [252/300][280/283], lr: 0.00000054 	 loss = 0.0046(0.2656)
2023/12/03 02:11:54 - INFO - root -   Epoch: [252/300] 	 loss = 0.2640
2023/12/03 02:11:54 - INFO - root -   train_accuracy = 0.8905
2023/12/03 02:12:16 - INFO - root -   Epoch: [253/300][0/283], lr: 0.00000054 	 loss = 0.8523(0.8523)
2023/12/03 02:12:26 - INFO - root -   Epoch: [253/300][20/283], lr: 0.00000054 	 loss = 0.5769(0.4058)
2023/12/03 02:12:34 - INFO - root -   Epoch: [253/300][40/283], lr: 0.00000054 	 loss = 0.0160(0.3461)
2023/12/03 02:12:59 - INFO - root -   Epoch: [253/300][60/283], lr: 0.00000054 	 loss = 1.1374(0.3777)
2023/12/03 02:13:20 - INFO - root -   Epoch: [253/300][80/283], lr: 0.00000054 	 loss = 0.2976(0.3448)
2023/12/03 02:13:34 - INFO - root -   Epoch: [253/300][100/283], lr: 0.00000054 	 loss = 0.0033(0.3160)
2023/12/03 02:13:50 - INFO - root -   Epoch: [253/300][120/283], lr: 0.00000054 	 loss = 0.0126(0.3186)
2023/12/03 02:14:09 - INFO - root -   Epoch: [253/300][140/283], lr: 0.00000054 	 loss = 0.0736(0.3004)
2023/12/03 02:14:21 - INFO - root -   Epoch: [253/300][160/283], lr: 0.00000054 	 loss = 0.0054(0.2976)
2023/12/03 02:14:37 - INFO - root -   Epoch: [253/300][180/283], lr: 0.00000054 	 loss = 0.0121(0.2895)
2023/12/03 02:14:49 - INFO - root -   Epoch: [253/300][200/283], lr: 0.00000054 	 loss = 0.0191(0.2764)
2023/12/03 02:15:05 - INFO - root -   Epoch: [253/300][220/283], lr: 0.00000054 	 loss = 0.0030(0.2631)
2023/12/03 02:15:22 - INFO - root -   Epoch: [253/300][240/283], lr: 0.00000054 	 loss = 0.0038(0.2787)
2023/12/03 02:15:34 - INFO - root -   Epoch: [253/300][260/283], lr: 0.00000054 	 loss = 0.2004(0.2738)
2023/12/03 02:15:42 - INFO - root -   Epoch: [253/300][280/283], lr: 0.00000054 	 loss = 0.0093(0.2743)
2023/12/03 02:15:43 - INFO - root -   Epoch: [253/300] 	 loss = 0.2742
2023/12/03 02:15:43 - INFO - root -   train_accuracy = 0.8693
2023/12/03 02:15:58 - INFO - root -   Epoch: [254/300][0/283], lr: 0.00000054 	 loss = 0.0107(0.0107)
2023/12/03 02:16:16 - INFO - root -   Epoch: [254/300][20/283], lr: 0.00000054 	 loss = 0.2017(0.1483)
2023/12/03 02:16:24 - INFO - root -   Epoch: [254/300][40/283], lr: 0.00000054 	 loss = 0.2826(0.2193)
2023/12/03 02:16:44 - INFO - root -   Epoch: [254/300][60/283], lr: 0.00000054 	 loss = 1.1006(0.2454)
2023/12/03 02:16:53 - INFO - root -   Epoch: [254/300][80/283], lr: 0.00000054 	 loss = 0.5277(0.2630)
2023/12/03 02:17:03 - INFO - root -   Epoch: [254/300][100/283], lr: 0.00000054 	 loss = 0.0034(0.2342)
2023/12/03 02:17:16 - INFO - root -   Epoch: [254/300][120/283], lr: 0.00000054 	 loss = 0.1170(0.2524)
2023/12/03 02:17:30 - INFO - root -   Epoch: [254/300][140/283], lr: 0.00000054 	 loss = 0.0095(0.2394)
2023/12/03 02:17:39 - INFO - root -   Epoch: [254/300][160/283], lr: 0.00000054 	 loss = 0.2897(0.2346)
2023/12/03 02:18:03 - INFO - root -   Epoch: [254/300][180/283], lr: 0.00000054 	 loss = 0.1778(0.2336)
2023/12/03 02:18:20 - INFO - root -   Epoch: [254/300][200/283], lr: 0.00000054 	 loss = 0.9576(0.2355)
2023/12/03 02:18:28 - INFO - root -   Epoch: [254/300][220/283], lr: 0.00000054 	 loss = 0.1540(0.2339)
2023/12/03 02:18:45 - INFO - root -   Epoch: [254/300][240/283], lr: 0.00000054 	 loss = 0.0961(0.2333)
2023/12/03 02:18:59 - INFO - root -   Epoch: [254/300][260/283], lr: 0.00000054 	 loss = 0.0107(0.2268)
2023/12/03 02:19:07 - INFO - root -   Epoch: [254/300][280/283], lr: 0.00000054 	 loss = 0.9579(0.2316)
2023/12/03 02:19:08 - INFO - root -   Epoch: [254/300] 	 loss = 0.2314
2023/12/03 02:19:17 - INFO - root -   precision = 0.8322
2023/12/03 02:19:17 - INFO - root -   eval_loss = 0.5743
2023/12/03 02:19:17 - INFO - root -   eval_acc = 0.8322
2023/12/03 02:19:18 - INFO - root -   train_accuracy = 0.9099
2023/12/03 02:19:19 - INFO - root -   Epoch: [255/300][0/283], lr: 0.00000055 	 loss = 0.1001(0.1001)
2023/12/03 02:19:56 - INFO - root -   Epoch: [255/300][20/283], lr: 0.00000055 	 loss = 0.1290(0.3610)
2023/12/03 02:20:04 - INFO - root -   Epoch: [255/300][40/283], lr: 0.00000055 	 loss = 0.1691(0.3219)
2023/12/03 02:20:26 - INFO - root -   Epoch: [255/300][60/283], lr: 0.00000055 	 loss = 1.0700(0.3013)
2023/12/03 02:20:43 - INFO - root -   Epoch: [255/300][80/283], lr: 0.00000055 	 loss = 0.5396(0.3043)
2023/12/03 02:20:51 - INFO - root -   Epoch: [255/300][100/283], lr: 0.00000055 	 loss = 0.0035(0.2661)
2023/12/03 02:21:04 - INFO - root -   Epoch: [255/300][120/283], lr: 0.00000055 	 loss = 0.0102(0.2711)
2023/12/03 02:21:25 - INFO - root -   Epoch: [255/300][140/283], lr: 0.00000055 	 loss = 0.0240(0.2671)
2023/12/03 02:21:44 - INFO - root -   Epoch: [255/300][160/283], lr: 0.00000055 	 loss = 0.0851(0.2537)
2023/12/03 02:21:55 - INFO - root -   Epoch: [255/300][180/283], lr: 0.00000055 	 loss = 0.1390(0.2381)
2023/12/03 02:22:17 - INFO - root -   Epoch: [255/300][200/283], lr: 0.00000055 	 loss = 0.0408(0.2400)
2023/12/03 02:22:29 - INFO - root -   Epoch: [255/300][220/283], lr: 0.00000055 	 loss = 0.0242(0.2429)
2023/12/03 02:22:37 - INFO - root -   Epoch: [255/300][240/283], lr: 0.00000055 	 loss = 0.0060(0.2402)
2023/12/03 02:22:58 - INFO - root -   Epoch: [255/300][260/283], lr: 0.00000055 	 loss = 0.0236(0.2424)
2023/12/03 02:23:06 - INFO - root -   Epoch: [255/300][280/283], lr: 0.00000055 	 loss = 0.4763(0.2351)
2023/12/03 02:23:07 - INFO - root -   Epoch: [255/300] 	 loss = 0.2338
2023/12/03 02:23:07 - INFO - root -   train_accuracy = 0.9011
2023/12/03 02:23:30 - INFO - root -   Epoch: [256/300][0/283], lr: 0.00000055 	 loss = 0.5358(0.5358)
2023/12/03 02:23:38 - INFO - root -   Epoch: [256/300][20/283], lr: 0.00000055 	 loss = 0.8635(0.2719)
2023/12/03 02:24:05 - INFO - root -   Epoch: [256/300][40/283], lr: 0.00000055 	 loss = 0.2228(0.2610)
2023/12/03 02:24:13 - INFO - root -   Epoch: [256/300][60/283], lr: 0.00000055 	 loss = 0.4560(0.2771)
2023/12/03 02:24:28 - INFO - root -   Epoch: [256/300][80/283], lr: 0.00000055 	 loss = 0.0509(0.2663)
2023/12/03 02:24:36 - INFO - root -   Epoch: [256/300][100/283], lr: 0.00000055 	 loss = 0.0139(0.2526)
2023/12/03 02:25:00 - INFO - root -   Epoch: [256/300][120/283], lr: 0.00000055 	 loss = 0.5449(0.2517)
2023/12/03 02:25:08 - INFO - root -   Epoch: [256/300][140/283], lr: 0.00000055 	 loss = 0.0152(0.2361)
2023/12/03 02:25:26 - INFO - root -   Epoch: [256/300][160/283], lr: 0.00000055 	 loss = 0.0018(0.2362)
2023/12/03 02:25:34 - INFO - root -   Epoch: [256/300][180/283], lr: 0.00000055 	 loss = 0.0716(0.2222)
2023/12/03 02:25:52 - INFO - root -   Epoch: [256/300][200/283], lr: 0.00000055 	 loss = 1.0264(0.2216)
2023/12/03 02:26:05 - INFO - root -   Epoch: [256/300][220/283], lr: 0.00000055 	 loss = 0.0032(0.2109)
2023/12/03 02:26:19 - INFO - root -   Epoch: [256/300][240/283], lr: 0.00000055 	 loss = 0.0040(0.2096)
2023/12/03 02:26:33 - INFO - root -   Epoch: [256/300][260/283], lr: 0.00000055 	 loss = 0.0035(0.2044)
2023/12/03 02:26:41 - INFO - root -   Epoch: [256/300][280/283], lr: 0.00000055 	 loss = 0.0062(0.1976)
2023/12/03 02:26:42 - INFO - root -   Epoch: [256/300] 	 loss = 0.1968
2023/12/03 02:26:42 - INFO - root -   train_accuracy = 0.9117
2023/12/03 02:26:58 - INFO - root -   Epoch: [257/300][0/283], lr: 0.00000055 	 loss = 0.2377(0.2377)
2023/12/03 02:27:17 - INFO - root -   Epoch: [257/300][20/283], lr: 0.00000055 	 loss = 0.1746(0.2361)
2023/12/03 02:27:25 - INFO - root -   Epoch: [257/300][40/283], lr: 0.00000055 	 loss = 0.0915(0.2546)
2023/12/03 02:27:52 - INFO - root -   Epoch: [257/300][60/283], lr: 0.00000055 	 loss = 1.8766(0.2867)
2023/12/03 02:28:00 - INFO - root -   Epoch: [257/300][80/283], lr: 0.00000055 	 loss = 0.2021(0.2804)
2023/12/03 02:28:18 - INFO - root -   Epoch: [257/300][100/283], lr: 0.00000055 	 loss = 0.3285(0.2706)
2023/12/03 02:28:40 - INFO - root -   Epoch: [257/300][120/283], lr: 0.00000055 	 loss = 1.2327(0.2949)
2023/12/03 02:28:49 - INFO - root -   Epoch: [257/300][140/283], lr: 0.00000055 	 loss = 0.0825(0.2695)
2023/12/03 02:29:05 - INFO - root -   Epoch: [257/300][160/283], lr: 0.00000055 	 loss = 0.0281(0.2455)
2023/12/03 02:29:13 - INFO - root -   Epoch: [257/300][180/283], lr: 0.00000055 	 loss = 0.5181(0.2273)
2023/12/03 02:29:34 - INFO - root -   Epoch: [257/300][200/283], lr: 0.00000055 	 loss = 0.0192(0.2268)
2023/12/03 02:29:48 - INFO - root -   Epoch: [257/300][220/283], lr: 0.00000055 	 loss = 0.0124(0.2149)
2023/12/03 02:30:02 - INFO - root -   Epoch: [257/300][240/283], lr: 0.00000055 	 loss = 0.0513(0.2318)
2023/12/03 02:30:23 - INFO - root -   Epoch: [257/300][260/283], lr: 0.00000055 	 loss = 0.0061(0.2334)
2023/12/03 02:30:31 - INFO - root -   Epoch: [257/300][280/283], lr: 0.00000055 	 loss = 0.0390(0.2241)
2023/12/03 02:30:32 - INFO - root -   Epoch: [257/300] 	 loss = 0.2226
2023/12/03 02:30:32 - INFO - root -   train_accuracy = 0.9134
2023/12/03 02:30:49 - INFO - root -   Epoch: [258/300][0/283], lr: 0.00000055 	 loss = 0.1228(0.1228)
2023/12/03 02:31:13 - INFO - root -   Epoch: [258/300][20/283], lr: 0.00000055 	 loss = 0.1836(0.3187)
2023/12/03 02:31:22 - INFO - root -   Epoch: [258/300][40/283], lr: 0.00000055 	 loss = 0.0231(0.2868)
2023/12/03 02:31:34 - INFO - root -   Epoch: [258/300][60/283], lr: 0.00000055 	 loss = 0.0532(0.2686)
2023/12/03 02:31:59 - INFO - root -   Epoch: [258/300][80/283], lr: 0.00000055 	 loss = 0.0105(0.2617)
2023/12/03 02:32:16 - INFO - root -   Epoch: [258/300][100/283], lr: 0.00000055 	 loss = 0.0413(0.2323)
2023/12/03 02:32:29 - INFO - root -   Epoch: [258/300][120/283], lr: 0.00000055 	 loss = 0.2821(0.2496)
2023/12/03 02:32:50 - INFO - root -   Epoch: [258/300][140/283], lr: 0.00000055 	 loss = 0.1118(0.2259)
2023/12/03 02:32:58 - INFO - root -   Epoch: [258/300][160/283], lr: 0.00000055 	 loss = 0.0030(0.2046)
2023/12/03 02:33:23 - INFO - root -   Epoch: [258/300][180/283], lr: 0.00000055 	 loss = 0.3020(0.2021)
2023/12/03 02:33:39 - INFO - root -   Epoch: [258/300][200/283], lr: 0.00000055 	 loss = 0.1352(0.2009)
2023/12/03 02:33:47 - INFO - root -   Epoch: [258/300][220/283], lr: 0.00000055 	 loss = 0.0024(0.2036)
2023/12/03 02:34:04 - INFO - root -   Epoch: [258/300][240/283], lr: 0.00000055 	 loss = 0.3558(0.2086)
2023/12/03 02:34:11 - INFO - root -   Epoch: [258/300][260/283], lr: 0.00000055 	 loss = 0.2426(0.2048)
2023/12/03 02:34:19 - INFO - root -   Epoch: [258/300][280/283], lr: 0.00000055 	 loss = 0.9614(0.2116)
2023/12/03 02:34:20 - INFO - root -   Epoch: [258/300] 	 loss = 0.2104
2023/12/03 02:34:20 - INFO - root -   train_accuracy = 0.9152
2023/12/03 02:34:22 - INFO - root -   Epoch: [259/300][0/283], lr: 0.00000055 	 loss = 0.0027(0.0027)
2023/12/03 02:34:53 - INFO - root -   Epoch: [259/300][20/283], lr: 0.00000055 	 loss = 0.3210(0.3834)
2023/12/03 02:35:05 - INFO - root -   Epoch: [259/300][40/283], lr: 0.00000055 	 loss = 0.1605(0.3282)
2023/12/03 02:35:19 - INFO - root -   Epoch: [259/300][60/283], lr: 0.00000055 	 loss = 1.1755(0.3412)
2023/12/03 02:35:30 - INFO - root -   Epoch: [259/300][80/283], lr: 0.00000055 	 loss = 0.3310(0.3249)
2023/12/03 02:35:49 - INFO - root -   Epoch: [259/300][100/283], lr: 0.00000055 	 loss = 0.1644(0.2976)
2023/12/03 02:35:57 - INFO - root -   Epoch: [259/300][120/283], lr: 0.00000055 	 loss = 0.0142(0.2893)
2023/12/03 02:36:12 - INFO - root -   Epoch: [259/300][140/283], lr: 0.00000055 	 loss = 0.0166(0.2655)
2023/12/03 02:36:25 - INFO - root -   Epoch: [259/300][160/283], lr: 0.00000055 	 loss = 0.0008(0.2494)
2023/12/03 02:36:48 - INFO - root -   Epoch: [259/300][180/283], lr: 0.00000055 	 loss = 0.1972(0.2345)
2023/12/03 02:36:57 - INFO - root -   Epoch: [259/300][200/283], lr: 0.00000055 	 loss = 0.0712(0.2368)
2023/12/03 02:37:10 - INFO - root -   Epoch: [259/300][220/283], lr: 0.00000055 	 loss = 0.0533(0.2280)
2023/12/03 02:37:34 - INFO - root -   Epoch: [259/300][240/283], lr: 0.00000055 	 loss = 0.0088(0.2262)
2023/12/03 02:37:44 - INFO - root -   Epoch: [259/300][260/283], lr: 0.00000055 	 loss = 0.0043(0.2199)
2023/12/03 02:37:58 - INFO - root -   Epoch: [259/300][280/283], lr: 0.00000055 	 loss = 0.0019(0.2190)
2023/12/03 02:37:59 - INFO - root -   Epoch: [259/300] 	 loss = 0.2188
2023/12/03 02:38:09 - INFO - root -   precision = 0.8741
2023/12/03 02:38:09 - INFO - root -   eval_loss = 0.4602
2023/12/03 02:38:09 - INFO - root -   eval_acc = 0.8741
2023/12/03 02:38:09 - INFO - root -   train_accuracy = 0.9170
2023/12/03 02:38:10 - INFO - root -   Epoch: [260/300][0/283], lr: 0.00000055 	 loss = 0.0088(0.0088)
2023/12/03 02:38:35 - INFO - root -   Epoch: [260/300][20/283], lr: 0.00000055 	 loss = 0.3625(0.3288)
2023/12/03 02:38:51 - INFO - root -   Epoch: [260/300][40/283], lr: 0.00000055 	 loss = 0.1676(0.2914)
2023/12/03 02:38:59 - INFO - root -   Epoch: [260/300][60/283], lr: 0.00000055 	 loss = 0.4430(0.2838)
2023/12/03 02:39:19 - INFO - root -   Epoch: [260/300][80/283], lr: 0.00000055 	 loss = 0.1512(0.3008)
2023/12/03 02:39:29 - INFO - root -   Epoch: [260/300][100/283], lr: 0.00000055 	 loss = 0.0108(0.2905)
2023/12/03 02:39:44 - INFO - root -   Epoch: [260/300][120/283], lr: 0.00000055 	 loss = 0.0816(0.2762)
2023/12/03 02:39:54 - INFO - root -   Epoch: [260/300][140/283], lr: 0.00000055 	 loss = 0.0177(0.2542)
2023/12/03 02:40:15 - INFO - root -   Epoch: [260/300][160/283], lr: 0.00000055 	 loss = 0.0730(0.2548)
2023/12/03 02:40:23 - INFO - root -   Epoch: [260/300][180/283], lr: 0.00000055 	 loss = 0.0091(0.2472)
2023/12/03 02:40:40 - INFO - root -   Epoch: [260/300][200/283], lr: 0.00000055 	 loss = 0.2098(0.2437)
2023/12/03 02:40:48 - INFO - root -   Epoch: [260/300][220/283], lr: 0.00000055 	 loss = 0.2341(0.2380)
2023/12/03 02:40:56 - INFO - root -   Epoch: [260/300][240/283], lr: 0.00000055 	 loss = 0.0036(0.2328)
2023/12/03 02:41:15 - INFO - root -   Epoch: [260/300][260/283], lr: 0.00000055 	 loss = 0.0633(0.2277)
2023/12/03 02:41:26 - INFO - root -   Epoch: [260/300][280/283], lr: 0.00000055 	 loss = 1.0508(0.2263)
2023/12/03 02:41:27 - INFO - root -   Epoch: [260/300] 	 loss = 0.2253
2023/12/03 02:41:27 - INFO - root -   train_accuracy = 0.8975
2023/12/03 02:41:42 - INFO - root -   Epoch: [261/300][0/283], lr: 0.00000056 	 loss = 0.9983(0.9983)
2023/12/03 02:41:59 - INFO - root -   Epoch: [261/300][20/283], lr: 0.00000056 	 loss = 0.2403(0.1294)
2023/12/03 02:42:07 - INFO - root -   Epoch: [261/300][40/283], lr: 0.00000056 	 loss = 0.0031(0.2481)
2023/12/03 02:42:14 - INFO - root -   Epoch: [261/300][60/283], lr: 0.00000056 	 loss = 1.4880(0.2843)
2023/12/03 02:42:36 - INFO - root -   Epoch: [261/300][80/283], lr: 0.00000056 	 loss = 0.0678(0.2633)
2023/12/03 02:42:48 - INFO - root -   Epoch: [261/300][100/283], lr: 0.00000056 	 loss = 0.0196(0.2495)
2023/12/03 02:43:11 - INFO - root -   Epoch: [261/300][120/283], lr: 0.00000056 	 loss = 0.0054(0.2568)
2023/12/03 02:43:19 - INFO - root -   Epoch: [261/300][140/283], lr: 0.00000056 	 loss = 0.0114(0.2583)
2023/12/03 02:43:46 - INFO - root -   Epoch: [261/300][160/283], lr: 0.00000056 	 loss = 0.8034(0.2445)
2023/12/03 02:43:54 - INFO - root -   Epoch: [261/300][180/283], lr: 0.00000056 	 loss = 0.1347(0.2348)
2023/12/03 02:44:27 - INFO - root -   Epoch: [261/300][200/283], lr: 0.00000056 	 loss = 0.1447(0.2271)
2023/12/03 02:44:40 - INFO - root -   Epoch: [261/300][220/283], lr: 0.00000056 	 loss = 0.1983(0.2127)
2023/12/03 02:44:53 - INFO - root -   Epoch: [261/300][240/283], lr: 0.00000056 	 loss = 0.0048(0.2210)
2023/12/03 02:45:01 - INFO - root -   Epoch: [261/300][260/283], lr: 0.00000056 	 loss = 0.0168(0.2134)
2023/12/03 02:45:12 - INFO - root -   Epoch: [261/300][280/283], lr: 0.00000056 	 loss = 0.8384(0.2116)
2023/12/03 02:45:13 - INFO - root -   Epoch: [261/300] 	 loss = 0.2106
2023/12/03 02:45:13 - INFO - root -   train_accuracy = 0.9081
2023/12/03 02:45:15 - INFO - root -   Epoch: [262/300][0/283], lr: 0.00000056 	 loss = 0.0482(0.0482)
2023/12/03 02:45:42 - INFO - root -   Epoch: [262/300][20/283], lr: 0.00000056 	 loss = 0.1372(0.1938)
2023/12/03 02:45:57 - INFO - root -   Epoch: [262/300][40/283], lr: 0.00000056 	 loss = 0.0020(0.1758)
2023/12/03 02:46:19 - INFO - root -   Epoch: [262/300][60/283], lr: 0.00000056 	 loss = 1.8124(0.2470)
2023/12/03 02:46:27 - INFO - root -   Epoch: [262/300][80/283], lr: 0.00000056 	 loss = 0.6873(0.2799)
2023/12/03 02:46:35 - INFO - root -   Epoch: [262/300][100/283], lr: 0.00000056 	 loss = 0.4559(0.2743)
2023/12/03 02:46:45 - INFO - root -   Epoch: [262/300][120/283], lr: 0.00000056 	 loss = 0.0247(0.2747)
2023/12/03 02:47:13 - INFO - root -   Epoch: [262/300][140/283], lr: 0.00000056 	 loss = 0.0857(0.2614)
2023/12/03 02:47:24 - INFO - root -   Epoch: [262/300][160/283], lr: 0.00000056 	 loss = 0.0298(0.2451)
2023/12/03 02:47:41 - INFO - root -   Epoch: [262/300][180/283], lr: 0.00000056 	 loss = 0.3282(0.2433)
2023/12/03 02:47:58 - INFO - root -   Epoch: [262/300][200/283], lr: 0.00000056 	 loss = 0.1321(0.2319)
2023/12/03 02:48:06 - INFO - root -   Epoch: [262/300][220/283], lr: 0.00000056 	 loss = 0.0068(0.2194)
2023/12/03 02:48:14 - INFO - root -   Epoch: [262/300][240/283], lr: 0.00000056 	 loss = 0.0108(0.2116)
2023/12/03 02:48:34 - INFO - root -   Epoch: [262/300][260/283], lr: 0.00000056 	 loss = 0.0035(0.2194)
2023/12/03 02:48:44 - INFO - root -   Epoch: [262/300][280/283], lr: 0.00000056 	 loss = 0.9692(0.2256)
2023/12/03 02:48:45 - INFO - root -   Epoch: [262/300] 	 loss = 0.2246
2023/12/03 02:48:45 - INFO - root -   train_accuracy = 0.9134
2023/12/03 02:48:48 - INFO - root -   Epoch: [263/300][0/283], lr: 0.00000056 	 loss = 0.0484(0.0484)
2023/12/03 02:49:25 - INFO - root -   Epoch: [263/300][20/283], lr: 0.00000056 	 loss = 0.2093(0.3990)
2023/12/03 02:49:46 - INFO - root -   Epoch: [263/300][40/283], lr: 0.00000056 	 loss = 0.5175(0.3942)
2023/12/03 02:50:02 - INFO - root -   Epoch: [263/300][60/283], lr: 0.00000056 	 loss = 2.3266(0.4006)
2023/12/03 02:50:10 - INFO - root -   Epoch: [263/300][80/283], lr: 0.00000056 	 loss = 0.0044(0.3787)
2023/12/03 02:50:27 - INFO - root -   Epoch: [263/300][100/283], lr: 0.00000056 	 loss = 0.0778(0.3401)
2023/12/03 02:50:46 - INFO - root -   Epoch: [263/300][120/283], lr: 0.00000056 	 loss = 0.1160(0.3389)
2023/12/03 02:51:02 - INFO - root -   Epoch: [263/300][140/283], lr: 0.00000056 	 loss = 0.0151(0.3247)
2023/12/03 02:51:10 - INFO - root -   Epoch: [263/300][160/283], lr: 0.00000056 	 loss = 0.1313(0.3116)
2023/12/03 02:51:23 - INFO - root -   Epoch: [263/300][180/283], lr: 0.00000056 	 loss = 0.7896(0.2985)
2023/12/03 02:51:45 - INFO - root -   Epoch: [263/300][200/283], lr: 0.00000056 	 loss = 0.1225(0.2787)
2023/12/03 02:52:03 - INFO - root -   Epoch: [263/300][220/283], lr: 0.00000056 	 loss = 0.0012(0.2644)
2023/12/03 02:52:11 - INFO - root -   Epoch: [263/300][240/283], lr: 0.00000056 	 loss = 0.2305(0.2638)
2023/12/03 02:52:34 - INFO - root -   Epoch: [263/300][260/283], lr: 0.00000056 	 loss = 0.3318(0.2742)
2023/12/03 02:52:42 - INFO - root -   Epoch: [263/300][280/283], lr: 0.00000056 	 loss = 0.5747(0.2700)
2023/12/03 02:52:43 - INFO - root -   Epoch: [263/300] 	 loss = 0.2686
2023/12/03 02:52:43 - INFO - root -   train_accuracy = 0.8799
2023/12/03 02:52:56 - INFO - root -   Epoch: [264/300][0/283], lr: 0.00000056 	 loss = 0.0707(0.0707)
2023/12/03 02:53:13 - INFO - root -   Epoch: [264/300][20/283], lr: 0.00000056 	 loss = 0.1682(0.2106)
2023/12/03 02:53:29 - INFO - root -   Epoch: [264/300][40/283], lr: 0.00000056 	 loss = 0.1070(0.1654)
2023/12/03 02:53:38 - INFO - root -   Epoch: [264/300][60/283], lr: 0.00000056 	 loss = 0.0518(0.2095)
2023/12/03 02:53:53 - INFO - root -   Epoch: [264/300][80/283], lr: 0.00000056 	 loss = 0.0307(0.2092)
2023/12/03 02:54:01 - INFO - root -   Epoch: [264/300][100/283], lr: 0.00000056 	 loss = 0.0031(0.1959)
2023/12/03 02:54:20 - INFO - root -   Epoch: [264/300][120/283], lr: 0.00000056 	 loss = 0.1989(0.2201)
2023/12/03 02:54:37 - INFO - root -   Epoch: [264/300][140/283], lr: 0.00000056 	 loss = 0.1560(0.2136)
2023/12/03 02:54:58 - INFO - root -   Epoch: [264/300][160/283], lr: 0.00000056 	 loss = 0.4727(0.2208)
2023/12/03 02:55:12 - INFO - root -   Epoch: [264/300][180/283], lr: 0.00000056 	 loss = 0.0376(0.2221)
2023/12/03 02:55:20 - INFO - root -   Epoch: [264/300][200/283], lr: 0.00000056 	 loss = 0.0095(0.2058)
2023/12/03 02:55:34 - INFO - root -   Epoch: [264/300][220/283], lr: 0.00000056 	 loss = 0.0029(0.1998)
2023/12/03 02:55:57 - INFO - root -   Epoch: [264/300][240/283], lr: 0.00000056 	 loss = 0.0590(0.2119)
2023/12/03 02:56:12 - INFO - root -   Epoch: [264/300][260/283], lr: 0.00000056 	 loss = 0.0055(0.2078)
2023/12/03 02:56:20 - INFO - root -   Epoch: [264/300][280/283], lr: 0.00000056 	 loss = 1.1178(0.2092)
2023/12/03 02:56:21 - INFO - root -   Epoch: [264/300] 	 loss = 0.2078
2023/12/03 02:56:31 - INFO - root -   precision = 0.8392
2023/12/03 02:56:31 - INFO - root -   eval_loss = 0.5548
2023/12/03 02:56:31 - INFO - root -   eval_acc = 0.8392
2023/12/03 02:56:31 - INFO - root -   train_accuracy = 0.9187
2023/12/03 02:56:33 - INFO - root -   Epoch: [265/300][0/283], lr: 0.00000056 	 loss = 0.9890(0.9890)
2023/12/03 02:56:59 - INFO - root -   Epoch: [265/300][20/283], lr: 0.00000056 	 loss = 0.3665(0.3764)
2023/12/03 02:57:07 - INFO - root -   Epoch: [265/300][40/283], lr: 0.00000056 	 loss = 0.0054(0.3367)
2023/12/03 02:57:34 - INFO - root -   Epoch: [265/300][60/283], lr: 0.00000056 	 loss = 1.6372(0.3586)
2023/12/03 02:57:44 - INFO - root -   Epoch: [265/300][80/283], lr: 0.00000056 	 loss = 0.0225(0.3150)
2023/12/03 02:58:03 - INFO - root -   Epoch: [265/300][100/283], lr: 0.00000056 	 loss = 0.0036(0.3170)
2023/12/03 02:58:21 - INFO - root -   Epoch: [265/300][120/283], lr: 0.00000056 	 loss = 0.0453(0.3245)
2023/12/03 02:58:38 - INFO - root -   Epoch: [265/300][140/283], lr: 0.00000056 	 loss = 0.0421(0.2979)
2023/12/03 02:58:45 - INFO - root -   Epoch: [265/300][160/283], lr: 0.00000056 	 loss = 0.0069(0.2875)
2023/12/03 02:59:10 - INFO - root -   Epoch: [265/300][180/283], lr: 0.00000056 	 loss = 0.1815(0.2762)
2023/12/03 02:59:18 - INFO - root -   Epoch: [265/300][200/283], lr: 0.00000056 	 loss = 0.0163(0.2687)
2023/12/03 02:59:32 - INFO - root -   Epoch: [265/300][220/283], lr: 0.00000056 	 loss = 0.1829(0.2636)
2023/12/03 02:59:54 - INFO - root -   Epoch: [265/300][240/283], lr: 0.00000056 	 loss = 0.2784(0.2587)
2023/12/03 03:00:02 - INFO - root -   Epoch: [265/300][260/283], lr: 0.00000056 	 loss = 0.2075(0.2544)
2023/12/03 03:00:10 - INFO - root -   Epoch: [265/300][280/283], lr: 0.00000056 	 loss = 0.5939(0.2437)
2023/12/03 03:00:11 - INFO - root -   Epoch: [265/300] 	 loss = 0.2431
2023/12/03 03:00:11 - INFO - root -   train_accuracy = 0.8993
2023/12/03 03:00:26 - INFO - root -   Epoch: [266/300][0/283], lr: 0.00000057 	 loss = 0.0313(0.0313)
2023/12/03 03:00:59 - INFO - root -   Epoch: [266/300][20/283], lr: 0.00000057 	 loss = 0.5817(0.4259)
2023/12/03 03:01:08 - INFO - root -   Epoch: [266/300][40/283], lr: 0.00000057 	 loss = 0.5360(0.3365)
2023/12/03 03:01:21 - INFO - root -   Epoch: [266/300][60/283], lr: 0.00000057 	 loss = 1.7491(0.3108)
2023/12/03 03:01:29 - INFO - root -   Epoch: [266/300][80/283], lr: 0.00000057 	 loss = 0.0403(0.2819)
2023/12/03 03:01:49 - INFO - root -   Epoch: [266/300][100/283], lr: 0.00000057 	 loss = 0.0037(0.2726)
2023/12/03 03:02:11 - INFO - root -   Epoch: [266/300][120/283], lr: 0.00000057 	 loss = 0.1531(0.2870)
2023/12/03 03:02:27 - INFO - root -   Epoch: [266/300][140/283], lr: 0.00000057 	 loss = 0.1136(0.2896)
2023/12/03 03:02:41 - INFO - root -   Epoch: [266/300][160/283], lr: 0.00000057 	 loss = 0.3802(0.2924)
2023/12/03 03:02:57 - INFO - root -   Epoch: [266/300][180/283], lr: 0.00000057 	 loss = 0.7818(0.2763)
2023/12/03 03:03:20 - INFO - root -   Epoch: [266/300][200/283], lr: 0.00000057 	 loss = 0.9325(0.2742)
2023/12/03 03:03:28 - INFO - root -   Epoch: [266/300][220/283], lr: 0.00000057 	 loss = 0.0036(0.2669)
2023/12/03 03:03:52 - INFO - root -   Epoch: [266/300][240/283], lr: 0.00000057 	 loss = 0.0053(0.2647)
2023/12/03 03:04:00 - INFO - root -   Epoch: [266/300][260/283], lr: 0.00000057 	 loss = 0.0176(0.2537)
2023/12/03 03:04:08 - INFO - root -   Epoch: [266/300][280/283], lr: 0.00000057 	 loss = 0.0634(0.2467)
2023/12/03 03:04:09 - INFO - root -   Epoch: [266/300] 	 loss = 0.2455
2023/12/03 03:04:09 - INFO - root -   train_accuracy = 0.8958
2023/12/03 03:04:23 - INFO - root -   Epoch: [267/300][0/283], lr: 0.00000057 	 loss = 1.2520(1.2520)
2023/12/03 03:04:42 - INFO - root -   Epoch: [267/300][20/283], lr: 0.00000057 	 loss = 0.1607(0.2712)
2023/12/03 03:04:54 - INFO - root -   Epoch: [267/300][40/283], lr: 0.00000057 	 loss = 0.0047(0.2555)
2023/12/03 03:05:15 - INFO - root -   Epoch: [267/300][60/283], lr: 0.00000057 	 loss = 0.0683(0.3139)
2023/12/03 03:05:26 - INFO - root -   Epoch: [267/300][80/283], lr: 0.00000057 	 loss = 0.0087(0.2928)
2023/12/03 03:05:48 - INFO - root -   Epoch: [267/300][100/283], lr: 0.00000057 	 loss = 0.0690(0.2715)
2023/12/03 03:05:56 - INFO - root -   Epoch: [267/300][120/283], lr: 0.00000057 	 loss = 1.1219(0.2917)
2023/12/03 03:06:16 - INFO - root -   Epoch: [267/300][140/283], lr: 0.00000057 	 loss = 0.1764(0.2676)
2023/12/03 03:06:24 - INFO - root -   Epoch: [267/300][160/283], lr: 0.00000057 	 loss = 0.2274(0.2674)
2023/12/03 03:06:40 - INFO - root -   Epoch: [267/300][180/283], lr: 0.00000057 	 loss = 0.5585(0.2513)
2023/12/03 03:06:48 - INFO - root -   Epoch: [267/300][200/283], lr: 0.00000057 	 loss = 0.1554(0.2428)
2023/12/03 03:07:03 - INFO - root -   Epoch: [267/300][220/283], lr: 0.00000057 	 loss = 0.6472(0.2349)
2023/12/03 03:07:21 - INFO - root -   Epoch: [267/300][240/283], lr: 0.00000057 	 loss = 0.0036(0.2356)
2023/12/03 03:07:29 - INFO - root -   Epoch: [267/300][260/283], lr: 0.00000057 	 loss = 0.0154(0.2305)
2023/12/03 03:07:37 - INFO - root -   Epoch: [267/300][280/283], lr: 0.00000057 	 loss = 0.3306(0.2305)
2023/12/03 03:07:38 - INFO - root -   Epoch: [267/300] 	 loss = 0.2300
2023/12/03 03:07:38 - INFO - root -   train_accuracy = 0.9011
2023/12/03 03:07:40 - INFO - root -   Epoch: [268/300][0/283], lr: 0.00000057 	 loss = 0.0052(0.0052)
2023/12/03 03:08:13 - INFO - root -   Epoch: [268/300][20/283], lr: 0.00000057 	 loss = 0.2566(0.2833)
2023/12/03 03:08:21 - INFO - root -   Epoch: [268/300][40/283], lr: 0.00000057 	 loss = 0.0071(0.2896)
2023/12/03 03:08:29 - INFO - root -   Epoch: [268/300][60/283], lr: 0.00000057 	 loss = 0.0790(0.2784)
2023/12/03 03:08:48 - INFO - root -   Epoch: [268/300][80/283], lr: 0.00000057 	 loss = 0.0666(0.2775)
2023/12/03 03:09:05 - INFO - root -   Epoch: [268/300][100/283], lr: 0.00000057 	 loss = 0.3467(0.2549)
2023/12/03 03:09:15 - INFO - root -   Epoch: [268/300][120/283], lr: 0.00000057 	 loss = 0.1481(0.2828)
2023/12/03 03:09:31 - INFO - root -   Epoch: [268/300][140/283], lr: 0.00000057 	 loss = 0.0951(0.2787)
2023/12/03 03:09:39 - INFO - root -   Epoch: [268/300][160/283], lr: 0.00000057 	 loss = 0.0864(0.2640)
2023/12/03 03:09:54 - INFO - root -   Epoch: [268/300][180/283], lr: 0.00000057 	 loss = 0.2119(0.2538)
2023/12/03 03:10:10 - INFO - root -   Epoch: [268/300][200/283], lr: 0.00000057 	 loss = 0.0051(0.2464)
2023/12/03 03:10:22 - INFO - root -   Epoch: [268/300][220/283], lr: 0.00000057 	 loss = 0.2687(0.2405)
2023/12/03 03:10:33 - INFO - root -   Epoch: [268/300][240/283], lr: 0.00000057 	 loss = 0.0201(0.2362)
2023/12/03 03:10:54 - INFO - root -   Epoch: [268/300][260/283], lr: 0.00000057 	 loss = 0.4604(0.2323)
2023/12/03 03:11:02 - INFO - root -   Epoch: [268/300][280/283], lr: 0.00000057 	 loss = 0.0746(0.2300)
2023/12/03 03:11:03 - INFO - root -   Epoch: [268/300] 	 loss = 0.2285
2023/12/03 03:11:03 - INFO - root -   train_accuracy = 0.9117
2023/12/03 03:11:17 - INFO - root -   Epoch: [269/300][0/283], lr: 0.00000057 	 loss = 0.4939(0.4939)
2023/12/03 03:11:27 - INFO - root -   Epoch: [269/300][20/283], lr: 0.00000057 	 loss = 0.1401(0.2440)
2023/12/03 03:11:38 - INFO - root -   Epoch: [269/300][40/283], lr: 0.00000057 	 loss = 0.0146(0.2175)
2023/12/03 03:11:54 - INFO - root -   Epoch: [269/300][60/283], lr: 0.00000057 	 loss = 0.5159(0.2371)
2023/12/03 03:12:12 - INFO - root -   Epoch: [269/300][80/283], lr: 0.00000057 	 loss = 0.0632(0.2315)
2023/12/03 03:12:27 - INFO - root -   Epoch: [269/300][100/283], lr: 0.00000057 	 loss = 0.0125(0.2479)
2023/12/03 03:12:48 - INFO - root -   Epoch: [269/300][120/283], lr: 0.00000057 	 loss = 0.0334(0.2581)
2023/12/03 03:13:02 - INFO - root -   Epoch: [269/300][140/283], lr: 0.00000057 	 loss = 0.1855(0.2507)
2023/12/03 03:13:15 - INFO - root -   Epoch: [269/300][160/283], lr: 0.00000057 	 loss = 0.0241(0.2437)
2023/12/03 03:13:26 - INFO - root -   Epoch: [269/300][180/283], lr: 0.00000057 	 loss = 0.3185(0.2371)
2023/12/03 03:13:45 - INFO - root -   Epoch: [269/300][200/283], lr: 0.00000057 	 loss = 0.0110(0.2369)
2023/12/03 03:13:54 - INFO - root -   Epoch: [269/300][220/283], lr: 0.00000057 	 loss = 0.0113(0.2239)
2023/12/03 03:14:06 - INFO - root -   Epoch: [269/300][240/283], lr: 0.00000057 	 loss = 0.0852(0.2232)
2023/12/03 03:14:26 - INFO - root -   Epoch: [269/300][260/283], lr: 0.00000057 	 loss = 0.0029(0.2169)
2023/12/03 03:14:34 - INFO - root -   Epoch: [269/300][280/283], lr: 0.00000057 	 loss = 0.0067(0.2147)
2023/12/03 03:14:35 - INFO - root -   Epoch: [269/300] 	 loss = 0.2139
2023/12/03 03:14:45 - INFO - root -   precision = 0.8741
2023/12/03 03:14:45 - INFO - root -   eval_loss = 0.5396
2023/12/03 03:14:45 - INFO - root -   eval_acc = 0.8741
2023/12/03 03:14:45 - INFO - root -   train_accuracy = 0.8922
2023/12/03 03:15:04 - INFO - root -   Epoch: [270/300][0/283], lr: 0.00000057 	 loss = 0.7689(0.7689)
2023/12/03 03:15:29 - INFO - root -   Epoch: [270/300][20/283], lr: 0.00000057 	 loss = 0.0934(0.3889)
2023/12/03 03:15:44 - INFO - root -   Epoch: [270/300][40/283], lr: 0.00000057 	 loss = 0.4483(0.3117)
2023/12/03 03:15:52 - INFO - root -   Epoch: [270/300][60/283], lr: 0.00000057 	 loss = 2.5061(0.3235)
2023/12/03 03:16:02 - INFO - root -   Epoch: [270/300][80/283], lr: 0.00000057 	 loss = 0.0027(0.3429)
2023/12/03 03:16:25 - INFO - root -   Epoch: [270/300][100/283], lr: 0.00000057 	 loss = 0.0139(0.3330)
2023/12/03 03:16:33 - INFO - root -   Epoch: [270/300][120/283], lr: 0.00000057 	 loss = 0.5285(0.3377)
2023/12/03 03:16:50 - INFO - root -   Epoch: [270/300][140/283], lr: 0.00000057 	 loss = 0.2716(0.3109)
2023/12/03 03:17:05 - INFO - root -   Epoch: [270/300][160/283], lr: 0.00000057 	 loss = 0.1240(0.2948)
2023/12/03 03:17:19 - INFO - root -   Epoch: [270/300][180/283], lr: 0.00000057 	 loss = 0.0111(0.2795)
2023/12/03 03:17:33 - INFO - root -   Epoch: [270/300][200/283], lr: 0.00000057 	 loss = 0.0150(0.2623)
2023/12/03 03:17:44 - INFO - root -   Epoch: [270/300][220/283], lr: 0.00000057 	 loss = 0.0020(0.2521)
2023/12/03 03:17:58 - INFO - root -   Epoch: [270/300][240/283], lr: 0.00000057 	 loss = 0.0039(0.2512)
2023/12/03 03:18:06 - INFO - root -   Epoch: [270/300][260/283], lr: 0.00000057 	 loss = 0.1758(0.2464)
2023/12/03 03:18:19 - INFO - root -   Epoch: [270/300][280/283], lr: 0.00000057 	 loss = 1.1375(0.2392)
2023/12/03 03:18:20 - INFO - root -   Epoch: [270/300] 	 loss = 0.2381
2023/12/03 03:18:20 - INFO - root -   train_accuracy = 0.9134
2023/12/03 03:18:35 - INFO - root -   Epoch: [271/300][0/283], lr: 0.00000057 	 loss = 0.0375(0.0375)
2023/12/03 03:18:50 - INFO - root -   Epoch: [271/300][20/283], lr: 0.00000057 	 loss = 0.0868(0.3434)
2023/12/03 03:18:58 - INFO - root -   Epoch: [271/300][40/283], lr: 0.00000057 	 loss = 0.0381(0.2607)
2023/12/03 03:19:19 - INFO - root -   Epoch: [271/300][60/283], lr: 0.00000057 	 loss = 0.8594(0.2986)
2023/12/03 03:19:35 - INFO - root -   Epoch: [271/300][80/283], lr: 0.00000057 	 loss = 0.0597(0.2851)
2023/12/03 03:19:57 - INFO - root -   Epoch: [271/300][100/283], lr: 0.00000057 	 loss = 0.0063(0.2823)
2023/12/03 03:20:14 - INFO - root -   Epoch: [271/300][120/283], lr: 0.00000057 	 loss = 1.0572(0.2773)
2023/12/03 03:20:26 - INFO - root -   Epoch: [271/300][140/283], lr: 0.00000057 	 loss = 0.0704(0.2600)
2023/12/03 03:20:42 - INFO - root -   Epoch: [271/300][160/283], lr: 0.00000057 	 loss = 1.5085(0.2489)
2023/12/03 03:21:04 - INFO - root -   Epoch: [271/300][180/283], lr: 0.00000057 	 loss = 0.0693(0.2391)
2023/12/03 03:21:12 - INFO - root -   Epoch: [271/300][200/283], lr: 0.00000057 	 loss = 0.0055(0.2418)
2023/12/03 03:21:37 - INFO - root -   Epoch: [271/300][220/283], lr: 0.00000057 	 loss = 0.0005(0.2430)
2023/12/03 03:21:52 - INFO - root -   Epoch: [271/300][240/283], lr: 0.00000057 	 loss = 0.1363(0.2476)
2023/12/03 03:22:17 - INFO - root -   Epoch: [271/300][260/283], lr: 0.00000057 	 loss = 0.0010(0.2432)
2023/12/03 03:22:25 - INFO - root -   Epoch: [271/300][280/283], lr: 0.00000057 	 loss = 0.0441(0.2437)
2023/12/03 03:22:26 - INFO - root -   Epoch: [271/300] 	 loss = 0.2425
2023/12/03 03:22:26 - INFO - root -   train_accuracy = 0.9011
2023/12/03 03:22:37 - INFO - root -   Epoch: [272/300][0/283], lr: 0.00000058 	 loss = 0.0200(0.0200)
2023/12/03 03:22:47 - INFO - root -   Epoch: [272/300][20/283], lr: 0.00000058 	 loss = 0.1032(0.1784)
2023/12/03 03:23:11 - INFO - root -   Epoch: [272/300][40/283], lr: 0.00000058 	 loss = 0.1894(0.1801)
2023/12/03 03:23:19 - INFO - root -   Epoch: [272/300][60/283], lr: 0.00000058 	 loss = 1.0151(0.2248)
2023/12/03 03:23:35 - INFO - root -   Epoch: [272/300][80/283], lr: 0.00000058 	 loss = 0.0088(0.2709)
2023/12/03 03:23:53 - INFO - root -   Epoch: [272/300][100/283], lr: 0.00000058 	 loss = 0.0442(0.2488)
2023/12/03 03:24:09 - INFO - root -   Epoch: [272/300][120/283], lr: 0.00000058 	 loss = 0.1954(0.2779)
2023/12/03 03:24:24 - INFO - root -   Epoch: [272/300][140/283], lr: 0.00000058 	 loss = 0.0430(0.2618)
2023/12/03 03:24:46 - INFO - root -   Epoch: [272/300][160/283], lr: 0.00000058 	 loss = 0.1028(0.2586)
2023/12/03 03:25:05 - INFO - root -   Epoch: [272/300][180/283], lr: 0.00000058 	 loss = 0.0203(0.2458)
2023/12/03 03:25:13 - INFO - root -   Epoch: [272/300][200/283], lr: 0.00000058 	 loss = 0.1634(0.2413)
2023/12/03 03:25:24 - INFO - root -   Epoch: [272/300][220/283], lr: 0.00000058 	 loss = 0.1759(0.2318)
2023/12/03 03:25:40 - INFO - root -   Epoch: [272/300][240/283], lr: 0.00000058 	 loss = 0.0011(0.2398)
2023/12/03 03:25:51 - INFO - root -   Epoch: [272/300][260/283], lr: 0.00000058 	 loss = 0.0046(0.2343)
2023/12/03 03:26:00 - INFO - root -   Epoch: [272/300][280/283], lr: 0.00000058 	 loss = 0.0022(0.2332)
2023/12/03 03:26:01 - INFO - root -   Epoch: [272/300] 	 loss = 0.2329
2023/12/03 03:26:01 - INFO - root -   train_accuracy = 0.8975
2023/12/03 03:26:15 - INFO - root -   Epoch: [273/300][0/283], lr: 0.00000058 	 loss = 0.0086(0.0086)
2023/12/03 03:26:30 - INFO - root -   Epoch: [273/300][20/283], lr: 0.00000058 	 loss = 0.3479(0.2157)
2023/12/03 03:26:43 - INFO - root -   Epoch: [273/300][40/283], lr: 0.00000058 	 loss = 0.4090(0.2034)
2023/12/03 03:27:11 - INFO - root -   Epoch: [273/300][60/283], lr: 0.00000058 	 loss = 0.7772(0.2191)
2023/12/03 03:27:25 - INFO - root -   Epoch: [273/300][80/283], lr: 0.00000058 	 loss = 0.1823(0.2487)
2023/12/03 03:27:46 - INFO - root -   Epoch: [273/300][100/283], lr: 0.00000058 	 loss = 0.2768(0.2433)
2023/12/03 03:28:01 - INFO - root -   Epoch: [273/300][120/283], lr: 0.00000058 	 loss = 0.0080(0.2291)
2023/12/03 03:28:10 - INFO - root -   Epoch: [273/300][140/283], lr: 0.00000058 	 loss = 0.0044(0.2219)
2023/12/03 03:28:31 - INFO - root -   Epoch: [273/300][160/283], lr: 0.00000058 	 loss = 0.0117(0.2146)
2023/12/03 03:28:45 - INFO - root -   Epoch: [273/300][180/283], lr: 0.00000058 	 loss = 0.0746(0.2029)
2023/12/03 03:28:59 - INFO - root -   Epoch: [273/300][200/283], lr: 0.00000058 	 loss = 0.0066(0.1989)
2023/12/03 03:29:18 - INFO - root -   Epoch: [273/300][220/283], lr: 0.00000058 	 loss = 0.1107(0.1897)
2023/12/03 03:29:29 - INFO - root -   Epoch: [273/300][240/283], lr: 0.00000058 	 loss = 0.0048(0.1861)
2023/12/03 03:29:42 - INFO - root -   Epoch: [273/300][260/283], lr: 0.00000058 	 loss = 0.0161(0.1856)
2023/12/03 03:29:50 - INFO - root -   Epoch: [273/300][280/283], lr: 0.00000058 	 loss = 0.3476(0.1767)
2023/12/03 03:29:51 - INFO - root -   Epoch: [273/300] 	 loss = 0.1762
2023/12/03 03:29:51 - INFO - root -   train_accuracy = 0.9399
2023/12/03 03:30:06 - INFO - root -   Epoch: [274/300][0/283], lr: 0.00000058 	 loss = 0.0443(0.0443)
2023/12/03 03:30:19 - INFO - root -   Epoch: [274/300][20/283], lr: 0.00000058 	 loss = 0.1005(0.2453)
2023/12/03 03:30:27 - INFO - root -   Epoch: [274/300][40/283], lr: 0.00000058 	 loss = 0.0072(0.1865)
2023/12/03 03:30:49 - INFO - root -   Epoch: [274/300][60/283], lr: 0.00000058 	 loss = 1.8916(0.2978)
2023/12/03 03:31:06 - INFO - root -   Epoch: [274/300][80/283], lr: 0.00000058 	 loss = 0.0227(0.2795)
2023/12/03 03:31:30 - INFO - root -   Epoch: [274/300][100/283], lr: 0.00000058 	 loss = 0.0023(0.2778)
2023/12/03 03:31:42 - INFO - root -   Epoch: [274/300][120/283], lr: 0.00000058 	 loss = 0.6047(0.2954)
2023/12/03 03:32:05 - INFO - root -   Epoch: [274/300][140/283], lr: 0.00000058 	 loss = 0.0860(0.2832)
2023/12/03 03:32:18 - INFO - root -   Epoch: [274/300][160/283], lr: 0.00000058 	 loss = 0.0048(0.2750)
2023/12/03 03:32:35 - INFO - root -   Epoch: [274/300][180/283], lr: 0.00000058 	 loss = 0.1075(0.2536)
2023/12/03 03:32:43 - INFO - root -   Epoch: [274/300][200/283], lr: 0.00000058 	 loss = 0.1442(0.2504)
2023/12/03 03:33:01 - INFO - root -   Epoch: [274/300][220/283], lr: 0.00000058 	 loss = 0.0024(0.2326)
2023/12/03 03:33:18 - INFO - root -   Epoch: [274/300][240/283], lr: 0.00000058 	 loss = 0.2006(0.2392)
2023/12/03 03:33:26 - INFO - root -   Epoch: [274/300][260/283], lr: 0.00000058 	 loss = 0.0102(0.2333)
2023/12/03 03:33:34 - INFO - root -   Epoch: [274/300][280/283], lr: 0.00000058 	 loss = 0.4550(0.2237)
2023/12/03 03:33:35 - INFO - root -   Epoch: [274/300] 	 loss = 0.2222
2023/12/03 03:33:45 - INFO - root -   precision = 0.8462
2023/12/03 03:33:45 - INFO - root -   eval_loss = 0.5735
2023/12/03 03:33:45 - INFO - root -   eval_acc = 0.8462
2023/12/03 03:33:45 - INFO - root -   train_accuracy = 0.9117
2023/12/03 03:33:59 - INFO - root -   Epoch: [275/300][0/283], lr: 0.00000058 	 loss = 0.4370(0.4370)
2023/12/03 03:34:15 - INFO - root -   Epoch: [275/300][20/283], lr: 0.00000058 	 loss = 0.7602(0.3750)
2023/12/03 03:34:33 - INFO - root -   Epoch: [275/300][40/283], lr: 0.00000058 	 loss = 0.0120(0.2825)
2023/12/03 03:34:43 - INFO - root -   Epoch: [275/300][60/283], lr: 0.00000058 	 loss = 3.0501(0.3109)
2023/12/03 03:35:02 - INFO - root -   Epoch: [275/300][80/283], lr: 0.00000058 	 loss = 0.9032(0.2991)
2023/12/03 03:35:10 - INFO - root -   Epoch: [275/300][100/283], lr: 0.00000058 	 loss = 0.0023(0.2724)
2023/12/03 03:35:24 - INFO - root -   Epoch: [275/300][120/283], lr: 0.00000058 	 loss = 0.1527(0.2522)
2023/12/03 03:35:45 - INFO - root -   Epoch: [275/300][140/283], lr: 0.00000058 	 loss = 0.5599(0.2482)
2023/12/03 03:35:56 - INFO - root -   Epoch: [275/300][160/283], lr: 0.00000058 	 loss = 0.8821(0.2400)
2023/12/03 03:36:13 - INFO - root -   Epoch: [275/300][180/283], lr: 0.00000058 	 loss = 0.8227(0.2343)
2023/12/03 03:36:34 - INFO - root -   Epoch: [275/300][200/283], lr: 0.00000058 	 loss = 1.0651(0.2375)
2023/12/03 03:36:47 - INFO - root -   Epoch: [275/300][220/283], lr: 0.00000058 	 loss = 0.1327(0.2279)
2023/12/03 03:37:12 - INFO - root -   Epoch: [275/300][240/283], lr: 0.00000058 	 loss = 0.0035(0.2292)
2023/12/03 03:37:22 - INFO - root -   Epoch: [275/300][260/283], lr: 0.00000058 	 loss = 0.0006(0.2282)
2023/12/03 03:37:30 - INFO - root -   Epoch: [275/300][280/283], lr: 0.00000058 	 loss = 0.0115(0.2245)
2023/12/03 03:37:31 - INFO - root -   Epoch: [275/300] 	 loss = 0.2230
2023/12/03 03:37:31 - INFO - root -   train_accuracy = 0.9117
2023/12/03 03:37:54 - INFO - root -   Epoch: [276/300][0/283], lr: 0.00000058 	 loss = 0.1607(0.1607)
2023/12/03 03:38:01 - INFO - root -   Epoch: [276/300][20/283], lr: 0.00000058 	 loss = 0.0312(0.2546)
2023/12/03 03:38:12 - INFO - root -   Epoch: [276/300][40/283], lr: 0.00000058 	 loss = 0.0022(0.2221)
2023/12/03 03:38:20 - INFO - root -   Epoch: [276/300][60/283], lr: 0.00000058 	 loss = 2.3796(0.2352)
2023/12/03 03:38:41 - INFO - root -   Epoch: [276/300][80/283], lr: 0.00000058 	 loss = 0.0077(0.2696)
2023/12/03 03:38:55 - INFO - root -   Epoch: [276/300][100/283], lr: 0.00000058 	 loss = 0.0063(0.2443)
2023/12/03 03:39:03 - INFO - root -   Epoch: [276/300][120/283], lr: 0.00000058 	 loss = 0.5780(0.2562)
2023/12/03 03:39:29 - INFO - root -   Epoch: [276/300][140/283], lr: 0.00000058 	 loss = 0.1869(0.2428)
2023/12/03 03:39:45 - INFO - root -   Epoch: [276/300][160/283], lr: 0.00000058 	 loss = 0.0021(0.2371)
2023/12/03 03:40:05 - INFO - root -   Epoch: [276/300][180/283], lr: 0.00000058 	 loss = 0.7881(0.2300)
2023/12/03 03:40:19 - INFO - root -   Epoch: [276/300][200/283], lr: 0.00000058 	 loss = 0.0463(0.2360)
2023/12/03 03:40:32 - INFO - root -   Epoch: [276/300][220/283], lr: 0.00000058 	 loss = 0.0012(0.2308)
2023/12/03 03:40:48 - INFO - root -   Epoch: [276/300][240/283], lr: 0.00000058 	 loss = 0.2023(0.2422)
2023/12/03 03:41:02 - INFO - root -   Epoch: [276/300][260/283], lr: 0.00000058 	 loss = 0.0043(0.2433)
2023/12/03 03:41:10 - INFO - root -   Epoch: [276/300][280/283], lr: 0.00000058 	 loss = 0.2106(0.2373)
2023/12/03 03:41:11 - INFO - root -   Epoch: [276/300] 	 loss = 0.2362
2023/12/03 03:41:11 - INFO - root -   train_accuracy = 0.8958
2023/12/03 03:41:27 - INFO - root -   Epoch: [277/300][0/283], lr: 0.00000058 	 loss = 1.8071(1.8071)
2023/12/03 03:41:45 - INFO - root -   Epoch: [277/300][20/283], lr: 0.00000058 	 loss = 0.2267(0.3694)
2023/12/03 03:42:06 - INFO - root -   Epoch: [277/300][40/283], lr: 0.00000058 	 loss = 0.0437(0.2885)
2023/12/03 03:42:18 - INFO - root -   Epoch: [277/300][60/283], lr: 0.00000058 	 loss = 1.2979(0.3278)
2023/12/03 03:42:36 - INFO - root -   Epoch: [277/300][80/283], lr: 0.00000058 	 loss = 0.0371(0.3059)
2023/12/03 03:42:46 - INFO - root -   Epoch: [277/300][100/283], lr: 0.00000058 	 loss = 0.0079(0.2739)
2023/12/03 03:43:15 - INFO - root -   Epoch: [277/300][120/283], lr: 0.00000058 	 loss = 0.2715(0.2747)
2023/12/03 03:43:24 - INFO - root -   Epoch: [277/300][140/283], lr: 0.00000058 	 loss = 0.0015(0.2624)
2023/12/03 03:43:35 - INFO - root -   Epoch: [277/300][160/283], lr: 0.00000058 	 loss = 0.1037(0.2389)
2023/12/03 03:43:56 - INFO - root -   Epoch: [277/300][180/283], lr: 0.00000058 	 loss = 0.0825(0.2295)
2023/12/03 03:44:04 - INFO - root -   Epoch: [277/300][200/283], lr: 0.00000058 	 loss = 0.7054(0.2260)
2023/12/03 03:44:15 - INFO - root -   Epoch: [277/300][220/283], lr: 0.00000058 	 loss = 0.0021(0.2270)
2023/12/03 03:44:36 - INFO - root -   Epoch: [277/300][240/283], lr: 0.00000058 	 loss = 0.1161(0.2357)
2023/12/03 03:44:55 - INFO - root -   Epoch: [277/300][260/283], lr: 0.00000058 	 loss = 0.0009(0.2292)
2023/12/03 03:45:03 - INFO - root -   Epoch: [277/300][280/283], lr: 0.00000058 	 loss = 0.0227(0.2241)
2023/12/03 03:45:04 - INFO - root -   Epoch: [277/300] 	 loss = 0.2230
2023/12/03 03:45:04 - INFO - root -   train_accuracy = 0.9081
2023/12/03 03:45:27 - INFO - root -   Epoch: [278/300][0/283], lr: 0.00000059 	 loss = 1.1251(1.1251)
2023/12/03 03:45:35 - INFO - root -   Epoch: [278/300][20/283], lr: 0.00000059 	 loss = 0.2653(0.2155)
2023/12/03 03:45:55 - INFO - root -   Epoch: [278/300][40/283], lr: 0.00000059 	 loss = 0.0352(0.2375)
2023/12/03 03:46:18 - INFO - root -   Epoch: [278/300][60/283], lr: 0.00000059 	 loss = 0.8700(0.3076)
2023/12/03 03:46:46 - INFO - root -   Epoch: [278/300][80/283], lr: 0.00000059 	 loss = 0.1489(0.3202)
2023/12/03 03:46:54 - INFO - root -   Epoch: [278/300][100/283], lr: 0.00000059 	 loss = 0.0259(0.3075)
2023/12/03 03:47:14 - INFO - root -   Epoch: [278/300][120/283], lr: 0.00000059 	 loss = 0.7868(0.3041)
2023/12/03 03:47:30 - INFO - root -   Epoch: [278/300][140/283], lr: 0.00000059 	 loss = 0.0017(0.2694)
2023/12/03 03:47:44 - INFO - root -   Epoch: [278/300][160/283], lr: 0.00000059 	 loss = 0.9107(0.2492)
2023/12/03 03:47:56 - INFO - root -   Epoch: [278/300][180/283], lr: 0.00000059 	 loss = 0.0419(0.2335)
2023/12/03 03:48:12 - INFO - root -   Epoch: [278/300][200/283], lr: 0.00000059 	 loss = 0.0173(0.2219)
2023/12/03 03:48:23 - INFO - root -   Epoch: [278/300][220/283], lr: 0.00000059 	 loss = 0.1174(0.2121)
2023/12/03 03:48:39 - INFO - root -   Epoch: [278/300][240/283], lr: 0.00000059 	 loss = 0.1278(0.2095)
2023/12/03 03:48:58 - INFO - root -   Epoch: [278/300][260/283], lr: 0.00000059 	 loss = 1.5231(0.2197)
2023/12/03 03:49:06 - INFO - root -   Epoch: [278/300][280/283], lr: 0.00000059 	 loss = 0.0152(0.2144)
2023/12/03 03:49:07 - INFO - root -   Epoch: [278/300] 	 loss = 0.2129
2023/12/03 03:49:07 - INFO - root -   train_accuracy = 0.9187
2023/12/03 03:49:10 - INFO - root -   Epoch: [279/300][0/283], lr: 0.00000059 	 loss = 0.0048(0.0048)
2023/12/03 03:49:34 - INFO - root -   Epoch: [279/300][20/283], lr: 0.00000059 	 loss = 0.2018(0.1554)
2023/12/03 03:49:56 - INFO - root -   Epoch: [279/300][40/283], lr: 0.00000059 	 loss = 0.3083(0.2218)
2023/12/03 03:50:10 - INFO - root -   Epoch: [279/300][60/283], lr: 0.00000059 	 loss = 0.0673(0.2266)
2023/12/03 03:50:22 - INFO - root -   Epoch: [279/300][80/283], lr: 0.00000059 	 loss = 0.0111(0.2205)
2023/12/03 03:50:37 - INFO - root -   Epoch: [279/300][100/283], lr: 0.00000059 	 loss = 0.0007(0.1992)
2023/12/03 03:50:46 - INFO - root -   Epoch: [279/300][120/283], lr: 0.00000059 	 loss = 0.5877(0.2179)
2023/12/03 03:51:12 - INFO - root -   Epoch: [279/300][140/283], lr: 0.00000059 	 loss = 0.0043(0.2050)
2023/12/03 03:51:19 - INFO - root -   Epoch: [279/300][160/283], lr: 0.00000059 	 loss = 0.0607(0.1998)
2023/12/03 03:51:33 - INFO - root -   Epoch: [279/300][180/283], lr: 0.00000059 	 loss = 0.9816(0.1947)
2023/12/03 03:51:41 - INFO - root -   Epoch: [279/300][200/283], lr: 0.00000059 	 loss = 0.2156(0.1893)
2023/12/03 03:51:54 - INFO - root -   Epoch: [279/300][220/283], lr: 0.00000059 	 loss = 0.0015(0.1801)
2023/12/03 03:52:18 - INFO - root -   Epoch: [279/300][240/283], lr: 0.00000059 	 loss = 0.0029(0.1805)
2023/12/03 03:52:36 - INFO - root -   Epoch: [279/300][260/283], lr: 0.00000059 	 loss = 0.0018(0.1794)
2023/12/03 03:52:44 - INFO - root -   Epoch: [279/300][280/283], lr: 0.00000059 	 loss = 1.0081(0.1865)
2023/12/03 03:52:44 - INFO - root -   Epoch: [279/300] 	 loss = 0.1852
2023/12/03 03:52:54 - INFO - root -   precision = 0.8671
2023/12/03 03:52:54 - INFO - root -   eval_loss = 0.5815
2023/12/03 03:52:54 - INFO - root -   eval_acc = 0.8671
2023/12/03 03:52:54 - INFO - root -   train_accuracy = 0.9223
2023/12/03 03:53:09 - INFO - root -   Epoch: [280/300][0/283], lr: 0.00000059 	 loss = 0.7400(0.7400)
2023/12/03 03:53:29 - INFO - root -   Epoch: [280/300][20/283], lr: 0.00000059 	 loss = 0.2851(0.3397)
2023/12/03 03:53:37 - INFO - root -   Epoch: [280/300][40/283], lr: 0.00000059 	 loss = 0.3326(0.3178)
2023/12/03 03:53:49 - INFO - root -   Epoch: [280/300][60/283], lr: 0.00000059 	 loss = 0.1132(0.3331)
2023/12/03 03:54:06 - INFO - root -   Epoch: [280/300][80/283], lr: 0.00000059 	 loss = 0.0218(0.3242)
2023/12/03 03:54:24 - INFO - root -   Epoch: [280/300][100/283], lr: 0.00000059 	 loss = 0.6098(0.2984)
2023/12/03 03:54:34 - INFO - root -   Epoch: [280/300][120/283], lr: 0.00000059 	 loss = 1.7981(0.3072)
2023/12/03 03:54:51 - INFO - root -   Epoch: [280/300][140/283], lr: 0.00000059 	 loss = 0.0005(0.2800)
2023/12/03 03:55:06 - INFO - root -   Epoch: [280/300][160/283], lr: 0.00000059 	 loss = 0.0148(0.2606)
2023/12/03 03:55:26 - INFO - root -   Epoch: [280/300][180/283], lr: 0.00000059 	 loss = 0.1497(0.2435)
2023/12/03 03:55:40 - INFO - root -   Epoch: [280/300][200/283], lr: 0.00000059 	 loss = 0.0525(0.2308)
2023/12/03 03:55:48 - INFO - root -   Epoch: [280/300][220/283], lr: 0.00000059 	 loss = 0.0022(0.2197)
2023/12/03 03:56:10 - INFO - root -   Epoch: [280/300][240/283], lr: 0.00000059 	 loss = 0.1671(0.2183)
2023/12/03 03:56:19 - INFO - root -   Epoch: [280/300][260/283], lr: 0.00000059 	 loss = 0.0091(0.2143)
2023/12/03 03:56:34 - INFO - root -   Epoch: [280/300][280/283], lr: 0.00000059 	 loss = 0.0121(0.2199)
2023/12/03 03:56:35 - INFO - root -   Epoch: [280/300] 	 loss = 0.2199
2023/12/03 03:56:35 - INFO - root -   train_accuracy = 0.9099
2023/12/03 03:56:36 - INFO - root -   Epoch: [281/300][0/283], lr: 0.00000059 	 loss = 0.0114(0.0114)
2023/12/03 03:57:05 - INFO - root -   Epoch: [281/300][20/283], lr: 0.00000059 	 loss = 0.1444(0.3135)
2023/12/03 03:57:19 - INFO - root -   Epoch: [281/300][40/283], lr: 0.00000059 	 loss = 0.0072(0.2876)
2023/12/03 03:57:27 - INFO - root -   Epoch: [281/300][60/283], lr: 0.00000059 	 loss = 0.7658(0.2519)
2023/12/03 03:57:43 - INFO - root -   Epoch: [281/300][80/283], lr: 0.00000059 	 loss = 0.0640(0.2244)
2023/12/03 03:57:58 - INFO - root -   Epoch: [281/300][100/283], lr: 0.00000059 	 loss = 0.3409(0.2070)
2023/12/03 03:58:13 - INFO - root -   Epoch: [281/300][120/283], lr: 0.00000059 	 loss = 0.6638(0.2751)
2023/12/03 03:58:21 - INFO - root -   Epoch: [281/300][140/283], lr: 0.00000059 	 loss = 0.2313(0.2474)
2023/12/03 03:58:34 - INFO - root -   Epoch: [281/300][160/283], lr: 0.00000059 	 loss = 0.0644(0.2295)
2023/12/03 03:58:51 - INFO - root -   Epoch: [281/300][180/283], lr: 0.00000059 	 loss = 0.1577(0.2101)
2023/12/03 03:58:59 - INFO - root -   Epoch: [281/300][200/283], lr: 0.00000059 	 loss = 0.0470(0.1994)
2023/12/03 03:59:10 - INFO - root -   Epoch: [281/300][220/283], lr: 0.00000059 	 loss = 0.0857(0.2028)
2023/12/03 03:59:30 - INFO - root -   Epoch: [281/300][240/283], lr: 0.00000059 	 loss = 0.0376(0.1970)
2023/12/03 03:59:46 - INFO - root -   Epoch: [281/300][260/283], lr: 0.00000059 	 loss = 0.0394(0.1916)
2023/12/03 03:59:54 - INFO - root -   Epoch: [281/300][280/283], lr: 0.00000059 	 loss = 0.0119(0.1915)
2023/12/03 03:59:55 - INFO - root -   Epoch: [281/300] 	 loss = 0.1902
2023/12/03 03:59:55 - INFO - root -   train_accuracy = 0.9311
2023/12/03 03:59:57 - INFO - root -   Epoch: [282/300][0/283], lr: 0.00000059 	 loss = 0.0154(0.0154)
2023/12/03 04:00:35 - INFO - root -   Epoch: [282/300][20/283], lr: 0.00000059 	 loss = 0.1235(0.4461)
2023/12/03 04:00:43 - INFO - root -   Epoch: [282/300][40/283], lr: 0.00000059 	 loss = 0.2555(0.4224)
2023/12/03 04:01:00 - INFO - root -   Epoch: [282/300][60/283], lr: 0.00000059 	 loss = 0.7395(0.3752)
2023/12/03 04:01:10 - INFO - root -   Epoch: [282/300][80/283], lr: 0.00000059 	 loss = 0.0038(0.3121)
2023/12/03 04:01:34 - INFO - root -   Epoch: [282/300][100/283], lr: 0.00000059 	 loss = 0.0045(0.2890)
2023/12/03 04:01:42 - INFO - root -   Epoch: [282/300][120/283], lr: 0.00000059 	 loss = 0.0958(0.2932)
2023/12/03 04:02:01 - INFO - root -   Epoch: [282/300][140/283], lr: 0.00000059 	 loss = 0.2051(0.2656)
2023/12/03 04:02:22 - INFO - root -   Epoch: [282/300][160/283], lr: 0.00000059 	 loss = 0.0138(0.2454)
2023/12/03 04:02:29 - INFO - root -   Epoch: [282/300][180/283], lr: 0.00000059 	 loss = 0.1602(0.2296)
2023/12/03 04:02:47 - INFO - root -   Epoch: [282/300][200/283], lr: 0.00000059 	 loss = 0.0842(0.2277)
2023/12/03 04:02:59 - INFO - root -   Epoch: [282/300][220/283], lr: 0.00000059 	 loss = 0.1637(0.2213)
2023/12/03 04:03:07 - INFO - root -   Epoch: [282/300][240/283], lr: 0.00000059 	 loss = 0.1004(0.2122)
2023/12/03 04:03:20 - INFO - root -   Epoch: [282/300][260/283], lr: 0.00000059 	 loss = 0.0296(0.2064)
2023/12/03 04:03:28 - INFO - root -   Epoch: [282/300][280/283], lr: 0.00000059 	 loss = 0.0241(0.2023)
2023/12/03 04:03:29 - INFO - root -   Epoch: [282/300] 	 loss = 0.2009
2023/12/03 04:03:29 - INFO - root -   train_accuracy = 0.9223
2023/12/03 04:03:31 - INFO - root -   Epoch: [283/300][0/283], lr: 0.00000060 	 loss = 0.0054(0.0054)
2023/12/03 04:04:08 - INFO - root -   Epoch: [283/300][20/283], lr: 0.00000060 	 loss = 0.0123(0.4506)
2023/12/03 04:04:20 - INFO - root -   Epoch: [283/300][40/283], lr: 0.00000060 	 loss = 0.0011(0.4192)
2023/12/03 04:04:38 - INFO - root -   Epoch: [283/300][60/283], lr: 0.00000060 	 loss = 0.5441(0.3418)
2023/12/03 04:05:01 - INFO - root -   Epoch: [283/300][80/283], lr: 0.00000060 	 loss = 0.0604(0.3248)
2023/12/03 04:05:09 - INFO - root -   Epoch: [283/300][100/283], lr: 0.00000060 	 loss = 0.1706(0.3081)
2023/12/03 04:05:24 - INFO - root -   Epoch: [283/300][120/283], lr: 0.00000060 	 loss = 0.0051(0.3032)
2023/12/03 04:05:46 - INFO - root -   Epoch: [283/300][140/283], lr: 0.00000060 	 loss = 0.1075(0.2740)
2023/12/03 04:05:53 - INFO - root -   Epoch: [283/300][160/283], lr: 0.00000060 	 loss = 0.5607(0.2667)
2023/12/03 04:06:11 - INFO - root -   Epoch: [283/300][180/283], lr: 0.00000060 	 loss = 0.6398(0.2474)
2023/12/03 04:06:30 - INFO - root -   Epoch: [283/300][200/283], lr: 0.00000060 	 loss = 0.6467(0.2429)
2023/12/03 04:06:47 - INFO - root -   Epoch: [283/300][220/283], lr: 0.00000060 	 loss = 0.0990(0.2312)
2023/12/03 04:07:03 - INFO - root -   Epoch: [283/300][240/283], lr: 0.00000060 	 loss = 0.0032(0.2298)
2023/12/03 04:07:13 - INFO - root -   Epoch: [283/300][260/283], lr: 0.00000060 	 loss = 0.0015(0.2303)
2023/12/03 04:07:21 - INFO - root -   Epoch: [283/300][280/283], lr: 0.00000060 	 loss = 0.0909(0.2221)
2023/12/03 04:07:22 - INFO - root -   Epoch: [283/300] 	 loss = 0.2235
2023/12/03 04:07:22 - INFO - root -   train_accuracy = 0.9064
2023/12/03 04:07:37 - INFO - root -   Epoch: [284/300][0/283], lr: 0.00000060 	 loss = 0.0421(0.0421)
2023/12/03 04:07:56 - INFO - root -   Epoch: [284/300][20/283], lr: 0.00000060 	 loss = 0.0172(0.2026)
2023/12/03 04:08:15 - INFO - root -   Epoch: [284/300][40/283], lr: 0.00000060 	 loss = 0.0017(0.2855)
2023/12/03 04:08:23 - INFO - root -   Epoch: [284/300][60/283], lr: 0.00000060 	 loss = 0.4136(0.2667)
2023/12/03 04:08:35 - INFO - root -   Epoch: [284/300][80/283], lr: 0.00000060 	 loss = 0.0107(0.2431)
2023/12/03 04:08:58 - INFO - root -   Epoch: [284/300][100/283], lr: 0.00000060 	 loss = 0.1394(0.2313)
2023/12/03 04:09:14 - INFO - root -   Epoch: [284/300][120/283], lr: 0.00000060 	 loss = 0.1280(0.2642)
2023/12/03 04:09:22 - INFO - root -   Epoch: [284/300][140/283], lr: 0.00000060 	 loss = 0.0069(0.2320)
2023/12/03 04:09:51 - INFO - root -   Epoch: [284/300][160/283], lr: 0.00000060 	 loss = 0.9099(0.2368)
2023/12/03 04:10:07 - INFO - root -   Epoch: [284/300][180/283], lr: 0.00000060 	 loss = 0.0137(0.2266)
2023/12/03 04:10:23 - INFO - root -   Epoch: [284/300][200/283], lr: 0.00000060 	 loss = 0.0300(0.2242)
2023/12/03 04:10:39 - INFO - root -   Epoch: [284/300][220/283], lr: 0.00000060 	 loss = 0.0085(0.2106)
2023/12/03 04:10:48 - INFO - root -   Epoch: [284/300][240/283], lr: 0.00000060 	 loss = 0.0011(0.2092)
2023/12/03 04:11:11 - INFO - root -   Epoch: [284/300][260/283], lr: 0.00000060 	 loss = 0.0022(0.2105)
2023/12/03 04:11:19 - INFO - root -   Epoch: [284/300][280/283], lr: 0.00000060 	 loss = 0.5345(0.2081)
2023/12/03 04:11:20 - INFO - root -   Epoch: [284/300] 	 loss = 0.2067
2023/12/03 04:11:30 - INFO - root -   precision = 0.8531
2023/12/03 04:11:30 - INFO - root -   eval_loss = 0.4954
2023/12/03 04:11:30 - INFO - root -   eval_acc = 0.8531
2023/12/03 04:11:30 - INFO - root -   train_accuracy = 0.9152
2023/12/03 04:11:43 - INFO - root -   Epoch: [285/300][0/283], lr: 0.00000060 	 loss = 0.5697(0.5697)
2023/12/03 04:12:06 - INFO - root -   Epoch: [285/300][20/283], lr: 0.00000060 	 loss = 1.5378(0.2910)
2023/12/03 04:12:22 - INFO - root -   Epoch: [285/300][40/283], lr: 0.00000060 	 loss = 0.3985(0.3130)
2023/12/03 04:12:30 - INFO - root -   Epoch: [285/300][60/283], lr: 0.00000060 	 loss = 0.3199(0.2895)
2023/12/03 04:12:46 - INFO - root -   Epoch: [285/300][80/283], lr: 0.00000060 	 loss = 0.0058(0.2800)
2023/12/03 04:13:10 - INFO - root -   Epoch: [285/300][100/283], lr: 0.00000060 	 loss = 0.0224(0.3082)
2023/12/03 04:13:18 - INFO - root -   Epoch: [285/300][120/283], lr: 0.00000060 	 loss = 0.0552(0.3134)
2023/12/03 04:13:26 - INFO - root -   Epoch: [285/300][140/283], lr: 0.00000060 	 loss = 0.0075(0.2837)
2023/12/03 04:13:48 - INFO - root -   Epoch: [285/300][160/283], lr: 0.00000060 	 loss = 0.0033(0.2774)
2023/12/03 04:14:03 - INFO - root -   Epoch: [285/300][180/283], lr: 0.00000060 	 loss = 1.0034(0.2770)
2023/12/03 04:14:11 - INFO - root -   Epoch: [285/300][200/283], lr: 0.00000060 	 loss = 0.2001(0.2580)
2023/12/03 04:14:26 - INFO - root -   Epoch: [285/300][220/283], lr: 0.00000060 	 loss = 0.0885(0.2477)
2023/12/03 04:14:52 - INFO - root -   Epoch: [285/300][240/283], lr: 0.00000060 	 loss = 0.0018(0.2447)
2023/12/03 04:14:59 - INFO - root -   Epoch: [285/300][260/283], lr: 0.00000060 	 loss = 0.1008(0.2414)
2023/12/03 04:15:07 - INFO - root -   Epoch: [285/300][280/283], lr: 0.00000060 	 loss = 0.0429(0.2360)
2023/12/03 04:15:08 - INFO - root -   Epoch: [285/300] 	 loss = 0.2344
2023/12/03 04:15:08 - INFO - root -   train_accuracy = 0.9170
2023/12/03 04:15:23 - INFO - root -   Epoch: [286/300][0/283], lr: 0.00000060 	 loss = 0.5300(0.5300)
2023/12/03 04:15:37 - INFO - root -   Epoch: [286/300][20/283], lr: 0.00000060 	 loss = 0.6240(0.4948)
2023/12/03 04:15:52 - INFO - root -   Epoch: [286/300][40/283], lr: 0.00000060 	 loss = 0.1148(0.3781)
2023/12/03 04:16:11 - INFO - root -   Epoch: [286/300][60/283], lr: 0.00000060 	 loss = 1.8732(0.3377)
2023/12/03 04:16:29 - INFO - root -   Epoch: [286/300][80/283], lr: 0.00000060 	 loss = 0.1701(0.3102)
2023/12/03 04:16:51 - INFO - root -   Epoch: [286/300][100/283], lr: 0.00000060 	 loss = 0.1921(0.2847)
2023/12/03 04:17:15 - INFO - root -   Epoch: [286/300][120/283], lr: 0.00000060 	 loss = 0.1604(0.2704)
2023/12/03 04:17:23 - INFO - root -   Epoch: [286/300][140/283], lr: 0.00000060 	 loss = 0.0063(0.2624)
2023/12/03 04:17:31 - INFO - root -   Epoch: [286/300][160/283], lr: 0.00000060 	 loss = 0.0036(0.2358)
2023/12/03 04:17:58 - INFO - root -   Epoch: [286/300][180/283], lr: 0.00000060 	 loss = 0.0379(0.2238)
2023/12/03 04:18:07 - INFO - root -   Epoch: [286/300][200/283], lr: 0.00000060 	 loss = 0.0267(0.2221)
2023/12/03 04:18:18 - INFO - root -   Epoch: [286/300][220/283], lr: 0.00000060 	 loss = 0.0008(0.2127)
2023/12/03 04:18:30 - INFO - root -   Epoch: [286/300][240/283], lr: 0.00000060 	 loss = 0.0101(0.2097)
2023/12/03 04:18:42 - INFO - root -   Epoch: [286/300][260/283], lr: 0.00000060 	 loss = 0.0030(0.2059)
2023/12/03 04:18:50 - INFO - root -   Epoch: [286/300][280/283], lr: 0.00000060 	 loss = 0.0537(0.1969)
2023/12/03 04:18:51 - INFO - root -   Epoch: [286/300] 	 loss = 0.1957
2023/12/03 04:18:51 - INFO - root -   train_accuracy = 0.9276
2023/12/03 04:19:18 - INFO - root -   Epoch: [287/300][0/283], lr: 0.00000060 	 loss = 0.1020(0.1020)
2023/12/03 04:19:28 - INFO - root -   Epoch: [287/300][20/283], lr: 0.00000060 	 loss = 0.1198(0.2454)
2023/12/03 04:19:46 - INFO - root -   Epoch: [287/300][40/283], lr: 0.00000060 	 loss = 0.2597(0.3113)
2023/12/03 04:19:58 - INFO - root -   Epoch: [287/300][60/283], lr: 0.00000060 	 loss = 1.3404(0.3394)
2023/12/03 04:20:06 - INFO - root -   Epoch: [287/300][80/283], lr: 0.00000060 	 loss = 0.8037(0.3067)
2023/12/03 04:20:28 - INFO - root -   Epoch: [287/300][100/283], lr: 0.00000060 	 loss = 0.0528(0.2730)
2023/12/03 04:20:37 - INFO - root -   Epoch: [287/300][120/283], lr: 0.00000060 	 loss = 0.1053(0.2715)
2023/12/03 04:20:45 - INFO - root -   Epoch: [287/300][140/283], lr: 0.00000060 	 loss = 0.0891(0.2583)
2023/12/03 04:21:11 - INFO - root -   Epoch: [287/300][160/283], lr: 0.00000060 	 loss = 0.0466(0.2371)
2023/12/03 04:21:24 - INFO - root -   Epoch: [287/300][180/283], lr: 0.00000060 	 loss = 0.2286(0.2241)
2023/12/03 04:21:32 - INFO - root -   Epoch: [287/300][200/283], lr: 0.00000060 	 loss = 0.0630(0.2202)
2023/12/03 04:21:51 - INFO - root -   Epoch: [287/300][220/283], lr: 0.00000060 	 loss = 0.0017(0.2107)
2023/12/03 04:22:03 - INFO - root -   Epoch: [287/300][240/283], lr: 0.00000060 	 loss = 0.1096(0.2119)
2023/12/03 04:22:15 - INFO - root -   Epoch: [287/300][260/283], lr: 0.00000060 	 loss = 0.0478(0.2047)
2023/12/03 04:22:23 - INFO - root -   Epoch: [287/300][280/283], lr: 0.00000060 	 loss = 0.0060(0.2016)
2023/12/03 04:22:24 - INFO - root -   Epoch: [287/300] 	 loss = 0.2006
2023/12/03 04:22:24 - INFO - root -   train_accuracy = 0.9276
2023/12/03 04:22:26 - INFO - root -   Epoch: [288/300][0/283], lr: 0.00000060 	 loss = 0.0094(0.0094)
2023/12/03 04:22:52 - INFO - root -   Epoch: [288/300][20/283], lr: 0.00000060 	 loss = 0.0201(0.2360)
2023/12/03 04:23:00 - INFO - root -   Epoch: [288/300][40/283], lr: 0.00000060 	 loss = 0.1238(0.1887)
2023/12/03 04:23:19 - INFO - root -   Epoch: [288/300][60/283], lr: 0.00000060 	 loss = 0.0390(0.2433)
2023/12/03 04:23:48 - INFO - root -   Epoch: [288/300][80/283], lr: 0.00000060 	 loss = 0.9496(0.2666)
2023/12/03 04:24:00 - INFO - root -   Epoch: [288/300][100/283], lr: 0.00000060 	 loss = 0.0921(0.2619)
2023/12/03 04:24:15 - INFO - root -   Epoch: [288/300][120/283], lr: 0.00000060 	 loss = 0.6253(0.2769)
2023/12/03 04:24:34 - INFO - root -   Epoch: [288/300][140/283], lr: 0.00000060 	 loss = 0.0079(0.2502)
2023/12/03 04:24:42 - INFO - root -   Epoch: [288/300][160/283], lr: 0.00000060 	 loss = 0.0023(0.2344)
2023/12/03 04:25:01 - INFO - root -   Epoch: [288/300][180/283], lr: 0.00000060 	 loss = 0.1620(0.2275)
2023/12/03 04:25:17 - INFO - root -   Epoch: [288/300][200/283], lr: 0.00000060 	 loss = 0.0886(0.2201)
2023/12/03 04:25:25 - INFO - root -   Epoch: [288/300][220/283], lr: 0.00000060 	 loss = 0.0609(0.2136)
2023/12/03 04:25:33 - INFO - root -   Epoch: [288/300][240/283], lr: 0.00000060 	 loss = 0.0052(0.2093)
2023/12/03 04:25:50 - INFO - root -   Epoch: [288/300][260/283], lr: 0.00000060 	 loss = 0.0003(0.1988)
2023/12/03 04:25:58 - INFO - root -   Epoch: [288/300][280/283], lr: 0.00000060 	 loss = 0.0008(0.2049)
2023/12/03 04:25:59 - INFO - root -   Epoch: [288/300] 	 loss = 0.2039
2023/12/03 04:25:59 - INFO - root -   train_accuracy = 0.9223
2023/12/03 04:26:11 - INFO - root -   Epoch: [289/300][0/283], lr: 0.00000061 	 loss = 0.8204(0.8204)
2023/12/03 04:26:30 - INFO - root -   Epoch: [289/300][20/283], lr: 0.00000061 	 loss = 0.3282(0.2756)
2023/12/03 04:26:38 - INFO - root -   Epoch: [289/300][40/283], lr: 0.00000061 	 loss = 0.0904(0.2406)
2023/12/03 04:27:01 - INFO - root -   Epoch: [289/300][60/283], lr: 0.00000061 	 loss = 0.0373(0.2350)
2023/12/03 04:27:09 - INFO - root -   Epoch: [289/300][80/283], lr: 0.00000061 	 loss = 0.1513(0.2372)
2023/12/03 04:27:22 - INFO - root -   Epoch: [289/300][100/283], lr: 0.00000061 	 loss = 0.0013(0.2227)
2023/12/03 04:27:37 - INFO - root -   Epoch: [289/300][120/283], lr: 0.00000061 	 loss = 0.0090(0.2263)
2023/12/03 04:27:45 - INFO - root -   Epoch: [289/300][140/283], lr: 0.00000061 	 loss = 0.0022(0.2182)
2023/12/03 04:27:58 - INFO - root -   Epoch: [289/300][160/283], lr: 0.00000061 	 loss = 0.0053(0.2031)
2023/12/03 04:28:12 - INFO - root -   Epoch: [289/300][180/283], lr: 0.00000061 	 loss = 0.0973(0.1934)
2023/12/03 04:28:26 - INFO - root -   Epoch: [289/300][200/283], lr: 0.00000061 	 loss = 0.8865(0.1975)
2023/12/03 04:28:48 - INFO - root -   Epoch: [289/300][220/283], lr: 0.00000061 	 loss = 0.1490(0.1969)
2023/12/03 04:29:06 - INFO - root -   Epoch: [289/300][240/283], lr: 0.00000061 	 loss = 0.4177(0.2009)
2023/12/03 04:29:19 - INFO - root -   Epoch: [289/300][260/283], lr: 0.00000061 	 loss = 0.0938(0.1943)
2023/12/03 04:29:28 - INFO - root -   Epoch: [289/300][280/283], lr: 0.00000061 	 loss = 0.5535(0.2001)
2023/12/03 04:29:28 - INFO - root -   Epoch: [289/300] 	 loss = 0.1996
2023/12/03 04:29:38 - INFO - root -   precision = 0.8462
2023/12/03 04:29:38 - INFO - root -   eval_loss = 0.6018
2023/12/03 04:29:38 - INFO - root -   eval_acc = 0.8462
2023/12/03 04:29:38 - INFO - root -   train_accuracy = 0.9099
2023/12/03 04:29:41 - INFO - root -   Epoch: [290/300][0/283], lr: 0.00000061 	 loss = 0.0119(0.0119)
2023/12/03 04:30:09 - INFO - root -   Epoch: [290/300][20/283], lr: 0.00000061 	 loss = 0.3237(0.2893)
2023/12/03 04:30:28 - INFO - root -   Epoch: [290/300][40/283], lr: 0.00000061 	 loss = 0.0039(0.2855)
2023/12/03 04:30:41 - INFO - root -   Epoch: [290/300][60/283], lr: 0.00000061 	 loss = 0.1247(0.2717)
2023/12/03 04:30:57 - INFO - root -   Epoch: [290/300][80/283], lr: 0.00000061 	 loss = 0.0780(0.2509)
2023/12/03 04:31:12 - INFO - root -   Epoch: [290/300][100/283], lr: 0.00000061 	 loss = 0.0042(0.2448)
2023/12/03 04:31:20 - INFO - root -   Epoch: [290/300][120/283], lr: 0.00000061 	 loss = 0.0083(0.2889)
2023/12/03 04:31:40 - INFO - root -   Epoch: [290/300][140/283], lr: 0.00000061 	 loss = 0.3345(0.2943)
2023/12/03 04:31:48 - INFO - root -   Epoch: [290/300][160/283], lr: 0.00000061 	 loss = 0.0186(0.2802)
2023/12/03 04:32:07 - INFO - root -   Epoch: [290/300][180/283], lr: 0.00000061 	 loss = 0.1028(0.2632)
2023/12/03 04:32:21 - INFO - root -   Epoch: [290/300][200/283], lr: 0.00000061 	 loss = 0.0347(0.2690)
2023/12/03 04:32:29 - INFO - root -   Epoch: [290/300][220/283], lr: 0.00000061 	 loss = 0.0004(0.2591)
2023/12/03 04:32:37 - INFO - root -   Epoch: [290/300][240/283], lr: 0.00000061 	 loss = 0.0387(0.2513)
2023/12/03 04:32:50 - INFO - root -   Epoch: [290/300][260/283], lr: 0.00000061 	 loss = 0.0021(0.2412)
2023/12/03 04:32:58 - INFO - root -   Epoch: [290/300][280/283], lr: 0.00000061 	 loss = 0.0082(0.2316)
2023/12/03 04:32:58 - INFO - root -   Epoch: [290/300] 	 loss = 0.2302
2023/12/03 04:32:58 - INFO - root -   train_accuracy = 0.9046
2023/12/03 04:33:00 - INFO - root -   Epoch: [291/300][0/283], lr: 0.00000061 	 loss = 0.0139(0.0139)
2023/12/03 04:33:23 - INFO - root -   Epoch: [291/300][20/283], lr: 0.00000061 	 loss = 1.8159(0.2449)
2023/12/03 04:33:33 - INFO - root -   Epoch: [291/300][40/283], lr: 0.00000061 	 loss = 0.0015(0.2248)
2023/12/03 04:34:05 - INFO - root -   Epoch: [291/300][60/283], lr: 0.00000061 	 loss = 1.3192(0.2807)
2023/12/03 04:34:13 - INFO - root -   Epoch: [291/300][80/283], lr: 0.00000061 	 loss = 0.2908(0.2933)
2023/12/03 04:34:25 - INFO - root -   Epoch: [291/300][100/283], lr: 0.00000061 	 loss = 0.0226(0.2558)
2023/12/03 04:34:44 - INFO - root -   Epoch: [291/300][120/283], lr: 0.00000061 	 loss = 0.0134(0.2499)
2023/12/03 04:34:53 - INFO - root -   Epoch: [291/300][140/283], lr: 0.00000061 	 loss = 0.0069(0.2351)
2023/12/03 04:35:09 - INFO - root -   Epoch: [291/300][160/283], lr: 0.00000061 	 loss = 0.0084(0.2198)
2023/12/03 04:35:20 - INFO - root -   Epoch: [291/300][180/283], lr: 0.00000061 	 loss = 0.0605(0.2087)
2023/12/03 04:35:34 - INFO - root -   Epoch: [291/300][200/283], lr: 0.00000061 	 loss = 0.0064(0.2001)
2023/12/03 04:35:49 - INFO - root -   Epoch: [291/300][220/283], lr: 0.00000061 	 loss = 0.0108(0.2020)
2023/12/03 04:36:06 - INFO - root -   Epoch: [291/300][240/283], lr: 0.00000061 	 loss = 0.0024(0.1924)
2023/12/03 04:36:21 - INFO - root -   Epoch: [291/300][260/283], lr: 0.00000061 	 loss = 0.0004(0.1921)
2023/12/03 04:36:30 - INFO - root -   Epoch: [291/300][280/283], lr: 0.00000061 	 loss = 0.0068(0.1923)
2023/12/03 04:36:31 - INFO - root -   Epoch: [291/300] 	 loss = 0.1914
2023/12/03 04:36:31 - INFO - root -   train_accuracy = 0.9152
2023/12/03 04:36:45 - INFO - root -   Epoch: [292/300][0/283], lr: 0.00000061 	 loss = 0.2089(0.2089)
2023/12/03 04:37:12 - INFO - root -   Epoch: [292/300][20/283], lr: 0.00000061 	 loss = 0.2243(0.1736)
2023/12/03 04:37:20 - INFO - root -   Epoch: [292/300][40/283], lr: 0.00000061 	 loss = 0.2631(0.2224)
2023/12/03 04:37:42 - INFO - root -   Epoch: [292/300][60/283], lr: 0.00000061 	 loss = 0.7375(0.2522)
2023/12/03 04:37:58 - INFO - root -   Epoch: [292/300][80/283], lr: 0.00000061 	 loss = 0.0347(0.2505)
2023/12/03 04:38:06 - INFO - root -   Epoch: [292/300][100/283], lr: 0.00000061 	 loss = 0.0024(0.2363)
2023/12/03 04:38:18 - INFO - root -   Epoch: [292/300][120/283], lr: 0.00000061 	 loss = 0.2291(0.2657)
2023/12/03 04:38:30 - INFO - root -   Epoch: [292/300][140/283], lr: 0.00000061 	 loss = 0.0047(0.2409)
2023/12/03 04:38:54 - INFO - root -   Epoch: [292/300][160/283], lr: 0.00000061 	 loss = 0.0097(0.2312)
2023/12/03 04:39:02 - INFO - root -   Epoch: [292/300][180/283], lr: 0.00000061 	 loss = 0.0195(0.2391)
2023/12/03 04:39:14 - INFO - root -   Epoch: [292/300][200/283], lr: 0.00000061 	 loss = 0.0926(0.2338)
2023/12/03 04:39:22 - INFO - root -   Epoch: [292/300][220/283], lr: 0.00000061 	 loss = 0.0143(0.2383)
2023/12/03 04:39:30 - INFO - root -   Epoch: [292/300][240/283], lr: 0.00000061 	 loss = 0.2250(0.2394)
2023/12/03 04:39:44 - INFO - root -   Epoch: [292/300][260/283], lr: 0.00000061 	 loss = 0.0035(0.2272)
2023/12/03 04:39:55 - INFO - root -   Epoch: [292/300][280/283], lr: 0.00000061 	 loss = 0.0051(0.2190)
2023/12/03 04:39:55 - INFO - root -   Epoch: [292/300] 	 loss = 0.2185
2023/12/03 04:39:55 - INFO - root -   train_accuracy = 0.9064
2023/12/03 04:39:57 - INFO - root -   Epoch: [293/300][0/283], lr: 0.00000061 	 loss = 0.0855(0.0855)
2023/12/03 04:40:21 - INFO - root -   Epoch: [293/300][20/283], lr: 0.00000061 	 loss = 0.2244(0.1802)
2023/12/03 04:40:34 - INFO - root -   Epoch: [293/300][40/283], lr: 0.00000061 	 loss = 0.1958(0.2521)
2023/12/03 04:40:51 - INFO - root -   Epoch: [293/300][60/283], lr: 0.00000061 	 loss = 0.0125(0.2364)
2023/12/03 04:41:00 - INFO - root -   Epoch: [293/300][80/283], lr: 0.00000061 	 loss = 0.1415(0.2408)
2023/12/03 04:41:15 - INFO - root -   Epoch: [293/300][100/283], lr: 0.00000061 	 loss = 0.0071(0.2253)
2023/12/03 04:41:40 - INFO - root -   Epoch: [293/300][120/283], lr: 0.00000061 	 loss = 0.0139(0.2247)
2023/12/03 04:41:55 - INFO - root -   Epoch: [293/300][140/283], lr: 0.00000061 	 loss = 0.0030(0.2099)
2023/12/03 04:42:19 - INFO - root -   Epoch: [293/300][160/283], lr: 0.00000061 	 loss = 0.1589(0.2109)
2023/12/03 04:42:31 - INFO - root -   Epoch: [293/300][180/283], lr: 0.00000061 	 loss = 0.0870(0.2110)
2023/12/03 04:42:47 - INFO - root -   Epoch: [293/300][200/283], lr: 0.00000061 	 loss = 0.0136(0.2059)
2023/12/03 04:43:10 - INFO - root -   Epoch: [293/300][220/283], lr: 0.00000061 	 loss = 0.0015(0.1991)
2023/12/03 04:43:18 - INFO - root -   Epoch: [293/300][240/283], lr: 0.00000061 	 loss = 0.1158(0.1990)
2023/12/03 04:43:36 - INFO - root -   Epoch: [293/300][260/283], lr: 0.00000061 	 loss = 0.0522(0.2054)
2023/12/03 04:43:44 - INFO - root -   Epoch: [293/300][280/283], lr: 0.00000061 	 loss = 0.0336(0.2047)
2023/12/03 04:43:45 - INFO - root -   Epoch: [293/300] 	 loss = 0.2041
2023/12/03 04:43:45 - INFO - root -   train_accuracy = 0.9170
2023/12/03 04:43:46 - INFO - root -   Epoch: [294/300][0/283], lr: 0.00000061 	 loss = 0.0639(0.0639)
2023/12/03 04:44:15 - INFO - root -   Epoch: [294/300][20/283], lr: 0.00000061 	 loss = 0.0475(0.4958)
2023/12/03 04:44:30 - INFO - root -   Epoch: [294/300][40/283], lr: 0.00000061 	 loss = 0.0302(0.4135)
2023/12/03 04:44:50 - INFO - root -   Epoch: [294/300][60/283], lr: 0.00000061 	 loss = 1.5712(0.3879)
2023/12/03 04:45:03 - INFO - root -   Epoch: [294/300][80/283], lr: 0.00000061 	 loss = 0.0094(0.3628)
2023/12/03 04:45:22 - INFO - root -   Epoch: [294/300][100/283], lr: 0.00000061 	 loss = 0.1690(0.3126)
2023/12/03 04:45:30 - INFO - root -   Epoch: [294/300][120/283], lr: 0.00000061 	 loss = 0.9172(0.2954)
2023/12/03 04:45:39 - INFO - root -   Epoch: [294/300][140/283], lr: 0.00000061 	 loss = 0.0020(0.2659)
2023/12/03 04:46:00 - INFO - root -   Epoch: [294/300][160/283], lr: 0.00000061 	 loss = 0.0435(0.2574)
2023/12/03 04:46:18 - INFO - root -   Epoch: [294/300][180/283], lr: 0.00000061 	 loss = 0.0441(0.2475)
2023/12/03 04:46:32 - INFO - root -   Epoch: [294/300][200/283], lr: 0.00000061 	 loss = 0.1102(0.2334)
2023/12/03 04:46:53 - INFO - root -   Epoch: [294/300][220/283], lr: 0.00000061 	 loss = 0.0006(0.2322)
2023/12/03 04:47:05 - INFO - root -   Epoch: [294/300][240/283], lr: 0.00000061 	 loss = 0.0068(0.2280)
2023/12/03 04:47:21 - INFO - root -   Epoch: [294/300][260/283], lr: 0.00000061 	 loss = 0.0022(0.2293)
2023/12/03 04:47:33 - INFO - root -   Epoch: [294/300][280/283], lr: 0.00000061 	 loss = 0.0015(0.2328)
2023/12/03 04:47:34 - INFO - root -   Epoch: [294/300] 	 loss = 0.2323
2023/12/03 04:47:43 - INFO - root -   precision = 0.8322
2023/12/03 04:47:43 - INFO - root -   eval_loss = 0.5384
2023/12/03 04:47:43 - INFO - root -   eval_acc = 0.8322
2023/12/03 04:47:43 - INFO - root -   train_accuracy = 0.8993
2023/12/03 04:47:56 - INFO - root -   Epoch: [295/300][0/283], lr: 0.00000062 	 loss = 0.0859(0.0859)
2023/12/03 04:48:10 - INFO - root -   Epoch: [295/300][20/283], lr: 0.00000062 	 loss = 0.0950(0.2274)
2023/12/03 04:48:32 - INFO - root -   Epoch: [295/300][40/283], lr: 0.00000062 	 loss = 0.0093(0.2625)
2023/12/03 04:48:50 - INFO - root -   Epoch: [295/300][60/283], lr: 0.00000062 	 loss = 0.4030(0.3265)
2023/12/03 04:49:00 - INFO - root -   Epoch: [295/300][80/283], lr: 0.00000062 	 loss = 0.1955(0.2998)
2023/12/03 04:49:14 - INFO - root -   Epoch: [295/300][100/283], lr: 0.00000062 	 loss = 0.0162(0.2604)
2023/12/03 04:49:31 - INFO - root -   Epoch: [295/300][120/283], lr: 0.00000062 	 loss = 1.2447(0.2746)
2023/12/03 04:49:40 - INFO - root -   Epoch: [295/300][140/283], lr: 0.00000062 	 loss = 0.1501(0.2554)
2023/12/03 04:49:48 - INFO - root -   Epoch: [295/300][160/283], lr: 0.00000062 	 loss = 0.2115(0.2422)
2023/12/03 04:50:07 - INFO - root -   Epoch: [295/300][180/283], lr: 0.00000062 	 loss = 0.1282(0.2301)
2023/12/03 04:50:15 - INFO - root -   Epoch: [295/300][200/283], lr: 0.00000062 	 loss = 0.0323(0.2138)
2023/12/03 04:50:35 - INFO - root -   Epoch: [295/300][220/283], lr: 0.00000062 	 loss = 0.0013(0.2128)
2023/12/03 04:50:56 - INFO - root -   Epoch: [295/300][240/283], lr: 0.00000062 	 loss = 0.0022(0.2095)
2023/12/03 04:51:16 - INFO - root -   Epoch: [295/300][260/283], lr: 0.00000062 	 loss = 0.1889(0.2116)
2023/12/03 04:51:24 - INFO - root -   Epoch: [295/300][280/283], lr: 0.00000062 	 loss = 0.7553(0.2113)
2023/12/03 04:51:25 - INFO - root -   Epoch: [295/300] 	 loss = 0.2099
2023/12/03 04:51:25 - INFO - root -   train_accuracy = 0.8975
2023/12/03 04:51:29 - INFO - root -   Epoch: [296/300][0/283], lr: 0.00000062 	 loss = 0.0053(0.0053)
2023/12/03 04:51:59 - INFO - root -   Epoch: [296/300][20/283], lr: 0.00000062 	 loss = 0.8223(0.3401)
2023/12/03 04:52:10 - INFO - root -   Epoch: [296/300][40/283], lr: 0.00000062 	 loss = 0.0546(0.3277)
2023/12/03 04:52:25 - INFO - root -   Epoch: [296/300][60/283], lr: 0.00000062 	 loss = 2.1439(0.3136)
2023/12/03 04:52:41 - INFO - root -   Epoch: [296/300][80/283], lr: 0.00000062 	 loss = 0.0067(0.2955)
2023/12/03 04:52:49 - INFO - root -   Epoch: [296/300][100/283], lr: 0.00000062 	 loss = 0.1010(0.2681)
2023/12/03 04:53:11 - INFO - root -   Epoch: [296/300][120/283], lr: 0.00000062 	 loss = 0.6824(0.2727)
2023/12/03 04:53:19 - INFO - root -   Epoch: [296/300][140/283], lr: 0.00000062 	 loss = 0.0356(0.2527)
2023/12/03 04:53:40 - INFO - root -   Epoch: [296/300][160/283], lr: 0.00000062 	 loss = 0.0862(0.2583)
2023/12/03 04:53:56 - INFO - root -   Epoch: [296/300][180/283], lr: 0.00000062 	 loss = 0.2999(0.2482)
2023/12/03 04:54:04 - INFO - root -   Epoch: [296/300][200/283], lr: 0.00000062 	 loss = 0.0477(0.2393)
2023/12/03 04:54:20 - INFO - root -   Epoch: [296/300][220/283], lr: 0.00000062 	 loss = 0.0604(0.2391)
2023/12/03 04:54:44 - INFO - root -   Epoch: [296/300][240/283], lr: 0.00000062 	 loss = 0.0098(0.2383)
2023/12/03 04:55:00 - INFO - root -   Epoch: [296/300][260/283], lr: 0.00000062 	 loss = 0.0836(0.2354)
2023/12/03 04:55:08 - INFO - root -   Epoch: [296/300][280/283], lr: 0.00000062 	 loss = 0.9297(0.2292)
2023/12/03 04:55:09 - INFO - root -   Epoch: [296/300] 	 loss = 0.2279
2023/12/03 04:55:09 - INFO - root -   train_accuracy = 0.9081
2023/12/03 04:55:10 - INFO - root -   Epoch: [297/300][0/283], lr: 0.00000062 	 loss = 0.0067(0.0067)
2023/12/03 04:55:26 - INFO - root -   Epoch: [297/300][20/283], lr: 0.00000062 	 loss = 0.1361(0.1722)
2023/12/03 04:55:34 - INFO - root -   Epoch: [297/300][40/283], lr: 0.00000062 	 loss = 0.0328(0.1217)
2023/12/03 04:55:51 - INFO - root -   Epoch: [297/300][60/283], lr: 0.00000062 	 loss = 1.3861(0.2015)
2023/12/03 04:56:22 - INFO - root -   Epoch: [297/300][80/283], lr: 0.00000062 	 loss = 0.8920(0.2239)
2023/12/03 04:56:30 - INFO - root -   Epoch: [297/300][100/283], lr: 0.00000062 	 loss = 0.0838(0.2308)
2023/12/03 04:56:53 - INFO - root -   Epoch: [297/300][120/283], lr: 0.00000062 	 loss = 0.0097(0.2507)
2023/12/03 04:57:08 - INFO - root -   Epoch: [297/300][140/283], lr: 0.00000062 	 loss = 0.0052(0.2558)
2023/12/03 04:57:19 - INFO - root -   Epoch: [297/300][160/283], lr: 0.00000062 	 loss = 0.5851(0.2442)
2023/12/03 04:57:37 - INFO - root -   Epoch: [297/300][180/283], lr: 0.00000062 	 loss = 0.0862(0.2338)
2023/12/03 04:57:45 - INFO - root -   Epoch: [297/300][200/283], lr: 0.00000062 	 loss = 0.0631(0.2284)
2023/12/03 04:58:03 - INFO - root -   Epoch: [297/300][220/283], lr: 0.00000062 	 loss = 0.0035(0.2212)
2023/12/03 04:58:28 - INFO - root -   Epoch: [297/300][240/283], lr: 0.00000062 	 loss = 0.0406(0.2199)
2023/12/03 04:58:36 - INFO - root -   Epoch: [297/300][260/283], lr: 0.00000062 	 loss = 0.1586(0.2152)
2023/12/03 04:58:44 - INFO - root -   Epoch: [297/300][280/283], lr: 0.00000062 	 loss = 0.0026(0.2090)
2023/12/03 04:58:45 - INFO - root -   Epoch: [297/300] 	 loss = 0.2077
2023/12/03 04:58:45 - INFO - root -   train_accuracy = 0.9170
2023/12/03 04:58:47 - INFO - root -   Epoch: [298/300][0/283], lr: 0.00000062 	 loss = 0.1684(0.1684)
2023/12/03 04:59:21 - INFO - root -   Epoch: [298/300][20/283], lr: 0.00000062 	 loss = 0.1515(0.3304)
2023/12/03 04:59:35 - INFO - root -   Epoch: [298/300][40/283], lr: 0.00000062 	 loss = 0.5939(0.3130)
2023/12/03 04:59:55 - INFO - root -   Epoch: [298/300][60/283], lr: 0.00000062 	 loss = 0.0989(0.3005)
2023/12/03 05:00:13 - INFO - root -   Epoch: [298/300][80/283], lr: 0.00000062 	 loss = 0.5222(0.2790)
2023/12/03 05:00:21 - INFO - root -   Epoch: [298/300][100/283], lr: 0.00000062 	 loss = 0.1229(0.2630)
2023/12/03 05:00:40 - INFO - root -   Epoch: [298/300][120/283], lr: 0.00000062 	 loss = 0.3427(0.2797)
2023/12/03 05:00:49 - INFO - root -   Epoch: [298/300][140/283], lr: 0.00000062 	 loss = 0.0174(0.2662)
2023/12/03 05:01:15 - INFO - root -   Epoch: [298/300][160/283], lr: 0.00000062 	 loss = 1.2363(0.2618)
2023/12/03 05:01:23 - INFO - root -   Epoch: [298/300][180/283], lr: 0.00000062 	 loss = 0.1897(0.2498)
2023/12/03 05:01:37 - INFO - root -   Epoch: [298/300][200/283], lr: 0.00000062 	 loss = 0.0891(0.2369)
2023/12/03 05:01:45 - INFO - root -   Epoch: [298/300][220/283], lr: 0.00000062 	 loss = 0.1032(0.2325)
2023/12/03 05:02:15 - INFO - root -   Epoch: [298/300][240/283], lr: 0.00000062 	 loss = 0.1088(0.2262)
2023/12/03 05:02:27 - INFO - root -   Epoch: [298/300][260/283], lr: 0.00000062 	 loss = 0.1302(0.2238)
2023/12/03 05:02:35 - INFO - root -   Epoch: [298/300][280/283], lr: 0.00000062 	 loss = 1.3286(0.2265)
2023/12/03 05:02:36 - INFO - root -   Epoch: [298/300] 	 loss = 0.2250
2023/12/03 05:02:36 - INFO - root -   train_accuracy = 0.9117
2023/12/03 05:02:39 - INFO - root -   Epoch: [299/300][0/283], lr: 0.00000062 	 loss = 0.0035(0.0035)
2023/12/03 05:03:05 - INFO - root -   Epoch: [299/300][20/283], lr: 0.00000062 	 loss = 0.2256(0.2460)
2023/12/03 05:03:14 - INFO - root -   Epoch: [299/300][40/283], lr: 0.00000062 	 loss = 0.0117(0.2481)
2023/12/03 05:03:35 - INFO - root -   Epoch: [299/300][60/283], lr: 0.00000062 	 loss = 0.8892(0.2668)
2023/12/03 05:03:52 - INFO - root -   Epoch: [299/300][80/283], lr: 0.00000062 	 loss = 0.0093(0.2400)
2023/12/03 05:04:09 - INFO - root -   Epoch: [299/300][100/283], lr: 0.00000062 	 loss = 0.3067(0.2274)
2023/12/03 05:04:23 - INFO - root -   Epoch: [299/300][120/283], lr: 0.00000062 	 loss = 0.4594(0.2139)
2023/12/03 05:04:41 - INFO - root -   Epoch: [299/300][140/283], lr: 0.00000062 	 loss = 0.0097(0.1978)
2023/12/03 05:04:59 - INFO - root -   Epoch: [299/300][160/283], lr: 0.00000062 	 loss = 0.0051(0.1890)
2023/12/03 05:05:09 - INFO - root -   Epoch: [299/300][180/283], lr: 0.00000062 	 loss = 0.0263(0.1804)
2023/12/03 05:05:17 - INFO - root -   Epoch: [299/300][200/283], lr: 0.00000062 	 loss = 0.0042(0.1793)
2023/12/03 05:05:30 - INFO - root -   Epoch: [299/300][220/283], lr: 0.00000062 	 loss = 0.0786(0.1750)
2023/12/03 05:05:53 - INFO - root -   Epoch: [299/300][240/283], lr: 0.00000062 	 loss = 0.0101(0.1760)
2023/12/03 05:06:01 - INFO - root -   Epoch: [299/300][260/283], lr: 0.00000062 	 loss = 0.0007(0.1733)
2023/12/03 05:06:10 - INFO - root -   Epoch: [299/300][280/283], lr: 0.00000062 	 loss = 0.3020(0.1707)
2023/12/03 05:06:11 - INFO - root -   Epoch: [299/300] 	 loss = 0.1699
2023/12/03 05:06:20 - INFO - root -   precision = 0.8531
2023/12/03 05:06:20 - INFO - root -   eval_loss = 0.6231
2023/12/03 05:06:20 - INFO - root -   eval_acc = 0.8531
2023/12/03 05:06:21 - INFO - root -   train_accuracy = 0.9258
