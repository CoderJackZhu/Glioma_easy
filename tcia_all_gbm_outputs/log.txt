2023/10/23 15:27:37 - INFO - root -   Num train examples = 169
2023/10/23 15:27:37 - INFO - root -   Num val examples = 43
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/10/23 15:27:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/10/23 15:27:37 - INFO - root -   backend = nccl
2023/10/23 15:27:37 - INFO - root -   batch_size = 2
2023/10/23 15:27:37 - INFO - root -   dropout = 0.5
2023/10/23 15:27:37 - INFO - root -   epochs = 300
2023/10/23 15:27:37 - INFO - root -   eval_freq = 5
2023/10/23 15:27:37 - INFO - root -   focal_loss = False
2023/10/23 15:27:37 - INFO - root -   input_size = 224
2023/10/23 15:27:37 - INFO - root -   is_pretrained = False
2023/10/23 15:27:37 - INFO - root -   label_smooth = False
2023/10/23 15:27:37 - INFO - root -   local_rank = -1
2023/10/23 15:27:37 - INFO - root -   lr = 1e-05
2023/10/23 15:27:37 - INFO - root -   lr_decay_rate = 0.1
2023/10/23 15:27:37 - INFO - root -   lr_steps = [50, 100]
2023/10/23 15:27:37 - INFO - root -   lr_type = cosine
2023/10/23 15:27:37 - INFO - root -   model_depth = 34
2023/10/23 15:27:37 - INFO - root -   model_name = resnet50
2023/10/23 15:27:37 - INFO - root -   momentum = 0.9
2023/10/23 15:27:37 - INFO - root -   num_classes = 2
2023/10/23 15:27:37 - INFO - root -   output = ./tcia_all_gbm_outputs
2023/10/23 15:27:37 - INFO - root -   print_freq = 20
2023/10/23 15:27:37 - INFO - root -   resume = 
2023/10/23 15:27:37 - INFO - root -   start_epoch = 0
2023/10/23 15:27:37 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/23 15:27:37 - INFO - root -   tune_from = /media/spgou/DATA/ZYJ/Glioma_easy/tcia_roi_gbm_outputs/precision_0.9302_num_120/epoch_120.pth
2023/10/23 15:27:37 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/23 15:27:37 - INFO - root -   warmup_epoch = 20
2023/10/23 15:27:37 - INFO - root -   warmup_multiplier = 100
2023/10/23 15:27:37 - INFO - root -   weight_decay = 0.0005
2023/10/23 15:27:37 - INFO - root -   workers = 8
2023/10/23 15:28:09 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 0.9211(0.9211)
2023/10/23 15:29:40 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 0.9933(0.9084)
2023/10/23 15:30:54 - INFO - root -   Epoch: [0/300][40/84], lr: 0.00000010 	 loss = 0.7906(0.9178)
2023/10/23 15:32:36 - INFO - root -   Epoch: [0/300][60/84], lr: 0.00000010 	 loss = 0.3555(0.9127)
2023/10/23 15:33:45 - INFO - root -   Epoch: [0/300][80/84], lr: 0.00000010 	 loss = 0.2352(0.8795)
2023/10/23 15:33:47 - INFO - root -   Epoch: [0/300] 	 loss = 0.8707
2023/10/23 15:33:47 - INFO - root -   train_accuracy = 0.5000
2023/10/23 15:34:17 - INFO - root -   Epoch: [1/300][0/84], lr: 0.00000011 	 loss = 1.1458(1.1458)
2023/10/23 15:35:52 - INFO - root -   Epoch: [1/300][20/84], lr: 0.00000011 	 loss = 0.5964(0.9106)
2023/10/23 15:37:10 - INFO - root -   Epoch: [1/300][40/84], lr: 0.00000011 	 loss = 0.6674(0.8267)
2023/10/23 15:38:44 - INFO - root -   Epoch: [1/300][60/84], lr: 0.00000011 	 loss = 0.4587(0.8105)
2023/10/23 15:39:36 - INFO - root -   Epoch: [1/300][80/84], lr: 0.00000011 	 loss = 0.5270(0.7914)
2023/10/23 15:39:40 - INFO - root -   Epoch: [1/300] 	 loss = 0.7832
2023/10/23 15:39:40 - INFO - root -   train_accuracy = 0.5119
2023/10/23 15:40:10 - INFO - root -   Epoch: [2/300][0/84], lr: 0.00000011 	 loss = 0.8432(0.8432)
2023/10/23 15:41:37 - INFO - root -   Epoch: [2/300][20/84], lr: 0.00000011 	 loss = 0.3011(0.7868)
2023/10/23 15:43:09 - INFO - root -   Epoch: [2/300][40/84], lr: 0.00000011 	 loss = 0.3523(0.8098)
2023/10/23 15:44:40 - INFO - root -   Epoch: [2/300][60/84], lr: 0.00000011 	 loss = 0.7459(0.7567)
2023/10/23 15:45:40 - INFO - root -   Epoch: [2/300][80/84], lr: 0.00000011 	 loss = 0.8237(0.7705)
2023/10/23 15:45:48 - INFO - root -   Epoch: [2/300] 	 loss = 0.7686
2023/10/23 15:45:48 - INFO - root -   train_accuracy = 0.5060
2023/10/23 15:46:17 - INFO - root -   Epoch: [3/300][0/84], lr: 0.00000012 	 loss = 0.4423(0.4423)
2023/10/23 15:48:02 - INFO - root -   Epoch: [3/300][20/84], lr: 0.00000012 	 loss = 0.5659(0.7980)
2023/10/23 15:49:12 - INFO - root -   Epoch: [3/300][40/84], lr: 0.00000012 	 loss = 0.4529(0.7339)
2023/10/23 15:50:41 - INFO - root -   Epoch: [3/300][60/84], lr: 0.00000012 	 loss = 0.8808(0.7610)
2023/10/23 15:51:39 - INFO - root -   Epoch: [3/300][80/84], lr: 0.00000012 	 loss = 1.1308(0.8024)
2023/10/23 15:51:44 - INFO - root -   Epoch: [3/300] 	 loss = 0.7902
2023/10/23 15:51:44 - INFO - root -   train_accuracy = 0.4405
2023/10/23 15:52:15 - INFO - root -   Epoch: [4/300][0/84], lr: 0.00000012 	 loss = 0.5974(0.5974)
2023/10/23 15:53:47 - INFO - root -   Epoch: [4/300][20/84], lr: 0.00000012 	 loss = 0.5548(0.6951)
2023/10/23 15:55:16 - INFO - root -   Epoch: [4/300][40/84], lr: 0.00000012 	 loss = 0.9875(0.7105)
2023/10/23 15:56:50 - INFO - root -   Epoch: [4/300][60/84], lr: 0.00000012 	 loss = 1.4658(0.7071)
2023/10/23 15:57:35 - INFO - root -   Epoch: [4/300][80/84], lr: 0.00000012 	 loss = 0.7887(0.7440)
2023/10/23 15:57:40 - INFO - root -   Epoch: [4/300] 	 loss = 0.7429
2023/10/23 15:59:02 - INFO - root -   precision = 0.5581
2023/10/23 15:59:02 - INFO - root -   eval_loss = 0.6932
2023/10/23 15:59:02 - INFO - root -   eval_acc = 0.5581
2023/10/23 15:59:03 - INFO - root -   train_accuracy = 0.5238
2023/10/23 15:59:34 - INFO - root -   Epoch: [5/300][0/84], lr: 0.00000013 	 loss = 1.0346(1.0346)
2023/10/23 16:01:07 - INFO - root -   Epoch: [5/300][20/84], lr: 0.00000013 	 loss = 0.8723(0.7259)
2023/10/23 16:02:56 - INFO - root -   Epoch: [5/300][40/84], lr: 0.00000013 	 loss = 0.9110(0.7565)
2023/10/23 16:03:52 - INFO - root -   Epoch: [5/300][60/84], lr: 0.00000013 	 loss = 1.3194(0.7412)
2023/10/23 16:05:02 - INFO - root -   Epoch: [5/300][80/84], lr: 0.00000013 	 loss = 0.5240(0.7672)
2023/10/23 16:05:03 - INFO - root -   Epoch: [5/300] 	 loss = 0.7680
2023/10/23 16:05:03 - INFO - root -   train_accuracy = 0.5000
2023/10/23 16:05:34 - INFO - root -   Epoch: [6/300][0/84], lr: 0.00000014 	 loss = 0.4328(0.4328)
2023/10/23 16:06:56 - INFO - root -   Epoch: [6/300][20/84], lr: 0.00000014 	 loss = 0.4272(0.7612)
2023/10/23 16:08:17 - INFO - root -   Epoch: [6/300][40/84], lr: 0.00000014 	 loss = 0.3220(0.7129)
2023/10/23 16:10:07 - INFO - root -   Epoch: [6/300][60/84], lr: 0.00000014 	 loss = 0.8905(0.7361)
2023/10/23 16:11:03 - INFO - root -   Epoch: [6/300][80/84], lr: 0.00000014 	 loss = 1.1612(0.7596)
2023/10/23 16:11:07 - INFO - root -   Epoch: [6/300] 	 loss = 0.7640
2023/10/23 16:11:07 - INFO - root -   train_accuracy = 0.5119
2023/10/23 16:11:39 - INFO - root -   Epoch: [7/300][0/84], lr: 0.00000014 	 loss = 0.6658(0.6658)
2023/10/23 16:13:09 - INFO - root -   Epoch: [7/300][20/84], lr: 0.00000014 	 loss = 0.3670(0.8045)
2023/10/23 16:14:42 - INFO - root -   Epoch: [7/300][40/84], lr: 0.00000014 	 loss = 0.4617(0.8263)
2023/10/23 16:16:16 - INFO - root -   Epoch: [7/300][60/84], lr: 0.00000014 	 loss = 1.1479(0.8395)
2023/10/23 16:17:20 - INFO - root -   Epoch: [7/300][80/84], lr: 0.00000014 	 loss = 0.6797(0.8392)
2023/10/23 16:17:27 - INFO - root -   Epoch: [7/300] 	 loss = 0.8342
2023/10/23 16:17:27 - INFO - root -   train_accuracy = 0.4762
2023/10/23 16:18:04 - INFO - root -   Epoch: [8/300][0/84], lr: 0.00000015 	 loss = 0.2196(0.2196)
2023/10/23 16:19:29 - INFO - root -   Epoch: [8/300][20/84], lr: 0.00000015 	 loss = 0.5598(0.7942)
2023/10/23 16:20:59 - INFO - root -   Epoch: [8/300][40/84], lr: 0.00000015 	 loss = 0.3943(0.6979)
2023/10/23 16:22:25 - INFO - root -   Epoch: [8/300][60/84], lr: 0.00000015 	 loss = 1.3204(0.7298)
2023/10/23 16:23:35 - INFO - root -   Epoch: [8/300][80/84], lr: 0.00000015 	 loss = 0.4205(0.7354)
2023/10/23 16:23:38 - INFO - root -   Epoch: [8/300] 	 loss = 0.7429
2023/10/23 16:23:38 - INFO - root -   train_accuracy = 0.5655
2023/10/23 16:24:11 - INFO - root -   Epoch: [9/300][0/84], lr: 0.00000015 	 loss = 0.5706(0.5706)
2023/10/23 16:25:44 - INFO - root -   Epoch: [9/300][20/84], lr: 0.00000015 	 loss = 0.4848(0.6850)
2023/10/23 16:27:05 - INFO - root -   Epoch: [9/300][40/84], lr: 0.00000015 	 loss = 1.2926(0.7982)
2023/10/23 16:28:38 - INFO - root -   Epoch: [9/300][60/84], lr: 0.00000015 	 loss = 0.5578(0.8054)
2023/10/23 16:29:40 - INFO - root -   Epoch: [9/300][80/84], lr: 0.00000015 	 loss = 1.1449(0.7705)
2023/10/23 16:29:48 - INFO - root -   Epoch: [9/300] 	 loss = 0.7789
2023/10/23 16:31:11 - INFO - root -   precision = 0.5349
2023/10/23 16:31:11 - INFO - root -   eval_loss = 0.6885
2023/10/23 16:31:11 - INFO - root -   eval_acc = 0.5349
2023/10/23 16:31:12 - INFO - root -   train_accuracy = 0.5179
2023/10/23 16:31:43 - INFO - root -   Epoch: [10/300][0/84], lr: 0.00000016 	 loss = 0.4032(0.4032)
2023/10/23 16:32:59 - INFO - root -   Epoch: [10/300][20/84], lr: 0.00000016 	 loss = 0.4448(0.7470)
2023/10/23 16:34:31 - INFO - root -   Epoch: [10/300][40/84], lr: 0.00000016 	 loss = 0.2378(0.6759)
2023/10/23 16:35:56 - INFO - root -   Epoch: [10/300][60/84], lr: 0.00000016 	 loss = 0.2853(0.6697)
2023/10/23 16:37:07 - INFO - root -   Epoch: [10/300][80/84], lr: 0.00000016 	 loss = 1.6069(0.6964)
2023/10/23 16:37:10 - INFO - root -   Epoch: [10/300] 	 loss = 0.7121
2023/10/23 16:37:10 - INFO - root -   train_accuracy = 0.6190
2023/10/23 16:37:49 - INFO - root -   Epoch: [11/300][0/84], lr: 0.00000016 	 loss = 0.5873(0.5873)
2023/10/23 16:39:16 - INFO - root -   Epoch: [11/300][20/84], lr: 0.00000016 	 loss = 0.8878(0.8133)
2023/10/23 16:40:44 - INFO - root -   Epoch: [11/300][40/84], lr: 0.00000016 	 loss = 0.6547(0.7621)
2023/10/23 16:42:01 - INFO - root -   Epoch: [11/300][60/84], lr: 0.00000016 	 loss = 1.3359(0.7476)
2023/10/23 16:43:15 - INFO - root -   Epoch: [11/300][80/84], lr: 0.00000016 	 loss = 0.7610(0.7301)
2023/10/23 16:43:17 - INFO - root -   Epoch: [11/300] 	 loss = 0.7304
2023/10/23 16:43:17 - INFO - root -   train_accuracy = 0.5952
2023/10/23 16:43:56 - INFO - root -   Epoch: [12/300][0/84], lr: 0.00000017 	 loss = 0.4418(0.4418)
2023/10/23 16:45:12 - INFO - root -   Epoch: [12/300][20/84], lr: 0.00000017 	 loss = 0.7220(0.6800)
2023/10/23 16:47:00 - INFO - root -   Epoch: [12/300][40/84], lr: 0.00000017 	 loss = 0.8004(0.6881)
2023/10/23 16:48:19 - INFO - root -   Epoch: [12/300][60/84], lr: 0.00000017 	 loss = 0.6982(0.7092)
2023/10/23 16:49:18 - INFO - root -   Epoch: [12/300][80/84], lr: 0.00000017 	 loss = 0.6911(0.7460)
2023/10/23 16:49:19 - INFO - root -   Epoch: [12/300] 	 loss = 0.7557
2023/10/23 16:49:19 - INFO - root -   train_accuracy = 0.5238
2023/10/23 16:49:49 - INFO - root -   Epoch: [13/300][0/84], lr: 0.00000018 	 loss = 1.2112(1.2112)
2023/10/23 16:51:22 - INFO - root -   Epoch: [13/300][20/84], lr: 0.00000018 	 loss = 0.4660(0.7992)
2023/10/23 16:52:43 - INFO - root -   Epoch: [13/300][40/84], lr: 0.00000018 	 loss = 0.2692(0.7517)
2023/10/23 16:54:17 - INFO - root -   Epoch: [13/300][60/84], lr: 0.00000018 	 loss = 0.8138(0.7409)
2023/10/23 16:55:21 - INFO - root -   Epoch: [13/300][80/84], lr: 0.00000018 	 loss = 0.9356(0.7705)
2023/10/23 16:55:32 - INFO - root -   Epoch: [13/300] 	 loss = 0.7706
2023/10/23 16:55:32 - INFO - root -   train_accuracy = 0.5000
2023/10/23 16:56:10 - INFO - root -   Epoch: [14/300][0/84], lr: 0.00000018 	 loss = 0.6295(0.6295)
2023/10/23 16:57:43 - INFO - root -   Epoch: [14/300][20/84], lr: 0.00000018 	 loss = 0.3777(0.8293)
2023/10/23 16:59:17 - INFO - root -   Epoch: [14/300][40/84], lr: 0.00000018 	 loss = 0.9523(0.8125)
2023/10/23 17:00:55 - INFO - root -   Epoch: [14/300][60/84], lr: 0.00000018 	 loss = 0.6523(0.7606)
2023/10/23 17:01:45 - INFO - root -   Epoch: [14/300][80/84], lr: 0.00000018 	 loss = 0.9203(0.7814)
2023/10/23 17:01:48 - INFO - root -   Epoch: [14/300] 	 loss = 0.7819
2023/10/23 17:03:07 - INFO - root -   precision = 0.6279
2023/10/23 17:03:07 - INFO - root -   eval_loss = 0.6868
2023/10/23 17:03:07 - INFO - root -   eval_acc = 0.6279
2023/10/23 17:03:08 - INFO - root -   train_accuracy = 0.4940
2023/10/23 17:03:45 - INFO - root -   Epoch: [15/300][0/84], lr: 0.00000019 	 loss = 0.6484(0.6484)
2023/10/23 17:05:09 - INFO - root -   Epoch: [15/300][20/84], lr: 0.00000019 	 loss = 1.6472(0.8302)
2023/10/23 17:07:06 - INFO - root -   Epoch: [15/300][40/84], lr: 0.00000019 	 loss = 0.7364(0.7636)
2023/10/23 17:08:10 - INFO - root -   Epoch: [15/300][60/84], lr: 0.00000019 	 loss = 0.8241(0.7691)
2023/10/23 17:09:10 - INFO - root -   Epoch: [15/300][80/84], lr: 0.00000019 	 loss = 0.9450(0.7982)
2023/10/23 17:09:11 - INFO - root -   Epoch: [15/300] 	 loss = 0.7929
2023/10/23 17:09:11 - INFO - root -   train_accuracy = 0.4643
2023/10/23 17:09:41 - INFO - root -   Epoch: [16/300][0/84], lr: 0.00000019 	 loss = 0.5623(0.5623)
2023/10/23 17:11:20 - INFO - root -   Epoch: [16/300][20/84], lr: 0.00000019 	 loss = 0.9393(0.7521)
2023/10/23 17:12:41 - INFO - root -   Epoch: [16/300][40/84], lr: 0.00000019 	 loss = 0.6023(0.7816)
2023/10/23 17:14:22 - INFO - root -   Epoch: [16/300][60/84], lr: 0.00000019 	 loss = 0.7321(0.7532)
2023/10/23 17:15:08 - INFO - root -   Epoch: [16/300][80/84], lr: 0.00000019 	 loss = 0.6856(0.7417)
2023/10/23 17:15:11 - INFO - root -   Epoch: [16/300] 	 loss = 0.7488
2023/10/23 17:15:11 - INFO - root -   train_accuracy = 0.5119
2023/10/23 17:15:48 - INFO - root -   Epoch: [17/300][0/84], lr: 0.00000020 	 loss = 0.7859(0.7859)
2023/10/23 17:17:03 - INFO - root -   Epoch: [17/300][20/84], lr: 0.00000020 	 loss = 0.2627(0.7424)
2023/10/23 17:18:19 - INFO - root -   Epoch: [17/300][40/84], lr: 0.00000020 	 loss = 0.7823(0.7544)
2023/10/23 17:19:46 - INFO - root -   Epoch: [17/300][60/84], lr: 0.00000020 	 loss = 0.5214(0.7220)
2023/10/23 17:20:59 - INFO - root -   Epoch: [17/300][80/84], lr: 0.00000020 	 loss = 0.5582(0.7439)
2023/10/23 17:21:01 - INFO - root -   Epoch: [17/300] 	 loss = 0.7554
2023/10/23 17:21:01 - INFO - root -   train_accuracy = 0.4881
2023/10/23 17:21:31 - INFO - root -   Epoch: [18/300][0/84], lr: 0.00000021 	 loss = 0.5329(0.5329)
2023/10/23 17:23:04 - INFO - root -   Epoch: [18/300][20/84], lr: 0.00000021 	 loss = 0.5925(0.7557)
2023/10/23 17:24:20 - INFO - root -   Epoch: [18/300][40/84], lr: 0.00000021 	 loss = 1.5125(0.8491)
2023/10/23 17:25:57 - INFO - root -   Epoch: [18/300][60/84], lr: 0.00000021 	 loss = 0.4693(0.8232)
2023/10/23 17:27:00 - INFO - root -   Epoch: [18/300][80/84], lr: 0.00000021 	 loss = 0.8267(0.8189)
2023/10/23 17:27:07 - INFO - root -   Epoch: [18/300] 	 loss = 0.8127
2023/10/23 17:27:07 - INFO - root -   train_accuracy = 0.5060
2023/10/23 17:27:54 - INFO - root -   Epoch: [19/300][0/84], lr: 0.00000021 	 loss = 1.1094(1.1094)
2023/10/23 17:29:01 - INFO - root -   Epoch: [19/300][20/84], lr: 0.00000021 	 loss = 0.6929(0.8462)
2023/10/23 17:30:24 - INFO - root -   Epoch: [19/300][40/84], lr: 0.00000021 	 loss = 0.6212(0.8155)
2023/10/23 17:32:10 - INFO - root -   Epoch: [19/300][60/84], lr: 0.00000021 	 loss = 0.9946(0.8193)
2023/10/23 17:33:05 - INFO - root -   Epoch: [19/300][80/84], lr: 0.00000021 	 loss = 0.7428(0.7847)
2023/10/23 17:33:11 - INFO - root -   Epoch: [19/300] 	 loss = 0.7921
2023/10/23 17:34:31 - INFO - root -   precision = 0.6744
2023/10/23 17:34:31 - INFO - root -   eval_loss = 0.6855
2023/10/23 17:34:31 - INFO - root -   eval_acc = 0.6744
2023/10/23 17:34:32 - INFO - root -   train_accuracy = 0.5417
2023/10/23 17:35:02 - INFO - root -   Epoch: [20/300][0/84], lr: 0.00000022 	 loss = 0.8496(0.8496)
2023/10/23 17:36:24 - INFO - root -   Epoch: [20/300][20/84], lr: 0.00000022 	 loss = 0.6609(0.6908)
2023/10/23 17:37:49 - INFO - root -   Epoch: [20/300][40/84], lr: 0.00000022 	 loss = 0.9230(0.7029)
2023/10/23 17:39:30 - INFO - root -   Epoch: [20/300][60/84], lr: 0.00000022 	 loss = 0.4589(0.7159)
2023/10/23 17:40:29 - INFO - root -   Epoch: [20/300][80/84], lr: 0.00000022 	 loss = 1.6382(0.7296)
2023/10/23 17:40:34 - INFO - root -   Epoch: [20/300] 	 loss = 0.7346
2023/10/23 17:40:34 - INFO - root -   train_accuracy = 0.5238
2023/10/23 17:41:03 - INFO - root -   Epoch: [21/300][0/84], lr: 0.00000022 	 loss = 0.8131(0.8131)
2023/10/23 17:42:24 - INFO - root -   Epoch: [21/300][20/84], lr: 0.00000022 	 loss = 0.7413(0.8387)
2023/10/23 17:43:57 - INFO - root -   Epoch: [21/300][40/84], lr: 0.00000022 	 loss = 0.6014(0.8194)
2023/10/23 17:45:29 - INFO - root -   Epoch: [21/300][60/84], lr: 0.00000022 	 loss = 1.4487(0.8160)
2023/10/23 17:46:37 - INFO - root -   Epoch: [21/300][80/84], lr: 0.00000022 	 loss = 0.8618(0.7978)
2023/10/23 17:46:38 - INFO - root -   Epoch: [21/300] 	 loss = 0.7857
2023/10/23 17:46:38 - INFO - root -   train_accuracy = 0.4881
2023/10/23 17:47:17 - INFO - root -   Epoch: [22/300][0/84], lr: 0.00000023 	 loss = 0.3885(0.3885)
2023/10/23 17:48:26 - INFO - root -   Epoch: [22/300][20/84], lr: 0.00000023 	 loss = 0.2287(0.7031)
2023/10/23 17:50:06 - INFO - root -   Epoch: [22/300][40/84], lr: 0.00000023 	 loss = 0.2750(0.6889)
2023/10/23 17:51:17 - INFO - root -   Epoch: [22/300][60/84], lr: 0.00000023 	 loss = 0.6601(0.7342)
2023/10/23 17:52:32 - INFO - root -   Epoch: [22/300][80/84], lr: 0.00000023 	 loss = 0.4062(0.7493)
2023/10/23 17:52:35 - INFO - root -   Epoch: [22/300] 	 loss = 0.7484
2023/10/23 17:52:35 - INFO - root -   train_accuracy = 0.5298
2023/10/23 17:53:05 - INFO - root -   Epoch: [23/300][0/84], lr: 0.00000024 	 loss = 0.4434(0.4434)
2023/10/23 17:54:27 - INFO - root -   Epoch: [23/300][20/84], lr: 0.00000024 	 loss = 0.8077(0.7330)
2023/10/23 17:56:02 - INFO - root -   Epoch: [23/300][40/84], lr: 0.00000024 	 loss = 0.5961(0.7562)
2023/10/23 17:57:24 - INFO - root -   Epoch: [23/300][60/84], lr: 0.00000024 	 loss = 0.5148(0.7352)
2023/10/23 17:58:32 - INFO - root -   Epoch: [23/300][80/84], lr: 0.00000024 	 loss = 1.1212(0.7341)
2023/10/23 17:58:36 - INFO - root -   Epoch: [23/300] 	 loss = 0.7426
2023/10/23 17:58:36 - INFO - root -   train_accuracy = 0.5536
2023/10/23 17:59:06 - INFO - root -   Epoch: [24/300][0/84], lr: 0.00000024 	 loss = 1.4171(1.4171)
2023/10/23 18:00:49 - INFO - root -   Epoch: [24/300][20/84], lr: 0.00000024 	 loss = 0.5508(0.8002)
2023/10/23 18:01:54 - INFO - root -   Epoch: [24/300][40/84], lr: 0.00000024 	 loss = 1.2997(0.8255)
2023/10/23 18:03:27 - INFO - root -   Epoch: [24/300][60/84], lr: 0.00000024 	 loss = 0.3689(0.7614)
2023/10/23 18:04:19 - INFO - root -   Epoch: [24/300][80/84], lr: 0.00000024 	 loss = 0.9341(0.7808)
2023/10/23 18:04:23 - INFO - root -   Epoch: [24/300] 	 loss = 0.7727
2023/10/23 18:05:42 - INFO - root -   precision = 0.6279
2023/10/23 18:05:42 - INFO - root -   eval_loss = 0.6784
2023/10/23 18:05:42 - INFO - root -   eval_acc = 0.6279
2023/10/23 18:05:43 - INFO - root -   train_accuracy = 0.5298
2023/10/23 18:06:22 - INFO - root -   Epoch: [25/300][0/84], lr: 0.00000025 	 loss = 0.6888(0.6888)
2023/10/23 18:07:43 - INFO - root -   Epoch: [25/300][20/84], lr: 0.00000025 	 loss = 0.3724(0.7453)
2023/10/23 18:09:10 - INFO - root -   Epoch: [25/300][40/84], lr: 0.00000025 	 loss = 0.7489(0.7712)
2023/10/23 18:10:45 - INFO - root -   Epoch: [25/300][60/84], lr: 0.00000025 	 loss = 0.5482(0.7765)
2023/10/23 18:11:45 - INFO - root -   Epoch: [25/300][80/84], lr: 0.00000025 	 loss = 0.4250(0.7728)
2023/10/23 18:11:46 - INFO - root -   Epoch: [25/300] 	 loss = 0.7705
2023/10/23 18:11:46 - INFO - root -   train_accuracy = 0.5238
2023/10/23 18:12:15 - INFO - root -   Epoch: [26/300][0/84], lr: 0.00000025 	 loss = 0.7565(0.7565)
2023/10/23 18:13:40 - INFO - root -   Epoch: [26/300][20/84], lr: 0.00000025 	 loss = 0.6270(0.7700)
2023/10/23 18:15:36 - INFO - root -   Epoch: [26/300][40/84], lr: 0.00000025 	 loss = 1.3919(0.8379)
2023/10/23 18:16:48 - INFO - root -   Epoch: [26/300][60/84], lr: 0.00000025 	 loss = 0.9457(0.8257)
2023/10/23 18:17:52 - INFO - root -   Epoch: [26/300][80/84], lr: 0.00000025 	 loss = 0.5484(0.8183)
2023/10/23 18:17:54 - INFO - root -   Epoch: [26/300] 	 loss = 0.8325
2023/10/23 18:17:54 - INFO - root -   train_accuracy = 0.4524
2023/10/23 18:18:24 - INFO - root -   Epoch: [27/300][0/84], lr: 0.00000026 	 loss = 0.5179(0.5179)
2023/10/23 18:19:39 - INFO - root -   Epoch: [27/300][20/84], lr: 0.00000026 	 loss = 0.5190(0.7386)
2023/10/23 18:21:18 - INFO - root -   Epoch: [27/300][40/84], lr: 0.00000026 	 loss = 0.9145(0.7378)
2023/10/23 18:22:45 - INFO - root -   Epoch: [27/300][60/84], lr: 0.00000026 	 loss = 0.4195(0.7260)
2023/10/23 18:23:39 - INFO - root -   Epoch: [27/300][80/84], lr: 0.00000026 	 loss = 0.5946(0.7252)
2023/10/23 18:23:43 - INFO - root -   Epoch: [27/300] 	 loss = 0.7270
2023/10/23 18:23:43 - INFO - root -   train_accuracy = 0.5774
2023/10/23 18:24:13 - INFO - root -   Epoch: [28/300][0/84], lr: 0.00000027 	 loss = 0.6013(0.6013)
2023/10/23 18:25:43 - INFO - root -   Epoch: [28/300][20/84], lr: 0.00000027 	 loss = 0.2845(0.7947)
2023/10/23 18:27:09 - INFO - root -   Epoch: [28/300][40/84], lr: 0.00000027 	 loss = 0.5052(0.8020)
2023/10/23 18:28:44 - INFO - root -   Epoch: [28/300][60/84], lr: 0.00000027 	 loss = 0.8776(0.7777)
2023/10/23 18:29:44 - INFO - root -   Epoch: [28/300][80/84], lr: 0.00000027 	 loss = 1.1307(0.7932)
2023/10/23 18:29:47 - INFO - root -   Epoch: [28/300] 	 loss = 0.7974
2023/10/23 18:29:47 - INFO - root -   train_accuracy = 0.5060
2023/10/23 18:30:17 - INFO - root -   Epoch: [29/300][0/84], lr: 0.00000027 	 loss = 0.4195(0.4195)
2023/10/23 18:31:33 - INFO - root -   Epoch: [29/300][20/84], lr: 0.00000027 	 loss = 0.8541(0.6274)
2023/10/23 18:33:01 - INFO - root -   Epoch: [29/300][40/84], lr: 0.00000027 	 loss = 0.4399(0.6932)
2023/10/23 18:34:18 - INFO - root -   Epoch: [29/300][60/84], lr: 0.00000027 	 loss = 0.7366(0.6767)
2023/10/23 18:35:28 - INFO - root -   Epoch: [29/300][80/84], lr: 0.00000027 	 loss = 0.8832(0.7512)
2023/10/23 18:35:35 - INFO - root -   Epoch: [29/300] 	 loss = 0.7536
2023/10/23 18:36:53 - INFO - root -   precision = 0.5814
2023/10/23 18:36:53 - INFO - root -   eval_loss = 0.6881
2023/10/23 18:36:53 - INFO - root -   eval_acc = 0.5814
2023/10/23 18:36:54 - INFO - root -   train_accuracy = 0.5298
2023/10/23 18:37:40 - INFO - root -   Epoch: [30/300][0/84], lr: 0.00000028 	 loss = 0.9542(0.9542)
2023/10/23 18:38:53 - INFO - root -   Epoch: [30/300][20/84], lr: 0.00000028 	 loss = 0.8264(0.8071)
2023/10/23 18:40:39 - INFO - root -   Epoch: [30/300][40/84], lr: 0.00000028 	 loss = 0.3968(0.7621)
2023/10/23 18:41:46 - INFO - root -   Epoch: [30/300][60/84], lr: 0.00000028 	 loss = 0.5543(0.7420)
2023/10/23 18:43:07 - INFO - root -   Epoch: [30/300][80/84], lr: 0.00000028 	 loss = 1.4274(0.7892)
2023/10/23 18:43:08 - INFO - root -   Epoch: [30/300] 	 loss = 0.7903
2023/10/23 18:43:08 - INFO - root -   train_accuracy = 0.4881
2023/10/23 18:43:54 - INFO - root -   Epoch: [31/300][0/84], lr: 0.00000028 	 loss = 0.3524(0.3524)
2023/10/23 18:45:07 - INFO - root -   Epoch: [31/300][20/84], lr: 0.00000028 	 loss = 0.4484(0.8228)
2023/10/23 18:46:54 - INFO - root -   Epoch: [31/300][40/84], lr: 0.00000028 	 loss = 0.6602(0.8356)
2023/10/23 18:48:04 - INFO - root -   Epoch: [31/300][60/84], lr: 0.00000028 	 loss = 0.3996(0.7598)
2023/10/23 18:49:07 - INFO - root -   Epoch: [31/300][80/84], lr: 0.00000028 	 loss = 0.6347(0.7409)
2023/10/23 18:49:08 - INFO - root -   Epoch: [31/300] 	 loss = 0.7414
2023/10/23 18:49:08 - INFO - root -   train_accuracy = 0.5595
2023/10/23 18:49:55 - INFO - root -   Epoch: [32/300][0/84], lr: 0.00000029 	 loss = 0.4647(0.4647)
2023/10/23 18:51:09 - INFO - root -   Epoch: [32/300][20/84], lr: 0.00000029 	 loss = 0.4036(0.6730)
2023/10/23 18:52:33 - INFO - root -   Epoch: [32/300][40/84], lr: 0.00000029 	 loss = 0.4471(0.7543)
2023/10/23 18:53:54 - INFO - root -   Epoch: [32/300][60/84], lr: 0.00000029 	 loss = 0.5259(0.7203)
2023/10/23 18:55:15 - INFO - root -   Epoch: [32/300][80/84], lr: 0.00000029 	 loss = 0.5822(0.7193)
2023/10/23 18:55:16 - INFO - root -   Epoch: [32/300] 	 loss = 0.7138
2023/10/23 18:55:16 - INFO - root -   train_accuracy = 0.6012
2023/10/23 18:55:54 - INFO - root -   Epoch: [33/300][0/84], lr: 0.00000029 	 loss = 0.2879(0.2879)
2023/10/23 18:57:18 - INFO - root -   Epoch: [33/300][20/84], lr: 0.00000029 	 loss = 0.5682(0.7452)
2023/10/23 18:58:53 - INFO - root -   Epoch: [33/300][40/84], lr: 0.00000029 	 loss = 0.6573(0.7221)
2023/10/23 19:00:06 - INFO - root -   Epoch: [33/300][60/84], lr: 0.00000029 	 loss = 0.8462(0.7391)
2023/10/23 19:01:08 - INFO - root -   Epoch: [33/300][80/84], lr: 0.00000029 	 loss = 0.7541(0.7702)
2023/10/23 19:01:09 - INFO - root -   Epoch: [33/300] 	 loss = 0.7697
2023/10/23 19:01:09 - INFO - root -   train_accuracy = 0.4881
2023/10/23 19:02:06 - INFO - root -   Epoch: [34/300][0/84], lr: 0.00000030 	 loss = 1.1240(1.1240)
2023/10/23 19:03:12 - INFO - root -   Epoch: [34/300][20/84], lr: 0.00000030 	 loss = 0.6326(0.7941)
2023/10/23 19:04:46 - INFO - root -   Epoch: [34/300][40/84], lr: 0.00000030 	 loss = 0.8417(0.7649)
2023/10/23 19:05:59 - INFO - root -   Epoch: [34/300][60/84], lr: 0.00000030 	 loss = 1.5401(0.7340)
2023/10/23 19:07:09 - INFO - root -   Epoch: [34/300][80/84], lr: 0.00000030 	 loss = 1.0901(0.7627)
2023/10/23 19:07:10 - INFO - root -   Epoch: [34/300] 	 loss = 0.7693
2023/10/23 19:08:29 - INFO - root -   precision = 0.6047
2023/10/23 19:08:29 - INFO - root -   eval_loss = 0.6826
2023/10/23 19:08:29 - INFO - root -   eval_acc = 0.6047
2023/10/23 19:08:30 - INFO - root -   train_accuracy = 0.5417
2023/10/23 19:09:15 - INFO - root -   Epoch: [35/300][0/84], lr: 0.00000031 	 loss = 0.4354(0.4354)
2023/10/23 19:10:22 - INFO - root -   Epoch: [35/300][20/84], lr: 0.00000031 	 loss = 0.6539(0.7248)
2023/10/23 19:12:16 - INFO - root -   Epoch: [35/300][40/84], lr: 0.00000031 	 loss = 0.6791(0.7174)
2023/10/23 19:13:22 - INFO - root -   Epoch: [35/300][60/84], lr: 0.00000031 	 loss = 0.4878(0.7562)
2023/10/23 19:14:31 - INFO - root -   Epoch: [35/300][80/84], lr: 0.00000031 	 loss = 0.5956(0.7612)
2023/10/23 19:14:32 - INFO - root -   Epoch: [35/300] 	 loss = 0.7564
2023/10/23 19:14:32 - INFO - root -   train_accuracy = 0.5238
2023/10/23 19:15:10 - INFO - root -   Epoch: [36/300][0/84], lr: 0.00000031 	 loss = 0.9175(0.9175)
2023/10/23 19:16:25 - INFO - root -   Epoch: [36/300][20/84], lr: 0.00000031 	 loss = 0.2974(0.7530)
2023/10/23 19:18:18 - INFO - root -   Epoch: [36/300][40/84], lr: 0.00000031 	 loss = 0.6729(0.7316)
2023/10/23 19:19:33 - INFO - root -   Epoch: [36/300][60/84], lr: 0.00000031 	 loss = 0.9087(0.7589)
2023/10/23 19:20:39 - INFO - root -   Epoch: [36/300][80/84], lr: 0.00000031 	 loss = 1.1290(0.7738)
2023/10/23 19:20:40 - INFO - root -   Epoch: [36/300] 	 loss = 0.7971
2023/10/23 19:20:40 - INFO - root -   train_accuracy = 0.5357
2023/10/23 19:21:10 - INFO - root -   Epoch: [37/300][0/84], lr: 0.00000032 	 loss = 0.3197(0.3197)
2023/10/23 19:22:39 - INFO - root -   Epoch: [37/300][20/84], lr: 0.00000032 	 loss = 0.3741(0.7260)
2023/10/23 19:23:58 - INFO - root -   Epoch: [37/300][40/84], lr: 0.00000032 	 loss = 0.9395(0.7524)
2023/10/23 19:25:26 - INFO - root -   Epoch: [37/300][60/84], lr: 0.00000032 	 loss = 1.1943(0.7376)
2023/10/23 19:26:28 - INFO - root -   Epoch: [37/300][80/84], lr: 0.00000032 	 loss = 0.9979(0.7330)
2023/10/23 19:26:29 - INFO - root -   Epoch: [37/300] 	 loss = 0.7382
2023/10/23 19:26:29 - INFO - root -   train_accuracy = 0.5000
2023/10/23 19:27:07 - INFO - root -   Epoch: [38/300][0/84], lr: 0.00000032 	 loss = 0.4406(0.4406)
2023/10/23 19:28:25 - INFO - root -   Epoch: [38/300][20/84], lr: 0.00000032 	 loss = 0.5027(0.7532)
2023/10/23 19:29:47 - INFO - root -   Epoch: [38/300][40/84], lr: 0.00000032 	 loss = 0.4471(0.8276)
2023/10/23 19:31:18 - INFO - root -   Epoch: [38/300][60/84], lr: 0.00000032 	 loss = 0.9898(0.7907)
2023/10/23 19:32:20 - INFO - root -   Epoch: [38/300][80/84], lr: 0.00000032 	 loss = 0.4883(0.7794)
2023/10/23 19:32:30 - INFO - root -   Epoch: [38/300] 	 loss = 0.7704
2023/10/23 19:32:30 - INFO - root -   train_accuracy = 0.5595
2023/10/23 19:33:00 - INFO - root -   Epoch: [39/300][0/84], lr: 0.00000033 	 loss = 0.5705(0.5705)
2023/10/23 19:34:47 - INFO - root -   Epoch: [39/300][20/84], lr: 0.00000033 	 loss = 0.8315(0.7311)
2023/10/23 19:36:28 - INFO - root -   Epoch: [39/300][40/84], lr: 0.00000033 	 loss = 0.5499(0.7658)
2023/10/23 19:37:26 - INFO - root -   Epoch: [39/300][60/84], lr: 0.00000033 	 loss = 0.6284(0.7781)
2023/10/23 19:38:35 - INFO - root -   Epoch: [39/300][80/84], lr: 0.00000033 	 loss = 0.7756(0.7807)
2023/10/23 19:38:36 - INFO - root -   Epoch: [39/300] 	 loss = 0.7760
2023/10/23 19:39:55 - INFO - root -   precision = 0.6047
2023/10/23 19:39:55 - INFO - root -   eval_loss = 0.6777
2023/10/23 19:39:55 - INFO - root -   eval_acc = 0.6047
2023/10/23 19:39:56 - INFO - root -   train_accuracy = 0.4762
2023/10/23 19:40:34 - INFO - root -   Epoch: [40/300][0/84], lr: 0.00000034 	 loss = 0.4937(0.4937)
2023/10/23 19:41:48 - INFO - root -   Epoch: [40/300][20/84], lr: 0.00000034 	 loss = 0.2340(0.6814)
2023/10/23 19:43:11 - INFO - root -   Epoch: [40/300][40/84], lr: 0.00000034 	 loss = 0.8368(0.7788)
2023/10/23 19:44:34 - INFO - root -   Epoch: [40/300][60/84], lr: 0.00000034 	 loss = 0.4300(0.7415)
2023/10/23 19:45:45 - INFO - root -   Epoch: [40/300][80/84], lr: 0.00000034 	 loss = 0.5944(0.7487)
2023/10/23 19:45:47 - INFO - root -   Epoch: [40/300] 	 loss = 0.7464
2023/10/23 19:45:47 - INFO - root -   train_accuracy = 0.4940
2023/10/23 19:46:25 - INFO - root -   Epoch: [41/300][0/84], lr: 0.00000034 	 loss = 0.6955(0.6955)
2023/10/23 19:47:57 - INFO - root -   Epoch: [41/300][20/84], lr: 0.00000034 	 loss = 1.5797(0.7771)
2023/10/23 19:49:18 - INFO - root -   Epoch: [41/300][40/84], lr: 0.00000034 	 loss = 0.5711(0.7660)
2023/10/23 19:50:48 - INFO - root -   Epoch: [41/300][60/84], lr: 0.00000034 	 loss = 1.2988(0.7620)
2023/10/23 19:51:54 - INFO - root -   Epoch: [41/300][80/84], lr: 0.00000034 	 loss = 1.5551(0.7868)
2023/10/23 19:52:02 - INFO - root -   Epoch: [41/300] 	 loss = 0.7776
2023/10/23 19:52:02 - INFO - root -   train_accuracy = 0.5119
2023/10/23 19:52:32 - INFO - root -   Epoch: [42/300][0/84], lr: 0.00000035 	 loss = 1.0372(1.0372)
2023/10/23 19:54:11 - INFO - root -   Epoch: [42/300][20/84], lr: 0.00000035 	 loss = 0.2812(0.8105)
2023/10/23 19:55:32 - INFO - root -   Epoch: [42/300][40/84], lr: 0.00000035 	 loss = 0.9543(0.7948)
2023/10/23 19:57:18 - INFO - root -   Epoch: [42/300][60/84], lr: 0.00000035 	 loss = 0.5539(0.7720)
2023/10/23 19:58:02 - INFO - root -   Epoch: [42/300][80/84], lr: 0.00000035 	 loss = 1.0035(0.7944)
2023/10/23 19:58:06 - INFO - root -   Epoch: [42/300] 	 loss = 0.7999
2023/10/23 19:58:06 - INFO - root -   train_accuracy = 0.5000
2023/10/23 19:58:43 - INFO - root -   Epoch: [43/300][0/84], lr: 0.00000035 	 loss = 0.7238(0.7238)
2023/10/23 20:00:03 - INFO - root -   Epoch: [43/300][20/84], lr: 0.00000035 	 loss = 0.7037(0.8296)
2023/10/23 20:01:50 - INFO - root -   Epoch: [43/300][40/84], lr: 0.00000035 	 loss = 0.7690(0.8307)
2023/10/23 20:03:06 - INFO - root -   Epoch: [43/300][60/84], lr: 0.00000035 	 loss = 0.5841(0.8129)
2023/10/23 20:04:05 - INFO - root -   Epoch: [43/300][80/84], lr: 0.00000035 	 loss = 0.5751(0.8109)
2023/10/23 20:04:09 - INFO - root -   Epoch: [43/300] 	 loss = 0.8107
2023/10/23 20:04:09 - INFO - root -   train_accuracy = 0.4345
2023/10/23 20:04:55 - INFO - root -   Epoch: [44/300][0/84], lr: 0.00000036 	 loss = 0.5357(0.5357)
2023/10/23 20:06:01 - INFO - root -   Epoch: [44/300][20/84], lr: 0.00000036 	 loss = 1.1879(0.8145)
2023/10/23 20:07:36 - INFO - root -   Epoch: [44/300][40/84], lr: 0.00000036 	 loss = 0.6978(0.7549)
2023/10/23 20:08:58 - INFO - root -   Epoch: [44/300][60/84], lr: 0.00000036 	 loss = 0.8690(0.7578)
2023/10/23 20:10:04 - INFO - root -   Epoch: [44/300][80/84], lr: 0.00000036 	 loss = 1.0231(0.7805)
2023/10/23 20:10:06 - INFO - root -   Epoch: [44/300] 	 loss = 0.7981
2023/10/23 20:11:24 - INFO - root -   precision = 0.6047
2023/10/23 20:11:24 - INFO - root -   eval_loss = 0.6820
2023/10/23 20:11:24 - INFO - root -   eval_acc = 0.6047
2023/10/23 20:11:26 - INFO - root -   train_accuracy = 0.4583
2023/10/23 20:12:03 - INFO - root -   Epoch: [45/300][0/84], lr: 0.00000037 	 loss = 0.3137(0.3137)
2023/10/23 20:13:32 - INFO - root -   Epoch: [45/300][20/84], lr: 0.00000037 	 loss = 0.3181(0.7028)
2023/10/23 20:15:03 - INFO - root -   Epoch: [45/300][40/84], lr: 0.00000037 	 loss = 0.7287(0.7194)
2023/10/23 20:16:12 - INFO - root -   Epoch: [45/300][60/84], lr: 0.00000037 	 loss = 0.5598(0.7369)
2023/10/23 20:17:30 - INFO - root -   Epoch: [45/300][80/84], lr: 0.00000037 	 loss = 0.5276(0.7284)
2023/10/23 20:17:31 - INFO - root -   Epoch: [45/300] 	 loss = 0.7416
2023/10/23 20:17:31 - INFO - root -   train_accuracy = 0.5119
2023/10/23 20:18:08 - INFO - root -   Epoch: [46/300][0/84], lr: 0.00000037 	 loss = 0.4205(0.4205)
2023/10/23 20:19:31 - INFO - root -   Epoch: [46/300][20/84], lr: 0.00000037 	 loss = 0.6081(0.6897)
2023/10/23 20:20:52 - INFO - root -   Epoch: [46/300][40/84], lr: 0.00000037 	 loss = 1.3408(0.7298)
2023/10/23 20:22:20 - INFO - root -   Epoch: [46/300][60/84], lr: 0.00000037 	 loss = 0.5018(0.7365)
2023/10/23 20:23:19 - INFO - root -   Epoch: [46/300][80/84], lr: 0.00000037 	 loss = 1.0223(0.7101)
2023/10/23 20:23:22 - INFO - root -   Epoch: [46/300] 	 loss = 0.7221
2023/10/23 20:23:22 - INFO - root -   train_accuracy = 0.5238
2023/10/23 20:23:52 - INFO - root -   Epoch: [47/300][0/84], lr: 0.00000038 	 loss = 0.3830(0.3830)
2023/10/23 20:25:16 - INFO - root -   Epoch: [47/300][20/84], lr: 0.00000038 	 loss = 0.6532(0.7745)
2023/10/23 20:26:45 - INFO - root -   Epoch: [47/300][40/84], lr: 0.00000038 	 loss = 1.0159(0.8463)
2023/10/23 20:28:16 - INFO - root -   Epoch: [47/300][60/84], lr: 0.00000038 	 loss = 1.3471(0.8068)
2023/10/23 20:29:17 - INFO - root -   Epoch: [47/300][80/84], lr: 0.00000038 	 loss = 0.4528(0.7982)
2023/10/23 20:29:18 - INFO - root -   Epoch: [47/300] 	 loss = 0.7972
2023/10/23 20:29:18 - INFO - root -   train_accuracy = 0.4762
2023/10/23 20:29:48 - INFO - root -   Epoch: [48/300][0/84], lr: 0.00000038 	 loss = 0.7056(0.7056)
2023/10/23 20:31:06 - INFO - root -   Epoch: [48/300][20/84], lr: 0.00000038 	 loss = 0.6133(0.6961)
2023/10/23 20:32:42 - INFO - root -   Epoch: [48/300][40/84], lr: 0.00000038 	 loss = 0.5005(0.7312)
2023/10/23 20:33:59 - INFO - root -   Epoch: [48/300][60/84], lr: 0.00000038 	 loss = 0.4580(0.7421)
2023/10/23 20:35:16 - INFO - root -   Epoch: [48/300][80/84], lr: 0.00000038 	 loss = 0.5131(0.7307)
2023/10/23 20:35:17 - INFO - root -   Epoch: [48/300] 	 loss = 0.7270
2023/10/23 20:35:17 - INFO - root -   train_accuracy = 0.5298
2023/10/23 20:36:03 - INFO - root -   Epoch: [49/300][0/84], lr: 0.00000039 	 loss = 0.3635(0.3635)
2023/10/23 20:37:11 - INFO - root -   Epoch: [49/300][20/84], lr: 0.00000039 	 loss = 0.6581(0.7585)
2023/10/23 20:39:07 - INFO - root -   Epoch: [49/300][40/84], lr: 0.00000039 	 loss = 0.9146(0.7312)
2023/10/23 20:40:17 - INFO - root -   Epoch: [49/300][60/84], lr: 0.00000039 	 loss = 0.4503(0.7384)
2023/10/23 20:41:13 - INFO - root -   Epoch: [49/300][80/84], lr: 0.00000039 	 loss = 1.0947(0.7382)
2023/10/23 20:41:15 - INFO - root -   Epoch: [49/300] 	 loss = 0.7407
2023/10/23 20:42:34 - INFO - root -   precision = 0.5581
2023/10/23 20:42:34 - INFO - root -   eval_loss = 0.6832
2023/10/23 20:42:34 - INFO - root -   eval_acc = 0.5581
2023/10/23 20:42:35 - INFO - root -   train_accuracy = 0.4881
2023/10/23 20:43:05 - INFO - root -   Epoch: [50/300][0/84], lr: 0.00000039 	 loss = 1.2273(1.2273)
2023/10/23 20:44:28 - INFO - root -   Epoch: [50/300][20/84], lr: 0.00000039 	 loss = 0.6857(0.8075)
2023/10/23 20:45:51 - INFO - root -   Epoch: [50/300][40/84], lr: 0.00000039 	 loss = 0.9959(0.8186)
2023/10/23 20:47:23 - INFO - root -   Epoch: [50/300][60/84], lr: 0.00000039 	 loss = 0.7632(0.7630)
2023/10/23 20:48:36 - INFO - root -   Epoch: [50/300][80/84], lr: 0.00000039 	 loss = 0.7990(0.7652)
2023/10/23 20:48:42 - INFO - root -   Epoch: [50/300] 	 loss = 0.7721
2023/10/23 20:48:42 - INFO - root -   train_accuracy = 0.4940
2023/10/23 20:49:20 - INFO - root -   Epoch: [51/300][0/84], lr: 0.00000040 	 loss = 0.8037(0.8037)
2023/10/23 20:50:51 - INFO - root -   Epoch: [51/300][20/84], lr: 0.00000040 	 loss = 1.8123(0.7793)
2023/10/23 20:52:26 - INFO - root -   Epoch: [51/300][40/84], lr: 0.00000040 	 loss = 0.6328(0.8307)
2023/10/23 20:53:31 - INFO - root -   Epoch: [51/300][60/84], lr: 0.00000040 	 loss = 0.7476(0.8071)
2023/10/23 20:54:44 - INFO - root -   Epoch: [51/300][80/84], lr: 0.00000040 	 loss = 0.3969(0.7802)
2023/10/23 20:54:48 - INFO - root -   Epoch: [51/300] 	 loss = 0.7775
2023/10/23 20:54:48 - INFO - root -   train_accuracy = 0.4940
2023/10/23 20:55:25 - INFO - root -   Epoch: [52/300][0/84], lr: 0.00000041 	 loss = 0.9384(0.9384)
2023/10/23 20:56:41 - INFO - root -   Epoch: [52/300][20/84], lr: 0.00000041 	 loss = 0.5614(0.6566)
2023/10/23 20:58:12 - INFO - root -   Epoch: [52/300][40/84], lr: 0.00000041 	 loss = 0.8068(0.6831)
2023/10/23 20:59:51 - INFO - root -   Epoch: [52/300][60/84], lr: 0.00000041 	 loss = 0.4892(0.6923)
2023/10/23 21:00:39 - INFO - root -   Epoch: [52/300][80/84], lr: 0.00000041 	 loss = 1.0009(0.6972)
2023/10/23 21:00:42 - INFO - root -   Epoch: [52/300] 	 loss = 0.7006
2023/10/23 21:00:42 - INFO - root -   train_accuracy = 0.6131
2023/10/23 21:01:12 - INFO - root -   Epoch: [53/300][0/84], lr: 0.00000041 	 loss = 0.4095(0.4095)
2023/10/23 21:02:36 - INFO - root -   Epoch: [53/300][20/84], lr: 0.00000041 	 loss = 0.6140(0.7810)
2023/10/23 21:03:57 - INFO - root -   Epoch: [53/300][40/84], lr: 0.00000041 	 loss = 0.3866(0.8072)
2023/10/23 21:05:13 - INFO - root -   Epoch: [53/300][60/84], lr: 0.00000041 	 loss = 0.6494(0.7632)
2023/10/23 21:06:31 - INFO - root -   Epoch: [53/300][80/84], lr: 0.00000041 	 loss = 0.4995(0.7503)
2023/10/23 21:06:35 - INFO - root -   Epoch: [53/300] 	 loss = 0.7575
2023/10/23 21:06:35 - INFO - root -   train_accuracy = 0.5476
2023/10/23 21:07:05 - INFO - root -   Epoch: [54/300][0/84], lr: 0.00000042 	 loss = 1.0732(1.0732)
2023/10/23 21:08:48 - INFO - root -   Epoch: [54/300][20/84], lr: 0.00000042 	 loss = 0.4836(0.7107)
2023/10/23 21:09:53 - INFO - root -   Epoch: [54/300][40/84], lr: 0.00000042 	 loss = 0.4602(0.7422)
2023/10/23 21:11:26 - INFO - root -   Epoch: [54/300][60/84], lr: 0.00000042 	 loss = 0.7354(0.7344)
2023/10/23 21:12:34 - INFO - root -   Epoch: [54/300][80/84], lr: 0.00000042 	 loss = 0.8655(0.7447)
2023/10/23 21:12:35 - INFO - root -   Epoch: [54/300] 	 loss = 0.7551
2023/10/23 21:13:55 - INFO - root -   precision = 0.5581
2023/10/23 21:13:55 - INFO - root -   eval_loss = 0.6819
2023/10/23 21:13:55 - INFO - root -   eval_acc = 0.5581
2023/10/23 21:13:56 - INFO - root -   train_accuracy = 0.5298
2023/10/23 21:14:26 - INFO - root -   Epoch: [55/300][0/84], lr: 0.00000042 	 loss = 0.7618(0.7618)
2023/10/23 21:15:57 - INFO - root -   Epoch: [55/300][20/84], lr: 0.00000042 	 loss = 0.8776(0.7384)
2023/10/23 21:17:13 - INFO - root -   Epoch: [55/300][40/84], lr: 0.00000042 	 loss = 0.7768(0.7214)
2023/10/23 21:18:53 - INFO - root -   Epoch: [55/300][60/84], lr: 0.00000042 	 loss = 0.7642(0.7234)
2023/10/23 21:20:01 - INFO - root -   Epoch: [55/300][80/84], lr: 0.00000042 	 loss = 0.4639(0.7105)
2023/10/23 21:20:05 - INFO - root -   Epoch: [55/300] 	 loss = 0.7056
2023/10/23 21:20:05 - INFO - root -   train_accuracy = 0.5536
2023/10/23 21:20:42 - INFO - root -   Epoch: [56/300][0/84], lr: 0.00000043 	 loss = 0.6796(0.6796)
2023/10/23 21:22:06 - INFO - root -   Epoch: [56/300][20/84], lr: 0.00000043 	 loss = 0.5455(0.6584)
2023/10/23 21:23:27 - INFO - root -   Epoch: [56/300][40/84], lr: 0.00000043 	 loss = 0.6958(0.7032)
2023/10/23 21:24:56 - INFO - root -   Epoch: [56/300][60/84], lr: 0.00000043 	 loss = 0.4379(0.7250)
2023/10/23 21:26:03 - INFO - root -   Epoch: [56/300][80/84], lr: 0.00000043 	 loss = 0.7283(0.7478)
2023/10/23 21:26:06 - INFO - root -   Epoch: [56/300] 	 loss = 0.7467
2023/10/23 21:26:06 - INFO - root -   train_accuracy = 0.5119
2023/10/23 21:26:43 - INFO - root -   Epoch: [57/300][0/84], lr: 0.00000044 	 loss = 0.7297(0.7297)
2023/10/23 21:28:04 - INFO - root -   Epoch: [57/300][20/84], lr: 0.00000044 	 loss = 0.6259(0.6978)
2023/10/23 21:29:44 - INFO - root -   Epoch: [57/300][40/84], lr: 0.00000044 	 loss = 0.4326(0.7833)
2023/10/23 21:31:06 - INFO - root -   Epoch: [57/300][60/84], lr: 0.00000044 	 loss = 0.3764(0.7696)
2023/10/23 21:32:00 - INFO - root -   Epoch: [57/300][80/84], lr: 0.00000044 	 loss = 0.9017(0.7841)
2023/10/23 21:32:02 - INFO - root -   Epoch: [57/300] 	 loss = 0.7904
2023/10/23 21:32:02 - INFO - root -   train_accuracy = 0.4702
2023/10/23 21:32:40 - INFO - root -   Epoch: [58/300][0/84], lr: 0.00000044 	 loss = 1.1320(1.1320)
2023/10/23 21:34:01 - INFO - root -   Epoch: [58/300][20/84], lr: 0.00000044 	 loss = 0.9940(0.7840)
2023/10/23 21:35:18 - INFO - root -   Epoch: [58/300][40/84], lr: 0.00000044 	 loss = 0.6009(0.7793)
2023/10/23 21:36:49 - INFO - root -   Epoch: [58/300][60/84], lr: 0.00000044 	 loss = 0.4872(0.7600)
2023/10/23 21:37:58 - INFO - root -   Epoch: [58/300][80/84], lr: 0.00000044 	 loss = 0.3875(0.7574)
2023/10/23 21:38:02 - INFO - root -   Epoch: [58/300] 	 loss = 0.7538
2023/10/23 21:38:02 - INFO - root -   train_accuracy = 0.5238
2023/10/23 21:38:39 - INFO - root -   Epoch: [59/300][0/84], lr: 0.00000045 	 loss = 0.6121(0.6121)
2023/10/23 21:39:57 - INFO - root -   Epoch: [59/300][20/84], lr: 0.00000045 	 loss = 1.1989(0.7731)
2023/10/23 21:41:18 - INFO - root -   Epoch: [59/300][40/84], lr: 0.00000045 	 loss = 1.2540(0.7635)
2023/10/23 21:42:50 - INFO - root -   Epoch: [59/300][60/84], lr: 0.00000045 	 loss = 1.0424(0.7733)
2023/10/23 21:43:59 - INFO - root -   Epoch: [59/300][80/84], lr: 0.00000045 	 loss = 0.7936(0.7709)
2023/10/23 21:44:02 - INFO - root -   Epoch: [59/300] 	 loss = 0.7760
2023/10/23 21:45:21 - INFO - root -   precision = 0.4186
2023/10/23 21:45:21 - INFO - root -   eval_loss = 0.6806
2023/10/23 21:45:21 - INFO - root -   eval_acc = 0.4186
2023/10/23 21:45:22 - INFO - root -   train_accuracy = 0.4821
2023/10/23 21:45:51 - INFO - root -   Epoch: [60/300][0/84], lr: 0.00000045 	 loss = 0.5950(0.5950)
2023/10/23 21:47:07 - INFO - root -   Epoch: [60/300][20/84], lr: 0.00000045 	 loss = 0.4420(0.7389)
2023/10/23 21:48:33 - INFO - root -   Epoch: [60/300][40/84], lr: 0.00000045 	 loss = 0.7406(0.7624)
2023/10/23 21:50:16 - INFO - root -   Epoch: [60/300][60/84], lr: 0.00000045 	 loss = 0.6071(0.7441)
2023/10/23 21:51:14 - INFO - root -   Epoch: [60/300][80/84], lr: 0.00000045 	 loss = 1.1230(0.7597)
2023/10/23 21:51:17 - INFO - root -   Epoch: [60/300] 	 loss = 0.7531
2023/10/23 21:51:17 - INFO - root -   train_accuracy = 0.5000
2023/10/23 21:51:56 - INFO - root -   Epoch: [61/300][0/84], lr: 0.00000046 	 loss = 0.5179(0.5179)
2023/10/23 21:53:10 - INFO - root -   Epoch: [61/300][20/84], lr: 0.00000046 	 loss = 0.6882(0.7319)
2023/10/23 21:54:41 - INFO - root -   Epoch: [61/300][40/84], lr: 0.00000046 	 loss = 0.6427(0.7628)
2023/10/23 21:56:01 - INFO - root -   Epoch: [61/300][60/84], lr: 0.00000046 	 loss = 0.5919(0.7443)
2023/10/23 21:57:05 - INFO - root -   Epoch: [61/300][80/84], lr: 0.00000046 	 loss = 0.5292(0.7386)
2023/10/23 21:57:10 - INFO - root -   Epoch: [61/300] 	 loss = 0.7350
2023/10/23 21:57:10 - INFO - root -   train_accuracy = 0.5119
2023/10/23 21:57:49 - INFO - root -   Epoch: [62/300][0/84], lr: 0.00000047 	 loss = 0.7408(0.7408)
2023/10/23 21:59:08 - INFO - root -   Epoch: [62/300][20/84], lr: 0.00000047 	 loss = 0.5222(0.7171)
2023/10/23 22:00:31 - INFO - root -   Epoch: [62/300][40/84], lr: 0.00000047 	 loss = 1.3251(0.7598)
2023/10/23 22:02:17 - INFO - root -   Epoch: [62/300][60/84], lr: 0.00000047 	 loss = 0.5864(0.7549)
2023/10/23 22:03:15 - INFO - root -   Epoch: [62/300][80/84], lr: 0.00000047 	 loss = 0.2441(0.7682)
2023/10/23 22:03:19 - INFO - root -   Epoch: [62/300] 	 loss = 0.7735
2023/10/23 22:03:19 - INFO - root -   train_accuracy = 0.4881
2023/10/23 22:03:57 - INFO - root -   Epoch: [63/300][0/84], lr: 0.00000047 	 loss = 0.9467(0.9467)
2023/10/23 22:05:04 - INFO - root -   Epoch: [63/300][20/84], lr: 0.00000047 	 loss = 0.6556(0.8002)
2023/10/23 22:06:41 - INFO - root -   Epoch: [63/300][40/84], lr: 0.00000047 	 loss = 1.0681(0.7981)
2023/10/23 22:07:53 - INFO - root -   Epoch: [63/300][60/84], lr: 0.00000047 	 loss = 0.6614(0.7569)
2023/10/23 22:09:07 - INFO - root -   Epoch: [63/300][80/84], lr: 0.00000047 	 loss = 0.8588(0.7705)
2023/10/23 22:09:08 - INFO - root -   Epoch: [63/300] 	 loss = 0.7730
2023/10/23 22:09:08 - INFO - root -   train_accuracy = 0.4583
2023/10/23 22:09:47 - INFO - root -   Epoch: [64/300][0/84], lr: 0.00000048 	 loss = 0.5836(0.5836)
2023/10/23 22:11:07 - INFO - root -   Epoch: [64/300][20/84], lr: 0.00000048 	 loss = 0.7969(0.7449)
2023/10/23 22:12:39 - INFO - root -   Epoch: [64/300][40/84], lr: 0.00000048 	 loss = 0.6684(0.7627)
2023/10/23 22:13:53 - INFO - root -   Epoch: [64/300][60/84], lr: 0.00000048 	 loss = 0.5096(0.7874)
2023/10/23 22:15:03 - INFO - root -   Epoch: [64/300][80/84], lr: 0.00000048 	 loss = 0.7077(0.7799)
2023/10/23 22:15:05 - INFO - root -   Epoch: [64/300] 	 loss = 0.7871
2023/10/23 22:16:24 - INFO - root -   precision = 0.5814
2023/10/23 22:16:24 - INFO - root -   eval_loss = 0.6920
2023/10/23 22:16:24 - INFO - root -   eval_acc = 0.5814
2023/10/23 22:16:25 - INFO - root -   train_accuracy = 0.5000
2023/10/23 22:17:11 - INFO - root -   Epoch: [65/300][0/84], lr: 0.00000048 	 loss = 0.4787(0.4787)
2023/10/23 22:18:32 - INFO - root -   Epoch: [65/300][20/84], lr: 0.00000048 	 loss = 0.9857(0.8169)
2023/10/23 22:20:25 - INFO - root -   Epoch: [65/300][40/84], lr: 0.00000048 	 loss = 0.6866(0.8242)
2023/10/23 22:21:24 - INFO - root -   Epoch: [65/300][60/84], lr: 0.00000048 	 loss = 0.7694(0.8019)
2023/10/23 22:22:33 - INFO - root -   Epoch: [65/300][80/84], lr: 0.00000048 	 loss = 0.6387(0.7630)
2023/10/23 22:22:40 - INFO - root -   Epoch: [65/300] 	 loss = 0.7673
2023/10/23 22:22:40 - INFO - root -   train_accuracy = 0.5000
2023/10/23 22:23:09 - INFO - root -   Epoch: [66/300][0/84], lr: 0.00000049 	 loss = 0.5744(0.5744)
2023/10/23 22:24:31 - INFO - root -   Epoch: [66/300][20/84], lr: 0.00000049 	 loss = 0.4252(0.7958)
2023/10/23 22:25:56 - INFO - root -   Epoch: [66/300][40/84], lr: 0.00000049 	 loss = 0.5500(0.7747)
2023/10/23 22:27:15 - INFO - root -   Epoch: [66/300][60/84], lr: 0.00000049 	 loss = 0.3164(0.7507)
2023/10/23 22:28:31 - INFO - root -   Epoch: [66/300][80/84], lr: 0.00000049 	 loss = 0.6484(0.7261)
2023/10/23 22:28:32 - INFO - root -   Epoch: [66/300] 	 loss = 0.7243
2023/10/23 22:28:32 - INFO - root -   train_accuracy = 0.5119
2023/10/23 22:29:10 - INFO - root -   Epoch: [67/300][0/84], lr: 0.00000049 	 loss = 0.3945(0.3945)
2023/10/23 22:30:33 - INFO - root -   Epoch: [67/300][20/84], lr: 0.00000049 	 loss = 0.5902(0.6489)
2023/10/23 22:32:05 - INFO - root -   Epoch: [67/300][40/84], lr: 0.00000049 	 loss = 0.2308(0.6770)
2023/10/23 22:33:31 - INFO - root -   Epoch: [67/300][60/84], lr: 0.00000049 	 loss = 0.7519(0.7241)
2023/10/23 22:34:30 - INFO - root -   Epoch: [67/300][80/84], lr: 0.00000049 	 loss = 0.7781(0.7063)
2023/10/23 22:34:32 - INFO - root -   Epoch: [67/300] 	 loss = 0.7135
2023/10/23 22:34:32 - INFO - root -   train_accuracy = 0.5417
2023/10/23 22:35:11 - INFO - root -   Epoch: [68/300][0/84], lr: 0.00000050 	 loss = 0.8626(0.8626)
2023/10/23 22:36:32 - INFO - root -   Epoch: [68/300][20/84], lr: 0.00000050 	 loss = 0.6660(0.8410)
2023/10/23 22:37:57 - INFO - root -   Epoch: [68/300][40/84], lr: 0.00000050 	 loss = 0.6993(0.8457)
2023/10/23 22:39:18 - INFO - root -   Epoch: [68/300][60/84], lr: 0.00000050 	 loss = 0.6194(0.8016)
2023/10/23 22:40:27 - INFO - root -   Epoch: [68/300][80/84], lr: 0.00000050 	 loss = 0.8363(0.7922)
2023/10/23 22:40:34 - INFO - root -   Epoch: [68/300] 	 loss = 0.7899
2023/10/23 22:40:34 - INFO - root -   train_accuracy = 0.4524
2023/10/23 22:41:05 - INFO - root -   Epoch: [69/300][0/84], lr: 0.00000051 	 loss = 0.3558(0.3558)
2023/10/23 22:42:28 - INFO - root -   Epoch: [69/300][20/84], lr: 0.00000051 	 loss = 0.6977(0.7651)
2023/10/23 22:44:18 - INFO - root -   Epoch: [69/300][40/84], lr: 0.00000051 	 loss = 0.6947(0.7540)
2023/10/23 22:45:30 - INFO - root -   Epoch: [69/300][60/84], lr: 0.00000051 	 loss = 0.6678(0.7486)
2023/10/23 22:46:33 - INFO - root -   Epoch: [69/300][80/84], lr: 0.00000051 	 loss = 0.8175(0.7469)
2023/10/23 22:46:35 - INFO - root -   Epoch: [69/300] 	 loss = 0.7499
2023/10/23 22:47:53 - INFO - root -   precision = 0.6047
2023/10/23 22:47:53 - INFO - root -   eval_loss = 0.6826
2023/10/23 22:47:53 - INFO - root -   eval_acc = 0.6047
2023/10/23 22:47:54 - INFO - root -   train_accuracy = 0.5000
2023/10/23 22:48:39 - INFO - root -   Epoch: [70/300][0/84], lr: 0.00000051 	 loss = 0.6735(0.6735)
2023/10/23 22:50:01 - INFO - root -   Epoch: [70/300][20/84], lr: 0.00000051 	 loss = 0.6014(0.7463)
2023/10/23 22:51:40 - INFO - root -   Epoch: [70/300][40/84], lr: 0.00000051 	 loss = 0.7110(0.7118)
2023/10/23 22:52:45 - INFO - root -   Epoch: [70/300][60/84], lr: 0.00000051 	 loss = 0.6628(0.7011)
2023/10/23 22:53:54 - INFO - root -   Epoch: [70/300][80/84], lr: 0.00000051 	 loss = 0.5503(0.6803)
2023/10/23 22:53:56 - INFO - root -   Epoch: [70/300] 	 loss = 0.6734
2023/10/23 22:53:56 - INFO - root -   train_accuracy = 0.5833
2023/10/23 22:54:34 - INFO - root -   Epoch: [71/300][0/84], lr: 0.00000052 	 loss = 0.2813(0.2813)
2023/10/23 22:55:51 - INFO - root -   Epoch: [71/300][20/84], lr: 0.00000052 	 loss = 0.5277(0.7252)
2023/10/23 22:57:13 - INFO - root -   Epoch: [71/300][40/84], lr: 0.00000052 	 loss = 0.6187(0.7423)
2023/10/23 22:58:51 - INFO - root -   Epoch: [71/300][60/84], lr: 0.00000052 	 loss = 0.7850(0.7390)
2023/10/23 22:59:45 - INFO - root -   Epoch: [71/300][80/84], lr: 0.00000052 	 loss = 0.5318(0.7098)
2023/10/23 22:59:50 - INFO - root -   Epoch: [71/300] 	 loss = 0.7153
2023/10/23 22:59:50 - INFO - root -   train_accuracy = 0.5714
2023/10/23 23:00:29 - INFO - root -   Epoch: [72/300][0/84], lr: 0.00000052 	 loss = 0.4892(0.4892)
2023/10/23 23:01:52 - INFO - root -   Epoch: [72/300][20/84], lr: 0.00000052 	 loss = 0.4504(0.7848)
2023/10/23 23:03:23 - INFO - root -   Epoch: [72/300][40/84], lr: 0.00000052 	 loss = 0.7926(0.7728)
2023/10/23 23:04:36 - INFO - root -   Epoch: [72/300][60/84], lr: 0.00000052 	 loss = 0.4478(0.7432)
2023/10/23 23:05:41 - INFO - root -   Epoch: [72/300][80/84], lr: 0.00000052 	 loss = 0.6581(0.7420)
2023/10/23 23:05:45 - INFO - root -   Epoch: [72/300] 	 loss = 0.7452
2023/10/23 23:05:45 - INFO - root -   train_accuracy = 0.5119
2023/10/23 23:06:22 - INFO - root -   Epoch: [73/300][0/84], lr: 0.00000053 	 loss = 0.3369(0.3369)
2023/10/23 23:07:45 - INFO - root -   Epoch: [73/300][20/84], lr: 0.00000053 	 loss = 0.7187(0.6769)
2023/10/23 23:09:23 - INFO - root -   Epoch: [73/300][40/84], lr: 0.00000053 	 loss = 0.8069(0.6807)
2023/10/23 23:10:40 - INFO - root -   Epoch: [73/300][60/84], lr: 0.00000053 	 loss = 0.9725(0.7143)
2023/10/23 23:11:46 - INFO - root -   Epoch: [73/300][80/84], lr: 0.00000053 	 loss = 0.8059(0.7184)
2023/10/23 23:11:48 - INFO - root -   Epoch: [73/300] 	 loss = 0.7337
2023/10/23 23:11:48 - INFO - root -   train_accuracy = 0.4940
2023/10/23 23:12:33 - INFO - root -   Epoch: [74/300][0/84], lr: 0.00000054 	 loss = 0.9138(0.9138)
2023/10/23 23:13:50 - INFO - root -   Epoch: [74/300][20/84], lr: 0.00000054 	 loss = 0.6983(0.7634)
2023/10/23 23:15:37 - INFO - root -   Epoch: [74/300][40/84], lr: 0.00000054 	 loss = 0.4437(0.7505)
2023/10/23 23:16:55 - INFO - root -   Epoch: [74/300][60/84], lr: 0.00000054 	 loss = 0.5780(0.7106)
2023/10/23 23:17:52 - INFO - root -   Epoch: [74/300][80/84], lr: 0.00000054 	 loss = 0.7705(0.7159)
2023/10/23 23:17:59 - INFO - root -   Epoch: [74/300] 	 loss = 0.7154
2023/10/23 23:19:18 - INFO - root -   precision = 0.6047
2023/10/23 23:19:18 - INFO - root -   eval_loss = 0.6790
2023/10/23 23:19:18 - INFO - root -   eval_acc = 0.6047
2023/10/23 23:19:19 - INFO - root -   train_accuracy = 0.5774
2023/10/23 23:19:49 - INFO - root -   Epoch: [75/300][0/84], lr: 0.00000054 	 loss = 0.4199(0.4199)
2023/10/23 23:21:26 - INFO - root -   Epoch: [75/300][20/84], lr: 0.00000054 	 loss = 0.7077(0.7125)
2023/10/23 23:22:32 - INFO - root -   Epoch: [75/300][40/84], lr: 0.00000054 	 loss = 0.6972(0.6914)
2023/10/23 23:24:18 - INFO - root -   Epoch: [75/300][60/84], lr: 0.00000054 	 loss = 0.9558(0.6683)
2023/10/23 23:25:15 - INFO - root -   Epoch: [75/300][80/84], lr: 0.00000054 	 loss = 1.6327(0.7016)
2023/10/23 23:25:17 - INFO - root -   Epoch: [75/300] 	 loss = 0.7031
2023/10/23 23:25:17 - INFO - root -   train_accuracy = 0.5655
2023/10/23 23:25:56 - INFO - root -   Epoch: [76/300][0/84], lr: 0.00000055 	 loss = 0.7079(0.7079)
2023/10/23 23:27:11 - INFO - root -   Epoch: [76/300][20/84], lr: 0.00000055 	 loss = 0.9450(0.6414)
2023/10/23 23:28:43 - INFO - root -   Epoch: [76/300][40/84], lr: 0.00000055 	 loss = 0.4622(0.6794)
2023/10/23 23:30:14 - INFO - root -   Epoch: [76/300][60/84], lr: 0.00000055 	 loss = 0.7717(0.7012)
2023/10/23 23:31:15 - INFO - root -   Epoch: [76/300][80/84], lr: 0.00000055 	 loss = 0.6448(0.7239)
2023/10/23 23:31:21 - INFO - root -   Epoch: [76/300] 	 loss = 0.7312
2023/10/23 23:31:21 - INFO - root -   train_accuracy = 0.5179
2023/10/23 23:32:15 - INFO - root -   Epoch: [77/300][0/84], lr: 0.00000055 	 loss = 0.3616(0.3616)
2023/10/23 23:33:26 - INFO - root -   Epoch: [77/300][20/84], lr: 0.00000055 	 loss = 1.2026(0.7051)
2023/10/23 23:35:03 - INFO - root -   Epoch: [77/300][40/84], lr: 0.00000055 	 loss = 0.8055(0.7049)
2023/10/23 23:36:19 - INFO - root -   Epoch: [77/300][60/84], lr: 0.00000055 	 loss = 0.6401(0.6922)
2023/10/23 23:37:35 - INFO - root -   Epoch: [77/300][80/84], lr: 0.00000055 	 loss = 0.9127(0.7242)
2023/10/23 23:37:36 - INFO - root -   Epoch: [77/300] 	 loss = 0.7223
2023/10/23 23:37:36 - INFO - root -   train_accuracy = 0.4881
2023/10/23 23:38:06 - INFO - root -   Epoch: [78/300][0/84], lr: 0.00000056 	 loss = 0.5523(0.5523)
2023/10/23 23:39:53 - INFO - root -   Epoch: [78/300][20/84], lr: 0.00000056 	 loss = 0.5710(0.7442)
2023/10/23 23:40:53 - INFO - root -   Epoch: [78/300][40/84], lr: 0.00000056 	 loss = 0.8612(0.7658)
2023/10/23 23:42:38 - INFO - root -   Epoch: [78/300][60/84], lr: 0.00000056 	 loss = 0.6509(0.7526)
2023/10/23 23:43:36 - INFO - root -   Epoch: [78/300][80/84], lr: 0.00000056 	 loss = 0.3701(0.7750)
2023/10/23 23:43:42 - INFO - root -   Epoch: [78/300] 	 loss = 0.7737
2023/10/23 23:43:42 - INFO - root -   train_accuracy = 0.4583
2023/10/23 23:44:20 - INFO - root -   Epoch: [79/300][0/84], lr: 0.00000057 	 loss = 0.7894(0.7894)
2023/10/23 23:45:45 - INFO - root -   Epoch: [79/300][20/84], lr: 0.00000057 	 loss = 0.7594(0.6560)
2023/10/23 23:46:59 - INFO - root -   Epoch: [79/300][40/84], lr: 0.00000057 	 loss = 0.9139(0.7061)
2023/10/23 23:48:33 - INFO - root -   Epoch: [79/300][60/84], lr: 0.00000057 	 loss = 0.3309(0.6957)
2023/10/23 23:49:40 - INFO - root -   Epoch: [79/300][80/84], lr: 0.00000057 	 loss = 0.2283(0.6893)
2023/10/23 23:49:42 - INFO - root -   Epoch: [79/300] 	 loss = 0.6923
2023/10/23 23:51:01 - INFO - root -   precision = 0.4419
2023/10/23 23:51:01 - INFO - root -   eval_loss = 0.6775
2023/10/23 23:51:01 - INFO - root -   eval_acc = 0.4419
2023/10/23 23:51:02 - INFO - root -   train_accuracy = 0.5595
2023/10/23 23:51:40 - INFO - root -   Epoch: [80/300][0/84], lr: 0.00000057 	 loss = 0.2490(0.2490)
2023/10/23 23:52:56 - INFO - root -   Epoch: [80/300][20/84], lr: 0.00000057 	 loss = 0.7380(0.7016)
2023/10/23 23:54:27 - INFO - root -   Epoch: [80/300][40/84], lr: 0.00000057 	 loss = 0.6522(0.7122)
2023/10/23 23:56:07 - INFO - root -   Epoch: [80/300][60/84], lr: 0.00000057 	 loss = 0.8828(0.7208)
2023/10/23 23:56:53 - INFO - root -   Epoch: [80/300][80/84], lr: 0.00000057 	 loss = 0.6239(0.7248)
2023/10/23 23:56:58 - INFO - root -   Epoch: [80/300] 	 loss = 0.7262
2023/10/23 23:56:58 - INFO - root -   train_accuracy = 0.5238
2023/10/23 23:57:27 - INFO - root -   Epoch: [81/300][0/84], lr: 0.00000058 	 loss = 0.6193(0.6193)
2023/10/23 23:58:59 - INFO - root -   Epoch: [81/300][20/84], lr: 0.00000058 	 loss = 0.6706(0.7834)
2023/10/24 00:00:30 - INFO - root -   Epoch: [81/300][40/84], lr: 0.00000058 	 loss = 0.7402(0.7258)
2023/10/24 00:01:50 - INFO - root -   Epoch: [81/300][60/84], lr: 0.00000058 	 loss = 0.6321(0.7239)
2023/10/24 00:02:55 - INFO - root -   Epoch: [81/300][80/84], lr: 0.00000058 	 loss = 0.8418(0.7360)
2023/10/24 00:02:59 - INFO - root -   Epoch: [81/300] 	 loss = 0.7477
2023/10/24 00:02:59 - INFO - root -   train_accuracy = 0.5298
2023/10/24 00:03:29 - INFO - root -   Epoch: [82/300][0/84], lr: 0.00000058 	 loss = 0.5259(0.5259)
2023/10/24 00:04:51 - INFO - root -   Epoch: [82/300][20/84], lr: 0.00000058 	 loss = 0.7429(0.8748)
2023/10/24 00:06:16 - INFO - root -   Epoch: [82/300][40/84], lr: 0.00000058 	 loss = 0.4392(0.8176)
2023/10/24 00:07:35 - INFO - root -   Epoch: [82/300][60/84], lr: 0.00000058 	 loss = 0.4761(0.8033)
2023/10/24 00:08:50 - INFO - root -   Epoch: [82/300][80/84], lr: 0.00000058 	 loss = 0.7553(0.7917)
2023/10/24 00:08:54 - INFO - root -   Epoch: [82/300] 	 loss = 0.7939
2023/10/24 00:08:54 - INFO - root -   train_accuracy = 0.4702
2023/10/24 00:09:31 - INFO - root -   Epoch: [83/300][0/84], lr: 0.00000059 	 loss = 0.5384(0.5384)
2023/10/24 00:10:45 - INFO - root -   Epoch: [83/300][20/84], lr: 0.00000059 	 loss = 0.6995(0.7089)
2023/10/24 00:12:01 - INFO - root -   Epoch: [83/300][40/84], lr: 0.00000059 	 loss = 0.9674(0.7687)
2023/10/24 00:13:39 - INFO - root -   Epoch: [83/300][60/84], lr: 0.00000059 	 loss = 0.6464(0.7558)
2023/10/24 00:14:46 - INFO - root -   Epoch: [83/300][80/84], lr: 0.00000059 	 loss = 0.5748(0.7368)
2023/10/24 00:14:50 - INFO - root -   Epoch: [83/300] 	 loss = 0.7405
2023/10/24 00:14:50 - INFO - root -   train_accuracy = 0.4940
2023/10/24 00:15:19 - INFO - root -   Epoch: [84/300][0/84], lr: 0.00000060 	 loss = 0.4857(0.4857)
2023/10/24 00:16:41 - INFO - root -   Epoch: [84/300][20/84], lr: 0.00000060 	 loss = 0.9415(0.7714)
2023/10/24 00:17:57 - INFO - root -   Epoch: [84/300][40/84], lr: 0.00000060 	 loss = 0.4786(0.8050)
2023/10/24 00:19:41 - INFO - root -   Epoch: [84/300][60/84], lr: 0.00000060 	 loss = 1.0366(0.8248)
2023/10/24 00:20:35 - INFO - root -   Epoch: [84/300][80/84], lr: 0.00000060 	 loss = 0.5680(0.7887)
2023/10/24 00:20:40 - INFO - root -   Epoch: [84/300] 	 loss = 0.7998
2023/10/24 00:21:59 - INFO - root -   precision = 0.6279
2023/10/24 00:21:59 - INFO - root -   eval_loss = 0.6769
2023/10/24 00:21:59 - INFO - root -   eval_acc = 0.6279
2023/10/24 00:22:00 - INFO - root -   train_accuracy = 0.4048
2023/10/24 00:22:49 - INFO - root -   Epoch: [85/300][0/84], lr: 0.00000060 	 loss = 0.9282(0.9282)
2023/10/24 00:24:18 - INFO - root -   Epoch: [85/300][20/84], lr: 0.00000060 	 loss = 0.8973(0.7316)
2023/10/24 00:25:37 - INFO - root -   Epoch: [85/300][40/84], lr: 0.00000060 	 loss = 0.8665(0.7399)
2023/10/24 00:27:07 - INFO - root -   Epoch: [85/300][60/84], lr: 0.00000060 	 loss = 0.9637(0.7246)
2023/10/24 00:28:10 - INFO - root -   Epoch: [85/300][80/84], lr: 0.00000060 	 loss = 0.8009(0.7155)
2023/10/24 00:28:11 - INFO - root -   Epoch: [85/300] 	 loss = 0.7315
2023/10/24 00:28:11 - INFO - root -   train_accuracy = 0.5179
2023/10/24 00:28:41 - INFO - root -   Epoch: [86/300][0/84], lr: 0.00000061 	 loss = 0.6758(0.6758)
2023/10/24 00:30:22 - INFO - root -   Epoch: [86/300][20/84], lr: 0.00000061 	 loss = 0.8636(0.7734)
2023/10/24 00:31:33 - INFO - root -   Epoch: [86/300][40/84], lr: 0.00000061 	 loss = 0.7458(0.7570)
2023/10/24 00:33:16 - INFO - root -   Epoch: [86/300][60/84], lr: 0.00000061 	 loss = 0.4786(0.7285)
2023/10/24 00:34:03 - INFO - root -   Epoch: [86/300][80/84], lr: 0.00000061 	 loss = 0.7840(0.7413)
2023/10/24 00:34:06 - INFO - root -   Epoch: [86/300] 	 loss = 0.7422
2023/10/24 00:34:06 - INFO - root -   train_accuracy = 0.5060
2023/10/24 00:34:44 - INFO - root -   Epoch: [87/300][0/84], lr: 0.00000061 	 loss = 0.7896(0.7896)
2023/10/24 00:35:58 - INFO - root -   Epoch: [87/300][20/84], lr: 0.00000061 	 loss = 0.6052(0.8476)
2023/10/24 00:37:31 - INFO - root -   Epoch: [87/300][40/84], lr: 0.00000061 	 loss = 1.0590(0.8295)
2023/10/24 00:39:04 - INFO - root -   Epoch: [87/300][60/84], lr: 0.00000061 	 loss = 0.5668(0.7826)
2023/10/24 00:39:56 - INFO - root -   Epoch: [87/300][80/84], lr: 0.00000061 	 loss = 0.6673(0.7742)
2023/10/24 00:39:59 - INFO - root -   Epoch: [87/300] 	 loss = 0.7621
2023/10/24 00:39:59 - INFO - root -   train_accuracy = 0.4167
2023/10/24 00:40:29 - INFO - root -   Epoch: [88/300][0/84], lr: 0.00000062 	 loss = 0.5519(0.5519)
2023/10/24 00:41:51 - INFO - root -   Epoch: [88/300][20/84], lr: 0.00000062 	 loss = 0.7105(0.7678)
2023/10/24 00:43:06 - INFO - root -   Epoch: [88/300][40/84], lr: 0.00000062 	 loss = 0.4945(0.7494)
2023/10/24 00:44:37 - INFO - root -   Epoch: [88/300][60/84], lr: 0.00000062 	 loss = 1.1269(0.7337)
2023/10/24 00:45:41 - INFO - root -   Epoch: [88/300][80/84], lr: 0.00000062 	 loss = 0.7126(0.7307)
2023/10/24 00:45:45 - INFO - root -   Epoch: [88/300] 	 loss = 0.7297
2023/10/24 00:45:45 - INFO - root -   train_accuracy = 0.5119
2023/10/24 00:46:22 - INFO - root -   Epoch: [89/300][0/84], lr: 0.00000062 	 loss = 0.3544(0.3544)
2023/10/24 00:47:46 - INFO - root -   Epoch: [89/300][20/84], lr: 0.00000062 	 loss = 0.6285(0.7135)
2023/10/24 00:49:26 - INFO - root -   Epoch: [89/300][40/84], lr: 0.00000062 	 loss = 0.6698(0.7178)
2023/10/24 00:50:52 - INFO - root -   Epoch: [89/300][60/84], lr: 0.00000062 	 loss = 0.7905(0.7221)
2023/10/24 00:51:41 - INFO - root -   Epoch: [89/300][80/84], lr: 0.00000062 	 loss = 1.0330(0.7379)
2023/10/24 00:51:43 - INFO - root -   Epoch: [89/300] 	 loss = 0.7377
2023/10/24 00:53:02 - INFO - root -   precision = 0.5814
2023/10/24 00:53:02 - INFO - root -   eval_loss = 0.6809
2023/10/24 00:53:02 - INFO - root -   eval_acc = 0.5814
2023/10/24 00:53:03 - INFO - root -   train_accuracy = 0.5119
2023/10/24 00:53:41 - INFO - root -   Epoch: [90/300][0/84], lr: 0.00000063 	 loss = 1.2218(1.2218)
2023/10/24 00:54:55 - INFO - root -   Epoch: [90/300][20/84], lr: 0.00000063 	 loss = 0.3175(0.6925)
2023/10/24 00:56:19 - INFO - root -   Epoch: [90/300][40/84], lr: 0.00000063 	 loss = 1.2812(0.7528)
2023/10/24 00:57:43 - INFO - root -   Epoch: [90/300][60/84], lr: 0.00000063 	 loss = 0.7063(0.7490)
2023/10/24 00:58:56 - INFO - root -   Epoch: [90/300][80/84], lr: 0.00000063 	 loss = 0.9638(0.7238)
2023/10/24 00:59:03 - INFO - root -   Epoch: [90/300] 	 loss = 0.7246
2023/10/24 00:59:03 - INFO - root -   train_accuracy = 0.4821
2023/10/24 00:59:33 - INFO - root -   Epoch: [91/300][0/84], lr: 0.00000064 	 loss = 0.4502(0.4502)
2023/10/24 01:00:49 - INFO - root -   Epoch: [91/300][20/84], lr: 0.00000064 	 loss = 0.5824(0.6845)
2023/10/24 01:02:22 - INFO - root -   Epoch: [91/300][40/84], lr: 0.00000064 	 loss = 0.5154(0.6812)
2023/10/24 01:03:53 - INFO - root -   Epoch: [91/300][60/84], lr: 0.00000064 	 loss = 0.8402(0.7141)
2023/10/24 01:04:54 - INFO - root -   Epoch: [91/300][80/84], lr: 0.00000064 	 loss = 0.6368(0.7140)
2023/10/24 01:04:58 - INFO - root -   Epoch: [91/300] 	 loss = 0.7196
2023/10/24 01:04:58 - INFO - root -   train_accuracy = 0.5357
2023/10/24 01:05:37 - INFO - root -   Epoch: [92/300][0/84], lr: 0.00000064 	 loss = 0.5295(0.5295)
2023/10/24 01:06:58 - INFO - root -   Epoch: [92/300][20/84], lr: 0.00000064 	 loss = 0.5644(0.6907)
2023/10/24 01:08:23 - INFO - root -   Epoch: [92/300][40/84], lr: 0.00000064 	 loss = 0.2676(0.7636)
2023/10/24 01:09:50 - INFO - root -   Epoch: [92/300][60/84], lr: 0.00000064 	 loss = 0.7454(0.7354)
2023/10/24 01:10:58 - INFO - root -   Epoch: [92/300][80/84], lr: 0.00000064 	 loss = 0.7783(0.7305)
2023/10/24 01:11:00 - INFO - root -   Epoch: [92/300] 	 loss = 0.7320
2023/10/24 01:11:00 - INFO - root -   train_accuracy = 0.5060
2023/10/24 01:11:29 - INFO - root -   Epoch: [93/300][0/84], lr: 0.00000065 	 loss = 0.4612(0.4612)
2023/10/24 01:12:52 - INFO - root -   Epoch: [93/300][20/84], lr: 0.00000065 	 loss = 1.3073(0.7222)
2023/10/24 01:14:28 - INFO - root -   Epoch: [93/300][40/84], lr: 0.00000065 	 loss = 1.1634(0.7140)
2023/10/24 01:16:07 - INFO - root -   Epoch: [93/300][60/84], lr: 0.00000065 	 loss = 0.6826(0.7209)
2023/10/24 01:17:07 - INFO - root -   Epoch: [93/300][80/84], lr: 0.00000065 	 loss = 0.9014(0.7297)
2023/10/24 01:17:09 - INFO - root -   Epoch: [93/300] 	 loss = 0.7349
2023/10/24 01:17:09 - INFO - root -   train_accuracy = 0.5060
2023/10/24 01:17:38 - INFO - root -   Epoch: [94/300][0/84], lr: 0.00000065 	 loss = 0.4371(0.4371)
2023/10/24 01:19:01 - INFO - root -   Epoch: [94/300][20/84], lr: 0.00000065 	 loss = 0.7483(0.7785)
2023/10/24 01:20:33 - INFO - root -   Epoch: [94/300][40/84], lr: 0.00000065 	 loss = 0.5891(0.8162)
2023/10/24 01:21:46 - INFO - root -   Epoch: [94/300][60/84], lr: 0.00000065 	 loss = 0.6880(0.7647)
2023/10/24 01:23:09 - INFO - root -   Epoch: [94/300][80/84], lr: 0.00000065 	 loss = 0.8291(0.7726)
2023/10/24 01:23:11 - INFO - root -   Epoch: [94/300] 	 loss = 0.7754
2023/10/24 01:24:30 - INFO - root -   precision = 0.6279
2023/10/24 01:24:30 - INFO - root -   eval_loss = 0.6769
2023/10/24 01:24:30 - INFO - root -   eval_acc = 0.6279
2023/10/24 01:24:31 - INFO - root -   train_accuracy = 0.4583
2023/10/24 01:25:08 - INFO - root -   Epoch: [95/300][0/84], lr: 0.00000066 	 loss = 0.8870(0.8870)
2023/10/24 01:26:32 - INFO - root -   Epoch: [95/300][20/84], lr: 0.00000066 	 loss = 0.8489(0.7750)
2023/10/24 01:28:02 - INFO - root -   Epoch: [95/300][40/84], lr: 0.00000066 	 loss = 0.7413(0.7330)
2023/10/24 01:29:21 - INFO - root -   Epoch: [95/300][60/84], lr: 0.00000066 	 loss = 0.5171(0.7188)
2023/10/24 01:30:33 - INFO - root -   Epoch: [95/300][80/84], lr: 0.00000066 	 loss = 0.4050(0.6922)
2023/10/24 01:30:35 - INFO - root -   Epoch: [95/300] 	 loss = 0.6996
2023/10/24 01:30:35 - INFO - root -   train_accuracy = 0.5476
2023/10/24 01:31:04 - INFO - root -   Epoch: [96/300][0/84], lr: 0.00000067 	 loss = 0.6005(0.6005)
2023/10/24 01:32:42 - INFO - root -   Epoch: [96/300][20/84], lr: 0.00000067 	 loss = 0.3658(0.6007)
2023/10/24 01:33:59 - INFO - root -   Epoch: [96/300][40/84], lr: 0.00000067 	 loss = 1.2386(0.6628)
2023/10/24 01:35:21 - INFO - root -   Epoch: [96/300][60/84], lr: 0.00000067 	 loss = 0.5199(0.6726)
2023/10/24 01:36:30 - INFO - root -   Epoch: [96/300][80/84], lr: 0.00000067 	 loss = 0.7197(0.6871)
2023/10/24 01:36:32 - INFO - root -   Epoch: [96/300] 	 loss = 0.6859
2023/10/24 01:36:32 - INFO - root -   train_accuracy = 0.5536
2023/10/24 01:37:02 - INFO - root -   Epoch: [97/300][0/84], lr: 0.00000067 	 loss = 0.4920(0.4920)
2023/10/24 01:38:40 - INFO - root -   Epoch: [97/300][20/84], lr: 0.00000067 	 loss = 0.8560(0.7482)
2023/10/24 01:39:50 - INFO - root -   Epoch: [97/300][40/84], lr: 0.00000067 	 loss = 0.9489(0.7749)
2023/10/24 01:41:18 - INFO - root -   Epoch: [97/300][60/84], lr: 0.00000067 	 loss = 0.7805(0.7565)
2023/10/24 01:42:24 - INFO - root -   Epoch: [97/300][80/84], lr: 0.00000067 	 loss = 0.7060(0.7424)
2023/10/24 01:42:35 - INFO - root -   Epoch: [97/300] 	 loss = 0.7421
2023/10/24 01:42:35 - INFO - root -   train_accuracy = 0.4881
2023/10/24 01:43:05 - INFO - root -   Epoch: [98/300][0/84], lr: 0.00000068 	 loss = 0.5334(0.5334)
2023/10/24 01:44:34 - INFO - root -   Epoch: [98/300][20/84], lr: 0.00000068 	 loss = 0.5296(0.6941)
2023/10/24 01:45:48 - INFO - root -   Epoch: [98/300][40/84], lr: 0.00000068 	 loss = 0.6970(0.6955)
2023/10/24 01:47:15 - INFO - root -   Epoch: [98/300][60/84], lr: 0.00000068 	 loss = 0.2827(0.7089)
2023/10/24 01:48:22 - INFO - root -   Epoch: [98/300][80/84], lr: 0.00000068 	 loss = 0.5072(0.7109)
2023/10/24 01:48:29 - INFO - root -   Epoch: [98/300] 	 loss = 0.7040
2023/10/24 01:48:29 - INFO - root -   train_accuracy = 0.5119
2023/10/24 01:49:00 - INFO - root -   Epoch: [99/300][0/84], lr: 0.00000068 	 loss = 1.1921(1.1921)
2023/10/24 01:50:20 - INFO - root -   Epoch: [99/300][20/84], lr: 0.00000068 	 loss = 1.0385(0.7023)
2023/10/24 01:51:58 - INFO - root -   Epoch: [99/300][40/84], lr: 0.00000068 	 loss = 0.5641(0.7204)
2023/10/24 01:53:15 - INFO - root -   Epoch: [99/300][60/84], lr: 0.00000068 	 loss = 0.5764(0.6916)
2023/10/24 01:54:25 - INFO - root -   Epoch: [99/300][80/84], lr: 0.00000068 	 loss = 0.4568(0.7112)
2023/10/24 01:54:32 - INFO - root -   Epoch: [99/300] 	 loss = 0.7120
2023/10/24 01:55:51 - INFO - root -   precision = 0.5581
2023/10/24 01:55:51 - INFO - root -   eval_loss = 0.6988
2023/10/24 01:55:51 - INFO - root -   eval_acc = 0.5581
2023/10/24 01:55:52 - INFO - root -   train_accuracy = 0.5655
2023/10/24 01:56:22 - INFO - root -   Epoch: [100/300][0/84], lr: 0.00000069 	 loss = 0.3179(0.3179)
2023/10/24 01:58:00 - INFO - root -   Epoch: [100/300][20/84], lr: 0.00000069 	 loss = 0.7714(0.7942)
2023/10/24 01:59:18 - INFO - root -   Epoch: [100/300][40/84], lr: 0.00000069 	 loss = 0.4676(0.7750)
2023/10/24 02:00:58 - INFO - root -   Epoch: [100/300][60/84], lr: 0.00000069 	 loss = 0.3886(0.7359)
2023/10/24 02:02:01 - INFO - root -   Epoch: [100/300][80/84], lr: 0.00000069 	 loss = 0.9211(0.7411)
2023/10/24 02:02:06 - INFO - root -   Epoch: [100/300] 	 loss = 0.7447
2023/10/24 02:02:06 - INFO - root -   train_accuracy = 0.5417
2023/10/24 02:02:47 - INFO - root -   Epoch: [101/300][0/84], lr: 0.00000070 	 loss = 1.0449(1.0449)
2023/10/24 02:04:11 - INFO - root -   Epoch: [101/300][20/84], lr: 0.00000070 	 loss = 0.3144(0.8510)
2023/10/24 02:05:39 - INFO - root -   Epoch: [101/300][40/84], lr: 0.00000070 	 loss = 0.8225(0.7997)
2023/10/24 02:07:02 - INFO - root -   Epoch: [101/300][60/84], lr: 0.00000070 	 loss = 0.7255(0.7747)
2023/10/24 02:07:56 - INFO - root -   Epoch: [101/300][80/84], lr: 0.00000070 	 loss = 0.5063(0.7469)
2023/10/24 02:08:01 - INFO - root -   Epoch: [101/300] 	 loss = 0.7501
2023/10/24 02:08:01 - INFO - root -   train_accuracy = 0.5238
2023/10/24 02:08:40 - INFO - root -   Epoch: [102/300][0/84], lr: 0.00000070 	 loss = 0.6378(0.6378)
2023/10/24 02:09:55 - INFO - root -   Epoch: [102/300][20/84], lr: 0.00000070 	 loss = 0.4146(0.8045)
2023/10/24 02:11:35 - INFO - root -   Epoch: [102/300][40/84], lr: 0.00000070 	 loss = 1.2432(0.7880)
2023/10/24 02:12:58 - INFO - root -   Epoch: [102/300][60/84], lr: 0.00000070 	 loss = 0.6287(0.7750)
2023/10/24 02:14:13 - INFO - root -   Epoch: [102/300][80/84], lr: 0.00000070 	 loss = 0.6988(0.7789)
2023/10/24 02:14:14 - INFO - root -   Epoch: [102/300] 	 loss = 0.7743
2023/10/24 02:14:14 - INFO - root -   train_accuracy = 0.4524
2023/10/24 02:14:52 - INFO - root -   Epoch: [103/300][0/84], lr: 0.00000071 	 loss = 0.5317(0.5317)
2023/10/24 02:16:21 - INFO - root -   Epoch: [103/300][20/84], lr: 0.00000071 	 loss = 0.7862(0.7824)
2023/10/24 02:17:32 - INFO - root -   Epoch: [103/300][40/84], lr: 0.00000071 	 loss = 0.6084(0.7702)
2023/10/24 02:19:13 - INFO - root -   Epoch: [103/300][60/84], lr: 0.00000071 	 loss = 0.4087(0.7291)
2023/10/24 02:20:17 - INFO - root -   Epoch: [103/300][80/84], lr: 0.00000071 	 loss = 0.8593(0.7318)
2023/10/24 02:20:22 - INFO - root -   Epoch: [103/300] 	 loss = 0.7358
2023/10/24 02:20:22 - INFO - root -   train_accuracy = 0.4881
2023/10/24 02:20:51 - INFO - root -   Epoch: [104/300][0/84], lr: 0.00000071 	 loss = 0.5464(0.5464)
2023/10/24 02:22:23 - INFO - root -   Epoch: [104/300][20/84], lr: 0.00000071 	 loss = 0.4560(0.6933)
2023/10/24 02:23:50 - INFO - root -   Epoch: [104/300][40/84], lr: 0.00000071 	 loss = 0.7277(0.6921)
2023/10/24 02:25:09 - INFO - root -   Epoch: [104/300][60/84], lr: 0.00000071 	 loss = 1.2526(0.7094)
2023/10/24 02:26:17 - INFO - root -   Epoch: [104/300][80/84], lr: 0.00000071 	 loss = 0.4957(0.7031)
2023/10/24 02:26:24 - INFO - root -   Epoch: [104/300] 	 loss = 0.7043
2023/10/24 02:27:42 - INFO - root -   precision = 0.5581
2023/10/24 02:27:42 - INFO - root -   eval_loss = 0.6822
2023/10/24 02:27:42 - INFO - root -   eval_acc = 0.5581
2023/10/24 02:27:43 - INFO - root -   train_accuracy = 0.5417
2023/10/24 02:28:13 - INFO - root -   Epoch: [105/300][0/84], lr: 0.00000072 	 loss = 0.6913(0.6913)
2023/10/24 02:29:37 - INFO - root -   Epoch: [105/300][20/84], lr: 0.00000072 	 loss = 0.6538(0.7419)
2023/10/24 02:30:51 - INFO - root -   Epoch: [105/300][40/84], lr: 0.00000072 	 loss = 0.8159(0.7632)
2023/10/24 02:32:15 - INFO - root -   Epoch: [105/300][60/84], lr: 0.00000072 	 loss = 0.4184(0.7383)
2023/10/24 02:33:27 - INFO - root -   Epoch: [105/300][80/84], lr: 0.00000072 	 loss = 0.4785(0.7484)
2023/10/24 02:33:31 - INFO - root -   Epoch: [105/300] 	 loss = 0.7445
2023/10/24 02:33:31 - INFO - root -   train_accuracy = 0.5238
2023/10/24 02:34:01 - INFO - root -   Epoch: [106/300][0/84], lr: 0.00000072 	 loss = 0.4916(0.4916)
2023/10/24 02:35:25 - INFO - root -   Epoch: [106/300][20/84], lr: 0.00000072 	 loss = 0.6877(0.7517)
2023/10/24 02:37:03 - INFO - root -   Epoch: [106/300][40/84], lr: 0.00000072 	 loss = 0.6237(0.7426)
2023/10/24 02:38:18 - INFO - root -   Epoch: [106/300][60/84], lr: 0.00000072 	 loss = 0.5230(0.7166)
2023/10/24 02:39:26 - INFO - root -   Epoch: [106/300][80/84], lr: 0.00000072 	 loss = 0.7954(0.7017)
2023/10/24 02:39:29 - INFO - root -   Epoch: [106/300] 	 loss = 0.7000
2023/10/24 02:39:29 - INFO - root -   train_accuracy = 0.5833
2023/10/24 02:40:08 - INFO - root -   Epoch: [107/300][0/84], lr: 0.00000073 	 loss = 0.8467(0.8467)
2023/10/24 02:41:26 - INFO - root -   Epoch: [107/300][20/84], lr: 0.00000073 	 loss = 0.7341(0.7465)
2023/10/24 02:43:16 - INFO - root -   Epoch: [107/300][40/84], lr: 0.00000073 	 loss = 0.7029(0.7584)
2023/10/24 02:44:37 - INFO - root -   Epoch: [107/300][60/84], lr: 0.00000073 	 loss = 1.0203(0.7334)
2023/10/24 02:45:32 - INFO - root -   Epoch: [107/300][80/84], lr: 0.00000073 	 loss = 0.4191(0.7262)
2023/10/24 02:45:33 - INFO - root -   Epoch: [107/300] 	 loss = 0.7332
2023/10/24 02:45:33 - INFO - root -   train_accuracy = 0.5536
2023/10/24 02:46:11 - INFO - root -   Epoch: [108/300][0/84], lr: 0.00000074 	 loss = 0.5638(0.5638)
2023/10/24 02:47:35 - INFO - root -   Epoch: [108/300][20/84], lr: 0.00000074 	 loss = 0.8876(0.8069)
2023/10/24 02:48:49 - INFO - root -   Epoch: [108/300][40/84], lr: 0.00000074 	 loss = 0.7194(0.7856)
2023/10/24 02:50:19 - INFO - root -   Epoch: [108/300][60/84], lr: 0.00000074 	 loss = 0.8817(0.7398)
2023/10/24 02:51:16 - INFO - root -   Epoch: [108/300][80/84], lr: 0.00000074 	 loss = 0.8943(0.7430)
2023/10/24 02:51:22 - INFO - root -   Epoch: [108/300] 	 loss = 0.7488
2023/10/24 02:51:22 - INFO - root -   train_accuracy = 0.4762
2023/10/24 02:51:52 - INFO - root -   Epoch: [109/300][0/84], lr: 0.00000074 	 loss = 0.4273(0.4273)
2023/10/24 02:53:06 - INFO - root -   Epoch: [109/300][20/84], lr: 0.00000074 	 loss = 0.6052(0.7152)
2023/10/24 02:54:40 - INFO - root -   Epoch: [109/300][40/84], lr: 0.00000074 	 loss = 0.3277(0.7100)
2023/10/24 02:56:00 - INFO - root -   Epoch: [109/300][60/84], lr: 0.00000074 	 loss = 0.8229(0.7206)
2023/10/24 02:57:21 - INFO - root -   Epoch: [109/300][80/84], lr: 0.00000074 	 loss = 0.8087(0.7240)
2023/10/24 02:57:22 - INFO - root -   Epoch: [109/300] 	 loss = 0.7250
2023/10/24 02:58:41 - INFO - root -   precision = 0.5814
2023/10/24 02:58:41 - INFO - root -   eval_loss = 0.6906
2023/10/24 02:58:41 - INFO - root -   eval_acc = 0.5814
2023/10/24 02:58:42 - INFO - root -   train_accuracy = 0.5417
2023/10/24 02:59:12 - INFO - root -   Epoch: [110/300][0/84], lr: 0.00000075 	 loss = 0.6402(0.6402)
2023/10/24 03:00:34 - INFO - root -   Epoch: [110/300][20/84], lr: 0.00000075 	 loss = 0.6345(0.7286)
2023/10/24 03:02:02 - INFO - root -   Epoch: [110/300][40/84], lr: 0.00000075 	 loss = 0.3437(0.6914)
2023/10/24 03:03:45 - INFO - root -   Epoch: [110/300][60/84], lr: 0.00000075 	 loss = 0.3201(0.7030)
2023/10/24 03:04:47 - INFO - root -   Epoch: [110/300][80/84], lr: 0.00000075 	 loss = 0.4337(0.7365)
2023/10/24 03:04:50 - INFO - root -   Epoch: [110/300] 	 loss = 0.7383
2023/10/24 03:04:50 - INFO - root -   train_accuracy = 0.5417
2023/10/24 03:05:21 - INFO - root -   Epoch: [111/300][0/84], lr: 0.00000075 	 loss = 0.2298(0.2298)
2023/10/24 03:06:50 - INFO - root -   Epoch: [111/300][20/84], lr: 0.00000075 	 loss = 0.7231(0.6664)
2023/10/24 03:08:06 - INFO - root -   Epoch: [111/300][40/84], lr: 0.00000075 	 loss = 0.6324(0.6988)
2023/10/24 03:09:35 - INFO - root -   Epoch: [111/300][60/84], lr: 0.00000075 	 loss = 1.0923(0.7213)
2023/10/24 03:10:48 - INFO - root -   Epoch: [111/300][80/84], lr: 0.00000075 	 loss = 1.0345(0.7464)
2023/10/24 03:10:49 - INFO - root -   Epoch: [111/300] 	 loss = 0.7473
2023/10/24 03:10:49 - INFO - root -   train_accuracy = 0.5119
2023/10/24 03:11:19 - INFO - root -   Epoch: [112/300][0/84], lr: 0.00000076 	 loss = 0.3303(0.3303)
2023/10/24 03:12:58 - INFO - root -   Epoch: [112/300][20/84], lr: 0.00000076 	 loss = 0.8964(0.6991)
2023/10/24 03:14:34 - INFO - root -   Epoch: [112/300][40/84], lr: 0.00000076 	 loss = 0.5501(0.7017)
2023/10/24 03:15:53 - INFO - root -   Epoch: [112/300][60/84], lr: 0.00000076 	 loss = 0.4184(0.6931)
2023/10/24 03:16:56 - INFO - root -   Epoch: [112/300][80/84], lr: 0.00000076 	 loss = 0.5561(0.7097)
2023/10/24 03:16:59 - INFO - root -   Epoch: [112/300] 	 loss = 0.7102
2023/10/24 03:16:59 - INFO - root -   train_accuracy = 0.5595
2023/10/24 03:17:38 - INFO - root -   Epoch: [113/300][0/84], lr: 0.00000077 	 loss = 0.7995(0.7995)
2023/10/24 03:19:08 - INFO - root -   Epoch: [113/300][20/84], lr: 0.00000077 	 loss = 0.7297(0.8613)
2023/10/24 03:20:14 - INFO - root -   Epoch: [113/300][40/84], lr: 0.00000077 	 loss = 0.5820(0.7794)
2023/10/24 03:22:00 - INFO - root -   Epoch: [113/300][60/84], lr: 0.00000077 	 loss = 0.9126(0.7603)
2023/10/24 03:22:49 - INFO - root -   Epoch: [113/300][80/84], lr: 0.00000077 	 loss = 1.4602(0.7347)
2023/10/24 03:22:55 - INFO - root -   Epoch: [113/300] 	 loss = 0.7366
2023/10/24 03:22:55 - INFO - root -   train_accuracy = 0.5714
2023/10/24 03:23:26 - INFO - root -   Epoch: [114/300][0/84], lr: 0.00000077 	 loss = 0.1835(0.1835)
2023/10/24 03:24:48 - INFO - root -   Epoch: [114/300][20/84], lr: 0.00000077 	 loss = 0.3070(0.6919)
2023/10/24 03:26:37 - INFO - root -   Epoch: [114/300][40/84], lr: 0.00000077 	 loss = 0.5318(0.6797)
2023/10/24 03:27:36 - INFO - root -   Epoch: [114/300][60/84], lr: 0.00000077 	 loss = 0.6451(0.6610)
2023/10/24 03:28:47 - INFO - root -   Epoch: [114/300][80/84], lr: 0.00000077 	 loss = 0.6143(0.6802)
2023/10/24 03:28:48 - INFO - root -   Epoch: [114/300] 	 loss = 0.6887
2023/10/24 03:30:07 - INFO - root -   precision = 0.5814
2023/10/24 03:30:07 - INFO - root -   eval_loss = 0.6824
2023/10/24 03:30:07 - INFO - root -   eval_acc = 0.5814
2023/10/24 03:30:08 - INFO - root -   train_accuracy = 0.5000
2023/10/24 03:30:47 - INFO - root -   Epoch: [115/300][0/84], lr: 0.00000078 	 loss = 0.7660(0.7660)
2023/10/24 03:32:01 - INFO - root -   Epoch: [115/300][20/84], lr: 0.00000078 	 loss = 0.8251(0.7380)
2023/10/24 03:33:25 - INFO - root -   Epoch: [115/300][40/84], lr: 0.00000078 	 loss = 0.8048(0.7590)
2023/10/24 03:34:52 - INFO - root -   Epoch: [115/300][60/84], lr: 0.00000078 	 loss = 0.5272(0.7194)
2023/10/24 03:36:16 - INFO - root -   Epoch: [115/300][80/84], lr: 0.00000078 	 loss = 0.7344(0.7243)
2023/10/24 03:36:21 - INFO - root -   Epoch: [115/300] 	 loss = 0.7237
2023/10/24 03:36:21 - INFO - root -   train_accuracy = 0.5238
2023/10/24 03:36:58 - INFO - root -   Epoch: [116/300][0/84], lr: 0.00000078 	 loss = 0.3287(0.3287)
2023/10/24 03:38:27 - INFO - root -   Epoch: [116/300][20/84], lr: 0.00000078 	 loss = 1.1942(0.7588)
2023/10/24 03:39:57 - INFO - root -   Epoch: [116/300][40/84], lr: 0.00000078 	 loss = 0.5257(0.7342)
2023/10/24 03:41:40 - INFO - root -   Epoch: [116/300][60/84], lr: 0.00000078 	 loss = 0.3715(0.7084)
2023/10/24 03:42:32 - INFO - root -   Epoch: [116/300][80/84], lr: 0.00000078 	 loss = 0.5642(0.7146)
2023/10/24 03:42:34 - INFO - root -   Epoch: [116/300] 	 loss = 0.7099
2023/10/24 03:42:34 - INFO - root -   train_accuracy = 0.5179
2023/10/24 03:43:19 - INFO - root -   Epoch: [117/300][0/84], lr: 0.00000079 	 loss = 0.6543(0.6543)
2023/10/24 03:44:33 - INFO - root -   Epoch: [117/300][20/84], lr: 0.00000079 	 loss = 0.7540(0.8058)
2023/10/24 03:46:17 - INFO - root -   Epoch: [117/300][40/84], lr: 0.00000079 	 loss = 0.4123(0.7520)
2023/10/24 03:47:34 - INFO - root -   Epoch: [117/300][60/84], lr: 0.00000079 	 loss = 0.4346(0.7254)
2023/10/24 03:48:33 - INFO - root -   Epoch: [117/300][80/84], lr: 0.00000079 	 loss = 0.6599(0.7301)
2023/10/24 03:48:38 - INFO - root -   Epoch: [117/300] 	 loss = 0.7361
2023/10/24 03:48:38 - INFO - root -   train_accuracy = 0.4881
2023/10/24 03:49:25 - INFO - root -   Epoch: [118/300][0/84], lr: 0.00000080 	 loss = 1.0740(1.0740)
2023/10/24 03:50:40 - INFO - root -   Epoch: [118/300][20/84], lr: 0.00000080 	 loss = 0.9985(0.7626)
2023/10/24 03:52:34 - INFO - root -   Epoch: [118/300][40/84], lr: 0.00000080 	 loss = 0.4986(0.7527)
2023/10/24 03:53:39 - INFO - root -   Epoch: [118/300][60/84], lr: 0.00000080 	 loss = 0.5499(0.7198)
2023/10/24 03:54:43 - INFO - root -   Epoch: [118/300][80/84], lr: 0.00000080 	 loss = 0.8595(0.7197)
2023/10/24 03:54:44 - INFO - root -   Epoch: [118/300] 	 loss = 0.7226
2023/10/24 03:54:44 - INFO - root -   train_accuracy = 0.5238
2023/10/24 03:55:15 - INFO - root -   Epoch: [119/300][0/84], lr: 0.00000080 	 loss = 0.4069(0.4069)
2023/10/24 03:56:37 - INFO - root -   Epoch: [119/300][20/84], lr: 0.00000080 	 loss = 0.6959(0.7035)
2023/10/24 03:57:52 - INFO - root -   Epoch: [119/300][40/84], lr: 0.00000080 	 loss = 1.3236(0.7372)
2023/10/24 03:59:50 - INFO - root -   Epoch: [119/300][60/84], lr: 0.00000080 	 loss = 0.9096(0.7070)
2023/10/24 04:00:47 - INFO - root -   Epoch: [119/300][80/84], lr: 0.00000080 	 loss = 0.7922(0.7172)
2023/10/24 04:00:51 - INFO - root -   Epoch: [119/300] 	 loss = 0.7221
2023/10/24 04:02:10 - INFO - root -   precision = 0.6744
2023/10/24 04:02:10 - INFO - root -   eval_loss = 0.6747
2023/10/24 04:02:10 - INFO - root -   eval_acc = 0.6744
2023/10/24 04:02:12 - INFO - root -   train_accuracy = 0.5298
2023/10/24 04:02:42 - INFO - root -   Epoch: [120/300][0/84], lr: 0.00000081 	 loss = 0.6407(0.6407)
2023/10/24 04:04:04 - INFO - root -   Epoch: [120/300][20/84], lr: 0.00000081 	 loss = 0.6508(0.7250)
2023/10/24 04:05:37 - INFO - root -   Epoch: [120/300][40/84], lr: 0.00000081 	 loss = 0.5212(0.7471)
2023/10/24 04:07:15 - INFO - root -   Epoch: [120/300][60/84], lr: 0.00000081 	 loss = 0.6743(0.7139)
2023/10/24 04:08:23 - INFO - root -   Epoch: [120/300][80/84], lr: 0.00000081 	 loss = 0.4375(0.7224)
2023/10/24 04:08:24 - INFO - root -   Epoch: [120/300] 	 loss = 0.7296
2023/10/24 04:08:24 - INFO - root -   train_accuracy = 0.5357
2023/10/24 04:09:03 - INFO - root -   Epoch: [121/300][0/84], lr: 0.00000081 	 loss = 0.9687(0.9687)
2023/10/24 04:10:15 - INFO - root -   Epoch: [121/300][20/84], lr: 0.00000081 	 loss = 0.5922(0.7859)
2023/10/24 04:11:59 - INFO - root -   Epoch: [121/300][40/84], lr: 0.00000081 	 loss = 0.6755(0.7413)
2023/10/24 04:13:14 - INFO - root -   Epoch: [121/300][60/84], lr: 0.00000081 	 loss = 0.2876(0.7254)
2023/10/24 04:14:28 - INFO - root -   Epoch: [121/300][80/84], lr: 0.00000081 	 loss = 0.2925(0.7136)
2023/10/24 04:14:30 - INFO - root -   Epoch: [121/300] 	 loss = 0.7171
2023/10/24 04:14:30 - INFO - root -   train_accuracy = 0.5298
2023/10/24 04:15:00 - INFO - root -   Epoch: [122/300][0/84], lr: 0.00000082 	 loss = 0.2506(0.2506)
2023/10/24 04:16:32 - INFO - root -   Epoch: [122/300][20/84], lr: 0.00000082 	 loss = 0.5952(0.8181)
2023/10/24 04:17:47 - INFO - root -   Epoch: [122/300][40/84], lr: 0.00000082 	 loss = 0.7097(0.7610)
2023/10/24 04:19:30 - INFO - root -   Epoch: [122/300][60/84], lr: 0.00000082 	 loss = 0.3756(0.7305)
2023/10/24 04:20:29 - INFO - root -   Epoch: [122/300][80/84], lr: 0.00000082 	 loss = 0.5611(0.7093)
2023/10/24 04:20:35 - INFO - root -   Epoch: [122/300] 	 loss = 0.7079
2023/10/24 04:20:35 - INFO - root -   train_accuracy = 0.5417
2023/10/24 04:21:13 - INFO - root -   Epoch: [123/300][0/84], lr: 0.00000082 	 loss = 0.5909(0.5909)
2023/10/24 04:22:25 - INFO - root -   Epoch: [123/300][20/84], lr: 0.00000082 	 loss = 0.6471(0.7333)
2023/10/24 04:24:12 - INFO - root -   Epoch: [123/300][40/84], lr: 0.00000082 	 loss = 0.8716(0.7388)
2023/10/24 04:25:20 - INFO - root -   Epoch: [123/300][60/84], lr: 0.00000082 	 loss = 0.7251(0.7281)
2023/10/24 04:26:28 - INFO - root -   Epoch: [123/300][80/84], lr: 0.00000082 	 loss = 0.6584(0.7226)
2023/10/24 04:26:30 - INFO - root -   Epoch: [123/300] 	 loss = 0.7255
2023/10/24 04:26:30 - INFO - root -   train_accuracy = 0.5179
2023/10/24 04:27:00 - INFO - root -   Epoch: [124/300][0/84], lr: 0.00000083 	 loss = 0.3061(0.3061)
2023/10/24 04:28:22 - INFO - root -   Epoch: [124/300][20/84], lr: 0.00000083 	 loss = 0.6841(0.7839)
2023/10/24 04:29:53 - INFO - root -   Epoch: [124/300][40/84], lr: 0.00000083 	 loss = 0.7608(0.7390)
2023/10/24 04:31:07 - INFO - root -   Epoch: [124/300][60/84], lr: 0.00000083 	 loss = 0.8078(0.7341)
2023/10/24 04:32:30 - INFO - root -   Epoch: [124/300][80/84], lr: 0.00000083 	 loss = 0.4863(0.7223)
2023/10/24 04:32:32 - INFO - root -   Epoch: [124/300] 	 loss = 0.7181
2023/10/24 04:33:50 - INFO - root -   precision = 0.6512
2023/10/24 04:33:50 - INFO - root -   eval_loss = 0.6760
2023/10/24 04:33:50 - INFO - root -   eval_acc = 0.6512
2023/10/24 04:33:51 - INFO - root -   train_accuracy = 0.5476
2023/10/24 04:34:21 - INFO - root -   Epoch: [125/300][0/84], lr: 0.00000084 	 loss = 0.2784(0.2784)
2023/10/24 04:35:43 - INFO - root -   Epoch: [125/300][20/84], lr: 0.00000084 	 loss = 0.7393(0.7546)
2023/10/24 04:37:22 - INFO - root -   Epoch: [125/300][40/84], lr: 0.00000084 	 loss = 1.3004(0.7885)
2023/10/24 04:38:36 - INFO - root -   Epoch: [125/300][60/84], lr: 0.00000084 	 loss = 0.7242(0.7587)
2023/10/24 04:39:43 - INFO - root -   Epoch: [125/300][80/84], lr: 0.00000084 	 loss = 0.4943(0.7577)
2023/10/24 04:39:45 - INFO - root -   Epoch: [125/300] 	 loss = 0.7508
2023/10/24 04:39:45 - INFO - root -   train_accuracy = 0.5119
2023/10/24 04:40:14 - INFO - root -   Epoch: [126/300][0/84], lr: 0.00000084 	 loss = 0.7478(0.7478)
2023/10/24 04:41:37 - INFO - root -   Epoch: [126/300][20/84], lr: 0.00000084 	 loss = 0.9889(0.7311)
2023/10/24 04:42:51 - INFO - root -   Epoch: [126/300][40/84], lr: 0.00000084 	 loss = 0.4997(0.7363)
2023/10/24 04:44:13 - INFO - root -   Epoch: [126/300][60/84], lr: 0.00000084 	 loss = 0.6230(0.7084)
2023/10/24 04:45:38 - INFO - root -   Epoch: [126/300][80/84], lr: 0.00000084 	 loss = 0.5252(0.7104)
2023/10/24 04:45:39 - INFO - root -   Epoch: [126/300] 	 loss = 0.7107
2023/10/24 04:45:39 - INFO - root -   train_accuracy = 0.5238
2023/10/24 04:46:18 - INFO - root -   Epoch: [127/300][0/84], lr: 0.00000085 	 loss = 0.4712(0.4712)
2023/10/24 04:47:30 - INFO - root -   Epoch: [127/300][20/84], lr: 0.00000085 	 loss = 0.7603(0.6581)
2023/10/24 04:48:48 - INFO - root -   Epoch: [127/300][40/84], lr: 0.00000085 	 loss = 0.4057(0.7269)
2023/10/24 04:50:18 - INFO - root -   Epoch: [127/300][60/84], lr: 0.00000085 	 loss = 0.3077(0.7495)
2023/10/24 04:51:25 - INFO - root -   Epoch: [127/300][80/84], lr: 0.00000085 	 loss = 0.7621(0.7487)
2023/10/24 04:51:27 - INFO - root -   Epoch: [127/300] 	 loss = 0.7533
2023/10/24 04:51:27 - INFO - root -   train_accuracy = 0.4702
2023/10/24 04:51:56 - INFO - root -   Epoch: [128/300][0/84], lr: 0.00000085 	 loss = 0.3224(0.3224)
2023/10/24 04:53:34 - INFO - root -   Epoch: [128/300][20/84], lr: 0.00000085 	 loss = 1.2103(0.7020)
2023/10/24 04:54:53 - INFO - root -   Epoch: [128/300][40/84], lr: 0.00000085 	 loss = 0.6996(0.7175)
2023/10/24 04:56:21 - INFO - root -   Epoch: [128/300][60/84], lr: 0.00000085 	 loss = 0.6777(0.6972)
2023/10/24 04:57:28 - INFO - root -   Epoch: [128/300][80/84], lr: 0.00000085 	 loss = 0.6341(0.7025)
2023/10/24 04:57:34 - INFO - root -   Epoch: [128/300] 	 loss = 0.6970
2023/10/24 04:57:34 - INFO - root -   train_accuracy = 0.5655
2023/10/24 04:58:05 - INFO - root -   Epoch: [129/300][0/84], lr: 0.00000086 	 loss = 0.4435(0.4435)
2023/10/24 04:59:26 - INFO - root -   Epoch: [129/300][20/84], lr: 0.00000086 	 loss = 0.6761(0.7931)
2023/10/24 05:00:48 - INFO - root -   Epoch: [129/300][40/84], lr: 0.00000086 	 loss = 0.6600(0.7887)
2023/10/24 05:02:12 - INFO - root -   Epoch: [129/300][60/84], lr: 0.00000086 	 loss = 0.9517(0.7652)
2023/10/24 05:03:27 - INFO - root -   Epoch: [129/300][80/84], lr: 0.00000086 	 loss = 1.1179(0.7305)
2023/10/24 05:03:28 - INFO - root -   Epoch: [129/300] 	 loss = 0.7239
2023/10/24 05:04:47 - INFO - root -   precision = 0.4651
2023/10/24 05:04:47 - INFO - root -   eval_loss = 0.6746
2023/10/24 05:04:47 - INFO - root -   eval_acc = 0.4651
2023/10/24 05:04:48 - INFO - root -   train_accuracy = 0.4940
2023/10/24 05:05:28 - INFO - root -   Epoch: [130/300][0/84], lr: 0.00000087 	 loss = 1.2126(1.2126)
2023/10/24 05:06:46 - INFO - root -   Epoch: [130/300][20/84], lr: 0.00000087 	 loss = 0.7445(0.7488)
2023/10/24 05:08:14 - INFO - root -   Epoch: [130/300][40/84], lr: 0.00000087 	 loss = 0.5494(0.7650)
2023/10/24 05:09:45 - INFO - root -   Epoch: [130/300][60/84], lr: 0.00000087 	 loss = 0.6080(0.7383)
2023/10/24 05:10:38 - INFO - root -   Epoch: [130/300][80/84], lr: 0.00000087 	 loss = 0.8793(0.7246)
2023/10/24 05:10:39 - INFO - root -   Epoch: [130/300] 	 loss = 0.7247
2023/10/24 05:10:39 - INFO - root -   train_accuracy = 0.5417
2023/10/24 05:11:17 - INFO - root -   Epoch: [131/300][0/84], lr: 0.00000087 	 loss = 0.4986(0.4986)
2023/10/24 05:12:41 - INFO - root -   Epoch: [131/300][20/84], lr: 0.00000087 	 loss = 0.5285(0.7394)
2023/10/24 05:14:31 - INFO - root -   Epoch: [131/300][40/84], lr: 0.00000087 	 loss = 0.5092(0.7692)
2023/10/24 05:15:43 - INFO - root -   Epoch: [131/300][60/84], lr: 0.00000087 	 loss = 0.8856(0.7624)
2023/10/24 05:16:48 - INFO - root -   Epoch: [131/300][80/84], lr: 0.00000087 	 loss = 0.5310(0.7398)
2023/10/24 05:16:53 - INFO - root -   Epoch: [131/300] 	 loss = 0.7299
2023/10/24 05:16:53 - INFO - root -   train_accuracy = 0.4940
2023/10/24 05:17:22 - INFO - root -   Epoch: [132/300][0/84], lr: 0.00000088 	 loss = 0.5300(0.5300)
2023/10/24 05:18:38 - INFO - root -   Epoch: [132/300][20/84], lr: 0.00000088 	 loss = 0.4272(0.6409)
2023/10/24 05:20:20 - INFO - root -   Epoch: [132/300][40/84], lr: 0.00000088 	 loss = 0.9796(0.7322)
2023/10/24 05:21:51 - INFO - root -   Epoch: [132/300][60/84], lr: 0.00000088 	 loss = 0.4799(0.7242)
2023/10/24 05:22:49 - INFO - root -   Epoch: [132/300][80/84], lr: 0.00000088 	 loss = 0.3884(0.7027)
2023/10/24 05:22:53 - INFO - root -   Epoch: [132/300] 	 loss = 0.7115
2023/10/24 05:22:53 - INFO - root -   train_accuracy = 0.5714
2023/10/24 05:23:31 - INFO - root -   Epoch: [133/300][0/84], lr: 0.00000088 	 loss = 0.9020(0.9020)
2023/10/24 05:24:51 - INFO - root -   Epoch: [133/300][20/84], lr: 0.00000088 	 loss = 0.6559(0.7052)
2023/10/24 05:26:09 - INFO - root -   Epoch: [133/300][40/84], lr: 0.00000088 	 loss = 0.6727(0.6844)
2023/10/24 05:27:34 - INFO - root -   Epoch: [133/300][60/84], lr: 0.00000088 	 loss = 0.5542(0.6921)
2023/10/24 05:29:01 - INFO - root -   Epoch: [133/300][80/84], lr: 0.00000088 	 loss = 0.5002(0.7014)
2023/10/24 05:29:02 - INFO - root -   Epoch: [133/300] 	 loss = 0.7136
2023/10/24 05:29:02 - INFO - root -   train_accuracy = 0.5476
2023/10/24 05:29:47 - INFO - root -   Epoch: [134/300][0/84], lr: 0.00000089 	 loss = 0.4784(0.4784)
2023/10/24 05:31:04 - INFO - root -   Epoch: [134/300][20/84], lr: 0.00000089 	 loss = 0.8125(0.8109)
2023/10/24 05:32:35 - INFO - root -   Epoch: [134/300][40/84], lr: 0.00000089 	 loss = 0.8570(0.7849)
2023/10/24 05:33:55 - INFO - root -   Epoch: [134/300][60/84], lr: 0.00000089 	 loss = 0.4505(0.7647)
2023/10/24 05:35:04 - INFO - root -   Epoch: [134/300][80/84], lr: 0.00000089 	 loss = 0.6071(0.7485)
2023/10/24 05:35:06 - INFO - root -   Epoch: [134/300] 	 loss = 0.7470
2023/10/24 05:36:25 - INFO - root -   precision = 0.6744
2023/10/24 05:36:25 - INFO - root -   eval_loss = 0.6748
2023/10/24 05:36:25 - INFO - root -   eval_acc = 0.6744
2023/10/24 05:36:26 - INFO - root -   train_accuracy = 0.4821
2023/10/24 05:37:12 - INFO - root -   Epoch: [135/300][0/84], lr: 0.00000090 	 loss = 0.5453(0.5453)
2023/10/24 05:38:19 - INFO - root -   Epoch: [135/300][20/84], lr: 0.00000090 	 loss = 0.9273(0.6936)
2023/10/24 05:40:13 - INFO - root -   Epoch: [135/300][40/84], lr: 0.00000090 	 loss = 0.7490(0.7085)
2023/10/24 05:41:30 - INFO - root -   Epoch: [135/300][60/84], lr: 0.00000090 	 loss = 0.5512(0.7035)
2023/10/24 05:42:29 - INFO - root -   Epoch: [135/300][80/84], lr: 0.00000090 	 loss = 0.7121(0.7231)
2023/10/24 05:42:33 - INFO - root -   Epoch: [135/300] 	 loss = 0.7291
2023/10/24 05:42:33 - INFO - root -   train_accuracy = 0.4940
2023/10/24 05:43:11 - INFO - root -   Epoch: [136/300][0/84], lr: 0.00000090 	 loss = 0.6334(0.6334)
2023/10/24 05:44:34 - INFO - root -   Epoch: [136/300][20/84], lr: 0.00000090 	 loss = 0.6943(0.7626)
2023/10/24 05:45:55 - INFO - root -   Epoch: [136/300][40/84], lr: 0.00000090 	 loss = 0.7285(0.7750)
2023/10/24 05:47:44 - INFO - root -   Epoch: [136/300][60/84], lr: 0.00000090 	 loss = 0.5954(0.7487)
2023/10/24 05:48:36 - INFO - root -   Epoch: [136/300][80/84], lr: 0.00000090 	 loss = 0.5813(0.7390)
2023/10/24 05:48:42 - INFO - root -   Epoch: [136/300] 	 loss = 0.7423
2023/10/24 05:48:42 - INFO - root -   train_accuracy = 0.4881
2023/10/24 05:49:28 - INFO - root -   Epoch: [137/300][0/84], lr: 0.00000091 	 loss = 0.5637(0.5637)
2023/10/24 05:50:33 - INFO - root -   Epoch: [137/300][20/84], lr: 0.00000091 	 loss = 0.7651(0.7126)
2023/10/24 05:52:05 - INFO - root -   Epoch: [137/300][40/84], lr: 0.00000091 	 loss = 0.4871(0.7327)
2023/10/24 05:53:28 - INFO - root -   Epoch: [137/300][60/84], lr: 0.00000091 	 loss = 0.5090(0.7171)
2023/10/24 05:54:32 - INFO - root -   Epoch: [137/300][80/84], lr: 0.00000091 	 loss = 1.2008(0.7166)
2023/10/24 05:54:38 - INFO - root -   Epoch: [137/300] 	 loss = 0.7176
2023/10/24 05:54:38 - INFO - root -   train_accuracy = 0.5417
2023/10/24 05:55:08 - INFO - root -   Epoch: [138/300][0/84], lr: 0.00000091 	 loss = 0.4144(0.4144)
2023/10/24 05:56:23 - INFO - root -   Epoch: [138/300][20/84], lr: 0.00000091 	 loss = 0.5959(0.6849)
2023/10/24 05:58:01 - INFO - root -   Epoch: [138/300][40/84], lr: 0.00000091 	 loss = 0.4413(0.6993)
2023/10/24 05:59:33 - INFO - root -   Epoch: [138/300][60/84], lr: 0.00000091 	 loss = 0.2930(0.6949)
2023/10/24 06:00:35 - INFO - root -   Epoch: [138/300][80/84], lr: 0.00000091 	 loss = 0.7781(0.6728)
2023/10/24 06:00:37 - INFO - root -   Epoch: [138/300] 	 loss = 0.6830
2023/10/24 06:00:37 - INFO - root -   train_accuracy = 0.5774
2023/10/24 06:01:15 - INFO - root -   Epoch: [139/300][0/84], lr: 0.00000092 	 loss = 0.6652(0.6652)
2023/10/24 06:02:37 - INFO - root -   Epoch: [139/300][20/84], lr: 0.00000092 	 loss = 0.4873(0.8150)
2023/10/24 06:04:10 - INFO - root -   Epoch: [139/300][40/84], lr: 0.00000092 	 loss = 0.7144(0.7790)
2023/10/24 06:05:42 - INFO - root -   Epoch: [139/300][60/84], lr: 0.00000092 	 loss = 0.7087(0.7477)
2023/10/24 06:06:35 - INFO - root -   Epoch: [139/300][80/84], lr: 0.00000092 	 loss = 0.6525(0.7311)
2023/10/24 06:06:40 - INFO - root -   Epoch: [139/300] 	 loss = 0.7386
2023/10/24 06:07:59 - INFO - root -   precision = 0.6047
2023/10/24 06:07:59 - INFO - root -   eval_loss = 0.6749
2023/10/24 06:07:59 - INFO - root -   eval_acc = 0.6047
2023/10/24 06:08:00 - INFO - root -   train_accuracy = 0.5179
2023/10/24 06:08:29 - INFO - root -   Epoch: [140/300][0/84], lr: 0.00000093 	 loss = 0.7150(0.7150)
2023/10/24 06:09:58 - INFO - root -   Epoch: [140/300][20/84], lr: 0.00000093 	 loss = 0.4579(0.7772)
2023/10/24 06:11:06 - INFO - root -   Epoch: [140/300][40/84], lr: 0.00000093 	 loss = 0.5039(0.7709)
2023/10/24 06:13:21 - INFO - root -   Epoch: [140/300][60/84], lr: 0.00000093 	 loss = 0.5204(0.7348)
2023/10/24 06:14:08 - INFO - root -   Epoch: [140/300][80/84], lr: 0.00000093 	 loss = 0.3853(0.7242)
2023/10/24 06:14:12 - INFO - root -   Epoch: [140/300] 	 loss = 0.7194
2023/10/24 06:14:12 - INFO - root -   train_accuracy = 0.5476
2023/10/24 06:14:51 - INFO - root -   Epoch: [141/300][0/84], lr: 0.00000093 	 loss = 0.6449(0.6449)
2023/10/24 06:16:13 - INFO - root -   Epoch: [141/300][20/84], lr: 0.00000093 	 loss = 0.9540(0.6482)
2023/10/24 06:17:33 - INFO - root -   Epoch: [141/300][40/84], lr: 0.00000093 	 loss = 0.8406(0.7225)
2023/10/24 06:19:18 - INFO - root -   Epoch: [141/300][60/84], lr: 0.00000093 	 loss = 0.7348(0.7067)
2023/10/24 06:20:20 - INFO - root -   Epoch: [141/300][80/84], lr: 0.00000093 	 loss = 1.0571(0.7161)
2023/10/24 06:20:23 - INFO - root -   Epoch: [141/300] 	 loss = 0.7176
2023/10/24 06:20:23 - INFO - root -   train_accuracy = 0.5417
2023/10/24 06:21:02 - INFO - root -   Epoch: [142/300][0/84], lr: 0.00000094 	 loss = 0.4820(0.4820)
2023/10/24 06:22:23 - INFO - root -   Epoch: [142/300][20/84], lr: 0.00000094 	 loss = 0.4713(0.7807)
2023/10/24 06:23:59 - INFO - root -   Epoch: [142/300][40/84], lr: 0.00000094 	 loss = 0.8439(0.7552)
2023/10/24 06:25:12 - INFO - root -   Epoch: [142/300][60/84], lr: 0.00000094 	 loss = 0.5618(0.7534)
2023/10/24 06:26:29 - INFO - root -   Epoch: [142/300][80/84], lr: 0.00000094 	 loss = 0.7439(0.7355)
2023/10/24 06:26:30 - INFO - root -   Epoch: [142/300] 	 loss = 0.7303
2023/10/24 06:26:30 - INFO - root -   train_accuracy = 0.5238
2023/10/24 06:27:15 - INFO - root -   Epoch: [143/300][0/84], lr: 0.00000094 	 loss = 0.4002(0.4002)
2023/10/24 06:28:45 - INFO - root -   Epoch: [143/300][20/84], lr: 0.00000094 	 loss = 0.4832(0.7108)
2023/10/24 06:30:02 - INFO - root -   Epoch: [143/300][40/84], lr: 0.00000094 	 loss = 0.6599(0.7706)
2023/10/24 06:31:32 - INFO - root -   Epoch: [143/300][60/84], lr: 0.00000094 	 loss = 0.5458(0.7597)
2023/10/24 06:32:42 - INFO - root -   Epoch: [143/300][80/84], lr: 0.00000094 	 loss = 0.5260(0.7539)
2023/10/24 06:32:45 - INFO - root -   Epoch: [143/300] 	 loss = 0.7586
2023/10/24 06:32:45 - INFO - root -   train_accuracy = 0.4583
2023/10/24 06:33:24 - INFO - root -   Epoch: [144/300][0/84], lr: 0.00000095 	 loss = 0.5978(0.5978)
2023/10/24 06:34:47 - INFO - root -   Epoch: [144/300][20/84], lr: 0.00000095 	 loss = 0.8124(0.7154)
2023/10/24 06:36:00 - INFO - root -   Epoch: [144/300][40/84], lr: 0.00000095 	 loss = 0.7423(0.7367)
2023/10/24 06:37:31 - INFO - root -   Epoch: [144/300][60/84], lr: 0.00000095 	 loss = 0.6373(0.7215)
2023/10/24 06:38:38 - INFO - root -   Epoch: [144/300][80/84], lr: 0.00000095 	 loss = 0.9652(0.7120)
2023/10/24 06:38:39 - INFO - root -   Epoch: [144/300] 	 loss = 0.7216
2023/10/24 06:39:58 - INFO - root -   precision = 0.5814
2023/10/24 06:39:58 - INFO - root -   eval_loss = 0.6840
2023/10/24 06:39:58 - INFO - root -   eval_acc = 0.5814
2023/10/24 06:39:59 - INFO - root -   train_accuracy = 0.5060
2023/10/24 06:40:47 - INFO - root -   Epoch: [145/300][0/84], lr: 0.00000095 	 loss = 0.7898(0.7898)
2023/10/24 06:42:03 - INFO - root -   Epoch: [145/300][20/84], lr: 0.00000095 	 loss = 0.7944(0.7108)
2023/10/24 06:43:43 - INFO - root -   Epoch: [145/300][40/84], lr: 0.00000095 	 loss = 0.3602(0.7304)
2023/10/24 06:44:48 - INFO - root -   Epoch: [145/300][60/84], lr: 0.00000095 	 loss = 0.8635(0.7054)
2023/10/24 06:46:03 - INFO - root -   Epoch: [145/300][80/84], lr: 0.00000095 	 loss = 0.6015(0.6947)
2023/10/24 06:46:04 - INFO - root -   Epoch: [145/300] 	 loss = 0.7030
2023/10/24 06:46:04 - INFO - root -   train_accuracy = 0.5238
2023/10/24 06:46:34 - INFO - root -   Epoch: [146/300][0/84], lr: 0.00000096 	 loss = 0.2698(0.2698)
2023/10/24 06:48:05 - INFO - root -   Epoch: [146/300][20/84], lr: 0.00000096 	 loss = 0.9275(0.7137)
2023/10/24 06:49:20 - INFO - root -   Epoch: [146/300][40/84], lr: 0.00000096 	 loss = 0.5853(0.7201)
2023/10/24 06:50:58 - INFO - root -   Epoch: [146/300][60/84], lr: 0.00000096 	 loss = 0.8135(0.7059)
2023/10/24 06:52:01 - INFO - root -   Epoch: [146/300][80/84], lr: 0.00000096 	 loss = 0.9387(0.7191)
2023/10/24 06:52:06 - INFO - root -   Epoch: [146/300] 	 loss = 0.7188
2023/10/24 06:52:06 - INFO - root -   train_accuracy = 0.5238
2023/10/24 06:52:43 - INFO - root -   Epoch: [147/300][0/84], lr: 0.00000097 	 loss = 0.3388(0.3388)
2023/10/24 06:53:59 - INFO - root -   Epoch: [147/300][20/84], lr: 0.00000097 	 loss = 0.5974(0.7409)
2023/10/24 06:55:22 - INFO - root -   Epoch: [147/300][40/84], lr: 0.00000097 	 loss = 0.2915(0.7554)
2023/10/24 06:56:57 - INFO - root -   Epoch: [147/300][60/84], lr: 0.00000097 	 loss = 0.5246(0.7254)
2023/10/24 06:58:06 - INFO - root -   Epoch: [147/300][80/84], lr: 0.00000097 	 loss = 0.7438(0.7424)
2023/10/24 06:58:08 - INFO - root -   Epoch: [147/300] 	 loss = 0.7496
2023/10/24 06:58:08 - INFO - root -   train_accuracy = 0.4643
2023/10/24 06:58:46 - INFO - root -   Epoch: [148/300][0/84], lr: 0.00000097 	 loss = 0.5514(0.5514)
2023/10/24 07:00:07 - INFO - root -   Epoch: [148/300][20/84], lr: 0.00000097 	 loss = 0.6663(0.7698)
2023/10/24 07:01:39 - INFO - root -   Epoch: [148/300][40/84], lr: 0.00000097 	 loss = 0.5011(0.7456)
2023/10/24 07:02:51 - INFO - root -   Epoch: [148/300][60/84], lr: 0.00000097 	 loss = 1.1707(0.7435)
2023/10/24 07:04:01 - INFO - root -   Epoch: [148/300][80/84], lr: 0.00000097 	 loss = 0.4775(0.7198)
2023/10/24 07:04:02 - INFO - root -   Epoch: [148/300] 	 loss = 0.7297
2023/10/24 07:04:02 - INFO - root -   train_accuracy = 0.5476
2023/10/24 07:04:40 - INFO - root -   Epoch: [149/300][0/84], lr: 0.00000098 	 loss = 0.7698(0.7698)
2023/10/24 07:05:55 - INFO - root -   Epoch: [149/300][20/84], lr: 0.00000098 	 loss = 0.7357(0.7515)
2023/10/24 07:07:30 - INFO - root -   Epoch: [149/300][40/84], lr: 0.00000098 	 loss = 0.6397(0.7251)
2023/10/24 07:08:58 - INFO - root -   Epoch: [149/300][60/84], lr: 0.00000098 	 loss = 1.0581(0.7420)
2023/10/24 07:09:56 - INFO - root -   Epoch: [149/300][80/84], lr: 0.00000098 	 loss = 1.0125(0.7445)
2023/10/24 07:10:01 - INFO - root -   Epoch: [149/300] 	 loss = 0.7442
2023/10/24 07:11:21 - INFO - root -   precision = 0.4651
2023/10/24 07:11:21 - INFO - root -   eval_loss = 0.6743
2023/10/24 07:11:21 - INFO - root -   eval_acc = 0.4651
2023/10/24 07:11:22 - INFO - root -   train_accuracy = 0.4583
2023/10/24 07:11:52 - INFO - root -   Epoch: [150/300][0/84], lr: 0.00000098 	 loss = 0.4509(0.4509)
2023/10/24 07:13:21 - INFO - root -   Epoch: [150/300][20/84], lr: 0.00000098 	 loss = 0.6307(0.6931)
2023/10/24 07:14:30 - INFO - root -   Epoch: [150/300][40/84], lr: 0.00000098 	 loss = 0.4765(0.6963)
2023/10/24 07:16:16 - INFO - root -   Epoch: [150/300][60/84], lr: 0.00000098 	 loss = 0.5524(0.6818)
2023/10/24 07:17:05 - INFO - root -   Epoch: [150/300][80/84], lr: 0.00000098 	 loss = 0.4958(0.6896)
2023/10/24 07:17:10 - INFO - root -   Epoch: [150/300] 	 loss = 0.6909
2023/10/24 07:17:10 - INFO - root -   train_accuracy = 0.5655
2023/10/24 07:17:49 - INFO - root -   Epoch: [151/300][0/84], lr: 0.00000099 	 loss = 0.4257(0.4257)
2023/10/24 07:19:02 - INFO - root -   Epoch: [151/300][20/84], lr: 0.00000099 	 loss = 0.7065(0.7470)
2023/10/24 07:20:35 - INFO - root -   Epoch: [151/300][40/84], lr: 0.00000099 	 loss = 0.6130(0.7397)
2023/10/24 07:21:57 - INFO - root -   Epoch: [151/300][60/84], lr: 0.00000099 	 loss = 0.6491(0.7323)
2023/10/24 07:23:09 - INFO - root -   Epoch: [151/300][80/84], lr: 0.00000099 	 loss = 0.6131(0.7188)
2023/10/24 07:23:12 - INFO - root -   Epoch: [151/300] 	 loss = 0.7233
2023/10/24 07:23:12 - INFO - root -   train_accuracy = 0.5179
2023/10/24 07:23:52 - INFO - root -   Epoch: [152/300][0/84], lr: 0.00000100 	 loss = 0.6317(0.6317)
2023/10/24 07:25:06 - INFO - root -   Epoch: [152/300][20/84], lr: 0.00000100 	 loss = 0.3930(0.7201)
2023/10/24 07:26:53 - INFO - root -   Epoch: [152/300][40/84], lr: 0.00000100 	 loss = 0.4461(0.7232)
2023/10/24 07:27:59 - INFO - root -   Epoch: [152/300][60/84], lr: 0.00000100 	 loss = 0.5586(0.7257)
2023/10/24 07:29:15 - INFO - root -   Epoch: [152/300][80/84], lr: 0.00000100 	 loss = 0.9972(0.7090)
2023/10/24 07:29:16 - INFO - root -   Epoch: [152/300] 	 loss = 0.7102
2023/10/24 07:29:16 - INFO - root -   train_accuracy = 0.5476
2023/10/24 07:29:46 - INFO - root -   Epoch: [153/300][0/84], lr: 0.00000100 	 loss = 0.8070(0.8070)
2023/10/24 07:31:09 - INFO - root -   Epoch: [153/300][20/84], lr: 0.00000100 	 loss = 0.5544(0.7241)
2023/10/24 07:33:10 - INFO - root -   Epoch: [153/300][40/84], lr: 0.00000100 	 loss = 0.7356(0.7318)
2023/10/24 07:34:12 - INFO - root -   Epoch: [153/300][60/84], lr: 0.00000100 	 loss = 0.4840(0.7185)
2023/10/24 07:35:22 - INFO - root -   Epoch: [153/300][80/84], lr: 0.00000100 	 loss = 0.9744(0.7168)
2023/10/24 07:35:23 - INFO - root -   Epoch: [153/300] 	 loss = 0.7221
2023/10/24 07:35:23 - INFO - root -   train_accuracy = 0.5476
2023/10/24 07:35:53 - INFO - root -   Epoch: [154/300][0/84], lr: 0.00000101 	 loss = 0.8142(0.8142)
2023/10/24 07:37:14 - INFO - root -   Epoch: [154/300][20/84], lr: 0.00000101 	 loss = 0.4518(0.7396)
2023/10/24 07:38:46 - INFO - root -   Epoch: [154/300][40/84], lr: 0.00000101 	 loss = 0.8499(0.7665)
2023/10/24 07:40:00 - INFO - root -   Epoch: [154/300][60/84], lr: 0.00000101 	 loss = 0.7673(0.7344)
2023/10/24 07:41:11 - INFO - root -   Epoch: [154/300][80/84], lr: 0.00000101 	 loss = 0.6839(0.7341)
2023/10/24 07:41:12 - INFO - root -   Epoch: [154/300] 	 loss = 0.7457
2023/10/24 07:42:33 - INFO - root -   precision = 0.6047
2023/10/24 07:42:33 - INFO - root -   eval_loss = 0.6806
2023/10/24 07:42:33 - INFO - root -   eval_acc = 0.6047
2023/10/24 07:42:34 - INFO - root -   train_accuracy = 0.4821
2023/10/24 07:43:11 - INFO - root -   Epoch: [155/300][0/84], lr: 0.00000101 	 loss = 1.2426(1.2426)
2023/10/24 07:44:39 - INFO - root -   Epoch: [155/300][20/84], lr: 0.00000101 	 loss = 0.8104(0.8050)
2023/10/24 07:46:06 - INFO - root -   Epoch: [155/300][40/84], lr: 0.00000101 	 loss = 0.6295(0.7833)
2023/10/24 07:47:34 - INFO - root -   Epoch: [155/300][60/84], lr: 0.00000101 	 loss = 0.2869(0.7624)
2023/10/24 07:48:43 - INFO - root -   Epoch: [155/300][80/84], lr: 0.00000101 	 loss = 0.8764(0.7404)
2023/10/24 07:48:48 - INFO - root -   Epoch: [155/300] 	 loss = 0.7376
2023/10/24 07:48:48 - INFO - root -   train_accuracy = 0.4881
2023/10/24 07:49:25 - INFO - root -   Epoch: [156/300][0/84], lr: 0.00000102 	 loss = 0.3823(0.3823)
2023/10/24 07:50:40 - INFO - root -   Epoch: [156/300][20/84], lr: 0.00000102 	 loss = 0.9530(0.6572)
2023/10/24 07:52:27 - INFO - root -   Epoch: [156/300][40/84], lr: 0.00000102 	 loss = 0.6934(0.6691)
2023/10/24 07:53:34 - INFO - root -   Epoch: [156/300][60/84], lr: 0.00000102 	 loss = 0.7935(0.6854)
2023/10/24 07:54:50 - INFO - root -   Epoch: [156/300][80/84], lr: 0.00000102 	 loss = 0.7083(0.6833)
2023/10/24 07:54:51 - INFO - root -   Epoch: [156/300] 	 loss = 0.6817
2023/10/24 07:54:51 - INFO - root -   train_accuracy = 0.5655
2023/10/24 07:55:20 - INFO - root -   Epoch: [157/300][0/84], lr: 0.00000103 	 loss = 0.5993(0.5993)
2023/10/24 07:57:03 - INFO - root -   Epoch: [157/300][20/84], lr: 0.00000103 	 loss = 0.6739(0.8158)
2023/10/24 07:58:07 - INFO - root -   Epoch: [157/300][40/84], lr: 0.00000103 	 loss = 0.4015(0.7783)
2023/10/24 07:59:49 - INFO - root -   Epoch: [157/300][60/84], lr: 0.00000103 	 loss = 0.6781(0.7457)
2023/10/24 08:00:43 - INFO - root -   Epoch: [157/300][80/84], lr: 0.00000103 	 loss = 0.4709(0.7329)
2023/10/24 08:00:48 - INFO - root -   Epoch: [157/300] 	 loss = 0.7312
2023/10/24 08:00:48 - INFO - root -   train_accuracy = 0.5417
2023/10/24 08:01:26 - INFO - root -   Epoch: [158/300][0/84], lr: 0.00000103 	 loss = 0.4200(0.4200)
2023/10/24 08:02:32 - INFO - root -   Epoch: [158/300][20/84], lr: 0.00000103 	 loss = 0.9365(0.6367)
2023/10/24 08:04:13 - INFO - root -   Epoch: [158/300][40/84], lr: 0.00000103 	 loss = 0.9291(0.7363)
2023/10/24 08:05:33 - INFO - root -   Epoch: [158/300][60/84], lr: 0.00000103 	 loss = 0.8683(0.7153)
2023/10/24 08:06:27 - INFO - root -   Epoch: [158/300][80/84], lr: 0.00000103 	 loss = 0.7055(0.6909)
2023/10/24 08:06:29 - INFO - root -   Epoch: [158/300] 	 loss = 0.6870
2023/10/24 08:06:29 - INFO - root -   train_accuracy = 0.5833
2023/10/24 08:07:08 - INFO - root -   Epoch: [159/300][0/84], lr: 0.00000104 	 loss = 0.9749(0.9749)
2023/10/24 08:08:23 - INFO - root -   Epoch: [159/300][20/84], lr: 0.00000104 	 loss = 0.7012(0.7039)
2023/10/24 08:09:38 - INFO - root -   Epoch: [159/300][40/84], lr: 0.00000104 	 loss = 0.8484(0.7507)
2023/10/24 08:11:07 - INFO - root -   Epoch: [159/300][60/84], lr: 0.00000104 	 loss = 0.7202(0.7360)
2023/10/24 08:12:17 - INFO - root -   Epoch: [159/300][80/84], lr: 0.00000104 	 loss = 0.8590(0.7351)
2023/10/24 08:12:19 - INFO - root -   Epoch: [159/300] 	 loss = 0.7274
2023/10/24 08:13:40 - INFO - root -   precision = 0.6512
2023/10/24 08:13:40 - INFO - root -   eval_loss = 0.6696
2023/10/24 08:13:40 - INFO - root -   eval_acc = 0.6512
2023/10/24 08:13:41 - INFO - root -   train_accuracy = 0.5060
2023/10/24 08:14:37 - INFO - root -   Epoch: [160/300][0/84], lr: 0.00000104 	 loss = 0.5843(0.5843)
2023/10/24 08:15:58 - INFO - root -   Epoch: [160/300][20/84], lr: 0.00000104 	 loss = 0.4371(0.6633)
2023/10/24 08:17:39 - INFO - root -   Epoch: [160/300][40/84], lr: 0.00000104 	 loss = 0.7801(0.7078)
2023/10/24 08:18:52 - INFO - root -   Epoch: [160/300][60/84], lr: 0.00000104 	 loss = 0.6216(0.6866)
2023/10/24 08:19:53 - INFO - root -   Epoch: [160/300][80/84], lr: 0.00000104 	 loss = 0.6830(0.7068)
2023/10/24 08:19:55 - INFO - root -   Epoch: [160/300] 	 loss = 0.7092
2023/10/24 08:19:55 - INFO - root -   train_accuracy = 0.5476
2023/10/24 08:20:24 - INFO - root -   Epoch: [161/300][0/84], lr: 0.00000105 	 loss = 0.6439(0.6439)
2023/10/24 08:21:55 - INFO - root -   Epoch: [161/300][20/84], lr: 0.00000105 	 loss = 0.5898(0.7076)
2023/10/24 08:23:15 - INFO - root -   Epoch: [161/300][40/84], lr: 0.00000105 	 loss = 0.4645(0.7139)
2023/10/24 08:24:41 - INFO - root -   Epoch: [161/300][60/84], lr: 0.00000105 	 loss = 0.3663(0.6817)
2023/10/24 08:25:59 - INFO - root -   Epoch: [161/300][80/84], lr: 0.00000105 	 loss = 0.8977(0.6943)
2023/10/24 08:26:02 - INFO - root -   Epoch: [161/300] 	 loss = 0.6951
2023/10/24 08:26:02 - INFO - root -   train_accuracy = 0.5476
2023/10/24 08:26:39 - INFO - root -   Epoch: [162/300][0/84], lr: 0.00000105 	 loss = 0.3847(0.3847)
2023/10/24 08:27:53 - INFO - root -   Epoch: [162/300][20/84], lr: 0.00000105 	 loss = 0.4967(0.6822)
2023/10/24 08:29:40 - INFO - root -   Epoch: [162/300][40/84], lr: 0.00000105 	 loss = 1.1650(0.7039)
2023/10/24 08:30:54 - INFO - root -   Epoch: [162/300][60/84], lr: 0.00000105 	 loss = 0.5563(0.7061)
2023/10/24 08:31:57 - INFO - root -   Epoch: [162/300][80/84], lr: 0.00000105 	 loss = 0.8207(0.6967)
2023/10/24 08:31:59 - INFO - root -   Epoch: [162/300] 	 loss = 0.6968
2023/10/24 08:31:59 - INFO - root -   train_accuracy = 0.5357
2023/10/24 08:32:53 - INFO - root -   Epoch: [163/300][0/84], lr: 0.00000106 	 loss = 0.8391(0.8391)
2023/10/24 08:33:50 - INFO - root -   Epoch: [163/300][20/84], lr: 0.00000106 	 loss = 0.7295(0.7045)
2023/10/24 08:35:31 - INFO - root -   Epoch: [163/300][40/84], lr: 0.00000106 	 loss = 0.4962(0.7327)
2023/10/24 08:36:39 - INFO - root -   Epoch: [163/300][60/84], lr: 0.00000106 	 loss = 0.4909(0.7005)
2023/10/24 08:38:00 - INFO - root -   Epoch: [163/300][80/84], lr: 0.00000106 	 loss = 0.9351(0.6968)
2023/10/24 08:38:02 - INFO - root -   Epoch: [163/300] 	 loss = 0.7030
2023/10/24 08:38:02 - INFO - root -   train_accuracy = 0.5357
2023/10/24 08:38:39 - INFO - root -   Epoch: [164/300][0/84], lr: 0.00000107 	 loss = 0.6440(0.6440)
2023/10/24 08:40:00 - INFO - root -   Epoch: [164/300][20/84], lr: 0.00000107 	 loss = 0.7314(0.7051)
2023/10/24 08:41:17 - INFO - root -   Epoch: [164/300][40/84], lr: 0.00000107 	 loss = 0.8844(0.7167)
2023/10/24 08:42:56 - INFO - root -   Epoch: [164/300][60/84], lr: 0.00000107 	 loss = 0.7401(0.6943)
2023/10/24 08:44:00 - INFO - root -   Epoch: [164/300][80/84], lr: 0.00000107 	 loss = 0.9129(0.6896)
2023/10/24 08:44:08 - INFO - root -   Epoch: [164/300] 	 loss = 0.6920
2023/10/24 08:45:27 - INFO - root -   precision = 0.6977
2023/10/24 08:45:27 - INFO - root -   eval_loss = 0.6700
2023/10/24 08:45:27 - INFO - root -   eval_acc = 0.6977
2023/10/24 08:45:28 - INFO - root -   train_accuracy = 0.5536
2023/10/24 08:46:06 - INFO - root -   Epoch: [165/300][0/84], lr: 0.00000107 	 loss = 0.7043(0.7043)
2023/10/24 08:47:20 - INFO - root -   Epoch: [165/300][20/84], lr: 0.00000107 	 loss = 1.0944(0.7584)
2023/10/24 08:49:06 - INFO - root -   Epoch: [165/300][40/84], lr: 0.00000107 	 loss = 0.7685(0.7673)
2023/10/24 08:50:29 - INFO - root -   Epoch: [165/300][60/84], lr: 0.00000107 	 loss = 0.4136(0.7293)
2023/10/24 08:51:27 - INFO - root -   Epoch: [165/300][80/84], lr: 0.00000107 	 loss = 0.8355(0.7248)
2023/10/24 08:51:30 - INFO - root -   Epoch: [165/300] 	 loss = 0.7273
2023/10/24 08:51:30 - INFO - root -   train_accuracy = 0.5238
2023/10/24 08:52:00 - INFO - root -   Epoch: [166/300][0/84], lr: 0.00000108 	 loss = 0.4064(0.4064)
2023/10/24 08:53:34 - INFO - root -   Epoch: [166/300][20/84], lr: 0.00000108 	 loss = 0.6126(0.7115)
2023/10/24 08:54:52 - INFO - root -   Epoch: [166/300][40/84], lr: 0.00000108 	 loss = 1.1508(0.7360)
2023/10/24 08:56:12 - INFO - root -   Epoch: [166/300][60/84], lr: 0.00000108 	 loss = 0.3701(0.7239)
2023/10/24 08:57:13 - INFO - root -   Epoch: [166/300][80/84], lr: 0.00000108 	 loss = 0.4476(0.7254)
2023/10/24 08:57:21 - INFO - root -   Epoch: [166/300] 	 loss = 0.7366
2023/10/24 08:57:21 - INFO - root -   train_accuracy = 0.5298
2023/10/24 08:57:51 - INFO - root -   Epoch: [167/300][0/84], lr: 0.00000108 	 loss = 0.7596(0.7596)
2023/10/24 08:59:14 - INFO - root -   Epoch: [167/300][20/84], lr: 0.00000108 	 loss = 0.5715(0.7211)
2023/10/24 09:00:49 - INFO - root -   Epoch: [167/300][40/84], lr: 0.00000108 	 loss = 0.4075(0.6908)
2023/10/24 09:02:30 - INFO - root -   Epoch: [167/300][60/84], lr: 0.00000108 	 loss = 0.7244(0.6803)
2023/10/24 09:03:24 - INFO - root -   Epoch: [167/300][80/84], lr: 0.00000108 	 loss = 0.4835(0.6960)
2023/10/24 09:03:28 - INFO - root -   Epoch: [167/300] 	 loss = 0.6946
2023/10/24 09:03:28 - INFO - root -   train_accuracy = 0.5655
2023/10/24 09:04:06 - INFO - root -   Epoch: [168/300][0/84], lr: 0.00000109 	 loss = 0.5754(0.5754)
2023/10/24 09:05:28 - INFO - root -   Epoch: [168/300][20/84], lr: 0.00000109 	 loss = 0.9020(0.6890)
2023/10/24 09:06:58 - INFO - root -   Epoch: [168/300][40/84], lr: 0.00000109 	 loss = 0.6375(0.7509)
2023/10/24 09:08:50 - INFO - root -   Epoch: [168/300][60/84], lr: 0.00000109 	 loss = 0.5230(0.7247)
2023/10/24 09:09:33 - INFO - root -   Epoch: [168/300][80/84], lr: 0.00000109 	 loss = 0.5726(0.7141)
2023/10/24 09:09:34 - INFO - root -   Epoch: [168/300] 	 loss = 0.7213
2023/10/24 09:09:34 - INFO - root -   train_accuracy = 0.5714
2023/10/24 09:10:13 - INFO - root -   Epoch: [169/300][0/84], lr: 0.00000110 	 loss = 0.7880(0.7880)
2023/10/24 09:11:40 - INFO - root -   Epoch: [169/300][20/84], lr: 0.00000110 	 loss = 0.7837(0.7018)
2023/10/24 09:12:58 - INFO - root -   Epoch: [169/300][40/84], lr: 0.00000110 	 loss = 0.7963(0.7277)
2023/10/24 09:14:34 - INFO - root -   Epoch: [169/300][60/84], lr: 0.00000110 	 loss = 0.4786(0.7247)
2023/10/24 09:15:29 - INFO - root -   Epoch: [169/300][80/84], lr: 0.00000110 	 loss = 0.7073(0.7208)
2023/10/24 09:15:35 - INFO - root -   Epoch: [169/300] 	 loss = 0.7196
2023/10/24 09:16:54 - INFO - root -   precision = 0.6512
2023/10/24 09:16:54 - INFO - root -   eval_loss = 0.6745
2023/10/24 09:16:54 - INFO - root -   eval_acc = 0.6512
2023/10/24 09:16:55 - INFO - root -   train_accuracy = 0.5238
2023/10/24 09:17:41 - INFO - root -   Epoch: [170/300][0/84], lr: 0.00000110 	 loss = 0.6142(0.6142)
2023/10/24 09:19:03 - INFO - root -   Epoch: [170/300][20/84], lr: 0.00000110 	 loss = 0.8658(0.7644)
2023/10/24 09:20:09 - INFO - root -   Epoch: [170/300][40/84], lr: 0.00000110 	 loss = 0.7627(0.7680)
2023/10/24 09:22:00 - INFO - root -   Epoch: [170/300][60/84], lr: 0.00000110 	 loss = 0.4883(0.7478)
2023/10/24 09:22:52 - INFO - root -   Epoch: [170/300][80/84], lr: 0.00000110 	 loss = 1.2992(0.7295)
2023/10/24 09:22:56 - INFO - root -   Epoch: [170/300] 	 loss = 0.7297
2023/10/24 09:22:56 - INFO - root -   train_accuracy = 0.4762
2023/10/24 09:23:34 - INFO - root -   Epoch: [171/300][0/84], lr: 0.00000111 	 loss = 0.8061(0.8061)
2023/10/24 09:24:58 - INFO - root -   Epoch: [171/300][20/84], lr: 0.00000111 	 loss = 0.5214(0.7355)
2023/10/24 09:26:21 - INFO - root -   Epoch: [171/300][40/84], lr: 0.00000111 	 loss = 0.8311(0.7459)
2023/10/24 09:27:56 - INFO - root -   Epoch: [171/300][60/84], lr: 0.00000111 	 loss = 0.2941(0.7217)
2023/10/24 09:28:56 - INFO - root -   Epoch: [171/300][80/84], lr: 0.00000111 	 loss = 0.5153(0.7286)
2023/10/24 09:29:06 - INFO - root -   Epoch: [171/300] 	 loss = 0.7295
2023/10/24 09:29:06 - INFO - root -   train_accuracy = 0.4821
2023/10/24 09:29:44 - INFO - root -   Epoch: [172/300][0/84], lr: 0.00000111 	 loss = 0.4945(0.4945)
2023/10/24 09:31:06 - INFO - root -   Epoch: [172/300][20/84], lr: 0.00000111 	 loss = 0.7867(0.6696)
2023/10/24 09:32:38 - INFO - root -   Epoch: [172/300][40/84], lr: 0.00000111 	 loss = 0.7731(0.7005)
2023/10/24 09:33:56 - INFO - root -   Epoch: [172/300][60/84], lr: 0.00000111 	 loss = 0.4327(0.6905)
2023/10/24 09:35:06 - INFO - root -   Epoch: [172/300][80/84], lr: 0.00000111 	 loss = 0.9575(0.6687)
2023/10/24 09:35:14 - INFO - root -   Epoch: [172/300] 	 loss = 0.6686
2023/10/24 09:35:14 - INFO - root -   train_accuracy = 0.5774
2023/10/24 09:35:59 - INFO - root -   Epoch: [173/300][0/84], lr: 0.00000112 	 loss = 0.9320(0.9320)
2023/10/24 09:37:06 - INFO - root -   Epoch: [173/300][20/84], lr: 0.00000112 	 loss = 0.4676(0.8227)
2023/10/24 09:38:46 - INFO - root -   Epoch: [173/300][40/84], lr: 0.00000112 	 loss = 0.5852(0.7328)
2023/10/24 09:39:53 - INFO - root -   Epoch: [173/300][60/84], lr: 0.00000112 	 loss = 0.4854(0.7331)
2023/10/24 09:41:03 - INFO - root -   Epoch: [173/300][80/84], lr: 0.00000112 	 loss = 0.5864(0.7343)
2023/10/24 09:41:04 - INFO - root -   Epoch: [173/300] 	 loss = 0.7379
2023/10/24 09:41:04 - INFO - root -   train_accuracy = 0.4583
2023/10/24 09:41:34 - INFO - root -   Epoch: [174/300][0/84], lr: 0.00000113 	 loss = 0.3190(0.3190)
2023/10/24 09:42:51 - INFO - root -   Epoch: [174/300][20/84], lr: 0.00000113 	 loss = 0.4982(0.5997)
2023/10/24 09:44:10 - INFO - root -   Epoch: [174/300][40/84], lr: 0.00000113 	 loss = 0.3366(0.6518)
2023/10/24 09:45:43 - INFO - root -   Epoch: [174/300][60/84], lr: 0.00000113 	 loss = 0.6177(0.6409)
2023/10/24 09:46:52 - INFO - root -   Epoch: [174/300][80/84], lr: 0.00000113 	 loss = 0.6360(0.6626)
2023/10/24 09:46:59 - INFO - root -   Epoch: [174/300] 	 loss = 0.6658
2023/10/24 09:48:18 - INFO - root -   precision = 0.6047
2023/10/24 09:48:18 - INFO - root -   eval_loss = 0.6819
2023/10/24 09:48:18 - INFO - root -   eval_acc = 0.6047
2023/10/24 09:48:19 - INFO - root -   train_accuracy = 0.5833
2023/10/24 09:48:57 - INFO - root -   Epoch: [175/300][0/84], lr: 0.00000113 	 loss = 1.1854(1.1854)
2023/10/24 09:50:19 - INFO - root -   Epoch: [175/300][20/84], lr: 0.00000113 	 loss = 0.6334(0.7675)
2023/10/24 09:51:44 - INFO - root -   Epoch: [175/300][40/84], lr: 0.00000113 	 loss = 0.6521(0.7784)
2023/10/24 09:53:03 - INFO - root -   Epoch: [175/300][60/84], lr: 0.00000113 	 loss = 0.3767(0.7294)
2023/10/24 09:54:21 - INFO - root -   Epoch: [175/300][80/84], lr: 0.00000113 	 loss = 0.6407(0.7085)
2023/10/24 09:54:25 - INFO - root -   Epoch: [175/300] 	 loss = 0.7140
2023/10/24 09:54:25 - INFO - root -   train_accuracy = 0.5298
2023/10/24 09:55:11 - INFO - root -   Epoch: [176/300][0/84], lr: 0.00000114 	 loss = 0.5690(0.5690)
2023/10/24 09:56:27 - INFO - root -   Epoch: [176/300][20/84], lr: 0.00000114 	 loss = 0.5700(0.6934)
2023/10/24 09:57:48 - INFO - root -   Epoch: [176/300][40/84], lr: 0.00000114 	 loss = 0.3743(0.7102)
2023/10/24 09:59:36 - INFO - root -   Epoch: [176/300][60/84], lr: 0.00000114 	 loss = 0.6281(0.7339)
2023/10/24 10:00:36 - INFO - root -   Epoch: [176/300][80/84], lr: 0.00000114 	 loss = 0.6610(0.7359)
2023/10/24 10:00:41 - INFO - root -   Epoch: [176/300] 	 loss = 0.7380
2023/10/24 10:00:41 - INFO - root -   train_accuracy = 0.4762
2023/10/24 10:01:11 - INFO - root -   Epoch: [177/300][0/84], lr: 0.00000114 	 loss = 0.3927(0.3927)
2023/10/24 10:02:41 - INFO - root -   Epoch: [177/300][20/84], lr: 0.00000114 	 loss = 0.5100(0.6500)
2023/10/24 10:04:20 - INFO - root -   Epoch: [177/300][40/84], lr: 0.00000114 	 loss = 0.8840(0.7513)
2023/10/24 10:05:51 - INFO - root -   Epoch: [177/300][60/84], lr: 0.00000114 	 loss = 0.4246(0.7462)
2023/10/24 10:06:39 - INFO - root -   Epoch: [177/300][80/84], lr: 0.00000114 	 loss = 0.6250(0.7475)
2023/10/24 10:06:46 - INFO - root -   Epoch: [177/300] 	 loss = 0.7433
2023/10/24 10:06:46 - INFO - root -   train_accuracy = 0.5060
2023/10/24 10:07:26 - INFO - root -   Epoch: [178/300][0/84], lr: 0.00000115 	 loss = 0.5494(0.5494)
2023/10/24 10:08:40 - INFO - root -   Epoch: [178/300][20/84], lr: 0.00000115 	 loss = 0.7514(0.7610)
2023/10/24 10:10:31 - INFO - root -   Epoch: [178/300][40/84], lr: 0.00000115 	 loss = 0.7203(0.7721)
2023/10/24 10:11:45 - INFO - root -   Epoch: [178/300][60/84], lr: 0.00000115 	 loss = 0.4528(0.7404)
2023/10/24 10:12:51 - INFO - root -   Epoch: [178/300][80/84], lr: 0.00000115 	 loss = 0.5492(0.7368)
2023/10/24 10:12:52 - INFO - root -   Epoch: [178/300] 	 loss = 0.7431
2023/10/24 10:12:52 - INFO - root -   train_accuracy = 0.4762
2023/10/24 10:13:30 - INFO - root -   Epoch: [179/300][0/84], lr: 0.00000115 	 loss = 0.3360(0.3360)
2023/10/24 10:14:42 - INFO - root -   Epoch: [179/300][20/84], lr: 0.00000115 	 loss = 0.3998(0.7047)
2023/10/24 10:16:10 - INFO - root -   Epoch: [179/300][40/84], lr: 0.00000115 	 loss = 0.3528(0.7095)
2023/10/24 10:17:28 - INFO - root -   Epoch: [179/300][60/84], lr: 0.00000115 	 loss = 0.8026(0.7213)
2023/10/24 10:18:50 - INFO - root -   Epoch: [179/300][80/84], lr: 0.00000115 	 loss = 0.6751(0.7134)
2023/10/24 10:18:51 - INFO - root -   Epoch: [179/300] 	 loss = 0.7106
2023/10/24 10:20:10 - INFO - root -   precision = 0.6512
2023/10/24 10:20:10 - INFO - root -   eval_loss = 0.6740
2023/10/24 10:20:10 - INFO - root -   eval_acc = 0.6512
2023/10/24 10:20:12 - INFO - root -   train_accuracy = 0.5536
2023/10/24 10:20:49 - INFO - root -   Epoch: [180/300][0/84], lr: 0.00000116 	 loss = 0.8293(0.8293)
2023/10/24 10:22:02 - INFO - root -   Epoch: [180/300][20/84], lr: 0.00000116 	 loss = 0.7603(0.7302)
2023/10/24 10:23:43 - INFO - root -   Epoch: [180/300][40/84], lr: 0.00000116 	 loss = 0.5365(0.7395)
2023/10/24 10:24:57 - INFO - root -   Epoch: [180/300][60/84], lr: 0.00000116 	 loss = 0.5768(0.7633)
2023/10/24 10:26:02 - INFO - root -   Epoch: [180/300][80/84], lr: 0.00000116 	 loss = 0.5151(0.7503)
2023/10/24 10:26:03 - INFO - root -   Epoch: [180/300] 	 loss = 0.7549
2023/10/24 10:26:03 - INFO - root -   train_accuracy = 0.5000
2023/10/24 10:26:32 - INFO - root -   Epoch: [181/300][0/84], lr: 0.00000117 	 loss = 0.4225(0.4225)
2023/10/24 10:28:24 - INFO - root -   Epoch: [181/300][20/84], lr: 0.00000117 	 loss = 0.5583(0.7405)
2023/10/24 10:29:28 - INFO - root -   Epoch: [181/300][40/84], lr: 0.00000117 	 loss = 0.4610(0.7223)
2023/10/24 10:31:19 - INFO - root -   Epoch: [181/300][60/84], lr: 0.00000117 	 loss = 0.4867(0.7031)
2023/10/24 10:32:14 - INFO - root -   Epoch: [181/300][80/84], lr: 0.00000117 	 loss = 0.6601(0.7033)
2023/10/24 10:32:17 - INFO - root -   Epoch: [181/300] 	 loss = 0.7062
2023/10/24 10:32:17 - INFO - root -   train_accuracy = 0.5417
2023/10/24 10:33:03 - INFO - root -   Epoch: [182/300][0/84], lr: 0.00000117 	 loss = 0.5524(0.5524)
2023/10/24 10:34:27 - INFO - root -   Epoch: [182/300][20/84], lr: 0.00000117 	 loss = 0.5342(0.7364)
2023/10/24 10:35:52 - INFO - root -   Epoch: [182/300][40/84], lr: 0.00000117 	 loss = 1.0777(0.7886)
2023/10/24 10:37:16 - INFO - root -   Epoch: [182/300][60/84], lr: 0.00000117 	 loss = 0.7076(0.7744)
2023/10/24 10:38:22 - INFO - root -   Epoch: [182/300][80/84], lr: 0.00000117 	 loss = 0.3088(0.7587)
2023/10/24 10:38:24 - INFO - root -   Epoch: [182/300] 	 loss = 0.7612
2023/10/24 10:38:24 - INFO - root -   train_accuracy = 0.4524
2023/10/24 10:39:03 - INFO - root -   Epoch: [183/300][0/84], lr: 0.00000118 	 loss = 0.3242(0.3242)
2023/10/24 10:40:17 - INFO - root -   Epoch: [183/300][20/84], lr: 0.00000118 	 loss = 0.5564(0.6999)
2023/10/24 10:41:49 - INFO - root -   Epoch: [183/300][40/84], lr: 0.00000118 	 loss = 0.7113(0.7203)
2023/10/24 10:43:10 - INFO - root -   Epoch: [183/300][60/84], lr: 0.00000118 	 loss = 0.5884(0.7012)
2023/10/24 10:44:26 - INFO - root -   Epoch: [183/300][80/84], lr: 0.00000118 	 loss = 0.9597(0.7121)
2023/10/24 10:44:32 - INFO - root -   Epoch: [183/300] 	 loss = 0.7144
2023/10/24 10:44:32 - INFO - root -   train_accuracy = 0.5060
2023/10/24 10:45:09 - INFO - root -   Epoch: [184/300][0/84], lr: 0.00000118 	 loss = 0.7334(0.7334)
2023/10/24 10:46:25 - INFO - root -   Epoch: [184/300][20/84], lr: 0.00000118 	 loss = 0.3173(0.6749)
2023/10/24 10:47:47 - INFO - root -   Epoch: [184/300][40/84], lr: 0.00000118 	 loss = 0.9443(0.7020)
2023/10/24 10:49:06 - INFO - root -   Epoch: [184/300][60/84], lr: 0.00000118 	 loss = 0.5877(0.6803)
2023/10/24 10:50:16 - INFO - root -   Epoch: [184/300][80/84], lr: 0.00000118 	 loss = 0.9986(0.7019)
2023/10/24 10:50:19 - INFO - root -   Epoch: [184/300] 	 loss = 0.7019
2023/10/24 10:51:41 - INFO - root -   precision = 0.4884
2023/10/24 10:51:41 - INFO - root -   eval_loss = 0.6822
2023/10/24 10:51:41 - INFO - root -   eval_acc = 0.4884
2023/10/24 10:51:42 - INFO - root -   train_accuracy = 0.5476
2023/10/24 10:52:29 - INFO - root -   Epoch: [185/300][0/84], lr: 0.00000119 	 loss = 0.5080(0.5080)
2023/10/24 10:53:45 - INFO - root -   Epoch: [185/300][20/84], lr: 0.00000119 	 loss = 0.4667(0.7496)
2023/10/24 10:55:10 - INFO - root -   Epoch: [185/300][40/84], lr: 0.00000119 	 loss = 0.4474(0.7278)
2023/10/24 10:56:58 - INFO - root -   Epoch: [185/300][60/84], lr: 0.00000119 	 loss = 0.6720(0.7004)
2023/10/24 10:57:55 - INFO - root -   Epoch: [185/300][80/84], lr: 0.00000119 	 loss = 0.7431(0.7062)
2023/10/24 10:57:56 - INFO - root -   Epoch: [185/300] 	 loss = 0.7155
2023/10/24 10:57:56 - INFO - root -   train_accuracy = 0.5298
2023/10/24 10:58:25 - INFO - root -   Epoch: [186/300][0/84], lr: 0.00000120 	 loss = 0.4280(0.4280)
2023/10/24 10:59:55 - INFO - root -   Epoch: [186/300][20/84], lr: 0.00000120 	 loss = 0.4386(0.6478)
2023/10/24 11:01:13 - INFO - root -   Epoch: [186/300][40/84], lr: 0.00000120 	 loss = 0.5462(0.6890)
2023/10/24 11:03:02 - INFO - root -   Epoch: [186/300][60/84], lr: 0.00000120 	 loss = 0.6848(0.6890)
2023/10/24 11:04:04 - INFO - root -   Epoch: [186/300][80/84], lr: 0.00000120 	 loss = 1.1294(0.6891)
2023/10/24 11:04:09 - INFO - root -   Epoch: [186/300] 	 loss = 0.6911
2023/10/24 11:04:09 - INFO - root -   train_accuracy = 0.5238
2023/10/24 11:04:39 - INFO - root -   Epoch: [187/300][0/84], lr: 0.00000120 	 loss = 0.2358(0.2358)
2023/10/24 11:05:59 - INFO - root -   Epoch: [187/300][20/84], lr: 0.00000120 	 loss = 0.5542(0.6887)
2023/10/24 11:07:36 - INFO - root -   Epoch: [187/300][40/84], lr: 0.00000120 	 loss = 0.8917(0.7128)
2023/10/24 11:08:41 - INFO - root -   Epoch: [187/300][60/84], lr: 0.00000120 	 loss = 0.9457(0.6944)
2023/10/24 11:10:00 - INFO - root -   Epoch: [187/300][80/84], lr: 0.00000120 	 loss = 0.7617(0.7056)
2023/10/24 11:10:02 - INFO - root -   Epoch: [187/300] 	 loss = 0.7049
2023/10/24 11:10:02 - INFO - root -   train_accuracy = 0.5060
2023/10/24 11:10:42 - INFO - root -   Epoch: [188/300][0/84], lr: 0.00000121 	 loss = 0.4276(0.4276)
2023/10/24 11:12:09 - INFO - root -   Epoch: [188/300][20/84], lr: 0.00000121 	 loss = 0.4589(0.7010)
2023/10/24 11:13:19 - INFO - root -   Epoch: [188/300][40/84], lr: 0.00000121 	 loss = 1.2486(0.7220)
2023/10/24 11:14:59 - INFO - root -   Epoch: [188/300][60/84], lr: 0.00000121 	 loss = 0.8530(0.7269)
2023/10/24 11:15:53 - INFO - root -   Epoch: [188/300][80/84], lr: 0.00000121 	 loss = 0.7812(0.7298)
2023/10/24 11:15:58 - INFO - root -   Epoch: [188/300] 	 loss = 0.7348
2023/10/24 11:15:58 - INFO - root -   train_accuracy = 0.5179
2023/10/24 11:16:29 - INFO - root -   Epoch: [189/300][0/84], lr: 0.00000121 	 loss = 0.4410(0.4410)
2023/10/24 11:17:51 - INFO - root -   Epoch: [189/300][20/84], lr: 0.00000121 	 loss = 0.7419(0.7242)
2023/10/24 11:19:18 - INFO - root -   Epoch: [189/300][40/84], lr: 0.00000121 	 loss = 0.5653(0.6968)
2023/10/24 11:21:10 - INFO - root -   Epoch: [189/300][60/84], lr: 0.00000121 	 loss = 1.1879(0.7136)
2023/10/24 11:22:00 - INFO - root -   Epoch: [189/300][80/84], lr: 0.00000121 	 loss = 0.6356(0.6975)
2023/10/24 11:22:05 - INFO - root -   Epoch: [189/300] 	 loss = 0.6948
2023/10/24 11:23:30 - INFO - root -   precision = 0.6512
2023/10/24 11:23:30 - INFO - root -   eval_loss = 0.6760
2023/10/24 11:23:30 - INFO - root -   eval_acc = 0.6512
2023/10/24 11:23:31 - INFO - root -   train_accuracy = 0.5417
2023/10/24 11:24:08 - INFO - root -   Epoch: [190/300][0/84], lr: 0.00000122 	 loss = 0.5745(0.5745)
2023/10/24 11:25:32 - INFO - root -   Epoch: [190/300][20/84], lr: 0.00000122 	 loss = 0.9366(0.7268)
2023/10/24 11:26:52 - INFO - root -   Epoch: [190/300][40/84], lr: 0.00000122 	 loss = 0.7729(0.7079)
2023/10/24 11:28:36 - INFO - root -   Epoch: [190/300][60/84], lr: 0.00000122 	 loss = 0.7235(0.7071)
2023/10/24 11:29:28 - INFO - root -   Epoch: [190/300][80/84], lr: 0.00000122 	 loss = 0.7406(0.7270)
2023/10/24 11:29:34 - INFO - root -   Epoch: [190/300] 	 loss = 0.7234
2023/10/24 11:29:34 - INFO - root -   train_accuracy = 0.4464
2023/10/24 11:30:21 - INFO - root -   Epoch: [191/300][0/84], lr: 0.00000123 	 loss = 0.5576(0.5576)
2023/10/24 11:31:34 - INFO - root -   Epoch: [191/300][20/84], lr: 0.00000123 	 loss = 0.8081(0.7576)
2023/10/24 11:32:50 - INFO - root -   Epoch: [191/300][40/84], lr: 0.00000123 	 loss = 0.5114(0.7493)
2023/10/24 11:34:47 - INFO - root -   Epoch: [191/300][60/84], lr: 0.00000123 	 loss = 0.2907(0.7141)
2023/10/24 11:35:37 - INFO - root -   Epoch: [191/300][80/84], lr: 0.00000123 	 loss = 0.7026(0.7128)
2023/10/24 11:35:41 - INFO - root -   Epoch: [191/300] 	 loss = 0.7119
2023/10/24 11:35:41 - INFO - root -   train_accuracy = 0.5060
2023/10/24 11:36:19 - INFO - root -   Epoch: [192/300][0/84], lr: 0.00000123 	 loss = 0.6250(0.6250)
2023/10/24 11:37:35 - INFO - root -   Epoch: [192/300][20/84], lr: 0.00000123 	 loss = 0.6737(0.7132)
2023/10/24 11:39:06 - INFO - root -   Epoch: [192/300][40/84], lr: 0.00000123 	 loss = 0.5021(0.7445)
2023/10/24 11:40:28 - INFO - root -   Epoch: [192/300][60/84], lr: 0.00000123 	 loss = 0.5408(0.7011)
2023/10/24 11:41:36 - INFO - root -   Epoch: [192/300][80/84], lr: 0.00000123 	 loss = 0.6805(0.7027)
2023/10/24 11:41:39 - INFO - root -   Epoch: [192/300] 	 loss = 0.6980
2023/10/24 11:41:39 - INFO - root -   train_accuracy = 0.5595
2023/10/24 11:42:08 - INFO - root -   Epoch: [193/300][0/84], lr: 0.00000124 	 loss = 0.5818(0.5818)
2023/10/24 11:43:40 - INFO - root -   Epoch: [193/300][20/84], lr: 0.00000124 	 loss = 0.8900(0.7199)
2023/10/24 11:45:01 - INFO - root -   Epoch: [193/300][40/84], lr: 0.00000124 	 loss = 0.3541(0.7766)
2023/10/24 11:46:18 - INFO - root -   Epoch: [193/300][60/84], lr: 0.00000124 	 loss = 0.6105(0.7519)
2023/10/24 11:47:39 - INFO - root -   Epoch: [193/300][80/84], lr: 0.00000124 	 loss = 0.4211(0.7313)
2023/10/24 11:47:43 - INFO - root -   Epoch: [193/300] 	 loss = 0.7312
2023/10/24 11:47:43 - INFO - root -   train_accuracy = 0.5060
2023/10/24 11:48:14 - INFO - root -   Epoch: [194/300][0/84], lr: 0.00000124 	 loss = 0.6089(0.6089)
2023/10/24 11:49:36 - INFO - root -   Epoch: [194/300][20/84], lr: 0.00000124 	 loss = 0.7265(0.7626)
2023/10/24 11:50:55 - INFO - root -   Epoch: [194/300][40/84], lr: 0.00000124 	 loss = 0.4962(0.7214)
2023/10/24 11:52:38 - INFO - root -   Epoch: [194/300][60/84], lr: 0.00000124 	 loss = 0.4469(0.7151)
2023/10/24 11:53:44 - INFO - root -   Epoch: [194/300][80/84], lr: 0.00000124 	 loss = 0.5162(0.7056)
2023/10/24 11:53:47 - INFO - root -   Epoch: [194/300] 	 loss = 0.7095
2023/10/24 11:55:11 - INFO - root -   precision = 0.4651
2023/10/24 11:55:11 - INFO - root -   eval_loss = 0.6807
2023/10/24 11:55:11 - INFO - root -   eval_acc = 0.4651
2023/10/24 11:55:12 - INFO - root -   train_accuracy = 0.5476
2023/10/24 11:55:50 - INFO - root -   Epoch: [195/300][0/84], lr: 0.00000125 	 loss = 0.4715(0.4715)
2023/10/24 11:57:17 - INFO - root -   Epoch: [195/300][20/84], lr: 0.00000125 	 loss = 0.8042(0.7352)
2023/10/24 11:58:33 - INFO - root -   Epoch: [195/300][40/84], lr: 0.00000125 	 loss = 0.8595(0.7298)
2023/10/24 12:00:08 - INFO - root -   Epoch: [195/300][60/84], lr: 0.00000125 	 loss = 0.5929(0.6985)
2023/10/24 12:01:01 - INFO - root -   Epoch: [195/300][80/84], lr: 0.00000125 	 loss = 0.7004(0.6944)
2023/10/24 12:01:11 - INFO - root -   Epoch: [195/300] 	 loss = 0.7010
2023/10/24 12:01:11 - INFO - root -   train_accuracy = 0.5417
2023/10/24 12:01:49 - INFO - root -   Epoch: [196/300][0/84], lr: 0.00000126 	 loss = 0.3883(0.3883)
2023/10/24 12:02:55 - INFO - root -   Epoch: [196/300][20/84], lr: 0.00000126 	 loss = 0.8403(0.6670)
2023/10/24 12:04:35 - INFO - root -   Epoch: [196/300][40/84], lr: 0.00000126 	 loss = 0.7181(0.7061)
2023/10/24 12:05:43 - INFO - root -   Epoch: [196/300][60/84], lr: 0.00000126 	 loss = 0.7004(0.7058)
2023/10/24 12:07:06 - INFO - root -   Epoch: [196/300][80/84], lr: 0.00000126 	 loss = 0.6933(0.7182)
2023/10/24 12:07:10 - INFO - root -   Epoch: [196/300] 	 loss = 0.7177
2023/10/24 12:07:10 - INFO - root -   train_accuracy = 0.5357
2023/10/24 12:07:57 - INFO - root -   Epoch: [197/300][0/84], lr: 0.00000126 	 loss = 0.7075(0.7075)
2023/10/24 12:09:12 - INFO - root -   Epoch: [197/300][20/84], lr: 0.00000126 	 loss = 0.7869(0.7711)
2023/10/24 12:10:41 - INFO - root -   Epoch: [197/300][40/84], lr: 0.00000126 	 loss = 0.5801(0.7221)
2023/10/24 12:12:07 - INFO - root -   Epoch: [197/300][60/84], lr: 0.00000126 	 loss = 0.4217(0.7068)
2023/10/24 12:13:17 - INFO - root -   Epoch: [197/300][80/84], lr: 0.00000126 	 loss = 0.4830(0.6991)
2023/10/24 12:13:18 - INFO - root -   Epoch: [197/300] 	 loss = 0.7059
2023/10/24 12:13:18 - INFO - root -   train_accuracy = 0.5298
2023/10/24 12:13:49 - INFO - root -   Epoch: [198/300][0/84], lr: 0.00000127 	 loss = 0.4591(0.4591)
2023/10/24 12:15:25 - INFO - root -   Epoch: [198/300][20/84], lr: 0.00000127 	 loss = 0.6739(0.6495)
2023/10/24 12:16:35 - INFO - root -   Epoch: [198/300][40/84], lr: 0.00000127 	 loss = 0.8736(0.7159)
2023/10/24 12:18:30 - INFO - root -   Epoch: [198/300][60/84], lr: 0.00000127 	 loss = 0.8952(0.7131)
2023/10/24 12:19:15 - INFO - root -   Epoch: [198/300][80/84], lr: 0.00000127 	 loss = 0.4182(0.7007)
2023/10/24 12:19:20 - INFO - root -   Epoch: [198/300] 	 loss = 0.7003
2023/10/24 12:19:20 - INFO - root -   train_accuracy = 0.5714
2023/10/24 12:20:06 - INFO - root -   Epoch: [199/300][0/84], lr: 0.00000127 	 loss = 0.7648(0.7648)
2023/10/24 12:21:12 - INFO - root -   Epoch: [199/300][20/84], lr: 0.00000127 	 loss = 0.5383(0.7259)
2023/10/24 12:22:45 - INFO - root -   Epoch: [199/300][40/84], lr: 0.00000127 	 loss = 0.8660(0.7283)
2023/10/24 12:24:11 - INFO - root -   Epoch: [199/300][60/84], lr: 0.00000127 	 loss = 0.7277(0.7312)
2023/10/24 12:25:25 - INFO - root -   Epoch: [199/300][80/84], lr: 0.00000127 	 loss = 0.6994(0.7318)
2023/10/24 12:25:26 - INFO - root -   Epoch: [199/300] 	 loss = 0.7313
2023/10/24 12:26:52 - INFO - root -   precision = 0.6744
2023/10/24 12:26:52 - INFO - root -   eval_loss = 0.6740
2023/10/24 12:26:52 - INFO - root -   eval_acc = 0.6744
2023/10/24 12:26:53 - INFO - root -   train_accuracy = 0.4702
2023/10/24 12:27:25 - INFO - root -   Epoch: [200/300][0/84], lr: 0.00000128 	 loss = 0.6994(0.6994)
2023/10/24 12:28:47 - INFO - root -   Epoch: [200/300][20/84], lr: 0.00000128 	 loss = 0.7167(0.7218)
2023/10/24 12:30:24 - INFO - root -   Epoch: [200/300][40/84], lr: 0.00000128 	 loss = 0.7595(0.7236)
2023/10/24 12:31:38 - INFO - root -   Epoch: [200/300][60/84], lr: 0.00000128 	 loss = 0.7138(0.7003)
2023/10/24 12:32:38 - INFO - root -   Epoch: [200/300][80/84], lr: 0.00000128 	 loss = 0.7484(0.7126)
2023/10/24 12:32:39 - INFO - root -   Epoch: [200/300] 	 loss = 0.7198
2023/10/24 12:32:39 - INFO - root -   train_accuracy = 0.5476
2023/10/24 12:33:26 - INFO - root -   Epoch: [201/300][0/84], lr: 0.00000128 	 loss = 0.6364(0.6364)
2023/10/24 12:34:32 - INFO - root -   Epoch: [201/300][20/84], lr: 0.00000128 	 loss = 0.4006(0.7066)
2023/10/24 12:36:27 - INFO - root -   Epoch: [201/300][40/84], lr: 0.00000128 	 loss = 1.0644(0.7239)
2023/10/24 12:37:40 - INFO - root -   Epoch: [201/300][60/84], lr: 0.00000128 	 loss = 0.7110(0.7025)
2023/10/24 12:38:48 - INFO - root -   Epoch: [201/300][80/84], lr: 0.00000128 	 loss = 0.6126(0.7057)
2023/10/24 12:38:49 - INFO - root -   Epoch: [201/300] 	 loss = 0.7013
2023/10/24 12:38:49 - INFO - root -   train_accuracy = 0.5298
2023/10/24 12:39:27 - INFO - root -   Epoch: [202/300][0/84], lr: 0.00000129 	 loss = 0.4853(0.4853)
2023/10/24 12:40:42 - INFO - root -   Epoch: [202/300][20/84], lr: 0.00000129 	 loss = 0.8115(0.7718)
2023/10/24 12:42:35 - INFO - root -   Epoch: [202/300][40/84], lr: 0.00000129 	 loss = 0.9623(0.7407)
2023/10/24 12:43:32 - INFO - root -   Epoch: [202/300][60/84], lr: 0.00000129 	 loss = 0.8569(0.7297)
2023/10/24 12:44:49 - INFO - root -   Epoch: [202/300][80/84], lr: 0.00000129 	 loss = 0.5409(0.7127)
2023/10/24 12:44:51 - INFO - root -   Epoch: [202/300] 	 loss = 0.7186
2023/10/24 12:44:51 - INFO - root -   train_accuracy = 0.5119
2023/10/24 12:45:29 - INFO - root -   Epoch: [203/300][0/84], lr: 0.00000130 	 loss = 0.4795(0.4795)
2023/10/24 12:47:04 - INFO - root -   Epoch: [203/300][20/84], lr: 0.00000130 	 loss = 0.4684(0.6980)
2023/10/24 12:48:19 - INFO - root -   Epoch: [203/300][40/84], lr: 0.00000130 	 loss = 0.5900(0.7045)
2023/10/24 12:49:48 - INFO - root -   Epoch: [203/300][60/84], lr: 0.00000130 	 loss = 0.5880(0.7039)
2023/10/24 12:50:42 - INFO - root -   Epoch: [203/300][80/84], lr: 0.00000130 	 loss = 0.7662(0.6951)
2023/10/24 12:50:48 - INFO - root -   Epoch: [203/300] 	 loss = 0.7018
2023/10/24 12:50:48 - INFO - root -   train_accuracy = 0.5357
2023/10/24 12:51:34 - INFO - root -   Epoch: [204/300][0/84], lr: 0.00000130 	 loss = 0.9778(0.9778)
2023/10/24 12:52:48 - INFO - root -   Epoch: [204/300][20/84], lr: 0.00000130 	 loss = 0.5807(0.7267)
2023/10/24 12:54:15 - INFO - root -   Epoch: [204/300][40/84], lr: 0.00000130 	 loss = 0.6620(0.7179)
2023/10/24 12:55:58 - INFO - root -   Epoch: [204/300][60/84], lr: 0.00000130 	 loss = 0.4287(0.7040)
2023/10/24 12:56:49 - INFO - root -   Epoch: [204/300][80/84], lr: 0.00000130 	 loss = 0.8579(0.7137)
2023/10/24 12:56:56 - INFO - root -   Epoch: [204/300] 	 loss = 0.7212
2023/10/24 12:58:21 - INFO - root -   precision = 0.5581
2023/10/24 12:58:21 - INFO - root -   eval_loss = 0.6813
2023/10/24 12:58:21 - INFO - root -   eval_acc = 0.5581
2023/10/24 12:58:22 - INFO - root -   train_accuracy = 0.4940
2023/10/24 12:58:53 - INFO - root -   Epoch: [205/300][0/84], lr: 0.00000131 	 loss = 0.6664(0.6664)
2023/10/24 13:00:27 - INFO - root -   Epoch: [205/300][20/84], lr: 0.00000131 	 loss = 0.8282(0.6959)
2023/10/24 13:01:41 - INFO - root -   Epoch: [205/300][40/84], lr: 0.00000131 	 loss = 0.5842(0.7038)
2023/10/24 13:03:08 - INFO - root -   Epoch: [205/300][60/84], lr: 0.00000131 	 loss = 0.4323(0.7007)
2023/10/24 13:04:16 - INFO - root -   Epoch: [205/300][80/84], lr: 0.00000131 	 loss = 0.6395(0.7130)
2023/10/24 13:04:23 - INFO - root -   Epoch: [205/300] 	 loss = 0.7175
2023/10/24 13:04:23 - INFO - root -   train_accuracy = 0.5357
2023/10/24 13:04:53 - INFO - root -   Epoch: [206/300][0/84], lr: 0.00000131 	 loss = 0.5502(0.5502)
2023/10/24 13:06:07 - INFO - root -   Epoch: [206/300][20/84], lr: 0.00000131 	 loss = 0.6194(0.7140)
2023/10/24 13:07:48 - INFO - root -   Epoch: [206/300][40/84], lr: 0.00000131 	 loss = 1.0888(0.7264)
2023/10/24 13:08:55 - INFO - root -   Epoch: [206/300][60/84], lr: 0.00000131 	 loss = 0.5095(0.7225)
2023/10/24 13:10:20 - INFO - root -   Epoch: [206/300][80/84], lr: 0.00000131 	 loss = 0.7512(0.7228)
2023/10/24 13:10:21 - INFO - root -   Epoch: [206/300] 	 loss = 0.7204
2023/10/24 13:10:21 - INFO - root -   train_accuracy = 0.5119
2023/10/24 13:10:59 - INFO - root -   Epoch: [207/300][0/84], lr: 0.00000132 	 loss = 0.5989(0.5989)
2023/10/24 13:12:28 - INFO - root -   Epoch: [207/300][20/84], lr: 0.00000132 	 loss = 0.8551(0.7385)
2023/10/24 13:14:15 - INFO - root -   Epoch: [207/300][40/84], lr: 0.00000132 	 loss = 0.6616(0.7122)
2023/10/24 13:15:28 - INFO - root -   Epoch: [207/300][60/84], lr: 0.00000132 	 loss = 0.6185(0.7047)
2023/10/24 13:16:24 - INFO - root -   Epoch: [207/300][80/84], lr: 0.00000132 	 loss = 0.7700(0.6877)
2023/10/24 13:16:26 - INFO - root -   Epoch: [207/300] 	 loss = 0.6887
2023/10/24 13:16:26 - INFO - root -   train_accuracy = 0.4940
2023/10/24 13:16:56 - INFO - root -   Epoch: [208/300][0/84], lr: 0.00000133 	 loss = 0.7923(0.7923)
2023/10/24 13:18:33 - INFO - root -   Epoch: [208/300][20/84], lr: 0.00000133 	 loss = 0.8612(0.7797)
2023/10/24 13:19:40 - INFO - root -   Epoch: [208/300][40/84], lr: 0.00000133 	 loss = 0.9938(0.7836)
2023/10/24 13:21:13 - INFO - root -   Epoch: [208/300][60/84], lr: 0.00000133 	 loss = 0.5790(0.7569)
2023/10/24 13:22:09 - INFO - root -   Epoch: [208/300][80/84], lr: 0.00000133 	 loss = 0.7417(0.7554)
2023/10/24 13:22:14 - INFO - root -   Epoch: [208/300] 	 loss = 0.7524
2023/10/24 13:22:14 - INFO - root -   train_accuracy = 0.4702
2023/10/24 13:22:45 - INFO - root -   Epoch: [209/300][0/84], lr: 0.00000133 	 loss = 0.4568(0.4568)
2023/10/24 13:24:17 - INFO - root -   Epoch: [209/300][20/84], lr: 0.00000133 	 loss = 0.7480(0.6898)
2023/10/24 13:26:03 - INFO - root -   Epoch: [209/300][40/84], lr: 0.00000133 	 loss = 0.7817(0.7155)
2023/10/24 13:27:15 - INFO - root -   Epoch: [209/300][60/84], lr: 0.00000133 	 loss = 0.4391(0.7015)
2023/10/24 13:28:25 - INFO - root -   Epoch: [209/300][80/84], lr: 0.00000133 	 loss = 0.5617(0.7046)
2023/10/24 13:28:27 - INFO - root -   Epoch: [209/300] 	 loss = 0.7062
2023/10/24 13:29:52 - INFO - root -   precision = 0.6512
2023/10/24 13:29:52 - INFO - root -   eval_loss = 0.6735
2023/10/24 13:29:52 - INFO - root -   eval_acc = 0.6512
2023/10/24 13:29:54 - INFO - root -   train_accuracy = 0.5000
2023/10/24 13:30:24 - INFO - root -   Epoch: [210/300][0/84], lr: 0.00000134 	 loss = 0.6382(0.6382)
2023/10/24 13:32:01 - INFO - root -   Epoch: [210/300][20/84], lr: 0.00000134 	 loss = 0.7573(0.6963)
2023/10/24 13:33:21 - INFO - root -   Epoch: [210/300][40/84], lr: 0.00000134 	 loss = 0.5652(0.7144)
2023/10/24 13:34:49 - INFO - root -   Epoch: [210/300][60/84], lr: 0.00000134 	 loss = 0.6018(0.7081)
2023/10/24 13:35:51 - INFO - root -   Epoch: [210/300][80/84], lr: 0.00000134 	 loss = 0.5577(0.6972)
2023/10/24 13:35:54 - INFO - root -   Epoch: [210/300] 	 loss = 0.7015
2023/10/24 13:35:54 - INFO - root -   train_accuracy = 0.5238
2023/10/24 13:36:25 - INFO - root -   Epoch: [211/300][0/84], lr: 0.00000134 	 loss = 0.3090(0.3090)
2023/10/24 13:37:54 - INFO - root -   Epoch: [211/300][20/84], lr: 0.00000134 	 loss = 0.4837(0.7302)
2023/10/24 13:39:30 - INFO - root -   Epoch: [211/300][40/84], lr: 0.00000134 	 loss = 0.8195(0.7267)
2023/10/24 13:40:56 - INFO - root -   Epoch: [211/300][60/84], lr: 0.00000134 	 loss = 0.4736(0.6955)
2023/10/24 13:41:56 - INFO - root -   Epoch: [211/300][80/84], lr: 0.00000134 	 loss = 0.4948(0.7059)
2023/10/24 13:41:58 - INFO - root -   Epoch: [211/300] 	 loss = 0.7123
2023/10/24 13:41:58 - INFO - root -   train_accuracy = 0.5714
2023/10/24 13:42:36 - INFO - root -   Epoch: [212/300][0/84], lr: 0.00000135 	 loss = 0.9836(0.9836)
2023/10/24 13:43:51 - INFO - root -   Epoch: [212/300][20/84], lr: 0.00000135 	 loss = 0.6791(0.6826)
2023/10/24 13:45:22 - INFO - root -   Epoch: [212/300][40/84], lr: 0.00000135 	 loss = 0.5682(0.7237)
2023/10/24 13:46:57 - INFO - root -   Epoch: [212/300][60/84], lr: 0.00000135 	 loss = 0.4288(0.6933)
2023/10/24 13:47:55 - INFO - root -   Epoch: [212/300][80/84], lr: 0.00000135 	 loss = 0.9637(0.7007)
2023/10/24 13:48:00 - INFO - root -   Epoch: [212/300] 	 loss = 0.7010
2023/10/24 13:48:00 - INFO - root -   train_accuracy = 0.5536
2023/10/24 13:48:30 - INFO - root -   Epoch: [213/300][0/84], lr: 0.00000136 	 loss = 0.3927(0.3927)
2023/10/24 13:49:44 - INFO - root -   Epoch: [213/300][20/84], lr: 0.00000136 	 loss = 0.4722(0.6655)
2023/10/24 13:51:15 - INFO - root -   Epoch: [213/300][40/84], lr: 0.00000136 	 loss = 0.6402(0.7190)
2023/10/24 13:52:59 - INFO - root -   Epoch: [213/300][60/84], lr: 0.00000136 	 loss = 0.6365(0.7312)
2023/10/24 13:53:54 - INFO - root -   Epoch: [213/300][80/84], lr: 0.00000136 	 loss = 0.7719(0.7295)
2023/10/24 13:53:57 - INFO - root -   Epoch: [213/300] 	 loss = 0.7315
2023/10/24 13:53:57 - INFO - root -   train_accuracy = 0.5179
2023/10/24 13:54:34 - INFO - root -   Epoch: [214/300][0/84], lr: 0.00000136 	 loss = 0.4115(0.4115)
2023/10/24 13:56:00 - INFO - root -   Epoch: [214/300][20/84], lr: 0.00000136 	 loss = 0.6583(0.7050)
2023/10/24 13:57:05 - INFO - root -   Epoch: [214/300][40/84], lr: 0.00000136 	 loss = 0.7463(0.7135)
2023/10/24 13:58:50 - INFO - root -   Epoch: [214/300][60/84], lr: 0.00000136 	 loss = 0.6932(0.6999)
2023/10/24 14:00:06 - INFO - root -   Epoch: [214/300][80/84], lr: 0.00000136 	 loss = 0.6457(0.7061)
2023/10/24 14:00:07 - INFO - root -   Epoch: [214/300] 	 loss = 0.7048
2023/10/24 14:01:26 - INFO - root -   precision = 0.5349
2023/10/24 14:01:26 - INFO - root -   eval_loss = 0.6803
2023/10/24 14:01:26 - INFO - root -   eval_acc = 0.5349
2023/10/24 14:01:27 - INFO - root -   train_accuracy = 0.5000
2023/10/24 14:01:57 - INFO - root -   Epoch: [215/300][0/84], lr: 0.00000137 	 loss = 0.6878(0.6878)
2023/10/24 14:03:11 - INFO - root -   Epoch: [215/300][20/84], lr: 0.00000137 	 loss = 0.7275(0.6595)
2023/10/24 14:04:45 - INFO - root -   Epoch: [215/300][40/84], lr: 0.00000137 	 loss = 0.8318(0.7044)
2023/10/24 14:06:07 - INFO - root -   Epoch: [215/300][60/84], lr: 0.00000137 	 loss = 0.7897(0.6860)
2023/10/24 14:07:19 - INFO - root -   Epoch: [215/300][80/84], lr: 0.00000137 	 loss = 0.7870(0.6855)
2023/10/24 14:07:20 - INFO - root -   Epoch: [215/300] 	 loss = 0.6899
2023/10/24 14:07:20 - INFO - root -   train_accuracy = 0.5536
2023/10/24 14:07:59 - INFO - root -   Epoch: [216/300][0/84], lr: 0.00000137 	 loss = 0.3850(0.3850)
2023/10/24 14:09:13 - INFO - root -   Epoch: [216/300][20/84], lr: 0.00000137 	 loss = 0.7337(0.7216)
2023/10/24 14:11:02 - INFO - root -   Epoch: [216/300][40/84], lr: 0.00000137 	 loss = 0.6002(0.7076)
2023/10/24 14:12:16 - INFO - root -   Epoch: [216/300][60/84], lr: 0.00000137 	 loss = 0.7227(0.7038)
2023/10/24 14:13:19 - INFO - root -   Epoch: [216/300][80/84], lr: 0.00000137 	 loss = 0.7538(0.7065)
2023/10/24 14:13:23 - INFO - root -   Epoch: [216/300] 	 loss = 0.7103
2023/10/24 14:13:23 - INFO - root -   train_accuracy = 0.4881
2023/10/24 14:13:53 - INFO - root -   Epoch: [217/300][0/84], lr: 0.00000138 	 loss = 0.5813(0.5813)
2023/10/24 14:15:18 - INFO - root -   Epoch: [217/300][20/84], lr: 0.00000138 	 loss = 0.7664(0.7151)
2023/10/24 14:16:25 - INFO - root -   Epoch: [217/300][40/84], lr: 0.00000138 	 loss = 0.4524(0.6978)
2023/10/24 14:18:11 - INFO - root -   Epoch: [217/300][60/84], lr: 0.00000138 	 loss = 0.5298(0.6834)
2023/10/24 14:19:15 - INFO - root -   Epoch: [217/300][80/84], lr: 0.00000138 	 loss = 0.6211(0.6689)
2023/10/24 14:19:21 - INFO - root -   Epoch: [217/300] 	 loss = 0.6722
2023/10/24 14:19:21 - INFO - root -   train_accuracy = 0.5417
2023/10/24 14:19:59 - INFO - root -   Epoch: [218/300][0/84], lr: 0.00000138 	 loss = 1.1883(1.1883)
2023/10/24 14:21:15 - INFO - root -   Epoch: [218/300][20/84], lr: 0.00000138 	 loss = 0.5908(0.7209)
2023/10/24 14:22:52 - INFO - root -   Epoch: [218/300][40/84], lr: 0.00000138 	 loss = 0.2789(0.7058)
2023/10/24 14:24:17 - INFO - root -   Epoch: [218/300][60/84], lr: 0.00000138 	 loss = 0.4101(0.7100)
2023/10/24 14:25:38 - INFO - root -   Epoch: [218/300][80/84], lr: 0.00000138 	 loss = 0.5845(0.6980)
2023/10/24 14:25:39 - INFO - root -   Epoch: [218/300] 	 loss = 0.6987
2023/10/24 14:25:39 - INFO - root -   train_accuracy = 0.5417
2023/10/24 14:26:25 - INFO - root -   Epoch: [219/300][0/84], lr: 0.00000139 	 loss = 0.7641(0.7641)
2023/10/24 14:27:44 - INFO - root -   Epoch: [219/300][20/84], lr: 0.00000139 	 loss = 0.8965(0.6582)
2023/10/24 14:29:18 - INFO - root -   Epoch: [219/300][40/84], lr: 0.00000139 	 loss = 0.5758(0.6808)
2023/10/24 14:30:44 - INFO - root -   Epoch: [219/300][60/84], lr: 0.00000139 	 loss = 0.9580(0.6830)
2023/10/24 14:31:39 - INFO - root -   Epoch: [219/300][80/84], lr: 0.00000139 	 loss = 0.9545(0.6989)
2023/10/24 14:31:40 - INFO - root -   Epoch: [219/300] 	 loss = 0.7014
2023/10/24 14:33:04 - INFO - root -   precision = 0.6744
2023/10/24 14:33:04 - INFO - root -   eval_loss = 0.6807
2023/10/24 14:33:04 - INFO - root -   eval_acc = 0.6744
2023/10/24 14:33:05 - INFO - root -   train_accuracy = 0.5476
2023/10/24 14:33:47 - INFO - root -   Epoch: [220/300][0/84], lr: 0.00000140 	 loss = 0.6360(0.6360)
2023/10/24 14:35:02 - INFO - root -   Epoch: [220/300][20/84], lr: 0.00000140 	 loss = 0.5045(0.7405)
2023/10/24 14:36:35 - INFO - root -   Epoch: [220/300][40/84], lr: 0.00000140 	 loss = 0.4917(0.7186)
2023/10/24 14:37:48 - INFO - root -   Epoch: [220/300][60/84], lr: 0.00000140 	 loss = 0.7731(0.7150)
2023/10/24 14:39:06 - INFO - root -   Epoch: [220/300][80/84], lr: 0.00000140 	 loss = 0.5968(0.7209)
2023/10/24 14:39:08 - INFO - root -   Epoch: [220/300] 	 loss = 0.7193
2023/10/24 14:39:08 - INFO - root -   train_accuracy = 0.4881
2023/10/24 14:39:45 - INFO - root -   Epoch: [221/300][0/84], lr: 0.00000140 	 loss = 0.3674(0.3674)
2023/10/24 14:41:02 - INFO - root -   Epoch: [221/300][20/84], lr: 0.00000140 	 loss = 0.7050(0.6809)
2023/10/24 14:42:31 - INFO - root -   Epoch: [221/300][40/84], lr: 0.00000140 	 loss = 0.7178(0.7344)
2023/10/24 14:44:07 - INFO - root -   Epoch: [221/300][60/84], lr: 0.00000140 	 loss = 0.4646(0.7113)
2023/10/24 14:45:08 - INFO - root -   Epoch: [221/300][80/84], lr: 0.00000140 	 loss = 0.6312(0.7068)
2023/10/24 14:45:10 - INFO - root -   Epoch: [221/300] 	 loss = 0.7067
2023/10/24 14:45:10 - INFO - root -   train_accuracy = 0.5476
2023/10/24 14:45:43 - INFO - root -   Epoch: [222/300][0/84], lr: 0.00000141 	 loss = 0.3533(0.3533)
2023/10/24 14:47:11 - INFO - root -   Epoch: [222/300][20/84], lr: 0.00000141 	 loss = 0.6078(0.7116)
2023/10/24 14:48:47 - INFO - root -   Epoch: [222/300][40/84], lr: 0.00000141 	 loss = 0.5713(0.7370)
2023/10/24 14:50:12 - INFO - root -   Epoch: [222/300][60/84], lr: 0.00000141 	 loss = 0.6367(0.7155)
2023/10/24 14:51:10 - INFO - root -   Epoch: [222/300][80/84], lr: 0.00000141 	 loss = 0.7203(0.7125)
2023/10/24 14:51:17 - INFO - root -   Epoch: [222/300] 	 loss = 0.7078
2023/10/24 14:51:17 - INFO - root -   train_accuracy = 0.4762
2023/10/24 14:51:55 - INFO - root -   Epoch: [223/300][0/84], lr: 0.00000141 	 loss = 0.4295(0.4295)
2023/10/24 14:53:21 - INFO - root -   Epoch: [223/300][20/84], lr: 0.00000141 	 loss = 0.7879(0.6593)
2023/10/24 14:54:52 - INFO - root -   Epoch: [223/300][40/84], lr: 0.00000141 	 loss = 0.3784(0.6858)
2023/10/24 14:56:06 - INFO - root -   Epoch: [223/300][60/84], lr: 0.00000141 	 loss = 0.4441(0.6829)
2023/10/24 14:57:08 - INFO - root -   Epoch: [223/300][80/84], lr: 0.00000141 	 loss = 1.4962(0.7012)
2023/10/24 14:57:09 - INFO - root -   Epoch: [223/300] 	 loss = 0.7098
2023/10/24 14:57:09 - INFO - root -   train_accuracy = 0.5536
2023/10/24 14:57:39 - INFO - root -   Epoch: [224/300][0/84], lr: 0.00000142 	 loss = 0.1371(0.1371)
2023/10/24 14:59:08 - INFO - root -   Epoch: [224/300][20/84], lr: 0.00000142 	 loss = 0.7319(0.7025)
2023/10/24 15:00:26 - INFO - root -   Epoch: [224/300][40/84], lr: 0.00000142 	 loss = 0.4973(0.7172)
2023/10/24 15:01:57 - INFO - root -   Epoch: [224/300][60/84], lr: 0.00000142 	 loss = 0.6348(0.7023)
2023/10/24 15:03:11 - INFO - root -   Epoch: [224/300][80/84], lr: 0.00000142 	 loss = 0.8323(0.6987)
2023/10/24 15:03:14 - INFO - root -   Epoch: [224/300] 	 loss = 0.6961
2023/10/24 15:04:39 - INFO - root -   precision = 0.6047
2023/10/24 15:04:39 - INFO - root -   eval_loss = 0.6743
2023/10/24 15:04:39 - INFO - root -   eval_acc = 0.6047
2023/10/24 15:04:40 - INFO - root -   train_accuracy = 0.5417
2023/10/24 15:05:13 - INFO - root -   Epoch: [225/300][0/84], lr: 0.00000143 	 loss = 0.8214(0.8214)
2023/10/24 15:06:48 - INFO - root -   Epoch: [225/300][20/84], lr: 0.00000143 	 loss = 0.5696(0.7349)
2023/10/24 15:08:10 - INFO - root -   Epoch: [225/300][40/84], lr: 0.00000143 	 loss = 0.8981(0.7474)
2023/10/24 15:09:30 - INFO - root -   Epoch: [225/300][60/84], lr: 0.00000143 	 loss = 0.4616(0.7241)
2023/10/24 15:10:28 - INFO - root -   Epoch: [225/300][80/84], lr: 0.00000143 	 loss = 0.8636(0.7208)
2023/10/24 15:10:30 - INFO - root -   Epoch: [225/300] 	 loss = 0.7243
2023/10/24 15:10:30 - INFO - root -   train_accuracy = 0.4821
2023/10/24 15:11:10 - INFO - root -   Epoch: [226/300][0/84], lr: 0.00000143 	 loss = 0.5917(0.5917)
2023/10/24 15:12:25 - INFO - root -   Epoch: [226/300][20/84], lr: 0.00000143 	 loss = 0.7171(0.7356)
2023/10/24 15:14:03 - INFO - root -   Epoch: [226/300][40/84], lr: 0.00000143 	 loss = 1.2005(0.7342)
2023/10/24 15:15:17 - INFO - root -   Epoch: [226/300][60/84], lr: 0.00000143 	 loss = 0.7752(0.7373)
2023/10/24 15:16:29 - INFO - root -   Epoch: [226/300][80/84], lr: 0.00000143 	 loss = 0.8242(0.7216)
2023/10/24 15:16:31 - INFO - root -   Epoch: [226/300] 	 loss = 0.7244
2023/10/24 15:16:31 - INFO - root -   train_accuracy = 0.5417
2023/10/24 15:17:09 - INFO - root -   Epoch: [227/300][0/84], lr: 0.00000144 	 loss = 0.3755(0.3755)
2023/10/24 15:18:24 - INFO - root -   Epoch: [227/300][20/84], lr: 0.00000144 	 loss = 0.8704(0.7283)
2023/10/24 15:20:01 - INFO - root -   Epoch: [227/300][40/84], lr: 0.00000144 	 loss = 0.9840(0.7266)
2023/10/24 15:21:16 - INFO - root -   Epoch: [227/300][60/84], lr: 0.00000144 	 loss = 0.5179(0.7022)
2023/10/24 15:22:33 - INFO - root -   Epoch: [227/300][80/84], lr: 0.00000144 	 loss = 0.6018(0.6978)
2023/10/24 15:22:35 - INFO - root -   Epoch: [227/300] 	 loss = 0.7004
2023/10/24 15:22:35 - INFO - root -   train_accuracy = 0.5238
2023/10/24 15:23:21 - INFO - root -   Epoch: [228/300][0/84], lr: 0.00000144 	 loss = 0.7267(0.7267)
2023/10/24 15:24:42 - INFO - root -   Epoch: [228/300][20/84], lr: 0.00000144 	 loss = 0.7183(0.7595)
2023/10/24 15:26:28 - INFO - root -   Epoch: [228/300][40/84], lr: 0.00000144 	 loss = 1.2542(0.7431)
2023/10/24 15:27:31 - INFO - root -   Epoch: [228/300][60/84], lr: 0.00000144 	 loss = 0.6264(0.7149)
2023/10/24 15:28:47 - INFO - root -   Epoch: [228/300][80/84], lr: 0.00000144 	 loss = 0.8005(0.7249)
2023/10/24 15:28:48 - INFO - root -   Epoch: [228/300] 	 loss = 0.7196
2023/10/24 15:28:48 - INFO - root -   train_accuracy = 0.5119
2023/10/24 15:29:34 - INFO - root -   Epoch: [229/300][0/84], lr: 0.00000145 	 loss = 0.4842(0.4842)
2023/10/24 15:30:48 - INFO - root -   Epoch: [229/300][20/84], lr: 0.00000145 	 loss = 0.8014(0.6408)
2023/10/24 15:32:31 - INFO - root -   Epoch: [229/300][40/84], lr: 0.00000145 	 loss = 0.5004(0.6738)
2023/10/24 15:33:39 - INFO - root -   Epoch: [229/300][60/84], lr: 0.00000145 	 loss = 0.5175(0.6896)
2023/10/24 15:34:48 - INFO - root -   Epoch: [229/300][80/84], lr: 0.00000145 	 loss = 0.9372(0.6950)
2023/10/24 15:34:54 - INFO - root -   Epoch: [229/300] 	 loss = 0.7024
2023/10/24 15:36:20 - INFO - root -   precision = 0.4651
2023/10/24 15:36:20 - INFO - root -   eval_loss = 0.6731
2023/10/24 15:36:20 - INFO - root -   eval_acc = 0.4651
2023/10/24 15:36:21 - INFO - root -   train_accuracy = 0.5952
2023/10/24 15:36:51 - INFO - root -   Epoch: [230/300][0/84], lr: 0.00000146 	 loss = 0.4058(0.4058)
2023/10/24 15:38:22 - INFO - root -   Epoch: [230/300][20/84], lr: 0.00000146 	 loss = 0.7932(0.6797)
2023/10/24 15:39:51 - INFO - root -   Epoch: [230/300][40/84], lr: 0.00000146 	 loss = 0.6757(0.7114)
2023/10/24 15:41:31 - INFO - root -   Epoch: [230/300][60/84], lr: 0.00000146 	 loss = 0.4263(0.6999)
2023/10/24 15:42:32 - INFO - root -   Epoch: [230/300][80/84], lr: 0.00000146 	 loss = 0.5992(0.7039)
2023/10/24 15:42:34 - INFO - root -   Epoch: [230/300] 	 loss = 0.7099
2023/10/24 15:42:34 - INFO - root -   train_accuracy = 0.5060
2023/10/24 15:43:05 - INFO - root -   Epoch: [231/300][0/84], lr: 0.00000146 	 loss = 0.4340(0.4340)
2023/10/24 15:44:36 - INFO - root -   Epoch: [231/300][20/84], lr: 0.00000146 	 loss = 0.4897(0.6583)
2023/10/24 15:45:55 - INFO - root -   Epoch: [231/300][40/84], lr: 0.00000146 	 loss = 0.5145(0.6850)
2023/10/24 15:47:41 - INFO - root -   Epoch: [231/300][60/84], lr: 0.00000146 	 loss = 0.5392(0.6926)
2023/10/24 15:48:35 - INFO - root -   Epoch: [231/300][80/84], lr: 0.00000146 	 loss = 0.8505(0.7080)
2023/10/24 15:48:44 - INFO - root -   Epoch: [231/300] 	 loss = 0.7063
2023/10/24 15:48:44 - INFO - root -   train_accuracy = 0.5238
2023/10/24 15:49:14 - INFO - root -   Epoch: [232/300][0/84], lr: 0.00000147 	 loss = 0.5338(0.5338)
2023/10/24 15:50:35 - INFO - root -   Epoch: [232/300][20/84], lr: 0.00000147 	 loss = 0.5594(0.7142)
2023/10/24 15:52:03 - INFO - root -   Epoch: [232/300][40/84], lr: 0.00000147 	 loss = 0.5742(0.6930)
2023/10/24 15:53:21 - INFO - root -   Epoch: [232/300][60/84], lr: 0.00000147 	 loss = 0.5326(0.7053)
2023/10/24 15:54:33 - INFO - root -   Epoch: [232/300][80/84], lr: 0.00000147 	 loss = 0.7303(0.6914)
2023/10/24 15:54:36 - INFO - root -   Epoch: [232/300] 	 loss = 0.6883
2023/10/24 15:54:36 - INFO - root -   train_accuracy = 0.5714
2023/10/24 15:55:06 - INFO - root -   Epoch: [233/300][0/84], lr: 0.00000147 	 loss = 0.5017(0.5017)
2023/10/24 15:56:36 - INFO - root -   Epoch: [233/300][20/84], lr: 0.00000147 	 loss = 0.5585(0.6764)
2023/10/24 15:58:13 - INFO - root -   Epoch: [233/300][40/84], lr: 0.00000147 	 loss = 0.5749(0.7169)
2023/10/24 15:59:33 - INFO - root -   Epoch: [233/300][60/84], lr: 0.00000147 	 loss = 0.5425(0.7062)
2023/10/24 16:00:37 - INFO - root -   Epoch: [233/300][80/84], lr: 0.00000147 	 loss = 0.6185(0.7147)
2023/10/24 16:00:43 - INFO - root -   Epoch: [233/300] 	 loss = 0.7158
2023/10/24 16:00:43 - INFO - root -   train_accuracy = 0.5536
2023/10/24 16:01:20 - INFO - root -   Epoch: [234/300][0/84], lr: 0.00000148 	 loss = 0.7773(0.7773)
2023/10/24 16:02:35 - INFO - root -   Epoch: [234/300][20/84], lr: 0.00000148 	 loss = 0.5173(0.6668)
2023/10/24 16:04:21 - INFO - root -   Epoch: [234/300][40/84], lr: 0.00000148 	 loss = 0.6479(0.7165)
2023/10/24 16:05:36 - INFO - root -   Epoch: [234/300][60/84], lr: 0.00000148 	 loss = 0.7210(0.7057)
2023/10/24 16:06:44 - INFO - root -   Epoch: [234/300][80/84], lr: 0.00000148 	 loss = 0.6439(0.6889)
2023/10/24 16:06:46 - INFO - root -   Epoch: [234/300] 	 loss = 0.6872
2023/10/24 16:08:10 - INFO - root -   precision = 0.6279
2023/10/24 16:08:10 - INFO - root -   eval_loss = 0.6790
2023/10/24 16:08:10 - INFO - root -   eval_acc = 0.6279
2023/10/24 16:08:11 - INFO - root -   train_accuracy = 0.5476
2023/10/24 16:08:42 - INFO - root -   Epoch: [235/300][0/84], lr: 0.00000148 	 loss = 0.6129(0.6129)
2023/10/24 16:09:57 - INFO - root -   Epoch: [235/300][20/84], lr: 0.00000148 	 loss = 0.3896(0.7221)
2023/10/24 16:11:37 - INFO - root -   Epoch: [235/300][40/84], lr: 0.00000148 	 loss = 0.6774(0.7015)
2023/10/24 16:12:52 - INFO - root -   Epoch: [235/300][60/84], lr: 0.00000148 	 loss = 0.6658(0.6887)
2023/10/24 16:14:09 - INFO - root -   Epoch: [235/300][80/84], lr: 0.00000148 	 loss = 0.4491(0.6868)
2023/10/24 16:14:15 - INFO - root -   Epoch: [235/300] 	 loss = 0.6938
2023/10/24 16:14:15 - INFO - root -   train_accuracy = 0.5298
2023/10/24 16:14:47 - INFO - root -   Epoch: [236/300][0/84], lr: 0.00000149 	 loss = 0.6701(0.6701)
2023/10/24 16:16:16 - INFO - root -   Epoch: [236/300][20/84], lr: 0.00000149 	 loss = 0.6391(0.7116)
2023/10/24 16:17:47 - INFO - root -   Epoch: [236/300][40/84], lr: 0.00000149 	 loss = 0.5146(0.6915)
2023/10/24 16:19:34 - INFO - root -   Epoch: [236/300][60/84], lr: 0.00000149 	 loss = 0.5631(0.7116)
2023/10/24 16:20:32 - INFO - root -   Epoch: [236/300][80/84], lr: 0.00000149 	 loss = 0.9005(0.7143)
2023/10/24 16:20:37 - INFO - root -   Epoch: [236/300] 	 loss = 0.7166
2023/10/24 16:20:37 - INFO - root -   train_accuracy = 0.4881
2023/10/24 16:21:17 - INFO - root -   Epoch: [237/300][0/84], lr: 0.00000150 	 loss = 0.8178(0.8178)
2023/10/24 16:22:45 - INFO - root -   Epoch: [237/300][20/84], lr: 0.00000150 	 loss = 0.6456(0.7160)
2023/10/24 16:24:04 - INFO - root -   Epoch: [237/300][40/84], lr: 0.00000150 	 loss = 0.6120(0.7104)
2023/10/24 16:25:32 - INFO - root -   Epoch: [237/300][60/84], lr: 0.00000150 	 loss = 0.4871(0.6893)
2023/10/24 16:26:30 - INFO - root -   Epoch: [237/300][80/84], lr: 0.00000150 	 loss = 0.9503(0.6986)
2023/10/24 16:26:33 - INFO - root -   Epoch: [237/300] 	 loss = 0.6994
2023/10/24 16:26:33 - INFO - root -   train_accuracy = 0.6012
2023/10/24 16:27:14 - INFO - root -   Epoch: [238/300][0/84], lr: 0.00000150 	 loss = 0.3504(0.3504)
2023/10/24 16:28:36 - INFO - root -   Epoch: [238/300][20/84], lr: 0.00000150 	 loss = 0.5968(0.6231)
2023/10/24 16:29:56 - INFO - root -   Epoch: [238/300][40/84], lr: 0.00000150 	 loss = 0.5845(0.6625)
2023/10/24 16:31:40 - INFO - root -   Epoch: [238/300][60/84], lr: 0.00000150 	 loss = 0.6594(0.6672)
2023/10/24 16:32:37 - INFO - root -   Epoch: [238/300][80/84], lr: 0.00000150 	 loss = 0.8023(0.6787)
2023/10/24 16:32:43 - INFO - root -   Epoch: [238/300] 	 loss = 0.6844
2023/10/24 16:32:43 - INFO - root -   train_accuracy = 0.5655
2023/10/24 16:33:21 - INFO - root -   Epoch: [239/300][0/84], lr: 0.00000151 	 loss = 0.4633(0.4633)
2023/10/24 16:34:58 - INFO - root -   Epoch: [239/300][20/84], lr: 0.00000151 	 loss = 1.0734(0.6671)
2023/10/24 16:36:06 - INFO - root -   Epoch: [239/300][40/84], lr: 0.00000151 	 loss = 0.6495(0.7069)
2023/10/24 16:37:57 - INFO - root -   Epoch: [239/300][60/84], lr: 0.00000151 	 loss = 0.7282(0.7182)
2023/10/24 16:38:45 - INFO - root -   Epoch: [239/300][80/84], lr: 0.00000151 	 loss = 0.5419(0.7005)
2023/10/24 16:38:48 - INFO - root -   Epoch: [239/300] 	 loss = 0.7032
2023/10/24 16:40:15 - INFO - root -   precision = 0.6512
2023/10/24 16:40:15 - INFO - root -   eval_loss = 0.6800
2023/10/24 16:40:15 - INFO - root -   eval_acc = 0.6512
2023/10/24 16:40:16 - INFO - root -   train_accuracy = 0.5536
2023/10/24 16:40:57 - INFO - root -   Epoch: [240/300][0/84], lr: 0.00000151 	 loss = 0.4986(0.4986)
2023/10/24 16:42:15 - INFO - root -   Epoch: [240/300][20/84], lr: 0.00000151 	 loss = 0.4973(0.7232)
2023/10/24 16:43:35 - INFO - root -   Epoch: [240/300][40/84], lr: 0.00000151 	 loss = 0.5716(0.7262)
2023/10/24 16:45:16 - INFO - root -   Epoch: [240/300][60/84], lr: 0.00000151 	 loss = 0.6093(0.7006)
2023/10/24 16:46:21 - INFO - root -   Epoch: [240/300][80/84], lr: 0.00000151 	 loss = 1.1117(0.6976)
2023/10/24 16:46:25 - INFO - root -   Epoch: [240/300] 	 loss = 0.6978
2023/10/24 16:46:25 - INFO - root -   train_accuracy = 0.5536
2023/10/24 16:46:54 - INFO - root -   Epoch: [241/300][0/84], lr: 0.00000152 	 loss = 0.4614(0.4614)
2023/10/24 16:48:25 - INFO - root -   Epoch: [241/300][20/84], lr: 0.00000152 	 loss = 1.0629(0.7195)
2023/10/24 16:49:47 - INFO - root -   Epoch: [241/300][40/84], lr: 0.00000152 	 loss = 0.6248(0.7214)
2023/10/24 16:51:28 - INFO - root -   Epoch: [241/300][60/84], lr: 0.00000152 	 loss = 0.6315(0.6922)
2023/10/24 16:52:32 - INFO - root -   Epoch: [241/300][80/84], lr: 0.00000152 	 loss = 0.9219(0.6998)
2023/10/24 16:52:35 - INFO - root -   Epoch: [241/300] 	 loss = 0.6972
2023/10/24 16:52:35 - INFO - root -   train_accuracy = 0.5655
2023/10/24 16:53:13 - INFO - root -   Epoch: [242/300][0/84], lr: 0.00000153 	 loss = 0.7947(0.7947)
2023/10/24 16:54:36 - INFO - root -   Epoch: [242/300][20/84], lr: 0.00000153 	 loss = 0.4563(0.6306)
2023/10/24 16:56:00 - INFO - root -   Epoch: [242/300][40/84], lr: 0.00000153 	 loss = 0.7014(0.7118)
2023/10/24 16:57:39 - INFO - root -   Epoch: [242/300][60/84], lr: 0.00000153 	 loss = 0.3549(0.7017)
2023/10/24 16:58:37 - INFO - root -   Epoch: [242/300][80/84], lr: 0.00000153 	 loss = 0.8399(0.7052)
2023/10/24 16:58:41 - INFO - root -   Epoch: [242/300] 	 loss = 0.7074
2023/10/24 16:58:41 - INFO - root -   train_accuracy = 0.5774
2023/10/24 16:59:11 - INFO - root -   Epoch: [243/300][0/84], lr: 0.00000153 	 loss = 0.5048(0.5048)
2023/10/24 17:00:46 - INFO - root -   Epoch: [243/300][20/84], lr: 0.00000153 	 loss = 0.9841(0.6539)
2023/10/24 17:01:58 - INFO - root -   Epoch: [243/300][40/84], lr: 0.00000153 	 loss = 0.4429(0.6902)
2023/10/24 17:03:41 - INFO - root -   Epoch: [243/300][60/84], lr: 0.00000153 	 loss = 0.3918(0.6858)
2023/10/24 17:04:43 - INFO - root -   Epoch: [243/300][80/84], lr: 0.00000153 	 loss = 0.6861(0.6871)
2023/10/24 17:04:53 - INFO - root -   Epoch: [243/300] 	 loss = 0.6876
2023/10/24 17:04:53 - INFO - root -   train_accuracy = 0.5655
2023/10/24 17:05:30 - INFO - root -   Epoch: [244/300][0/84], lr: 0.00000154 	 loss = 0.5244(0.5244)
2023/10/24 17:06:55 - INFO - root -   Epoch: [244/300][20/84], lr: 0.00000154 	 loss = 0.7785(0.6955)
2023/10/24 17:08:21 - INFO - root -   Epoch: [244/300][40/84], lr: 0.00000154 	 loss = 0.6511(0.6906)
2023/10/24 17:09:54 - INFO - root -   Epoch: [244/300][60/84], lr: 0.00000154 	 loss = 0.3516(0.6941)
2023/10/24 17:10:54 - INFO - root -   Epoch: [244/300][80/84], lr: 0.00000154 	 loss = 0.7139(0.6826)
2023/10/24 17:10:58 - INFO - root -   Epoch: [244/300] 	 loss = 0.6830
2023/10/24 17:12:21 - INFO - root -   precision = 0.5814
2023/10/24 17:12:21 - INFO - root -   eval_loss = 0.6764
2023/10/24 17:12:21 - INFO - root -   eval_acc = 0.5814
2023/10/24 17:12:22 - INFO - root -   train_accuracy = 0.5536
2023/10/24 17:12:52 - INFO - root -   Epoch: [245/300][0/84], lr: 0.00000154 	 loss = 0.5091(0.5091)
2023/10/24 17:14:26 - INFO - root -   Epoch: [245/300][20/84], lr: 0.00000154 	 loss = 0.4730(0.6627)
2023/10/24 17:15:45 - INFO - root -   Epoch: [245/300][40/84], lr: 0.00000154 	 loss = 1.5402(0.7072)
2023/10/24 17:17:32 - INFO - root -   Epoch: [245/300][60/84], lr: 0.00000154 	 loss = 0.4385(0.7099)
2023/10/24 17:18:34 - INFO - root -   Epoch: [245/300][80/84], lr: 0.00000154 	 loss = 0.8911(0.6975)
2023/10/24 17:18:38 - INFO - root -   Epoch: [245/300] 	 loss = 0.6992
2023/10/24 17:18:38 - INFO - root -   train_accuracy = 0.5476
2023/10/24 17:19:19 - INFO - root -   Epoch: [246/300][0/84], lr: 0.00000155 	 loss = 0.4809(0.4809)
2023/10/24 17:20:43 - INFO - root -   Epoch: [246/300][20/84], lr: 0.00000155 	 loss = 0.6574(0.6822)
2023/10/24 17:21:58 - INFO - root -   Epoch: [246/300][40/84], lr: 0.00000155 	 loss = 0.3110(0.6938)
2023/10/24 17:23:16 - INFO - root -   Epoch: [246/300][60/84], lr: 0.00000155 	 loss = 0.7395(0.6707)
2023/10/24 17:24:40 - INFO - root -   Epoch: [246/300][80/84], lr: 0.00000155 	 loss = 0.5945(0.6767)
2023/10/24 17:24:41 - INFO - root -   Epoch: [246/300] 	 loss = 0.6725
2023/10/24 17:24:41 - INFO - root -   train_accuracy = 0.5893
2023/10/24 17:25:11 - INFO - root -   Epoch: [247/300][0/84], lr: 0.00000156 	 loss = 0.5700(0.5700)
2023/10/24 17:26:33 - INFO - root -   Epoch: [247/300][20/84], lr: 0.00000156 	 loss = 0.6471(0.6792)
2023/10/24 17:27:47 - INFO - root -   Epoch: [247/300][40/84], lr: 0.00000156 	 loss = 0.5536(0.7170)
2023/10/24 17:29:33 - INFO - root -   Epoch: [247/300][60/84], lr: 0.00000156 	 loss = 0.5707(0.7010)
2023/10/24 17:30:38 - INFO - root -   Epoch: [247/300][80/84], lr: 0.00000156 	 loss = 0.6334(0.6929)
2023/10/24 17:30:42 - INFO - root -   Epoch: [247/300] 	 loss = 0.6924
2023/10/24 17:30:42 - INFO - root -   train_accuracy = 0.5714
2023/10/24 17:31:12 - INFO - root -   Epoch: [248/300][0/84], lr: 0.00000156 	 loss = 0.4029(0.4029)
2023/10/24 17:32:36 - INFO - root -   Epoch: [248/300][20/84], lr: 0.00000156 	 loss = 0.6964(0.7405)
2023/10/24 17:34:01 - INFO - root -   Epoch: [248/300][40/84], lr: 0.00000156 	 loss = 0.8626(0.7440)
2023/10/24 17:35:43 - INFO - root -   Epoch: [248/300][60/84], lr: 0.00000156 	 loss = 0.8239(0.7238)
2023/10/24 17:36:47 - INFO - root -   Epoch: [248/300][80/84], lr: 0.00000156 	 loss = 0.8415(0.7335)
2023/10/24 17:36:50 - INFO - root -   Epoch: [248/300] 	 loss = 0.7347
2023/10/24 17:36:50 - INFO - root -   train_accuracy = 0.4940
2023/10/24 17:37:27 - INFO - root -   Epoch: [249/300][0/84], lr: 0.00000157 	 loss = 0.6011(0.6011)
2023/10/24 17:38:51 - INFO - root -   Epoch: [249/300][20/84], lr: 0.00000157 	 loss = 0.9061(0.6973)
2023/10/24 17:39:59 - INFO - root -   Epoch: [249/300][40/84], lr: 0.00000157 	 loss = 0.5648(0.7037)
2023/10/24 17:41:39 - INFO - root -   Epoch: [249/300][60/84], lr: 0.00000157 	 loss = 0.6514(0.6773)
2023/10/24 17:42:49 - INFO - root -   Epoch: [249/300][80/84], lr: 0.00000157 	 loss = 0.7003(0.6842)
2023/10/24 17:42:54 - INFO - root -   Epoch: [249/300] 	 loss = 0.6942
2023/10/24 17:44:19 - INFO - root -   precision = 0.6744
2023/10/24 17:44:19 - INFO - root -   eval_loss = 0.6729
2023/10/24 17:44:19 - INFO - root -   eval_acc = 0.6744
2023/10/24 17:44:20 - INFO - root -   train_accuracy = 0.5714
2023/10/24 17:44:58 - INFO - root -   Epoch: [250/300][0/84], lr: 0.00000157 	 loss = 0.5715(0.5715)
2023/10/24 17:46:35 - INFO - root -   Epoch: [250/300][20/84], lr: 0.00000157 	 loss = 0.7489(0.7272)
2023/10/24 17:47:54 - INFO - root -   Epoch: [250/300][40/84], lr: 0.00000157 	 loss = 0.4115(0.7366)
2023/10/24 17:49:36 - INFO - root -   Epoch: [250/300][60/84], lr: 0.00000157 	 loss = 0.7795(0.6855)
2023/10/24 17:50:28 - INFO - root -   Epoch: [250/300][80/84], lr: 0.00000157 	 loss = 0.4184(0.6874)
2023/10/24 17:50:29 - INFO - root -   Epoch: [250/300] 	 loss = 0.6902
2023/10/24 17:50:29 - INFO - root -   train_accuracy = 0.5476
2023/10/24 17:51:07 - INFO - root -   Epoch: [251/300][0/84], lr: 0.00000158 	 loss = 0.4765(0.4765)
2023/10/24 17:52:46 - INFO - root -   Epoch: [251/300][20/84], lr: 0.00000158 	 loss = 0.7429(0.6487)
2023/10/24 17:54:01 - INFO - root -   Epoch: [251/300][40/84], lr: 0.00000158 	 loss = 0.7143(0.7185)
2023/10/24 17:55:42 - INFO - root -   Epoch: [251/300][60/84], lr: 0.00000158 	 loss = 0.4800(0.7045)
2023/10/24 17:56:47 - INFO - root -   Epoch: [251/300][80/84], lr: 0.00000158 	 loss = 0.5318(0.7005)
2023/10/24 17:56:51 - INFO - root -   Epoch: [251/300] 	 loss = 0.7056
2023/10/24 17:56:51 - INFO - root -   train_accuracy = 0.5000
2023/10/24 17:57:29 - INFO - root -   Epoch: [252/300][0/84], lr: 0.00000159 	 loss = 0.5499(0.5499)
2023/10/24 17:58:51 - INFO - root -   Epoch: [252/300][20/84], lr: 0.00000159 	 loss = 0.5369(0.6709)
2023/10/24 18:00:15 - INFO - root -   Epoch: [252/300][40/84], lr: 0.00000159 	 loss = 0.5317(0.6823)
2023/10/24 18:01:58 - INFO - root -   Epoch: [252/300][60/84], lr: 0.00000159 	 loss = 0.5945(0.6708)
2023/10/24 18:02:50 - INFO - root -   Epoch: [252/300][80/84], lr: 0.00000159 	 loss = 1.0769(0.6744)
2023/10/24 18:02:55 - INFO - root -   Epoch: [252/300] 	 loss = 0.6796
2023/10/24 18:02:55 - INFO - root -   train_accuracy = 0.5774
2023/10/24 18:03:33 - INFO - root -   Epoch: [253/300][0/84], lr: 0.00000159 	 loss = 0.2792(0.2792)
2023/10/24 18:05:05 - INFO - root -   Epoch: [253/300][20/84], lr: 0.00000159 	 loss = 0.6065(0.6701)
2023/10/24 18:06:21 - INFO - root -   Epoch: [253/300][40/84], lr: 0.00000159 	 loss = 1.0333(0.7128)
2023/10/24 18:07:58 - INFO - root -   Epoch: [253/300][60/84], lr: 0.00000159 	 loss = 0.6559(0.7060)
2023/10/24 18:09:00 - INFO - root -   Epoch: [253/300][80/84], lr: 0.00000159 	 loss = 0.8851(0.7121)
2023/10/24 18:09:05 - INFO - root -   Epoch: [253/300] 	 loss = 0.7116
2023/10/24 18:09:05 - INFO - root -   train_accuracy = 0.5119
2023/10/24 18:09:43 - INFO - root -   Epoch: [254/300][0/84], lr: 0.00000160 	 loss = 0.6097(0.6097)
2023/10/24 18:11:08 - INFO - root -   Epoch: [254/300][20/84], lr: 0.00000160 	 loss = 0.6840(0.6877)
2023/10/24 18:12:28 - INFO - root -   Epoch: [254/300][40/84], lr: 0.00000160 	 loss = 0.5634(0.7295)
2023/10/24 18:13:58 - INFO - root -   Epoch: [254/300][60/84], lr: 0.00000160 	 loss = 0.6969(0.7019)
2023/10/24 18:15:12 - INFO - root -   Epoch: [254/300][80/84], lr: 0.00000160 	 loss = 0.7378(0.7142)
2023/10/24 18:15:17 - INFO - root -   Epoch: [254/300] 	 loss = 0.7130
2023/10/24 18:16:36 - INFO - root -   precision = 0.5116
2023/10/24 18:16:36 - INFO - root -   eval_loss = 0.6718
2023/10/24 18:16:36 - INFO - root -   eval_acc = 0.5116
2023/10/24 18:16:37 - INFO - root -   train_accuracy = 0.5417
2023/10/24 18:17:08 - INFO - root -   Epoch: [255/300][0/84], lr: 0.00000160 	 loss = 0.5321(0.5321)
2023/10/24 18:18:36 - INFO - root -   Epoch: [255/300][20/84], lr: 0.00000160 	 loss = 0.4805(0.7153)
2023/10/24 18:19:58 - INFO - root -   Epoch: [255/300][40/84], lr: 0.00000160 	 loss = 0.6600(0.7115)
2023/10/24 18:21:32 - INFO - root -   Epoch: [255/300][60/84], lr: 0.00000160 	 loss = 0.5906(0.6820)
2023/10/24 18:22:24 - INFO - root -   Epoch: [255/300][80/84], lr: 0.00000160 	 loss = 0.4603(0.6925)
2023/10/24 18:22:31 - INFO - root -   Epoch: [255/300] 	 loss = 0.6954
2023/10/24 18:22:31 - INFO - root -   train_accuracy = 0.5595
2023/10/24 18:23:01 - INFO - root -   Epoch: [256/300][0/84], lr: 0.00000161 	 loss = 0.7884(0.7884)
2023/10/24 18:24:15 - INFO - root -   Epoch: [256/300][20/84], lr: 0.00000161 	 loss = 0.6987(0.6606)
2023/10/24 18:26:13 - INFO - root -   Epoch: [256/300][40/84], lr: 0.00000161 	 loss = 0.6333(0.6549)
2023/10/24 18:27:31 - INFO - root -   Epoch: [256/300][60/84], lr: 0.00000161 	 loss = 0.5609(0.6429)
2023/10/24 18:28:33 - INFO - root -   Epoch: [256/300][80/84], lr: 0.00000161 	 loss = 0.3514(0.6513)
2023/10/24 18:28:34 - INFO - root -   Epoch: [256/300] 	 loss = 0.6530
2023/10/24 18:28:34 - INFO - root -   train_accuracy = 0.6071
2023/10/24 18:29:14 - INFO - root -   Epoch: [257/300][0/84], lr: 0.00000161 	 loss = 0.4690(0.4690)
2023/10/24 18:30:26 - INFO - root -   Epoch: [257/300][20/84], lr: 0.00000161 	 loss = 0.8154(0.6932)
2023/10/24 18:31:54 - INFO - root -   Epoch: [257/300][40/84], lr: 0.00000161 	 loss = 0.4184(0.6985)
2023/10/24 18:33:12 - INFO - root -   Epoch: [257/300][60/84], lr: 0.00000161 	 loss = 0.5692(0.6943)
2023/10/24 18:34:26 - INFO - root -   Epoch: [257/300][80/84], lr: 0.00000161 	 loss = 0.6976(0.6982)
2023/10/24 18:34:28 - INFO - root -   Epoch: [257/300] 	 loss = 0.7001
2023/10/24 18:34:28 - INFO - root -   train_accuracy = 0.5655
2023/10/24 18:34:58 - INFO - root -   Epoch: [258/300][0/84], lr: 0.00000162 	 loss = 0.5644(0.5644)
2023/10/24 18:36:44 - INFO - root -   Epoch: [258/300][20/84], lr: 0.00000162 	 loss = 0.7084(0.7074)
2023/10/24 18:37:48 - INFO - root -   Epoch: [258/300][40/84], lr: 0.00000162 	 loss = 0.3790(0.6878)
2023/10/24 18:39:24 - INFO - root -   Epoch: [258/300][60/84], lr: 0.00000162 	 loss = 0.4619(0.6894)
2023/10/24 18:40:19 - INFO - root -   Epoch: [258/300][80/84], lr: 0.00000162 	 loss = 0.4633(0.7019)
2023/10/24 18:40:26 - INFO - root -   Epoch: [258/300] 	 loss = 0.7102
2023/10/24 18:40:26 - INFO - root -   train_accuracy = 0.5238
2023/10/24 18:41:04 - INFO - root -   Epoch: [259/300][0/84], lr: 0.00000163 	 loss = 0.6022(0.6022)
2023/10/24 18:42:21 - INFO - root -   Epoch: [259/300][20/84], lr: 0.00000163 	 loss = 0.5600(0.6716)
2023/10/24 18:44:15 - INFO - root -   Epoch: [259/300][40/84], lr: 0.00000163 	 loss = 0.9613(0.7085)
2023/10/24 18:45:13 - INFO - root -   Epoch: [259/300][60/84], lr: 0.00000163 	 loss = 0.6928(0.6839)
2023/10/24 18:46:22 - INFO - root -   Epoch: [259/300][80/84], lr: 0.00000163 	 loss = 0.5317(0.6780)
2023/10/24 18:46:29 - INFO - root -   Epoch: [259/300] 	 loss = 0.6787
2023/10/24 18:47:49 - INFO - root -   precision = 0.6279
2023/10/24 18:47:49 - INFO - root -   eval_loss = 0.6754
2023/10/24 18:47:49 - INFO - root -   eval_acc = 0.6279
2023/10/24 18:47:50 - INFO - root -   train_accuracy = 0.5774
2023/10/24 18:48:29 - INFO - root -   Epoch: [260/300][0/84], lr: 0.00000163 	 loss = 0.5847(0.5847)
2023/10/24 18:49:54 - INFO - root -   Epoch: [260/300][20/84], lr: 0.00000163 	 loss = 0.8188(0.7398)
2023/10/24 18:51:12 - INFO - root -   Epoch: [260/300][40/84], lr: 0.00000163 	 loss = 0.5043(0.7188)
2023/10/24 18:52:52 - INFO - root -   Epoch: [260/300][60/84], lr: 0.00000163 	 loss = 0.5205(0.7062)
2023/10/24 18:53:50 - INFO - root -   Epoch: [260/300][80/84], lr: 0.00000163 	 loss = 0.4862(0.7068)
2023/10/24 18:53:54 - INFO - root -   Epoch: [260/300] 	 loss = 0.7132
2023/10/24 18:53:54 - INFO - root -   train_accuracy = 0.4643
2023/10/24 18:54:24 - INFO - root -   Epoch: [261/300][0/84], lr: 0.00000164 	 loss = 0.5808(0.5808)
2023/10/24 18:55:54 - INFO - root -   Epoch: [261/300][20/84], lr: 0.00000164 	 loss = 0.5620(0.6718)
2023/10/24 18:57:31 - INFO - root -   Epoch: [261/300][40/84], lr: 0.00000164 	 loss = 0.7525(0.6820)
2023/10/24 18:58:33 - INFO - root -   Epoch: [261/300][60/84], lr: 0.00000164 	 loss = 0.6105(0.7052)
2023/10/24 18:59:48 - INFO - root -   Epoch: [261/300][80/84], lr: 0.00000164 	 loss = 0.8818(0.7080)
2023/10/24 18:59:50 - INFO - root -   Epoch: [261/300] 	 loss = 0.7088
2023/10/24 18:59:50 - INFO - root -   train_accuracy = 0.5595
2023/10/24 19:00:20 - INFO - root -   Epoch: [262/300][0/84], lr: 0.00000164 	 loss = 0.6721(0.6721)
2023/10/24 19:01:45 - INFO - root -   Epoch: [262/300][20/84], lr: 0.00000164 	 loss = 0.4898(0.7123)
2023/10/24 19:03:16 - INFO - root -   Epoch: [262/300][40/84], lr: 0.00000164 	 loss = 0.5468(0.6992)
2023/10/24 19:04:32 - INFO - root -   Epoch: [262/300][60/84], lr: 0.00000164 	 loss = 0.7067(0.6814)
2023/10/24 19:05:45 - INFO - root -   Epoch: [262/300][80/84], lr: 0.00000164 	 loss = 0.6105(0.6845)
2023/10/24 19:05:50 - INFO - root -   Epoch: [262/300] 	 loss = 0.6900
2023/10/24 19:05:50 - INFO - root -   train_accuracy = 0.5536
2023/10/24 19:06:28 - INFO - root -   Epoch: [263/300][0/84], lr: 0.00000165 	 loss = 0.7171(0.7171)
2023/10/24 19:07:42 - INFO - root -   Epoch: [263/300][20/84], lr: 0.00000165 	 loss = 0.6191(0.6931)
2023/10/24 19:09:13 - INFO - root -   Epoch: [263/300][40/84], lr: 0.00000165 	 loss = 0.6009(0.6709)
2023/10/24 19:10:43 - INFO - root -   Epoch: [263/300][60/84], lr: 0.00000165 	 loss = 0.6095(0.6602)
2023/10/24 19:11:49 - INFO - root -   Epoch: [263/300][80/84], lr: 0.00000165 	 loss = 0.5230(0.6844)
2023/10/24 19:11:52 - INFO - root -   Epoch: [263/300] 	 loss = 0.6898
2023/10/24 19:11:52 - INFO - root -   train_accuracy = 0.5179
2023/10/24 19:12:30 - INFO - root -   Epoch: [264/300][0/84], lr: 0.00000166 	 loss = 0.5586(0.5586)
2023/10/24 19:13:45 - INFO - root -   Epoch: [264/300][20/84], lr: 0.00000166 	 loss = 0.7898(0.7581)
2023/10/24 19:15:17 - INFO - root -   Epoch: [264/300][40/84], lr: 0.00000166 	 loss = 0.6203(0.7429)
2023/10/24 19:16:40 - INFO - root -   Epoch: [264/300][60/84], lr: 0.00000166 	 loss = 0.5212(0.7256)
2023/10/24 19:17:50 - INFO - root -   Epoch: [264/300][80/84], lr: 0.00000166 	 loss = 0.7943(0.7177)
2023/10/24 19:17:51 - INFO - root -   Epoch: [264/300] 	 loss = 0.7180
2023/10/24 19:19:13 - INFO - root -   precision = 0.5581
2023/10/24 19:19:13 - INFO - root -   eval_loss = 0.6723
2023/10/24 19:19:13 - INFO - root -   eval_acc = 0.5581
2023/10/24 19:19:14 - INFO - root -   train_accuracy = 0.5119
2023/10/24 19:19:51 - INFO - root -   Epoch: [265/300][0/84], lr: 0.00000166 	 loss = 0.8203(0.8203)
2023/10/24 19:21:04 - INFO - root -   Epoch: [265/300][20/84], lr: 0.00000166 	 loss = 0.5031(0.7256)
2023/10/24 19:22:45 - INFO - root -   Epoch: [265/300][40/84], lr: 0.00000166 	 loss = 0.6570(0.6973)
2023/10/24 19:24:12 - INFO - root -   Epoch: [265/300][60/84], lr: 0.00000166 	 loss = 0.7133(0.6987)
2023/10/24 19:25:12 - INFO - root -   Epoch: [265/300][80/84], lr: 0.00000166 	 loss = 0.4868(0.6897)
2023/10/24 19:25:13 - INFO - root -   Epoch: [265/300] 	 loss = 0.6941
2023/10/24 19:25:13 - INFO - root -   train_accuracy = 0.5655
2023/10/24 19:25:43 - INFO - root -   Epoch: [266/300][0/84], lr: 0.00000167 	 loss = 0.4360(0.4360)
2023/10/24 19:27:25 - INFO - root -   Epoch: [266/300][20/84], lr: 0.00000167 	 loss = 0.6163(0.7155)
2023/10/24 19:28:54 - INFO - root -   Epoch: [266/300][40/84], lr: 0.00000167 	 loss = 0.6839(0.6975)
2023/10/24 19:30:32 - INFO - root -   Epoch: [266/300][60/84], lr: 0.00000167 	 loss = 0.7532(0.7116)
