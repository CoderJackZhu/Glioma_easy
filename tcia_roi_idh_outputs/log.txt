2023/10/18 17:03:40 - INFO - root -   Num train examples = 169
2023/10/18 17:03:40 - INFO - root -   Num val examples = 43
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/10/18 17:03:40 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/10/18 17:03:40 - INFO - root -   backend = nccl
2023/10/18 17:03:40 - INFO - root -   batch_size = 2
2023/10/18 17:03:40 - INFO - root -   dropout = 0.5
2023/10/18 17:03:40 - INFO - root -   epochs = 300
2023/10/18 17:03:40 - INFO - root -   eval_freq = 5
2023/10/18 17:03:40 - INFO - root -   focal_loss = False
2023/10/18 17:03:40 - INFO - root -   input_size = 224
2023/10/18 17:03:40 - INFO - root -   is_pretrained = False
2023/10/18 17:03:40 - INFO - root -   label_smooth = False
2023/10/18 17:03:40 - INFO - root -   local_rank = -1
2023/10/18 17:03:40 - INFO - root -   lr = 1e-05
2023/10/18 17:03:40 - INFO - root -   lr_decay_rate = 0.1
2023/10/18 17:03:40 - INFO - root -   lr_steps = [50, 100]
2023/10/18 17:03:40 - INFO - root -   lr_type = cosine
2023/10/18 17:03:40 - INFO - root -   model_depth = 34
2023/10/18 17:03:40 - INFO - root -   model_name = resnet50
2023/10/18 17:03:40 - INFO - root -   momentum = 0.9
2023/10/18 17:03:40 - INFO - root -   num_classes = 2
2023/10/18 17:03:40 - INFO - root -   output = ./tcia_roi_idh_outputs
2023/10/18 17:03:40 - INFO - root -   print_freq = 20
2023/10/18 17:03:40 - INFO - root -   resume = 
2023/10/18 17:03:40 - INFO - root -   start_epoch = 0
2023/10/18 17:03:40 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/18 17:03:40 - INFO - root -   tune_from = 
2023/10/18 17:03:40 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/18 17:03:40 - INFO - root -   warmup_epoch = 20
2023/10/18 17:03:40 - INFO - root -   warmup_multiplier = 100
2023/10/18 17:03:40 - INFO - root -   weight_decay = 0.0005
2023/10/18 17:03:40 - INFO - root -   workers = 8
2023/10/18 17:15:09 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 1.5219(1.5219)
2023/10/18 17:15:18 - INFO - root -   Num train examples = 169
2023/10/18 17:15:18 - INFO - root -   Num val examples = 43
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/10/18 17:15:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/10/18 17:15:19 - INFO - root -   backend = nccl
2023/10/18 17:15:19 - INFO - root -   batch_size = 2
2023/10/18 17:15:19 - INFO - root -   dropout = 0.5
2023/10/18 17:15:19 - INFO - root -   epochs = 300
2023/10/18 17:15:19 - INFO - root -   eval_freq = 5
2023/10/18 17:15:19 - INFO - root -   focal_loss = False
2023/10/18 17:15:19 - INFO - root -   input_size = 224
2023/10/18 17:15:19 - INFO - root -   is_pretrained = False
2023/10/18 17:15:19 - INFO - root -   label_smooth = False
2023/10/18 17:15:19 - INFO - root -   local_rank = -1
2023/10/18 17:15:19 - INFO - root -   lr = 1e-05
2023/10/18 17:15:19 - INFO - root -   lr_decay_rate = 0.1
2023/10/18 17:15:19 - INFO - root -   lr_steps = [50, 100]
2023/10/18 17:15:19 - INFO - root -   lr_type = cosine
2023/10/18 17:15:19 - INFO - root -   model_depth = 34
2023/10/18 17:15:19 - INFO - root -   model_name = resnet50
2023/10/18 17:15:19 - INFO - root -   momentum = 0.9
2023/10/18 17:15:19 - INFO - root -   num_classes = 2
2023/10/18 17:15:19 - INFO - root -   output = ./tcia_roi_idh_outputs
2023/10/18 17:15:19 - INFO - root -   print_freq = 20
2023/10/18 17:15:19 - INFO - root -   resume = 
2023/10/18 17:15:19 - INFO - root -   start_epoch = 0
2023/10/18 17:15:19 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/18 17:15:19 - INFO - root -   tune_from = 
2023/10/18 17:15:19 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/18 17:15:19 - INFO - root -   warmup_epoch = 20
2023/10/18 17:15:19 - INFO - root -   warmup_multiplier = 100
2023/10/18 17:15:19 - INFO - root -   weight_decay = 0.0005
2023/10/18 17:15:19 - INFO - root -   workers = 8
2023/10/18 17:15:49 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 0.3737(0.3737)
2023/10/18 17:16:48 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 0.8392(0.8214)
2023/10/18 17:17:52 - INFO - root -   Epoch: [0/300][40/84], lr: 0.00000010 	 loss = 1.2737(0.7786)
2023/10/18 17:19:02 - INFO - root -   Epoch: [0/300][60/84], lr: 0.00000010 	 loss = 0.5414(0.7979)
2023/10/18 17:19:54 - INFO - root -   Epoch: [0/300][80/84], lr: 0.00000010 	 loss = 0.6239(0.8145)
2023/10/18 17:19:55 - INFO - root -   Epoch: [0/300] 	 loss = 0.8037
2023/10/18 17:19:55 - INFO - root -   train_accuracy = 0.4821
2023/10/18 17:20:17 - INFO - root -   Epoch: [1/300][0/84], lr: 0.00000011 	 loss = 0.4300(0.4300)
2023/10/18 17:21:24 - INFO - root -   Epoch: [1/300][20/84], lr: 0.00000011 	 loss = 0.9533(0.6818)
2023/10/18 17:22:49 - INFO - root -   Epoch: [1/300][40/84], lr: 0.00000011 	 loss = 0.6657(0.6893)
2023/10/18 17:23:44 - INFO - root -   Epoch: [1/300][60/84], lr: 0.00000011 	 loss = 0.5938(0.6600)
2023/10/18 17:24:37 - INFO - root -   Epoch: [1/300][80/84], lr: 0.00000011 	 loss = 0.6745(0.6662)
2023/10/18 17:24:41 - INFO - root -   Epoch: [1/300] 	 loss = 0.6724
2023/10/18 17:24:41 - INFO - root -   train_accuracy = 0.6071
2023/10/18 17:25:11 - INFO - root -   Epoch: [2/300][0/84], lr: 0.00000011 	 loss = 1.0552(1.0552)
2023/10/18 17:26:24 - INFO - root -   Epoch: [2/300][20/84], lr: 0.00000011 	 loss = 0.6018(0.7609)
2023/10/18 17:27:20 - INFO - root -   Epoch: [2/300][40/84], lr: 0.00000011 	 loss = 0.3883(0.7473)
2023/10/18 17:28:37 - INFO - root -   Epoch: [2/300][60/84], lr: 0.00000011 	 loss = 0.8082(0.7229)
2023/10/18 17:29:25 - INFO - root -   Epoch: [2/300][80/84], lr: 0.00000011 	 loss = 0.2342(0.7175)
2023/10/18 17:29:27 - INFO - root -   Epoch: [2/300] 	 loss = 0.7148
2023/10/18 17:29:27 - INFO - root -   train_accuracy = 0.5952
2023/10/18 17:29:58 - INFO - root -   Epoch: [3/300][0/84], lr: 0.00000012 	 loss = 0.6209(0.6209)
2023/10/18 17:30:56 - INFO - root -   Epoch: [3/300][20/84], lr: 0.00000012 	 loss = 1.1426(0.7880)
2023/10/18 17:32:00 - INFO - root -   Epoch: [3/300][40/84], lr: 0.00000012 	 loss = 0.2896(0.7467)
2023/10/18 17:33:26 - INFO - root -   Epoch: [3/300][60/84], lr: 0.00000012 	 loss = 0.1967(0.7674)
2023/10/18 17:34:06 - INFO - root -   Epoch: [3/300][80/84], lr: 0.00000012 	 loss = 0.3938(0.7726)
2023/10/18 17:34:08 - INFO - root -   Epoch: [3/300] 	 loss = 0.7729
2023/10/18 17:34:08 - INFO - root -   train_accuracy = 0.5476
2023/10/18 17:34:30 - INFO - root -   Epoch: [4/300][0/84], lr: 0.00000012 	 loss = 0.4558(0.4558)
2023/10/18 17:35:38 - INFO - root -   Epoch: [4/300][20/84], lr: 0.00000012 	 loss = 1.4241(0.7881)
2023/10/18 17:36:49 - INFO - root -   Epoch: [4/300][40/84], lr: 0.00000012 	 loss = 0.3409(0.7800)
2023/10/18 17:38:00 - INFO - root -   Epoch: [4/300][60/84], lr: 0.00000012 	 loss = 1.2444(0.7563)
2023/10/18 17:38:48 - INFO - root -   Epoch: [4/300][80/84], lr: 0.00000012 	 loss = 0.1528(0.7409)
2023/10/18 17:38:49 - INFO - root -   Epoch: [4/300] 	 loss = 0.7450
2023/10/18 17:39:46 - INFO - root -   precision = 0.5814
2023/10/18 17:39:46 - INFO - root -   eval_loss = 0.6604
2023/10/18 17:39:47 - INFO - root -   train_accuracy = 0.5833
2023/10/18 17:40:25 - INFO - root -   Epoch: [5/300][0/84], lr: 0.00000013 	 loss = 0.8473(0.8473)
2023/10/18 17:41:24 - INFO - root -   Epoch: [5/300][20/84], lr: 0.00000013 	 loss = 1.2479(0.7703)
2023/10/18 17:42:43 - INFO - root -   Epoch: [5/300][40/84], lr: 0.00000013 	 loss = 0.3173(0.7105)
2023/10/18 17:43:51 - INFO - root -   Epoch: [5/300][60/84], lr: 0.00000013 	 loss = 0.7004(0.6947)
2023/10/18 17:44:34 - INFO - root -   Epoch: [5/300][80/84], lr: 0.00000013 	 loss = 0.9711(0.7052)
2023/10/18 17:44:36 - INFO - root -   Epoch: [5/300] 	 loss = 0.7074
2023/10/18 17:44:36 - INFO - root -   train_accuracy = 0.5357
2023/10/18 17:45:13 - INFO - root -   Epoch: [6/300][0/84], lr: 0.00000014 	 loss = 0.9383(0.9383)
2023/10/18 17:45:57 - INFO - root -   Epoch: [6/300][20/84], lr: 0.00000014 	 loss = 0.7558(0.7686)
2023/10/18 17:47:11 - INFO - root -   Epoch: [6/300][40/84], lr: 0.00000014 	 loss = 0.4864(0.7864)
2023/10/18 17:48:07 - INFO - root -   Epoch: [6/300][60/84], lr: 0.00000014 	 loss = 0.4276(0.7580)
2023/10/18 17:49:07 - INFO - root -   Epoch: [6/300][80/84], lr: 0.00000014 	 loss = 0.5106(0.7547)
2023/10/18 17:49:08 - INFO - root -   Epoch: [6/300] 	 loss = 0.7571
2023/10/18 17:49:08 - INFO - root -   train_accuracy = 0.5357
2023/10/18 17:49:29 - INFO - root -   Epoch: [7/300][0/84], lr: 0.00000014 	 loss = 0.7055(0.7055)
2023/10/18 17:50:50 - INFO - root -   Epoch: [7/300][20/84], lr: 0.00000014 	 loss = 0.8440(0.7665)
2023/10/18 17:51:41 - INFO - root -   Epoch: [7/300][40/84], lr: 0.00000014 	 loss = 0.3898(0.7239)
2023/10/18 17:53:09 - INFO - root -   Epoch: [7/300][60/84], lr: 0.00000014 	 loss = 0.3385(0.7239)
2023/10/18 17:53:48 - INFO - root -   Epoch: [7/300][80/84], lr: 0.00000014 	 loss = 0.2805(0.6895)
2023/10/18 17:53:50 - INFO - root -   Epoch: [7/300] 	 loss = 0.7084
2023/10/18 17:53:50 - INFO - root -   train_accuracy = 0.5714
2023/10/18 17:54:21 - INFO - root -   Epoch: [8/300][0/84], lr: 0.00000015 	 loss = 0.5536(0.5536)
2023/10/18 17:55:25 - INFO - root -   Epoch: [8/300][20/84], lr: 0.00000015 	 loss = 0.2959(0.6740)
2023/10/18 17:56:48 - INFO - root -   Epoch: [8/300][40/84], lr: 0.00000015 	 loss = 0.5974(0.6564)
2023/10/18 17:57:43 - INFO - root -   Epoch: [8/300][60/84], lr: 0.00000015 	 loss = 0.1871(0.6983)
2023/10/18 17:58:38 - INFO - root -   Epoch: [8/300][80/84], lr: 0.00000015 	 loss = 0.2236(0.6823)
2023/10/18 17:58:39 - INFO - root -   Epoch: [8/300] 	 loss = 0.6925
2023/10/18 17:58:39 - INFO - root -   train_accuracy = 0.6071
2023/10/18 17:59:02 - INFO - root -   Epoch: [9/300][0/84], lr: 0.00000015 	 loss = 1.3075(1.3075)
2023/10/18 18:00:08 - INFO - root -   Epoch: [9/300][20/84], lr: 0.00000015 	 loss = 1.2084(0.7610)
2023/10/18 18:01:19 - INFO - root -   Epoch: [9/300][40/84], lr: 0.00000015 	 loss = 0.6958(0.7171)
2023/10/18 18:02:30 - INFO - root -   Epoch: [9/300][60/84], lr: 0.00000015 	 loss = 0.3103(0.7215)
2023/10/18 18:03:25 - INFO - root -   Epoch: [9/300][80/84], lr: 0.00000015 	 loss = 0.4830(0.7405)
2023/10/18 18:03:27 - INFO - root -   Epoch: [9/300] 	 loss = 0.7483
2023/10/18 18:04:23 - INFO - root -   precision = 0.6744
2023/10/18 18:04:23 - INFO - root -   eval_loss = 0.6008
2023/10/18 18:04:24 - INFO - root -   train_accuracy = 0.5357
2023/10/18 18:04:55 - INFO - root -   Epoch: [10/300][0/84], lr: 0.00000016 	 loss = 0.8225(0.8225)
2023/10/18 18:06:02 - INFO - root -   Epoch: [10/300][20/84], lr: 0.00000016 	 loss = 0.6103(0.6902)
2023/10/18 18:07:16 - INFO - root -   Epoch: [10/300][40/84], lr: 0.00000016 	 loss = 0.1745(0.6440)
2023/10/18 18:08:03 - INFO - root -   Epoch: [10/300][60/84], lr: 0.00000016 	 loss = 0.7093(0.6577)
2023/10/18 18:08:58 - INFO - root -   Epoch: [10/300][80/84], lr: 0.00000016 	 loss = 0.7486(0.6672)
2023/10/18 18:08:59 - INFO - root -   Epoch: [10/300] 	 loss = 0.6751
2023/10/18 18:08:59 - INFO - root -   train_accuracy = 0.6190
2023/10/18 18:09:29 - INFO - root -   Epoch: [11/300][0/84], lr: 0.00000016 	 loss = 0.6646(0.6646)
2023/10/18 18:10:40 - INFO - root -   Epoch: [11/300][20/84], lr: 0.00000016 	 loss = 1.1188(0.6877)
2023/10/18 18:11:43 - INFO - root -   Epoch: [11/300][40/84], lr: 0.00000016 	 loss = 0.4533(0.6784)
2023/10/18 18:12:34 - INFO - root -   Epoch: [11/300][60/84], lr: 0.00000016 	 loss = 0.6399(0.6461)
2023/10/18 18:13:28 - INFO - root -   Epoch: [11/300][80/84], lr: 0.00000016 	 loss = 0.3593(0.6604)
2023/10/18 18:13:29 - INFO - root -   Epoch: [11/300] 	 loss = 0.6707
2023/10/18 18:13:29 - INFO - root -   train_accuracy = 0.5952
2023/10/18 18:13:52 - INFO - root -   Epoch: [12/300][0/84], lr: 0.00000017 	 loss = 0.2990(0.2990)
2023/10/18 18:14:59 - INFO - root -   Epoch: [12/300][20/84], lr: 0.00000017 	 loss = 0.7469(0.6402)
2023/10/18 18:16:13 - INFO - root -   Epoch: [12/300][40/84], lr: 0.00000017 	 loss = 0.2129(0.7297)
2023/10/18 18:17:04 - INFO - root -   Epoch: [12/300][60/84], lr: 0.00000017 	 loss = 0.1657(0.6881)
2023/10/18 18:18:08 - INFO - root -   Epoch: [12/300][80/84], lr: 0.00000017 	 loss = 0.2696(0.6918)
2023/10/18 18:18:10 - INFO - root -   Epoch: [12/300] 	 loss = 0.6971
2023/10/18 18:18:10 - INFO - root -   train_accuracy = 0.6012
2023/10/18 18:18:32 - INFO - root -   Epoch: [13/300][0/84], lr: 0.00000018 	 loss = 0.2808(0.2808)
2023/10/18 18:19:42 - INFO - root -   Epoch: [13/300][20/84], lr: 0.00000018 	 loss = 0.9902(0.6806)
2023/10/18 18:20:47 - INFO - root -   Epoch: [13/300][40/84], lr: 0.00000018 	 loss = 0.5139(0.6935)
2023/10/18 18:21:55 - INFO - root -   Epoch: [13/300][60/84], lr: 0.00000018 	 loss = 0.5011(0.6814)
2023/10/18 18:22:45 - INFO - root -   Epoch: [13/300][80/84], lr: 0.00000018 	 loss = 0.4481(0.6841)
2023/10/18 18:22:46 - INFO - root -   Epoch: [13/300] 	 loss = 0.6835
2023/10/18 18:22:46 - INFO - root -   train_accuracy = 0.5952
2023/10/18 18:23:16 - INFO - root -   Epoch: [14/300][0/84], lr: 0.00000018 	 loss = 0.5496(0.5496)
2023/10/18 18:24:40 - INFO - root -   Epoch: [14/300][20/84], lr: 0.00000018 	 loss = 0.3188(0.6689)
2023/10/18 18:25:38 - INFO - root -   Epoch: [14/300][40/84], lr: 0.00000018 	 loss = 0.3161(0.6666)
2023/10/18 18:26:39 - INFO - root -   Epoch: [14/300][60/84], lr: 0.00000018 	 loss = 0.1081(0.6804)
2023/10/18 18:27:32 - INFO - root -   Epoch: [14/300][80/84], lr: 0.00000018 	 loss = 0.3844(0.6657)
2023/10/18 18:27:34 - INFO - root -   Epoch: [14/300] 	 loss = 0.6812
2023/10/18 18:28:30 - INFO - root -   precision = 0.7674
2023/10/18 18:28:30 - INFO - root -   eval_loss = 0.5494
2023/10/18 18:28:31 - INFO - root -   train_accuracy = 0.5714
2023/10/18 18:29:10 - INFO - root -   Epoch: [15/300][0/84], lr: 0.00000019 	 loss = 1.1259(1.1259)
2023/10/18 18:30:16 - INFO - root -   Epoch: [15/300][20/84], lr: 0.00000019 	 loss = 0.4860(0.6278)
2023/10/18 18:31:33 - INFO - root -   Epoch: [15/300][40/84], lr: 0.00000019 	 loss = 0.1638(0.6340)
2023/10/18 18:32:32 - INFO - root -   Epoch: [15/300][60/84], lr: 0.00000019 	 loss = 0.3877(0.6611)
2023/10/18 18:33:22 - INFO - root -   Epoch: [15/300][80/84], lr: 0.00000019 	 loss = 0.4349(0.6658)
2023/10/18 18:33:23 - INFO - root -   Epoch: [15/300] 	 loss = 0.6855
2023/10/18 18:33:23 - INFO - root -   train_accuracy = 0.6190
2023/10/18 18:33:45 - INFO - root -   Epoch: [16/300][0/84], lr: 0.00000019 	 loss = 0.4606(0.4606)
2023/10/18 18:34:58 - INFO - root -   Epoch: [16/300][20/84], lr: 0.00000019 	 loss = 0.1691(0.6395)
2023/10/18 18:35:59 - INFO - root -   Epoch: [16/300][40/84], lr: 0.00000019 	 loss = 1.0407(0.5818)
2023/10/18 18:37:05 - INFO - root -   Epoch: [16/300][60/84], lr: 0.00000019 	 loss = 0.5152(0.6244)
2023/10/18 18:37:49 - INFO - root -   Epoch: [16/300][80/84], lr: 0.00000019 	 loss = 0.2535(0.6228)
2023/10/18 18:37:51 - INFO - root -   Epoch: [16/300] 	 loss = 0.6317
2023/10/18 18:37:51 - INFO - root -   train_accuracy = 0.6429
2023/10/18 18:38:13 - INFO - root -   Epoch: [17/300][0/84], lr: 0.00000020 	 loss = 0.3790(0.3790)
2023/10/18 18:39:28 - INFO - root -   Epoch: [17/300][20/84], lr: 0.00000020 	 loss = 0.8220(0.6609)
2023/10/18 18:40:36 - INFO - root -   Epoch: [17/300][40/84], lr: 0.00000020 	 loss = 0.3487(0.6837)
2023/10/18 18:41:34 - INFO - root -   Epoch: [17/300][60/84], lr: 0.00000020 	 loss = 0.8642(0.7093)
2023/10/18 18:42:23 - INFO - root -   Epoch: [17/300][80/84], lr: 0.00000020 	 loss = 0.2667(0.6997)
2023/10/18 18:42:26 - INFO - root -   Epoch: [17/300] 	 loss = 0.6952
2023/10/18 18:42:26 - INFO - root -   train_accuracy = 0.5952
2023/10/18 18:42:48 - INFO - root -   Epoch: [18/300][0/84], lr: 0.00000021 	 loss = 0.5256(0.5256)
2023/10/18 18:43:56 - INFO - root -   Epoch: [18/300][20/84], lr: 0.00000021 	 loss = 0.8891(0.5872)
2023/10/18 18:45:12 - INFO - root -   Epoch: [18/300][40/84], lr: 0.00000021 	 loss = 0.4506(0.6152)
2023/10/18 18:46:19 - INFO - root -   Epoch: [18/300][60/84], lr: 0.00000021 	 loss = 0.3117(0.6378)
2023/10/18 18:47:15 - INFO - root -   Epoch: [18/300][80/84], lr: 0.00000021 	 loss = 0.1778(0.6302)
2023/10/18 18:47:16 - INFO - root -   Epoch: [18/300] 	 loss = 0.6436
2023/10/18 18:47:16 - INFO - root -   train_accuracy = 0.6250
2023/10/18 18:47:45 - INFO - root -   Epoch: [19/300][0/84], lr: 0.00000021 	 loss = 0.5121(0.5121)
2023/10/18 18:48:45 - INFO - root -   Epoch: [19/300][20/84], lr: 0.00000021 	 loss = 0.8053(0.6615)
2023/10/18 18:50:05 - INFO - root -   Epoch: [19/300][40/84], lr: 0.00000021 	 loss = 0.2616(0.6256)
2023/10/18 18:51:04 - INFO - root -   Epoch: [19/300][60/84], lr: 0.00000021 	 loss = 0.4695(0.6279)
2023/10/18 18:51:54 - INFO - root -   Epoch: [19/300][80/84], lr: 0.00000021 	 loss = 0.4948(0.6308)
2023/10/18 18:51:56 - INFO - root -   Epoch: [19/300] 	 loss = 0.6424
2023/10/18 18:52:53 - INFO - root -   precision = 0.7442
2023/10/18 18:52:53 - INFO - root -   eval_loss = 0.5102
2023/10/18 18:52:54 - INFO - root -   train_accuracy = 0.6369
2023/10/18 18:53:15 - INFO - root -   Epoch: [20/300][0/84], lr: 0.00000022 	 loss = 0.5475(0.5475)
2023/10/18 18:54:35 - INFO - root -   Epoch: [20/300][20/84], lr: 0.00000022 	 loss = 0.6947(0.6608)
2023/10/18 18:55:28 - INFO - root -   Epoch: [20/300][40/84], lr: 0.00000022 	 loss = 0.1761(0.6285)
2023/10/18 18:56:45 - INFO - root -   Epoch: [20/300][60/84], lr: 0.00000022 	 loss = 0.1466(0.6066)
2023/10/18 18:57:39 - INFO - root -   Epoch: [20/300][80/84], lr: 0.00000022 	 loss = 0.5493(0.5952)
2023/10/18 18:57:40 - INFO - root -   Epoch: [20/300] 	 loss = 0.6027
2023/10/18 18:57:40 - INFO - root -   train_accuracy = 0.6667
2023/10/18 18:58:17 - INFO - root -   Epoch: [21/300][0/84], lr: 0.00000022 	 loss = 1.2600(1.2600)
2023/10/18 18:59:15 - INFO - root -   Epoch: [21/300][20/84], lr: 0.00000022 	 loss = 0.2869(0.5686)
2023/10/18 19:00:21 - INFO - root -   Epoch: [21/300][40/84], lr: 0.00000022 	 loss = 0.6195(0.5493)
2023/10/18 19:01:28 - INFO - root -   Epoch: [21/300][60/84], lr: 0.00000022 	 loss = 0.2466(0.5792)
2023/10/18 19:02:19 - INFO - root -   Epoch: [21/300][80/84], lr: 0.00000022 	 loss = 0.1626(0.6415)
2023/10/18 19:02:20 - INFO - root -   Epoch: [21/300] 	 loss = 0.6513
2023/10/18 19:02:20 - INFO - root -   train_accuracy = 0.6607
2023/10/18 19:02:42 - INFO - root -   Epoch: [22/300][0/84], lr: 0.00000023 	 loss = 0.1939(0.1939)
2023/10/18 19:03:49 - INFO - root -   Epoch: [22/300][20/84], lr: 0.00000023 	 loss = 0.6096(0.6606)
2023/10/18 19:04:53 - INFO - root -   Epoch: [22/300][40/84], lr: 0.00000023 	 loss = 0.2160(0.5951)
2023/10/18 19:06:16 - INFO - root -   Epoch: [22/300][60/84], lr: 0.00000023 	 loss = 0.3682(0.5762)
2023/10/18 19:06:54 - INFO - root -   Epoch: [22/300][80/84], lr: 0.00000023 	 loss = 0.2619(0.6328)
2023/10/18 19:06:56 - INFO - root -   Epoch: [22/300] 	 loss = 0.6455
2023/10/18 19:06:56 - INFO - root -   train_accuracy = 0.6548
2023/10/18 19:07:18 - INFO - root -   Epoch: [23/300][0/84], lr: 0.00000024 	 loss = 1.2142(1.2142)
2023/10/18 19:08:16 - INFO - root -   Epoch: [23/300][20/84], lr: 0.00000024 	 loss = 0.3802(0.6197)
2023/10/18 19:09:24 - INFO - root -   Epoch: [23/300][40/84], lr: 0.00000024 	 loss = 0.7937(0.6083)
2023/10/18 19:10:46 - INFO - root -   Epoch: [23/300][60/84], lr: 0.00000024 	 loss = 0.3968(0.6158)
2023/10/18 19:11:34 - INFO - root -   Epoch: [23/300][80/84], lr: 0.00000024 	 loss = 0.6036(0.6524)
2023/10/18 19:11:36 - INFO - root -   Epoch: [23/300] 	 loss = 0.6541
2023/10/18 19:11:36 - INFO - root -   train_accuracy = 0.6369
2023/10/18 19:11:58 - INFO - root -   Epoch: [24/300][0/84], lr: 0.00000024 	 loss = 0.3486(0.3486)
2023/10/18 19:13:05 - INFO - root -   Epoch: [24/300][20/84], lr: 0.00000024 	 loss = 0.3910(0.6359)
2023/10/18 19:14:17 - INFO - root -   Epoch: [24/300][40/84], lr: 0.00000024 	 loss = 0.8086(0.6024)
2023/10/18 19:15:34 - INFO - root -   Epoch: [24/300][60/84], lr: 0.00000024 	 loss = 0.2628(0.6048)
2023/10/18 19:16:14 - INFO - root -   Epoch: [24/300][80/84], lr: 0.00000024 	 loss = 0.5608(0.6033)
2023/10/18 19:16:15 - INFO - root -   Epoch: [24/300] 	 loss = 0.6127
2023/10/18 19:17:12 - INFO - root -   precision = 0.7442
2023/10/18 19:17:12 - INFO - root -   eval_loss = 0.4919
2023/10/18 19:17:13 - INFO - root -   train_accuracy = 0.6369
2023/10/18 19:17:49 - INFO - root -   Epoch: [25/300][0/84], lr: 0.00000025 	 loss = 0.3545(0.3545)
2023/10/18 19:18:47 - INFO - root -   Epoch: [25/300][20/84], lr: 0.00000025 	 loss = 0.1474(0.6766)
2023/10/18 19:20:11 - INFO - root -   Epoch: [25/300][40/84], lr: 0.00000025 	 loss = 0.4795(0.6266)
2023/10/18 19:20:53 - INFO - root -   Epoch: [25/300][60/84], lr: 0.00000025 	 loss = 0.5041(0.6360)
2023/10/18 19:21:58 - INFO - root -   Epoch: [25/300][80/84], lr: 0.00000025 	 loss = 0.6983(0.6525)
2023/10/18 19:22:01 - INFO - root -   Epoch: [25/300] 	 loss = 0.6528
2023/10/18 19:22:01 - INFO - root -   train_accuracy = 0.6190
2023/10/18 19:22:30 - INFO - root -   Epoch: [26/300][0/84], lr: 0.00000025 	 loss = 0.1436(0.1436)
2023/10/18 19:23:30 - INFO - root -   Epoch: [26/300][20/84], lr: 0.00000025 	 loss = 0.7291(0.5312)
2023/10/18 19:24:39 - INFO - root -   Epoch: [26/300][40/84], lr: 0.00000025 	 loss = 0.2382(0.4748)
2023/10/18 19:25:37 - INFO - root -   Epoch: [26/300][60/84], lr: 0.00000025 	 loss = 0.4851(0.4722)
2023/10/18 19:26:30 - INFO - root -   Epoch: [26/300][80/84], lr: 0.00000025 	 loss = 0.2793(0.5125)
2023/10/18 19:26:35 - INFO - root -   Epoch: [26/300] 	 loss = 0.5218
2023/10/18 19:26:35 - INFO - root -   train_accuracy = 0.7500
2023/10/18 19:27:05 - INFO - root -   Epoch: [27/300][0/84], lr: 0.00000026 	 loss = 0.4598(0.4598)
2023/10/18 19:28:11 - INFO - root -   Epoch: [27/300][20/84], lr: 0.00000026 	 loss = 0.5177(0.6123)
2023/10/18 19:29:36 - INFO - root -   Epoch: [27/300][40/84], lr: 0.00000026 	 loss = 0.3326(0.5534)
2023/10/18 19:30:22 - INFO - root -   Epoch: [27/300][60/84], lr: 0.00000026 	 loss = 0.4292(0.6281)
2023/10/18 19:31:25 - INFO - root -   Epoch: [27/300][80/84], lr: 0.00000026 	 loss = 0.2969(0.6180)
2023/10/18 19:31:26 - INFO - root -   Epoch: [27/300] 	 loss = 0.6216
2023/10/18 19:31:27 - INFO - root -   train_accuracy = 0.7024
2023/10/18 19:31:57 - INFO - root -   Epoch: [28/300][0/84], lr: 0.00000027 	 loss = 0.4444(0.4444)
2023/10/18 19:32:50 - INFO - root -   Epoch: [28/300][20/84], lr: 0.00000027 	 loss = 1.2772(0.5738)
2023/10/18 19:33:47 - INFO - root -   Epoch: [28/300][40/84], lr: 0.00000027 	 loss = 0.4772(0.5033)
2023/10/18 19:34:55 - INFO - root -   Epoch: [28/300][60/84], lr: 0.00000027 	 loss = 0.2344(0.5239)
2023/10/18 19:35:55 - INFO - root -   Epoch: [28/300][80/84], lr: 0.00000027 	 loss = 0.0789(0.5500)
2023/10/18 19:35:59 - INFO - root -   Epoch: [28/300] 	 loss = 0.5568
2023/10/18 19:35:59 - INFO - root -   train_accuracy = 0.7440
2023/10/18 19:36:28 - INFO - root -   Epoch: [29/300][0/84], lr: 0.00000027 	 loss = 0.6670(0.6670)
2023/10/18 19:37:30 - INFO - root -   Epoch: [29/300][20/84], lr: 0.00000027 	 loss = 0.2405(0.5512)
2023/10/18 19:38:36 - INFO - root -   Epoch: [29/300][40/84], lr: 0.00000027 	 loss = 0.1257(0.5969)
2023/10/18 19:39:43 - INFO - root -   Epoch: [29/300][60/84], lr: 0.00000027 	 loss = 1.0160(0.6209)
2023/10/18 19:40:36 - INFO - root -   Epoch: [29/300][80/84], lr: 0.00000027 	 loss = 0.0954(0.6021)
2023/10/18 19:40:38 - INFO - root -   Epoch: [29/300] 	 loss = 0.6192
2023/10/18 19:41:35 - INFO - root -   precision = 0.8605
2023/10/18 19:41:35 - INFO - root -   eval_loss = 0.4415
2023/10/18 19:41:36 - INFO - root -   train_accuracy = 0.6488
2023/10/18 19:42:06 - INFO - root -   Epoch: [30/300][0/84], lr: 0.00000028 	 loss = 0.2497(0.2497)
2023/10/18 19:43:20 - INFO - root -   Epoch: [30/300][20/84], lr: 0.00000028 	 loss = 0.2091(0.5839)
2023/10/18 19:44:25 - INFO - root -   Epoch: [30/300][40/84], lr: 0.00000028 	 loss = 0.9781(0.5843)
2023/10/18 19:45:33 - INFO - root -   Epoch: [30/300][60/84], lr: 0.00000028 	 loss = 0.1643(0.6117)
2023/10/18 19:46:19 - INFO - root -   Epoch: [30/300][80/84], lr: 0.00000028 	 loss = 0.0699(0.6159)
2023/10/18 19:46:21 - INFO - root -   Epoch: [30/300] 	 loss = 0.6244
2023/10/18 19:46:21 - INFO - root -   train_accuracy = 0.6845
2023/10/18 19:46:43 - INFO - root -   Epoch: [31/300][0/84], lr: 0.00000028 	 loss = 0.3301(0.3301)
2023/10/18 19:47:44 - INFO - root -   Epoch: [31/300][20/84], lr: 0.00000028 	 loss = 0.3740(0.6635)
2023/10/18 19:48:58 - INFO - root -   Epoch: [31/300][40/84], lr: 0.00000028 	 loss = 0.3540(0.7225)
2023/10/18 19:50:00 - INFO - root -   Epoch: [31/300][60/84], lr: 0.00000028 	 loss = 0.2256(0.6999)
2023/10/18 19:50:46 - INFO - root -   Epoch: [31/300][80/84], lr: 0.00000028 	 loss = 0.2470(0.6728)
2023/10/18 19:50:49 - INFO - root -   Epoch: [31/300] 	 loss = 0.6737
2023/10/18 19:50:49 - INFO - root -   train_accuracy = 0.6488
2023/10/18 19:51:18 - INFO - root -   Epoch: [32/300][0/84], lr: 0.00000029 	 loss = 0.3471(0.3471)
2023/10/18 19:52:18 - INFO - root -   Epoch: [32/300][20/84], lr: 0.00000029 	 loss = 0.2608(0.5634)
2023/10/18 19:53:20 - INFO - root -   Epoch: [32/300][40/84], lr: 0.00000029 	 loss = 0.9128(0.5600)
2023/10/18 19:54:31 - INFO - root -   Epoch: [32/300][60/84], lr: 0.00000029 	 loss = 0.3364(0.5406)
2023/10/18 19:55:22 - INFO - root -   Epoch: [32/300][80/84], lr: 0.00000029 	 loss = 0.2809(0.5316)
2023/10/18 19:55:26 - INFO - root -   Epoch: [32/300] 	 loss = 0.5341
2023/10/18 19:55:26 - INFO - root -   train_accuracy = 0.7381
2023/10/18 19:55:57 - INFO - root -   Epoch: [33/300][0/84], lr: 0.00000029 	 loss = 0.4932(0.4932)
2023/10/18 19:57:12 - INFO - root -   Epoch: [33/300][20/84], lr: 0.00000029 	 loss = 0.6996(0.5549)
2023/10/18 19:58:08 - INFO - root -   Epoch: [33/300][40/84], lr: 0.00000029 	 loss = 0.3819(0.5139)
2023/10/18 19:59:24 - INFO - root -   Epoch: [33/300][60/84], lr: 0.00000029 	 loss = 0.0816(0.5338)
2023/10/18 20:00:01 - INFO - root -   Epoch: [33/300][80/84], lr: 0.00000029 	 loss = 0.4732(0.5845)
2023/10/18 20:00:05 - INFO - root -   Epoch: [33/300] 	 loss = 0.5972
2023/10/18 20:00:05 - INFO - root -   train_accuracy = 0.6607
2023/10/18 20:00:27 - INFO - root -   Epoch: [34/300][0/84], lr: 0.00000030 	 loss = 0.5059(0.5059)
2023/10/18 20:01:49 - INFO - root -   Epoch: [34/300][20/84], lr: 0.00000030 	 loss = 0.5633(0.5961)
2023/10/18 20:02:34 - INFO - root -   Epoch: [34/300][40/84], lr: 0.00000030 	 loss = 0.4211(0.6395)
2023/10/18 20:04:01 - INFO - root -   Epoch: [34/300][60/84], lr: 0.00000030 	 loss = 0.2829(0.6393)
2023/10/18 20:04:49 - INFO - root -   Epoch: [34/300][80/84], lr: 0.00000030 	 loss = 0.3536(0.6262)
2023/10/18 20:04:54 - INFO - root -   Epoch: [34/300] 	 loss = 0.6314
2023/10/18 20:05:51 - INFO - root -   precision = 0.7674
2023/10/18 20:05:51 - INFO - root -   eval_loss = 0.4413
2023/10/18 20:05:52 - INFO - root -   train_accuracy = 0.6548
2023/10/18 20:06:13 - INFO - root -   Epoch: [35/300][0/84], lr: 0.00000031 	 loss = 0.1618(0.1618)
2023/10/18 20:07:41 - INFO - root -   Epoch: [35/300][20/84], lr: 0.00000031 	 loss = 0.4086(0.5711)
2023/10/18 20:08:30 - INFO - root -   Epoch: [35/300][40/84], lr: 0.00000031 	 loss = 0.1061(0.5165)
2023/10/18 20:09:50 - INFO - root -   Epoch: [35/300][60/84], lr: 0.00000031 	 loss = 0.2856(0.5170)
2023/10/18 20:10:25 - INFO - root -   Epoch: [35/300][80/84], lr: 0.00000031 	 loss = 0.1071(0.5178)
2023/10/18 20:10:31 - INFO - root -   Epoch: [35/300] 	 loss = 0.5561
2023/10/18 20:10:31 - INFO - root -   train_accuracy = 0.6786
2023/10/18 20:11:01 - INFO - root -   Epoch: [36/300][0/84], lr: 0.00000031 	 loss = 0.1951(0.1951)
2023/10/18 20:12:12 - INFO - root -   Epoch: [36/300][20/84], lr: 0.00000031 	 loss = 0.1233(0.6855)
2023/10/18 20:13:32 - INFO - root -   Epoch: [36/300][40/84], lr: 0.00000031 	 loss = 0.2035(0.5494)
2023/10/18 20:14:22 - INFO - root -   Epoch: [36/300][60/84], lr: 0.00000031 	 loss = 1.1823(0.5622)
2023/10/18 20:15:15 - INFO - root -   Epoch: [36/300][80/84], lr: 0.00000031 	 loss = 0.2522(0.5624)
2023/10/18 20:15:16 - INFO - root -   Epoch: [36/300] 	 loss = 0.5775
2023/10/18 20:15:16 - INFO - root -   train_accuracy = 0.7143
2023/10/18 20:15:46 - INFO - root -   Epoch: [37/300][0/84], lr: 0.00000032 	 loss = 0.4607(0.4607)
2023/10/18 20:16:53 - INFO - root -   Epoch: [37/300][20/84], lr: 0.00000032 	 loss = 0.7170(0.5389)
2023/10/18 20:18:11 - INFO - root -   Epoch: [37/300][40/84], lr: 0.00000032 	 loss = 0.2541(0.5390)
2023/10/18 20:19:00 - INFO - root -   Epoch: [37/300][60/84], lr: 0.00000032 	 loss = 0.3605(0.5226)
2023/10/18 20:20:03 - INFO - root -   Epoch: [37/300][80/84], lr: 0.00000032 	 loss = 0.2454(0.5338)
2023/10/18 20:20:04 - INFO - root -   Epoch: [37/300] 	 loss = 0.5521
2023/10/18 20:20:04 - INFO - root -   train_accuracy = 0.7202
2023/10/18 20:20:26 - INFO - root -   Epoch: [38/300][0/84], lr: 0.00000032 	 loss = 0.0894(0.0894)
2023/10/18 20:21:28 - INFO - root -   Epoch: [38/300][20/84], lr: 0.00000032 	 loss = 0.2862(0.5256)
2023/10/18 20:22:29 - INFO - root -   Epoch: [38/300][40/84], lr: 0.00000032 	 loss = 0.1476(0.5357)
2023/10/18 20:23:45 - INFO - root -   Epoch: [38/300][60/84], lr: 0.00000032 	 loss = 0.4746(0.5679)
2023/10/18 20:24:33 - INFO - root -   Epoch: [38/300][80/84], lr: 0.00000032 	 loss = 0.1754(0.5422)
2023/10/18 20:24:35 - INFO - root -   Epoch: [38/300] 	 loss = 0.5482
2023/10/18 20:24:35 - INFO - root -   train_accuracy = 0.7143
2023/10/18 20:25:15 - INFO - root -   Epoch: [39/300][0/84], lr: 0.00000033 	 loss = 1.3759(1.3759)
2023/10/18 20:26:17 - INFO - root -   Epoch: [39/300][20/84], lr: 0.00000033 	 loss = 0.6886(0.5889)
2023/10/18 20:27:36 - INFO - root -   Epoch: [39/300][40/84], lr: 0.00000033 	 loss = 0.1298(0.5452)
2023/10/18 20:28:26 - INFO - root -   Epoch: [39/300][60/84], lr: 0.00000033 	 loss = 0.3425(0.6116)
2023/10/18 20:29:22 - INFO - root -   Epoch: [39/300][80/84], lr: 0.00000033 	 loss = 0.4219(0.5770)
2023/10/18 20:29:24 - INFO - root -   Epoch: [39/300] 	 loss = 0.5947
2023/10/18 20:30:22 - INFO - root -   precision = 0.8140
2023/10/18 20:30:22 - INFO - root -   eval_loss = 0.4133
2023/10/18 20:30:23 - INFO - root -   train_accuracy = 0.6905
2023/10/18 20:30:45 - INFO - root -   Epoch: [40/300][0/84], lr: 0.00000034 	 loss = 0.5828(0.5828)
2023/10/18 20:31:51 - INFO - root -   Epoch: [40/300][20/84], lr: 0.00000034 	 loss = 0.3025(0.5732)
2023/10/18 20:33:14 - INFO - root -   Epoch: [40/300][40/84], lr: 0.00000034 	 loss = 0.4684(0.5221)
2023/10/18 20:34:08 - INFO - root -   Epoch: [40/300][60/84], lr: 0.00000034 	 loss = 0.3753(0.5623)
2023/10/18 20:35:07 - INFO - root -   Epoch: [40/300][80/84], lr: 0.00000034 	 loss = 0.2462(0.5606)
2023/10/18 20:35:09 - INFO - root -   Epoch: [40/300] 	 loss = 0.5793
2023/10/18 20:35:09 - INFO - root -   train_accuracy = 0.7500
2023/10/18 20:35:30 - INFO - root -   Epoch: [41/300][0/84], lr: 0.00000034 	 loss = 0.2967(0.2967)
2023/10/18 20:36:42 - INFO - root -   Epoch: [41/300][20/84], lr: 0.00000034 	 loss = 0.1232(0.5575)
2023/10/18 20:37:51 - INFO - root -   Epoch: [41/300][40/84], lr: 0.00000034 	 loss = 0.3487(0.4806)
2023/10/18 20:38:53 - INFO - root -   Epoch: [41/300][60/84], lr: 0.00000034 	 loss = 0.0494(0.5355)
2023/10/18 20:39:49 - INFO - root -   Epoch: [41/300][80/84], lr: 0.00000034 	 loss = 0.0273(0.5508)
2023/10/18 20:39:50 - INFO - root -   Epoch: [41/300] 	 loss = 0.5523
2023/10/18 20:39:50 - INFO - root -   train_accuracy = 0.7024
2023/10/18 20:40:12 - INFO - root -   Epoch: [42/300][0/84], lr: 0.00000035 	 loss = 0.1637(0.1637)
2023/10/18 20:41:26 - INFO - root -   Epoch: [42/300][20/84], lr: 0.00000035 	 loss = 0.3192(0.4716)
2023/10/18 20:42:18 - INFO - root -   Epoch: [42/300][40/84], lr: 0.00000035 	 loss = 0.2943(0.4631)
2023/10/18 20:43:41 - INFO - root -   Epoch: [42/300][60/84], lr: 0.00000035 	 loss = 0.1183(0.4825)
2023/10/18 20:44:26 - INFO - root -   Epoch: [42/300][80/84], lr: 0.00000035 	 loss = 0.3815(0.5091)
2023/10/18 20:44:32 - INFO - root -   Epoch: [42/300] 	 loss = 0.5307
2023/10/18 20:44:32 - INFO - root -   train_accuracy = 0.7262
2023/10/18 20:45:02 - INFO - root -   Epoch: [43/300][0/84], lr: 0.00000035 	 loss = 0.2336(0.2336)
2023/10/18 20:46:00 - INFO - root -   Epoch: [43/300][20/84], lr: 0.00000035 	 loss = 0.4287(0.6211)
2023/10/18 20:47:20 - INFO - root -   Epoch: [43/300][40/84], lr: 0.00000035 	 loss = 0.3951(0.4890)
2023/10/18 20:48:08 - INFO - root -   Epoch: [43/300][60/84], lr: 0.00000035 	 loss = 0.2630(0.5332)
2023/10/18 20:49:18 - INFO - root -   Epoch: [43/300][80/84], lr: 0.00000035 	 loss = 0.1567(0.5566)
2023/10/18 20:49:19 - INFO - root -   Epoch: [43/300] 	 loss = 0.5725
2023/10/18 20:49:19 - INFO - root -   train_accuracy = 0.7202
2023/10/18 20:49:41 - INFO - root -   Epoch: [44/300][0/84], lr: 0.00000036 	 loss = 0.7580(0.7580)
2023/10/18 20:50:39 - INFO - root -   Epoch: [44/300][20/84], lr: 0.00000036 	 loss = 0.2062(0.4705)
2023/10/18 20:51:48 - INFO - root -   Epoch: [44/300][40/84], lr: 0.00000036 	 loss = 0.0892(0.5235)
2023/10/18 20:53:04 - INFO - root -   Epoch: [44/300][60/84], lr: 0.00000036 	 loss = 0.2868(0.5892)
2023/10/18 20:53:56 - INFO - root -   Epoch: [44/300][80/84], lr: 0.00000036 	 loss = 0.5303(0.5834)
2023/10/18 20:53:57 - INFO - root -   Epoch: [44/300] 	 loss = 0.5918
2023/10/18 20:54:54 - INFO - root -   precision = 0.8372
2023/10/18 20:54:54 - INFO - root -   eval_loss = 0.4119
2023/10/18 20:54:55 - INFO - root -   train_accuracy = 0.6964
2023/10/18 20:55:24 - INFO - root -   Epoch: [45/300][0/84], lr: 0.00000037 	 loss = 0.3470(0.3470)
2023/10/18 20:56:35 - INFO - root -   Epoch: [45/300][20/84], lr: 0.00000037 	 loss = 0.7425(0.5478)
2023/10/18 20:57:33 - INFO - root -   Epoch: [45/300][40/84], lr: 0.00000037 	 loss = 0.1887(0.5243)
2023/10/18 20:58:35 - INFO - root -   Epoch: [45/300][60/84], lr: 0.00000037 	 loss = 0.1765(0.5087)
2023/10/18 20:59:25 - INFO - root -   Epoch: [45/300][80/84], lr: 0.00000037 	 loss = 0.0291(0.4977)
2023/10/18 20:59:28 - INFO - root -   Epoch: [45/300] 	 loss = 0.5104
2023/10/18 20:59:28 - INFO - root -   train_accuracy = 0.7202
2023/10/18 20:59:49 - INFO - root -   Epoch: [46/300][0/84], lr: 0.00000037 	 loss = 0.1599(0.1599)
2023/10/18 21:00:57 - INFO - root -   Epoch: [46/300][20/84], lr: 0.00000037 	 loss = 0.1833(0.5723)
2023/10/18 21:02:11 - INFO - root -   Epoch: [46/300][40/84], lr: 0.00000037 	 loss = 0.3802(0.4926)
2023/10/18 21:03:03 - INFO - root -   Epoch: [46/300][60/84], lr: 0.00000037 	 loss = 0.3463(0.5235)
2023/10/18 21:03:54 - INFO - root -   Epoch: [46/300][80/84], lr: 0.00000037 	 loss = 0.1000(0.5276)
2023/10/18 21:03:59 - INFO - root -   Epoch: [46/300] 	 loss = 0.5541
2023/10/18 21:03:59 - INFO - root -   train_accuracy = 0.7024
2023/10/18 21:04:37 - INFO - root -   Epoch: [47/300][0/84], lr: 0.00000038 	 loss = 0.4260(0.4260)
2023/10/18 21:05:34 - INFO - root -   Epoch: [47/300][20/84], lr: 0.00000038 	 loss = 0.5700(0.4648)
2023/10/18 21:06:36 - INFO - root -   Epoch: [47/300][40/84], lr: 0.00000038 	 loss = 0.5312(0.5182)
2023/10/18 21:07:48 - INFO - root -   Epoch: [47/300][60/84], lr: 0.00000038 	 loss = 0.4225(0.5157)
2023/10/18 21:08:41 - INFO - root -   Epoch: [47/300][80/84], lr: 0.00000038 	 loss = 0.2684(0.5554)
2023/10/18 21:08:42 - INFO - root -   Epoch: [47/300] 	 loss = 0.5690
2023/10/18 21:08:42 - INFO - root -   train_accuracy = 0.6845
2023/10/18 21:09:11 - INFO - root -   Epoch: [48/300][0/84], lr: 0.00000038 	 loss = 0.3398(0.3398)
2023/10/18 21:10:17 - INFO - root -   Epoch: [48/300][20/84], lr: 0.00000038 	 loss = 0.4847(0.4158)
2023/10/18 21:11:39 - INFO - root -   Epoch: [48/300][40/84], lr: 0.00000038 	 loss = 0.1163(0.4252)
2023/10/18 21:12:43 - INFO - root -   Epoch: [48/300][60/84], lr: 0.00000038 	 loss = 0.0907(0.4374)
2023/10/18 21:13:26 - INFO - root -   Epoch: [48/300][80/84], lr: 0.00000038 	 loss = 0.3560(0.4683)
2023/10/18 21:13:27 - INFO - root -   Epoch: [48/300] 	 loss = 0.4832
2023/10/18 21:13:27 - INFO - root -   train_accuracy = 0.8095
2023/10/18 21:13:57 - INFO - root -   Epoch: [49/300][0/84], lr: 0.00000039 	 loss = 0.5861(0.5861)
2023/10/18 21:15:11 - INFO - root -   Epoch: [49/300][20/84], lr: 0.00000039 	 loss = 0.0760(0.5841)
2023/10/18 21:16:27 - INFO - root -   Epoch: [49/300][40/84], lr: 0.00000039 	 loss = 1.9657(0.6106)
2023/10/18 21:17:35 - INFO - root -   Epoch: [49/300][60/84], lr: 0.00000039 	 loss = 0.5775(0.6310)
2023/10/18 21:18:09 - INFO - root -   Epoch: [49/300][80/84], lr: 0.00000039 	 loss = 0.1498(0.6177)
2023/10/18 21:18:15 - INFO - root -   Epoch: [49/300] 	 loss = 0.6139
2023/10/18 21:19:11 - INFO - root -   precision = 0.8372
2023/10/18 21:19:11 - INFO - root -   eval_loss = 0.3753
2023/10/18 21:19:12 - INFO - root -   train_accuracy = 0.6726
2023/10/18 21:19:43 - INFO - root -   Epoch: [50/300][0/84], lr: 0.00000039 	 loss = 0.5170(0.5170)
2023/10/18 21:20:43 - INFO - root -   Epoch: [50/300][20/84], lr: 0.00000039 	 loss = 0.2232(0.4829)
2023/10/18 21:21:43 - INFO - root -   Epoch: [50/300][40/84], lr: 0.00000039 	 loss = 0.3000(0.4576)
2023/10/18 21:23:11 - INFO - root -   Epoch: [50/300][60/84], lr: 0.00000039 	 loss = 0.1661(0.4675)
2023/10/18 21:23:52 - INFO - root -   Epoch: [50/300][80/84], lr: 0.00000039 	 loss = 0.1470(0.4928)
2023/10/18 21:24:00 - INFO - root -   Epoch: [50/300] 	 loss = 0.4992
2023/10/18 21:24:00 - INFO - root -   train_accuracy = 0.7321
2023/10/18 21:24:29 - INFO - root -   Epoch: [51/300][0/84], lr: 0.00000040 	 loss = 0.1694(0.1694)
2023/10/18 21:25:29 - INFO - root -   Epoch: [51/300][20/84], lr: 0.00000040 	 loss = 0.0849(0.5321)
2023/10/18 21:26:33 - INFO - root -   Epoch: [51/300][40/84], lr: 0.00000040 	 loss = 0.1901(0.4722)
2023/10/18 21:27:40 - INFO - root -   Epoch: [51/300][60/84], lr: 0.00000040 	 loss = 0.3092(0.4922)
2023/10/18 21:28:36 - INFO - root -   Epoch: [51/300][80/84], lr: 0.00000040 	 loss = 0.0194(0.4876)
2023/10/18 21:28:40 - INFO - root -   Epoch: [51/300] 	 loss = 0.5030
2023/10/18 21:28:40 - INFO - root -   train_accuracy = 0.7560
2023/10/18 21:29:10 - INFO - root -   Epoch: [52/300][0/84], lr: 0.00000041 	 loss = 0.4066(0.4066)
2023/10/18 21:30:12 - INFO - root -   Epoch: [52/300][20/84], lr: 0.00000041 	 loss = 0.3120(0.6711)
2023/10/18 21:31:34 - INFO - root -   Epoch: [52/300][40/84], lr: 0.00000041 	 loss = 0.5405(0.5872)
2023/10/18 21:32:35 - INFO - root -   Epoch: [52/300][60/84], lr: 0.00000041 	 loss = 0.2800(0.6297)
2023/10/18 21:33:20 - INFO - root -   Epoch: [52/300][80/84], lr: 0.00000041 	 loss = 0.0713(0.6241)
2023/10/18 21:33:21 - INFO - root -   Epoch: [52/300] 	 loss = 0.6573
2023/10/18 21:33:21 - INFO - root -   train_accuracy = 0.6488
2023/10/18 21:33:43 - INFO - root -   Epoch: [53/300][0/84], lr: 0.00000041 	 loss = 0.9835(0.9835)
2023/10/18 21:34:56 - INFO - root -   Epoch: [53/300][20/84], lr: 0.00000041 	 loss = 0.3912(0.5904)
2023/10/18 21:35:53 - INFO - root -   Epoch: [53/300][40/84], lr: 0.00000041 	 loss = 0.1981(0.5599)
2023/10/18 21:37:09 - INFO - root -   Epoch: [53/300][60/84], lr: 0.00000041 	 loss = 0.5052(0.6165)
2023/10/18 21:38:04 - INFO - root -   Epoch: [53/300][80/84], lr: 0.00000041 	 loss = 0.3621(0.5884)
2023/10/18 21:38:05 - INFO - root -   Epoch: [53/300] 	 loss = 0.6021
2023/10/18 21:38:05 - INFO - root -   train_accuracy = 0.6429
2023/10/18 21:38:28 - INFO - root -   Epoch: [54/300][0/84], lr: 0.00000042 	 loss = 0.2738(0.2738)
2023/10/18 21:39:41 - INFO - root -   Epoch: [54/300][20/84], lr: 0.00000042 	 loss = 0.5559(0.5199)
2023/10/18 21:40:39 - INFO - root -   Epoch: [54/300][40/84], lr: 0.00000042 	 loss = 0.2839(0.4774)
2023/10/18 21:41:46 - INFO - root -   Epoch: [54/300][60/84], lr: 0.00000042 	 loss = 0.2393(0.5281)
2023/10/18 21:42:36 - INFO - root -   Epoch: [54/300][80/84], lr: 0.00000042 	 loss = 0.4997(0.5424)
2023/10/18 21:42:40 - INFO - root -   Epoch: [54/300] 	 loss = 0.5733
2023/10/18 21:43:37 - INFO - root -   precision = 0.8605
2023/10/18 21:43:37 - INFO - root -   eval_loss = 0.3675
2023/10/18 21:43:38 - INFO - root -   train_accuracy = 0.7321
2023/10/18 21:43:59 - INFO - root -   Epoch: [55/300][0/84], lr: 0.00000042 	 loss = 0.1965(0.1965)
2023/10/18 21:45:14 - INFO - root -   Epoch: [55/300][20/84], lr: 0.00000042 	 loss = 0.3597(0.5640)
2023/10/18 21:46:05 - INFO - root -   Epoch: [55/300][40/84], lr: 0.00000042 	 loss = 0.2051(0.4924)
2023/10/18 21:47:18 - INFO - root -   Epoch: [55/300][60/84], lr: 0.00000042 	 loss = 0.1002(0.4806)
2023/10/18 21:48:13 - INFO - root -   Epoch: [55/300][80/84], lr: 0.00000042 	 loss = 0.0400(0.4819)
2023/10/18 21:48:14 - INFO - root -   Epoch: [55/300] 	 loss = 0.5085
2023/10/18 21:48:14 - INFO - root -   train_accuracy = 0.7917
2023/10/18 21:48:44 - INFO - root -   Epoch: [56/300][0/84], lr: 0.00000043 	 loss = 0.8755(0.8755)
2023/10/18 21:49:47 - INFO - root -   Epoch: [56/300][20/84], lr: 0.00000043 	 loss = 0.5809(0.5470)
2023/10/18 21:50:47 - INFO - root -   Epoch: [56/300][40/84], lr: 0.00000043 	 loss = 0.5362(0.4938)
2023/10/18 21:51:53 - INFO - root -   Epoch: [56/300][60/84], lr: 0.00000043 	 loss = 0.5767(0.5563)
2023/10/18 21:53:02 - INFO - root -   Epoch: [56/300][80/84], lr: 0.00000043 	 loss = 0.5742(0.5555)
2023/10/18 21:53:03 - INFO - root -   Epoch: [56/300] 	 loss = 0.5695
2023/10/18 21:53:03 - INFO - root -   train_accuracy = 0.7262
2023/10/18 21:53:32 - INFO - root -   Epoch: [57/300][0/84], lr: 0.00000044 	 loss = 0.3979(0.3979)
2023/10/18 21:54:33 - INFO - root -   Epoch: [57/300][20/84], lr: 0.00000044 	 loss = 0.6016(0.6294)
2023/10/18 21:55:48 - INFO - root -   Epoch: [57/300][40/84], lr: 0.00000044 	 loss = 0.2260(0.5058)
2023/10/18 21:57:15 - INFO - root -   Epoch: [57/300][60/84], lr: 0.00000044 	 loss = 1.3170(0.5244)
2023/10/18 21:57:46 - INFO - root -   Epoch: [57/300][80/84], lr: 0.00000044 	 loss = 0.1206(0.5233)
2023/10/18 21:57:47 - INFO - root -   Epoch: [57/300] 	 loss = 0.5334
2023/10/18 21:57:47 - INFO - root -   train_accuracy = 0.6964
2023/10/18 21:58:10 - INFO - root -   Epoch: [58/300][0/84], lr: 0.00000044 	 loss = 0.1753(0.1753)
2023/10/18 21:59:30 - INFO - root -   Epoch: [58/300][20/84], lr: 0.00000044 	 loss = 0.1363(0.5241)
2023/10/18 22:00:38 - INFO - root -   Epoch: [58/300][40/84], lr: 0.00000044 	 loss = 0.1029(0.4875)
2023/10/18 22:01:45 - INFO - root -   Epoch: [58/300][60/84], lr: 0.00000044 	 loss = 0.5654(0.5307)
2023/10/18 22:02:23 - INFO - root -   Epoch: [58/300][80/84], lr: 0.00000044 	 loss = 0.0689(0.5218)
2023/10/18 22:02:24 - INFO - root -   Epoch: [58/300] 	 loss = 0.5374
2023/10/18 22:02:24 - INFO - root -   train_accuracy = 0.7857
2023/10/18 22:02:46 - INFO - root -   Epoch: [59/300][0/84], lr: 0.00000045 	 loss = 0.6601(0.6601)
2023/10/18 22:03:45 - INFO - root -   Epoch: [59/300][20/84], lr: 0.00000045 	 loss = 0.8357(0.5751)
2023/10/18 22:04:54 - INFO - root -   Epoch: [59/300][40/84], lr: 0.00000045 	 loss = 0.2351(0.5509)
2023/10/18 22:06:08 - INFO - root -   Epoch: [59/300][60/84], lr: 0.00000045 	 loss = 0.5111(0.5354)
2023/10/18 22:06:59 - INFO - root -   Epoch: [59/300][80/84], lr: 0.00000045 	 loss = 0.1589(0.5293)
2023/10/18 22:07:00 - INFO - root -   Epoch: [59/300] 	 loss = 0.5424
2023/10/18 22:07:57 - INFO - root -   precision = 0.8372
2023/10/18 22:07:57 - INFO - root -   eval_loss = 0.3614
2023/10/18 22:07:58 - INFO - root -   train_accuracy = 0.7440
2023/10/18 22:08:27 - INFO - root -   Epoch: [60/300][0/84], lr: 0.00000045 	 loss = 0.2281(0.2281)
2023/10/18 22:09:32 - INFO - root -   Epoch: [60/300][20/84], lr: 0.00000045 	 loss = 0.0665(0.5686)
2023/10/18 22:10:40 - INFO - root -   Epoch: [60/300][40/84], lr: 0.00000045 	 loss = 0.0536(0.5150)
2023/10/18 22:11:42 - INFO - root -   Epoch: [60/300][60/84], lr: 0.00000045 	 loss = 0.7508(0.5357)
2023/10/18 22:12:29 - INFO - root -   Epoch: [60/300][80/84], lr: 0.00000045 	 loss = 0.2655(0.5646)
2023/10/18 22:12:32 - INFO - root -   Epoch: [60/300] 	 loss = 0.5847
2023/10/18 22:12:32 - INFO - root -   train_accuracy = 0.7083
2023/10/18 22:13:02 - INFO - root -   Epoch: [61/300][0/84], lr: 0.00000046 	 loss = 0.1753(0.1753)
2023/10/18 22:14:17 - INFO - root -   Epoch: [61/300][20/84], lr: 0.00000046 	 loss = 0.1404(0.3990)
2023/10/18 22:15:24 - INFO - root -   Epoch: [61/300][40/84], lr: 0.00000046 	 loss = 0.1272(0.4474)
2023/10/18 22:16:32 - INFO - root -   Epoch: [61/300][60/84], lr: 0.00000046 	 loss = 0.6164(0.4883)
2023/10/18 22:17:14 - INFO - root -   Epoch: [61/300][80/84], lr: 0.00000046 	 loss = 0.1746(0.5124)
2023/10/18 22:17:17 - INFO - root -   Epoch: [61/300] 	 loss = 0.5136
2023/10/18 22:17:17 - INFO - root -   train_accuracy = 0.7619
2023/10/18 22:17:38 - INFO - root -   Epoch: [62/300][0/84], lr: 0.00000047 	 loss = 0.1416(0.1416)
2023/10/18 22:18:44 - INFO - root -   Epoch: [62/300][20/84], lr: 0.00000047 	 loss = 0.1206(0.4088)
2023/10/18 22:20:01 - INFO - root -   Epoch: [62/300][40/84], lr: 0.00000047 	 loss = 0.2940(0.3653)
2023/10/18 22:20:59 - INFO - root -   Epoch: [62/300][60/84], lr: 0.00000047 	 loss = 0.2474(0.4421)
2023/10/18 22:21:45 - INFO - root -   Epoch: [62/300][80/84], lr: 0.00000047 	 loss = 0.1478(0.4501)
2023/10/18 22:21:47 - INFO - root -   Epoch: [62/300] 	 loss = 0.4612
2023/10/18 22:21:47 - INFO - root -   train_accuracy = 0.7560
2023/10/18 22:22:10 - INFO - root -   Epoch: [63/300][0/84], lr: 0.00000047 	 loss = 0.7496(0.7496)
2023/10/18 22:23:29 - INFO - root -   Epoch: [63/300][20/84], lr: 0.00000047 	 loss = 0.4550(0.4911)
2023/10/18 22:24:15 - INFO - root -   Epoch: [63/300][40/84], lr: 0.00000047 	 loss = 0.0230(0.4783)
2023/10/18 22:25:23 - INFO - root -   Epoch: [63/300][60/84], lr: 0.00000047 	 loss = 0.3241(0.5140)
2023/10/18 22:26:20 - INFO - root -   Epoch: [63/300][80/84], lr: 0.00000047 	 loss = 0.0316(0.5605)
2023/10/18 22:26:21 - INFO - root -   Epoch: [63/300] 	 loss = 0.5645
2023/10/18 22:26:21 - INFO - root -   train_accuracy = 0.6667
2023/10/18 22:26:43 - INFO - root -   Epoch: [64/300][0/84], lr: 0.00000048 	 loss = 0.1265(0.1265)
2023/10/18 22:27:44 - INFO - root -   Epoch: [64/300][20/84], lr: 0.00000048 	 loss = 0.0823(0.5696)
2023/10/18 22:29:08 - INFO - root -   Epoch: [64/300][40/84], lr: 0.00000048 	 loss = 0.2634(0.5013)
2023/10/18 22:30:14 - INFO - root -   Epoch: [64/300][60/84], lr: 0.00000048 	 loss = 0.0579(0.5261)
2023/10/18 22:31:10 - INFO - root -   Epoch: [64/300][80/84], lr: 0.00000048 	 loss = 0.0436(0.5090)
2023/10/18 22:31:13 - INFO - root -   Epoch: [64/300] 	 loss = 0.5162
2023/10/18 22:32:09 - INFO - root -   precision = 0.7674
2023/10/18 22:32:09 - INFO - root -   eval_loss = 0.4076
2023/10/18 22:32:10 - INFO - root -   train_accuracy = 0.7083
2023/10/18 22:32:40 - INFO - root -   Epoch: [65/300][0/84], lr: 0.00000048 	 loss = 2.5365(2.5365)
2023/10/18 22:33:46 - INFO - root -   Epoch: [65/300][20/84], lr: 0.00000048 	 loss = 0.7159(0.6075)
2023/10/18 22:35:11 - INFO - root -   Epoch: [65/300][40/84], lr: 0.00000048 	 loss = 0.0398(0.5021)
2023/10/18 22:36:18 - INFO - root -   Epoch: [65/300][60/84], lr: 0.00000048 	 loss = 0.3529(0.4996)
2023/10/18 22:37:10 - INFO - root -   Epoch: [65/300][80/84], lr: 0.00000048 	 loss = 0.0499(0.5185)
2023/10/18 22:37:12 - INFO - root -   Epoch: [65/300] 	 loss = 0.5375
2023/10/18 22:37:12 - INFO - root -   train_accuracy = 0.7440
2023/10/18 22:37:34 - INFO - root -   Epoch: [66/300][0/84], lr: 0.00000049 	 loss = 0.3266(0.3266)
2023/10/18 22:38:38 - INFO - root -   Epoch: [66/300][20/84], lr: 0.00000049 	 loss = 0.1455(0.4353)
2023/10/18 22:39:43 - INFO - root -   Epoch: [66/300][40/84], lr: 0.00000049 	 loss = 0.1081(0.4213)
2023/10/18 22:41:02 - INFO - root -   Epoch: [66/300][60/84], lr: 0.00000049 	 loss = 0.0354(0.4535)
2023/10/18 22:41:49 - INFO - root -   Epoch: [66/300][80/84], lr: 0.00000049 	 loss = 0.1615(0.4862)
2023/10/18 22:41:52 - INFO - root -   Epoch: [66/300] 	 loss = 0.4996
2023/10/18 22:41:52 - INFO - root -   train_accuracy = 0.7857
2023/10/18 22:42:22 - INFO - root -   Epoch: [67/300][0/84], lr: 0.00000049 	 loss = 0.8024(0.8024)
2023/10/18 22:43:20 - INFO - root -   Epoch: [67/300][20/84], lr: 0.00000049 	 loss = 0.1595(0.6105)
2023/10/18 22:44:26 - INFO - root -   Epoch: [67/300][40/84], lr: 0.00000049 	 loss = 0.1477(0.5181)
2023/10/18 22:45:44 - INFO - root -   Epoch: [67/300][60/84], lr: 0.00000049 	 loss = 0.0932(0.5175)
2023/10/18 22:46:35 - INFO - root -   Epoch: [67/300][80/84], lr: 0.00000049 	 loss = 0.0633(0.5069)
2023/10/18 22:46:40 - INFO - root -   Epoch: [67/300] 	 loss = 0.5073
2023/10/18 22:46:40 - INFO - root -   train_accuracy = 0.7381
2023/10/18 22:47:01 - INFO - root -   Epoch: [68/300][0/84], lr: 0.00000050 	 loss = 0.0737(0.0737)
2023/10/18 22:48:24 - INFO - root -   Epoch: [68/300][20/84], lr: 0.00000050 	 loss = 0.3831(0.4512)
2023/10/18 22:49:15 - INFO - root -   Epoch: [68/300][40/84], lr: 0.00000050 	 loss = 0.0732(0.4033)
2023/10/18 22:50:28 - INFO - root -   Epoch: [68/300][60/84], lr: 0.00000050 	 loss = 0.1646(0.4312)
2023/10/18 22:51:19 - INFO - root -   Epoch: [68/300][80/84], lr: 0.00000050 	 loss = 0.1955(0.4803)
2023/10/18 22:51:20 - INFO - root -   Epoch: [68/300] 	 loss = 0.5015
2023/10/18 22:51:20 - INFO - root -   train_accuracy = 0.7202
2023/10/18 22:51:50 - INFO - root -   Epoch: [69/300][0/84], lr: 0.00000051 	 loss = 0.3667(0.3667)
2023/10/18 22:52:56 - INFO - root -   Epoch: [69/300][20/84], lr: 0.00000051 	 loss = 0.4938(0.4454)
2023/10/18 22:54:18 - INFO - root -   Epoch: [69/300][40/84], lr: 0.00000051 	 loss = 0.2775(0.4134)
2023/10/18 22:55:18 - INFO - root -   Epoch: [69/300][60/84], lr: 0.00000051 	 loss = 0.6314(0.4851)
2023/10/18 22:56:11 - INFO - root -   Epoch: [69/300][80/84], lr: 0.00000051 	 loss = 0.2777(0.4854)
2023/10/18 22:56:12 - INFO - root -   Epoch: [69/300] 	 loss = 0.5018
2023/10/18 22:57:09 - INFO - root -   precision = 0.8372
2023/10/18 22:57:09 - INFO - root -   eval_loss = 0.3266
2023/10/18 22:57:10 - INFO - root -   train_accuracy = 0.7500
2023/10/18 22:57:40 - INFO - root -   Epoch: [70/300][0/84], lr: 0.00000051 	 loss = 0.4828(0.4828)
2023/10/18 22:58:54 - INFO - root -   Epoch: [70/300][20/84], lr: 0.00000051 	 loss = 0.1556(0.4759)
2023/10/18 22:59:44 - INFO - root -   Epoch: [70/300][40/84], lr: 0.00000051 	 loss = 0.0941(0.4539)
2023/10/18 23:00:57 - INFO - root -   Epoch: [70/300][60/84], lr: 0.00000051 	 loss = 0.0751(0.4889)
2023/10/18 23:01:48 - INFO - root -   Epoch: [70/300][80/84], lr: 0.00000051 	 loss = 0.2361(0.4898)
2023/10/18 23:01:53 - INFO - root -   Epoch: [70/300] 	 loss = 0.4914
2023/10/18 23:01:53 - INFO - root -   train_accuracy = 0.7560
2023/10/18 23:02:15 - INFO - root -   Epoch: [71/300][0/84], lr: 0.00000052 	 loss = 0.1999(0.1999)
2023/10/18 23:03:42 - INFO - root -   Epoch: [71/300][20/84], lr: 0.00000052 	 loss = 0.1929(0.4168)
2023/10/18 23:04:28 - INFO - root -   Epoch: [71/300][40/84], lr: 0.00000052 	 loss = 0.6113(0.4409)
2023/10/18 23:05:58 - INFO - root -   Epoch: [71/300][60/84], lr: 0.00000052 	 loss = 0.0784(0.4771)
2023/10/18 23:06:34 - INFO - root -   Epoch: [71/300][80/84], lr: 0.00000052 	 loss = 0.0749(0.5022)
2023/10/18 23:06:38 - INFO - root -   Epoch: [71/300] 	 loss = 0.5235
2023/10/18 23:06:38 - INFO - root -   train_accuracy = 0.7560
2023/10/18 23:07:07 - INFO - root -   Epoch: [72/300][0/84], lr: 0.00000052 	 loss = 0.2218(0.2218)
2023/10/18 23:08:05 - INFO - root -   Epoch: [72/300][20/84], lr: 0.00000052 	 loss = 0.2763(0.5363)
2023/10/18 23:09:09 - INFO - root -   Epoch: [72/300][40/84], lr: 0.00000052 	 loss = 0.3287(0.4610)
2023/10/18 23:10:10 - INFO - root -   Epoch: [72/300][60/84], lr: 0.00000052 	 loss = 0.2597(0.4804)
2023/10/18 23:11:11 - INFO - root -   Epoch: [72/300][80/84], lr: 0.00000052 	 loss = 0.2006(0.4606)
2023/10/18 23:11:12 - INFO - root -   Epoch: [72/300] 	 loss = 0.4868
2023/10/18 23:11:12 - INFO - root -   train_accuracy = 0.7619
2023/10/18 23:11:35 - INFO - root -   Epoch: [73/300][0/84], lr: 0.00000053 	 loss = 0.8640(0.8640)
2023/10/18 23:12:42 - INFO - root -   Epoch: [73/300][20/84], lr: 0.00000053 	 loss = 0.2897(0.5080)
2023/10/18 23:13:41 - INFO - root -   Epoch: [73/300][40/84], lr: 0.00000053 	 loss = 0.0595(0.4753)
2023/10/18 23:14:54 - INFO - root -   Epoch: [73/300][60/84], lr: 0.00000053 	 loss = 0.2434(0.5522)
2023/10/18 23:15:40 - INFO - root -   Epoch: [73/300][80/84], lr: 0.00000053 	 loss = 0.1033(0.5005)
2023/10/18 23:15:41 - INFO - root -   Epoch: [73/300] 	 loss = 0.5150
2023/10/18 23:15:41 - INFO - root -   train_accuracy = 0.7619
2023/10/18 23:16:03 - INFO - root -   Epoch: [74/300][0/84], lr: 0.00000054 	 loss = 0.3196(0.3196)
2023/10/18 23:17:17 - INFO - root -   Epoch: [74/300][20/84], lr: 0.00000054 	 loss = 0.4106(0.5424)
2023/10/18 23:18:21 - INFO - root -   Epoch: [74/300][40/84], lr: 0.00000054 	 loss = 0.3153(0.4820)
2023/10/18 23:19:32 - INFO - root -   Epoch: [74/300][60/84], lr: 0.00000054 	 loss = 0.0820(0.4897)
2023/10/18 23:20:19 - INFO - root -   Epoch: [74/300][80/84], lr: 0.00000054 	 loss = 0.2439(0.5095)
2023/10/18 23:20:22 - INFO - root -   Epoch: [74/300] 	 loss = 0.5201
2023/10/18 23:21:19 - INFO - root -   precision = 0.8140
2023/10/18 23:21:19 - INFO - root -   eval_loss = 0.3535
2023/10/18 23:21:20 - INFO - root -   train_accuracy = 0.7679
2023/10/18 23:21:50 - INFO - root -   Epoch: [75/300][0/84], lr: 0.00000054 	 loss = 0.4413(0.4413)
2023/10/18 23:23:00 - INFO - root -   Epoch: [75/300][20/84], lr: 0.00000054 	 loss = 0.1747(0.3975)
2023/10/18 23:23:57 - INFO - root -   Epoch: [75/300][40/84], lr: 0.00000054 	 loss = 0.1885(0.4406)
2023/10/18 23:24:56 - INFO - root -   Epoch: [75/300][60/84], lr: 0.00000054 	 loss = 0.2126(0.4406)
2023/10/18 23:25:49 - INFO - root -   Epoch: [75/300][80/84], lr: 0.00000054 	 loss = 0.0786(0.4980)
2023/10/18 23:25:52 - INFO - root -   Epoch: [75/300] 	 loss = 0.5130
2023/10/18 23:25:52 - INFO - root -   train_accuracy = 0.7560
2023/10/18 23:26:21 - INFO - root -   Epoch: [76/300][0/84], lr: 0.00000055 	 loss = 0.6249(0.6249)
2023/10/18 23:27:38 - INFO - root -   Epoch: [76/300][20/84], lr: 0.00000055 	 loss = 0.7969(0.6281)
2023/10/18 23:28:36 - INFO - root -   Epoch: [76/300][40/84], lr: 0.00000055 	 loss = 0.1303(0.5090)
2023/10/18 23:29:47 - INFO - root -   Epoch: [76/300][60/84], lr: 0.00000055 	 loss = 0.1985(0.5562)
2023/10/18 23:30:50 - INFO - root -   Epoch: [76/300][80/84], lr: 0.00000055 	 loss = 0.4795(0.5589)
2023/10/18 23:30:54 - INFO - root -   Epoch: [76/300] 	 loss = 0.5621
2023/10/18 23:30:54 - INFO - root -   train_accuracy = 0.6964
2023/10/18 23:31:15 - INFO - root -   Epoch: [77/300][0/84], lr: 0.00000055 	 loss = 0.2777(0.2777)
2023/10/18 23:32:39 - INFO - root -   Epoch: [77/300][20/84], lr: 0.00000055 	 loss = 0.2836(0.5488)
2023/10/18 23:33:42 - INFO - root -   Epoch: [77/300][40/84], lr: 0.00000055 	 loss = 0.4537(0.4754)
2023/10/18 23:35:06 - INFO - root -   Epoch: [77/300][60/84], lr: 0.00000055 	 loss = 0.1323(0.4992)
2023/10/18 23:35:59 - INFO - root -   Epoch: [77/300][80/84], lr: 0.00000055 	 loss = 0.0993(0.5001)
2023/10/18 23:36:01 - INFO - root -   Epoch: [77/300] 	 loss = 0.5294
2023/10/18 23:36:01 - INFO - root -   train_accuracy = 0.7262
2023/10/18 23:36:22 - INFO - root -   Epoch: [78/300][0/84], lr: 0.00000056 	 loss = 0.2339(0.2339)
2023/10/18 23:37:29 - INFO - root -   Epoch: [78/300][20/84], lr: 0.00000056 	 loss = 0.0704(0.5326)
2023/10/18 23:38:42 - INFO - root -   Epoch: [78/300][40/84], lr: 0.00000056 	 loss = 0.0563(0.5138)
2023/10/18 23:39:40 - INFO - root -   Epoch: [78/300][60/84], lr: 0.00000056 	 loss = 0.5636(0.5323)
2023/10/18 23:40:42 - INFO - root -   Epoch: [78/300][80/84], lr: 0.00000056 	 loss = 0.1841(0.5086)
2023/10/18 23:40:43 - INFO - root -   Epoch: [78/300] 	 loss = 0.5374
2023/10/18 23:40:43 - INFO - root -   train_accuracy = 0.7500
2023/10/18 23:41:06 - INFO - root -   Epoch: [79/300][0/84], lr: 0.00000057 	 loss = 0.4510(0.4510)
2023/10/18 23:42:15 - INFO - root -   Epoch: [79/300][20/84], lr: 0.00000057 	 loss = 0.1675(0.3596)
2023/10/18 23:43:16 - INFO - root -   Epoch: [79/300][40/84], lr: 0.00000057 	 loss = 0.2318(0.3715)
2023/10/18 23:44:23 - INFO - root -   Epoch: [79/300][60/84], lr: 0.00000057 	 loss = 0.4511(0.4015)
2023/10/18 23:45:10 - INFO - root -   Epoch: [79/300][80/84], lr: 0.00000057 	 loss = 0.2302(0.4635)
2023/10/18 23:45:12 - INFO - root -   Epoch: [79/300] 	 loss = 0.4870
2023/10/18 23:46:08 - INFO - root -   precision = 0.8837
2023/10/18 23:46:08 - INFO - root -   eval_loss = 0.3095
2023/10/18 23:46:09 - INFO - root -   train_accuracy = 0.7440
2023/10/18 23:46:39 - INFO - root -   Epoch: [80/300][0/84], lr: 0.00000057 	 loss = 0.9549(0.9549)
2023/10/18 23:47:46 - INFO - root -   Epoch: [80/300][20/84], lr: 0.00000057 	 loss = 0.4863(0.4272)
2023/10/18 23:48:56 - INFO - root -   Epoch: [80/300][40/84], lr: 0.00000057 	 loss = 0.4981(0.4709)
2023/10/18 23:50:10 - INFO - root -   Epoch: [80/300][60/84], lr: 0.00000057 	 loss = 0.1531(0.4747)
2023/10/18 23:50:59 - INFO - root -   Epoch: [80/300][80/84], lr: 0.00000057 	 loss = 0.1054(0.5229)
2023/10/18 23:51:02 - INFO - root -   Epoch: [80/300] 	 loss = 0.5406
2023/10/18 23:51:02 - INFO - root -   train_accuracy = 0.7619
2023/10/18 23:51:23 - INFO - root -   Epoch: [81/300][0/84], lr: 0.00000058 	 loss = 0.4400(0.4400)
2023/10/18 23:52:30 - INFO - root -   Epoch: [81/300][20/84], lr: 0.00000058 	 loss = 0.1646(0.4231)
2023/10/18 23:53:39 - INFO - root -   Epoch: [81/300][40/84], lr: 0.00000058 	 loss = 0.1377(0.4248)
2023/10/18 23:54:52 - INFO - root -   Epoch: [81/300][60/84], lr: 0.00000058 	 loss = 0.3355(0.5007)
2023/10/18 23:55:35 - INFO - root -   Epoch: [81/300][80/84], lr: 0.00000058 	 loss = 0.0514(0.4999)
2023/10/18 23:55:41 - INFO - root -   Epoch: [81/300] 	 loss = 0.5072
2023/10/18 23:55:41 - INFO - root -   train_accuracy = 0.7143
2023/10/18 23:56:20 - INFO - root -   Epoch: [82/300][0/84], lr: 0.00000058 	 loss = 0.3922(0.3922)
2023/10/18 23:57:17 - INFO - root -   Epoch: [82/300][20/84], lr: 0.00000058 	 loss = 0.5631(0.5336)
2023/10/18 23:58:28 - INFO - root -   Epoch: [82/300][40/84], lr: 0.00000058 	 loss = 0.0480(0.4657)
2023/10/18 23:59:33 - INFO - root -   Epoch: [82/300][60/84], lr: 0.00000058 	 loss = 0.0542(0.4465)
2023/10/19 00:00:18 - INFO - root -   Epoch: [82/300][80/84], lr: 0.00000058 	 loss = 0.4244(0.4671)
2023/10/19 00:00:25 - INFO - root -   Epoch: [82/300] 	 loss = 0.4771
2023/10/19 00:00:25 - INFO - root -   train_accuracy = 0.7619
2023/10/19 00:00:54 - INFO - root -   Epoch: [83/300][0/84], lr: 0.00000059 	 loss = 0.3325(0.3325)
2023/10/19 00:02:03 - INFO - root -   Epoch: [83/300][20/84], lr: 0.00000059 	 loss = 0.2267(0.4433)
2023/10/19 00:03:27 - INFO - root -   Epoch: [83/300][40/84], lr: 0.00000059 	 loss = 0.9528(0.4747)
2023/10/19 00:04:17 - INFO - root -   Epoch: [83/300][60/84], lr: 0.00000059 	 loss = 0.0536(0.4702)
2023/10/19 00:05:09 - INFO - root -   Epoch: [83/300][80/84], lr: 0.00000059 	 loss = 0.2233(0.4884)
2023/10/19 00:05:11 - INFO - root -   Epoch: [83/300] 	 loss = 0.5042
2023/10/19 00:05:11 - INFO - root -   train_accuracy = 0.7321
2023/10/19 00:05:32 - INFO - root -   Epoch: [84/300][0/84], lr: 0.00000060 	 loss = 0.1206(0.1206)
2023/10/19 00:06:46 - INFO - root -   Epoch: [84/300][20/84], lr: 0.00000060 	 loss = 0.2249(0.6029)
2023/10/19 00:07:36 - INFO - root -   Epoch: [84/300][40/84], lr: 0.00000060 	 loss = 0.3619(0.5493)
2023/10/19 00:08:51 - INFO - root -   Epoch: [84/300][60/84], lr: 0.00000060 	 loss = 0.3935(0.5347)
2023/10/19 00:09:36 - INFO - root -   Epoch: [84/300][80/84], lr: 0.00000060 	 loss = 0.0874(0.5165)
2023/10/19 00:09:42 - INFO - root -   Epoch: [84/300] 	 loss = 0.5423
2023/10/19 00:10:39 - INFO - root -   precision = 0.8605
2023/10/19 00:10:39 - INFO - root -   eval_loss = 0.3350
2023/10/19 00:10:40 - INFO - root -   train_accuracy = 0.7202
2023/10/19 00:11:01 - INFO - root -   Epoch: [85/300][0/84], lr: 0.00000060 	 loss = 0.1591(0.1591)
2023/10/19 00:12:09 - INFO - root -   Epoch: [85/300][20/84], lr: 0.00000060 	 loss = 0.4818(0.5416)
2023/10/19 00:13:16 - INFO - root -   Epoch: [85/300][40/84], lr: 0.00000060 	 loss = 0.1442(0.4662)
2023/10/19 00:14:33 - INFO - root -   Epoch: [85/300][60/84], lr: 0.00000060 	 loss = 0.1059(0.4889)
2023/10/19 00:15:21 - INFO - root -   Epoch: [85/300][80/84], lr: 0.00000060 	 loss = 0.3041(0.4787)
2023/10/19 00:15:23 - INFO - root -   Epoch: [85/300] 	 loss = 0.4934
2023/10/19 00:15:23 - INFO - root -   train_accuracy = 0.7679
2023/10/19 00:15:45 - INFO - root -   Epoch: [86/300][0/84], lr: 0.00000061 	 loss = 0.1166(0.1166)
2023/10/19 00:16:52 - INFO - root -   Epoch: [86/300][20/84], lr: 0.00000061 	 loss = 0.1933(0.4303)
2023/10/19 00:18:19 - INFO - root -   Epoch: [86/300][40/84], lr: 0.00000061 	 loss = 0.2080(0.4309)
2023/10/19 00:19:09 - INFO - root -   Epoch: [86/300][60/84], lr: 0.00000061 	 loss = 0.2219(0.4296)
2023/10/19 00:20:01 - INFO - root -   Epoch: [86/300][80/84], lr: 0.00000061 	 loss = 0.3025(0.4566)
2023/10/19 00:20:02 - INFO - root -   Epoch: [86/300] 	 loss = 0.4908
2023/10/19 00:20:02 - INFO - root -   train_accuracy = 0.8036
2023/10/19 00:20:24 - INFO - root -   Epoch: [87/300][0/84], lr: 0.00000061 	 loss = 0.1851(0.1851)
2023/10/19 00:21:30 - INFO - root -   Epoch: [87/300][20/84], lr: 0.00000061 	 loss = 0.0336(0.4178)
2023/10/19 00:22:36 - INFO - root -   Epoch: [87/300][40/84], lr: 0.00000061 	 loss = 0.8200(0.4820)
2023/10/19 00:23:52 - INFO - root -   Epoch: [87/300][60/84], lr: 0.00000061 	 loss = 0.0564(0.4212)
2023/10/19 00:24:31 - INFO - root -   Epoch: [87/300][80/84], lr: 0.00000061 	 loss = 0.4640(0.4328)
2023/10/19 00:24:34 - INFO - root -   Epoch: [87/300] 	 loss = 0.4373
2023/10/19 00:24:34 - INFO - root -   train_accuracy = 0.8155
2023/10/19 00:24:56 - INFO - root -   Epoch: [88/300][0/84], lr: 0.00000062 	 loss = 0.6034(0.6034)
2023/10/19 00:26:04 - INFO - root -   Epoch: [88/300][20/84], lr: 0.00000062 	 loss = 0.0888(0.4856)
2023/10/19 00:27:28 - INFO - root -   Epoch: [88/300][40/84], lr: 0.00000062 	 loss = 0.1361(0.4154)
2023/10/19 00:28:26 - INFO - root -   Epoch: [88/300][60/84], lr: 0.00000062 	 loss = 0.1687(0.4438)
2023/10/19 00:29:20 - INFO - root -   Epoch: [88/300][80/84], lr: 0.00000062 	 loss = 0.5415(0.4359)
2023/10/19 00:29:21 - INFO - root -   Epoch: [88/300] 	 loss = 0.4540
2023/10/19 00:29:21 - INFO - root -   train_accuracy = 0.7619
2023/10/19 00:30:00 - INFO - root -   Epoch: [89/300][0/84], lr: 0.00000062 	 loss = 0.2029(0.2029)
2023/10/19 00:30:58 - INFO - root -   Epoch: [89/300][20/84], lr: 0.00000062 	 loss = 0.6796(0.4465)
2023/10/19 00:32:15 - INFO - root -   Epoch: [89/300][40/84], lr: 0.00000062 	 loss = 0.3037(0.5492)
2023/10/19 00:33:05 - INFO - root -   Epoch: [89/300][60/84], lr: 0.00000062 	 loss = 0.1364(0.5253)
2023/10/19 00:34:10 - INFO - root -   Epoch: [89/300][80/84], lr: 0.00000062 	 loss = 0.0854(0.5384)
2023/10/19 00:34:11 - INFO - root -   Epoch: [89/300] 	 loss = 0.5477
2023/10/19 00:35:08 - INFO - root -   precision = 0.8837
2023/10/19 00:35:08 - INFO - root -   eval_loss = 0.3231
2023/10/19 00:35:09 - INFO - root -   train_accuracy = 0.7381
2023/10/19 00:35:30 - INFO - root -   Epoch: [90/300][0/84], lr: 0.00000063 	 loss = 0.0630(0.0630)
2023/10/19 00:36:36 - INFO - root -   Epoch: [90/300][20/84], lr: 0.00000063 	 loss = 0.0692(0.3968)
2023/10/19 00:37:53 - INFO - root -   Epoch: [90/300][40/84], lr: 0.00000063 	 loss = 0.1937(0.4429)
2023/10/19 00:38:53 - INFO - root -   Epoch: [90/300][60/84], lr: 0.00000063 	 loss = 0.2237(0.4753)
2023/10/19 00:39:49 - INFO - root -   Epoch: [90/300][80/84], lr: 0.00000063 	 loss = 0.0601(0.4631)
2023/10/19 00:39:50 - INFO - root -   Epoch: [90/300] 	 loss = 0.4636
2023/10/19 00:39:50 - INFO - root -   train_accuracy = 0.7560
2023/10/19 00:40:12 - INFO - root -   Epoch: [91/300][0/84], lr: 0.00000064 	 loss = 0.6619(0.6619)
2023/10/19 00:41:20 - INFO - root -   Epoch: [91/300][20/84], lr: 0.00000064 	 loss = 0.0360(0.5496)
2023/10/19 00:42:27 - INFO - root -   Epoch: [91/300][40/84], lr: 0.00000064 	 loss = 0.1642(0.5480)
2023/10/19 00:43:36 - INFO - root -   Epoch: [91/300][60/84], lr: 0.00000064 	 loss = 0.2102(0.5278)
2023/10/19 00:44:23 - INFO - root -   Epoch: [91/300][80/84], lr: 0.00000064 	 loss = 0.0268(0.5124)
2023/10/19 00:44:28 - INFO - root -   Epoch: [91/300] 	 loss = 0.5222
2023/10/19 00:44:28 - INFO - root -   train_accuracy = 0.7143
2023/10/19 00:44:49 - INFO - root -   Epoch: [92/300][0/84], lr: 0.00000064 	 loss = 0.1685(0.1685)
2023/10/19 00:45:47 - INFO - root -   Epoch: [92/300][20/84], lr: 0.00000064 	 loss = 0.5759(0.4174)
2023/10/19 00:47:09 - INFO - root -   Epoch: [92/300][40/84], lr: 0.00000064 	 loss = 0.4715(0.4221)
2023/10/19 00:48:20 - INFO - root -   Epoch: [92/300][60/84], lr: 0.00000064 	 loss = 0.0912(0.4844)
2023/10/19 00:49:02 - INFO - root -   Epoch: [92/300][80/84], lr: 0.00000064 	 loss = 0.0335(0.4588)
2023/10/19 00:49:04 - INFO - root -   Epoch: [92/300] 	 loss = 0.4693
2023/10/19 00:49:04 - INFO - root -   train_accuracy = 0.7560
2023/10/19 00:49:25 - INFO - root -   Epoch: [93/300][0/84], lr: 0.00000065 	 loss = 0.0673(0.0673)
2023/10/19 00:50:31 - INFO - root -   Epoch: [93/300][20/84], lr: 0.00000065 	 loss = 0.4672(0.5272)
2023/10/19 00:51:40 - INFO - root -   Epoch: [93/300][40/84], lr: 0.00000065 	 loss = 0.1066(0.5027)
2023/10/19 00:52:48 - INFO - root -   Epoch: [93/300][60/84], lr: 0.00000065 	 loss = 0.3031(0.4917)
2023/10/19 00:53:42 - INFO - root -   Epoch: [93/300][80/84], lr: 0.00000065 	 loss = 0.4778(0.4667)
2023/10/19 00:53:44 - INFO - root -   Epoch: [93/300] 	 loss = 0.4824
2023/10/19 00:53:44 - INFO - root -   train_accuracy = 0.7262
2023/10/19 00:54:06 - INFO - root -   Epoch: [94/300][0/84], lr: 0.00000065 	 loss = 0.1068(0.1068)
2023/10/19 00:55:20 - INFO - root -   Epoch: [94/300][20/84], lr: 0.00000065 	 loss = 0.0711(0.3595)
2023/10/19 00:56:31 - INFO - root -   Epoch: [94/300][40/84], lr: 0.00000065 	 loss = 0.4577(0.3501)
2023/10/19 00:57:40 - INFO - root -   Epoch: [94/300][60/84], lr: 0.00000065 	 loss = 0.0604(0.4058)
2023/10/19 00:58:20 - INFO - root -   Epoch: [94/300][80/84], lr: 0.00000065 	 loss = 0.0723(0.4233)
2023/10/19 00:58:21 - INFO - root -   Epoch: [94/300] 	 loss = 0.4328
2023/10/19 00:59:18 - INFO - root -   precision = 0.8837
2023/10/19 00:59:18 - INFO - root -   eval_loss = 0.2932
2023/10/19 00:59:19 - INFO - root -   train_accuracy = 0.8095
2023/10/19 00:59:41 - INFO - root -   Epoch: [95/300][0/84], lr: 0.00000066 	 loss = 0.3593(0.3593)
2023/10/19 01:00:57 - INFO - root -   Epoch: [95/300][20/84], lr: 0.00000066 	 loss = 0.8881(0.4667)
2023/10/19 01:02:05 - INFO - root -   Epoch: [95/300][40/84], lr: 0.00000066 	 loss = 0.0536(0.4141)
2023/10/19 01:03:06 - INFO - root -   Epoch: [95/300][60/84], lr: 0.00000066 	 loss = 0.3609(0.4824)
2023/10/19 01:03:54 - INFO - root -   Epoch: [95/300][80/84], lr: 0.00000066 	 loss = 0.0669(0.4471)
2023/10/19 01:03:55 - INFO - root -   Epoch: [95/300] 	 loss = 0.4635
2023/10/19 01:03:55 - INFO - root -   train_accuracy = 0.7798
2023/10/19 01:04:24 - INFO - root -   Epoch: [96/300][0/84], lr: 0.00000067 	 loss = 0.6558(0.6558)
2023/10/19 01:05:31 - INFO - root -   Epoch: [96/300][20/84], lr: 0.00000067 	 loss = 0.0535(0.5838)
2023/10/19 01:06:36 - INFO - root -   Epoch: [96/300][40/84], lr: 0.00000067 	 loss = 0.1354(0.5592)
2023/10/19 01:07:46 - INFO - root -   Epoch: [96/300][60/84], lr: 0.00000067 	 loss = 0.1025(0.5130)
2023/10/19 01:08:25 - INFO - root -   Epoch: [96/300][80/84], lr: 0.00000067 	 loss = 0.1132(0.4799)
2023/10/19 01:08:31 - INFO - root -   Epoch: [96/300] 	 loss = 0.5111
2023/10/19 01:08:31 - INFO - root -   train_accuracy = 0.7440
2023/10/19 01:09:08 - INFO - root -   Epoch: [97/300][0/84], lr: 0.00000067 	 loss = 0.9954(0.9954)
2023/10/19 01:10:01 - INFO - root -   Epoch: [97/300][20/84], lr: 0.00000067 	 loss = 0.7853(0.5818)
2023/10/19 01:11:19 - INFO - root -   Epoch: [97/300][40/84], lr: 0.00000067 	 loss = 0.0586(0.5110)
2023/10/19 01:12:13 - INFO - root -   Epoch: [97/300][60/84], lr: 0.00000067 	 loss = 0.0532(0.5454)
2023/10/19 01:13:08 - INFO - root -   Epoch: [97/300][80/84], lr: 0.00000067 	 loss = 0.0236(0.5211)
2023/10/19 01:13:10 - INFO - root -   Epoch: [97/300] 	 loss = 0.5213
2023/10/19 01:13:10 - INFO - root -   train_accuracy = 0.7738
2023/10/19 01:13:40 - INFO - root -   Epoch: [98/300][0/84], lr: 0.00000068 	 loss = 0.4702(0.4702)
2023/10/19 01:14:55 - INFO - root -   Epoch: [98/300][20/84], lr: 0.00000068 	 loss = 0.0838(0.5450)
2023/10/19 01:16:12 - INFO - root -   Epoch: [98/300][40/84], lr: 0.00000068 	 loss = 0.3043(0.4690)
2023/10/19 01:17:00 - INFO - root -   Epoch: [98/300][60/84], lr: 0.00000068 	 loss = 0.1292(0.4743)
2023/10/19 01:17:55 - INFO - root -   Epoch: [98/300][80/84], lr: 0.00000068 	 loss = 0.0172(0.4817)
2023/10/19 01:17:56 - INFO - root -   Epoch: [98/300] 	 loss = 0.4897
2023/10/19 01:17:56 - INFO - root -   train_accuracy = 0.7857
2023/10/19 01:18:17 - INFO - root -   Epoch: [99/300][0/84], lr: 0.00000068 	 loss = 0.1928(0.1928)
2023/10/19 01:19:22 - INFO - root -   Epoch: [99/300][20/84], lr: 0.00000068 	 loss = 0.0734(0.4854)
2023/10/19 01:20:44 - INFO - root -   Epoch: [99/300][40/84], lr: 0.00000068 	 loss = 0.0583(0.4544)
2023/10/19 01:21:42 - INFO - root -   Epoch: [99/300][60/84], lr: 0.00000068 	 loss = 0.1790(0.4831)
2023/10/19 01:22:27 - INFO - root -   Epoch: [99/300][80/84], lr: 0.00000068 	 loss = 0.0312(0.4569)
2023/10/19 01:22:29 - INFO - root -   Epoch: [99/300] 	 loss = 0.4684
2023/10/19 01:23:25 - INFO - root -   precision = 0.8837
2023/10/19 01:23:25 - INFO - root -   eval_loss = 0.3126
2023/10/19 01:23:26 - INFO - root -   train_accuracy = 0.7440
2023/10/19 01:23:48 - INFO - root -   Epoch: [100/300][0/84], lr: 0.00000069 	 loss = 0.1249(0.1249)
2023/10/19 01:25:04 - INFO - root -   Epoch: [100/300][20/84], lr: 0.00000069 	 loss = 0.6216(0.3795)
2023/10/19 01:26:02 - INFO - root -   Epoch: [100/300][40/84], lr: 0.00000069 	 loss = 0.4163(0.4400)
2023/10/19 01:27:25 - INFO - root -   Epoch: [100/300][60/84], lr: 0.00000069 	 loss = 0.9283(0.4432)
2023/10/19 01:28:09 - INFO - root -   Epoch: [100/300][80/84], lr: 0.00000069 	 loss = 0.0630(0.4838)
2023/10/19 01:28:12 - INFO - root -   Epoch: [100/300] 	 loss = 0.4951
2023/10/19 01:28:12 - INFO - root -   train_accuracy = 0.7679
2023/10/19 01:28:49 - INFO - root -   Epoch: [101/300][0/84], lr: 0.00000070 	 loss = 0.3106(0.3106)
2023/10/19 01:29:46 - INFO - root -   Epoch: [101/300][20/84], lr: 0.00000070 	 loss = 0.4552(0.4289)
2023/10/19 01:30:46 - INFO - root -   Epoch: [101/300][40/84], lr: 0.00000070 	 loss = 0.0997(0.4174)
2023/10/19 01:31:41 - INFO - root -   Epoch: [101/300][60/84], lr: 0.00000070 	 loss = 0.3267(0.4336)
2023/10/19 01:32:39 - INFO - root -   Epoch: [101/300][80/84], lr: 0.00000070 	 loss = 0.0811(0.4418)
2023/10/19 01:32:41 - INFO - root -   Epoch: [101/300] 	 loss = 0.4650
2023/10/19 01:32:41 - INFO - root -   train_accuracy = 0.7619
2023/10/19 01:33:03 - INFO - root -   Epoch: [102/300][0/84], lr: 0.00000070 	 loss = 0.0891(0.0891)
2023/10/19 01:34:11 - INFO - root -   Epoch: [102/300][20/84], lr: 0.00000070 	 loss = 0.1165(0.5653)
2023/10/19 01:35:28 - INFO - root -   Epoch: [102/300][40/84], lr: 0.00000070 	 loss = 0.0826(0.4618)
2023/10/19 01:36:25 - INFO - root -   Epoch: [102/300][60/84], lr: 0.00000070 	 loss = 0.0471(0.4690)
2023/10/19 01:37:20 - INFO - root -   Epoch: [102/300][80/84], lr: 0.00000070 	 loss = 0.1386(0.4764)
2023/10/19 01:37:22 - INFO - root -   Epoch: [102/300] 	 loss = 0.4902
2023/10/19 01:37:22 - INFO - root -   train_accuracy = 0.7619
2023/10/19 01:37:52 - INFO - root -   Epoch: [103/300][0/84], lr: 0.00000071 	 loss = 0.5624(0.5624)
2023/10/19 01:38:58 - INFO - root -   Epoch: [103/300][20/84], lr: 0.00000071 	 loss = 0.2951(0.4910)
2023/10/19 01:39:55 - INFO - root -   Epoch: [103/300][40/84], lr: 0.00000071 	 loss = 0.0463(0.4127)
2023/10/19 01:41:02 - INFO - root -   Epoch: [103/300][60/84], lr: 0.00000071 	 loss = 0.2750(0.4420)
2023/10/19 01:41:56 - INFO - root -   Epoch: [103/300][80/84], lr: 0.00000071 	 loss = 0.6299(0.4517)
2023/10/19 01:41:58 - INFO - root -   Epoch: [103/300] 	 loss = 0.4801
2023/10/19 01:41:58 - INFO - root -   train_accuracy = 0.7619
2023/10/19 01:42:28 - INFO - root -   Epoch: [104/300][0/84], lr: 0.00000071 	 loss = 0.7756(0.7756)
2023/10/19 01:43:37 - INFO - root -   Epoch: [104/300][20/84], lr: 0.00000071 	 loss = 0.2046(0.4703)
2023/10/19 01:45:02 - INFO - root -   Epoch: [104/300][40/84], lr: 0.00000071 	 loss = 0.5706(0.4305)
2023/10/19 01:45:48 - INFO - root -   Epoch: [104/300][60/84], lr: 0.00000071 	 loss = 0.1458(0.4446)
2023/10/19 01:46:45 - INFO - root -   Epoch: [104/300][80/84], lr: 0.00000071 	 loss = 0.1646(0.4342)
2023/10/19 01:46:47 - INFO - root -   Epoch: [104/300] 	 loss = 0.4411
2023/10/19 01:47:44 - INFO - root -   precision = 0.7907
2023/10/19 01:47:44 - INFO - root -   eval_loss = 0.3935
2023/10/19 01:47:45 - INFO - root -   train_accuracy = 0.8095
2023/10/19 01:48:06 - INFO - root -   Epoch: [105/300][0/84], lr: 0.00000072 	 loss = 0.4508(0.4508)
2023/10/19 01:49:20 - INFO - root -   Epoch: [105/300][20/84], lr: 0.00000072 	 loss = 0.1767(0.4689)
2023/10/19 01:50:28 - INFO - root -   Epoch: [105/300][40/84], lr: 0.00000072 	 loss = 0.3439(0.3925)
2023/10/19 01:51:35 - INFO - root -   Epoch: [105/300][60/84], lr: 0.00000072 	 loss = 0.0953(0.3860)
2023/10/19 01:52:19 - INFO - root -   Epoch: [105/300][80/84], lr: 0.00000072 	 loss = 0.0426(0.3736)
2023/10/19 01:52:21 - INFO - root -   Epoch: [105/300] 	 loss = 0.3863
2023/10/19 01:52:21 - INFO - root -   train_accuracy = 0.8452
2023/10/19 01:52:50 - INFO - root -   Epoch: [106/300][0/84], lr: 0.00000072 	 loss = 0.1670(0.1670)
2023/10/19 01:53:51 - INFO - root -   Epoch: [106/300][20/84], lr: 0.00000072 	 loss = 0.5170(0.3706)
2023/10/19 01:55:09 - INFO - root -   Epoch: [106/300][40/84], lr: 0.00000072 	 loss = 0.3450(0.3865)
2023/10/19 01:56:08 - INFO - root -   Epoch: [106/300][60/84], lr: 0.00000072 	 loss = 0.2591(0.4332)
2023/10/19 01:57:04 - INFO - root -   Epoch: [106/300][80/84], lr: 0.00000072 	 loss = 0.4089(0.4405)
2023/10/19 01:57:05 - INFO - root -   Epoch: [106/300] 	 loss = 0.4585
2023/10/19 01:57:05 - INFO - root -   train_accuracy = 0.8214
2023/10/19 01:57:43 - INFO - root -   Epoch: [107/300][0/84], lr: 0.00000073 	 loss = 0.1677(0.1677)
2023/10/19 01:58:42 - INFO - root -   Epoch: [107/300][20/84], lr: 0.00000073 	 loss = 0.0402(0.3661)
2023/10/19 01:59:38 - INFO - root -   Epoch: [107/300][40/84], lr: 0.00000073 	 loss = 0.1168(0.3158)
2023/10/19 02:00:56 - INFO - root -   Epoch: [107/300][60/84], lr: 0.00000073 	 loss = 0.0378(0.3561)
2023/10/19 02:01:44 - INFO - root -   Epoch: [107/300][80/84], lr: 0.00000073 	 loss = 0.0179(0.3913)
2023/10/19 02:01:49 - INFO - root -   Epoch: [107/300] 	 loss = 0.4055
2023/10/19 02:01:49 - INFO - root -   train_accuracy = 0.7917
2023/10/19 02:02:11 - INFO - root -   Epoch: [108/300][0/84], lr: 0.00000074 	 loss = 0.0705(0.0705)
2023/10/19 02:03:25 - INFO - root -   Epoch: [108/300][20/84], lr: 0.00000074 	 loss = 0.0903(0.3510)
2023/10/19 02:04:18 - INFO - root -   Epoch: [108/300][40/84], lr: 0.00000074 	 loss = 0.0857(0.3543)
2023/10/19 02:05:39 - INFO - root -   Epoch: [108/300][60/84], lr: 0.00000074 	 loss = 0.6275(0.3858)
2023/10/19 02:06:28 - INFO - root -   Epoch: [108/300][80/84], lr: 0.00000074 	 loss = 0.1687(0.3665)
2023/10/19 02:06:34 - INFO - root -   Epoch: [108/300] 	 loss = 0.4047
2023/10/19 02:06:34 - INFO - root -   train_accuracy = 0.8214
2023/10/19 02:07:03 - INFO - root -   Epoch: [109/300][0/84], lr: 0.00000074 	 loss = 0.3701(0.3701)
2023/10/19 02:07:55 - INFO - root -   Epoch: [109/300][20/84], lr: 0.00000074 	 loss = 0.0698(0.4442)
2023/10/19 02:09:09 - INFO - root -   Epoch: [109/300][40/84], lr: 0.00000074 	 loss = 0.2821(0.4367)
2023/10/19 02:10:21 - INFO - root -   Epoch: [109/300][60/84], lr: 0.00000074 	 loss = 0.0387(0.4266)
2023/10/19 02:11:05 - INFO - root -   Epoch: [109/300][80/84], lr: 0.00000074 	 loss = 0.3745(0.4589)
2023/10/19 02:11:11 - INFO - root -   Epoch: [109/300] 	 loss = 0.4700
2023/10/19 02:12:07 - INFO - root -   precision = 0.8605
2023/10/19 02:12:07 - INFO - root -   eval_loss = 0.3143
2023/10/19 02:12:08 - INFO - root -   train_accuracy = 0.7381
2023/10/19 02:12:38 - INFO - root -   Epoch: [110/300][0/84], lr: 0.00000075 	 loss = 0.7139(0.7139)
2023/10/19 02:13:49 - INFO - root -   Epoch: [110/300][20/84], lr: 0.00000075 	 loss = 0.0953(0.4695)
2023/10/19 02:14:37 - INFO - root -   Epoch: [110/300][40/84], lr: 0.00000075 	 loss = 0.0397(0.4203)
2023/10/19 02:15:50 - INFO - root -   Epoch: [110/300][60/84], lr: 0.00000075 	 loss = 0.5336(0.4237)
2023/10/19 02:16:38 - INFO - root -   Epoch: [110/300][80/84], lr: 0.00000075 	 loss = 0.6514(0.4326)
2023/10/19 02:16:45 - INFO - root -   Epoch: [110/300] 	 loss = 0.4364
2023/10/19 02:16:45 - INFO - root -   train_accuracy = 0.7917
2023/10/19 02:17:24 - INFO - root -   Epoch: [111/300][0/84], lr: 0.00000075 	 loss = 0.2389(0.2389)
2023/10/19 02:18:22 - INFO - root -   Epoch: [111/300][20/84], lr: 0.00000075 	 loss = 0.2572(0.4591)
2023/10/19 02:19:31 - INFO - root -   Epoch: [111/300][40/84], lr: 0.00000075 	 loss = 0.2787(0.4271)
2023/10/19 02:20:35 - INFO - root -   Epoch: [111/300][60/84], lr: 0.00000075 	 loss = 0.0473(0.4654)
2023/10/19 02:21:25 - INFO - root -   Epoch: [111/300][80/84], lr: 0.00000075 	 loss = 0.1520(0.4407)
2023/10/19 02:21:27 - INFO - root -   Epoch: [111/300] 	 loss = 0.4548
2023/10/19 02:21:27 - INFO - root -   train_accuracy = 0.8155
2023/10/19 02:21:48 - INFO - root -   Epoch: [112/300][0/84], lr: 0.00000076 	 loss = 0.0768(0.0768)
2023/10/19 02:22:56 - INFO - root -   Epoch: [112/300][20/84], lr: 0.00000076 	 loss = 0.1713(0.3203)
2023/10/19 02:24:18 - INFO - root -   Epoch: [112/300][40/84], lr: 0.00000076 	 loss = 0.2625(0.3115)
2023/10/19 02:25:13 - INFO - root -   Epoch: [112/300][60/84], lr: 0.00000076 	 loss = 0.1107(0.3444)
2023/10/19 02:26:06 - INFO - root -   Epoch: [112/300][80/84], lr: 0.00000076 	 loss = 0.0649(0.3366)
2023/10/19 02:26:07 - INFO - root -   Epoch: [112/300] 	 loss = 0.3542
2023/10/19 02:26:07 - INFO - root -   train_accuracy = 0.8631
2023/10/19 02:26:44 - INFO - root -   Epoch: [113/300][0/84], lr: 0.00000077 	 loss = 0.1403(0.1403)
2023/10/19 02:27:48 - INFO - root -   Epoch: [113/300][20/84], lr: 0.00000077 	 loss = 0.9674(0.4881)
2023/10/19 02:29:07 - INFO - root -   Epoch: [113/300][40/84], lr: 0.00000077 	 loss = 0.0585(0.4000)
2023/10/19 02:29:57 - INFO - root -   Epoch: [113/300][60/84], lr: 0.00000077 	 loss = 0.1018(0.4580)
2023/10/19 02:30:49 - INFO - root -   Epoch: [113/300][80/84], lr: 0.00000077 	 loss = 0.2585(0.4519)
2023/10/19 02:30:52 - INFO - root -   Epoch: [113/300] 	 loss = 0.4557
2023/10/19 02:30:52 - INFO - root -   train_accuracy = 0.7857
2023/10/19 02:31:29 - INFO - root -   Epoch: [114/300][0/84], lr: 0.00000077 	 loss = 1.2109(1.2109)
2023/10/19 02:32:20 - INFO - root -   Epoch: [114/300][20/84], lr: 0.00000077 	 loss = 0.0570(0.4600)
2023/10/19 02:33:26 - INFO - root -   Epoch: [114/300][40/84], lr: 0.00000077 	 loss = 0.2890(0.3830)
2023/10/19 02:34:33 - INFO - root -   Epoch: [114/300][60/84], lr: 0.00000077 	 loss = 0.2992(0.4000)
2023/10/19 02:35:19 - INFO - root -   Epoch: [114/300][80/84], lr: 0.00000077 	 loss = 0.0968(0.4157)
2023/10/19 02:35:22 - INFO - root -   Epoch: [114/300] 	 loss = 0.4291
2023/10/19 02:36:18 - INFO - root -   precision = 0.9070
2023/10/19 02:36:18 - INFO - root -   eval_loss = 0.2964
2023/10/19 02:36:19 - INFO - root -   train_accuracy = 0.8393
2023/10/19 02:36:42 - INFO - root -   Epoch: [115/300][0/84], lr: 0.00000078 	 loss = 0.8361(0.8361)
2023/10/19 02:37:48 - INFO - root -   Epoch: [115/300][20/84], lr: 0.00000078 	 loss = 0.0145(0.4267)
2023/10/19 02:38:55 - INFO - root -   Epoch: [115/300][40/84], lr: 0.00000078 	 loss = 0.4511(0.4217)
2023/10/19 02:40:07 - INFO - root -   Epoch: [115/300][60/84], lr: 0.00000078 	 loss = 0.1232(0.4324)
2023/10/19 02:41:01 - INFO - root -   Epoch: [115/300][80/84], lr: 0.00000078 	 loss = 0.2398(0.4470)
2023/10/19 02:41:02 - INFO - root -   Epoch: [115/300] 	 loss = 0.4608
2023/10/19 02:41:02 - INFO - root -   train_accuracy = 0.7619
2023/10/19 02:41:24 - INFO - root -   Epoch: [116/300][0/84], lr: 0.00000078 	 loss = 0.0594(0.0594)
2023/10/19 02:42:28 - INFO - root -   Epoch: [116/300][20/84], lr: 0.00000078 	 loss = 0.2669(0.2355)
2023/10/19 02:43:31 - INFO - root -   Epoch: [116/300][40/84], lr: 0.00000078 	 loss = 0.0925(0.2728)
2023/10/19 02:44:40 - INFO - root -   Epoch: [116/300][60/84], lr: 0.00000078 	 loss = 1.2210(0.3224)
2023/10/19 02:45:32 - INFO - root -   Epoch: [116/300][80/84], lr: 0.00000078 	 loss = 0.2752(0.3220)
2023/10/19 02:45:33 - INFO - root -   Epoch: [116/300] 	 loss = 0.3446
2023/10/19 02:45:33 - INFO - root -   train_accuracy = 0.8631
2023/10/19 02:46:03 - INFO - root -   Epoch: [117/300][0/84], lr: 0.00000079 	 loss = 0.1692(0.1692)
2023/10/19 02:47:02 - INFO - root -   Epoch: [117/300][20/84], lr: 0.00000079 	 loss = 0.7397(0.4868)
2023/10/19 02:48:10 - INFO - root -   Epoch: [117/300][40/84], lr: 0.00000079 	 loss = 0.3616(0.4804)
2023/10/19 02:49:15 - INFO - root -   Epoch: [117/300][60/84], lr: 0.00000079 	 loss = 0.0503(0.4593)
2023/10/19 02:50:09 - INFO - root -   Epoch: [117/300][80/84], lr: 0.00000079 	 loss = 0.3687(0.4323)
2023/10/19 02:50:10 - INFO - root -   Epoch: [117/300] 	 loss = 0.4344
2023/10/19 02:50:10 - INFO - root -   train_accuracy = 0.7917
2023/10/19 02:50:32 - INFO - root -   Epoch: [118/300][0/84], lr: 0.00000080 	 loss = 0.1094(0.1094)
2023/10/19 02:51:46 - INFO - root -   Epoch: [118/300][20/84], lr: 0.00000080 	 loss = 0.1197(0.2925)
2023/10/19 02:52:44 - INFO - root -   Epoch: [118/300][40/84], lr: 0.00000080 	 loss = 0.1167(0.3250)
2023/10/19 02:53:50 - INFO - root -   Epoch: [118/300][60/84], lr: 0.00000080 	 loss = 0.0664(0.3594)
2023/10/19 02:54:48 - INFO - root -   Epoch: [118/300][80/84], lr: 0.00000080 	 loss = 0.0903(0.3933)
2023/10/19 02:54:50 - INFO - root -   Epoch: [118/300] 	 loss = 0.4221
2023/10/19 02:54:50 - INFO - root -   train_accuracy = 0.8214
2023/10/19 02:55:12 - INFO - root -   Epoch: [119/300][0/84], lr: 0.00000080 	 loss = 0.2342(0.2342)
2023/10/19 02:56:18 - INFO - root -   Epoch: [119/300][20/84], lr: 0.00000080 	 loss = 0.6056(0.3828)
2023/10/19 02:57:16 - INFO - root -   Epoch: [119/300][40/84], lr: 0.00000080 	 loss = 0.0649(0.3755)
2023/10/19 02:58:19 - INFO - root -   Epoch: [119/300][60/84], lr: 0.00000080 	 loss = 0.2854(0.4001)
2023/10/19 02:59:20 - INFO - root -   Epoch: [119/300][80/84], lr: 0.00000080 	 loss = 0.3007(0.4159)
2023/10/19 02:59:22 - INFO - root -   Epoch: [119/300] 	 loss = 0.4315
2023/10/19 03:00:18 - INFO - root -   precision = 0.8837
2023/10/19 03:00:18 - INFO - root -   eval_loss = 0.3115
2023/10/19 03:00:19 - INFO - root -   train_accuracy = 0.7798
2023/10/19 03:00:41 - INFO - root -   Epoch: [120/300][0/84], lr: 0.00000081 	 loss = 0.0508(0.0508)
2023/10/19 03:01:39 - INFO - root -   Epoch: [120/300][20/84], lr: 0.00000081 	 loss = 0.0806(0.4145)
2023/10/19 03:02:46 - INFO - root -   Epoch: [120/300][40/84], lr: 0.00000081 	 loss = 0.1711(0.4380)
2023/10/19 03:04:03 - INFO - root -   Epoch: [120/300][60/84], lr: 0.00000081 	 loss = 0.4224(0.4273)
2023/10/19 03:04:47 - INFO - root -   Epoch: [120/300][80/84], lr: 0.00000081 	 loss = 0.5043(0.4321)
2023/10/19 03:04:50 - INFO - root -   Epoch: [120/300] 	 loss = 0.4537
2023/10/19 03:04:50 - INFO - root -   train_accuracy = 0.7976
2023/10/19 03:05:20 - INFO - root -   Epoch: [121/300][0/84], lr: 0.00000081 	 loss = 0.1776(0.1776)
2023/10/19 03:06:28 - INFO - root -   Epoch: [121/300][20/84], lr: 0.00000081 	 loss = 0.4295(0.4036)
2023/10/19 03:07:25 - INFO - root -   Epoch: [121/300][40/84], lr: 0.00000081 	 loss = 0.1040(0.3903)
2023/10/19 03:08:49 - INFO - root -   Epoch: [121/300][60/84], lr: 0.00000081 	 loss = 0.2143(0.3977)
2023/10/19 03:09:32 - INFO - root -   Epoch: [121/300][80/84], lr: 0.00000081 	 loss = 0.1305(0.4443)
2023/10/19 03:09:33 - INFO - root -   Epoch: [121/300] 	 loss = 0.4632
2023/10/19 03:09:33 - INFO - root -   train_accuracy = 0.7857
2023/10/19 03:10:06 - INFO - root -   Epoch: [122/300][0/84], lr: 0.00000082 	 loss = 0.2154(0.2154)
2023/10/19 03:11:23 - INFO - root -   Epoch: [122/300][20/84], lr: 0.00000082 	 loss = 0.5727(0.3471)
2023/10/19 03:12:22 - INFO - root -   Epoch: [122/300][40/84], lr: 0.00000082 	 loss = 0.5692(0.4171)
2023/10/19 03:13:52 - INFO - root -   Epoch: [122/300][60/84], lr: 0.00000082 	 loss = 0.0910(0.4405)
2023/10/19 03:14:30 - INFO - root -   Epoch: [122/300][80/84], lr: 0.00000082 	 loss = 0.2136(0.4717)
2023/10/19 03:14:34 - INFO - root -   Epoch: [122/300] 	 loss = 0.4659
2023/10/19 03:14:34 - INFO - root -   train_accuracy = 0.7798
2023/10/19 03:14:57 - INFO - root -   Epoch: [123/300][0/84], lr: 0.00000082 	 loss = 0.2597(0.2597)
2023/10/19 03:16:17 - INFO - root -   Epoch: [123/300][20/84], lr: 0.00000082 	 loss = 0.2279(0.3770)
2023/10/19 03:17:20 - INFO - root -   Epoch: [123/300][40/84], lr: 0.00000082 	 loss = 0.1024(0.3846)
2023/10/19 03:18:30 - INFO - root -   Epoch: [123/300][60/84], lr: 0.00000082 	 loss = 0.0696(0.4412)
2023/10/19 03:19:17 - INFO - root -   Epoch: [123/300][80/84], lr: 0.00000082 	 loss = 0.4501(0.4716)
2023/10/19 03:19:18 - INFO - root -   Epoch: [123/300] 	 loss = 0.4794
2023/10/19 03:19:18 - INFO - root -   train_accuracy = 0.7560
2023/10/19 03:19:40 - INFO - root -   Epoch: [124/300][0/84], lr: 0.00000083 	 loss = 0.1164(0.1164)
2023/10/19 03:20:59 - INFO - root -   Epoch: [124/300][20/84], lr: 0.00000083 	 loss = 0.0505(0.4452)
2023/10/19 03:22:11 - INFO - root -   Epoch: [124/300][40/84], lr: 0.00000083 	 loss = 0.6133(0.5103)
2023/10/19 03:23:15 - INFO - root -   Epoch: [124/300][60/84], lr: 0.00000083 	 loss = 0.2671(0.5145)
2023/10/19 03:24:04 - INFO - root -   Epoch: [124/300][80/84], lr: 0.00000083 	 loss = 0.1738(0.4991)
2023/10/19 03:24:05 - INFO - root -   Epoch: [124/300] 	 loss = 0.5038
2023/10/19 03:25:01 - INFO - root -   precision = 0.8605
2023/10/19 03:25:01 - INFO - root -   eval_loss = 0.3115
2023/10/19 03:25:02 - INFO - root -   train_accuracy = 0.7798
2023/10/19 03:25:32 - INFO - root -   Epoch: [125/300][0/84], lr: 0.00000084 	 loss = 0.6872(0.6872)
2023/10/19 03:26:43 - INFO - root -   Epoch: [125/300][20/84], lr: 0.00000084 	 loss = 0.3307(0.4637)
2023/10/19 03:28:08 - INFO - root -   Epoch: [125/300][40/84], lr: 0.00000084 	 loss = 0.4150(0.4575)
2023/10/19 03:28:59 - INFO - root -   Epoch: [125/300][60/84], lr: 0.00000084 	 loss = 0.1600(0.4637)
2023/10/19 03:29:47 - INFO - root -   Epoch: [125/300][80/84], lr: 0.00000084 	 loss = 0.1135(0.4545)
2023/10/19 03:29:48 - INFO - root -   Epoch: [125/300] 	 loss = 0.4657
2023/10/19 03:29:48 - INFO - root -   train_accuracy = 0.7679
2023/10/19 03:30:10 - INFO - root -   Epoch: [126/300][0/84], lr: 0.00000084 	 loss = 0.2133(0.2133)
2023/10/19 03:31:18 - INFO - root -   Epoch: [126/300][20/84], lr: 0.00000084 	 loss = 0.4336(0.4089)
2023/10/19 03:32:23 - INFO - root -   Epoch: [126/300][40/84], lr: 0.00000084 	 loss = 0.0495(0.3399)
2023/10/19 03:33:45 - INFO - root -   Epoch: [126/300][60/84], lr: 0.00000084 	 loss = 0.0249(0.3384)
2023/10/19 03:34:20 - INFO - root -   Epoch: [126/300][80/84], lr: 0.00000084 	 loss = 0.4483(0.3847)
2023/10/19 03:34:21 - INFO - root -   Epoch: [126/300] 	 loss = 0.4010
2023/10/19 03:34:21 - INFO - root -   train_accuracy = 0.8155
2023/10/19 03:34:42 - INFO - root -   Epoch: [127/300][0/84], lr: 0.00000085 	 loss = 0.1295(0.1295)
2023/10/19 03:35:57 - INFO - root -   Epoch: [127/300][20/84], lr: 0.00000085 	 loss = 0.1388(0.4014)
2023/10/19 03:37:08 - INFO - root -   Epoch: [127/300][40/84], lr: 0.00000085 	 loss = 0.0818(0.3354)
2023/10/19 03:38:03 - INFO - root -   Epoch: [127/300][60/84], lr: 0.00000085 	 loss = 0.1271(0.3630)
2023/10/19 03:38:56 - INFO - root -   Epoch: [127/300][80/84], lr: 0.00000085 	 loss = 0.0649(0.3984)
2023/10/19 03:38:59 - INFO - root -   Epoch: [127/300] 	 loss = 0.4075
2023/10/19 03:38:59 - INFO - root -   train_accuracy = 0.8036
2023/10/19 03:39:21 - INFO - root -   Epoch: [128/300][0/84], lr: 0.00000085 	 loss = 0.4821(0.4821)
2023/10/19 03:40:27 - INFO - root -   Epoch: [128/300][20/84], lr: 0.00000085 	 loss = 0.2322(0.3769)
2023/10/19 03:41:43 - INFO - root -   Epoch: [128/300][40/84], lr: 0.00000085 	 loss = 0.5448(0.3843)
2023/10/19 03:42:41 - INFO - root -   Epoch: [128/300][60/84], lr: 0.00000085 	 loss = 0.3655(0.4012)
2023/10/19 03:43:39 - INFO - root -   Epoch: [128/300][80/84], lr: 0.00000085 	 loss = 0.1293(0.3872)
2023/10/19 03:43:40 - INFO - root -   Epoch: [128/300] 	 loss = 0.3860
2023/10/19 03:43:40 - INFO - root -   train_accuracy = 0.8214
2023/10/19 03:44:10 - INFO - root -   Epoch: [129/300][0/84], lr: 0.00000086 	 loss = 0.7303(0.7303)
2023/10/19 03:45:18 - INFO - root -   Epoch: [129/300][20/84], lr: 0.00000086 	 loss = 0.0727(0.4683)
2023/10/19 03:46:37 - INFO - root -   Epoch: [129/300][40/84], lr: 0.00000086 	 loss = 0.2454(0.3502)
2023/10/19 03:47:32 - INFO - root -   Epoch: [129/300][60/84], lr: 0.00000086 	 loss = 0.0577(0.3843)
2023/10/19 03:48:19 - INFO - root -   Epoch: [129/300][80/84], lr: 0.00000086 	 loss = 0.0677(0.3832)
2023/10/19 03:48:20 - INFO - root -   Epoch: [129/300] 	 loss = 0.4078
2023/10/19 03:49:17 - INFO - root -   precision = 0.7907
2023/10/19 03:49:17 - INFO - root -   eval_loss = 0.3510
2023/10/19 03:49:18 - INFO - root -   train_accuracy = 0.8036
2023/10/19 03:49:48 - INFO - root -   Epoch: [130/300][0/84], lr: 0.00000087 	 loss = 1.4141(1.4141)
2023/10/19 03:50:52 - INFO - root -   Epoch: [130/300][20/84], lr: 0.00000087 	 loss = 0.6331(0.4960)
2023/10/19 03:51:52 - INFO - root -   Epoch: [130/300][40/84], lr: 0.00000087 	 loss = 0.0963(0.4519)
2023/10/19 03:53:15 - INFO - root -   Epoch: [130/300][60/84], lr: 0.00000087 	 loss = 0.2108(0.4438)
2023/10/19 03:53:56 - INFO - root -   Epoch: [130/300][80/84], lr: 0.00000087 	 loss = 0.0241(0.4407)
2023/10/19 03:53:59 - INFO - root -   Epoch: [130/300] 	 loss = 0.4606
2023/10/19 03:53:59 - INFO - root -   train_accuracy = 0.7798
2023/10/19 03:54:28 - INFO - root -   Epoch: [131/300][0/84], lr: 0.00000087 	 loss = 0.3727(0.3727)
2023/10/19 03:55:42 - INFO - root -   Epoch: [131/300][20/84], lr: 0.00000087 	 loss = 0.2328(0.3611)
2023/10/19 03:56:40 - INFO - root -   Epoch: [131/300][40/84], lr: 0.00000087 	 loss = 0.1600(0.4231)
2023/10/19 03:57:57 - INFO - root -   Epoch: [131/300][60/84], lr: 0.00000087 	 loss = 0.6935(0.4369)
2023/10/19 03:58:44 - INFO - root -   Epoch: [131/300][80/84], lr: 0.00000087 	 loss = 1.0586(0.4975)
2023/10/19 03:58:45 - INFO - root -   Epoch: [131/300] 	 loss = 0.5041
2023/10/19 03:58:45 - INFO - root -   train_accuracy = 0.7440
2023/10/19 03:59:07 - INFO - root -   Epoch: [132/300][0/84], lr: 0.00000088 	 loss = 0.4508(0.4508)
2023/10/19 04:00:27 - INFO - root -   Epoch: [132/300][20/84], lr: 0.00000088 	 loss = 0.4440(0.3747)
2023/10/19 04:01:29 - INFO - root -   Epoch: [132/300][40/84], lr: 0.00000088 	 loss = 0.1292(0.4021)
2023/10/19 04:02:37 - INFO - root -   Epoch: [132/300][60/84], lr: 0.00000088 	 loss = 0.2540(0.4401)
2023/10/19 04:03:18 - INFO - root -   Epoch: [132/300][80/84], lr: 0.00000088 	 loss = 0.2452(0.4216)
2023/10/19 04:03:26 - INFO - root -   Epoch: [132/300] 	 loss = 0.4276
2023/10/19 04:03:26 - INFO - root -   train_accuracy = 0.8095
2023/10/19 04:03:55 - INFO - root -   Epoch: [133/300][0/84], lr: 0.00000088 	 loss = 0.4348(0.4348)
2023/10/19 04:04:54 - INFO - root -   Epoch: [133/300][20/84], lr: 0.00000088 	 loss = 0.6223(0.3208)
2023/10/19 04:06:14 - INFO - root -   Epoch: [133/300][40/84], lr: 0.00000088 	 loss = 0.0167(0.4509)
2023/10/19 04:07:07 - INFO - root -   Epoch: [133/300][60/84], lr: 0.00000088 	 loss = 0.0279(0.4438)
2023/10/19 04:08:03 - INFO - root -   Epoch: [133/300][80/84], lr: 0.00000088 	 loss = 0.1084(0.4268)
2023/10/19 04:08:04 - INFO - root -   Epoch: [133/300] 	 loss = 0.4500
2023/10/19 04:08:04 - INFO - root -   train_accuracy = 0.8036
2023/10/19 04:08:35 - INFO - root -   Epoch: [134/300][0/84], lr: 0.00000089 	 loss = 0.3996(0.3996)
2023/10/19 04:09:34 - INFO - root -   Epoch: [134/300][20/84], lr: 0.00000089 	 loss = 0.1146(0.3871)
2023/10/19 04:10:50 - INFO - root -   Epoch: [134/300][40/84], lr: 0.00000089 	 loss = 0.0322(0.3514)
2023/10/19 04:11:41 - INFO - root -   Epoch: [134/300][60/84], lr: 0.00000089 	 loss = 0.0148(0.3890)
2023/10/19 04:12:46 - INFO - root -   Epoch: [134/300][80/84], lr: 0.00000089 	 loss = 0.2465(0.4313)
2023/10/19 04:12:47 - INFO - root -   Epoch: [134/300] 	 loss = 0.4602
2023/10/19 04:13:43 - INFO - root -   precision = 0.8837
2023/10/19 04:13:43 - INFO - root -   eval_loss = 0.2853
2023/10/19 04:13:44 - INFO - root -   train_accuracy = 0.8155
2023/10/19 04:14:06 - INFO - root -   Epoch: [135/300][0/84], lr: 0.00000090 	 loss = 0.2081(0.2081)
2023/10/19 04:15:15 - INFO - root -   Epoch: [135/300][20/84], lr: 0.00000090 	 loss = 0.0604(0.4269)
2023/10/19 04:16:20 - INFO - root -   Epoch: [135/300][40/84], lr: 0.00000090 	 loss = 0.1198(0.3984)
2023/10/19 04:17:37 - INFO - root -   Epoch: [135/300][60/84], lr: 0.00000090 	 loss = 0.0430(0.3979)
2023/10/19 04:18:20 - INFO - root -   Epoch: [135/300][80/84], lr: 0.00000090 	 loss = 0.1278(0.3832)
2023/10/19 04:18:26 - INFO - root -   Epoch: [135/300] 	 loss = 0.3851
2023/10/19 04:18:26 - INFO - root -   train_accuracy = 0.8274
2023/10/19 04:18:48 - INFO - root -   Epoch: [136/300][0/84], lr: 0.00000090 	 loss = 0.1001(0.1001)
2023/10/19 04:19:40 - INFO - root -   Epoch: [136/300][20/84], lr: 0.00000090 	 loss = 0.0585(0.4045)
2023/10/19 04:20:56 - INFO - root -   Epoch: [136/300][40/84], lr: 0.00000090 	 loss = 0.0470(0.3848)
2023/10/19 04:22:10 - INFO - root -   Epoch: [136/300][60/84], lr: 0.00000090 	 loss = 0.1195(0.3912)
2023/10/19 04:23:00 - INFO - root -   Epoch: [136/300][80/84], lr: 0.00000090 	 loss = 0.1549(0.4411)
2023/10/19 04:23:01 - INFO - root -   Epoch: [136/300] 	 loss = 0.4592
2023/10/19 04:23:01 - INFO - root -   train_accuracy = 0.8214
2023/10/19 04:23:38 - INFO - root -   Epoch: [137/300][0/84], lr: 0.00000091 	 loss = 0.3588(0.3588)
2023/10/19 04:24:37 - INFO - root -   Epoch: [137/300][20/84], lr: 0.00000091 	 loss = 0.0360(0.3625)
2023/10/19 04:25:45 - INFO - root -   Epoch: [137/300][40/84], lr: 0.00000091 	 loss = 0.1276(0.3159)
2023/10/19 04:26:38 - INFO - root -   Epoch: [137/300][60/84], lr: 0.00000091 	 loss = 0.2653(0.3733)
2023/10/19 04:27:30 - INFO - root -   Epoch: [137/300][80/84], lr: 0.00000091 	 loss = 0.0561(0.4151)
2023/10/19 04:27:31 - INFO - root -   Epoch: [137/300] 	 loss = 0.4153
2023/10/19 04:27:31 - INFO - root -   train_accuracy = 0.8155
2023/10/19 04:28:01 - INFO - root -   Epoch: [138/300][0/84], lr: 0.00000091 	 loss = 1.0619(1.0619)
2023/10/19 04:29:15 - INFO - root -   Epoch: [138/300][20/84], lr: 0.00000091 	 loss = 0.0491(0.3971)
2023/10/19 04:30:25 - INFO - root -   Epoch: [138/300][40/84], lr: 0.00000091 	 loss = 0.3711(0.3592)
2023/10/19 04:31:16 - INFO - root -   Epoch: [138/300][60/84], lr: 0.00000091 	 loss = 0.4719(0.3902)
2023/10/19 04:32:20 - INFO - root -   Epoch: [138/300][80/84], lr: 0.00000091 	 loss = 0.1859(0.3988)
2023/10/19 04:32:23 - INFO - root -   Epoch: [138/300] 	 loss = 0.4163
2023/10/19 04:32:23 - INFO - root -   train_accuracy = 0.7857
2023/10/19 04:32:53 - INFO - root -   Epoch: [139/300][0/84], lr: 0.00000092 	 loss = 0.0789(0.0789)
2023/10/19 04:33:52 - INFO - root -   Epoch: [139/300][20/84], lr: 0.00000092 	 loss = 0.3123(0.1975)
2023/10/19 04:34:53 - INFO - root -   Epoch: [139/300][40/84], lr: 0.00000092 	 loss = 0.0351(0.2362)
2023/10/19 04:35:59 - INFO - root -   Epoch: [139/300][60/84], lr: 0.00000092 	 loss = 0.8736(0.3121)
2023/10/19 04:36:58 - INFO - root -   Epoch: [139/300][80/84], lr: 0.00000092 	 loss = 0.1908(0.3806)
2023/10/19 04:37:01 - INFO - root -   Epoch: [139/300] 	 loss = 0.3880
2023/10/19 04:37:58 - INFO - root -   precision = 0.8837
2023/10/19 04:37:58 - INFO - root -   eval_loss = 0.2774
2023/10/19 04:37:59 - INFO - root -   train_accuracy = 0.7917
2023/10/19 04:38:29 - INFO - root -   Epoch: [140/300][0/84], lr: 0.00000093 	 loss = 0.1484(0.1484)
2023/10/19 04:39:48 - INFO - root -   Epoch: [140/300][20/84], lr: 0.00000093 	 loss = 0.2202(0.4753)
2023/10/19 04:40:51 - INFO - root -   Epoch: [140/300][40/84], lr: 0.00000093 	 loss = 0.3583(0.4169)
2023/10/19 04:41:44 - INFO - root -   Epoch: [140/300][60/84], lr: 0.00000093 	 loss = 0.3251(0.4511)
2023/10/19 04:42:36 - INFO - root -   Epoch: [140/300][80/84], lr: 0.00000093 	 loss = 0.0252(0.4649)
2023/10/19 04:42:38 - INFO - root -   Epoch: [140/300] 	 loss = 0.4611
2023/10/19 04:42:38 - INFO - root -   train_accuracy = 0.7738
2023/10/19 04:43:17 - INFO - root -   Epoch: [141/300][0/84], lr: 0.00000093 	 loss = 1.2784(1.2784)
2023/10/19 04:44:18 - INFO - root -   Epoch: [141/300][20/84], lr: 0.00000093 	 loss = 0.7080(0.4099)
2023/10/19 04:45:41 - INFO - root -   Epoch: [141/300][40/84], lr: 0.00000093 	 loss = 0.0547(0.3715)
2023/10/19 04:46:24 - INFO - root -   Epoch: [141/300][60/84], lr: 0.00000093 	 loss = 0.1213(0.4011)
2023/10/19 04:47:20 - INFO - root -   Epoch: [141/300][80/84], lr: 0.00000093 	 loss = 0.1554(0.4164)
2023/10/19 04:47:22 - INFO - root -   Epoch: [141/300] 	 loss = 0.4244
2023/10/19 04:47:22 - INFO - root -   train_accuracy = 0.8333
2023/10/19 04:47:44 - INFO - root -   Epoch: [142/300][0/84], lr: 0.00000094 	 loss = 0.2138(0.2138)
2023/10/19 04:49:02 - INFO - root -   Epoch: [142/300][20/84], lr: 0.00000094 	 loss = 0.0529(0.3700)
2023/10/19 04:50:01 - INFO - root -   Epoch: [142/300][40/84], lr: 0.00000094 	 loss = 0.3003(0.3533)
2023/10/19 04:51:15 - INFO - root -   Epoch: [142/300][60/84], lr: 0.00000094 	 loss = 0.0136(0.3338)
2023/10/19 04:52:01 - INFO - root -   Epoch: [142/300][80/84], lr: 0.00000094 	 loss = 0.0065(0.4032)
2023/10/19 04:52:02 - INFO - root -   Epoch: [142/300] 	 loss = 0.4093
2023/10/19 04:52:02 - INFO - root -   train_accuracy = 0.7738
2023/10/19 04:52:32 - INFO - root -   Epoch: [143/300][0/84], lr: 0.00000094 	 loss = 0.1007(0.1007)
2023/10/19 04:53:44 - INFO - root -   Epoch: [143/300][20/84], lr: 0.00000094 	 loss = 0.0197(0.3165)
2023/10/19 04:54:50 - INFO - root -   Epoch: [143/300][40/84], lr: 0.00000094 	 loss = 0.0984(0.3561)
2023/10/19 04:55:49 - INFO - root -   Epoch: [143/300][60/84], lr: 0.00000094 	 loss = 0.0190(0.3599)
2023/10/19 04:56:40 - INFO - root -   Epoch: [143/300][80/84], lr: 0.00000094 	 loss = 0.0756(0.3433)
2023/10/19 04:56:41 - INFO - root -   Epoch: [143/300] 	 loss = 0.3675
2023/10/19 04:56:41 - INFO - root -   train_accuracy = 0.8393
2023/10/19 04:57:10 - INFO - root -   Epoch: [144/300][0/84], lr: 0.00000095 	 loss = 0.2946(0.2946)
2023/10/19 04:58:34 - INFO - root -   Epoch: [144/300][20/84], lr: 0.00000095 	 loss = 0.1534(0.5222)
2023/10/19 04:59:17 - INFO - root -   Epoch: [144/300][40/84], lr: 0.00000095 	 loss = 0.1653(0.4045)
2023/10/19 05:00:50 - INFO - root -   Epoch: [144/300][60/84], lr: 0.00000095 	 loss = 0.3492(0.4079)
2023/10/19 05:01:31 - INFO - root -   Epoch: [144/300][80/84], lr: 0.00000095 	 loss = 0.1650(0.3951)
2023/10/19 05:01:32 - INFO - root -   Epoch: [144/300] 	 loss = 0.4138
2023/10/19 05:02:29 - INFO - root -   precision = 0.8837
2023/10/19 05:02:29 - INFO - root -   eval_loss = 0.2770
2023/10/19 05:02:30 - INFO - root -   train_accuracy = 0.7798
2023/10/19 05:03:08 - INFO - root -   Epoch: [145/300][0/84], lr: 0.00000095 	 loss = 0.2688(0.2688)
2023/10/19 05:03:58 - INFO - root -   Epoch: [145/300][20/84], lr: 0.00000095 	 loss = 0.2399(0.3557)
2023/10/19 05:05:20 - INFO - root -   Epoch: [145/300][40/84], lr: 0.00000095 	 loss = 0.5670(0.3481)
2023/10/19 05:06:14 - INFO - root -   Epoch: [145/300][60/84], lr: 0.00000095 	 loss = 0.0299(0.3442)
2023/10/19 05:07:02 - INFO - root -   Epoch: [145/300][80/84], lr: 0.00000095 	 loss = 0.0798(0.3577)
2023/10/19 05:07:03 - INFO - root -   Epoch: [145/300] 	 loss = 0.3906
2023/10/19 05:07:03 - INFO - root -   train_accuracy = 0.8036
2023/10/19 05:07:33 - INFO - root -   Epoch: [146/300][0/84], lr: 0.00000096 	 loss = 0.0955(0.0955)
2023/10/19 05:08:36 - INFO - root -   Epoch: [146/300][20/84], lr: 0.00000096 	 loss = 0.0259(0.3915)
2023/10/19 05:09:30 - INFO - root -   Epoch: [146/300][40/84], lr: 0.00000096 	 loss = 0.7059(0.3234)
2023/10/19 05:10:37 - INFO - root -   Epoch: [146/300][60/84], lr: 0.00000096 	 loss = 0.0138(0.3499)
2023/10/19 05:11:19 - INFO - root -   Epoch: [146/300][80/84], lr: 0.00000096 	 loss = 0.0941(0.3623)
2023/10/19 05:11:24 - INFO - root -   Epoch: [146/300] 	 loss = 0.3639
2023/10/19 05:11:24 - INFO - root -   train_accuracy = 0.8214
2023/10/19 05:11:54 - INFO - root -   Epoch: [147/300][0/84], lr: 0.00000097 	 loss = 0.2179(0.2179)
2023/10/19 05:12:54 - INFO - root -   Epoch: [147/300][20/84], lr: 0.00000097 	 loss = 0.0116(0.2557)
2023/10/19 05:14:18 - INFO - root -   Epoch: [147/300][40/84], lr: 0.00000097 	 loss = 0.1164(0.2496)
2023/10/19 05:15:08 - INFO - root -   Epoch: [147/300][60/84], lr: 0.00000097 	 loss = 0.6439(0.3357)
2023/10/19 05:16:02 - INFO - root -   Epoch: [147/300][80/84], lr: 0.00000097 	 loss = 0.1812(0.3901)
2023/10/19 05:16:04 - INFO - root -   Epoch: [147/300] 	 loss = 0.4153
2023/10/19 05:16:04 - INFO - root -   train_accuracy = 0.7917
2023/10/19 05:16:26 - INFO - root -   Epoch: [148/300][0/84], lr: 0.00000097 	 loss = 0.2429(0.2429)
2023/10/19 05:17:41 - INFO - root -   Epoch: [148/300][20/84], lr: 0.00000097 	 loss = 0.0399(0.4247)
2023/10/19 05:18:51 - INFO - root -   Epoch: [148/300][40/84], lr: 0.00000097 	 loss = 0.1552(0.4131)
2023/10/19 05:20:06 - INFO - root -   Epoch: [148/300][60/84], lr: 0.00000097 	 loss = 0.1691(0.4463)
2023/10/19 05:20:43 - INFO - root -   Epoch: [148/300][80/84], lr: 0.00000097 	 loss = 0.2261(0.4522)
2023/10/19 05:20:48 - INFO - root -   Epoch: [148/300] 	 loss = 0.4700
2023/10/19 05:20:48 - INFO - root -   train_accuracy = 0.7738
2023/10/19 05:21:19 - INFO - root -   Epoch: [149/300][0/84], lr: 0.00000098 	 loss = 0.1305(0.1305)
2023/10/19 05:22:29 - INFO - root -   Epoch: [149/300][20/84], lr: 0.00000098 	 loss = 0.0239(0.4436)
2023/10/19 05:23:26 - INFO - root -   Epoch: [149/300][40/84], lr: 0.00000098 	 loss = 0.0630(0.4036)
2023/10/19 05:24:48 - INFO - root -   Epoch: [149/300][60/84], lr: 0.00000098 	 loss = 0.8876(0.3860)
2023/10/19 05:25:42 - INFO - root -   Epoch: [149/300][80/84], lr: 0.00000098 	 loss = 0.2500(0.4316)
2023/10/19 05:25:45 - INFO - root -   Epoch: [149/300] 	 loss = 0.4425
2023/10/19 05:26:41 - INFO - root -   precision = 0.8605
2023/10/19 05:26:41 - INFO - root -   eval_loss = 0.3002
2023/10/19 05:26:42 - INFO - root -   train_accuracy = 0.7440
2023/10/19 05:27:04 - INFO - root -   Epoch: [150/300][0/84], lr: 0.00000098 	 loss = 0.3596(0.3596)
2023/10/19 05:28:12 - INFO - root -   Epoch: [150/300][20/84], lr: 0.00000098 	 loss = 0.0878(0.2846)
2023/10/19 05:29:29 - INFO - root -   Epoch: [150/300][40/84], lr: 0.00000098 	 loss = 0.0459(0.3150)
2023/10/19 05:30:34 - INFO - root -   Epoch: [150/300][60/84], lr: 0.00000098 	 loss = 0.0200(0.3355)
2023/10/19 05:31:21 - INFO - root -   Epoch: [150/300][80/84], lr: 0.00000098 	 loss = 0.0329(0.3334)
2023/10/19 05:31:23 - INFO - root -   Epoch: [150/300] 	 loss = 0.3358
2023/10/19 05:31:23 - INFO - root -   train_accuracy = 0.8571
2023/10/19 05:31:45 - INFO - root -   Epoch: [151/300][0/84], lr: 0.00000099 	 loss = 0.0240(0.0240)
2023/10/19 05:32:51 - INFO - root -   Epoch: [151/300][20/84], lr: 0.00000099 	 loss = 0.6548(0.3515)
2023/10/19 05:33:57 - INFO - root -   Epoch: [151/300][40/84], lr: 0.00000099 	 loss = 0.1186(0.3641)
2023/10/19 05:35:13 - INFO - root -   Epoch: [151/300][60/84], lr: 0.00000099 	 loss = 0.0879(0.3884)
2023/10/19 05:36:02 - INFO - root -   Epoch: [151/300][80/84], lr: 0.00000099 	 loss = 0.4043(0.4644)
2023/10/19 05:36:04 - INFO - root -   Epoch: [151/300] 	 loss = 0.4655
2023/10/19 05:36:04 - INFO - root -   train_accuracy = 0.7500
2023/10/19 05:36:25 - INFO - root -   Epoch: [152/300][0/84], lr: 0.00000100 	 loss = 0.0153(0.0153)
2023/10/19 05:37:24 - INFO - root -   Epoch: [152/300][20/84], lr: 0.00000100 	 loss = 0.2209(0.2872)
2023/10/19 05:38:48 - INFO - root -   Epoch: [152/300][40/84], lr: 0.00000100 	 loss = 0.0442(0.3048)
2023/10/19 05:39:46 - INFO - root -   Epoch: [152/300][60/84], lr: 0.00000100 	 loss = 0.1143(0.3887)
2023/10/19 05:40:41 - INFO - root -   Epoch: [152/300][80/84], lr: 0.00000100 	 loss = 0.0281(0.3730)
2023/10/19 05:40:43 - INFO - root -   Epoch: [152/300] 	 loss = 0.3725
2023/10/19 05:40:43 - INFO - root -   train_accuracy = 0.8036
2023/10/19 05:41:21 - INFO - root -   Epoch: [153/300][0/84], lr: 0.00000100 	 loss = 0.3876(0.3876)
2023/10/19 05:42:18 - INFO - root -   Epoch: [153/300][20/84], lr: 0.00000100 	 loss = 0.0174(0.3066)
2023/10/19 05:43:22 - INFO - root -   Epoch: [153/300][40/84], lr: 0.00000100 	 loss = 0.1242(0.3107)
2023/10/19 05:44:39 - INFO - root -   Epoch: [153/300][60/84], lr: 0.00000100 	 loss = 0.3115(0.3736)
2023/10/19 05:45:26 - INFO - root -   Epoch: [153/300][80/84], lr: 0.00000100 	 loss = 0.0204(0.4010)
2023/10/19 05:45:28 - INFO - root -   Epoch: [153/300] 	 loss = 0.4115
2023/10/19 05:45:28 - INFO - root -   train_accuracy = 0.8155
2023/10/19 05:45:58 - INFO - root -   Epoch: [154/300][0/84], lr: 0.00000101 	 loss = 0.2354(0.2354)
2023/10/19 05:46:49 - INFO - root -   Epoch: [154/300][20/84], lr: 0.00000101 	 loss = 0.9350(0.2810)
2023/10/19 05:48:13 - INFO - root -   Epoch: [154/300][40/84], lr: 0.00000101 	 loss = 0.1051(0.3270)
2023/10/19 05:49:15 - INFO - root -   Epoch: [154/300][60/84], lr: 0.00000101 	 loss = 0.0854(0.3389)
2023/10/19 05:50:08 - INFO - root -   Epoch: [154/300][80/84], lr: 0.00000101 	 loss = 0.1231(0.4022)
2023/10/19 05:50:10 - INFO - root -   Epoch: [154/300] 	 loss = 0.4028
2023/10/19 05:51:06 - INFO - root -   precision = 0.8837
2023/10/19 05:51:06 - INFO - root -   eval_loss = 0.2945
2023/10/19 05:51:07 - INFO - root -   train_accuracy = 0.7917
2023/10/19 05:51:45 - INFO - root -   Epoch: [155/300][0/84], lr: 0.00000101 	 loss = 1.2234(1.2234)
2023/10/19 05:52:41 - INFO - root -   Epoch: [155/300][20/84], lr: 0.00000101 	 loss = 0.0764(0.3805)
2023/10/19 05:53:57 - INFO - root -   Epoch: [155/300][40/84], lr: 0.00000101 	 loss = 0.0409(0.3716)
2023/10/19 05:55:04 - INFO - root -   Epoch: [155/300][60/84], lr: 0.00000101 	 loss = 0.2571(0.3832)
2023/10/19 05:55:44 - INFO - root -   Epoch: [155/300][80/84], lr: 0.00000101 	 loss = 0.0889(0.3940)
2023/10/19 05:55:45 - INFO - root -   Epoch: [155/300] 	 loss = 0.4143
2023/10/19 05:55:45 - INFO - root -   train_accuracy = 0.8095
2023/10/19 05:56:16 - INFO - root -   Epoch: [156/300][0/84], lr: 0.00000102 	 loss = 0.5687(0.5687)
2023/10/19 05:57:07 - INFO - root -   Epoch: [156/300][20/84], lr: 0.00000102 	 loss = 0.8940(0.3767)
2023/10/19 05:58:21 - INFO - root -   Epoch: [156/300][40/84], lr: 0.00000102 	 loss = 0.0756(0.4184)
2023/10/19 05:59:17 - INFO - root -   Epoch: [156/300][60/84], lr: 0.00000102 	 loss = 0.3213(0.4255)
2023/10/19 06:00:19 - INFO - root -   Epoch: [156/300][80/84], lr: 0.00000102 	 loss = 0.1645(0.4160)
2023/10/19 06:00:21 - INFO - root -   Epoch: [156/300] 	 loss = 0.4289
2023/10/19 06:00:21 - INFO - root -   train_accuracy = 0.8155
2023/10/19 06:00:50 - INFO - root -   Epoch: [157/300][0/84], lr: 0.00000103 	 loss = 0.2009(0.2009)
2023/10/19 06:01:50 - INFO - root -   Epoch: [157/300][20/84], lr: 0.00000103 	 loss = 0.0153(0.4195)
2023/10/19 06:02:56 - INFO - root -   Epoch: [157/300][40/84], lr: 0.00000103 	 loss = 0.4816(0.3583)
2023/10/19 06:04:09 - INFO - root -   Epoch: [157/300][60/84], lr: 0.00000103 	 loss = 0.0620(0.3813)
2023/10/19 06:04:54 - INFO - root -   Epoch: [157/300][80/84], lr: 0.00000103 	 loss = 0.2908(0.3981)
2023/10/19 06:04:59 - INFO - root -   Epoch: [157/300] 	 loss = 0.4196
2023/10/19 06:04:59 - INFO - root -   train_accuracy = 0.7917
2023/10/19 06:05:21 - INFO - root -   Epoch: [158/300][0/84], lr: 0.00000103 	 loss = 0.0490(0.0490)
2023/10/19 06:06:28 - INFO - root -   Epoch: [158/300][20/84], lr: 0.00000103 	 loss = 0.0841(0.3467)
2023/10/19 06:07:25 - INFO - root -   Epoch: [158/300][40/84], lr: 0.00000103 	 loss = 0.0433(0.3571)
2023/10/19 06:09:04 - INFO - root -   Epoch: [158/300][60/84], lr: 0.00000103 	 loss = 0.6464(0.3854)
2023/10/19 06:09:39 - INFO - root -   Epoch: [158/300][80/84], lr: 0.00000103 	 loss = 0.3217(0.4313)
2023/10/19 06:09:40 - INFO - root -   Epoch: [158/300] 	 loss = 0.4341
2023/10/19 06:09:40 - INFO - root -   train_accuracy = 0.7679
2023/10/19 06:10:10 - INFO - root -   Epoch: [159/300][0/84], lr: 0.00000104 	 loss = 0.5904(0.5904)
2023/10/19 06:11:22 - INFO - root -   Epoch: [159/300][20/84], lr: 0.00000104 	 loss = 0.5848(0.6561)
2023/10/19 06:12:24 - INFO - root -   Epoch: [159/300][40/84], lr: 0.00000104 	 loss = 0.4812(0.5027)
2023/10/19 06:13:33 - INFO - root -   Epoch: [159/300][60/84], lr: 0.00000104 	 loss = 0.0760(0.4677)
2023/10/19 06:14:16 - INFO - root -   Epoch: [159/300][80/84], lr: 0.00000104 	 loss = 0.0923(0.4566)
2023/10/19 06:14:19 - INFO - root -   Epoch: [159/300] 	 loss = 0.4495
2023/10/19 06:15:15 - INFO - root -   precision = 0.8837
2023/10/19 06:15:15 - INFO - root -   eval_loss = 0.2861
2023/10/19 06:15:16 - INFO - root -   train_accuracy = 0.7560
2023/10/19 06:15:38 - INFO - root -   Epoch: [160/300][0/84], lr: 0.00000104 	 loss = 0.1101(0.1101)
2023/10/19 06:16:44 - INFO - root -   Epoch: [160/300][20/84], lr: 0.00000104 	 loss = 0.1608(0.3380)
2023/10/19 06:17:51 - INFO - root -   Epoch: [160/300][40/84], lr: 0.00000104 	 loss = 0.2732(0.3253)
2023/10/19 06:18:59 - INFO - root -   Epoch: [160/300][60/84], lr: 0.00000104 	 loss = 0.0242(0.3500)
2023/10/19 06:19:42 - INFO - root -   Epoch: [160/300][80/84], lr: 0.00000104 	 loss = 0.0113(0.3561)
2023/10/19 06:19:44 - INFO - root -   Epoch: [160/300] 	 loss = 0.3655
2023/10/19 06:19:44 - INFO - root -   train_accuracy = 0.8214
2023/10/19 06:20:14 - INFO - root -   Epoch: [161/300][0/84], lr: 0.00000105 	 loss = 0.5296(0.5296)
2023/10/19 06:21:29 - INFO - root -   Epoch: [161/300][20/84], lr: 0.00000105 	 loss = 0.3566(0.3681)
2023/10/19 06:22:30 - INFO - root -   Epoch: [161/300][40/84], lr: 0.00000105 	 loss = 0.6233(0.3815)
2023/10/19 06:23:46 - INFO - root -   Epoch: [161/300][60/84], lr: 0.00000105 	 loss = 0.0222(0.3722)
2023/10/19 06:24:31 - INFO - root -   Epoch: [161/300][80/84], lr: 0.00000105 	 loss = 0.0519(0.4246)
2023/10/19 06:24:34 - INFO - root -   Epoch: [161/300] 	 loss = 0.4486
2023/10/19 06:24:34 - INFO - root -   train_accuracy = 0.7976
2023/10/19 06:25:03 - INFO - root -   Epoch: [162/300][0/84], lr: 0.00000105 	 loss = 0.2711(0.2711)
2023/10/19 06:25:54 - INFO - root -   Epoch: [162/300][20/84], lr: 0.00000105 	 loss = 0.0355(0.2376)
2023/10/19 06:26:55 - INFO - root -   Epoch: [162/300][40/84], lr: 0.00000105 	 loss = 0.1095(0.3129)
2023/10/19 06:28:24 - INFO - root -   Epoch: [162/300][60/84], lr: 0.00000105 	 loss = 0.2841(0.3472)
2023/10/19 06:29:14 - INFO - root -   Epoch: [162/300][80/84], lr: 0.00000105 	 loss = 0.0889(0.3837)
2023/10/19 06:29:15 - INFO - root -   Epoch: [162/300] 	 loss = 0.4027
2023/10/19 06:29:15 - INFO - root -   train_accuracy = 0.8214
2023/10/19 06:29:38 - INFO - root -   Epoch: [163/300][0/84], lr: 0.00000106 	 loss = 0.0545(0.0545)
2023/10/19 06:30:59 - INFO - root -   Epoch: [163/300][20/84], lr: 0.00000106 	 loss = 0.0428(0.4025)
2023/10/19 06:32:17 - INFO - root -   Epoch: [163/300][40/84], lr: 0.00000106 	 loss = 0.0644(0.3968)
2023/10/19 06:33:29 - INFO - root -   Epoch: [163/300][60/84], lr: 0.00000106 	 loss = 0.3632(0.3872)
2023/10/19 06:34:14 - INFO - root -   Epoch: [163/300][80/84], lr: 0.00000106 	 loss = 0.0131(0.3763)
2023/10/19 06:34:16 - INFO - root -   Epoch: [163/300] 	 loss = 0.3835
2023/10/19 06:34:16 - INFO - root -   train_accuracy = 0.8036
2023/10/19 06:34:37 - INFO - root -   Epoch: [164/300][0/84], lr: 0.00000107 	 loss = 0.0182(0.0182)
2023/10/19 06:35:45 - INFO - root -   Epoch: [164/300][20/84], lr: 0.00000107 	 loss = 0.1399(0.2540)
2023/10/19 06:36:46 - INFO - root -   Epoch: [164/300][40/84], lr: 0.00000107 	 loss = 0.3288(0.2328)
2023/10/19 06:38:00 - INFO - root -   Epoch: [164/300][60/84], lr: 0.00000107 	 loss = 0.0172(0.2775)
2023/10/19 06:38:49 - INFO - root -   Epoch: [164/300][80/84], lr: 0.00000107 	 loss = 0.0218(0.3342)
2023/10/19 06:38:50 - INFO - root -   Epoch: [164/300] 	 loss = 0.3321
2023/10/19 06:39:47 - INFO - root -   precision = 0.8605
2023/10/19 06:39:47 - INFO - root -   eval_loss = 0.2533
2023/10/19 06:39:48 - INFO - root -   train_accuracy = 0.8512
2023/10/19 06:40:09 - INFO - root -   Epoch: [165/300][0/84], lr: 0.00000107 	 loss = 0.1006(0.1006)
2023/10/19 06:41:23 - INFO - root -   Epoch: [165/300][20/84], lr: 0.00000107 	 loss = 0.7644(0.3243)
2023/10/19 06:42:20 - INFO - root -   Epoch: [165/300][40/84], lr: 0.00000107 	 loss = 0.1637(0.4060)
2023/10/19 06:43:35 - INFO - root -   Epoch: [165/300][60/84], lr: 0.00000107 	 loss = 0.5380(0.4122)
2023/10/19 06:44:28 - INFO - root -   Epoch: [165/300][80/84], lr: 0.00000107 	 loss = 0.3110(0.4302)
2023/10/19 06:44:29 - INFO - root -   Epoch: [165/300] 	 loss = 0.4461
2023/10/19 06:44:29 - INFO - root -   train_accuracy = 0.7917
2023/10/19 06:44:51 - INFO - root -   Epoch: [166/300][0/84], lr: 0.00000108 	 loss = 0.0414(0.0414)
2023/10/19 06:45:57 - INFO - root -   Epoch: [166/300][20/84], lr: 0.00000108 	 loss = 0.1356(0.3499)
2023/10/19 06:47:08 - INFO - root -   Epoch: [166/300][40/84], lr: 0.00000108 	 loss = 0.0220(0.4030)
2023/10/19 06:47:58 - INFO - root -   Epoch: [166/300][60/84], lr: 0.00000108 	 loss = 0.1928(0.4042)
2023/10/19 06:49:01 - INFO - root -   Epoch: [166/300][80/84], lr: 0.00000108 	 loss = 0.0581(0.4184)
2023/10/19 06:49:03 - INFO - root -   Epoch: [166/300] 	 loss = 0.4270
2023/10/19 06:49:03 - INFO - root -   train_accuracy = 0.7560
2023/10/19 06:49:25 - INFO - root -   Epoch: [167/300][0/84], lr: 0.00000108 	 loss = 0.1497(0.1497)
2023/10/19 06:50:34 - INFO - root -   Epoch: [167/300][20/84], lr: 0.00000108 	 loss = 0.1419(0.3963)
2023/10/19 06:51:56 - INFO - root -   Epoch: [167/300][40/84], lr: 0.00000108 	 loss = 0.1947(0.3660)
2023/10/19 06:53:00 - INFO - root -   Epoch: [167/300][60/84], lr: 0.00000108 	 loss = 0.1478(0.3878)
2023/10/19 06:53:44 - INFO - root -   Epoch: [167/300][80/84], lr: 0.00000108 	 loss = 0.0322(0.4438)
2023/10/19 06:53:45 - INFO - root -   Epoch: [167/300] 	 loss = 0.4607
2023/10/19 06:53:45 - INFO - root -   train_accuracy = 0.7798
2023/10/19 06:54:16 - INFO - root -   Epoch: [168/300][0/84], lr: 0.00000109 	 loss = 0.3145(0.3145)
2023/10/19 06:55:16 - INFO - root -   Epoch: [168/300][20/84], lr: 0.00000109 	 loss = 0.9242(0.3498)
2023/10/19 06:56:37 - INFO - root -   Epoch: [168/300][40/84], lr: 0.00000109 	 loss = 0.0498(0.3344)
2023/10/19 06:57:26 - INFO - root -   Epoch: [168/300][60/84], lr: 0.00000109 	 loss = 0.0588(0.3577)
2023/10/19 06:58:18 - INFO - root -   Epoch: [168/300][80/84], lr: 0.00000109 	 loss = 0.5297(0.3799)
2023/10/19 06:58:19 - INFO - root -   Epoch: [168/300] 	 loss = 0.3864
2023/10/19 06:58:19 - INFO - root -   train_accuracy = 0.8214
2023/10/19 06:59:03 - INFO - root -   Epoch: [169/300][0/84], lr: 0.00000110 	 loss = 0.5793(0.5793)
2023/10/19 06:59:55 - INFO - root -   Epoch: [169/300][20/84], lr: 0.00000110 	 loss = 0.0183(0.2777)
2023/10/19 07:01:29 - INFO - root -   Epoch: [169/300][40/84], lr: 0.00000110 	 loss = 0.1166(0.2790)
2023/10/19 07:02:20 - INFO - root -   Epoch: [169/300][60/84], lr: 0.00000110 	 loss = 0.0491(0.3454)
2023/10/19 07:03:12 - INFO - root -   Epoch: [169/300][80/84], lr: 0.00000110 	 loss = 0.2675(0.4124)
2023/10/19 07:03:13 - INFO - root -   Epoch: [169/300] 	 loss = 0.4104
2023/10/19 07:04:09 - INFO - root -   precision = 0.8605
2023/10/19 07:04:09 - INFO - root -   eval_loss = 0.3436
2023/10/19 07:04:10 - INFO - root -   train_accuracy = 0.8274
2023/10/19 07:04:32 - INFO - root -   Epoch: [170/300][0/84], lr: 0.00000110 	 loss = 0.0578(0.0578)
2023/10/19 07:05:40 - INFO - root -   Epoch: [170/300][20/84], lr: 0.00000110 	 loss = 0.0097(0.3571)
2023/10/19 07:06:49 - INFO - root -   Epoch: [170/300][40/84], lr: 0.00000110 	 loss = 0.0479(0.3842)
2023/10/19 07:07:57 - INFO - root -   Epoch: [170/300][60/84], lr: 0.00000110 	 loss = 0.1303(0.4271)
2023/10/19 07:08:48 - INFO - root -   Epoch: [170/300][80/84], lr: 0.00000110 	 loss = 0.0241(0.4346)
2023/10/19 07:08:49 - INFO - root -   Epoch: [170/300] 	 loss = 0.4561
2023/10/19 07:08:49 - INFO - root -   train_accuracy = 0.8036
2023/10/19 07:09:27 - INFO - root -   Epoch: [171/300][0/84], lr: 0.00000111 	 loss = 0.2344(0.2344)
2023/10/19 07:10:26 - INFO - root -   Epoch: [171/300][20/84], lr: 0.00000111 	 loss = 0.0608(0.3786)
2023/10/19 07:11:34 - INFO - root -   Epoch: [171/300][40/84], lr: 0.00000111 	 loss = 0.6762(0.4173)
2023/10/19 07:12:41 - INFO - root -   Epoch: [171/300][60/84], lr: 0.00000111 	 loss = 0.0680(0.4268)
2023/10/19 07:13:30 - INFO - root -   Epoch: [171/300][80/84], lr: 0.00000111 	 loss = 0.0670(0.4552)
2023/10/19 07:13:34 - INFO - root -   Epoch: [171/300] 	 loss = 0.4697
2023/10/19 07:13:34 - INFO - root -   train_accuracy = 0.7560
2023/10/19 07:13:56 - INFO - root -   Epoch: [172/300][0/84], lr: 0.00000111 	 loss = 0.0828(0.0828)
2023/10/19 07:15:02 - INFO - root -   Epoch: [172/300][20/84], lr: 0.00000111 	 loss = 0.0169(0.2821)
2023/10/19 07:16:15 - INFO - root -   Epoch: [172/300][40/84], lr: 0.00000111 	 loss = 0.0147(0.3434)
2023/10/19 07:17:25 - INFO - root -   Epoch: [172/300][60/84], lr: 0.00000111 	 loss = 0.0985(0.3506)
2023/10/19 07:18:11 - INFO - root -   Epoch: [172/300][80/84], lr: 0.00000111 	 loss = 0.0207(0.3196)
2023/10/19 07:18:14 - INFO - root -   Epoch: [172/300] 	 loss = 0.3401
2023/10/19 07:18:14 - INFO - root -   train_accuracy = 0.8214
2023/10/19 07:18:36 - INFO - root -   Epoch: [173/300][0/84], lr: 0.00000112 	 loss = 0.4985(0.4985)
2023/10/19 07:19:42 - INFO - root -   Epoch: [173/300][20/84], lr: 0.00000112 	 loss = 0.4053(0.4553)
2023/10/19 07:20:53 - INFO - root -   Epoch: [173/300][40/84], lr: 0.00000112 	 loss = 0.1355(0.4361)
2023/10/19 07:22:14 - INFO - root -   Epoch: [173/300][60/84], lr: 0.00000112 	 loss = 0.0686(0.4488)
2023/10/19 07:23:02 - INFO - root -   Epoch: [173/300][80/84], lr: 0.00000112 	 loss = 0.3010(0.4928)
2023/10/19 07:23:04 - INFO - root -   Epoch: [173/300] 	 loss = 0.4994
2023/10/19 07:23:04 - INFO - root -   train_accuracy = 0.7917
2023/10/19 07:23:26 - INFO - root -   Epoch: [174/300][0/84], lr: 0.00000113 	 loss = 0.1848(0.1848)
2023/10/19 07:24:50 - INFO - root -   Epoch: [174/300][20/84], lr: 0.00000113 	 loss = 0.0054(0.4137)
2023/10/19 07:25:52 - INFO - root -   Epoch: [174/300][40/84], lr: 0.00000113 	 loss = 0.2603(0.3883)
2023/10/19 07:27:03 - INFO - root -   Epoch: [174/300][60/84], lr: 0.00000113 	 loss = 0.1350(0.4666)
2023/10/19 07:27:46 - INFO - root -   Epoch: [174/300][80/84], lr: 0.00000113 	 loss = 0.1148(0.4707)
2023/10/19 07:27:52 - INFO - root -   Epoch: [174/300] 	 loss = 0.4836
2023/10/19 07:28:48 - INFO - root -   precision = 0.8837
2023/10/19 07:28:48 - INFO - root -   eval_loss = 0.2735
2023/10/19 07:28:49 - INFO - root -   train_accuracy = 0.7619
2023/10/19 07:29:19 - INFO - root -   Epoch: [175/300][0/84], lr: 0.00000113 	 loss = 0.3674(0.3674)
2023/10/19 07:30:08 - INFO - root -   Epoch: [175/300][20/84], lr: 0.00000113 	 loss = 0.0427(0.3157)
2023/10/19 07:31:17 - INFO - root -   Epoch: [175/300][40/84], lr: 0.00000113 	 loss = 0.0905(0.3206)
2023/10/19 07:32:28 - INFO - root -   Epoch: [175/300][60/84], lr: 0.00000113 	 loss = 0.0534(0.3171)
2023/10/19 07:33:25 - INFO - root -   Epoch: [175/300][80/84], lr: 0.00000113 	 loss = 0.0353(0.2913)
2023/10/19 07:33:26 - INFO - root -   Epoch: [175/300] 	 loss = 0.3230
2023/10/19 07:33:26 - INFO - root -   train_accuracy = 0.8869
2023/10/19 07:33:48 - INFO - root -   Epoch: [176/300][0/84], lr: 0.00000114 	 loss = 0.1420(0.1420)
2023/10/19 07:34:53 - INFO - root -   Epoch: [176/300][20/84], lr: 0.00000114 	 loss = 0.8546(0.3781)
2023/10/19 07:36:02 - INFO - root -   Epoch: [176/300][40/84], lr: 0.00000114 	 loss = 0.0529(0.4016)
2023/10/19 07:37:20 - INFO - root -   Epoch: [176/300][60/84], lr: 0.00000114 	 loss = 0.8987(0.4150)
2023/10/19 07:38:09 - INFO - root -   Epoch: [176/300][80/84], lr: 0.00000114 	 loss = 0.0350(0.4305)
2023/10/19 07:38:10 - INFO - root -   Epoch: [176/300] 	 loss = 0.4285
2023/10/19 07:38:10 - INFO - root -   train_accuracy = 0.7917
2023/10/19 07:38:40 - INFO - root -   Epoch: [177/300][0/84], lr: 0.00000114 	 loss = 1.1554(1.1554)
2023/10/19 07:39:46 - INFO - root -   Epoch: [177/300][20/84], lr: 0.00000114 	 loss = 0.0492(0.4454)
2023/10/19 07:40:45 - INFO - root -   Epoch: [177/300][40/84], lr: 0.00000114 	 loss = 0.0197(0.4178)
2023/10/19 07:42:06 - INFO - root -   Epoch: [177/300][60/84], lr: 0.00000114 	 loss = 0.1358(0.3841)
2023/10/19 07:42:52 - INFO - root -   Epoch: [177/300][80/84], lr: 0.00000114 	 loss = 0.1762(0.3929)
2023/10/19 07:42:56 - INFO - root -   Epoch: [177/300] 	 loss = 0.4052
2023/10/19 07:42:56 - INFO - root -   train_accuracy = 0.8095
2023/10/19 07:43:18 - INFO - root -   Epoch: [178/300][0/84], lr: 0.00000115 	 loss = 0.0907(0.0907)
2023/10/19 07:44:34 - INFO - root -   Epoch: [178/300][20/84], lr: 0.00000115 	 loss = 0.1077(0.3187)
2023/10/19 07:45:49 - INFO - root -   Epoch: [178/300][40/84], lr: 0.00000115 	 loss = 0.1173(0.3628)
2023/10/19 07:46:47 - INFO - root -   Epoch: [178/300][60/84], lr: 0.00000115 	 loss = 0.0341(0.3745)
2023/10/19 07:47:41 - INFO - root -   Epoch: [178/300][80/84], lr: 0.00000115 	 loss = 0.0240(0.3992)
2023/10/19 07:47:43 - INFO - root -   Epoch: [178/300] 	 loss = 0.3973
2023/10/19 07:47:43 - INFO - root -   train_accuracy = 0.7679
2023/10/19 07:48:05 - INFO - root -   Epoch: [179/300][0/84], lr: 0.00000115 	 loss = 0.3157(0.3157)
2023/10/19 07:49:22 - INFO - root -   Epoch: [179/300][20/84], lr: 0.00000115 	 loss = 0.3966(0.3444)
2023/10/19 07:50:10 - INFO - root -   Epoch: [179/300][40/84], lr: 0.00000115 	 loss = 0.0763(0.3251)
2023/10/19 07:51:26 - INFO - root -   Epoch: [179/300][60/84], lr: 0.00000115 	 loss = 0.0974(0.3221)
2023/10/19 07:52:09 - INFO - root -   Epoch: [179/300][80/84], lr: 0.00000115 	 loss = 0.3766(0.3307)
2023/10/19 07:52:15 - INFO - root -   Epoch: [179/300] 	 loss = 0.3260
2023/10/19 07:53:12 - INFO - root -   precision = 0.8605
2023/10/19 07:53:12 - INFO - root -   eval_loss = 0.2522
2023/10/19 07:53:13 - INFO - root -   train_accuracy = 0.8750
2023/10/19 07:53:34 - INFO - root -   Epoch: [180/300][0/84], lr: 0.00000116 	 loss = 0.1844(0.1844)
2023/10/19 07:54:48 - INFO - root -   Epoch: [180/300][20/84], lr: 0.00000116 	 loss = 0.0090(0.2396)
2023/10/19 07:55:47 - INFO - root -   Epoch: [180/300][40/84], lr: 0.00000116 	 loss = 0.3573(0.2618)
2023/10/19 07:56:48 - INFO - root -   Epoch: [180/300][60/84], lr: 0.00000116 	 loss = 0.3064(0.3263)
2023/10/19 07:57:54 - INFO - root -   Epoch: [180/300][80/84], lr: 0.00000116 	 loss = 0.0966(0.3571)
2023/10/19 07:57:55 - INFO - root -   Epoch: [180/300] 	 loss = 0.3516
2023/10/19 07:57:55 - INFO - root -   train_accuracy = 0.8690
2023/10/19 07:58:17 - INFO - root -   Epoch: [181/300][0/84], lr: 0.00000117 	 loss = 0.5053(0.5053)
2023/10/19 07:59:22 - INFO - root -   Epoch: [181/300][20/84], lr: 0.00000117 	 loss = 0.0203(0.2111)
2023/10/19 08:00:21 - INFO - root -   Epoch: [181/300][40/84], lr: 0.00000117 	 loss = 0.0410(0.2602)
2023/10/19 08:01:42 - INFO - root -   Epoch: [181/300][60/84], lr: 0.00000117 	 loss = 0.1207(0.3117)
2023/10/19 08:02:42 - INFO - root -   Epoch: [181/300][80/84], lr: 0.00000117 	 loss = 0.2743(0.3508)
2023/10/19 08:02:44 - INFO - root -   Epoch: [181/300] 	 loss = 0.3805
2023/10/19 08:02:44 - INFO - root -   train_accuracy = 0.8214
2023/10/19 08:03:14 - INFO - root -   Epoch: [182/300][0/84], lr: 0.00000117 	 loss = 0.4508(0.4508)
2023/10/19 08:04:14 - INFO - root -   Epoch: [182/300][20/84], lr: 0.00000117 	 loss = 0.0780(0.5938)
2023/10/19 08:05:22 - INFO - root -   Epoch: [182/300][40/84], lr: 0.00000117 	 loss = 0.2115(0.5568)
2023/10/19 08:06:28 - INFO - root -   Epoch: [182/300][60/84], lr: 0.00000117 	 loss = 0.1333(0.5645)
2023/10/19 08:07:20 - INFO - root -   Epoch: [182/300][80/84], lr: 0.00000117 	 loss = 0.1094(0.5314)
2023/10/19 08:07:21 - INFO - root -   Epoch: [182/300] 	 loss = 0.5308
2023/10/19 08:07:21 - INFO - root -   train_accuracy = 0.7381
2023/10/19 08:07:43 - INFO - root -   Epoch: [183/300][0/84], lr: 0.00000118 	 loss = 0.1297(0.1297)
2023/10/19 08:08:58 - INFO - root -   Epoch: [183/300][20/84], lr: 0.00000118 	 loss = 0.0444(0.3782)
2023/10/19 08:09:55 - INFO - root -   Epoch: [183/300][40/84], lr: 0.00000118 	 loss = 0.1330(0.3976)
2023/10/19 08:11:01 - INFO - root -   Epoch: [183/300][60/84], lr: 0.00000118 	 loss = 0.4833(0.4089)
2023/10/19 08:12:10 - INFO - root -   Epoch: [183/300][80/84], lr: 0.00000118 	 loss = 0.0284(0.3976)
2023/10/19 08:12:11 - INFO - root -   Epoch: [183/300] 	 loss = 0.4114
2023/10/19 08:12:11 - INFO - root -   train_accuracy = 0.7798
2023/10/19 08:12:32 - INFO - root -   Epoch: [184/300][0/84], lr: 0.00000118 	 loss = 0.0632(0.0632)
2023/10/19 08:13:51 - INFO - root -   Epoch: [184/300][20/84], lr: 0.00000118 	 loss = 0.2220(0.3199)
2023/10/19 08:14:43 - INFO - root -   Epoch: [184/300][40/84], lr: 0.00000118 	 loss = 0.0502(0.3337)
2023/10/19 08:15:55 - INFO - root -   Epoch: [184/300][60/84], lr: 0.00000118 	 loss = 0.2977(0.3808)
2023/10/19 08:16:50 - INFO - root -   Epoch: [184/300][80/84], lr: 0.00000118 	 loss = 0.1165(0.3720)
2023/10/19 08:16:52 - INFO - root -   Epoch: [184/300] 	 loss = 0.3823
2023/10/19 08:17:49 - INFO - root -   precision = 0.8837
2023/10/19 08:17:49 - INFO - root -   eval_loss = 0.2839
2023/10/19 08:17:50 - INFO - root -   train_accuracy = 0.8095
2023/10/19 08:18:20 - INFO - root -   Epoch: [185/300][0/84], lr: 0.00000119 	 loss = 0.1393(0.1393)
2023/10/19 08:19:20 - INFO - root -   Epoch: [185/300][20/84], lr: 0.00000119 	 loss = 0.0129(0.2776)
2023/10/19 08:20:30 - INFO - root -   Epoch: [185/300][40/84], lr: 0.00000119 	 loss = 0.0746(0.3475)
2023/10/19 08:21:44 - INFO - root -   Epoch: [185/300][60/84], lr: 0.00000119 	 loss = 0.0350(0.3497)
2023/10/19 08:22:34 - INFO - root -   Epoch: [185/300][80/84], lr: 0.00000119 	 loss = 0.3200(0.3387)
2023/10/19 08:22:36 - INFO - root -   Epoch: [185/300] 	 loss = 0.3493
2023/10/19 08:22:36 - INFO - root -   train_accuracy = 0.8036
2023/10/19 08:23:05 - INFO - root -   Epoch: [186/300][0/84], lr: 0.00000120 	 loss = 1.4646(1.4646)
2023/10/19 08:24:11 - INFO - root -   Epoch: [186/300][20/84], lr: 0.00000120 	 loss = 0.2443(0.4143)
2023/10/19 08:25:16 - INFO - root -   Epoch: [186/300][40/84], lr: 0.00000120 	 loss = 0.0239(0.3287)
2023/10/19 08:26:30 - INFO - root -   Epoch: [186/300][60/84], lr: 0.00000120 	 loss = 0.0404(0.3708)
2023/10/19 08:27:15 - INFO - root -   Epoch: [186/300][80/84], lr: 0.00000120 	 loss = 0.1386(0.3892)
2023/10/19 08:27:18 - INFO - root -   Epoch: [186/300] 	 loss = 0.4058
2023/10/19 08:27:18 - INFO - root -   train_accuracy = 0.8155
2023/10/19 08:27:55 - INFO - root -   Epoch: [187/300][0/84], lr: 0.00000120 	 loss = 0.3023(0.3023)
2023/10/19 08:28:57 - INFO - root -   Epoch: [187/300][20/84], lr: 0.00000120 	 loss = 0.2095(0.2839)
2023/10/19 08:29:56 - INFO - root -   Epoch: [187/300][40/84], lr: 0.00000120 	 loss = 0.1093(0.2940)
2023/10/19 08:31:07 - INFO - root -   Epoch: [187/300][60/84], lr: 0.00000120 	 loss = 0.4954(0.3518)
2023/10/19 08:31:55 - INFO - root -   Epoch: [187/300][80/84], lr: 0.00000120 	 loss = 0.0759(0.3525)
2023/10/19 08:32:01 - INFO - root -   Epoch: [187/300] 	 loss = 0.3563
2023/10/19 08:32:01 - INFO - root -   train_accuracy = 0.8452
2023/10/19 08:32:30 - INFO - root -   Epoch: [188/300][0/84], lr: 0.00000121 	 loss = 0.0923(0.0923)
2023/10/19 08:33:46 - INFO - root -   Epoch: [188/300][20/84], lr: 0.00000121 	 loss = 0.5267(0.2751)
2023/10/19 08:34:50 - INFO - root -   Epoch: [188/300][40/84], lr: 0.00000121 	 loss = 0.0538(0.2673)
2023/10/19 08:35:52 - INFO - root -   Epoch: [188/300][60/84], lr: 0.00000121 	 loss = 0.0438(0.3003)
2023/10/19 08:36:41 - INFO - root -   Epoch: [188/300][80/84], lr: 0.00000121 	 loss = 0.0419(0.3159)
2023/10/19 08:36:42 - INFO - root -   Epoch: [188/300] 	 loss = 0.3460
2023/10/19 08:36:42 - INFO - root -   train_accuracy = 0.7976
2023/10/19 08:37:04 - INFO - root -   Epoch: [189/300][0/84], lr: 0.00000121 	 loss = 0.0220(0.0220)
2023/10/19 08:38:11 - INFO - root -   Epoch: [189/300][20/84], lr: 0.00000121 	 loss = 0.0642(0.4865)
2023/10/19 08:39:06 - INFO - root -   Epoch: [189/300][40/84], lr: 0.00000121 	 loss = 0.0906(0.4023)
2023/10/19 08:40:29 - INFO - root -   Epoch: [189/300][60/84], lr: 0.00000121 	 loss = 0.1669(0.4232)
2023/10/19 08:41:15 - INFO - root -   Epoch: [189/300][80/84], lr: 0.00000121 	 loss = 0.6527(0.3954)
2023/10/19 08:41:16 - INFO - root -   Epoch: [189/300] 	 loss = 0.3959
2023/10/19 08:42:13 - INFO - root -   precision = 0.8605
2023/10/19 08:42:13 - INFO - root -   eval_loss = 0.2928
2023/10/19 08:42:13 - INFO - root -   train_accuracy = 0.7917
2023/10/19 08:42:52 - INFO - root -   Epoch: [190/300][0/84], lr: 0.00000122 	 loss = 0.4665(0.4665)
2023/10/19 08:43:42 - INFO - root -   Epoch: [190/300][20/84], lr: 0.00000122 	 loss = 0.1878(0.2616)
2023/10/19 08:44:51 - INFO - root -   Epoch: [190/300][40/84], lr: 0.00000122 	 loss = 0.0277(0.2925)
2023/10/19 08:45:58 - INFO - root -   Epoch: [190/300][60/84], lr: 0.00000122 	 loss = 0.3727(0.3542)
2023/10/19 08:46:45 - INFO - root -   Epoch: [190/300][80/84], lr: 0.00000122 	 loss = 0.1547(0.3476)
2023/10/19 08:46:46 - INFO - root -   Epoch: [190/300] 	 loss = 0.3721
2023/10/19 08:46:46 - INFO - root -   train_accuracy = 0.8036
2023/10/19 08:47:07 - INFO - root -   Epoch: [191/300][0/84], lr: 0.00000123 	 loss = 0.0755(0.0755)
2023/10/19 08:48:32 - INFO - root -   Epoch: [191/300][20/84], lr: 0.00000123 	 loss = 0.0726(0.2558)
2023/10/19 08:49:17 - INFO - root -   Epoch: [191/300][40/84], lr: 0.00000123 	 loss = 0.0384(0.2739)
2023/10/19 08:50:33 - INFO - root -   Epoch: [191/300][60/84], lr: 0.00000123 	 loss = 0.0193(0.2618)
2023/10/19 08:51:23 - INFO - root -   Epoch: [191/300][80/84], lr: 0.00000123 	 loss = 0.9756(0.3134)
2023/10/19 08:51:24 - INFO - root -   Epoch: [191/300] 	 loss = 0.3244
2023/10/19 08:51:24 - INFO - root -   train_accuracy = 0.8631
2023/10/19 08:51:46 - INFO - root -   Epoch: [192/300][0/84], lr: 0.00000123 	 loss = 0.1137(0.1137)
2023/10/19 08:53:00 - INFO - root -   Epoch: [192/300][20/84], lr: 0.00000123 	 loss = 0.5232(0.2516)
2023/10/19 08:53:52 - INFO - root -   Epoch: [192/300][40/84], lr: 0.00000123 	 loss = 0.0625(0.2735)
2023/10/19 08:55:05 - INFO - root -   Epoch: [192/300][60/84], lr: 0.00000123 	 loss = 0.1509(0.2892)
2023/10/19 08:55:56 - INFO - root -   Epoch: [192/300][80/84], lr: 0.00000123 	 loss = 0.5158(0.3226)
2023/10/19 08:55:59 - INFO - root -   Epoch: [192/300] 	 loss = 0.3286
2023/10/19 08:55:59 - INFO - root -   train_accuracy = 0.8333
2023/10/19 08:56:38 - INFO - root -   Epoch: [193/300][0/84], lr: 0.00000124 	 loss = 0.4530(0.4530)
2023/10/19 08:57:36 - INFO - root -   Epoch: [193/300][20/84], lr: 0.00000124 	 loss = 0.4148(0.2909)
2023/10/19 08:58:36 - INFO - root -   Epoch: [193/300][40/84], lr: 0.00000124 	 loss = 0.0464(0.2879)
2023/10/19 08:59:46 - INFO - root -   Epoch: [193/300][60/84], lr: 0.00000124 	 loss = 0.1686(0.3268)
2023/10/19 09:00:33 - INFO - root -   Epoch: [193/300][80/84], lr: 0.00000124 	 loss = 0.0624(0.3247)
2023/10/19 09:00:35 - INFO - root -   Epoch: [193/300] 	 loss = 0.3238
2023/10/19 09:00:35 - INFO - root -   train_accuracy = 0.8274
2023/10/19 09:01:05 - INFO - root -   Epoch: [194/300][0/84], lr: 0.00000124 	 loss = 0.3965(0.3965)
2023/10/19 09:02:10 - INFO - root -   Epoch: [194/300][20/84], lr: 0.00000124 	 loss = 0.3832(0.3034)
2023/10/19 09:03:06 - INFO - root -   Epoch: [194/300][40/84], lr: 0.00000124 	 loss = 0.0431(0.3263)
2023/10/19 09:04:16 - INFO - root -   Epoch: [194/300][60/84], lr: 0.00000124 	 loss = 0.6243(0.3576)
2023/10/19 09:05:13 - INFO - root -   Epoch: [194/300][80/84], lr: 0.00000124 	 loss = 0.0191(0.3459)
2023/10/19 09:05:14 - INFO - root -   Epoch: [194/300] 	 loss = 0.3435
2023/10/19 09:06:10 - INFO - root -   precision = 0.8605
2023/10/19 09:06:10 - INFO - root -   eval_loss = 0.3267
2023/10/19 09:06:11 - INFO - root -   train_accuracy = 0.8155
2023/10/19 09:06:41 - INFO - root -   Epoch: [195/300][0/84], lr: 0.00000125 	 loss = 0.0510(0.0510)
2023/10/19 09:07:42 - INFO - root -   Epoch: [195/300][20/84], lr: 0.00000125 	 loss = 0.0337(0.3329)
2023/10/19 09:08:58 - INFO - root -   Epoch: [195/300][40/84], lr: 0.00000125 	 loss = 0.0396(0.3662)
2023/10/19 09:10:05 - INFO - root -   Epoch: [195/300][60/84], lr: 0.00000125 	 loss = 0.0739(0.4072)
2023/10/19 09:10:52 - INFO - root -   Epoch: [195/300][80/84], lr: 0.00000125 	 loss = 0.0098(0.4062)
2023/10/19 09:10:54 - INFO - root -   Epoch: [195/300] 	 loss = 0.4262
2023/10/19 09:10:54 - INFO - root -   train_accuracy = 0.8155
2023/10/19 09:11:23 - INFO - root -   Epoch: [196/300][0/84], lr: 0.00000126 	 loss = 2.2131(2.2131)
2023/10/19 09:12:15 - INFO - root -   Epoch: [196/300][20/84], lr: 0.00000126 	 loss = 0.2546(0.3615)
2023/10/19 09:13:38 - INFO - root -   Epoch: [196/300][40/84], lr: 0.00000126 	 loss = 0.0171(0.3302)
2023/10/19 09:14:46 - INFO - root -   Epoch: [196/300][60/84], lr: 0.00000126 	 loss = 0.7473(0.3406)
2023/10/19 09:15:27 - INFO - root -   Epoch: [196/300][80/84], lr: 0.00000126 	 loss = 0.1050(0.3619)
2023/10/19 09:15:28 - INFO - root -   Epoch: [196/300] 	 loss = 0.3573
2023/10/19 09:15:28 - INFO - root -   train_accuracy = 0.8036
2023/10/19 09:15:58 - INFO - root -   Epoch: [197/300][0/84], lr: 0.00000126 	 loss = 0.3234(0.3234)
2023/10/19 09:17:15 - INFO - root -   Epoch: [197/300][20/84], lr: 0.00000126 	 loss = 0.9315(0.3613)
2023/10/19 09:18:17 - INFO - root -   Epoch: [197/300][40/84], lr: 0.00000126 	 loss = 0.3510(0.3433)
2023/10/19 09:19:17 - INFO - root -   Epoch: [197/300][60/84], lr: 0.00000126 	 loss = 0.5916(0.3319)
2023/10/19 09:20:03 - INFO - root -   Epoch: [197/300][80/84], lr: 0.00000126 	 loss = 0.7439(0.3133)
2023/10/19 09:20:09 - INFO - root -   Epoch: [197/300] 	 loss = 0.3401
2023/10/19 09:20:09 - INFO - root -   train_accuracy = 0.8512
2023/10/19 09:20:39 - INFO - root -   Epoch: [198/300][0/84], lr: 0.00000127 	 loss = 0.1001(0.1001)
2023/10/19 09:21:51 - INFO - root -   Epoch: [198/300][20/84], lr: 0.00000127 	 loss = 1.9680(0.3366)
2023/10/19 09:23:05 - INFO - root -   Epoch: [198/300][40/84], lr: 0.00000127 	 loss = 0.1615(0.3331)
2023/10/19 09:24:01 - INFO - root -   Epoch: [198/300][60/84], lr: 0.00000127 	 loss = 0.0300(0.3151)
2023/10/19 09:24:56 - INFO - root -   Epoch: [198/300][80/84], lr: 0.00000127 	 loss = 0.0159(0.3661)
2023/10/19 09:25:00 - INFO - root -   Epoch: [198/300] 	 loss = 0.3730
2023/10/19 09:25:00 - INFO - root -   train_accuracy = 0.8452
2023/10/19 09:25:30 - INFO - root -   Epoch: [199/300][0/84], lr: 0.00000127 	 loss = 0.0537(0.0537)
2023/10/19 09:26:20 - INFO - root -   Epoch: [199/300][20/84], lr: 0.00000127 	 loss = 0.0227(0.2321)
2023/10/19 09:27:36 - INFO - root -   Epoch: [199/300][40/84], lr: 0.00000127 	 loss = 0.1103(0.3589)
2023/10/19 09:28:35 - INFO - root -   Epoch: [199/300][60/84], lr: 0.00000127 	 loss = 0.5324(0.3747)
2023/10/19 09:29:33 - INFO - root -   Epoch: [199/300][80/84], lr: 0.00000127 	 loss = 0.6233(0.3973)
2023/10/19 09:29:35 - INFO - root -   Epoch: [199/300] 	 loss = 0.4016
2023/10/19 09:30:31 - INFO - root -   precision = 0.8372
2023/10/19 09:30:31 - INFO - root -   eval_loss = 0.3049
2023/10/19 09:30:32 - INFO - root -   train_accuracy = 0.7976
2023/10/19 09:31:02 - INFO - root -   Epoch: [200/300][0/84], lr: 0.00000128 	 loss = 0.2824(0.2824)
2023/10/19 09:32:02 - INFO - root -   Epoch: [200/300][20/84], lr: 0.00000128 	 loss = 0.0097(0.4221)
2023/10/19 09:33:09 - INFO - root -   Epoch: [200/300][40/84], lr: 0.00000128 	 loss = 0.2456(0.3780)
2023/10/19 09:34:06 - INFO - root -   Epoch: [200/300][60/84], lr: 0.00000128 	 loss = 0.0134(0.4014)
2023/10/19 09:35:06 - INFO - root -   Epoch: [200/300][80/84], lr: 0.00000128 	 loss = 0.4765(0.3584)
2023/10/19 09:35:07 - INFO - root -   Epoch: [200/300] 	 loss = 0.3540
2023/10/19 09:35:07 - INFO - root -   train_accuracy = 0.8333
2023/10/19 09:35:44 - INFO - root -   Epoch: [201/300][0/84], lr: 0.00000128 	 loss = 2.0380(2.0380)
2023/10/19 09:36:47 - INFO - root -   Epoch: [201/300][20/84], lr: 0.00000128 	 loss = 0.4483(0.4528)
2023/10/19 09:38:05 - INFO - root -   Epoch: [201/300][40/84], lr: 0.00000128 	 loss = 0.0972(0.3519)
2023/10/19 09:39:01 - INFO - root -   Epoch: [201/300][60/84], lr: 0.00000128 	 loss = 0.0314(0.3372)
2023/10/19 09:39:42 - INFO - root -   Epoch: [201/300][80/84], lr: 0.00000128 	 loss = 0.1554(0.3768)
2023/10/19 09:39:44 - INFO - root -   Epoch: [201/300] 	 loss = 0.3724
2023/10/19 09:39:44 - INFO - root -   train_accuracy = 0.8095
2023/10/19 09:40:23 - INFO - root -   Epoch: [202/300][0/84], lr: 0.00000129 	 loss = 1.0389(1.0389)
2023/10/19 09:41:14 - INFO - root -   Epoch: [202/300][20/84], lr: 0.00000129 	 loss = 0.4123(0.4401)
2023/10/19 09:42:23 - INFO - root -   Epoch: [202/300][40/84], lr: 0.00000129 	 loss = 0.6827(0.4297)
2023/10/19 09:43:25 - INFO - root -   Epoch: [202/300][60/84], lr: 0.00000129 	 loss = 0.0912(0.4167)
2023/10/19 09:44:20 - INFO - root -   Epoch: [202/300][80/84], lr: 0.00000129 	 loss = 0.1705(0.3834)
2023/10/19 09:44:21 - INFO - root -   Epoch: [202/300] 	 loss = 0.3792
2023/10/19 09:44:21 - INFO - root -   train_accuracy = 0.8214
2023/10/19 09:44:52 - INFO - root -   Epoch: [203/300][0/84], lr: 0.00000130 	 loss = 0.3964(0.3964)
2023/10/19 09:45:59 - INFO - root -   Epoch: [203/300][20/84], lr: 0.00000130 	 loss = 0.5722(0.4111)
2023/10/19 09:47:16 - INFO - root -   Epoch: [203/300][40/84], lr: 0.00000130 	 loss = 0.5919(0.3838)
2023/10/19 09:48:30 - INFO - root -   Epoch: [203/300][60/84], lr: 0.00000130 	 loss = 0.0288(0.3959)
2023/10/19 09:49:14 - INFO - root -   Epoch: [203/300][80/84], lr: 0.00000130 	 loss = 0.0491(0.3727)
2023/10/19 09:49:16 - INFO - root -   Epoch: [203/300] 	 loss = 0.3731
2023/10/19 09:49:16 - INFO - root -   train_accuracy = 0.8036
2023/10/19 09:49:54 - INFO - root -   Epoch: [204/300][0/84], lr: 0.00000130 	 loss = 0.0745(0.0745)
2023/10/19 09:51:00 - INFO - root -   Epoch: [204/300][20/84], lr: 0.00000130 	 loss = 0.5543(0.3277)
2023/10/19 09:52:26 - INFO - root -   Epoch: [204/300][40/84], lr: 0.00000130 	 loss = 0.2152(0.3126)
2023/10/19 09:53:22 - INFO - root -   Epoch: [204/300][60/84], lr: 0.00000130 	 loss = 0.2519(0.3610)
2023/10/19 09:54:13 - INFO - root -   Epoch: [204/300][80/84], lr: 0.00000130 	 loss = 0.0618(0.3434)
2023/10/19 09:54:15 - INFO - root -   Epoch: [204/300] 	 loss = 0.3462
2023/10/19 09:55:11 - INFO - root -   precision = 0.8605
2023/10/19 09:55:11 - INFO - root -   eval_loss = 0.2991
2023/10/19 09:55:12 - INFO - root -   train_accuracy = 0.8631
2023/10/19 09:55:42 - INFO - root -   Epoch: [205/300][0/84], lr: 0.00000131 	 loss = 1.0679(1.0679)
2023/10/19 09:56:41 - INFO - root -   Epoch: [205/300][20/84], lr: 0.00000131 	 loss = 0.4626(0.3665)
2023/10/19 09:58:06 - INFO - root -   Epoch: [205/300][40/84], lr: 0.00000131 	 loss = 0.0329(0.3841)
2023/10/19 09:58:49 - INFO - root -   Epoch: [205/300][60/84], lr: 0.00000131 	 loss = 0.1631(0.4074)
2023/10/19 09:59:54 - INFO - root -   Epoch: [205/300][80/84], lr: 0.00000131 	 loss = 0.2546(0.4054)
2023/10/19 09:59:55 - INFO - root -   Epoch: [205/300] 	 loss = 0.4289
2023/10/19 09:59:55 - INFO - root -   train_accuracy = 0.7679
2023/10/19 10:00:33 - INFO - root -   Epoch: [206/300][0/84], lr: 0.00000131 	 loss = 0.7253(0.7253)
2023/10/19 10:01:36 - INFO - root -   Epoch: [206/300][20/84], lr: 0.00000131 	 loss = 0.0103(0.2635)
2023/10/19 10:02:27 - INFO - root -   Epoch: [206/300][40/84], lr: 0.00000131 	 loss = 0.0541(0.3193)
2023/10/19 10:03:45 - INFO - root -   Epoch: [206/300][60/84], lr: 0.00000131 	 loss = 0.0427(0.3350)
2023/10/19 10:04:26 - INFO - root -   Epoch: [206/300][80/84], lr: 0.00000131 	 loss = 0.0517(0.3118)
2023/10/19 10:04:28 - INFO - root -   Epoch: [206/300] 	 loss = 0.3275
2023/10/19 10:04:28 - INFO - root -   train_accuracy = 0.8690
2023/10/19 10:04:59 - INFO - root -   Epoch: [207/300][0/84], lr: 0.00000132 	 loss = 0.1110(0.1110)
2023/10/19 10:06:10 - INFO - root -   Epoch: [207/300][20/84], lr: 0.00000132 	 loss = 0.1073(0.1904)
2023/10/19 10:07:02 - INFO - root -   Epoch: [207/300][40/84], lr: 0.00000132 	 loss = 0.0164(0.2389)
2023/10/19 10:08:25 - INFO - root -   Epoch: [207/300][60/84], lr: 0.00000132 	 loss = 0.0384(0.3146)
2023/10/19 10:09:11 - INFO - root -   Epoch: [207/300][80/84], lr: 0.00000132 	 loss = 0.4486(0.3219)
2023/10/19 10:09:15 - INFO - root -   Epoch: [207/300] 	 loss = 0.3253
2023/10/19 10:09:15 - INFO - root -   train_accuracy = 0.8214
2023/10/19 10:09:44 - INFO - root -   Epoch: [208/300][0/84], lr: 0.00000133 	 loss = 1.9639(1.9639)
2023/10/19 10:10:35 - INFO - root -   Epoch: [208/300][20/84], lr: 0.00000133 	 loss = 0.2571(0.5709)
2023/10/19 10:11:44 - INFO - root -   Epoch: [208/300][40/84], lr: 0.00000133 	 loss = 0.1463(0.4657)
2023/10/19 10:12:43 - INFO - root -   Epoch: [208/300][60/84], lr: 0.00000133 	 loss = 0.2927(0.4631)
2023/10/19 10:13:42 - INFO - root -   Epoch: [208/300][80/84], lr: 0.00000133 	 loss = 0.4743(0.4551)
2023/10/19 10:13:44 - INFO - root -   Epoch: [208/300] 	 loss = 0.4702
2023/10/19 10:13:44 - INFO - root -   train_accuracy = 0.7500
2023/10/19 10:14:06 - INFO - root -   Epoch: [209/300][0/84], lr: 0.00000133 	 loss = 0.0468(0.0468)
2023/10/19 10:15:06 - INFO - root -   Epoch: [209/300][20/84], lr: 0.00000133 	 loss = 0.6039(0.2852)
2023/10/19 10:16:40 - INFO - root -   Epoch: [209/300][40/84], lr: 0.00000133 	 loss = 0.0750(0.3037)
2023/10/19 10:17:50 - INFO - root -   Epoch: [209/300][60/84], lr: 0.00000133 	 loss = 0.0543(0.3474)
2023/10/19 10:18:25 - INFO - root -   Epoch: [209/300][80/84], lr: 0.00000133 	 loss = 0.0274(0.3324)
2023/10/19 10:18:26 - INFO - root -   Epoch: [209/300] 	 loss = 0.3296
2023/10/19 10:19:23 - INFO - root -   precision = 0.9070
2023/10/19 10:19:23 - INFO - root -   eval_loss = 0.2158
2023/10/19 10:19:24 - INFO - root -   train_accuracy = 0.8452
2023/10/19 10:19:54 - INFO - root -   Epoch: [210/300][0/84], lr: 0.00000134 	 loss = 0.3902(0.3902)
2023/10/19 10:20:49 - INFO - root -   Epoch: [210/300][20/84], lr: 0.00000134 	 loss = 0.8541(0.2393)
2023/10/19 10:22:02 - INFO - root -   Epoch: [210/300][40/84], lr: 0.00000134 	 loss = 0.7237(0.2953)
2023/10/19 10:23:10 - INFO - root -   Epoch: [210/300][60/84], lr: 0.00000134 	 loss = 0.1106(0.3334)
2023/10/19 10:23:49 - INFO - root -   Epoch: [210/300][80/84], lr: 0.00000134 	 loss = 0.0079(0.3262)
2023/10/19 10:23:53 - INFO - root -   Epoch: [210/300] 	 loss = 0.3537
2023/10/19 10:23:53 - INFO - root -   train_accuracy = 0.8155
2023/10/19 10:24:15 - INFO - root -   Epoch: [211/300][0/84], lr: 0.00000134 	 loss = 0.0245(0.0245)
2023/10/19 10:25:30 - INFO - root -   Epoch: [211/300][20/84], lr: 0.00000134 	 loss = 0.0215(0.2961)
2023/10/19 10:26:15 - INFO - root -   Epoch: [211/300][40/84], lr: 0.00000134 	 loss = 0.0468(0.2359)
2023/10/19 10:27:31 - INFO - root -   Epoch: [211/300][60/84], lr: 0.00000134 	 loss = 0.0684(0.2759)
2023/10/19 10:28:33 - INFO - root -   Epoch: [211/300][80/84], lr: 0.00000134 	 loss = 0.4375(0.2935)
2023/10/19 10:28:36 - INFO - root -   Epoch: [211/300] 	 loss = 0.3032
2023/10/19 10:28:36 - INFO - root -   train_accuracy = 0.8393
2023/10/19 10:29:15 - INFO - root -   Epoch: [212/300][0/84], lr: 0.00000135 	 loss = 0.6670(0.6670)
2023/10/19 10:30:14 - INFO - root -   Epoch: [212/300][20/84], lr: 0.00000135 	 loss = 0.5384(0.3880)
2023/10/19 10:31:30 - INFO - root -   Epoch: [212/300][40/84], lr: 0.00000135 	 loss = 0.1507(0.3504)
2023/10/19 10:32:28 - INFO - root -   Epoch: [212/300][60/84], lr: 0.00000135 	 loss = 0.1134(0.3332)
2023/10/19 10:33:20 - INFO - root -   Epoch: [212/300][80/84], lr: 0.00000135 	 loss = 0.1284(0.3614)
2023/10/19 10:33:21 - INFO - root -   Epoch: [212/300] 	 loss = 0.3613
2023/10/19 10:33:21 - INFO - root -   train_accuracy = 0.8155
2023/10/19 10:34:00 - INFO - root -   Epoch: [213/300][0/84], lr: 0.00000136 	 loss = 0.8274(0.8274)
2023/10/19 10:34:51 - INFO - root -   Epoch: [213/300][20/84], lr: 0.00000136 	 loss = 1.4708(0.4348)
2023/10/19 10:35:57 - INFO - root -   Epoch: [213/300][40/84], lr: 0.00000136 	 loss = 0.0396(0.3523)
2023/10/19 10:36:53 - INFO - root -   Epoch: [213/300][60/84], lr: 0.00000136 	 loss = 1.1429(0.3568)
2023/10/19 10:37:50 - INFO - root -   Epoch: [213/300][80/84], lr: 0.00000136 	 loss = 0.7494(0.3735)
2023/10/19 10:37:51 - INFO - root -   Epoch: [213/300] 	 loss = 0.3652
2023/10/19 10:37:51 - INFO - root -   train_accuracy = 0.7976
2023/10/19 10:38:14 - INFO - root -   Epoch: [214/300][0/84], lr: 0.00000136 	 loss = 0.1880(0.1880)
2023/10/19 10:39:35 - INFO - root -   Epoch: [214/300][20/84], lr: 0.00000136 	 loss = 0.0639(0.2275)
2023/10/19 10:40:36 - INFO - root -   Epoch: [214/300][40/84], lr: 0.00000136 	 loss = 0.0164(0.3139)
2023/10/19 10:41:51 - INFO - root -   Epoch: [214/300][60/84], lr: 0.00000136 	 loss = 0.1318(0.3090)
2023/10/19 10:42:30 - INFO - root -   Epoch: [214/300][80/84], lr: 0.00000136 	 loss = 0.0316(0.2969)
2023/10/19 10:42:33 - INFO - root -   Epoch: [214/300] 	 loss = 0.3005
2023/10/19 10:43:30 - INFO - root -   precision = 0.8372
2023/10/19 10:43:30 - INFO - root -   eval_loss = 0.2820
2023/10/19 10:43:31 - INFO - root -   train_accuracy = 0.8512
2023/10/19 10:44:10 - INFO - root -   Epoch: [215/300][0/84], lr: 0.00000137 	 loss = 0.6622(0.6622)
2023/10/19 10:45:13 - INFO - root -   Epoch: [215/300][20/84], lr: 0.00000137 	 loss = 1.2924(0.4893)
2023/10/19 10:46:17 - INFO - root -   Epoch: [215/300][40/84], lr: 0.00000137 	 loss = 0.0183(0.4729)
2023/10/19 10:47:31 - INFO - root -   Epoch: [215/300][60/84], lr: 0.00000137 	 loss = 0.0654(0.3888)
2023/10/19 10:48:28 - INFO - root -   Epoch: [215/300][80/84], lr: 0.00000137 	 loss = 1.1394(0.3993)
2023/10/19 10:48:30 - INFO - root -   Epoch: [215/300] 	 loss = 0.3991
2023/10/19 10:48:30 - INFO - root -   train_accuracy = 0.7976
2023/10/19 10:48:53 - INFO - root -   Epoch: [216/300][0/84], lr: 0.00000137 	 loss = 0.0369(0.0369)
2023/10/19 10:50:01 - INFO - root -   Epoch: [216/300][20/84], lr: 0.00000137 	 loss = 0.3222(0.2459)
2023/10/19 10:51:14 - INFO - root -   Epoch: [216/300][40/84], lr: 0.00000137 	 loss = 0.3262(0.3656)
2023/10/19 10:52:22 - INFO - root -   Epoch: [216/300][60/84], lr: 0.00000137 	 loss = 0.4990(0.3607)
2023/10/19 10:53:18 - INFO - root -   Epoch: [216/300][80/84], lr: 0.00000137 	 loss = 0.2254(0.3564)
2023/10/19 10:53:20 - INFO - root -   Epoch: [216/300] 	 loss = 0.3599
2023/10/19 10:53:20 - INFO - root -   train_accuracy = 0.8214
2023/10/19 10:53:42 - INFO - root -   Epoch: [217/300][0/84], lr: 0.00000138 	 loss = 0.0095(0.0095)
2023/10/19 10:54:56 - INFO - root -   Epoch: [217/300][20/84], lr: 0.00000138 	 loss = 0.0028(0.1493)
2023/10/19 10:55:54 - INFO - root -   Epoch: [217/300][40/84], lr: 0.00000138 	 loss = 0.0256(0.2204)
2023/10/19 10:57:17 - INFO - root -   Epoch: [217/300][60/84], lr: 0.00000138 	 loss = 0.0768(0.3028)
2023/10/19 10:57:53 - INFO - root -   Epoch: [217/300][80/84], lr: 0.00000138 	 loss = 0.0316(0.3454)
2023/10/19 10:57:55 - INFO - root -   Epoch: [217/300] 	 loss = 0.3486
2023/10/19 10:57:55 - INFO - root -   train_accuracy = 0.8393
2023/10/19 10:58:16 - INFO - root -   Epoch: [218/300][0/84], lr: 0.00000138 	 loss = 0.0455(0.0455)
2023/10/19 10:59:32 - INFO - root -   Epoch: [218/300][20/84], lr: 0.00000138 	 loss = 0.3865(0.3432)
2023/10/19 11:00:33 - INFO - root -   Epoch: [218/300][40/84], lr: 0.00000138 	 loss = 0.0178(0.3188)
2023/10/19 11:01:39 - INFO - root -   Epoch: [218/300][60/84], lr: 0.00000138 	 loss = 0.2381(0.4178)
2023/10/19 11:02:41 - INFO - root -   Epoch: [218/300][80/84], lr: 0.00000138 	 loss = 0.0100(0.4370)
2023/10/19 11:02:44 - INFO - root -   Epoch: [218/300] 	 loss = 0.4399
2023/10/19 11:02:44 - INFO - root -   train_accuracy = 0.7738
2023/10/19 11:03:22 - INFO - root -   Epoch: [219/300][0/84], lr: 0.00000139 	 loss = 0.2842(0.2842)
2023/10/19 11:04:19 - INFO - root -   Epoch: [219/300][20/84], lr: 0.00000139 	 loss = 0.1663(0.3767)
2023/10/19 11:05:34 - INFO - root -   Epoch: [219/300][40/84], lr: 0.00000139 	 loss = 0.0700(0.3945)
2023/10/19 11:06:37 - INFO - root -   Epoch: [219/300][60/84], lr: 0.00000139 	 loss = 0.1074(0.4160)
2023/10/19 11:07:28 - INFO - root -   Epoch: [219/300][80/84], lr: 0.00000139 	 loss = 0.0633(0.4174)
2023/10/19 11:07:29 - INFO - root -   Epoch: [219/300] 	 loss = 0.4337
2023/10/19 11:08:25 - INFO - root -   precision = 0.8837
2023/10/19 11:08:25 - INFO - root -   eval_loss = 0.2486
2023/10/19 11:08:26 - INFO - root -   train_accuracy = 0.7619
2023/10/19 11:08:48 - INFO - root -   Epoch: [220/300][0/84], lr: 0.00000140 	 loss = 0.1087(0.1087)
2023/10/19 11:09:48 - INFO - root -   Epoch: [220/300][20/84], lr: 0.00000140 	 loss = 0.0972(0.3249)
2023/10/19 11:11:15 - INFO - root -   Epoch: [220/300][40/84], lr: 0.00000140 	 loss = 0.0937(0.3192)
2023/10/19 11:12:13 - INFO - root -   Epoch: [220/300][60/84], lr: 0.00000140 	 loss = 0.0210(0.2983)
2023/10/19 11:12:56 - INFO - root -   Epoch: [220/300][80/84], lr: 0.00000140 	 loss = 0.0188(0.3320)
2023/10/19 11:12:57 - INFO - root -   Epoch: [220/300] 	 loss = 0.3324
2023/10/19 11:12:57 - INFO - root -   train_accuracy = 0.8452
2023/10/19 11:13:19 - INFO - root -   Epoch: [221/300][0/84], lr: 0.00000140 	 loss = 0.1374(0.1374)
2023/10/19 11:14:32 - INFO - root -   Epoch: [221/300][20/84], lr: 0.00000140 	 loss = 0.0766(0.3357)
2023/10/19 11:15:27 - INFO - root -   Epoch: [221/300][40/84], lr: 0.00000140 	 loss = 0.0425(0.2946)
2023/10/19 11:16:52 - INFO - root -   Epoch: [221/300][60/84], lr: 0.00000140 	 loss = 0.0035(0.3202)
2023/10/19 11:17:32 - INFO - root -   Epoch: [221/300][80/84], lr: 0.00000140 	 loss = 0.0230(0.3328)
2023/10/19 11:17:33 - INFO - root -   Epoch: [221/300] 	 loss = 0.3259
2023/10/19 11:17:33 - INFO - root -   train_accuracy = 0.8571
2023/10/19 11:18:03 - INFO - root -   Epoch: [222/300][0/84], lr: 0.00000141 	 loss = 0.1896(0.1896)
2023/10/19 11:19:11 - INFO - root -   Epoch: [222/300][20/84], lr: 0.00000141 	 loss = 0.1733(0.1918)
2023/10/19 11:20:02 - INFO - root -   Epoch: [222/300][40/84], lr: 0.00000141 	 loss = 0.0110(0.2221)
2023/10/19 11:21:25 - INFO - root -   Epoch: [222/300][60/84], lr: 0.00000141 	 loss = 1.4167(0.2947)
2023/10/19 11:21:58 - INFO - root -   Epoch: [222/300][80/84], lr: 0.00000141 	 loss = 0.0029(0.2621)
2023/10/19 11:22:01 - INFO - root -   Epoch: [222/300] 	 loss = 0.2612
2023/10/19 11:22:01 - INFO - root -   train_accuracy = 0.9107
2023/10/19 11:22:31 - INFO - root -   Epoch: [223/300][0/84], lr: 0.00000141 	 loss = 0.1526(0.1526)
2023/10/19 11:23:37 - INFO - root -   Epoch: [223/300][20/84], lr: 0.00000141 	 loss = 1.0694(0.3353)
2023/10/19 11:24:37 - INFO - root -   Epoch: [223/300][40/84], lr: 0.00000141 	 loss = 0.0359(0.2615)
2023/10/19 11:25:56 - INFO - root -   Epoch: [223/300][60/84], lr: 0.00000141 	 loss = 0.3003(0.3474)
2023/10/19 11:26:42 - INFO - root -   Epoch: [223/300][80/84], lr: 0.00000141 	 loss = 0.0113(0.4039)
2023/10/19 11:26:43 - INFO - root -   Epoch: [223/300] 	 loss = 0.3991
2023/10/19 11:26:43 - INFO - root -   train_accuracy = 0.8333
2023/10/19 11:27:05 - INFO - root -   Epoch: [224/300][0/84], lr: 0.00000142 	 loss = 0.0039(0.0039)
2023/10/19 11:28:28 - INFO - root -   Epoch: [224/300][20/84], lr: 0.00000142 	 loss = 0.0918(0.3994)
2023/10/19 11:29:45 - INFO - root -   Epoch: [224/300][40/84], lr: 0.00000142 	 loss = 0.0170(0.3719)
2023/10/19 11:30:44 - INFO - root -   Epoch: [224/300][60/84], lr: 0.00000142 	 loss = 0.1636(0.3357)
2023/10/19 11:31:31 - INFO - root -   Epoch: [224/300][80/84], lr: 0.00000142 	 loss = 0.0203(0.3465)
2023/10/19 11:31:37 - INFO - root -   Epoch: [224/300] 	 loss = 0.3431
2023/10/19 11:32:33 - INFO - root -   precision = 0.8837
2023/10/19 11:32:33 - INFO - root -   eval_loss = 0.2479
2023/10/19 11:32:34 - INFO - root -   train_accuracy = 0.8214
2023/10/19 11:32:57 - INFO - root -   Epoch: [225/300][0/84], lr: 0.00000143 	 loss = 0.2950(0.2950)
2023/10/19 11:34:02 - INFO - root -   Epoch: [225/300][20/84], lr: 0.00000143 	 loss = 0.0120(0.1881)
2023/10/19 11:35:06 - INFO - root -   Epoch: [225/300][40/84], lr: 0.00000143 	 loss = 0.3195(0.2943)
2023/10/19 11:36:36 - INFO - root -   Epoch: [225/300][60/84], lr: 0.00000143 	 loss = 0.2179(0.2840)
2023/10/19 11:37:08 - INFO - root -   Epoch: [225/300][80/84], lr: 0.00000143 	 loss = 0.0051(0.3348)
2023/10/19 11:37:09 - INFO - root -   Epoch: [225/300] 	 loss = 0.3597
2023/10/19 11:37:09 - INFO - root -   train_accuracy = 0.8095
2023/10/19 11:37:31 - INFO - root -   Epoch: [226/300][0/84], lr: 0.00000143 	 loss = 0.3326(0.3326)
2023/10/19 11:38:36 - INFO - root -   Epoch: [226/300][20/84], lr: 0.00000143 	 loss = 0.0920(0.3192)
2023/10/19 11:40:01 - INFO - root -   Epoch: [226/300][40/84], lr: 0.00000143 	 loss = 0.0269(0.3207)
2023/10/19 11:40:49 - INFO - root -   Epoch: [226/300][60/84], lr: 0.00000143 	 loss = 0.0773(0.3413)
2023/10/19 11:41:39 - INFO - root -   Epoch: [226/300][80/84], lr: 0.00000143 	 loss = 0.0117(0.3177)
2023/10/19 11:41:43 - INFO - root -   Epoch: [226/300] 	 loss = 0.3161
2023/10/19 11:41:43 - INFO - root -   train_accuracy = 0.8690
2023/10/19 11:42:06 - INFO - root -   Epoch: [227/300][0/84], lr: 0.00000144 	 loss = 0.1088(0.1088)
2023/10/19 11:43:23 - INFO - root -   Epoch: [227/300][20/84], lr: 0.00000144 	 loss = 0.0316(0.2732)
2023/10/19 11:44:07 - INFO - root -   Epoch: [227/300][40/84], lr: 0.00000144 	 loss = 0.0836(0.3587)
2023/10/19 11:45:29 - INFO - root -   Epoch: [227/300][60/84], lr: 0.00000144 	 loss = 0.2162(0.3847)
2023/10/19 11:46:14 - INFO - root -   Epoch: [227/300][80/84], lr: 0.00000144 	 loss = 0.0075(0.4020)
2023/10/19 11:46:17 - INFO - root -   Epoch: [227/300] 	 loss = 0.3990
2023/10/19 11:46:17 - INFO - root -   train_accuracy = 0.7976
2023/10/19 11:46:50 - INFO - root -   Epoch: [228/300][0/84], lr: 0.00000144 	 loss = 0.2156(0.2156)
2023/10/19 11:47:38 - INFO - root -   Epoch: [228/300][20/84], lr: 0.00000144 	 loss = 0.0253(0.2543)
2023/10/19 11:48:48 - INFO - root -   Epoch: [228/300][40/84], lr: 0.00000144 	 loss = 0.0378(0.2423)
2023/10/19 11:49:53 - INFO - root -   Epoch: [228/300][60/84], lr: 0.00000144 	 loss = 0.0541(0.2436)
2023/10/19 11:50:44 - INFO - root -   Epoch: [228/300][80/84], lr: 0.00000144 	 loss = 0.4673(0.2760)
2023/10/19 11:50:45 - INFO - root -   Epoch: [228/300] 	 loss = 0.2783
2023/10/19 11:50:45 - INFO - root -   train_accuracy = 0.8690
2023/10/19 11:51:07 - INFO - root -   Epoch: [229/300][0/84], lr: 0.00000145 	 loss = 1.0694(1.0694)
2023/10/19 11:52:00 - INFO - root -   Epoch: [229/300][20/84], lr: 0.00000145 	 loss = 0.0097(0.3511)
2023/10/19 11:53:10 - INFO - root -   Epoch: [229/300][40/84], lr: 0.00000145 	 loss = 0.1373(0.3236)
2023/10/19 11:54:13 - INFO - root -   Epoch: [229/300][60/84], lr: 0.00000145 	 loss = 0.1329(0.3414)
2023/10/19 11:55:17 - INFO - root -   Epoch: [229/300][80/84], lr: 0.00000145 	 loss = 0.1565(0.3311)
2023/10/19 11:55:19 - INFO - root -   Epoch: [229/300] 	 loss = 0.3378
2023/10/19 11:56:15 - INFO - root -   precision = 0.8837
2023/10/19 11:56:15 - INFO - root -   eval_loss = 0.3042
2023/10/19 11:56:16 - INFO - root -   train_accuracy = 0.8631
2023/10/19 11:56:48 - INFO - root -   Epoch: [230/300][0/84], lr: 0.00000146 	 loss = 0.1384(0.1384)
2023/10/19 11:57:40 - INFO - root -   Epoch: [230/300][20/84], lr: 0.00000146 	 loss = 0.0199(0.3527)
2023/10/19 11:58:49 - INFO - root -   Epoch: [230/300][40/84], lr: 0.00000146 	 loss = 0.0294(0.3176)
2023/10/19 12:00:05 - INFO - root -   Epoch: [230/300][60/84], lr: 0.00000146 	 loss = 0.6029(0.3171)
2023/10/19 12:00:52 - INFO - root -   Epoch: [230/300][80/84], lr: 0.00000146 	 loss = 0.0172(0.3295)
2023/10/19 12:00:53 - INFO - root -   Epoch: [230/300] 	 loss = 0.3359
2023/10/19 12:00:53 - INFO - root -   train_accuracy = 0.8452
2023/10/19 12:01:15 - INFO - root -   Epoch: [231/300][0/84], lr: 0.00000146 	 loss = 0.0885(0.0885)
2023/10/19 12:02:31 - INFO - root -   Epoch: [231/300][20/84], lr: 0.00000146 	 loss = 0.0524(0.3655)
2023/10/19 12:03:24 - INFO - root -   Epoch: [231/300][40/84], lr: 0.00000146 	 loss = 0.0304(0.2637)
2023/10/19 12:04:57 - INFO - root -   Epoch: [231/300][60/84], lr: 0.00000146 	 loss = 0.0097(0.2756)
2023/10/19 12:05:35 - INFO - root -   Epoch: [231/300][80/84], lr: 0.00000146 	 loss = 0.2605(0.2923)
2023/10/19 12:05:37 - INFO - root -   Epoch: [231/300] 	 loss = 0.3074
2023/10/19 12:05:37 - INFO - root -   train_accuracy = 0.8393
2023/10/19 12:06:07 - INFO - root -   Epoch: [232/300][0/84], lr: 0.00000147 	 loss = 0.4962(0.4962)
2023/10/19 12:07:12 - INFO - root -   Epoch: [232/300][20/84], lr: 0.00000147 	 loss = 1.7208(0.4222)
2023/10/19 12:08:02 - INFO - root -   Epoch: [232/300][40/84], lr: 0.00000147 	 loss = 0.0082(0.3910)
2023/10/19 12:09:26 - INFO - root -   Epoch: [232/300][60/84], lr: 0.00000147 	 loss = 0.0715(0.4318)
2023/10/19 12:10:18 - INFO - root -   Epoch: [232/300][80/84], lr: 0.00000147 	 loss = 0.0502(0.3899)
2023/10/19 12:10:19 - INFO - root -   Epoch: [232/300] 	 loss = 0.3985
2023/10/19 12:10:19 - INFO - root -   train_accuracy = 0.8095
2023/10/19 12:10:41 - INFO - root -   Epoch: [233/300][0/84], lr: 0.00000147 	 loss = 0.1303(0.1303)
2023/10/19 12:11:47 - INFO - root -   Epoch: [233/300][20/84], lr: 0.00000147 	 loss = 0.1649(0.3447)
2023/10/19 12:13:03 - INFO - root -   Epoch: [233/300][40/84], lr: 0.00000147 	 loss = 0.5481(0.3845)
2023/10/19 12:14:09 - INFO - root -   Epoch: [233/300][60/84], lr: 0.00000147 	 loss = 0.0365(0.3479)
2023/10/19 12:14:55 - INFO - root -   Epoch: [233/300][80/84], lr: 0.00000147 	 loss = 0.1310(0.3200)
2023/10/19 12:14:58 - INFO - root -   Epoch: [233/300] 	 loss = 0.3186
2023/10/19 12:14:58 - INFO - root -   train_accuracy = 0.8274
2023/10/19 12:15:29 - INFO - root -   Epoch: [234/300][0/84], lr: 0.00000148 	 loss = 0.1085(0.1085)
2023/10/19 12:16:34 - INFO - root -   Epoch: [234/300][20/84], lr: 0.00000148 	 loss = 0.6375(0.4009)
2023/10/19 12:17:40 - INFO - root -   Epoch: [234/300][40/84], lr: 0.00000148 	 loss = 0.0216(0.3831)
2023/10/19 12:18:48 - INFO - root -   Epoch: [234/300][60/84], lr: 0.00000148 	 loss = 0.0742(0.3911)
2023/10/19 12:19:39 - INFO - root -   Epoch: [234/300][80/84], lr: 0.00000148 	 loss = 0.0088(0.3971)
2023/10/19 12:19:41 - INFO - root -   Epoch: [234/300] 	 loss = 0.3953
2023/10/19 12:20:37 - INFO - root -   precision = 0.8605
2023/10/19 12:20:37 - INFO - root -   eval_loss = 0.2578
2023/10/19 12:20:38 - INFO - root -   train_accuracy = 0.8214
2023/10/19 12:21:01 - INFO - root -   Epoch: [235/300][0/84], lr: 0.00000148 	 loss = 0.0201(0.0201)
2023/10/19 12:22:15 - INFO - root -   Epoch: [235/300][20/84], lr: 0.00000148 	 loss = 0.0310(0.2252)
2023/10/19 12:23:16 - INFO - root -   Epoch: [235/300][40/84], lr: 0.00000148 	 loss = 0.2890(0.3457)
2023/10/19 12:24:24 - INFO - root -   Epoch: [235/300][60/84], lr: 0.00000148 	 loss = 0.2393(0.3589)
2023/10/19 12:25:23 - INFO - root -   Epoch: [235/300][80/84], lr: 0.00000148 	 loss = 0.0941(0.4004)
2023/10/19 12:25:27 - INFO - root -   Epoch: [235/300] 	 loss = 0.3924
2023/10/19 12:25:27 - INFO - root -   train_accuracy = 0.8155
2023/10/19 12:25:56 - INFO - root -   Epoch: [236/300][0/84], lr: 0.00000149 	 loss = 0.4259(0.4259)
2023/10/19 12:27:04 - INFO - root -   Epoch: [236/300][20/84], lr: 0.00000149 	 loss = 0.0452(0.2268)
2023/10/19 12:28:27 - INFO - root -   Epoch: [236/300][40/84], lr: 0.00000149 	 loss = 0.0152(0.2472)
2023/10/19 12:29:26 - INFO - root -   Epoch: [236/300][60/84], lr: 0.00000149 	 loss = 0.3231(0.2882)
2023/10/19 12:30:12 - INFO - root -   Epoch: [236/300][80/84], lr: 0.00000149 	 loss = 0.1861(0.3235)
2023/10/19 12:30:13 - INFO - root -   Epoch: [236/300] 	 loss = 0.3347
2023/10/19 12:30:13 - INFO - root -   train_accuracy = 0.8274
2023/10/19 12:30:53 - INFO - root -   Epoch: [237/300][0/84], lr: 0.00000150 	 loss = 0.3840(0.3840)
2023/10/19 12:31:51 - INFO - root -   Epoch: [237/300][20/84], lr: 0.00000150 	 loss = 0.2546(0.3716)
2023/10/19 12:32:58 - INFO - root -   Epoch: [237/300][40/84], lr: 0.00000150 	 loss = 0.0549(0.3414)
2023/10/19 12:34:06 - INFO - root -   Epoch: [237/300][60/84], lr: 0.00000150 	 loss = 0.5195(0.3104)
2023/10/19 12:34:55 - INFO - root -   Epoch: [237/300][80/84], lr: 0.00000150 	 loss = 0.0060(0.2936)
2023/10/19 12:34:57 - INFO - root -   Epoch: [237/300] 	 loss = 0.3012
2023/10/19 12:34:57 - INFO - root -   train_accuracy = 0.8274
2023/10/19 12:35:27 - INFO - root -   Epoch: [238/300][0/84], lr: 0.00000150 	 loss = 0.5355(0.5355)
2023/10/19 12:36:24 - INFO - root -   Epoch: [238/300][20/84], lr: 0.00000150 	 loss = 0.0031(0.2492)
2023/10/19 12:37:43 - INFO - root -   Epoch: [238/300][40/84], lr: 0.00000150 	 loss = 0.2538(0.2535)
2023/10/19 12:38:53 - INFO - root -   Epoch: [238/300][60/84], lr: 0.00000150 	 loss = 0.0829(0.3004)
2023/10/19 12:39:39 - INFO - root -   Epoch: [238/300][80/84], lr: 0.00000150 	 loss = 0.0047(0.3306)
2023/10/19 12:39:40 - INFO - root -   Epoch: [238/300] 	 loss = 0.3281
2023/10/19 12:39:40 - INFO - root -   train_accuracy = 0.8393
2023/10/19 12:40:02 - INFO - root -   Epoch: [239/300][0/84], lr: 0.00000151 	 loss = 0.0156(0.0156)
2023/10/19 12:41:13 - INFO - root -   Epoch: [239/300][20/84], lr: 0.00000151 	 loss = 0.0172(0.2873)
2023/10/19 12:42:06 - INFO - root -   Epoch: [239/300][40/84], lr: 0.00000151 	 loss = 0.0052(0.2958)
2023/10/19 12:43:11 - INFO - root -   Epoch: [239/300][60/84], lr: 0.00000151 	 loss = 0.1565(0.3127)
2023/10/19 12:44:01 - INFO - root -   Epoch: [239/300][80/84], lr: 0.00000151 	 loss = 0.2675(0.3155)
2023/10/19 12:44:07 - INFO - root -   Epoch: [239/300] 	 loss = 0.3240
2023/10/19 12:45:04 - INFO - root -   precision = 0.8837
2023/10/19 12:45:04 - INFO - root -   eval_loss = 0.2464
2023/10/19 12:45:05 - INFO - root -   train_accuracy = 0.8512
2023/10/19 12:45:43 - INFO - root -   Epoch: [240/300][0/84], lr: 0.00000151 	 loss = 0.4692(0.4692)
2023/10/19 12:46:43 - INFO - root -   Epoch: [240/300][20/84], lr: 0.00000151 	 loss = 0.0140(0.2817)
2023/10/19 12:47:50 - INFO - root -   Epoch: [240/300][40/84], lr: 0.00000151 	 loss = 0.2086(0.2191)
2023/10/19 12:48:51 - INFO - root -   Epoch: [240/300][60/84], lr: 0.00000151 	 loss = 0.0309(0.2293)
2023/10/19 12:49:42 - INFO - root -   Epoch: [240/300][80/84], lr: 0.00000151 	 loss = 0.0012(0.3033)
2023/10/19 12:49:45 - INFO - root -   Epoch: [240/300] 	 loss = 0.3134
2023/10/19 12:49:45 - INFO - root -   train_accuracy = 0.8452
2023/10/19 12:50:06 - INFO - root -   Epoch: [241/300][0/84], lr: 0.00000152 	 loss = 0.0046(0.0046)
2023/10/19 12:51:12 - INFO - root -   Epoch: [241/300][20/84], lr: 0.00000152 	 loss = 0.1963(0.2100)
2023/10/19 12:52:19 - INFO - root -   Epoch: [241/300][40/84], lr: 0.00000152 	 loss = 0.0149(0.2715)
2023/10/19 12:53:35 - INFO - root -   Epoch: [241/300][60/84], lr: 0.00000152 	 loss = 0.7569(0.2928)
2023/10/19 12:54:23 - INFO - root -   Epoch: [241/300][80/84], lr: 0.00000152 	 loss = 0.1753(0.3082)
2023/10/19 12:54:24 - INFO - root -   Epoch: [241/300] 	 loss = 0.3194
2023/10/19 12:54:24 - INFO - root -   train_accuracy = 0.8512
2023/10/19 12:54:46 - INFO - root -   Epoch: [242/300][0/84], lr: 0.00000153 	 loss = 0.0760(0.0760)
2023/10/19 12:55:52 - INFO - root -   Epoch: [242/300][20/84], lr: 0.00000153 	 loss = 0.1569(0.1751)
2023/10/19 12:57:01 - INFO - root -   Epoch: [242/300][40/84], lr: 0.00000153 	 loss = 0.2457(0.2113)
2023/10/19 12:58:16 - INFO - root -   Epoch: [242/300][60/84], lr: 0.00000153 	 loss = 1.0241(0.3004)
2023/10/19 12:59:08 - INFO - root -   Epoch: [242/300][80/84], lr: 0.00000153 	 loss = 0.0042(0.3446)
2023/10/19 12:59:43 - INFO - root -   Epoch: [242/300] 	 loss = 0.3493
2023/10/19 12:59:43 - INFO - root -   train_accuracy = 0.8393
2023/10/19 13:00:25 - INFO - root -   Epoch: [243/300][0/84], lr: 0.00000153 	 loss = 0.4633(0.4633)
2023/10/19 13:01:26 - INFO - root -   Epoch: [243/300][20/84], lr: 0.00000153 	 loss = 0.4113(0.3209)
2023/10/19 13:02:31 - INFO - root -   Epoch: [243/300][40/84], lr: 0.00000153 	 loss = 0.0212(0.3426)
