2023/09/27 14:05:54 - INFO - root -   Num train examples = 693
2023/09/27 14:05:54 - INFO - root -   Num val examples = 174
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/09/27 14:05:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/09/27 14:05:54 - INFO - root -   backend = nccl
2023/09/27 14:05:54 - INFO - root -   batch_size = 2
2023/09/27 14:05:54 - INFO - root -   dropout = 0.5
2023/09/27 14:05:54 - INFO - root -   epochs = 200
2023/09/27 14:05:54 - INFO - root -   eval_freq = 5
2023/09/27 14:05:54 - INFO - root -   focal_loss = False
2023/09/27 14:05:54 - INFO - root -   input_size = 224
2023/09/27 14:05:54 - INFO - root -   is_pretrained = False
2023/09/27 14:05:54 - INFO - root -   label_smooth = False
2023/09/27 14:05:54 - INFO - root -   local_rank = -1
2023/09/27 14:05:54 - INFO - root -   lr = 1e-05
2023/09/27 14:05:54 - INFO - root -   lr_decay_rate = 0.1
2023/09/27 14:05:54 - INFO - root -   lr_steps = [50, 100]
2023/09/27 14:05:54 - INFO - root -   lr_type = cosine
2023/09/27 14:05:54 - INFO - root -   model_depth = 34
2023/09/27 14:05:54 - INFO - root -   model_name = resnet50
2023/09/27 14:05:54 - INFO - root -   momentum = 0.9
2023/09/27 14:05:54 - INFO - root -   num_classes = 2
2023/09/27 14:05:54 - INFO - root -   output = ./all_roi_expand_aug2_cls2_outputs
2023/09/27 14:05:54 - INFO - root -   print_freq = 20
2023/09/27 14:05:54 - INFO - root -   resume = 
2023/09/27 14:05:54 - INFO - root -   start_epoch = 0
2023/09/27 14:05:54 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/09/27 14:05:54 - INFO - root -   tune_from = 
2023/09/27 14:05:54 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/09/27 14:05:54 - INFO - root -   warmup_epoch = 20
2023/09/27 14:05:54 - INFO - root -   warmup_multiplier = 100
2023/09/27 14:05:54 - INFO - root -   weight_decay = 0.0005
2023/09/27 14:05:54 - INFO - root -   workers = 8
2023/09/27 14:06:33 - INFO - root -   Epoch: [0/200][0/346], lr: 0.00000010 	 loss = 0.9691(0.9691)
2023/09/27 14:07:22 - INFO - root -   Epoch: [0/200][20/346], lr: 0.00000010 	 loss = 1.5881(0.6872)
2023/09/27 14:08:42 - INFO - root -   Epoch: [0/200][40/346], lr: 0.00000010 	 loss = 0.9063(0.7222)
2023/09/27 14:09:41 - INFO - root -   Epoch: [0/200][60/346], lr: 0.00000010 	 loss = 0.7525(0.7866)
2023/09/27 14:10:58 - INFO - root -   Epoch: [0/200][80/346], lr: 0.00000010 	 loss = 0.7327(0.8099)
2023/09/27 14:11:48 - INFO - root -   Epoch: [0/200][100/346], lr: 0.00000010 	 loss = 0.7814(0.8099)
2023/09/27 14:12:58 - INFO - root -   Epoch: [0/200][120/346], lr: 0.00000010 	 loss = 0.4504(0.8035)
2023/09/27 14:13:56 - INFO - root -   Epoch: [0/200][140/346], lr: 0.00000010 	 loss = 0.7880(0.8018)
2023/09/27 14:15:11 - INFO - root -   Epoch: [0/200][160/346], lr: 0.00000010 	 loss = 1.0878(0.8074)
2023/09/27 14:16:39 - INFO - root -   Epoch: [0/200][180/346], lr: 0.00000010 	 loss = 1.3546(0.7985)
2023/09/27 14:17:43 - INFO - root -   Epoch: [0/200][200/346], lr: 0.00000010 	 loss = 0.5121(0.7942)
2023/09/27 14:18:57 - INFO - root -   Epoch: [0/200][220/346], lr: 0.00000010 	 loss = 0.7155(0.8022)
2023/09/27 14:19:38 - INFO - root -   Epoch: [0/200][240/346], lr: 0.00000010 	 loss = 0.2466(0.7954)
2023/09/27 14:20:53 - INFO - root -   Epoch: [0/200][260/346], lr: 0.00000010 	 loss = 0.5613(0.8037)
2023/09/27 14:21:38 - INFO - root -   Epoch: [0/200][280/346], lr: 0.00000010 	 loss = 0.8987(0.7956)
2023/09/27 14:23:03 - INFO - root -   Epoch: [0/200][300/346], lr: 0.00000010 	 loss = 0.5915(0.7928)
2023/09/27 14:23:58 - INFO - root -   Epoch: [0/200][320/346], lr: 0.00000010 	 loss = 0.6112(0.7916)
2023/09/27 14:25:06 - INFO - root -   Epoch: [0/200][340/346], lr: 0.00000010 	 loss = 1.4463(0.7858)
2023/09/27 14:25:08 - INFO - root -   Epoch: [0/200] 	 loss = 0.7873
2023/09/27 14:25:08 - INFO - root -   train_accuracy = 0.5000
2023/09/27 14:25:38 - INFO - root -   Epoch: [1/200][0/346], lr: 0.00000010 	 loss = 0.7846(0.7846)
2023/09/27 14:26:46 - INFO - root -   Epoch: [1/200][20/346], lr: 0.00000010 	 loss = 1.1198(0.8505)
2023/09/27 14:27:50 - INFO - root -   Epoch: [1/200][40/346], lr: 0.00000010 	 loss = 0.7903(0.8232)
2023/09/27 14:29:05 - INFO - root -   Epoch: [1/200][60/346], lr: 0.00000010 	 loss = 0.9547(0.8254)
2023/09/27 14:30:00 - INFO - root -   Epoch: [1/200][80/346], lr: 0.00000010 	 loss = 1.0026(0.8068)
2023/09/27 14:31:09 - INFO - root -   Epoch: [1/200][100/346], lr: 0.00000010 	 loss = 0.4733(0.7886)
2023/09/27 14:32:15 - INFO - root -   Epoch: [1/200][120/346], lr: 0.00000010 	 loss = 0.9226(0.7742)
2023/09/27 14:33:19 - INFO - root -   Epoch: [1/200][140/346], lr: 0.00000010 	 loss = 1.3268(0.7696)
2023/09/27 14:34:15 - INFO - root -   Epoch: [1/200][160/346], lr: 0.00000010 	 loss = 0.5528(0.7612)
2023/09/27 14:35:47 - INFO - root -   Epoch: [1/200][180/346], lr: 0.00000010 	 loss = 0.6050(0.7631)
2023/09/27 14:36:36 - INFO - root -   Epoch: [1/200][200/346], lr: 0.00000010 	 loss = 0.6749(0.7680)
2023/09/27 14:37:44 - INFO - root -   Epoch: [1/200][220/346], lr: 0.00000010 	 loss = 0.7754(0.7732)
2023/09/27 14:38:34 - INFO - root -   Epoch: [1/200][240/346], lr: 0.00000010 	 loss = 0.9626(0.7763)
2023/09/27 14:40:00 - INFO - root -   Epoch: [1/200][260/346], lr: 0.00000010 	 loss = 0.7628(0.7701)
2023/09/27 14:40:58 - INFO - root -   Epoch: [1/200][280/346], lr: 0.00000010 	 loss = 0.5138(0.7696)
2023/09/27 14:42:21 - INFO - root -   Epoch: [1/200][300/346], lr: 0.00000010 	 loss = 0.2236(0.7585)
2023/09/27 14:43:21 - INFO - root -   Epoch: [1/200][320/346], lr: 0.00000010 	 loss = 0.3815(0.7590)
2023/09/27 14:44:28 - INFO - root -   Epoch: [1/200][340/346], lr: 0.00000010 	 loss = 1.1320(0.7580)
2023/09/27 14:44:30 - INFO - root -   Epoch: [1/200] 	 loss = 0.7573
2023/09/27 14:44:30 - INFO - root -   train_accuracy = 0.5014
2023/09/27 14:44:52 - INFO - root -   Epoch: [2/200][0/346], lr: 0.00000010 	 loss = 0.9267(0.9267)
2023/09/27 14:45:59 - INFO - root -   Epoch: [2/200][20/346], lr: 0.00000010 	 loss = 0.9573(0.7743)
2023/09/27 14:46:58 - INFO - root -   Epoch: [2/200][40/346], lr: 0.00000010 	 loss = 0.6614(0.7616)
2023/09/27 14:48:17 - INFO - root -   Epoch: [2/200][60/346], lr: 0.00000010 	 loss = 0.5507(0.7676)
2023/09/27 14:49:31 - INFO - root -   Epoch: [2/200][80/346], lr: 0.00000010 	 loss = 0.4918(0.7702)
2023/09/27 14:50:40 - INFO - root -   Epoch: [2/200][100/346], lr: 0.00000010 	 loss = 0.4493(0.7987)
2023/09/27 14:51:36 - INFO - root -   Epoch: [2/200][120/346], lr: 0.00000010 	 loss = 1.2007(0.8075)
2023/09/27 14:52:47 - INFO - root -   Epoch: [2/200][140/346], lr: 0.00000010 	 loss = 0.4350(0.7932)
2023/09/27 14:53:45 - INFO - root -   Epoch: [2/200][160/346], lr: 0.00000010 	 loss = 0.9985(0.7818)
2023/09/27 14:55:16 - INFO - root -   Epoch: [2/200][180/346], lr: 0.00000010 	 loss = 0.5496(0.7700)
2023/09/27 14:56:12 - INFO - root -   Epoch: [2/200][200/346], lr: 0.00000010 	 loss = 1.0150(0.7630)
2023/09/27 14:57:27 - INFO - root -   Epoch: [2/200][220/346], lr: 0.00000010 	 loss = 0.4960(0.7626)
2023/09/27 14:58:16 - INFO - root -   Epoch: [2/200][240/346], lr: 0.00000010 	 loss = 0.6375(0.7616)
2023/09/27 14:59:30 - INFO - root -   Epoch: [2/200][260/346], lr: 0.00000010 	 loss = 0.5540(0.7725)
2023/09/27 15:00:27 - INFO - root -   Epoch: [2/200][280/346], lr: 0.00000010 	 loss = 0.5747(0.7678)
2023/09/27 15:01:50 - INFO - root -   Epoch: [2/200][300/346], lr: 0.00000010 	 loss = 0.8452(0.7653)
2023/09/27 15:02:44 - INFO - root -   Epoch: [2/200][320/346], lr: 0.00000010 	 loss = 0.7229(0.7605)
2023/09/27 15:03:50 - INFO - root -   Epoch: [2/200][340/346], lr: 0.00000010 	 loss = 0.5661(0.7609)
2023/09/27 15:03:52 - INFO - root -   Epoch: [2/200] 	 loss = 0.7574
2023/09/27 15:03:52 - INFO - root -   train_accuracy = 0.5289
2023/09/27 15:04:24 - INFO - root -   Epoch: [3/200][0/346], lr: 0.00000010 	 loss = 1.3705(1.3705)
2023/09/27 15:05:32 - INFO - root -   Epoch: [3/200][20/346], lr: 0.00000010 	 loss = 1.1837(0.8725)
2023/09/27 15:06:32 - INFO - root -   Epoch: [3/200][40/346], lr: 0.00000010 	 loss = 0.3780(0.7646)
2023/09/27 15:07:50 - INFO - root -   Epoch: [3/200][60/346], lr: 0.00000010 	 loss = 1.2003(0.8265)
2023/09/27 15:09:00 - INFO - root -   Epoch: [3/200][80/346], lr: 0.00000010 	 loss = 0.4252(0.8101)
2023/09/27 15:10:05 - INFO - root -   Epoch: [3/200][100/346], lr: 0.00000010 	 loss = 1.0061(0.8065)
2023/09/27 15:11:08 - INFO - root -   Epoch: [3/200][120/346], lr: 0.00000010 	 loss = 0.9431(0.7737)
2023/09/27 15:12:25 - INFO - root -   Epoch: [3/200][140/346], lr: 0.00000010 	 loss = 0.7776(0.7601)
2023/09/27 15:13:36 - INFO - root -   Epoch: [3/200][160/346], lr: 0.00000010 	 loss = 0.3911(0.7522)
2023/09/27 15:14:35 - INFO - root -   Epoch: [3/200][180/346], lr: 0.00000010 	 loss = 0.6077(0.7475)
2023/09/27 15:15:37 - INFO - root -   Epoch: [3/200][200/346], lr: 0.00000010 	 loss = 0.5439(0.7439)
2023/09/27 15:16:46 - INFO - root -   Epoch: [3/200][220/346], lr: 0.00000010 	 loss = 0.4267(0.7436)
2023/09/27 15:18:00 - INFO - root -   Epoch: [3/200][240/346], lr: 0.00000010 	 loss = 0.7298(0.7476)
2023/09/27 15:19:11 - INFO - root -   Epoch: [3/200][260/346], lr: 0.00000010 	 loss = 0.8368(0.7542)
2023/09/27 15:20:21 - INFO - root -   Epoch: [3/200][280/346], lr: 0.00000010 	 loss = 0.6654(0.7556)
2023/09/27 15:21:18 - INFO - root -   Epoch: [3/200][300/346], lr: 0.00000010 	 loss = 0.7582(0.7582)
2023/09/27 15:22:22 - INFO - root -   Epoch: [3/200][320/346], lr: 0.00000010 	 loss = 0.7988(0.7585)
2023/09/27 15:23:26 - INFO - root -   Epoch: [3/200][340/346], lr: 0.00000010 	 loss = 0.9546(0.7592)
2023/09/27 15:23:31 - INFO - root -   Epoch: [3/200] 	 loss = 0.7602
2023/09/27 15:23:31 - INFO - root -   train_accuracy = 0.4986
2023/09/27 15:23:53 - INFO - root -   Epoch: [4/200][0/346], lr: 0.00000011 	 loss = 0.4504(0.4504)
2023/09/27 15:25:17 - INFO - root -   Epoch: [4/200][20/346], lr: 0.00000011 	 loss = 0.5611(0.6780)
2023/09/27 15:26:21 - INFO - root -   Epoch: [4/200][40/346], lr: 0.00000011 	 loss = 0.5141(0.7500)
2023/09/27 15:27:40 - INFO - root -   Epoch: [4/200][60/346], lr: 0.00000011 	 loss = 0.6602(0.7889)
2023/09/27 15:28:30 - INFO - root -   Epoch: [4/200][80/346], lr: 0.00000011 	 loss = 0.7213(0.7993)
2023/09/27 15:29:39 - INFO - root -   Epoch: [4/200][100/346], lr: 0.00000011 	 loss = 0.6592(0.7986)
2023/09/27 15:30:46 - INFO - root -   Epoch: [4/200][120/346], lr: 0.00000011 	 loss = 0.3310(0.7770)
2023/09/27 15:32:00 - INFO - root -   Epoch: [4/200][140/346], lr: 0.00000011 	 loss = 0.9808(0.7773)
2023/09/27 15:33:19 - INFO - root -   Epoch: [4/200][160/346], lr: 0.00000011 	 loss = 0.8948(0.7763)
2023/09/27 15:34:24 - INFO - root -   Epoch: [4/200][180/346], lr: 0.00000011 	 loss = 0.9996(0.7722)
2023/09/27 15:35:27 - INFO - root -   Epoch: [4/200][200/346], lr: 0.00000011 	 loss = 0.3519(0.7651)
2023/09/27 15:36:41 - INFO - root -   Epoch: [4/200][220/346], lr: 0.00000011 	 loss = 0.6198(0.7580)
2023/09/27 15:37:41 - INFO - root -   Epoch: [4/200][240/346], lr: 0.00000011 	 loss = 0.4723(0.7582)
2023/09/27 15:39:02 - INFO - root -   Epoch: [4/200][260/346], lr: 0.00000011 	 loss = 0.5757(0.7626)
2023/09/27 15:39:52 - INFO - root -   Epoch: [4/200][280/346], lr: 0.00000011 	 loss = 0.8574(0.7575)
2023/09/27 15:40:59 - INFO - root -   Epoch: [4/200][300/346], lr: 0.00000011 	 loss = 0.4827(0.7595)
2023/09/27 15:41:54 - INFO - root -   Epoch: [4/200][320/346], lr: 0.00000011 	 loss = 1.1337(0.7574)
2023/09/27 15:42:55 - INFO - root -   Epoch: [4/200][340/346], lr: 0.00000011 	 loss = 0.6123(0.7551)
2023/09/27 15:42:58 - INFO - root -   Epoch: [4/200] 	 loss = 0.7529
2023/09/27 15:46:48 - INFO - root -   precision = 0.5747
2023/09/27 15:46:48 - INFO - root -   eval_loss = 0.6708
2023/09/27 15:46:49 - INFO - root -   train_accuracy = 0.5289
2023/09/27 15:47:10 - INFO - root -   Epoch: [5/200][0/346], lr: 0.00000011 	 loss = 0.4029(0.4029)
2023/09/27 15:48:27 - INFO - root -   Epoch: [5/200][20/346], lr: 0.00000011 	 loss = 0.7174(0.7011)
2023/09/27 15:49:37 - INFO - root -   Epoch: [5/200][40/346], lr: 0.00000011 	 loss = 0.7479(0.6934)
2023/09/27 15:50:39 - INFO - root -   Epoch: [5/200][60/346], lr: 0.00000011 	 loss = 0.8020(0.7411)
2023/09/27 15:51:46 - INFO - root -   Epoch: [5/200][80/346], lr: 0.00000011 	 loss = 1.1330(0.7459)
2023/09/27 15:52:51 - INFO - root -   Epoch: [5/200][100/346], lr: 0.00000011 	 loss = 1.3455(0.7540)
2023/09/27 15:53:47 - INFO - root -   Epoch: [5/200][120/346], lr: 0.00000011 	 loss = 0.2303(0.7466)
2023/09/27 15:55:15 - INFO - root -   Epoch: [5/200][140/346], lr: 0.00000011 	 loss = 0.4933(0.7236)
2023/09/27 15:56:19 - INFO - root -   Epoch: [5/200][160/346], lr: 0.00000011 	 loss = 0.9658(0.7223)
2023/09/27 15:57:40 - INFO - root -   Epoch: [5/200][180/346], lr: 0.00000011 	 loss = 0.4409(0.7082)
2023/09/27 15:58:36 - INFO - root -   Epoch: [5/200][200/346], lr: 0.00000011 	 loss = 0.4368(0.7142)
2023/09/27 15:59:41 - INFO - root -   Epoch: [5/200][220/346], lr: 0.00000011 	 loss = 1.2109(0.7142)
2023/09/27 16:00:30 - INFO - root -   Epoch: [5/200][240/346], lr: 0.00000011 	 loss = 0.5518(0.7260)
2023/09/27 16:01:51 - INFO - root -   Epoch: [5/200][260/346], lr: 0.00000011 	 loss = 0.5581(0.7318)
2023/09/27 16:02:46 - INFO - root -   Epoch: [5/200][280/346], lr: 0.00000011 	 loss = 0.8389(0.7352)
2023/09/27 16:03:47 - INFO - root -   Epoch: [5/200][300/346], lr: 0.00000011 	 loss = 0.7628(0.7311)
2023/09/27 16:04:52 - INFO - root -   Epoch: [5/200][320/346], lr: 0.00000011 	 loss = 0.4005(0.7331)
2023/09/27 16:05:51 - INFO - root -   Epoch: [5/200][340/346], lr: 0.00000011 	 loss = 0.5730(0.7331)
2023/09/27 16:05:55 - INFO - root -   Epoch: [5/200] 	 loss = 0.7330
2023/09/27 16:05:55 - INFO - root -   train_accuracy = 0.5506
2023/09/27 16:06:26 - INFO - root -   Epoch: [6/200][0/346], lr: 0.00000011 	 loss = 0.9783(0.9783)
2023/09/27 16:07:43 - INFO - root -   Epoch: [6/200][20/346], lr: 0.00000011 	 loss = 0.9487(0.8187)
2023/09/27 16:08:44 - INFO - root -   Epoch: [6/200][40/346], lr: 0.00000011 	 loss = 0.6459(0.8242)
2023/09/27 16:10:01 - INFO - root -   Epoch: [6/200][60/346], lr: 0.00000011 	 loss = 0.4273(0.8142)
2023/09/27 16:11:02 - INFO - root -   Epoch: [6/200][80/346], lr: 0.00000011 	 loss = 0.3398(0.7925)
2023/09/27 16:12:17 - INFO - root -   Epoch: [6/200][100/346], lr: 0.00000011 	 loss = 0.6340(0.7777)
2023/09/27 16:13:30 - INFO - root -   Epoch: [6/200][120/346], lr: 0.00000011 	 loss = 1.5636(0.7710)
2023/09/27 16:14:39 - INFO - root -   Epoch: [6/200][140/346], lr: 0.00000011 	 loss = 0.8899(0.7806)
2023/09/27 16:15:47 - INFO - root -   Epoch: [6/200][160/346], lr: 0.00000011 	 loss = 0.4950(0.7772)
2023/09/27 16:17:01 - INFO - root -   Epoch: [6/200][180/346], lr: 0.00000011 	 loss = 0.4053(0.7727)
2023/09/27 16:17:58 - INFO - root -   Epoch: [6/200][200/346], lr: 0.00000011 	 loss = 0.3780(0.7623)
2023/09/27 16:19:12 - INFO - root -   Epoch: [6/200][220/346], lr: 0.00000011 	 loss = 0.3043(0.7551)
2023/09/27 16:20:05 - INFO - root -   Epoch: [6/200][240/346], lr: 0.00000011 	 loss = 1.1502(0.7681)
2023/09/27 16:21:38 - INFO - root -   Epoch: [6/200][260/346], lr: 0.00000011 	 loss = 0.7851(0.7711)
2023/09/27 16:22:26 - INFO - root -   Epoch: [6/200][280/346], lr: 0.00000011 	 loss = 0.6453(0.7690)
2023/09/27 16:23:49 - INFO - root -   Epoch: [6/200][300/346], lr: 0.00000011 	 loss = 0.2694(0.7750)
2023/09/27 16:24:48 - INFO - root -   Epoch: [6/200][320/346], lr: 0.00000011 	 loss = 1.0129(0.7745)
2023/09/27 16:25:54 - INFO - root -   Epoch: [6/200][340/346], lr: 0.00000011 	 loss = 0.8648(0.7739)
2023/09/27 16:25:56 - INFO - root -   Epoch: [6/200] 	 loss = 0.7711
2023/09/27 16:25:56 - INFO - root -   train_accuracy = 0.4942
2023/09/27 16:26:19 - INFO - root -   Epoch: [7/200][0/346], lr: 0.00000011 	 loss = 0.9576(0.9576)
2023/09/27 16:27:43 - INFO - root -   Epoch: [7/200][20/346], lr: 0.00000011 	 loss = 0.7692(0.7011)
2023/09/27 16:28:40 - INFO - root -   Epoch: [7/200][40/346], lr: 0.00000011 	 loss = 0.3595(0.7268)
2023/09/27 16:30:00 - INFO - root -   Epoch: [7/200][60/346], lr: 0.00000011 	 loss = 0.7841(0.7462)
2023/09/27 16:30:56 - INFO - root -   Epoch: [7/200][80/346], lr: 0.00000011 	 loss = 0.8799(0.7493)
2023/09/27 16:32:19 - INFO - root -   Epoch: [7/200][100/346], lr: 0.00000011 	 loss = 0.6244(0.7553)
2023/09/27 16:33:10 - INFO - root -   Epoch: [7/200][120/346], lr: 0.00000011 	 loss = 1.1492(0.7665)
2023/09/27 16:34:33 - INFO - root -   Epoch: [7/200][140/346], lr: 0.00000011 	 loss = 0.6603(0.7585)
2023/09/27 16:35:23 - INFO - root -   Epoch: [7/200][160/346], lr: 0.00000011 	 loss = 0.6583(0.7575)
2023/09/27 16:36:39 - INFO - root -   Epoch: [7/200][180/346], lr: 0.00000011 	 loss = 1.3507(0.7630)
2023/09/27 16:37:40 - INFO - root -   Epoch: [7/200][200/346], lr: 0.00000011 	 loss = 0.5413(0.7516)
2023/09/27 16:38:58 - INFO - root -   Epoch: [7/200][220/346], lr: 0.00000011 	 loss = 0.7837(0.7627)
2023/09/27 16:39:40 - INFO - root -   Epoch: [7/200][240/346], lr: 0.00000011 	 loss = 0.5481(0.7660)
2023/09/27 16:41:06 - INFO - root -   Epoch: [7/200][260/346], lr: 0.00000011 	 loss = 1.8214(0.7685)
2023/09/27 16:41:57 - INFO - root -   Epoch: [7/200][280/346], lr: 0.00000011 	 loss = 0.9677(0.7644)
2023/09/27 16:43:18 - INFO - root -   Epoch: [7/200][300/346], lr: 0.00000011 	 loss = 0.6732(0.7589)
2023/09/27 16:44:11 - INFO - root -   Epoch: [7/200][320/346], lr: 0.00000011 	 loss = 0.8263(0.7629)
2023/09/27 16:45:14 - INFO - root -   Epoch: [7/200][340/346], lr: 0.00000011 	 loss = 0.6234(0.7600)
2023/09/27 16:45:22 - INFO - root -   Epoch: [7/200] 	 loss = 0.7635
2023/09/27 16:45:22 - INFO - root -   train_accuracy = 0.5072
2023/09/27 16:45:44 - INFO - root -   Epoch: [8/200][0/346], lr: 0.00000011 	 loss = 0.5280(0.5280)
2023/09/27 16:46:41 - INFO - root -   Epoch: [8/200][20/346], lr: 0.00000011 	 loss = 0.8361(0.7967)
2023/09/27 16:47:52 - INFO - root -   Epoch: [8/200][40/346], lr: 0.00000011 	 loss = 0.8001(0.7786)
2023/09/27 16:49:08 - INFO - root -   Epoch: [8/200][60/346], lr: 0.00000011 	 loss = 1.1667(0.7615)
2023/09/27 16:50:13 - INFO - root -   Epoch: [8/200][80/346], lr: 0.00000011 	 loss = 0.4512(0.7286)
2023/09/27 16:51:28 - INFO - root -   Epoch: [8/200][100/346], lr: 0.00000011 	 loss = 1.0405(0.7275)
2023/09/27 16:52:32 - INFO - root -   Epoch: [8/200][120/346], lr: 0.00000011 	 loss = 0.7062(0.7336)
2023/09/27 16:53:35 - INFO - root -   Epoch: [8/200][140/346], lr: 0.00000011 	 loss = 0.4059(0.7331)
2023/09/27 16:54:43 - INFO - root -   Epoch: [8/200][160/346], lr: 0.00000011 	 loss = 0.2585(0.7423)
2023/09/27 16:55:42 - INFO - root -   Epoch: [8/200][180/346], lr: 0.00000011 	 loss = 0.5395(0.7509)
2023/09/27 16:56:59 - INFO - root -   Epoch: [8/200][200/346], lr: 0.00000011 	 loss = 0.6004(0.7427)
2023/09/27 16:58:02 - INFO - root -   Epoch: [8/200][220/346], lr: 0.00000011 	 loss = 0.5253(0.7328)
2023/09/27 16:59:03 - INFO - root -   Epoch: [8/200][240/346], lr: 0.00000011 	 loss = 0.4668(0.7252)
2023/09/27 17:00:11 - INFO - root -   Epoch: [8/200][260/346], lr: 0.00000011 	 loss = 0.5211(0.7358)
2023/09/27 17:01:15 - INFO - root -   Epoch: [8/200][280/346], lr: 0.00000011 	 loss = 0.8177(0.7443)
2023/09/27 17:02:30 - INFO - root -   Epoch: [8/200][300/346], lr: 0.00000011 	 loss = 1.0192(0.7364)
2023/09/27 17:03:19 - INFO - root -   Epoch: [8/200][320/346], lr: 0.00000011 	 loss = 0.4752(0.7352)
2023/09/27 17:04:22 - INFO - root -   Epoch: [8/200][340/346], lr: 0.00000011 	 loss = 1.1074(0.7348)
2023/09/27 17:04:26 - INFO - root -   Epoch: [8/200] 	 loss = 0.7384
2023/09/27 17:04:26 - INFO - root -   train_accuracy = 0.5434
2023/09/27 17:04:48 - INFO - root -   Epoch: [9/200][0/346], lr: 0.00000011 	 loss = 0.5831(0.5831)
2023/09/27 17:05:54 - INFO - root -   Epoch: [9/200][20/346], lr: 0.00000011 	 loss = 0.8808(0.7567)
2023/09/27 17:07:27 - INFO - root -   Epoch: [9/200][40/346], lr: 0.00000011 	 loss = 0.4728(0.7585)
2023/09/27 17:08:20 - INFO - root -   Epoch: [9/200][60/346], lr: 0.00000011 	 loss = 0.4937(0.7496)
2023/09/27 17:09:46 - INFO - root -   Epoch: [9/200][80/346], lr: 0.00000011 	 loss = 0.9074(0.7538)
2023/09/27 17:10:43 - INFO - root -   Epoch: [9/200][100/346], lr: 0.00000011 	 loss = 0.8752(0.7555)
2023/09/27 17:11:56 - INFO - root -   Epoch: [9/200][120/346], lr: 0.00000011 	 loss = 0.6815(0.7529)
2023/09/27 17:12:59 - INFO - root -   Epoch: [9/200][140/346], lr: 0.00000011 	 loss = 1.1207(0.7423)
2023/09/27 17:14:08 - INFO - root -   Epoch: [9/200][160/346], lr: 0.00000011 	 loss = 0.6754(0.7379)
2023/09/27 17:15:19 - INFO - root -   Epoch: [9/200][180/346], lr: 0.00000011 	 loss = 0.6743(0.7265)
2023/09/27 17:16:30 - INFO - root -   Epoch: [9/200][200/346], lr: 0.00000011 	 loss = 0.7687(0.7211)
2023/09/27 17:17:29 - INFO - root -   Epoch: [9/200][220/346], lr: 0.00000011 	 loss = 0.5030(0.7238)
2023/09/27 17:18:27 - INFO - root -   Epoch: [9/200][240/346], lr: 0.00000011 	 loss = 0.4702(0.7250)
2023/09/27 17:19:38 - INFO - root -   Epoch: [9/200][260/346], lr: 0.00000011 	 loss = 2.2317(0.7321)
2023/09/27 17:20:35 - INFO - root -   Epoch: [9/200][280/346], lr: 0.00000011 	 loss = 0.4683(0.7310)
2023/09/27 17:22:07 - INFO - root -   Epoch: [9/200][300/346], lr: 0.00000011 	 loss = 0.4433(0.7308)
2023/09/27 17:22:53 - INFO - root -   Epoch: [9/200][320/346], lr: 0.00000011 	 loss = 0.7343(0.7291)
2023/09/27 17:23:56 - INFO - root -   Epoch: [9/200][340/346], lr: 0.00000011 	 loss = 0.5362(0.7267)
2023/09/27 17:23:58 - INFO - root -   Epoch: [9/200] 	 loss = 0.7229
2023/09/27 17:27:51 - INFO - root -   precision = 0.5690
2023/09/27 17:27:51 - INFO - root -   eval_loss = 0.6535
2023/09/27 17:27:52 - INFO - root -   train_accuracy = 0.5809
2023/09/27 17:28:21 - INFO - root -   Epoch: [10/200][0/346], lr: 0.00000011 	 loss = 0.5850(0.5850)
2023/09/27 17:29:13 - INFO - root -   Epoch: [10/200][20/346], lr: 0.00000011 	 loss = 1.5618(0.7113)
2023/09/27 17:30:31 - INFO - root -   Epoch: [10/200][40/346], lr: 0.00000011 	 loss = 0.7087(0.7480)
2023/09/27 17:31:36 - INFO - root -   Epoch: [10/200][60/346], lr: 0.00000011 	 loss = 0.7768(0.7440)
2023/09/27 17:32:53 - INFO - root -   Epoch: [10/200][80/346], lr: 0.00000011 	 loss = 0.8874(0.7435)
2023/09/27 17:33:36 - INFO - root -   Epoch: [10/200][100/346], lr: 0.00000011 	 loss = 0.7226(0.7472)
2023/09/27 17:34:45 - INFO - root -   Epoch: [10/200][120/346], lr: 0.00000011 	 loss = 0.9299(0.7476)
2023/09/27 17:35:47 - INFO - root -   Epoch: [10/200][140/346], lr: 0.00000011 	 loss = 0.8197(0.7355)
2023/09/27 17:36:57 - INFO - root -   Epoch: [10/200][160/346], lr: 0.00000011 	 loss = 0.6189(0.7371)
2023/09/27 17:38:03 - INFO - root -   Epoch: [10/200][180/346], lr: 0.00000011 	 loss = 1.3784(0.7453)
2023/09/27 17:39:09 - INFO - root -   Epoch: [10/200][200/346], lr: 0.00000011 	 loss = 0.4826(0.7459)
2023/09/27 17:40:27 - INFO - root -   Epoch: [10/200][220/346], lr: 0.00000011 	 loss = 0.3706(0.7347)
2023/09/27 17:41:28 - INFO - root -   Epoch: [10/200][240/346], lr: 0.00000011 	 loss = 0.7165(0.7472)
2023/09/27 17:42:38 - INFO - root -   Epoch: [10/200][260/346], lr: 0.00000011 	 loss = 0.7769(0.7372)
2023/09/27 17:43:33 - INFO - root -   Epoch: [10/200][280/346], lr: 0.00000011 	 loss = 0.7387(0.7385)
2023/09/27 17:45:01 - INFO - root -   Epoch: [10/200][300/346], lr: 0.00000011 	 loss = 0.3671(0.7341)
2023/09/27 17:45:45 - INFO - root -   Epoch: [10/200][320/346], lr: 0.00000011 	 loss = 0.8393(0.7373)
2023/09/27 17:46:49 - INFO - root -   Epoch: [10/200][340/346], lr: 0.00000011 	 loss = 1.3095(0.7399)
2023/09/27 17:46:51 - INFO - root -   Epoch: [10/200] 	 loss = 0.7376
2023/09/27 17:46:51 - INFO - root -   train_accuracy = 0.5491
2023/09/27 17:47:23 - INFO - root -   Epoch: [11/200][0/346], lr: 0.00000012 	 loss = 0.1391(0.1391)
2023/09/27 17:48:39 - INFO - root -   Epoch: [11/200][20/346], lr: 0.00000012 	 loss = 1.7334(0.6710)
2023/09/27 17:49:36 - INFO - root -   Epoch: [11/200][40/346], lr: 0.00000012 	 loss = 0.8693(0.7203)
2023/09/27 17:50:50 - INFO - root -   Epoch: [11/200][60/346], lr: 0.00000012 	 loss = 0.5368(0.7220)
2023/09/27 17:51:55 - INFO - root -   Epoch: [11/200][80/346], lr: 0.00000012 	 loss = 0.8941(0.7355)
2023/09/27 17:53:17 - INFO - root -   Epoch: [11/200][100/346], lr: 0.00000012 	 loss = 0.5907(0.7330)
2023/09/27 17:54:07 - INFO - root -   Epoch: [11/200][120/346], lr: 0.00000012 	 loss = 0.8538(0.7372)
2023/09/27 17:55:23 - INFO - root -   Epoch: [11/200][140/346], lr: 0.00000012 	 loss = 0.8173(0.7503)
2023/09/27 17:56:16 - INFO - root -   Epoch: [11/200][160/346], lr: 0.00000012 	 loss = 0.3454(0.7323)
2023/09/27 17:57:38 - INFO - root -   Epoch: [11/200][180/346], lr: 0.00000012 	 loss = 0.8189(0.7274)
2023/09/27 17:58:38 - INFO - root -   Epoch: [11/200][200/346], lr: 0.00000012 	 loss = 0.4393(0.7232)
2023/09/27 17:59:37 - INFO - root -   Epoch: [11/200][220/346], lr: 0.00000012 	 loss = 0.4313(0.7178)
2023/09/27 18:00:55 - INFO - root -   Epoch: [11/200][240/346], lr: 0.00000012 	 loss = 0.4753(0.7213)
2023/09/27 18:02:07 - INFO - root -   Epoch: [11/200][260/346], lr: 0.00000012 	 loss = 0.5952(0.7226)
2023/09/27 18:03:12 - INFO - root -   Epoch: [11/200][280/346], lr: 0.00000012 	 loss = 0.3944(0.7185)
2023/09/27 18:04:20 - INFO - root -   Epoch: [11/200][300/346], lr: 0.00000012 	 loss = 0.5049(0.7226)
2023/09/27 18:05:30 - INFO - root -   Epoch: [11/200][320/346], lr: 0.00000012 	 loss = 0.6321(0.7234)
2023/09/27 18:06:19 - INFO - root -   Epoch: [11/200][340/346], lr: 0.00000012 	 loss = 0.9160(0.7189)
2023/09/27 18:06:29 - INFO - root -   Epoch: [11/200] 	 loss = 0.7177
2023/09/27 18:06:29 - INFO - root -   train_accuracy = 0.5549
2023/09/27 18:07:07 - INFO - root -   Epoch: [12/200][0/346], lr: 0.00000012 	 loss = 0.6125(0.6125)
2023/09/27 18:08:07 - INFO - root -   Epoch: [12/200][20/346], lr: 0.00000012 	 loss = 0.6844(0.7723)
2023/09/27 18:09:16 - INFO - root -   Epoch: [12/200][40/346], lr: 0.00000012 	 loss = 0.5153(0.7837)
2023/09/27 18:10:15 - INFO - root -   Epoch: [12/200][60/346], lr: 0.00000012 	 loss = 0.5174(0.7524)
2023/09/27 18:11:45 - INFO - root -   Epoch: [12/200][80/346], lr: 0.00000012 	 loss = 0.4023(0.7258)
2023/09/27 18:12:41 - INFO - root -   Epoch: [12/200][100/346], lr: 0.00000012 	 loss = 0.5340(0.7347)
2023/09/27 18:13:49 - INFO - root -   Epoch: [12/200][120/346], lr: 0.00000012 	 loss = 0.7968(0.7284)
2023/09/27 18:14:50 - INFO - root -   Epoch: [12/200][140/346], lr: 0.00000012 	 loss = 0.8978(0.7288)
2023/09/27 18:16:22 - INFO - root -   Epoch: [12/200][160/346], lr: 0.00000012 	 loss = 1.0712(0.7323)
2023/09/27 18:17:02 - INFO - root -   Epoch: [12/200][180/346], lr: 0.00000012 	 loss = 0.6585(0.7394)
2023/09/27 18:18:20 - INFO - root -   Epoch: [12/200][200/346], lr: 0.00000012 	 loss = 0.7062(0.7458)
2023/09/27 18:19:22 - INFO - root -   Epoch: [12/200][220/346], lr: 0.00000012 	 loss = 0.5041(0.7407)
2023/09/27 18:20:19 - INFO - root -   Epoch: [12/200][240/346], lr: 0.00000012 	 loss = 0.6099(0.7411)
2023/09/27 18:21:23 - INFO - root -   Epoch: [12/200][260/346], lr: 0.00000012 	 loss = 0.5559(0.7388)
2023/09/27 18:22:49 - INFO - root -   Epoch: [12/200][280/346], lr: 0.00000012 	 loss = 0.1579(0.7299)
2023/09/27 18:23:50 - INFO - root -   Epoch: [12/200][300/346], lr: 0.00000012 	 loss = 0.4657(0.7276)
2023/09/27 18:25:06 - INFO - root -   Epoch: [12/200][320/346], lr: 0.00000012 	 loss = 0.2329(0.7293)
2023/09/27 18:25:44 - INFO - root -   Epoch: [12/200][340/346], lr: 0.00000012 	 loss = 0.9702(0.7275)
2023/09/27 18:25:51 - INFO - root -   Epoch: [12/200] 	 loss = 0.7268
2023/09/27 18:25:51 - INFO - root -   train_accuracy = 0.5838
2023/09/27 18:26:32 - INFO - root -   Epoch: [13/200][0/346], lr: 0.00000012 	 loss = 0.7634(0.7634)
2023/09/27 18:27:16 - INFO - root -   Epoch: [13/200][20/346], lr: 0.00000012 	 loss = 0.7679(0.7517)
2023/09/27 18:28:27 - INFO - root -   Epoch: [13/200][40/346], lr: 0.00000012 	 loss = 0.7331(0.7024)
2023/09/27 18:29:45 - INFO - root -   Epoch: [13/200][60/346], lr: 0.00000012 	 loss = 2.1057(0.7258)
2023/09/27 18:30:53 - INFO - root -   Epoch: [13/200][80/346], lr: 0.00000012 	 loss = 0.5188(0.7303)
2023/09/27 18:32:16 - INFO - root -   Epoch: [13/200][100/346], lr: 0.00000012 	 loss = 0.7232(0.7589)
2023/09/27 18:33:20 - INFO - root -   Epoch: [13/200][120/346], lr: 0.00000012 	 loss = 0.9144(0.7596)
2023/09/27 18:34:08 - INFO - root -   Epoch: [13/200][140/346], lr: 0.00000012 	 loss = 0.5422(0.7667)
2023/09/27 18:35:28 - INFO - root -   Epoch: [13/200][160/346], lr: 0.00000012 	 loss = 0.2982(0.7593)
2023/09/27 18:36:38 - INFO - root -   Epoch: [13/200][180/346], lr: 0.00000012 	 loss = 0.8396(0.7540)
2023/09/27 18:37:39 - INFO - root -   Epoch: [13/200][200/346], lr: 0.00000012 	 loss = 0.9068(0.7587)
2023/09/27 18:38:40 - INFO - root -   Epoch: [13/200][220/346], lr: 0.00000012 	 loss = 0.1845(0.7578)
2023/09/27 18:39:52 - INFO - root -   Epoch: [13/200][240/346], lr: 0.00000012 	 loss = 0.4102(0.7546)
2023/09/27 18:40:54 - INFO - root -   Epoch: [13/200][260/346], lr: 0.00000012 	 loss = 0.5437(0.7523)
2023/09/27 18:41:58 - INFO - root -   Epoch: [13/200][280/346], lr: 0.00000012 	 loss = 0.8048(0.7504)
2023/09/27 18:43:11 - INFO - root -   Epoch: [13/200][300/346], lr: 0.00000012 	 loss = 1.0033(0.7506)
2023/09/27 18:44:11 - INFO - root -   Epoch: [13/200][320/346], lr: 0.00000012 	 loss = 0.5899(0.7520)
2023/09/27 18:45:13 - INFO - root -   Epoch: [13/200][340/346], lr: 0.00000012 	 loss = 0.7303(0.7537)
2023/09/27 18:45:15 - INFO - root -   Epoch: [13/200] 	 loss = 0.7518
2023/09/27 18:45:15 - INFO - root -   train_accuracy = 0.5260
2023/09/27 18:45:46 - INFO - root -   Epoch: [14/200][0/346], lr: 0.00000012 	 loss = 0.4982(0.4982)
2023/09/27 18:46:44 - INFO - root -   Epoch: [14/200][20/346], lr: 0.00000012 	 loss = 1.2946(0.7913)
2023/09/27 18:47:56 - INFO - root -   Epoch: [14/200][40/346], lr: 0.00000012 	 loss = 2.2179(0.7875)
2023/09/27 18:49:00 - INFO - root -   Epoch: [14/200][60/346], lr: 0.00000012 	 loss = 0.6778(0.7399)
2023/09/27 18:49:57 - INFO - root -   Epoch: [14/200][80/346], lr: 0.00000012 	 loss = 0.8781(0.7548)
2023/09/27 18:51:15 - INFO - root -   Epoch: [14/200][100/346], lr: 0.00000012 	 loss = 0.7512(0.7694)
2023/09/27 18:52:22 - INFO - root -   Epoch: [14/200][120/346], lr: 0.00000012 	 loss = 0.7944(0.7776)
2023/09/27 18:53:35 - INFO - root -   Epoch: [14/200][140/346], lr: 0.00000012 	 loss = 0.4870(0.7717)
2023/09/27 18:54:32 - INFO - root -   Epoch: [14/200][160/346], lr: 0.00000012 	 loss = 0.2508(0.7711)
2023/09/27 18:55:33 - INFO - root -   Epoch: [14/200][180/346], lr: 0.00000012 	 loss = 0.3875(0.7676)
2023/09/27 18:56:47 - INFO - root -   Epoch: [14/200][200/346], lr: 0.00000012 	 loss = 0.3312(0.7593)
2023/09/27 18:57:42 - INFO - root -   Epoch: [14/200][220/346], lr: 0.00000012 	 loss = 0.7162(0.7477)
2023/09/27 18:59:04 - INFO - root -   Epoch: [14/200][240/346], lr: 0.00000012 	 loss = 1.1593(0.7470)
2023/09/27 18:59:55 - INFO - root -   Epoch: [14/200][260/346], lr: 0.00000012 	 loss = 0.6652(0.7379)
2023/09/27 19:00:57 - INFO - root -   Epoch: [14/200][280/346], lr: 0.00000012 	 loss = 0.2601(0.7341)
2023/09/27 19:02:09 - INFO - root -   Epoch: [14/200][300/346], lr: 0.00000012 	 loss = 0.5616(0.7320)
2023/09/27 19:02:55 - INFO - root -   Epoch: [14/200][320/346], lr: 0.00000012 	 loss = 0.2315(0.7306)
2023/09/27 19:03:58 - INFO - root -   Epoch: [14/200][340/346], lr: 0.00000012 	 loss = 1.0157(0.7316)
2023/09/27 19:04:00 - INFO - root -   Epoch: [14/200] 	 loss = 0.7317
2023/09/27 19:07:52 - INFO - root -   precision = 0.6667
2023/09/27 19:07:52 - INFO - root -   eval_loss = 0.6385
2023/09/27 19:07:53 - INFO - root -   train_accuracy = 0.5491
2023/09/27 19:08:15 - INFO - root -   Epoch: [15/200][0/346], lr: 0.00000012 	 loss = 0.7045(0.7045)
2023/09/27 19:09:32 - INFO - root -   Epoch: [15/200][20/346], lr: 0.00000012 	 loss = 0.5101(0.6463)
2023/09/27 19:10:46 - INFO - root -   Epoch: [15/200][40/346], lr: 0.00000012 	 loss = 0.4695(0.6611)
2023/09/27 19:11:54 - INFO - root -   Epoch: [15/200][60/346], lr: 0.00000012 	 loss = 0.5207(0.6925)
2023/09/27 19:12:58 - INFO - root -   Epoch: [15/200][80/346], lr: 0.00000012 	 loss = 1.4121(0.7216)
2023/09/27 19:13:56 - INFO - root -   Epoch: [15/200][100/346], lr: 0.00000012 	 loss = 1.1978(0.7195)
2023/09/27 19:15:38 - INFO - root -   Epoch: [15/200][120/346], lr: 0.00000012 	 loss = 0.3153(0.7424)
2023/09/27 19:16:22 - INFO - root -   Epoch: [15/200][140/346], lr: 0.00000012 	 loss = 0.5954(0.7357)
2023/09/27 19:17:48 - INFO - root -   Epoch: [15/200][160/346], lr: 0.00000012 	 loss = 0.2559(0.7212)
2023/09/27 19:18:39 - INFO - root -   Epoch: [15/200][180/346], lr: 0.00000012 	 loss = 0.6289(0.7171)
2023/09/27 19:19:41 - INFO - root -   Epoch: [15/200][200/346], lr: 0.00000012 	 loss = 0.2082(0.7238)
2023/09/27 19:20:44 - INFO - root -   Epoch: [15/200][220/346], lr: 0.00000012 	 loss = 0.5502(0.7223)
2023/09/27 19:22:10 - INFO - root -   Epoch: [15/200][240/346], lr: 0.00000012 	 loss = 0.8896(0.7286)
2023/09/27 19:22:55 - INFO - root -   Epoch: [15/200][260/346], lr: 0.00000012 	 loss = 0.4283(0.7272)
2023/09/27 19:24:18 - INFO - root -   Epoch: [15/200][280/346], lr: 0.00000012 	 loss = 0.7602(0.7240)
2023/09/27 19:25:20 - INFO - root -   Epoch: [15/200][300/346], lr: 0.00000012 	 loss = 0.6846(0.7264)
2023/09/27 19:26:26 - INFO - root -   Epoch: [15/200][320/346], lr: 0.00000012 	 loss = 0.5209(0.7303)
2023/09/27 19:27:34 - INFO - root -   Epoch: [15/200][340/346], lr: 0.00000012 	 loss = 0.9354(0.7371)
2023/09/27 19:27:37 - INFO - root -   Epoch: [15/200] 	 loss = 0.7377
2023/09/27 19:27:37 - INFO - root -   train_accuracy = 0.5376
2023/09/27 19:27:59 - INFO - root -   Epoch: [16/200][0/346], lr: 0.00000012 	 loss = 0.6137(0.6137)
2023/09/27 19:29:12 - INFO - root -   Epoch: [16/200][20/346], lr: 0.00000012 	 loss = 0.7670(0.7524)
2023/09/27 19:30:14 - INFO - root -   Epoch: [16/200][40/346], lr: 0.00000012 	 loss = 0.8006(0.7309)
2023/09/27 19:31:31 - INFO - root -   Epoch: [16/200][60/346], lr: 0.00000012 	 loss = 0.4697(0.7351)
2023/09/27 19:32:40 - INFO - root -   Epoch: [16/200][80/346], lr: 0.00000012 	 loss = 0.6247(0.7419)
2023/09/27 19:33:41 - INFO - root -   Epoch: [16/200][100/346], lr: 0.00000012 	 loss = 0.6363(0.7340)
2023/09/27 19:35:04 - INFO - root -   Epoch: [16/200][120/346], lr: 0.00000012 	 loss = 0.6330(0.7453)
2023/09/27 19:36:06 - INFO - root -   Epoch: [16/200][140/346], lr: 0.00000012 	 loss = 0.8438(0.7434)
2023/09/27 19:37:10 - INFO - root -   Epoch: [16/200][160/346], lr: 0.00000012 	 loss = 0.3063(0.7472)
2023/09/27 19:38:03 - INFO - root -   Epoch: [16/200][180/346], lr: 0.00000012 	 loss = 0.5743(0.7407)
2023/09/27 19:39:27 - INFO - root -   Epoch: [16/200][200/346], lr: 0.00000012 	 loss = 0.6369(0.7457)
2023/09/27 19:40:10 - INFO - root -   Epoch: [16/200][220/346], lr: 0.00000012 	 loss = 0.5768(0.7437)
2023/09/27 19:41:24 - INFO - root -   Epoch: [16/200][240/346], lr: 0.00000012 	 loss = 0.2186(0.7366)
2023/09/27 19:42:28 - INFO - root -   Epoch: [16/200][260/346], lr: 0.00000012 	 loss = 0.6226(0.7448)
2023/09/27 19:43:35 - INFO - root -   Epoch: [16/200][280/346], lr: 0.00000012 	 loss = 1.2293(0.7375)
2023/09/27 19:44:47 - INFO - root -   Epoch: [16/200][300/346], lr: 0.00000012 	 loss = 0.5028(0.7284)
2023/09/27 19:46:02 - INFO - root -   Epoch: [16/200][320/346], lr: 0.00000012 	 loss = 0.6387(0.7201)
2023/09/27 19:46:38 - INFO - root -   Epoch: [16/200][340/346], lr: 0.00000012 	 loss = 0.6571(0.7170)
2023/09/27 19:46:41 - INFO - root -   Epoch: [16/200] 	 loss = 0.7175
2023/09/27 19:46:41 - INFO - root -   train_accuracy = 0.5477
2023/09/27 19:47:12 - INFO - root -   Epoch: [17/200][0/346], lr: 0.00000012 	 loss = 0.6473(0.6473)
2023/09/27 19:48:10 - INFO - root -   Epoch: [17/200][20/346], lr: 0.00000012 	 loss = 0.9079(0.7736)
2023/09/27 19:49:30 - INFO - root -   Epoch: [17/200][40/346], lr: 0.00000012 	 loss = 0.4876(0.7541)
2023/09/27 19:50:22 - INFO - root -   Epoch: [17/200][60/346], lr: 0.00000012 	 loss = 0.8329(0.7339)
2023/09/27 19:51:38 - INFO - root -   Epoch: [17/200][80/346], lr: 0.00000012 	 loss = 0.7552(0.7277)
2023/09/27 19:52:37 - INFO - root -   Epoch: [17/200][100/346], lr: 0.00000012 	 loss = 0.3611(0.7265)
2023/09/27 19:53:52 - INFO - root -   Epoch: [17/200][120/346], lr: 0.00000012 	 loss = 1.0466(0.7534)
2023/09/27 19:54:58 - INFO - root -   Epoch: [17/200][140/346], lr: 0.00000012 	 loss = 0.4548(0.7350)
2023/09/27 19:56:13 - INFO - root -   Epoch: [17/200][160/346], lr: 0.00000012 	 loss = 0.1991(0.7271)
2023/09/27 19:57:04 - INFO - root -   Epoch: [17/200][180/346], lr: 0.00000012 	 loss = 1.0271(0.7246)
2023/09/27 19:58:29 - INFO - root -   Epoch: [17/200][200/346], lr: 0.00000012 	 loss = 0.2252(0.7212)
2023/09/27 19:59:31 - INFO - root -   Epoch: [17/200][220/346], lr: 0.00000012 	 loss = 0.2419(0.7203)
2023/09/27 20:00:40 - INFO - root -   Epoch: [17/200][240/346], lr: 0.00000012 	 loss = 0.7478(0.7297)
2023/09/27 20:01:46 - INFO - root -   Epoch: [17/200][260/346], lr: 0.00000012 	 loss = 0.7672(0.7385)
2023/09/27 20:02:55 - INFO - root -   Epoch: [17/200][280/346], lr: 0.00000012 	 loss = 0.4900(0.7434)
2023/09/27 20:04:04 - INFO - root -   Epoch: [17/200][300/346], lr: 0.00000012 	 loss = 0.4634(0.7378)
2023/09/27 20:05:20 - INFO - root -   Epoch: [17/200][320/346], lr: 0.00000012 	 loss = 0.3038(0.7346)
2023/09/27 20:06:19 - INFO - root -   Epoch: [17/200][340/346], lr: 0.00000012 	 loss = 0.2924(0.7346)
2023/09/27 20:06:21 - INFO - root -   Epoch: [17/200] 	 loss = 0.7397
2023/09/27 20:06:21 - INFO - root -   train_accuracy = 0.5491
2023/09/27 20:06:59 - INFO - root -   Epoch: [18/200][0/346], lr: 0.00000013 	 loss = 0.5927(0.5927)
2023/09/27 20:08:00 - INFO - root -   Epoch: [18/200][20/346], lr: 0.00000013 	 loss = 1.1461(0.7481)
2023/09/27 20:09:03 - INFO - root -   Epoch: [18/200][40/346], lr: 0.00000013 	 loss = 1.3383(0.7891)
2023/09/27 20:10:10 - INFO - root -   Epoch: [18/200][60/346], lr: 0.00000013 	 loss = 0.4592(0.7650)
2023/09/27 20:11:28 - INFO - root -   Epoch: [18/200][80/346], lr: 0.00000013 	 loss = 0.2751(0.7599)
2023/09/27 20:12:11 - INFO - root -   Epoch: [18/200][100/346], lr: 0.00000013 	 loss = 0.5098(0.7485)
2023/09/27 20:13:33 - INFO - root -   Epoch: [18/200][120/346], lr: 0.00000013 	 loss = 0.8422(0.7564)
2023/09/27 20:14:42 - INFO - root -   Epoch: [18/200][140/346], lr: 0.00000013 	 loss = 0.4636(0.7602)
2023/09/27 20:15:40 - INFO - root -   Epoch: [18/200][160/346], lr: 0.00000013 	 loss = 0.3822(0.7444)
2023/09/27 20:16:56 - INFO - root -   Epoch: [18/200][180/346], lr: 0.00000013 	 loss = 0.8501(0.7250)
2023/09/27 20:17:52 - INFO - root -   Epoch: [18/200][200/346], lr: 0.00000013 	 loss = 0.8316(0.7164)
2023/09/27 20:19:05 - INFO - root -   Epoch: [18/200][220/346], lr: 0.00000013 	 loss = 0.3810(0.7159)
2023/09/27 20:20:11 - INFO - root -   Epoch: [18/200][240/346], lr: 0.00000013 	 loss = 0.9022(0.7165)
2023/09/27 20:21:13 - INFO - root -   Epoch: [18/200][260/346], lr: 0.00000013 	 loss = 1.4007(0.7225)
2023/09/27 20:22:19 - INFO - root -   Epoch: [18/200][280/346], lr: 0.00000013 	 loss = 0.8415(0.7261)
2023/09/27 20:23:38 - INFO - root -   Epoch: [18/200][300/346], lr: 0.00000013 	 loss = 0.6159(0.7262)
2023/09/27 20:24:49 - INFO - root -   Epoch: [18/200][320/346], lr: 0.00000013 	 loss = 0.4317(0.7297)
2023/09/27 20:25:47 - INFO - root -   Epoch: [18/200][340/346], lr: 0.00000013 	 loss = 0.6354(0.7226)
2023/09/27 20:25:54 - INFO - root -   Epoch: [18/200] 	 loss = 0.7232
2023/09/27 20:25:54 - INFO - root -   train_accuracy = 0.5607
2023/09/27 20:26:25 - INFO - root -   Epoch: [19/200][0/346], lr: 0.00000013 	 loss = 0.5718(0.5718)
2023/09/27 20:27:38 - INFO - root -   Epoch: [19/200][20/346], lr: 0.00000013 	 loss = 1.0590(0.7719)
2023/09/27 20:28:39 - INFO - root -   Epoch: [19/200][40/346], lr: 0.00000013 	 loss = 0.3331(0.7436)
2023/09/27 20:29:43 - INFO - root -   Epoch: [19/200][60/346], lr: 0.00000013 	 loss = 0.5040(0.7357)
2023/09/27 20:30:57 - INFO - root -   Epoch: [19/200][80/346], lr: 0.00000013 	 loss = 0.8101(0.7432)
2023/09/27 20:31:59 - INFO - root -   Epoch: [19/200][100/346], lr: 0.00000013 	 loss = 0.9191(0.7451)
2023/09/27 20:33:14 - INFO - root -   Epoch: [19/200][120/346], lr: 0.00000013 	 loss = 0.8557(0.7430)
2023/09/27 20:34:05 - INFO - root -   Epoch: [19/200][140/346], lr: 0.00000013 	 loss = 0.5630(0.7319)
2023/09/27 20:35:19 - INFO - root -   Epoch: [19/200][160/346], lr: 0.00000013 	 loss = 0.2331(0.7407)
2023/09/27 20:36:25 - INFO - root -   Epoch: [19/200][180/346], lr: 0.00000013 	 loss = 1.1162(0.7515)
2023/09/27 20:37:40 - INFO - root -   Epoch: [19/200][200/346], lr: 0.00000013 	 loss = 0.3704(0.7432)
2023/09/27 20:38:40 - INFO - root -   Epoch: [19/200][220/346], lr: 0.00000013 	 loss = 0.2865(0.7367)
2023/09/27 20:40:51 - INFO - root -   Epoch: [19/200][240/346], lr: 0.00000013 	 loss = 0.7238(0.7366)
2023/09/27 20:41:40 - INFO - root -   Epoch: [19/200][260/346], lr: 0.00000013 	 loss = 0.8548(0.7374)
2023/09/27 20:43:14 - INFO - root -   Epoch: [19/200][280/346], lr: 0.00000013 	 loss = 0.5754(0.7429)
2023/09/27 20:44:17 - INFO - root -   Epoch: [19/200][300/346], lr: 0.00000013 	 loss = 0.5023(0.7389)
2023/09/27 20:45:40 - INFO - root -   Epoch: [19/200][320/346], lr: 0.00000013 	 loss = 0.6768(0.7357)
2023/09/27 20:46:21 - INFO - root -   Epoch: [19/200][340/346], lr: 0.00000013 	 loss = 1.3132(0.7332)
2023/09/27 20:46:25 - INFO - root -   Epoch: [19/200] 	 loss = 0.7324
2023/09/27 20:50:17 - INFO - root -   precision = 0.6437
2023/09/27 20:50:17 - INFO - root -   eval_loss = 0.6333
2023/09/27 20:50:18 - INFO - root -   train_accuracy = 0.5376
2023/09/27 20:50:47 - INFO - root -   Epoch: [20/200][0/346], lr: 0.00000013 	 loss = 0.7174(0.7174)
2023/09/27 20:51:53 - INFO - root -   Epoch: [20/200][20/346], lr: 0.00000013 	 loss = 1.2742(0.6905)
2023/09/27 20:53:16 - INFO - root -   Epoch: [20/200][40/346], lr: 0.00000013 	 loss = 0.4703(0.6195)
2023/09/27 20:54:10 - INFO - root -   Epoch: [20/200][60/346], lr: 0.00000013 	 loss = 0.7614(0.6687)
2023/09/27 20:55:25 - INFO - root -   Epoch: [20/200][80/346], lr: 0.00000013 	 loss = 0.9183(0.6937)
2023/09/27 20:56:21 - INFO - root -   Epoch: [20/200][100/346], lr: 0.00000013 	 loss = 0.7604(0.6977)
2023/09/27 20:57:59 - INFO - root -   Epoch: [20/200][120/346], lr: 0.00000013 	 loss = 0.3079(0.6978)
2023/09/27 20:58:49 - INFO - root -   Epoch: [20/200][140/346], lr: 0.00000013 	 loss = 0.8853(0.6966)
2023/09/27 21:00:13 - INFO - root -   Epoch: [20/200][160/346], lr: 0.00000013 	 loss = 0.5152(0.7150)
2023/09/27 21:01:02 - INFO - root -   Epoch: [20/200][180/346], lr: 0.00000013 	 loss = 0.4647(0.7017)
2023/09/27 21:02:01 - INFO - root -   Epoch: [20/200][200/346], lr: 0.00000013 	 loss = 0.3892(0.6887)
2023/09/27 21:03:14 - INFO - root -   Epoch: [20/200][220/346], lr: 0.00000013 	 loss = 0.3723(0.6940)
2023/09/27 21:04:12 - INFO - root -   Epoch: [20/200][240/346], lr: 0.00000013 	 loss = 0.3655(0.6921)
2023/09/27 21:05:27 - INFO - root -   Epoch: [20/200][260/346], lr: 0.00000013 	 loss = 0.5158(0.6993)
2023/09/27 21:06:34 - INFO - root -   Epoch: [20/200][280/346], lr: 0.00000013 	 loss = 0.3782(0.7031)
2023/09/27 21:07:46 - INFO - root -   Epoch: [20/200][300/346], lr: 0.00000013 	 loss = 0.5464(0.7057)
2023/09/27 21:08:46 - INFO - root -   Epoch: [20/200][320/346], lr: 0.00000013 	 loss = 0.2811(0.7087)
2023/09/27 21:09:48 - INFO - root -   Epoch: [20/200][340/346], lr: 0.00000013 	 loss = 1.1022(0.7020)
2023/09/27 21:09:50 - INFO - root -   Epoch: [20/200] 	 loss = 0.7029
2023/09/27 21:09:50 - INFO - root -   train_accuracy = 0.5665
2023/09/27 21:10:19 - INFO - root -   Epoch: [21/200][0/346], lr: 0.00000013 	 loss = 0.6414(0.6414)
2023/09/27 21:11:26 - INFO - root -   Epoch: [21/200][20/346], lr: 0.00000013 	 loss = 1.8079(0.6852)
2023/09/27 21:12:34 - INFO - root -   Epoch: [21/200][40/346], lr: 0.00000013 	 loss = 0.9227(0.7334)
2023/09/27 21:13:36 - INFO - root -   Epoch: [21/200][60/346], lr: 0.00000013 	 loss = 0.9824(0.7376)
2023/09/27 21:14:45 - INFO - root -   Epoch: [21/200][80/346], lr: 0.00000013 	 loss = 0.2947(0.7329)
2023/09/27 21:15:44 - INFO - root -   Epoch: [21/200][100/346], lr: 0.00000013 	 loss = 0.5515(0.7434)
2023/09/27 21:16:51 - INFO - root -   Epoch: [21/200][120/346], lr: 0.00000013 	 loss = 0.5941(0.7508)
2023/09/27 21:17:50 - INFO - root -   Epoch: [21/200][140/346], lr: 0.00000013 	 loss = 0.9345(0.7434)
2023/09/27 21:18:58 - INFO - root -   Epoch: [21/200][160/346], lr: 0.00000013 	 loss = 0.1907(0.7323)
2023/09/27 21:20:01 - INFO - root -   Epoch: [21/200][180/346], lr: 0.00000013 	 loss = 0.8904(0.7289)
2023/09/27 21:21:29 - INFO - root -   Epoch: [21/200][200/346], lr: 0.00000013 	 loss = 0.2946(0.7310)
2023/09/27 21:22:26 - INFO - root -   Epoch: [21/200][220/346], lr: 0.00000013 	 loss = 0.6712(0.7293)
2023/09/27 21:23:50 - INFO - root -   Epoch: [21/200][240/346], lr: 0.00000013 	 loss = 0.7873(0.7297)
2023/09/27 21:24:33 - INFO - root -   Epoch: [21/200][260/346], lr: 0.00000013 	 loss = 1.4163(0.7229)
2023/09/27 21:25:58 - INFO - root -   Epoch: [21/200][280/346], lr: 0.00000013 	 loss = 0.7720(0.7344)
2023/09/27 21:26:47 - INFO - root -   Epoch: [21/200][300/346], lr: 0.00000013 	 loss = 0.6943(0.7296)
2023/09/27 21:28:09 - INFO - root -   Epoch: [21/200][320/346], lr: 0.00000013 	 loss = 0.3981(0.7300)
2023/09/27 21:28:49 - INFO - root -   Epoch: [21/200][340/346], lr: 0.00000013 	 loss = 0.9820(0.7272)
2023/09/27 21:28:55 - INFO - root -   Epoch: [21/200] 	 loss = 0.7266
2023/09/27 21:28:55 - INFO - root -   train_accuracy = 0.5636
2023/09/27 21:29:26 - INFO - root -   Epoch: [22/200][0/346], lr: 0.00000013 	 loss = 1.0409(1.0409)
2023/09/27 21:30:32 - INFO - root -   Epoch: [22/200][20/346], lr: 0.00000013 	 loss = 0.4746(0.7413)
2023/09/27 21:31:59 - INFO - root -   Epoch: [22/200][40/346], lr: 0.00000013 	 loss = 0.7624(0.7735)
2023/09/27 21:32:49 - INFO - root -   Epoch: [22/200][60/346], lr: 0.00000013 	 loss = 0.9543(0.7496)
2023/09/27 21:33:57 - INFO - root -   Epoch: [22/200][80/346], lr: 0.00000013 	 loss = 0.6111(0.7123)
2023/09/27 21:35:07 - INFO - root -   Epoch: [22/200][100/346], lr: 0.00000013 	 loss = 0.3548(0.7108)
2023/09/27 21:36:22 - INFO - root -   Epoch: [22/200][120/346], lr: 0.00000013 	 loss = 0.5873(0.7254)
2023/09/27 21:37:19 - INFO - root -   Epoch: [22/200][140/346], lr: 0.00000013 	 loss = 0.8159(0.7339)
2023/09/27 21:38:38 - INFO - root -   Epoch: [22/200][160/346], lr: 0.00000013 	 loss = 0.3006(0.7387)
2023/09/27 21:39:25 - INFO - root -   Epoch: [22/200][180/346], lr: 0.00000013 	 loss = 0.7529(0.7415)
2023/09/27 21:40:40 - INFO - root -   Epoch: [22/200][200/346], lr: 0.00000013 	 loss = 0.9270(0.7435)
2023/09/27 21:41:38 - INFO - root -   Epoch: [22/200][220/346], lr: 0.00000013 	 loss = 0.4711(0.7354)
2023/09/27 21:42:59 - INFO - root -   Epoch: [22/200][240/346], lr: 0.00000013 	 loss = 0.4836(0.7313)
2023/09/27 21:44:04 - INFO - root -   Epoch: [22/200][260/346], lr: 0.00000013 	 loss = 0.6563(0.7345)
2023/09/27 21:45:21 - INFO - root -   Epoch: [22/200][280/346], lr: 0.00000013 	 loss = 0.9712(0.7451)
2023/09/27 21:46:17 - INFO - root -   Epoch: [22/200][300/346], lr: 0.00000013 	 loss = 0.6943(0.7447)
2023/09/27 21:47:26 - INFO - root -   Epoch: [22/200][320/346], lr: 0.00000013 	 loss = 0.2496(0.7491)
2023/09/27 21:48:16 - INFO - root -   Epoch: [22/200][340/346], lr: 0.00000013 	 loss = 1.1787(0.7496)
2023/09/27 21:48:23 - INFO - root -   Epoch: [22/200] 	 loss = 0.7500
2023/09/27 21:48:23 - INFO - root -   train_accuracy = 0.5289
2023/09/27 21:48:53 - INFO - root -   Epoch: [23/200][0/346], lr: 0.00000013 	 loss = 0.5229(0.5229)
2023/09/27 21:49:59 - INFO - root -   Epoch: [23/200][20/346], lr: 0.00000013 	 loss = 0.4627(0.7084)
2023/09/27 21:50:56 - INFO - root -   Epoch: [23/200][40/346], lr: 0.00000013 	 loss = 0.5548(0.6879)
2023/09/27 21:52:15 - INFO - root -   Epoch: [23/200][60/346], lr: 0.00000013 	 loss = 0.7013(0.6964)
2023/09/27 21:53:18 - INFO - root -   Epoch: [23/200][80/346], lr: 0.00000013 	 loss = 0.4039(0.6902)
2023/09/27 21:54:31 - INFO - root -   Epoch: [23/200][100/346], lr: 0.00000013 	 loss = 1.0223(0.7176)
2023/09/27 21:55:24 - INFO - root -   Epoch: [23/200][120/346], lr: 0.00000013 	 loss = 0.2982(0.7236)
2023/09/27 21:56:36 - INFO - root -   Epoch: [23/200][140/346], lr: 0.00000013 	 loss = 0.6234(0.7192)
2023/09/27 21:57:38 - INFO - root -   Epoch: [23/200][160/346], lr: 0.00000013 	 loss = 0.8381(0.7105)
2023/09/27 21:58:41 - INFO - root -   Epoch: [23/200][180/346], lr: 0.00000013 	 loss = 0.4287(0.6994)
2023/09/27 21:59:50 - INFO - root -   Epoch: [23/200][200/346], lr: 0.00000013 	 loss = 0.7973(0.6980)
2023/09/27 22:01:07 - INFO - root -   Epoch: [23/200][220/346], lr: 0.00000013 	 loss = 0.3178(0.7074)
2023/09/27 22:02:00 - INFO - root -   Epoch: [23/200][240/346], lr: 0.00000013 	 loss = 0.7992(0.7136)
2023/09/27 22:03:00 - INFO - root -   Epoch: [23/200][260/346], lr: 0.00000013 	 loss = 0.6192(0.7110)
2023/09/27 22:04:10 - INFO - root -   Epoch: [23/200][280/346], lr: 0.00000013 	 loss = 0.7245(0.7097)
2023/09/27 22:05:19 - INFO - root -   Epoch: [23/200][300/346], lr: 0.00000013 	 loss = 0.5375(0.7151)
2023/09/27 22:06:15 - INFO - root -   Epoch: [23/200][320/346], lr: 0.00000013 	 loss = 0.3829(0.7114)
2023/09/27 22:07:31 - INFO - root -   Epoch: [23/200][340/346], lr: 0.00000013 	 loss = 1.0125(0.7097)
2023/09/27 22:07:34 - INFO - root -   Epoch: [23/200] 	 loss = 0.7061
2023/09/27 22:07:34 - INFO - root -   train_accuracy = 0.5694
2023/09/27 22:08:06 - INFO - root -   Epoch: [24/200][0/346], lr: 0.00000013 	 loss = 0.7377(0.7377)
2023/09/27 22:09:16 - INFO - root -   Epoch: [24/200][20/346], lr: 0.00000013 	 loss = 1.3330(0.7312)
2023/09/27 22:10:17 - INFO - root -   Epoch: [24/200][40/346], lr: 0.00000013 	 loss = 0.5763(0.7316)
2023/09/27 22:11:37 - INFO - root -   Epoch: [24/200][60/346], lr: 0.00000013 	 loss = 0.7862(0.7141)
2023/09/27 22:12:54 - INFO - root -   Epoch: [24/200][80/346], lr: 0.00000013 	 loss = 0.4629(0.6974)
2023/09/27 22:14:00 - INFO - root -   Epoch: [24/200][100/346], lr: 0.00000013 	 loss = 1.4692(0.7001)
2023/09/27 22:15:18 - INFO - root -   Epoch: [24/200][120/346], lr: 0.00000013 	 loss = 0.9418(0.7224)
2023/09/27 22:16:17 - INFO - root -   Epoch: [24/200][140/346], lr: 0.00000013 	 loss = 0.6682(0.7104)
2023/09/27 22:17:33 - INFO - root -   Epoch: [24/200][160/346], lr: 0.00000013 	 loss = 0.6668(0.7137)
2023/09/27 22:18:25 - INFO - root -   Epoch: [24/200][180/346], lr: 0.00000013 	 loss = 0.4677(0.7152)
2023/09/27 22:19:40 - INFO - root -   Epoch: [24/200][200/346], lr: 0.00000013 	 loss = 0.3350(0.7116)
2023/09/27 22:20:41 - INFO - root -   Epoch: [24/200][220/346], lr: 0.00000013 	 loss = 0.3938(0.7194)
2023/09/27 22:21:40 - INFO - root -   Epoch: [24/200][240/346], lr: 0.00000013 	 loss = 0.4339(0.7184)
2023/09/27 22:22:46 - INFO - root -   Epoch: [24/200][260/346], lr: 0.00000013 	 loss = 0.8382(0.7185)
2023/09/27 22:23:50 - INFO - root -   Epoch: [24/200][280/346], lr: 0.00000013 	 loss = 0.5492(0.7184)
2023/09/27 22:25:15 - INFO - root -   Epoch: [24/200][300/346], lr: 0.00000013 	 loss = 0.7174(0.7198)
2023/09/27 22:26:09 - INFO - root -   Epoch: [24/200][320/346], lr: 0.00000013 	 loss = 0.8831(0.7254)
2023/09/27 22:27:13 - INFO - root -   Epoch: [24/200][340/346], lr: 0.00000013 	 loss = 0.3877(0.7192)
2023/09/27 22:27:15 - INFO - root -   Epoch: [24/200] 	 loss = 0.7172
2023/09/27 22:31:06 - INFO - root -   precision = 0.6322
2023/09/27 22:31:06 - INFO - root -   eval_loss = 0.6334
2023/09/27 22:31:07 - INFO - root -   train_accuracy = 0.5564
2023/09/27 22:31:36 - INFO - root -   Epoch: [25/200][0/346], lr: 0.00000014 	 loss = 0.3846(0.3846)
2023/09/27 22:32:44 - INFO - root -   Epoch: [25/200][20/346], lr: 0.00000014 	 loss = 0.6156(0.7154)
2023/09/27 22:33:59 - INFO - root -   Epoch: [25/200][40/346], lr: 0.00000014 	 loss = 0.5932(0.7514)
2023/09/27 22:34:54 - INFO - root -   Epoch: [25/200][60/346], lr: 0.00000014 	 loss = 0.5835(0.7531)
2023/09/27 22:36:18 - INFO - root -   Epoch: [25/200][80/346], lr: 0.00000014 	 loss = 0.8505(0.7545)
2023/09/27 22:37:26 - INFO - root -   Epoch: [25/200][100/346], lr: 0.00000014 	 loss = 1.3753(0.7435)
2023/09/27 22:38:27 - INFO - root -   Epoch: [25/200][120/346], lr: 0.00000014 	 loss = 0.8550(0.7490)
2023/09/27 22:39:31 - INFO - root -   Epoch: [25/200][140/346], lr: 0.00000014 	 loss = 1.1141(0.7443)
2023/09/27 22:40:37 - INFO - root -   Epoch: [25/200][160/346], lr: 0.00000014 	 loss = 0.5187(0.7364)
2023/09/27 22:41:45 - INFO - root -   Epoch: [25/200][180/346], lr: 0.00000014 	 loss = 0.5233(0.7287)
2023/09/27 22:43:14 - INFO - root -   Epoch: [25/200][200/346], lr: 0.00000014 	 loss = 0.6008(0.7211)
2023/09/27 22:44:16 - INFO - root -   Epoch: [25/200][220/346], lr: 0.00000014 	 loss = 0.3170(0.7173)
2023/09/27 22:45:20 - INFO - root -   Epoch: [25/200][240/346], lr: 0.00000014 	 loss = 0.4224(0.7154)
2023/09/27 22:46:21 - INFO - root -   Epoch: [25/200][260/346], lr: 0.00000014 	 loss = 0.8984(0.7193)
2023/09/27 22:47:26 - INFO - root -   Epoch: [25/200][280/346], lr: 0.00000014 	 loss = 0.7290(0.7229)
2023/09/27 22:48:40 - INFO - root -   Epoch: [25/200][300/346], lr: 0.00000014 	 loss = 0.5207(0.7209)
2023/09/27 22:49:34 - INFO - root -   Epoch: [25/200][320/346], lr: 0.00000014 	 loss = 0.5885(0.7160)
2023/09/27 22:50:46 - INFO - root -   Epoch: [25/200][340/346], lr: 0.00000014 	 loss = 1.0396(0.7101)
2023/09/27 22:50:48 - INFO - root -   Epoch: [25/200] 	 loss = 0.7095
2023/09/27 22:50:48 - INFO - root -   train_accuracy = 0.5737
2023/09/27 22:51:10 - INFO - root -   Epoch: [26/200][0/346], lr: 0.00000014 	 loss = 0.6897(0.6897)
2023/09/27 22:52:45 - INFO - root -   Epoch: [26/200][20/346], lr: 0.00000014 	 loss = 0.5417(0.6864)
2023/09/27 22:53:46 - INFO - root -   Epoch: [26/200][40/346], lr: 0.00000014 	 loss = 0.3407(0.7483)
2023/09/27 22:54:52 - INFO - root -   Epoch: [26/200][60/346], lr: 0.00000014 	 loss = 0.3056(0.7434)
2023/09/27 22:55:36 - INFO - root -   Epoch: [26/200][80/346], lr: 0.00000014 	 loss = 1.0280(0.7112)
2023/09/27 22:56:37 - INFO - root -   Epoch: [26/200][100/346], lr: 0.00000014 	 loss = 0.8387(0.7316)
2023/09/27 22:57:37 - INFO - root -   Epoch: [26/200][120/346], lr: 0.00000014 	 loss = 0.5768(0.7234)
2023/09/27 22:59:07 - INFO - root -   Epoch: [26/200][140/346], lr: 0.00000014 	 loss = 1.0666(0.7129)
2023/09/27 22:59:58 - INFO - root -   Epoch: [26/200][160/346], lr: 0.00000014 	 loss = 0.6871(0.7130)
2023/09/27 23:01:00 - INFO - root -   Epoch: [26/200][180/346], lr: 0.00000014 	 loss = 0.4984(0.7030)
2023/09/27 23:02:05 - INFO - root -   Epoch: [26/200][200/346], lr: 0.00000014 	 loss = 0.3133(0.6959)
2023/09/27 23:03:26 - INFO - root -   Epoch: [26/200][220/346], lr: 0.00000014 	 loss = 0.2646(0.6882)
2023/09/27 23:04:31 - INFO - root -   Epoch: [26/200][240/346], lr: 0.00000014 	 loss = 0.2932(0.6929)
2023/09/27 23:05:38 - INFO - root -   Epoch: [26/200][260/346], lr: 0.00000014 	 loss = 0.8722(0.6979)
2023/09/27 23:06:53 - INFO - root -   Epoch: [26/200][280/346], lr: 0.00000014 	 loss = 0.7263(0.6912)
2023/09/27 23:07:46 - INFO - root -   Epoch: [26/200][300/346], lr: 0.00000014 	 loss = 0.5891(0.6824)
2023/09/27 23:09:16 - INFO - root -   Epoch: [26/200][320/346], lr: 0.00000014 	 loss = 0.8248(0.6825)
2023/09/27 23:09:54 - INFO - root -   Epoch: [26/200][340/346], lr: 0.00000014 	 loss = 0.8642(0.6892)
2023/09/27 23:10:01 - INFO - root -   Epoch: [26/200] 	 loss = 0.6879
2023/09/27 23:10:01 - INFO - root -   train_accuracy = 0.5896
2023/09/27 23:10:23 - INFO - root -   Epoch: [27/200][0/346], lr: 0.00000014 	 loss = 0.5673(0.5673)
2023/09/27 23:11:37 - INFO - root -   Epoch: [27/200][20/346], lr: 0.00000014 	 loss = 0.4054(0.6049)
2023/09/27 23:12:46 - INFO - root -   Epoch: [27/200][40/346], lr: 0.00000014 	 loss = 0.3760(0.6389)
2023/09/27 23:14:02 - INFO - root -   Epoch: [27/200][60/346], lr: 0.00000014 	 loss = 0.5445(0.6614)
2023/09/27 23:15:14 - INFO - root -   Epoch: [27/200][80/346], lr: 0.00000014 	 loss = 1.1151(0.6733)
2023/09/27 23:16:36 - INFO - root -   Epoch: [27/200][100/346], lr: 0.00000014 	 loss = 0.5045(0.6959)
2023/09/27 23:17:20 - INFO - root -   Epoch: [27/200][120/346], lr: 0.00000014 	 loss = 0.4317(0.7083)
2023/09/27 23:18:42 - INFO - root -   Epoch: [27/200][140/346], lr: 0.00000014 	 loss = 0.5794(0.6984)
2023/09/27 23:19:35 - INFO - root -   Epoch: [27/200][160/346], lr: 0.00000014 	 loss = 0.5697(0.6880)
2023/09/27 23:20:51 - INFO - root -   Epoch: [27/200][180/346], lr: 0.00000014 	 loss = 0.8725(0.6940)
2023/09/27 23:21:45 - INFO - root -   Epoch: [27/200][200/346], lr: 0.00000014 	 loss = 0.3742(0.6901)
2023/09/27 23:23:10 - INFO - root -   Epoch: [27/200][220/346], lr: 0.00000014 	 loss = 1.1514(0.6933)
2023/09/27 23:24:05 - INFO - root -   Epoch: [27/200][240/346], lr: 0.00000014 	 loss = 0.5362(0.6946)
2023/09/27 23:25:09 - INFO - root -   Epoch: [27/200][260/346], lr: 0.00000014 	 loss = 0.4812(0.6913)
2023/09/27 23:26:31 - INFO - root -   Epoch: [27/200][280/346], lr: 0.00000014 	 loss = 0.8199(0.6993)
2023/09/27 23:27:27 - INFO - root -   Epoch: [27/200][300/346], lr: 0.00000014 	 loss = 0.6477(0.7048)
2023/09/27 23:28:43 - INFO - root -   Epoch: [27/200][320/346], lr: 0.00000014 	 loss = 0.5531(0.7032)
2023/09/27 23:29:24 - INFO - root -   Epoch: [27/200][340/346], lr: 0.00000014 	 loss = 0.8514(0.7033)
2023/09/27 23:29:28 - INFO - root -   Epoch: [27/200] 	 loss = 0.7023
2023/09/27 23:29:28 - INFO - root -   train_accuracy = 0.5766
2023/09/27 23:29:50 - INFO - root -   Epoch: [28/200][0/346], lr: 0.00000014 	 loss = 0.8536(0.8536)
2023/09/27 23:30:53 - INFO - root -   Epoch: [28/200][20/346], lr: 0.00000014 	 loss = 0.4767(0.6669)
2023/09/27 23:31:54 - INFO - root -   Epoch: [28/200][40/346], lr: 0.00000014 	 loss = 0.3725(0.6853)
2023/09/27 23:33:17 - INFO - root -   Epoch: [28/200][60/346], lr: 0.00000014 	 loss = 1.2993(0.6913)
2023/09/27 23:34:17 - INFO - root -   Epoch: [28/200][80/346], lr: 0.00000014 	 loss = 0.8425(0.7013)
2023/09/27 23:35:30 - INFO - root -   Epoch: [28/200][100/346], lr: 0.00000014 	 loss = 0.5614(0.6878)
2023/09/27 23:36:21 - INFO - root -   Epoch: [28/200][120/346], lr: 0.00000014 	 loss = 0.5163(0.7101)
2023/09/27 23:37:37 - INFO - root -   Epoch: [28/200][140/346], lr: 0.00000014 	 loss = 0.6540(0.7032)
2023/09/27 23:38:32 - INFO - root -   Epoch: [28/200][160/346], lr: 0.00000014 	 loss = 0.1080(0.6924)
2023/09/27 23:39:54 - INFO - root -   Epoch: [28/200][180/346], lr: 0.00000014 	 loss = 0.5990(0.6810)
2023/09/27 23:41:01 - INFO - root -   Epoch: [28/200][200/346], lr: 0.00000014 	 loss = 0.4944(0.6820)
2023/09/27 23:42:17 - INFO - root -   Epoch: [28/200][220/346], lr: 0.00000014 	 loss = 0.2145(0.6858)
2023/09/27 23:43:03 - INFO - root -   Epoch: [28/200][240/346], lr: 0.00000014 	 loss = 0.3059(0.6892)
2023/09/27 23:44:45 - INFO - root -   Epoch: [28/200][260/346], lr: 0.00000014 	 loss = 0.6028(0.6932)
2023/09/27 23:45:42 - INFO - root -   Epoch: [28/200][280/346], lr: 0.00000014 	 loss = 0.3535(0.6933)
2023/09/27 23:46:52 - INFO - root -   Epoch: [28/200][300/346], lr: 0.00000014 	 loss = 0.2223(0.6912)
2023/09/27 23:48:02 - INFO - root -   Epoch: [28/200][320/346], lr: 0.00000014 	 loss = 0.3081(0.6897)
2023/09/27 23:48:52 - INFO - root -   Epoch: [28/200][340/346], lr: 0.00000014 	 loss = 0.9254(0.6907)
2023/09/27 23:48:56 - INFO - root -   Epoch: [28/200] 	 loss = 0.6906
2023/09/27 23:48:56 - INFO - root -   train_accuracy = 0.5954
2023/09/27 23:49:26 - INFO - root -   Epoch: [29/200][0/346], lr: 0.00000014 	 loss = 0.2098(0.2098)
2023/09/27 23:50:20 - INFO - root -   Epoch: [29/200][20/346], lr: 0.00000014 	 loss = 0.7606(0.6585)
2023/09/27 23:51:45 - INFO - root -   Epoch: [29/200][40/346], lr: 0.00000014 	 loss = 0.8049(0.6937)
2023/09/27 23:52:50 - INFO - root -   Epoch: [29/200][60/346], lr: 0.00000014 	 loss = 1.0528(0.7126)
2023/09/27 23:53:52 - INFO - root -   Epoch: [29/200][80/346], lr: 0.00000014 	 loss = 1.0825(0.7407)
2023/09/27 23:54:59 - INFO - root -   Epoch: [29/200][100/346], lr: 0.00000014 	 loss = 1.0584(0.7242)
2023/09/27 23:55:58 - INFO - root -   Epoch: [29/200][120/346], lr: 0.00000014 	 loss = 0.5944(0.7183)
2023/09/27 23:57:07 - INFO - root -   Epoch: [29/200][140/346], lr: 0.00000014 	 loss = 1.6940(0.7280)
2023/09/27 23:58:06 - INFO - root -   Epoch: [29/200][160/346], lr: 0.00000014 	 loss = 0.2739(0.7166)
2023/09/27 23:59:15 - INFO - root -   Epoch: [29/200][180/346], lr: 0.00000014 	 loss = 0.5334(0.7049)
2023/09/28 00:00:30 - INFO - root -   Epoch: [29/200][200/346], lr: 0.00000014 	 loss = 0.5621(0.7143)
2023/09/28 00:01:35 - INFO - root -   Epoch: [29/200][220/346], lr: 0.00000014 	 loss = 0.1582(0.7072)
2023/09/28 00:02:50 - INFO - root -   Epoch: [29/200][240/346], lr: 0.00000014 	 loss = 0.5321(0.7012)
2023/09/28 00:03:41 - INFO - root -   Epoch: [29/200][260/346], lr: 0.00000014 	 loss = 0.8685(0.7009)
2023/09/28 00:04:50 - INFO - root -   Epoch: [29/200][280/346], lr: 0.00000014 	 loss = 0.3340(0.7036)
2023/09/28 00:06:09 - INFO - root -   Epoch: [29/200][300/346], lr: 0.00000014 	 loss = 0.2261(0.7001)
2023/09/28 00:06:57 - INFO - root -   Epoch: [29/200][320/346], lr: 0.00000014 	 loss = 0.5947(0.7056)
2023/09/28 00:07:57 - INFO - root -   Epoch: [29/200][340/346], lr: 0.00000014 	 loss = 1.3336(0.7008)
2023/09/28 00:07:59 - INFO - root -   Epoch: [29/200] 	 loss = 0.7002
2023/09/28 00:11:51 - INFO - root -   precision = 0.6782
2023/09/28 00:11:51 - INFO - root -   eval_loss = 0.6240
2023/09/28 00:11:52 - INFO - root -   train_accuracy = 0.5621
2023/09/28 00:12:14 - INFO - root -   Epoch: [30/200][0/346], lr: 0.00000014 	 loss = 0.5823(0.5823)
2023/09/28 00:13:15 - INFO - root -   Epoch: [30/200][20/346], lr: 0.00000014 	 loss = 0.2220(0.7280)
2023/09/28 00:14:32 - INFO - root -   Epoch: [30/200][40/346], lr: 0.00000014 	 loss = 0.4788(0.7450)
2023/09/28 00:15:41 - INFO - root -   Epoch: [30/200][60/346], lr: 0.00000014 	 loss = 0.5072(0.6838)
2023/09/28 00:16:33 - INFO - root -   Epoch: [30/200][80/346], lr: 0.00000014 	 loss = 0.9071(0.6737)
2023/09/28 00:18:07 - INFO - root -   Epoch: [30/200][100/346], lr: 0.00000014 	 loss = 0.7133(0.6726)
2023/09/28 00:18:58 - INFO - root -   Epoch: [30/200][120/346], lr: 0.00000014 	 loss = 0.5593(0.7054)
2023/09/28 00:20:15 - INFO - root -   Epoch: [30/200][140/346], lr: 0.00000014 	 loss = 0.5369(0.7022)
2023/09/28 00:21:19 - INFO - root -   Epoch: [30/200][160/346], lr: 0.00000014 	 loss = 0.1554(0.6944)
2023/09/28 00:22:12 - INFO - root -   Epoch: [30/200][180/346], lr: 0.00000014 	 loss = 0.6490(0.6861)
2023/09/28 00:23:29 - INFO - root -   Epoch: [30/200][200/346], lr: 0.00000014 	 loss = 1.1798(0.6948)
2023/09/28 00:24:47 - INFO - root -   Epoch: [30/200][220/346], lr: 0.00000014 	 loss = 0.8666(0.6929)
2023/09/28 00:25:40 - INFO - root -   Epoch: [30/200][240/346], lr: 0.00000014 	 loss = 0.6063(0.6913)
2023/09/28 00:27:16 - INFO - root -   Epoch: [30/200][260/346], lr: 0.00000014 	 loss = 0.8988(0.6928)
2023/09/28 00:28:15 - INFO - root -   Epoch: [30/200][280/346], lr: 0.00000014 	 loss = 0.9678(0.6966)
2023/09/28 00:29:18 - INFO - root -   Epoch: [30/200][300/346], lr: 0.00000014 	 loss = 0.7569(0.7001)
2023/09/28 00:30:37 - INFO - root -   Epoch: [30/200][320/346], lr: 0.00000014 	 loss = 0.5818(0.7025)
2023/09/28 00:31:19 - INFO - root -   Epoch: [30/200][340/346], lr: 0.00000014 	 loss = 0.7382(0.6992)
2023/09/28 00:31:23 - INFO - root -   Epoch: [30/200] 	 loss = 0.6984
2023/09/28 00:31:23 - INFO - root -   train_accuracy = 0.5795
2023/09/28 00:31:45 - INFO - root -   Epoch: [31/200][0/346], lr: 0.00000014 	 loss = 0.4411(0.4411)
2023/09/28 00:32:46 - INFO - root -   Epoch: [31/200][20/346], lr: 0.00000014 	 loss = 0.4140(0.6781)
2023/09/28 00:34:26 - INFO - root -   Epoch: [31/200][40/346], lr: 0.00000014 	 loss = 0.6697(0.6939)
2023/09/28 00:35:25 - INFO - root -   Epoch: [31/200][60/346], lr: 0.00000014 	 loss = 0.8712(0.7156)
2023/09/28 00:36:44 - INFO - root -   Epoch: [31/200][80/346], lr: 0.00000014 	 loss = 0.3990(0.6885)
2023/09/28 00:37:27 - INFO - root -   Epoch: [31/200][100/346], lr: 0.00000014 	 loss = 0.9453(0.7030)
2023/09/28 00:38:49 - INFO - root -   Epoch: [31/200][120/346], lr: 0.00000014 	 loss = 1.2346(0.7022)
2023/09/28 00:39:37 - INFO - root -   Epoch: [31/200][140/346], lr: 0.00000014 	 loss = 0.6787(0.6899)
2023/09/28 00:41:05 - INFO - root -   Epoch: [31/200][160/346], lr: 0.00000014 	 loss = 0.2124(0.6974)
2023/09/28 00:41:54 - INFO - root -   Epoch: [31/200][180/346], lr: 0.00000014 	 loss = 1.4025(0.6941)
2023/09/28 00:43:06 - INFO - root -   Epoch: [31/200][200/346], lr: 0.00000014 	 loss = 0.3654(0.6906)
2023/09/28 00:44:18 - INFO - root -   Epoch: [31/200][220/346], lr: 0.00000014 	 loss = 0.2000(0.6972)
2023/09/28 00:45:06 - INFO - root -   Epoch: [31/200][240/346], lr: 0.00000014 	 loss = 0.1434(0.6970)
2023/09/28 00:46:11 - INFO - root -   Epoch: [31/200][260/346], lr: 0.00000014 	 loss = 0.6513(0.7013)
2023/09/28 00:47:17 - INFO - root -   Epoch: [31/200][280/346], lr: 0.00000014 	 loss = 0.7019(0.7036)
2023/09/28 00:48:25 - INFO - root -   Epoch: [31/200][300/346], lr: 0.00000014 	 loss = 0.5235(0.7030)
2023/09/28 00:49:28 - INFO - root -   Epoch: [31/200][320/346], lr: 0.00000014 	 loss = 0.4216(0.7023)
2023/09/28 00:50:39 - INFO - root -   Epoch: [31/200][340/346], lr: 0.00000014 	 loss = 0.7466(0.6996)
2023/09/28 00:50:41 - INFO - root -   Epoch: [31/200] 	 loss = 0.7011
2023/09/28 00:50:41 - INFO - root -   train_accuracy = 0.5896
2023/09/28 00:51:03 - INFO - root -   Epoch: [32/200][0/346], lr: 0.00000015 	 loss = 0.3292(0.3292)
2023/09/28 00:52:24 - INFO - root -   Epoch: [32/200][20/346], lr: 0.00000015 	 loss = 0.9278(0.6864)
2023/09/28 00:53:15 - INFO - root -   Epoch: [32/200][40/346], lr: 0.00000015 	 loss = 0.3722(0.6769)
2023/09/28 00:54:22 - INFO - root -   Epoch: [32/200][60/346], lr: 0.00000015 	 loss = 0.6717(0.6339)
2023/09/28 00:55:25 - INFO - root -   Epoch: [32/200][80/346], lr: 0.00000015 	 loss = 0.4067(0.6313)
2023/09/28 00:56:39 - INFO - root -   Epoch: [32/200][100/346], lr: 0.00000015 	 loss = 0.5758(0.6444)
2023/09/28 00:57:31 - INFO - root -   Epoch: [32/200][120/346], lr: 0.00000015 	 loss = 0.3344(0.6706)
2023/09/28 00:58:40 - INFO - root -   Epoch: [32/200][140/346], lr: 0.00000015 	 loss = 0.2818(0.6661)
2023/09/28 00:59:35 - INFO - root -   Epoch: [32/200][160/346], lr: 0.00000015 	 loss = 0.8151(0.6815)
2023/09/28 01:00:54 - INFO - root -   Epoch: [32/200][180/346], lr: 0.00000015 	 loss = 0.6650(0.6739)
2023/09/28 01:01:55 - INFO - root -   Epoch: [32/200][200/346], lr: 0.00000015 	 loss = 0.2515(0.6769)
2023/09/28 01:03:09 - INFO - root -   Epoch: [32/200][220/346], lr: 0.00000015 	 loss = 0.3995(0.6690)
2023/09/28 01:04:00 - INFO - root -   Epoch: [32/200][240/346], lr: 0.00000015 	 loss = 0.8320(0.6699)
2023/09/28 01:05:08 - INFO - root -   Epoch: [32/200][260/346], lr: 0.00000015 	 loss = 0.2950(0.6729)
2023/09/28 01:06:01 - INFO - root -   Epoch: [32/200][280/346], lr: 0.00000015 	 loss = 0.5652(0.6801)
2023/09/28 01:07:30 - INFO - root -   Epoch: [32/200][300/346], lr: 0.00000015 	 loss = 0.8138(0.6831)
2023/09/28 01:08:21 - INFO - root -   Epoch: [32/200][320/346], lr: 0.00000015 	 loss = 0.2732(0.6879)
2023/09/28 01:09:16 - INFO - root -   Epoch: [32/200][340/346], lr: 0.00000015 	 loss = 0.7622(0.6819)
2023/09/28 01:09:18 - INFO - root -   Epoch: [32/200] 	 loss = 0.6806
2023/09/28 01:09:18 - INFO - root -   train_accuracy = 0.6142
2023/09/28 01:09:56 - INFO - root -   Epoch: [33/200][0/346], lr: 0.00000015 	 loss = 0.9111(0.9111)
2023/09/28 01:10:48 - INFO - root -   Epoch: [33/200][20/346], lr: 0.00000015 	 loss = 0.3940(0.6970)
2023/09/28 01:11:57 - INFO - root -   Epoch: [33/200][40/346], lr: 0.00000015 	 loss = 0.8181(0.6557)
2023/09/28 01:13:12 - INFO - root -   Epoch: [33/200][60/346], lr: 0.00000015 	 loss = 0.6109(0.6546)
2023/09/28 01:14:34 - INFO - root -   Epoch: [33/200][80/346], lr: 0.00000015 	 loss = 0.7743(0.6396)
2023/09/28 01:15:23 - INFO - root -   Epoch: [33/200][100/346], lr: 0.00000015 	 loss = 0.8615(0.6500)
2023/09/28 01:16:39 - INFO - root -   Epoch: [33/200][120/346], lr: 0.00000015 	 loss = 1.6109(0.6766)
2023/09/28 01:17:39 - INFO - root -   Epoch: [33/200][140/346], lr: 0.00000015 	 loss = 0.7491(0.6702)
2023/09/28 01:18:43 - INFO - root -   Epoch: [33/200][160/346], lr: 0.00000015 	 loss = 0.3244(0.6781)
2023/09/28 01:19:54 - INFO - root -   Epoch: [33/200][180/346], lr: 0.00000015 	 loss = 0.7876(0.6798)
2023/09/28 01:20:53 - INFO - root -   Epoch: [33/200][200/346], lr: 0.00000015 	 loss = 1.1083(0.6861)
2023/09/28 01:22:00 - INFO - root -   Epoch: [33/200][220/346], lr: 0.00000015 	 loss = 0.5027(0.6939)
2023/09/28 01:23:03 - INFO - root -   Epoch: [33/200][240/346], lr: 0.00000015 	 loss = 0.6518(0.6959)
2023/09/28 01:24:31 - INFO - root -   Epoch: [33/200][260/346], lr: 0.00000015 	 loss = 0.9171(0.6999)
2023/09/28 01:25:25 - INFO - root -   Epoch: [33/200][280/346], lr: 0.00000015 	 loss = 0.3398(0.7015)
2023/09/28 01:26:38 - INFO - root -   Epoch: [33/200][300/346], lr: 0.00000015 	 loss = 0.3716(0.7039)
2023/09/28 01:27:35 - INFO - root -   Epoch: [33/200][320/346], lr: 0.00000015 	 loss = 0.3932(0.7025)
2023/09/28 01:28:31 - INFO - root -   Epoch: [33/200][340/346], lr: 0.00000015 	 loss = 0.8478(0.7022)
2023/09/28 01:28:39 - INFO - root -   Epoch: [33/200] 	 loss = 0.7016
2023/09/28 01:28:39 - INFO - root -   train_accuracy = 0.5723
2023/09/28 01:29:09 - INFO - root -   Epoch: [34/200][0/346], lr: 0.00000015 	 loss = 0.5158(0.5158)
2023/09/28 01:30:17 - INFO - root -   Epoch: [34/200][20/346], lr: 0.00000015 	 loss = 0.8248(0.7283)
2023/09/28 01:31:10 - INFO - root -   Epoch: [34/200][40/346], lr: 0.00000015 	 loss = 1.2974(0.7220)
2023/09/28 01:32:28 - INFO - root -   Epoch: [34/200][60/346], lr: 0.00000015 	 loss = 0.5409(0.7045)
2023/09/28 01:33:22 - INFO - root -   Epoch: [34/200][80/346], lr: 0.00000015 	 loss = 0.6782(0.6858)
2023/09/28 01:34:45 - INFO - root -   Epoch: [34/200][100/346], lr: 0.00000015 	 loss = 0.3799(0.6830)
2023/09/28 01:35:50 - INFO - root -   Epoch: [34/200][120/346], lr: 0.00000015 	 loss = 0.7380(0.7035)
2023/09/28 01:37:04 - INFO - root -   Epoch: [34/200][140/346], lr: 0.00000015 	 loss = 0.8528(0.6942)
2023/09/28 01:37:55 - INFO - root -   Epoch: [34/200][160/346], lr: 0.00000015 	 loss = 0.3094(0.6933)
2023/09/28 01:39:17 - INFO - root -   Epoch: [34/200][180/346], lr: 0.00000015 	 loss = 0.7808(0.7010)
2023/09/28 01:40:29 - INFO - root -   Epoch: [34/200][200/346], lr: 0.00000015 	 loss = 0.4364(0.6910)
2023/09/28 01:41:56 - INFO - root -   Epoch: [34/200][220/346], lr: 0.00000015 	 loss = 0.2487(0.6926)
2023/09/28 01:42:54 - INFO - root -   Epoch: [34/200][240/346], lr: 0.00000015 	 loss = 0.4244(0.7024)
2023/09/28 01:44:10 - INFO - root -   Epoch: [34/200][260/346], lr: 0.00000015 	 loss = 0.5918(0.7038)
2023/09/28 01:45:04 - INFO - root -   Epoch: [34/200][280/346], lr: 0.00000015 	 loss = 0.3577(0.7042)
2023/09/28 01:46:25 - INFO - root -   Epoch: [34/200][300/346], lr: 0.00000015 	 loss = 1.2994(0.7019)
2023/09/28 01:47:24 - INFO - root -   Epoch: [34/200][320/346], lr: 0.00000015 	 loss = 0.5120(0.7036)
2023/09/28 01:48:22 - INFO - root -   Epoch: [34/200][340/346], lr: 0.00000015 	 loss = 0.4880(0.6973)
2023/09/28 01:48:25 - INFO - root -   Epoch: [34/200] 	 loss = 0.6941
2023/09/28 01:52:15 - INFO - root -   precision = 0.6609
2023/09/28 01:52:15 - INFO - root -   eval_loss = 0.6356
2023/09/28 01:52:16 - INFO - root -   train_accuracy = 0.5968
2023/09/28 01:52:38 - INFO - root -   Epoch: [35/200][0/346], lr: 0.00000015 	 loss = 0.5112(0.5112)
2023/09/28 01:53:46 - INFO - root -   Epoch: [35/200][20/346], lr: 0.00000015 	 loss = 0.7464(0.6267)
2023/09/28 01:55:04 - INFO - root -   Epoch: [35/200][40/346], lr: 0.00000015 	 loss = 0.2428(0.6516)
2023/09/28 01:55:56 - INFO - root -   Epoch: [35/200][60/346], lr: 0.00000015 	 loss = 0.9441(0.6317)
2023/09/28 01:57:12 - INFO - root -   Epoch: [35/200][80/346], lr: 0.00000015 	 loss = 0.9362(0.6546)
2023/09/28 01:58:05 - INFO - root -   Epoch: [35/200][100/346], lr: 0.00000015 	 loss = 0.3204(0.6358)
2023/09/28 01:59:25 - INFO - root -   Epoch: [35/200][120/346], lr: 0.00000015 	 loss = 0.6125(0.6479)
2023/09/28 02:00:27 - INFO - root -   Epoch: [35/200][140/346], lr: 0.00000015 	 loss = 0.8722(0.6520)
2023/09/28 02:01:40 - INFO - root -   Epoch: [35/200][160/346], lr: 0.00000015 	 loss = 0.1673(0.6535)
2023/09/28 02:02:38 - INFO - root -   Epoch: [35/200][180/346], lr: 0.00000015 	 loss = 0.9328(0.6605)
2023/09/28 02:04:00 - INFO - root -   Epoch: [35/200][200/346], lr: 0.00000015 	 loss = 0.7367(0.6646)
2023/09/28 02:04:51 - INFO - root -   Epoch: [35/200][220/346], lr: 0.00000015 	 loss = 0.7586(0.6719)
2023/09/28 02:06:20 - INFO - root -   Epoch: [35/200][240/346], lr: 0.00000015 	 loss = 0.5442(0.6725)
2023/09/28 02:07:06 - INFO - root -   Epoch: [35/200][260/346], lr: 0.00000015 	 loss = 0.5045(0.6798)
2023/09/28 02:08:15 - INFO - root -   Epoch: [35/200][280/346], lr: 0.00000015 	 loss = 0.1822(0.6856)
2023/09/28 02:09:21 - INFO - root -   Epoch: [35/200][300/346], lr: 0.00000015 	 loss = 0.3561(0.6883)
2023/09/28 02:10:45 - INFO - root -   Epoch: [35/200][320/346], lr: 0.00000015 	 loss = 0.4006(0.6820)
2023/09/28 02:11:32 - INFO - root -   Epoch: [35/200][340/346], lr: 0.00000015 	 loss = 0.4830(0.6801)
2023/09/28 02:11:35 - INFO - root -   Epoch: [35/200] 	 loss = 0.6813
2023/09/28 02:11:35 - INFO - root -   train_accuracy = 0.5939
2023/09/28 02:12:27 - INFO - root -   Epoch: [36/200][0/346], lr: 0.00000015 	 loss = 1.0066(1.0066)
2023/09/28 02:13:26 - INFO - root -   Epoch: [36/200][20/346], lr: 0.00000015 	 loss = 0.7408(0.7124)
2023/09/28 02:14:34 - INFO - root -   Epoch: [36/200][40/346], lr: 0.00000015 	 loss = 0.5508(0.6561)
2023/09/28 02:15:33 - INFO - root -   Epoch: [36/200][60/346], lr: 0.00000015 	 loss = 0.5873(0.6664)
2023/09/28 02:16:35 - INFO - root -   Epoch: [36/200][80/346], lr: 0.00000015 	 loss = 1.1702(0.7036)
2023/09/28 02:17:44 - INFO - root -   Epoch: [36/200][100/346], lr: 0.00000015 	 loss = 0.7638(0.7196)
2023/09/28 02:19:05 - INFO - root -   Epoch: [36/200][120/346], lr: 0.00000015 	 loss = 0.4671(0.7142)
2023/09/28 02:20:02 - INFO - root -   Epoch: [36/200][140/346], lr: 0.00000015 	 loss = 0.6551(0.7224)
2023/09/28 02:21:21 - INFO - root -   Epoch: [36/200][160/346], lr: 0.00000015 	 loss = 0.6900(0.7057)
2023/09/28 02:22:09 - INFO - root -   Epoch: [36/200][180/346], lr: 0.00000015 	 loss = 0.3884(0.7032)
2023/09/28 02:23:19 - INFO - root -   Epoch: [36/200][200/346], lr: 0.00000015 	 loss = 0.8992(0.7086)
2023/09/28 02:24:26 - INFO - root -   Epoch: [36/200][220/346], lr: 0.00000015 	 loss = 0.0861(0.7067)
2023/09/28 02:25:18 - INFO - root -   Epoch: [36/200][240/346], lr: 0.00000015 	 loss = 0.2053(0.7073)
2023/09/28 02:26:26 - INFO - root -   Epoch: [36/200][260/346], lr: 0.00000015 	 loss = 0.8572(0.7060)
2023/09/28 02:27:27 - INFO - root -   Epoch: [36/200][280/346], lr: 0.00000015 	 loss = 0.4149(0.7099)
2023/09/28 02:28:56 - INFO - root -   Epoch: [36/200][300/346], lr: 0.00000015 	 loss = 0.6222(0.7090)
2023/09/28 02:30:00 - INFO - root -   Epoch: [36/200][320/346], lr: 0.00000015 	 loss = 0.6196(0.7079)
2023/09/28 02:30:52 - INFO - root -   Epoch: [36/200][340/346], lr: 0.00000015 	 loss = 0.4354(0.7035)
2023/09/28 02:30:54 - INFO - root -   Epoch: [36/200] 	 loss = 0.7029
2023/09/28 02:30:54 - INFO - root -   train_accuracy = 0.5621
2023/09/28 02:31:15 - INFO - root -   Epoch: [37/200][0/346], lr: 0.00000015 	 loss = 0.6164(0.6164)
2023/09/28 02:32:30 - INFO - root -   Epoch: [37/200][20/346], lr: 0.00000015 	 loss = 0.6817(0.6827)
2023/09/28 02:33:21 - INFO - root -   Epoch: [37/200][40/346], lr: 0.00000015 	 loss = 0.6676(0.6521)
2023/09/28 02:34:45 - INFO - root -   Epoch: [37/200][60/346], lr: 0.00000015 	 loss = 1.0377(0.6964)
2023/09/28 02:35:44 - INFO - root -   Epoch: [37/200][80/346], lr: 0.00000015 	 loss = 0.8874(0.6880)
2023/09/28 02:37:04 - INFO - root -   Epoch: [37/200][100/346], lr: 0.00000015 	 loss = 0.7247(0.6944)
2023/09/28 02:38:06 - INFO - root -   Epoch: [37/200][120/346], lr: 0.00000015 	 loss = 0.5784(0.7199)
2023/09/28 02:39:26 - INFO - root -   Epoch: [37/200][140/346], lr: 0.00000015 	 loss = 0.4653(0.7122)
2023/09/28 02:40:18 - INFO - root -   Epoch: [37/200][160/346], lr: 0.00000015 	 loss = 0.9020(0.7171)
2023/09/28 02:41:45 - INFO - root -   Epoch: [37/200][180/346], lr: 0.00000015 	 loss = 0.3469(0.7162)
2023/09/28 02:42:31 - INFO - root -   Epoch: [37/200][200/346], lr: 0.00000015 	 loss = 0.2667(0.7237)
2023/09/28 02:43:56 - INFO - root -   Epoch: [37/200][220/346], lr: 0.00000015 	 loss = 0.3143(0.7195)
2023/09/28 02:44:47 - INFO - root -   Epoch: [37/200][240/346], lr: 0.00000015 	 loss = 0.3216(0.7146)
2023/09/28 02:46:04 - INFO - root -   Epoch: [37/200][260/346], lr: 0.00000015 	 loss = 0.4457(0.7119)
2023/09/28 02:47:01 - INFO - root -   Epoch: [37/200][280/346], lr: 0.00000015 	 loss = 0.5801(0.7259)
2023/09/28 02:48:21 - INFO - root -   Epoch: [37/200][300/346], lr: 0.00000015 	 loss = 0.7371(0.7229)
2023/09/28 02:49:19 - INFO - root -   Epoch: [37/200][320/346], lr: 0.00000015 	 loss = 0.7747(0.7237)
2023/09/28 02:50:16 - INFO - root -   Epoch: [37/200][340/346], lr: 0.00000015 	 loss = 1.1808(0.7201)
2023/09/28 02:50:18 - INFO - root -   Epoch: [37/200] 	 loss = 0.7208
2023/09/28 02:50:18 - INFO - root -   train_accuracy = 0.5650
2023/09/28 02:50:56 - INFO - root -   Epoch: [38/200][0/346], lr: 0.00000015 	 loss = 0.6581(0.6581)
2023/09/28 02:51:58 - INFO - root -   Epoch: [38/200][20/346], lr: 0.00000015 	 loss = 0.4496(0.6324)
2023/09/28 02:53:13 - INFO - root -   Epoch: [38/200][40/346], lr: 0.00000015 	 loss = 0.4501(0.7263)
2023/09/28 02:54:28 - INFO - root -   Epoch: [38/200][60/346], lr: 0.00000015 	 loss = 0.1464(0.6878)
2023/09/28 02:55:17 - INFO - root -   Epoch: [38/200][80/346], lr: 0.00000015 	 loss = 0.9225(0.6936)
2023/09/28 02:56:26 - INFO - root -   Epoch: [38/200][100/346], lr: 0.00000015 	 loss = 0.9620(0.6855)
2023/09/28 02:57:40 - INFO - root -   Epoch: [38/200][120/346], lr: 0.00000015 	 loss = 0.8412(0.6721)
2023/09/28 02:58:31 - INFO - root -   Epoch: [38/200][140/346], lr: 0.00000015 	 loss = 0.9831(0.6665)
2023/09/28 02:59:40 - INFO - root -   Epoch: [38/200][160/346], lr: 0.00000015 	 loss = 0.3366(0.6762)
2023/09/28 03:00:43 - INFO - root -   Epoch: [38/200][180/346], lr: 0.00000015 	 loss = 0.4204(0.6752)
2023/09/28 03:02:07 - INFO - root -   Epoch: [38/200][200/346], lr: 0.00000015 	 loss = 0.2585(0.6701)
2023/09/28 03:03:05 - INFO - root -   Epoch: [38/200][220/346], lr: 0.00000015 	 loss = 0.8272(0.6729)
2023/09/28 03:04:34 - INFO - root -   Epoch: [38/200][240/346], lr: 0.00000015 	 loss = 0.6461(0.6769)
2023/09/28 03:05:18 - INFO - root -   Epoch: [38/200][260/346], lr: 0.00000015 	 loss = 0.9685(0.6800)
2023/09/28 03:06:28 - INFO - root -   Epoch: [38/200][280/346], lr: 0.00000015 	 loss = 0.3931(0.6849)
2023/09/28 03:07:34 - INFO - root -   Epoch: [38/200][300/346], lr: 0.00000015 	 loss = 0.3532(0.6837)
2023/09/28 03:08:45 - INFO - root -   Epoch: [38/200][320/346], lr: 0.00000015 	 loss = 0.5604(0.6857)
2023/09/28 03:09:49 - INFO - root -   Epoch: [38/200][340/346], lr: 0.00000015 	 loss = 0.6408(0.6853)
2023/09/28 03:09:54 - INFO - root -   Epoch: [38/200] 	 loss = 0.6860
2023/09/28 03:09:54 - INFO - root -   train_accuracy = 0.6040
2023/09/28 03:10:16 - INFO - root -   Epoch: [39/200][0/346], lr: 0.00000016 	 loss = 0.4212(0.4212)
2023/09/28 03:11:32 - INFO - root -   Epoch: [39/200][20/346], lr: 0.00000016 	 loss = 0.2869(0.6442)
2023/09/28 03:12:30 - INFO - root -   Epoch: [39/200][40/346], lr: 0.00000016 	 loss = 0.4044(0.6608)
2023/09/28 03:13:58 - INFO - root -   Epoch: [39/200][60/346], lr: 0.00000016 	 loss = 0.6764(0.6569)
2023/09/28 03:14:55 - INFO - root -   Epoch: [39/200][80/346], lr: 0.00000016 	 loss = 0.5970(0.6771)
2023/09/28 03:15:57 - INFO - root -   Epoch: [39/200][100/346], lr: 0.00000016 	 loss = 0.6529(0.6803)
2023/09/28 03:16:57 - INFO - root -   Epoch: [39/200][120/346], lr: 0.00000016 	 loss = 0.9482(0.6915)
2023/09/28 03:18:29 - INFO - root -   Epoch: [39/200][140/346], lr: 0.00000016 	 loss = 0.3190(0.6772)
2023/09/28 03:19:26 - INFO - root -   Epoch: [39/200][160/346], lr: 0.00000016 	 loss = 0.4210(0.6765)
2023/09/28 03:20:27 - INFO - root -   Epoch: [39/200][180/346], lr: 0.00000016 	 loss = 1.2174(0.6793)
2023/09/28 03:21:17 - INFO - root -   Epoch: [39/200][200/346], lr: 0.00000016 	 loss = 0.3468(0.6773)
2023/09/28 03:22:30 - INFO - root -   Epoch: [39/200][220/346], lr: 0.00000016 	 loss = 0.7109(0.6763)
2023/09/28 03:23:52 - INFO - root -   Epoch: [39/200][240/346], lr: 0.00000016 	 loss = 0.4234(0.6777)
2023/09/28 03:24:44 - INFO - root -   Epoch: [39/200][260/346], lr: 0.00000016 	 loss = 0.7805(0.6840)
2023/09/28 03:26:09 - INFO - root -   Epoch: [39/200][280/346], lr: 0.00000016 	 loss = 0.2737(0.6839)
2023/09/28 03:27:04 - INFO - root -   Epoch: [39/200][300/346], lr: 0.00000016 	 loss = 0.5730(0.6832)
2023/09/28 03:28:34 - INFO - root -   Epoch: [39/200][320/346], lr: 0.00000016 	 loss = 0.6605(0.6865)
2023/09/28 03:29:15 - INFO - root -   Epoch: [39/200][340/346], lr: 0.00000016 	 loss = 0.7761(0.6791)
2023/09/28 03:29:20 - INFO - root -   Epoch: [39/200] 	 loss = 0.6827
2023/09/28 03:33:13 - INFO - root -   precision = 0.6897
2023/09/28 03:33:13 - INFO - root -   eval_loss = 0.6126
2023/09/28 03:33:14 - INFO - root -   train_accuracy = 0.5939
2023/09/28 03:33:36 - INFO - root -   Epoch: [40/200][0/346], lr: 0.00000016 	 loss = 0.5017(0.5017)
2023/09/28 03:34:50 - INFO - root -   Epoch: [40/200][20/346], lr: 0.00000016 	 loss = 0.4843(0.6216)
2023/09/28 03:36:00 - INFO - root -   Epoch: [40/200][40/346], lr: 0.00000016 	 loss = 0.5715(0.7019)
2023/09/28 03:36:56 - INFO - root -   Epoch: [40/200][60/346], lr: 0.00000016 	 loss = 1.1109(0.6839)
2023/09/28 03:38:13 - INFO - root -   Epoch: [40/200][80/346], lr: 0.00000016 	 loss = 0.4401(0.7144)
2023/09/28 03:39:08 - INFO - root -   Epoch: [40/200][100/346], lr: 0.00000016 	 loss = 0.4775(0.7072)
2023/09/28 03:40:30 - INFO - root -   Epoch: [40/200][120/346], lr: 0.00000016 	 loss = 0.7748(0.7253)
2023/09/28 03:41:23 - INFO - root -   Epoch: [40/200][140/346], lr: 0.00000016 	 loss = 0.5667(0.7159)
2023/09/28 03:42:44 - INFO - root -   Epoch: [40/200][160/346], lr: 0.00000016 	 loss = 0.2257(0.7097)
2023/09/28 03:43:46 - INFO - root -   Epoch: [40/200][180/346], lr: 0.00000016 	 loss = 0.6405(0.7060)
2023/09/28 03:45:02 - INFO - root -   Epoch: [40/200][200/346], lr: 0.00000016 	 loss = 0.2808(0.6991)
2023/09/28 03:45:45 - INFO - root -   Epoch: [40/200][220/346], lr: 0.00000016 	 loss = 0.6370(0.6998)
2023/09/28 03:47:11 - INFO - root -   Epoch: [40/200][240/346], lr: 0.00000016 	 loss = 0.3790(0.6934)
2023/09/28 03:48:02 - INFO - root -   Epoch: [40/200][260/346], lr: 0.00000016 	 loss = 0.6090(0.6953)
2023/09/28 03:49:36 - INFO - root -   Epoch: [40/200][280/346], lr: 0.00000016 	 loss = 0.9446(0.6991)
2023/09/28 03:50:30 - INFO - root -   Epoch: [40/200][300/346], lr: 0.00000016 	 loss = 0.8317(0.7023)
2023/09/28 03:51:47 - INFO - root -   Epoch: [40/200][320/346], lr: 0.00000016 	 loss = 0.4967(0.7048)
2023/09/28 03:52:30 - INFO - root -   Epoch: [40/200][340/346], lr: 0.00000016 	 loss = 1.2845(0.7030)
2023/09/28 03:52:37 - INFO - root -   Epoch: [40/200] 	 loss = 0.7009
2023/09/28 03:52:37 - INFO - root -   train_accuracy = 0.5621
2023/09/28 03:53:07 - INFO - root -   Epoch: [41/200][0/346], lr: 0.00000016 	 loss = 0.6098(0.6098)
2023/09/28 03:54:08 - INFO - root -   Epoch: [41/200][20/346], lr: 0.00000016 	 loss = 1.0181(0.6671)
2023/09/28 03:55:15 - INFO - root -   Epoch: [41/200][40/346], lr: 0.00000016 	 loss = 0.5575(0.6363)
2023/09/28 03:56:31 - INFO - root -   Epoch: [41/200][60/346], lr: 0.00000016 	 loss = 0.6286(0.6290)
2023/09/28 03:57:31 - INFO - root -   Epoch: [41/200][80/346], lr: 0.00000016 	 loss = 0.5174(0.6339)
2023/09/28 03:58:54 - INFO - root -   Epoch: [41/200][100/346], lr: 0.00000016 	 loss = 0.6766(0.6259)
2023/09/28 03:59:58 - INFO - root -   Epoch: [41/200][120/346], lr: 0.00000016 	 loss = 0.5574(0.6404)
2023/09/28 04:01:02 - INFO - root -   Epoch: [41/200][140/346], lr: 0.00000016 	 loss = 0.7034(0.6325)
2023/09/28 04:02:07 - INFO - root -   Epoch: [41/200][160/346], lr: 0.00000016 	 loss = 0.4384(0.6423)
2023/09/28 04:03:23 - INFO - root -   Epoch: [41/200][180/346], lr: 0.00000016 	 loss = 1.2575(0.6509)
2023/09/28 04:04:20 - INFO - root -   Epoch: [41/200][200/346], lr: 0.00000016 	 loss = 0.4845(0.6510)
2023/09/28 04:05:47 - INFO - root -   Epoch: [41/200][220/346], lr: 0.00000016 	 loss = 0.2142(0.6478)
2023/09/28 04:06:50 - INFO - root -   Epoch: [41/200][240/346], lr: 0.00000016 	 loss = 0.7639(0.6569)
2023/09/28 04:08:03 - INFO - root -   Epoch: [41/200][260/346], lr: 0.00000016 	 loss = 0.6575(0.6560)
2023/09/28 04:08:51 - INFO - root -   Epoch: [41/200][280/346], lr: 0.00000016 	 loss = 0.3181(0.6677)
2023/09/28 04:10:05 - INFO - root -   Epoch: [41/200][300/346], lr: 0.00000016 	 loss = 0.3075(0.6700)
2023/09/28 04:11:13 - INFO - root -   Epoch: [41/200][320/346], lr: 0.00000016 	 loss = 0.7136(0.6735)
2023/09/28 04:12:06 - INFO - root -   Epoch: [41/200][340/346], lr: 0.00000016 	 loss = 1.4211(0.6737)
2023/09/28 04:12:10 - INFO - root -   Epoch: [41/200] 	 loss = 0.6724
2023/09/28 04:12:10 - INFO - root -   train_accuracy = 0.5939
2023/09/28 04:12:33 - INFO - root -   Epoch: [42/200][0/346], lr: 0.00000016 	 loss = 0.1700(0.1700)
2023/09/28 04:13:44 - INFO - root -   Epoch: [42/200][20/346], lr: 0.00000016 	 loss = 0.4105(0.5457)
2023/09/28 04:14:57 - INFO - root -   Epoch: [42/200][40/346], lr: 0.00000016 	 loss = 1.0934(0.6304)
2023/09/28 04:16:06 - INFO - root -   Epoch: [42/200][60/346], lr: 0.00000016 	 loss = 0.3782(0.6107)
2023/09/28 04:17:02 - INFO - root -   Epoch: [42/200][80/346], lr: 0.00000016 	 loss = 0.9113(0.6293)
2023/09/28 04:18:22 - INFO - root -   Epoch: [42/200][100/346], lr: 0.00000016 	 loss = 0.5192(0.6542)
2023/09/28 04:19:27 - INFO - root -   Epoch: [42/200][120/346], lr: 0.00000016 	 loss = 0.4994(0.6575)
2023/09/28 04:20:36 - INFO - root -   Epoch: [42/200][140/346], lr: 0.00000016 	 loss = 1.1178(0.6657)
2023/09/28 04:21:45 - INFO - root -   Epoch: [42/200][160/346], lr: 0.00000016 	 loss = 0.2235(0.6737)
2023/09/28 04:22:48 - INFO - root -   Epoch: [42/200][180/346], lr: 0.00000016 	 loss = 1.2305(0.6744)
2023/09/28 04:24:10 - INFO - root -   Epoch: [42/200][200/346], lr: 0.00000016 	 loss = 0.5780(0.6716)
2023/09/28 04:25:01 - INFO - root -   Epoch: [42/200][220/346], lr: 0.00000016 	 loss = 0.4913(0.6739)
2023/09/28 04:26:22 - INFO - root -   Epoch: [42/200][240/346], lr: 0.00000016 	 loss = 0.7332(0.6698)
2023/09/28 04:27:12 - INFO - root -   Epoch: [42/200][260/346], lr: 0.00000016 	 loss = 0.1904(0.6636)
2023/09/28 04:28:45 - INFO - root -   Epoch: [42/200][280/346], lr: 0.00000016 	 loss = 0.5468(0.6687)
2023/09/28 04:29:39 - INFO - root -   Epoch: [42/200][300/346], lr: 0.00000016 	 loss = 0.6202(0.6700)
2023/09/28 04:31:10 - INFO - root -   Epoch: [42/200][320/346], lr: 0.00000016 	 loss = 0.5287(0.6695)
2023/09/28 04:31:41 - INFO - root -   Epoch: [42/200][340/346], lr: 0.00000016 	 loss = 1.0092(0.6680)
2023/09/28 04:31:44 - INFO - root -   Epoch: [42/200] 	 loss = 0.6673
2023/09/28 04:31:44 - INFO - root -   train_accuracy = 0.6040
2023/09/28 04:32:06 - INFO - root -   Epoch: [43/200][0/346], lr: 0.00000016 	 loss = 0.3938(0.3938)
2023/09/28 04:33:13 - INFO - root -   Epoch: [43/200][20/346], lr: 0.00000016 	 loss = 0.4183(0.5611)
2023/09/28 04:34:50 - INFO - root -   Epoch: [43/200][40/346], lr: 0.00000016 	 loss = 0.4390(0.6354)
2023/09/28 04:35:49 - INFO - root -   Epoch: [43/200][60/346], lr: 0.00000016 	 loss = 0.7174(0.6435)
2023/09/28 04:37:05 - INFO - root -   Epoch: [43/200][80/346], lr: 0.00000016 	 loss = 0.4767(0.6604)
2023/09/28 04:37:53 - INFO - root -   Epoch: [43/200][100/346], lr: 0.00000016 	 loss = 1.1031(0.6673)
2023/09/28 04:39:13 - INFO - root -   Epoch: [43/200][120/346], lr: 0.00000016 	 loss = 1.4514(0.6929)
2023/09/28 04:40:02 - INFO - root -   Epoch: [43/200][140/346], lr: 0.00000016 	 loss = 0.4641(0.6876)
2023/09/28 04:41:16 - INFO - root -   Epoch: [43/200][160/346], lr: 0.00000016 	 loss = 0.1852(0.6825)
2023/09/28 04:42:25 - INFO - root -   Epoch: [43/200][180/346], lr: 0.00000016 	 loss = 0.7263(0.6721)
2023/09/28 04:43:28 - INFO - root -   Epoch: [43/200][200/346], lr: 0.00000016 	 loss = 0.5785(0.6693)
2023/09/28 04:44:51 - INFO - root -   Epoch: [43/200][220/346], lr: 0.00000016 	 loss = 0.3934(0.6734)
2023/09/28 04:46:10 - INFO - root -   Epoch: [43/200][240/346], lr: 0.00000016 	 loss = 0.4437(0.6745)
2023/09/28 04:47:12 - INFO - root -   Epoch: [43/200][260/346], lr: 0.00000016 	 loss = 0.4760(0.6711)
2023/09/28 04:48:30 - INFO - root -   Epoch: [43/200][280/346], lr: 0.00000016 	 loss = 0.1697(0.6803)
2023/09/28 04:49:26 - INFO - root -   Epoch: [43/200][300/346], lr: 0.00000016 	 loss = 0.6318(0.6816)
2023/09/28 04:50:46 - INFO - root -   Epoch: [43/200][320/346], lr: 0.00000016 	 loss = 0.1791(0.6860)
2023/09/28 04:51:24 - INFO - root -   Epoch: [43/200][340/346], lr: 0.00000016 	 loss = 0.9114(0.6885)
2023/09/28 04:51:30 - INFO - root -   Epoch: [43/200] 	 loss = 0.6858
2023/09/28 04:51:30 - INFO - root -   train_accuracy = 0.6012
2023/09/28 04:51:52 - INFO - root -   Epoch: [44/200][0/346], lr: 0.00000016 	 loss = 0.5463(0.5463)
2023/09/28 04:53:10 - INFO - root -   Epoch: [44/200][20/346], lr: 0.00000016 	 loss = 0.4227(0.7137)
2023/09/28 04:54:08 - INFO - root -   Epoch: [44/200][40/346], lr: 0.00000016 	 loss = 1.8890(0.7270)
2023/09/28 04:55:25 - INFO - root -   Epoch: [44/200][60/346], lr: 0.00000016 	 loss = 1.2825(0.6972)
2023/09/28 04:56:21 - INFO - root -   Epoch: [44/200][80/346], lr: 0.00000016 	 loss = 1.3692(0.7071)
2023/09/28 04:57:36 - INFO - root -   Epoch: [44/200][100/346], lr: 0.00000016 	 loss = 0.8390(0.7123)
2023/09/28 04:58:30 - INFO - root -   Epoch: [44/200][120/346], lr: 0.00000016 	 loss = 0.3146(0.7057)
2023/09/28 05:00:00 - INFO - root -   Epoch: [44/200][140/346], lr: 0.00000016 	 loss = 0.6962(0.6924)
2023/09/28 05:00:56 - INFO - root -   Epoch: [44/200][160/346], lr: 0.00000016 	 loss = 0.3084(0.6907)
2023/09/28 05:02:17 - INFO - root -   Epoch: [44/200][180/346], lr: 0.00000016 	 loss = 0.8577(0.6990)
2023/09/28 05:03:17 - INFO - root -   Epoch: [44/200][200/346], lr: 0.00000016 	 loss = 0.4703(0.7061)
2023/09/28 05:04:32 - INFO - root -   Epoch: [44/200][220/346], lr: 0.00000016 	 loss = 0.5384(0.7093)
2023/09/28 05:05:23 - INFO - root -   Epoch: [44/200][240/346], lr: 0.00000016 	 loss = 0.3032(0.7021)
2023/09/28 05:06:38 - INFO - root -   Epoch: [44/200][260/346], lr: 0.00000016 	 loss = 0.6655(0.6972)
2023/09/28 05:07:40 - INFO - root -   Epoch: [44/200][280/346], lr: 0.00000016 	 loss = 0.5960(0.7020)
2023/09/28 05:08:43 - INFO - root -   Epoch: [44/200][300/346], lr: 0.00000016 	 loss = 0.4966(0.7007)
2023/09/28 05:09:55 - INFO - root -   Epoch: [44/200][320/346], lr: 0.00000016 	 loss = 0.5588(0.7046)
2023/09/28 05:10:49 - INFO - root -   Epoch: [44/200][340/346], lr: 0.00000016 	 loss = 0.9357(0.6989)
2023/09/28 05:10:51 - INFO - root -   Epoch: [44/200] 	 loss = 0.6970
2023/09/28 05:14:41 - INFO - root -   precision = 0.6839
2023/09/28 05:14:41 - INFO - root -   eval_loss = 0.6179
2023/09/28 05:14:42 - INFO - root -   train_accuracy = 0.5824
2023/09/28 05:15:13 - INFO - root -   Epoch: [45/200][0/346], lr: 0.00000016 	 loss = 0.4662(0.4662)
2023/09/28 05:16:10 - INFO - root -   Epoch: [45/200][20/346], lr: 0.00000016 	 loss = 0.5478(0.5948)
2023/09/28 05:17:17 - INFO - root -   Epoch: [45/200][40/346], lr: 0.00000016 	 loss = 0.4735(0.6016)
2023/09/28 05:18:28 - INFO - root -   Epoch: [45/200][60/346], lr: 0.00000016 	 loss = 0.2995(0.6194)
2023/09/28 05:19:27 - INFO - root -   Epoch: [45/200][80/346], lr: 0.00000016 	 loss = 1.3373(0.6575)
2023/09/28 05:20:36 - INFO - root -   Epoch: [45/200][100/346], lr: 0.00000016 	 loss = 0.4204(0.6494)
2023/09/28 05:22:13 - INFO - root -   Epoch: [45/200][120/346], lr: 0.00000016 	 loss = 0.4441(0.6639)
2023/09/28 05:23:04 - INFO - root -   Epoch: [45/200][140/346], lr: 0.00000016 	 loss = 0.8061(0.6699)
2023/09/28 05:24:20 - INFO - root -   Epoch: [45/200][160/346], lr: 0.00000016 	 loss = 0.4560(0.6756)
2023/09/28 05:25:26 - INFO - root -   Epoch: [45/200][180/346], lr: 0.00000016 	 loss = 0.8995(0.6778)
2023/09/28 05:26:40 - INFO - root -   Epoch: [45/200][200/346], lr: 0.00000016 	 loss = 0.4033(0.6786)
2023/09/28 05:27:44 - INFO - root -   Epoch: [45/200][220/346], lr: 0.00000016 	 loss = 0.2938(0.6864)
2023/09/28 05:28:54 - INFO - root -   Epoch: [45/200][240/346], lr: 0.00000016 	 loss = 0.2227(0.6808)
2023/09/28 05:29:56 - INFO - root -   Epoch: [45/200][260/346], lr: 0.00000016 	 loss = 0.4995(0.6795)
2023/09/28 05:31:03 - INFO - root -   Epoch: [45/200][280/346], lr: 0.00000016 	 loss = 0.5662(0.6903)
2023/09/28 05:32:05 - INFO - root -   Epoch: [45/200][300/346], lr: 0.00000016 	 loss = 0.4390(0.6916)
2023/09/28 05:33:13 - INFO - root -   Epoch: [45/200][320/346], lr: 0.00000016 	 loss = 0.2244(0.6861)
2023/09/28 05:34:08 - INFO - root -   Epoch: [45/200][340/346], lr: 0.00000016 	 loss = 0.9823(0.6863)
2023/09/28 05:34:10 - INFO - root -   Epoch: [45/200] 	 loss = 0.6844
2023/09/28 05:34:10 - INFO - root -   train_accuracy = 0.5910
2023/09/28 05:34:42 - INFO - root -   Epoch: [46/200][0/346], lr: 0.00000017 	 loss = 0.4719(0.4719)
2023/09/28 05:35:42 - INFO - root -   Epoch: [46/200][20/346], lr: 0.00000017 	 loss = 0.5355(0.6481)
2023/09/28 05:36:57 - INFO - root -   Epoch: [46/200][40/346], lr: 0.00000017 	 loss = 0.4422(0.6494)
2023/09/28 05:38:08 - INFO - root -   Epoch: [46/200][60/346], lr: 0.00000017 	 loss = 0.8496(0.6404)
2023/09/28 05:39:07 - INFO - root -   Epoch: [46/200][80/346], lr: 0.00000017 	 loss = 0.9398(0.6635)
2023/09/28 05:40:22 - INFO - root -   Epoch: [46/200][100/346], lr: 0.00000017 	 loss = 0.7600(0.6573)
2023/09/28 05:41:44 - INFO - root -   Epoch: [46/200][120/346], lr: 0.00000017 	 loss = 0.5736(0.6705)
2023/09/28 05:42:34 - INFO - root -   Epoch: [46/200][140/346], lr: 0.00000017 	 loss = 1.1098(0.6708)
2023/09/28 05:43:44 - INFO - root -   Epoch: [46/200][160/346], lr: 0.00000017 	 loss = 0.3260(0.6718)
2023/09/28 05:44:47 - INFO - root -   Epoch: [46/200][180/346], lr: 0.00000017 	 loss = 0.4571(0.6643)
2023/09/28 05:46:01 - INFO - root -   Epoch: [46/200][200/346], lr: 0.00000017 	 loss = 0.3410(0.6654)
2023/09/28 05:47:04 - INFO - root -   Epoch: [46/200][220/346], lr: 0.00000017 	 loss = 0.3239(0.6702)
2023/09/28 05:48:16 - INFO - root -   Epoch: [46/200][240/346], lr: 0.00000017 	 loss = 0.5215(0.6642)
2023/09/28 05:49:09 - INFO - root -   Epoch: [46/200][260/346], lr: 0.00000017 	 loss = 0.4942(0.6600)
2023/09/28 05:50:17 - INFO - root -   Epoch: [46/200][280/346], lr: 0.00000017 	 loss = 1.2617(0.6631)
2023/09/28 05:51:31 - INFO - root -   Epoch: [46/200][300/346], lr: 0.00000017 	 loss = 0.5533(0.6715)
2023/09/28 05:52:39 - INFO - root -   Epoch: [46/200][320/346], lr: 0.00000017 	 loss = 0.4830(0.6731)
2023/09/28 05:53:35 - INFO - root -   Epoch: [46/200][340/346], lr: 0.00000017 	 loss = 0.7515(0.6703)
2023/09/28 05:53:37 - INFO - root -   Epoch: [46/200] 	 loss = 0.6685
2023/09/28 05:53:37 - INFO - root -   train_accuracy = 0.6113
2023/09/28 05:54:17 - INFO - root -   Epoch: [47/200][0/346], lr: 0.00000017 	 loss = 0.5836(0.5836)
2023/09/28 05:55:32 - INFO - root -   Epoch: [47/200][20/346], lr: 0.00000017 	 loss = 0.5068(0.6872)
2023/09/28 05:56:20 - INFO - root -   Epoch: [47/200][40/346], lr: 0.00000017 	 loss = 0.7756(0.7303)
2023/09/28 05:57:47 - INFO - root -   Epoch: [47/200][60/346], lr: 0.00000017 	 loss = 0.9440(0.7064)
2023/09/28 05:58:49 - INFO - root -   Epoch: [47/200][80/346], lr: 0.00000017 	 loss = 0.8292(0.7152)
2023/09/28 06:00:07 - INFO - root -   Epoch: [47/200][100/346], lr: 0.00000017 	 loss = 1.4878(0.7009)
2023/09/28 06:00:49 - INFO - root -   Epoch: [47/200][120/346], lr: 0.00000017 	 loss = 0.9543(0.7156)
2023/09/28 06:02:07 - INFO - root -   Epoch: [47/200][140/346], lr: 0.00000017 	 loss = 1.0690(0.7112)
2023/09/28 06:03:02 - INFO - root -   Epoch: [47/200][160/346], lr: 0.00000017 	 loss = 0.4695(0.7196)
2023/09/28 06:04:12 - INFO - root -   Epoch: [47/200][180/346], lr: 0.00000017 	 loss = 0.7466(0.7115)
2023/09/28 06:05:27 - INFO - root -   Epoch: [47/200][200/346], lr: 0.00000017 	 loss = 0.2124(0.7012)
2023/09/28 06:06:21 - INFO - root -   Epoch: [47/200][220/346], lr: 0.00000017 	 loss = 0.4342(0.6993)
2023/09/28 06:07:39 - INFO - root -   Epoch: [47/200][240/346], lr: 0.00000017 	 loss = 0.3264(0.6919)
2023/09/28 06:08:44 - INFO - root -   Epoch: [47/200][260/346], lr: 0.00000017 	 loss = 0.4808(0.6877)
2023/09/28 06:10:00 - INFO - root -   Epoch: [47/200][280/346], lr: 0.00000017 	 loss = 0.4752(0.6820)
2023/09/28 06:10:57 - INFO - root -   Epoch: [47/200][300/346], lr: 0.00000017 	 loss = 0.4073(0.6796)
2023/09/28 06:12:13 - INFO - root -   Epoch: [47/200][320/346], lr: 0.00000017 	 loss = 0.4769(0.6799)
2023/09/28 06:13:09 - INFO - root -   Epoch: [47/200][340/346], lr: 0.00000017 	 loss = 1.0920(0.6795)
2023/09/28 06:13:15 - INFO - root -   Epoch: [47/200] 	 loss = 0.6788
2023/09/28 06:13:15 - INFO - root -   train_accuracy = 0.5939
2023/09/28 06:13:36 - INFO - root -   Epoch: [48/200][0/346], lr: 0.00000017 	 loss = 0.2981(0.2981)
2023/09/28 06:14:58 - INFO - root -   Epoch: [48/200][20/346], lr: 0.00000017 	 loss = 0.5746(0.6110)
2023/09/28 06:16:00 - INFO - root -   Epoch: [48/200][40/346], lr: 0.00000017 	 loss = 0.4693(0.6513)
2023/09/28 06:17:15 - INFO - root -   Epoch: [48/200][60/346], lr: 0.00000017 	 loss = 0.5891(0.6619)
2023/09/28 06:18:10 - INFO - root -   Epoch: [48/200][80/346], lr: 0.00000017 	 loss = 0.9314(0.6540)
2023/09/28 06:19:13 - INFO - root -   Epoch: [48/200][100/346], lr: 0.00000017 	 loss = 1.1967(0.6605)
2023/09/28 06:20:29 - INFO - root -   Epoch: [48/200][120/346], lr: 0.00000017 	 loss = 0.3679(0.6873)
2023/09/28 06:21:33 - INFO - root -   Epoch: [48/200][140/346], lr: 0.00000017 	 loss = 0.8887(0.6787)
2023/09/28 06:22:47 - INFO - root -   Epoch: [48/200][160/346], lr: 0.00000017 	 loss = 0.4886(0.6797)
2023/09/28 06:23:36 - INFO - root -   Epoch: [48/200][180/346], lr: 0.00000017 	 loss = 0.9903(0.6721)
2023/09/28 06:24:49 - INFO - root -   Epoch: [48/200][200/346], lr: 0.00000017 	 loss = 0.5098(0.6736)
2023/09/28 06:26:00 - INFO - root -   Epoch: [48/200][220/346], lr: 0.00000017 	 loss = 0.3192(0.6694)
2023/09/28 06:27:20 - INFO - root -   Epoch: [48/200][240/346], lr: 0.00000017 	 loss = 0.2801(0.6713)
2023/09/28 06:28:18 - INFO - root -   Epoch: [48/200][260/346], lr: 0.00000017 	 loss = 0.7896(0.6755)
2023/09/28 06:29:31 - INFO - root -   Epoch: [48/200][280/346], lr: 0.00000017 	 loss = 0.9356(0.6748)
2023/09/28 06:30:34 - INFO - root -   Epoch: [48/200][300/346], lr: 0.00000017 	 loss = 0.4479(0.6790)
2023/09/28 06:31:42 - INFO - root -   Epoch: [48/200][320/346], lr: 0.00000017 	 loss = 0.5742(0.6756)
2023/09/28 06:32:34 - INFO - root -   Epoch: [48/200][340/346], lr: 0.00000017 	 loss = 1.0623(0.6723)
2023/09/28 06:32:36 - INFO - root -   Epoch: [48/200] 	 loss = 0.6722
2023/09/28 06:32:36 - INFO - root -   train_accuracy = 0.6098
2023/09/28 06:32:59 - INFO - root -   Epoch: [49/200][0/346], lr: 0.00000017 	 loss = 0.6033(0.6033)
2023/09/28 06:34:12 - INFO - root -   Epoch: [49/200][20/346], lr: 0.00000017 	 loss = 0.4228(0.7043)
2023/09/28 06:35:11 - INFO - root -   Epoch: [49/200][40/346], lr: 0.00000017 	 loss = 0.4970(0.6919)
2023/09/28 06:36:15 - INFO - root -   Epoch: [49/200][60/346], lr: 0.00000017 	 loss = 0.7935(0.6617)
2023/09/28 06:37:35 - INFO - root -   Epoch: [49/200][80/346], lr: 0.00000017 	 loss = 1.0915(0.6646)
2023/09/28 06:38:27 - INFO - root -   Epoch: [49/200][100/346], lr: 0.00000017 	 loss = 0.8790(0.6723)
2023/09/28 06:39:43 - INFO - root -   Epoch: [49/200][120/346], lr: 0.00000017 	 loss = 0.5525(0.6745)
2023/09/28 06:40:48 - INFO - root -   Epoch: [49/200][140/346], lr: 0.00000017 	 loss = 0.8521(0.6761)
2023/09/28 06:41:56 - INFO - root -   Epoch: [49/200][160/346], lr: 0.00000017 	 loss = 0.2579(0.6733)
2023/09/28 06:43:15 - INFO - root -   Epoch: [49/200][180/346], lr: 0.00000017 	 loss = 0.5441(0.6711)
2023/09/28 06:43:58 - INFO - root -   Epoch: [49/200][200/346], lr: 0.00000017 	 loss = 0.4635(0.6651)
2023/09/28 06:45:05 - INFO - root -   Epoch: [49/200][220/346], lr: 0.00000017 	 loss = 0.2772(0.6658)
2023/09/28 06:46:14 - INFO - root -   Epoch: [49/200][240/346], lr: 0.00000017 	 loss = 0.3561(0.6627)
2023/09/28 06:47:24 - INFO - root -   Epoch: [49/200][260/346], lr: 0.00000017 	 loss = 0.6654(0.6673)
2023/09/28 06:48:15 - INFO - root -   Epoch: [49/200][280/346], lr: 0.00000017 	 loss = 0.4418(0.6746)
2023/09/28 06:49:25 - INFO - root -   Epoch: [49/200][300/346], lr: 0.00000017 	 loss = 0.3273(0.6721)
2023/09/28 06:50:31 - INFO - root -   Epoch: [49/200][320/346], lr: 0.00000017 	 loss = 0.4614(0.6714)
2023/09/28 06:51:26 - INFO - root -   Epoch: [49/200][340/346], lr: 0.00000017 	 loss = 1.0144(0.6752)
2023/09/28 06:51:36 - INFO - root -   Epoch: [49/200] 	 loss = 0.6777
2023/09/28 06:55:28 - INFO - root -   precision = 0.6724
2023/09/28 06:55:28 - INFO - root -   eval_loss = 0.6126
2023/09/28 06:55:29 - INFO - root -   train_accuracy = 0.6156
2023/09/28 06:55:51 - INFO - root -   Epoch: [50/200][0/346], lr: 0.00000017 	 loss = 0.4370(0.4370)
2023/09/28 06:56:54 - INFO - root -   Epoch: [50/200][20/346], lr: 0.00000017 	 loss = 1.0257(0.6320)
2023/09/28 06:57:52 - INFO - root -   Epoch: [50/200][40/346], lr: 0.00000017 	 loss = 0.9673(0.6394)
2023/09/28 06:59:09 - INFO - root -   Epoch: [50/200][60/346], lr: 0.00000017 	 loss = 0.7968(0.6541)
2023/09/28 07:00:12 - INFO - root -   Epoch: [50/200][80/346], lr: 0.00000017 	 loss = 0.6900(0.6559)
2023/09/28 07:01:27 - INFO - root -   Epoch: [50/200][100/346], lr: 0.00000017 	 loss = 0.6814(0.6672)
2023/09/28 07:02:19 - INFO - root -   Epoch: [50/200][120/346], lr: 0.00000017 	 loss = 0.4357(0.6692)
2023/09/28 07:03:34 - INFO - root -   Epoch: [50/200][140/346], lr: 0.00000017 	 loss = 0.7718(0.6768)
2023/09/28 07:04:32 - INFO - root -   Epoch: [50/200][160/346], lr: 0.00000017 	 loss = 0.4320(0.6727)
2023/09/28 07:05:49 - INFO - root -   Epoch: [50/200][180/346], lr: 0.00000017 	 loss = 0.3655(0.6660)
2023/09/28 07:06:55 - INFO - root -   Epoch: [50/200][200/346], lr: 0.00000017 	 loss = 0.4657(0.6602)
2023/09/28 07:07:59 - INFO - root -   Epoch: [50/200][220/346], lr: 0.00000017 	 loss = 0.3446(0.6642)
2023/09/28 07:09:14 - INFO - root -   Epoch: [50/200][240/346], lr: 0.00000017 	 loss = 1.0735(0.6661)
2023/09/28 07:10:10 - INFO - root -   Epoch: [50/200][260/346], lr: 0.00000017 	 loss = 0.4935(0.6607)
2023/09/28 07:11:26 - INFO - root -   Epoch: [50/200][280/346], lr: 0.00000017 	 loss = 0.6570(0.6629)
2023/09/28 07:12:17 - INFO - root -   Epoch: [50/200][300/346], lr: 0.00000017 	 loss = 0.7139(0.6661)
2023/09/28 07:13:34 - INFO - root -   Epoch: [50/200][320/346], lr: 0.00000017 	 loss = 0.6473(0.6622)
2023/09/28 07:14:19 - INFO - root -   Epoch: [50/200][340/346], lr: 0.00000017 	 loss = 1.6071(0.6614)
2023/09/28 07:14:27 - INFO - root -   Epoch: [50/200] 	 loss = 0.6606
2023/09/28 07:14:27 - INFO - root -   train_accuracy = 0.6113
2023/09/28 07:14:48 - INFO - root -   Epoch: [51/200][0/346], lr: 0.00000017 	 loss = 0.1523(0.1523)
2023/09/28 07:16:01 - INFO - root -   Epoch: [51/200][20/346], lr: 0.00000017 	 loss = 1.2061(0.5790)
2023/09/28 07:17:12 - INFO - root -   Epoch: [51/200][40/346], lr: 0.00000017 	 loss = 1.2176(0.6417)
2023/09/28 07:18:12 - INFO - root -   Epoch: [51/200][60/346], lr: 0.00000017 	 loss = 0.2160(0.6185)
2023/09/28 07:19:19 - INFO - root -   Epoch: [51/200][80/346], lr: 0.00000017 	 loss = 0.5544(0.6366)
2023/09/28 07:20:29 - INFO - root -   Epoch: [51/200][100/346], lr: 0.00000017 	 loss = 0.7539(0.6406)
2023/09/28 07:21:27 - INFO - root -   Epoch: [51/200][120/346], lr: 0.00000017 	 loss = 0.6599(0.6599)
2023/09/28 07:22:43 - INFO - root -   Epoch: [51/200][140/346], lr: 0.00000017 	 loss = 0.9235(0.6582)
2023/09/28 07:23:46 - INFO - root -   Epoch: [51/200][160/346], lr: 0.00000017 	 loss = 0.3607(0.6585)
2023/09/28 07:24:45 - INFO - root -   Epoch: [51/200][180/346], lr: 0.00000017 	 loss = 1.1670(0.6609)
2023/09/28 07:26:10 - INFO - root -   Epoch: [51/200][200/346], lr: 0.00000017 	 loss = 0.3255(0.6642)
2023/09/28 07:27:14 - INFO - root -   Epoch: [51/200][220/346], lr: 0.00000017 	 loss = 0.5559(0.6723)
2023/09/28 07:28:11 - INFO - root -   Epoch: [51/200][240/346], lr: 0.00000017 	 loss = 0.5876(0.6754)
2023/09/28 07:29:18 - INFO - root -   Epoch: [51/200][260/346], lr: 0.00000017 	 loss = 0.6545(0.6781)
2023/09/28 07:30:18 - INFO - root -   Epoch: [51/200][280/346], lr: 0.00000017 	 loss = 0.5085(0.6906)
2023/09/28 07:31:22 - INFO - root -   Epoch: [51/200][300/346], lr: 0.00000017 	 loss = 0.7180(0.6925)
2023/09/28 07:32:50 - INFO - root -   Epoch: [51/200][320/346], lr: 0.00000017 	 loss = 0.5742(0.6912)
2023/09/28 07:33:32 - INFO - root -   Epoch: [51/200][340/346], lr: 0.00000017 	 loss = 0.4980(0.6899)
2023/09/28 07:33:36 - INFO - root -   Epoch: [51/200] 	 loss = 0.6864
2023/09/28 07:33:36 - INFO - root -   train_accuracy = 0.5997
2023/09/28 07:34:05 - INFO - root -   Epoch: [52/200][0/346], lr: 0.00000017 	 loss = 0.5308(0.5308)
2023/09/28 07:35:09 - INFO - root -   Epoch: [52/200][20/346], lr: 0.00000017 	 loss = 0.5808(0.6273)
2023/09/28 07:36:20 - INFO - root -   Epoch: [52/200][40/346], lr: 0.00000017 	 loss = 0.4020(0.6109)
2023/09/28 07:37:13 - INFO - root -   Epoch: [52/200][60/346], lr: 0.00000017 	 loss = 0.5552(0.6175)
2023/09/28 07:38:40 - INFO - root -   Epoch: [52/200][80/346], lr: 0.00000017 	 loss = 1.4144(0.6431)
2023/09/28 07:39:34 - INFO - root -   Epoch: [52/200][100/346], lr: 0.00000017 	 loss = 0.9454(0.6413)
2023/09/28 07:40:37 - INFO - root -   Epoch: [52/200][120/346], lr: 0.00000017 	 loss = 0.4492(0.6555)
2023/09/28 07:41:48 - INFO - root -   Epoch: [52/200][140/346], lr: 0.00000017 	 loss = 1.1574(0.6580)
2023/09/28 07:43:09 - INFO - root -   Epoch: [52/200][160/346], lr: 0.00000017 	 loss = 0.4227(0.6600)
2023/09/28 07:44:02 - INFO - root -   Epoch: [52/200][180/346], lr: 0.00000017 	 loss = 0.6843(0.6582)
2023/09/28 07:45:17 - INFO - root -   Epoch: [52/200][200/346], lr: 0.00000017 	 loss = 0.5125(0.6571)
2023/09/28 07:46:07 - INFO - root -   Epoch: [52/200][220/346], lr: 0.00000017 	 loss = 0.1844(0.6500)
2023/09/28 07:47:08 - INFO - root -   Epoch: [52/200][240/346], lr: 0.00000017 	 loss = 0.3678(0.6535)
2023/09/28 07:48:20 - INFO - root -   Epoch: [52/200][260/346], lr: 0.00000017 	 loss = 1.0412(0.6519)
2023/09/28 07:49:42 - INFO - root -   Epoch: [52/200][280/346], lr: 0.00000017 	 loss = 1.2108(0.6588)
2023/09/28 07:50:50 - INFO - root -   Epoch: [52/200][300/346], lr: 0.00000017 	 loss = 0.3676(0.6594)
2023/09/28 07:52:11 - INFO - root -   Epoch: [52/200][320/346], lr: 0.00000017 	 loss = 0.5789(0.6616)
2023/09/28 07:52:47 - INFO - root -   Epoch: [52/200][340/346], lr: 0.00000017 	 loss = 1.2254(0.6589)
2023/09/28 07:52:51 - INFO - root -   Epoch: [52/200] 	 loss = 0.6594
2023/09/28 07:52:51 - INFO - root -   train_accuracy = 0.6185
2023/09/28 07:53:21 - INFO - root -   Epoch: [53/200][0/346], lr: 0.00000018 	 loss = 0.6329(0.6329)
2023/09/28 07:54:37 - INFO - root -   Epoch: [53/200][20/346], lr: 0.00000018 	 loss = 0.6280(0.7108)
2023/09/28 07:55:32 - INFO - root -   Epoch: [53/200][40/346], lr: 0.00000018 	 loss = 0.6982(0.6955)
2023/09/28 07:56:56 - INFO - root -   Epoch: [53/200][60/346], lr: 0.00000018 	 loss = 0.2178(0.6631)
2023/09/28 07:57:50 - INFO - root -   Epoch: [53/200][80/346], lr: 0.00000018 	 loss = 0.6145(0.6742)
2023/09/28 07:59:07 - INFO - root -   Epoch: [53/200][100/346], lr: 0.00000018 	 loss = 0.9856(0.6690)
2023/09/28 07:59:59 - INFO - root -   Epoch: [53/200][120/346], lr: 0.00000018 	 loss = 0.4824(0.6885)
2023/09/28 08:01:23 - INFO - root -   Epoch: [53/200][140/346], lr: 0.00000018 	 loss = 1.1051(0.6795)
2023/09/28 08:02:19 - INFO - root -   Epoch: [53/200][160/346], lr: 0.00000018 	 loss = 0.2166(0.6666)
2023/09/28 08:03:28 - INFO - root -   Epoch: [53/200][180/346], lr: 0.00000018 	 loss = 0.9552(0.6672)
2023/09/28 08:04:40 - INFO - root -   Epoch: [53/200][200/346], lr: 0.00000018 	 loss = 0.2136(0.6624)
2023/09/28 08:05:51 - INFO - root -   Epoch: [53/200][220/346], lr: 0.00000018 	 loss = 0.2681(0.6654)
2023/09/28 08:06:59 - INFO - root -   Epoch: [53/200][240/346], lr: 0.00000018 	 loss = 0.5209(0.6585)
2023/09/28 08:08:10 - INFO - root -   Epoch: [53/200][260/346], lr: 0.00000018 	 loss = 0.5617(0.6564)
2023/09/28 08:09:14 - INFO - root -   Epoch: [53/200][280/346], lr: 0.00000018 	 loss = 0.5937(0.6624)
2023/09/28 08:10:21 - INFO - root -   Epoch: [53/200][300/346], lr: 0.00000018 	 loss = 0.2047(0.6578)
2023/09/28 08:11:40 - INFO - root -   Epoch: [53/200][320/346], lr: 0.00000018 	 loss = 0.5005(0.6633)
2023/09/28 08:12:34 - INFO - root -   Epoch: [53/200][340/346], lr: 0.00000018 	 loss = 0.9429(0.6603)
2023/09/28 08:12:36 - INFO - root -   Epoch: [53/200] 	 loss = 0.6640
2023/09/28 08:12:36 - INFO - root -   train_accuracy = 0.6171
2023/09/28 08:13:08 - INFO - root -   Epoch: [54/200][0/346], lr: 0.00000018 	 loss = 0.6445(0.6445)
2023/09/28 08:14:10 - INFO - root -   Epoch: [54/200][20/346], lr: 0.00000018 	 loss = 0.6987(0.5811)
2023/09/28 08:15:11 - INFO - root -   Epoch: [54/200][40/346], lr: 0.00000018 	 loss = 0.7506(0.6644)
2023/09/28 08:16:24 - INFO - root -   Epoch: [54/200][60/346], lr: 0.00000018 	 loss = 0.6957(0.6210)
2023/09/28 08:17:24 - INFO - root -   Epoch: [54/200][80/346], lr: 0.00000018 	 loss = 0.4936(0.6241)
2023/09/28 08:18:27 - INFO - root -   Epoch: [54/200][100/346], lr: 0.00000018 	 loss = 0.5883(0.6241)
2023/09/28 08:19:40 - INFO - root -   Epoch: [54/200][120/346], lr: 0.00000018 	 loss = 0.9843(0.6539)
2023/09/28 08:20:35 - INFO - root -   Epoch: [54/200][140/346], lr: 0.00000018 	 loss = 0.6472(0.6486)
2023/09/28 08:21:46 - INFO - root -   Epoch: [54/200][160/346], lr: 0.00000018 	 loss = 0.4666(0.6546)
2023/09/28 08:22:49 - INFO - root -   Epoch: [54/200][180/346], lr: 0.00000018 	 loss = 0.6409(0.6598)
2023/09/28 08:24:03 - INFO - root -   Epoch: [54/200][200/346], lr: 0.00000018 	 loss = 0.2799(0.6490)
2023/09/28 08:25:03 - INFO - root -   Epoch: [54/200][220/346], lr: 0.00000018 	 loss = 0.2259(0.6595)
2023/09/28 08:26:25 - INFO - root -   Epoch: [54/200][240/346], lr: 0.00000018 	 loss = 0.2497(0.6574)
2023/09/28 08:27:16 - INFO - root -   Epoch: [54/200][260/346], lr: 0.00000018 	 loss = 0.4383(0.6625)
2023/09/28 08:28:28 - INFO - root -   Epoch: [54/200][280/346], lr: 0.00000018 	 loss = 0.7746(0.6673)
2023/09/28 08:29:31 - INFO - root -   Epoch: [54/200][300/346], lr: 0.00000018 	 loss = 0.7028(0.6652)
2023/09/28 08:30:53 - INFO - root -   Epoch: [54/200][320/346], lr: 0.00000018 	 loss = 0.4156(0.6660)
2023/09/28 08:31:53 - INFO - root -   Epoch: [54/200][340/346], lr: 0.00000018 	 loss = 0.9864(0.6658)
2023/09/28 08:32:01 - INFO - root -   Epoch: [54/200] 	 loss = 0.6637
2023/09/28 08:35:54 - INFO - root -   precision = 0.7126
2023/09/28 08:35:54 - INFO - root -   eval_loss = 0.6105
2023/09/28 08:35:55 - INFO - root -   train_accuracy = 0.6069
2023/09/28 08:36:17 - INFO - root -   Epoch: [55/200][0/346], lr: 0.00000018 	 loss = 0.4704(0.4704)
2023/09/28 08:37:31 - INFO - root -   Epoch: [55/200][20/346], lr: 0.00000018 	 loss = 0.8313(0.7117)
2023/09/28 08:38:45 - INFO - root -   Epoch: [55/200][40/346], lr: 0.00000018 	 loss = 0.8184(0.7362)
2023/09/28 08:40:01 - INFO - root -   Epoch: [55/200][60/346], lr: 0.00000018 	 loss = 0.3939(0.6984)
2023/09/28 08:41:14 - INFO - root -   Epoch: [55/200][80/346], lr: 0.00000018 	 loss = 0.6075(0.6926)
2023/09/28 08:42:22 - INFO - root -   Epoch: [55/200][100/346], lr: 0.00000018 	 loss = 0.7681(0.6865)
2023/09/28 08:43:18 - INFO - root -   Epoch: [55/200][120/346], lr: 0.00000018 	 loss = 0.2949(0.6871)
2023/09/28 08:44:27 - INFO - root -   Epoch: [55/200][140/346], lr: 0.00000018 	 loss = 1.0741(0.6849)
2023/09/28 08:45:18 - INFO - root -   Epoch: [55/200][160/346], lr: 0.00000018 	 loss = 0.3283(0.6813)
2023/09/28 08:46:39 - INFO - root -   Epoch: [55/200][180/346], lr: 0.00000018 	 loss = 0.8259(0.6818)
2023/09/28 08:47:40 - INFO - root -   Epoch: [55/200][200/346], lr: 0.00000018 	 loss = 0.2910(0.6768)
2023/09/28 08:48:51 - INFO - root -   Epoch: [55/200][220/346], lr: 0.00000018 	 loss = 0.3276(0.6785)
2023/09/28 08:49:45 - INFO - root -   Epoch: [55/200][240/346], lr: 0.00000018 	 loss = 0.8001(0.6759)
2023/09/28 08:50:59 - INFO - root -   Epoch: [55/200][260/346], lr: 0.00000018 	 loss = 0.4354(0.6728)
2023/09/28 08:52:19 - INFO - root -   Epoch: [55/200][280/346], lr: 0.00000018 	 loss = 0.5770(0.6695)
2023/09/28 08:53:09 - INFO - root -   Epoch: [55/200][300/346], lr: 0.00000018 	 loss = 0.9020(0.6732)
2023/09/28 08:54:24 - INFO - root -   Epoch: [55/200][320/346], lr: 0.00000018 	 loss = 0.7070(0.6734)
2023/09/28 08:55:25 - INFO - root -   Epoch: [55/200][340/346], lr: 0.00000018 	 loss = 1.1095(0.6681)
2023/09/28 08:55:27 - INFO - root -   Epoch: [55/200] 	 loss = 0.6666
2023/09/28 08:55:27 - INFO - root -   train_accuracy = 0.6040
2023/09/28 08:55:49 - INFO - root -   Epoch: [56/200][0/346], lr: 0.00000018 	 loss = 0.5144(0.5144)
2023/09/28 08:56:57 - INFO - root -   Epoch: [56/200][20/346], lr: 0.00000018 	 loss = 0.5882(0.5867)
2023/09/28 08:57:55 - INFO - root -   Epoch: [56/200][40/346], lr: 0.00000018 	 loss = 0.4514(0.5826)
2023/09/28 08:59:24 - INFO - root -   Epoch: [56/200][60/346], lr: 0.00000018 	 loss = 0.4028(0.6218)
2023/09/28 09:00:14 - INFO - root -   Epoch: [56/200][80/346], lr: 0.00000018 	 loss = 0.6409(0.6268)
2023/09/28 09:01:39 - INFO - root -   Epoch: [56/200][100/346], lr: 0.00000018 	 loss = 0.7598(0.6149)
2023/09/28 09:02:44 - INFO - root -   Epoch: [56/200][120/346], lr: 0.00000018 	 loss = 0.5866(0.6352)
2023/09/28 09:03:32 - INFO - root -   Epoch: [56/200][140/346], lr: 0.00000018 	 loss = 1.0036(0.6440)
2023/09/28 09:04:49 - INFO - root -   Epoch: [56/200][160/346], lr: 0.00000018 	 loss = 0.3262(0.6556)
2023/09/28 09:05:54 - INFO - root -   Epoch: [56/200][180/346], lr: 0.00000018 	 loss = 0.6645(0.6497)
2023/09/28 09:06:55 - INFO - root -   Epoch: [56/200][200/346], lr: 0.00000018 	 loss = 0.5280(0.6481)
2023/09/28 09:08:10 - INFO - root -   Epoch: [56/200][220/346], lr: 0.00000018 	 loss = 0.3452(0.6510)
2023/09/28 09:09:09 - INFO - root -   Epoch: [56/200][240/346], lr: 0.00000018 	 loss = 0.4245(0.6528)
2023/09/28 09:10:24 - INFO - root -   Epoch: [56/200][260/346], lr: 0.00000018 	 loss = 0.4513(0.6591)
2023/09/28 09:11:19 - INFO - root -   Epoch: [56/200][280/346], lr: 0.00000018 	 loss = 0.5552(0.6711)
2023/09/28 09:12:34 - INFO - root -   Epoch: [56/200][300/346], lr: 0.00000018 	 loss = 0.2847(0.6741)
2023/09/28 09:13:26 - INFO - root -   Epoch: [56/200][320/346], lr: 0.00000018 	 loss = 0.5366(0.6787)
2023/09/28 09:14:41 - INFO - root -   Epoch: [56/200][340/346], lr: 0.00000018 	 loss = 0.5243(0.6750)
2023/09/28 09:14:43 - INFO - root -   Epoch: [56/200] 	 loss = 0.6737
2023/09/28 09:14:43 - INFO - root -   train_accuracy = 0.6026
2023/09/28 09:15:14 - INFO - root -   Epoch: [57/200][0/346], lr: 0.00000018 	 loss = 0.7602(0.7602)
2023/09/28 09:16:15 - INFO - root -   Epoch: [57/200][20/346], lr: 0.00000018 	 loss = 0.5194(0.5777)
2023/09/28 09:17:31 - INFO - root -   Epoch: [57/200][40/346], lr: 0.00000018 	 loss = 1.3457(0.6463)
2023/09/28 09:18:27 - INFO - root -   Epoch: [57/200][60/346], lr: 0.00000018 	 loss = 0.5030(0.6113)
2023/09/28 09:19:47 - INFO - root -   Epoch: [57/200][80/346], lr: 0.00000018 	 loss = 1.1221(0.6368)
2023/09/28 09:20:49 - INFO - root -   Epoch: [57/200][100/346], lr: 0.00000018 	 loss = 1.2054(0.6420)
2023/09/28 09:22:17 - INFO - root -   Epoch: [57/200][120/346], lr: 0.00000018 	 loss = 0.6154(0.6657)
2023/09/28 09:23:08 - INFO - root -   Epoch: [57/200][140/346], lr: 0.00000018 	 loss = 1.1829(0.6656)
2023/09/28 09:24:34 - INFO - root -   Epoch: [57/200][160/346], lr: 0.00000018 	 loss = 0.1987(0.6626)
2023/09/28 09:25:21 - INFO - root -   Epoch: [57/200][180/346], lr: 0.00000018 	 loss = 0.8183(0.6539)
2023/09/28 09:26:37 - INFO - root -   Epoch: [57/200][200/346], lr: 0.00000018 	 loss = 0.6212(0.6566)
2023/09/28 09:27:38 - INFO - root -   Epoch: [57/200][220/346], lr: 0.00000018 	 loss = 0.3691(0.6561)
2023/09/28 09:28:48 - INFO - root -   Epoch: [57/200][240/346], lr: 0.00000018 	 loss = 0.5629(0.6532)
2023/09/28 09:29:49 - INFO - root -   Epoch: [57/200][260/346], lr: 0.00000018 	 loss = 0.5504(0.6637)
2023/09/28 09:30:59 - INFO - root -   Epoch: [57/200][280/346], lr: 0.00000018 	 loss = 0.5490(0.6710)
2023/09/28 09:31:58 - INFO - root -   Epoch: [57/200][300/346], lr: 0.00000018 	 loss = 0.8279(0.6752)
2023/09/28 09:32:57 - INFO - root -   Epoch: [57/200][320/346], lr: 0.00000018 	 loss = 0.4348(0.6734)
2023/09/28 09:33:58 - INFO - root -   Epoch: [57/200][340/346], lr: 0.00000018 	 loss = 0.6198(0.6716)
2023/09/28 09:34:03 - INFO - root -   Epoch: [57/200] 	 loss = 0.6739
2023/09/28 09:34:03 - INFO - root -   train_accuracy = 0.6156
2023/09/28 09:34:32 - INFO - root -   Epoch: [58/200][0/346], lr: 0.00000018 	 loss = 0.3887(0.3887)
2023/09/28 09:35:30 - INFO - root -   Epoch: [58/200][20/346], lr: 0.00000018 	 loss = 0.6969(0.6301)
2023/09/28 09:36:38 - INFO - root -   Epoch: [58/200][40/346], lr: 0.00000018 	 loss = 0.7120(0.6707)
2023/09/28 09:37:52 - INFO - root -   Epoch: [58/200][60/346], lr: 0.00000018 	 loss = 0.4386(0.6295)
2023/09/28 09:38:52 - INFO - root -   Epoch: [58/200][80/346], lr: 0.00000018 	 loss = 0.5500(0.6311)
2023/09/28 09:40:02 - INFO - root -   Epoch: [58/200][100/346], lr: 0.00000018 	 loss = 1.0035(0.6393)
2023/09/28 09:41:30 - INFO - root -   Epoch: [58/200][120/346], lr: 0.00000018 	 loss = 0.7429(0.6292)
2023/09/28 09:42:14 - INFO - root -   Epoch: [58/200][140/346], lr: 0.00000018 	 loss = 0.8229(0.6193)
2023/09/28 09:43:34 - INFO - root -   Epoch: [58/200][160/346], lr: 0.00000018 	 loss = 0.2917(0.6380)
2023/09/28 09:44:32 - INFO - root -   Epoch: [58/200][180/346], lr: 0.00000018 	 loss = 0.8201(0.6505)
2023/09/28 09:45:39 - INFO - root -   Epoch: [58/200][200/346], lr: 0.00000018 	 loss = 0.3593(0.6469)
2023/09/28 09:46:41 - INFO - root -   Epoch: [58/200][220/346], lr: 0.00000018 	 loss = 0.5029(0.6526)
2023/09/28 09:47:42 - INFO - root -   Epoch: [58/200][240/346], lr: 0.00000018 	 loss = 0.6947(0.6614)
2023/09/28 09:49:08 - INFO - root -   Epoch: [58/200][260/346], lr: 0.00000018 	 loss = 0.6554(0.6601)
2023/09/28 09:50:13 - INFO - root -   Epoch: [58/200][280/346], lr: 0.00000018 	 loss = 0.5382(0.6672)
2023/09/28 09:51:26 - INFO - root -   Epoch: [58/200][300/346], lr: 0.00000018 	 loss = 0.4689(0.6635)
2023/09/28 09:52:18 - INFO - root -   Epoch: [58/200][320/346], lr: 0.00000018 	 loss = 0.6152(0.6586)
2023/09/28 09:53:24 - INFO - root -   Epoch: [58/200][340/346], lr: 0.00000018 	 loss = 0.7373(0.6502)
2023/09/28 09:53:31 - INFO - root -   Epoch: [58/200] 	 loss = 0.6473
2023/09/28 09:53:31 - INFO - root -   train_accuracy = 0.6416
2023/09/28 09:53:53 - INFO - root -   Epoch: [59/200][0/346], lr: 0.00000018 	 loss = 0.8676(0.8676)
2023/09/28 09:55:24 - INFO - root -   Epoch: [59/200][20/346], lr: 0.00000018 	 loss = 0.7751(0.6560)
2023/09/28 09:56:15 - INFO - root -   Epoch: [59/200][40/346], lr: 0.00000018 	 loss = 0.5927(0.6609)
2023/09/28 09:57:23 - INFO - root -   Epoch: [59/200][60/346], lr: 0.00000018 	 loss = 0.4046(0.6383)
2023/09/28 09:58:23 - INFO - root -   Epoch: [59/200][80/346], lr: 0.00000018 	 loss = 0.4704(0.6448)
2023/09/28 09:59:37 - INFO - root -   Epoch: [59/200][100/346], lr: 0.00000018 	 loss = 0.3821(0.6409)
2023/09/28 10:00:37 - INFO - root -   Epoch: [59/200][120/346], lr: 0.00000018 	 loss = 0.3715(0.6681)
2023/09/28 10:02:04 - INFO - root -   Epoch: [59/200][140/346], lr: 0.00000018 	 loss = 0.6788(0.6599)
2023/09/28 10:02:56 - INFO - root -   Epoch: [59/200][160/346], lr: 0.00000018 	 loss = 0.2521(0.6523)
2023/09/28 10:04:11 - INFO - root -   Epoch: [59/200][180/346], lr: 0.00000018 	 loss = 1.3414(0.6605)
2023/09/28 10:05:15 - INFO - root -   Epoch: [59/200][200/346], lr: 0.00000018 	 loss = 0.4046(0.6608)
2023/09/28 10:06:29 - INFO - root -   Epoch: [59/200][220/346], lr: 0.00000018 	 loss = 0.1617(0.6629)
2023/09/28 10:07:32 - INFO - root -   Epoch: [59/200][240/346], lr: 0.00000018 	 loss = 0.2220(0.6537)
2023/09/28 10:08:52 - INFO - root -   Epoch: [59/200][260/346], lr: 0.00000018 	 loss = 0.3811(0.6547)
2023/09/28 10:09:56 - INFO - root -   Epoch: [59/200][280/346], lr: 0.00000018 	 loss = 0.6858(0.6655)
2023/09/28 10:11:00 - INFO - root -   Epoch: [59/200][300/346], lr: 0.00000018 	 loss = 0.2665(0.6646)
2023/09/28 10:12:03 - INFO - root -   Epoch: [59/200][320/346], lr: 0.00000018 	 loss = 0.5115(0.6661)
2023/09/28 10:13:07 - INFO - root -   Epoch: [59/200][340/346], lr: 0.00000018 	 loss = 0.7233(0.6585)
2023/09/28 10:13:11 - INFO - root -   Epoch: [59/200] 	 loss = 0.6587
2023/09/28 10:17:05 - INFO - root -   precision = 0.7069
2023/09/28 10:17:05 - INFO - root -   eval_loss = 0.6167
2023/09/28 10:17:06 - INFO - root -   train_accuracy = 0.6358
2023/09/28 10:17:36 - INFO - root -   Epoch: [60/200][0/346], lr: 0.00000019 	 loss = 0.5235(0.5235)
2023/09/28 10:18:42 - INFO - root -   Epoch: [60/200][20/346], lr: 0.00000019 	 loss = 0.8063(0.6293)
2023/09/28 10:19:56 - INFO - root -   Epoch: [60/200][40/346], lr: 0.00000019 	 loss = 0.5468(0.6786)
2023/09/28 10:20:51 - INFO - root -   Epoch: [60/200][60/346], lr: 0.00000019 	 loss = 0.5782(0.6402)
2023/09/28 10:22:16 - INFO - root -   Epoch: [60/200][80/346], lr: 0.00000019 	 loss = 1.3116(0.6373)
2023/09/28 10:23:08 - INFO - root -   Epoch: [60/200][100/346], lr: 0.00000019 	 loss = 0.6926(0.6441)
2023/09/28 10:24:27 - INFO - root -   Epoch: [60/200][120/346], lr: 0.00000019 	 loss = 0.4903(0.6633)
2023/09/28 10:25:11 - INFO - root -   Epoch: [60/200][140/346], lr: 0.00000019 	 loss = 0.7910(0.6639)
2023/09/28 10:26:42 - INFO - root -   Epoch: [60/200][160/346], lr: 0.00000019 	 loss = 0.5577(0.6729)
2023/09/28 10:27:40 - INFO - root -   Epoch: [60/200][180/346], lr: 0.00000019 	 loss = 0.7354(0.6649)
2023/09/28 10:28:40 - INFO - root -   Epoch: [60/200][200/346], lr: 0.00000019 	 loss = 0.4527(0.6657)
2023/09/28 10:30:00 - INFO - root -   Epoch: [60/200][220/346], lr: 0.00000019 	 loss = 0.2530(0.6714)
2023/09/28 10:30:49 - INFO - root -   Epoch: [60/200][240/346], lr: 0.00000019 	 loss = 0.8230(0.6796)
2023/09/28 10:32:01 - INFO - root -   Epoch: [60/200][260/346], lr: 0.00000019 	 loss = 0.7573(0.6854)
2023/09/28 10:32:56 - INFO - root -   Epoch: [60/200][280/346], lr: 0.00000019 	 loss = 0.4369(0.6788)
2023/09/28 10:33:54 - INFO - root -   Epoch: [60/200][300/346], lr: 0.00000019 	 loss = 0.4105(0.6739)
2023/09/28 10:35:04 - INFO - root -   Epoch: [60/200][320/346], lr: 0.00000019 	 loss = 0.5173(0.6780)
2023/09/28 10:36:06 - INFO - root -   Epoch: [60/200][340/346], lr: 0.00000019 	 loss = 0.6365(0.6799)
2023/09/28 10:36:08 - INFO - root -   Epoch: [60/200] 	 loss = 0.6796
2023/09/28 10:36:08 - INFO - root -   train_accuracy = 0.6127
2023/09/28 10:36:30 - INFO - root -   Epoch: [61/200][0/346], lr: 0.00000019 	 loss = 0.4209(0.4209)
2023/09/28 10:37:43 - INFO - root -   Epoch: [61/200][20/346], lr: 0.00000019 	 loss = 0.2523(0.5165)
2023/09/28 10:38:41 - INFO - root -   Epoch: [61/200][40/346], lr: 0.00000019 	 loss = 0.6587(0.5536)
2023/09/28 10:40:09 - INFO - root -   Epoch: [61/200][60/346], lr: 0.00000019 	 loss = 1.1467(0.5703)
2023/09/28 10:40:52 - INFO - root -   Epoch: [61/200][80/346], lr: 0.00000019 	 loss = 1.1719(0.5734)
2023/09/28 10:42:09 - INFO - root -   Epoch: [61/200][100/346], lr: 0.00000019 	 loss = 0.6928(0.6065)
2023/09/28 10:43:09 - INFO - root -   Epoch: [61/200][120/346], lr: 0.00000019 	 loss = 1.0451(0.6373)
2023/09/28 10:44:32 - INFO - root -   Epoch: [61/200][140/346], lr: 0.00000019 	 loss = 0.7424(0.6349)
2023/09/28 10:45:24 - INFO - root -   Epoch: [61/200][160/346], lr: 0.00000019 	 loss = 0.4838(0.6397)
2023/09/28 10:46:46 - INFO - root -   Epoch: [61/200][180/346], lr: 0.00000019 	 loss = 0.6323(0.6353)
2023/09/28 10:47:34 - INFO - root -   Epoch: [61/200][200/346], lr: 0.00000019 	 loss = 0.2311(0.6343)
2023/09/28 10:48:58 - INFO - root -   Epoch: [61/200][220/346], lr: 0.00000019 	 loss = 0.5524(0.6440)
2023/09/28 10:50:02 - INFO - root -   Epoch: [61/200][240/346], lr: 0.00000019 	 loss = 0.2695(0.6446)
2023/09/28 10:51:27 - INFO - root -   Epoch: [61/200][260/346], lr: 0.00000019 	 loss = 0.6407(0.6485)
2023/09/28 10:52:25 - INFO - root -   Epoch: [61/200][280/346], lr: 0.00000019 	 loss = 0.5896(0.6582)
2023/09/28 10:53:43 - INFO - root -   Epoch: [61/200][300/346], lr: 0.00000019 	 loss = 0.2099(0.6491)
2023/09/28 10:54:27 - INFO - root -   Epoch: [61/200][320/346], lr: 0.00000019 	 loss = 0.3921(0.6494)
2023/09/28 10:55:30 - INFO - root -   Epoch: [61/200][340/346], lr: 0.00000019 	 loss = 1.1528(0.6542)
2023/09/28 10:55:32 - INFO - root -   Epoch: [61/200] 	 loss = 0.6548
2023/09/28 10:55:32 - INFO - root -   train_accuracy = 0.6373
2023/09/28 10:55:54 - INFO - root -   Epoch: [62/200][0/346], lr: 0.00000019 	 loss = 0.1569(0.1569)
2023/09/28 10:57:02 - INFO - root -   Epoch: [62/200][20/346], lr: 0.00000019 	 loss = 0.7263(0.5881)
2023/09/28 10:58:01 - INFO - root -   Epoch: [62/200][40/346], lr: 0.00000019 	 loss = 0.5327(0.6304)
2023/09/28 10:59:14 - INFO - root -   Epoch: [62/200][60/346], lr: 0.00000019 	 loss = 0.6107(0.6064)
2023/09/28 11:00:32 - INFO - root -   Epoch: [62/200][80/346], lr: 0.00000019 	 loss = 0.8341(0.6036)
2023/09/28 11:01:30 - INFO - root -   Epoch: [62/200][100/346], lr: 0.00000019 	 loss = 0.1732(0.6012)
2023/09/28 11:02:53 - INFO - root -   Epoch: [62/200][120/346], lr: 0.00000019 	 loss = 1.0275(0.6286)
2023/09/28 11:03:37 - INFO - root -   Epoch: [62/200][140/346], lr: 0.00000019 	 loss = 1.1721(0.6143)
2023/09/28 11:05:07 - INFO - root -   Epoch: [62/200][160/346], lr: 0.00000019 	 loss = 0.1014(0.6268)
2023/09/28 11:05:52 - INFO - root -   Epoch: [62/200][180/346], lr: 0.00000019 	 loss = 0.9499(0.6241)
2023/09/28 11:07:16 - INFO - root -   Epoch: [62/200][200/346], lr: 0.00000019 	 loss = 0.1851(0.6254)
2023/09/28 11:08:09 - INFO - root -   Epoch: [62/200][220/346], lr: 0.00000019 	 loss = 0.4485(0.6342)
2023/09/28 11:09:34 - INFO - root -   Epoch: [62/200][240/346], lr: 0.00000019 	 loss = 0.3903(0.6372)
2023/09/28 11:10:36 - INFO - root -   Epoch: [62/200][260/346], lr: 0.00000019 	 loss = 0.4439(0.6367)
2023/09/28 11:11:48 - INFO - root -   Epoch: [62/200][280/346], lr: 0.00000019 	 loss = 1.1669(0.6485)
2023/09/28 11:12:39 - INFO - root -   Epoch: [62/200][300/346], lr: 0.00000019 	 loss = 0.1838(0.6438)
2023/09/28 11:14:02 - INFO - root -   Epoch: [62/200][320/346], lr: 0.00000019 	 loss = 0.3708(0.6480)
2023/09/28 11:14:47 - INFO - root -   Epoch: [62/200][340/346], lr: 0.00000019 	 loss = 0.7292(0.6482)
2023/09/28 11:14:52 - INFO - root -   Epoch: [62/200] 	 loss = 0.6507
2023/09/28 11:14:52 - INFO - root -   train_accuracy = 0.6431
2023/09/28 11:15:21 - INFO - root -   Epoch: [63/200][0/346], lr: 0.00000019 	 loss = 0.5311(0.5311)
2023/09/28 11:16:21 - INFO - root -   Epoch: [63/200][20/346], lr: 0.00000019 	 loss = 0.4855(0.6730)
2023/09/28 11:17:45 - INFO - root -   Epoch: [63/200][40/346], lr: 0.00000019 	 loss = 0.8736(0.6716)
2023/09/28 11:18:39 - INFO - root -   Epoch: [63/200][60/346], lr: 0.00000019 	 loss = 0.3712(0.6456)
2023/09/28 11:19:52 - INFO - root -   Epoch: [63/200][80/346], lr: 0.00000019 	 loss = 0.6868(0.6562)
2023/09/28 11:21:04 - INFO - root -   Epoch: [63/200][100/346], lr: 0.00000019 	 loss = 1.0238(0.6422)
2023/09/28 11:22:17 - INFO - root -   Epoch: [63/200][120/346], lr: 0.00000019 	 loss = 0.4624(0.6684)
2023/09/28 11:23:26 - INFO - root -   Epoch: [63/200][140/346], lr: 0.00000019 	 loss = 0.5316(0.6609)
2023/09/28 11:24:20 - INFO - root -   Epoch: [63/200][160/346], lr: 0.00000019 	 loss = 0.6867(0.6751)
2023/09/28 11:25:34 - INFO - root -   Epoch: [63/200][180/346], lr: 0.00000019 	 loss = 0.2938(0.6663)
2023/09/28 11:26:28 - INFO - root -   Epoch: [63/200][200/346], lr: 0.00000019 	 loss = 0.1593(0.6509)
2023/09/28 11:27:48 - INFO - root -   Epoch: [63/200][220/346], lr: 0.00000019 	 loss = 1.0714(0.6600)
2023/09/28 11:28:54 - INFO - root -   Epoch: [63/200][240/346], lr: 0.00000019 	 loss = 0.3851(0.6613)
2023/09/28 11:30:05 - INFO - root -   Epoch: [63/200][260/346], lr: 0.00000019 	 loss = 0.7015(0.6637)
2023/09/28 11:31:08 - INFO - root -   Epoch: [63/200][280/346], lr: 0.00000019 	 loss = 0.5294(0.6700)
2023/09/28 11:32:21 - INFO - root -   Epoch: [63/200][300/346], lr: 0.00000019 	 loss = 0.5275(0.6722)
2023/09/28 11:33:22 - INFO - root -   Epoch: [63/200][320/346], lr: 0.00000019 	 loss = 0.2830(0.6774)
2023/09/28 11:34:23 - INFO - root -   Epoch: [63/200][340/346], lr: 0.00000019 	 loss = 1.3674(0.6775)
2023/09/28 11:34:28 - INFO - root -   Epoch: [63/200] 	 loss = 0.6786
2023/09/28 11:34:28 - INFO - root -   train_accuracy = 0.6012
2023/09/28 11:34:58 - INFO - root -   Epoch: [64/200][0/346], lr: 0.00000019 	 loss = 0.2711(0.2711)
2023/09/28 11:35:50 - INFO - root -   Epoch: [64/200][20/346], lr: 0.00000019 	 loss = 0.8708(0.6585)
2023/09/28 11:36:57 - INFO - root -   Epoch: [64/200][40/346], lr: 0.00000019 	 loss = 1.0464(0.6484)
2023/09/28 11:38:21 - INFO - root -   Epoch: [64/200][60/346], lr: 0.00000019 	 loss = 0.6467(0.6213)
2023/09/28 11:39:14 - INFO - root -   Epoch: [64/200][80/346], lr: 0.00000019 	 loss = 0.9591(0.6290)
2023/09/28 11:40:30 - INFO - root -   Epoch: [64/200][100/346], lr: 0.00000019 	 loss = 0.8271(0.6446)
2023/09/28 11:41:34 - INFO - root -   Epoch: [64/200][120/346], lr: 0.00000019 	 loss = 0.2529(0.6574)
2023/09/28 11:42:59 - INFO - root -   Epoch: [64/200][140/346], lr: 0.00000019 	 loss = 1.1419(0.6594)
2023/09/28 11:43:56 - INFO - root -   Epoch: [64/200][160/346], lr: 0.00000019 	 loss = 0.2081(0.6562)
2023/09/28 11:45:10 - INFO - root -   Epoch: [64/200][180/346], lr: 0.00000019 	 loss = 1.2734(0.6567)
2023/09/28 11:46:07 - INFO - root -   Epoch: [64/200][200/346], lr: 0.00000019 	 loss = 0.9605(0.6546)
2023/09/28 11:47:29 - INFO - root -   Epoch: [64/200][220/346], lr: 0.00000019 	 loss = 0.1788(0.6486)
2023/09/28 11:48:31 - INFO - root -   Epoch: [64/200][240/346], lr: 0.00000019 	 loss = 0.2063(0.6509)
2023/09/28 11:49:31 - INFO - root -   Epoch: [64/200][260/346], lr: 0.00000019 	 loss = 0.9365(0.6600)
2023/09/28 11:50:53 - INFO - root -   Epoch: [64/200][280/346], lr: 0.00000019 	 loss = 0.7277(0.6686)
2023/09/28 11:51:45 - INFO - root -   Epoch: [64/200][300/346], lr: 0.00000019 	 loss = 0.5753(0.6779)
2023/09/28 11:53:14 - INFO - root -   Epoch: [64/200][320/346], lr: 0.00000019 	 loss = 0.7581(0.6751)
2023/09/28 11:54:02 - INFO - root -   Epoch: [64/200][340/346], lr: 0.00000019 	 loss = 1.2589(0.6738)
2023/09/28 11:54:09 - INFO - root -   Epoch: [64/200] 	 loss = 0.6723
2023/09/28 11:58:03 - INFO - root -   precision = 0.7011
2023/09/28 11:58:03 - INFO - root -   eval_loss = 0.6013
2023/09/28 11:58:04 - INFO - root -   train_accuracy = 0.6127
2023/09/28 11:58:26 - INFO - root -   Epoch: [65/200][0/346], lr: 0.00000019 	 loss = 0.3632(0.3632)
2023/09/28 11:59:35 - INFO - root -   Epoch: [65/200][20/346], lr: 0.00000019 	 loss = 0.6955(0.5058)
2023/09/28 12:00:49 - INFO - root -   Epoch: [65/200][40/346], lr: 0.00000019 	 loss = 0.6543(0.5741)
2023/09/28 12:01:54 - INFO - root -   Epoch: [65/200][60/346], lr: 0.00000019 	 loss = 0.3224(0.5848)
2023/09/28 12:02:56 - INFO - root -   Epoch: [65/200][80/346], lr: 0.00000019 	 loss = 0.4889(0.6082)
2023/09/28 12:04:12 - INFO - root -   Epoch: [65/200][100/346], lr: 0.00000019 	 loss = 0.8641(0.6240)
2023/09/28 12:04:56 - INFO - root -   Epoch: [65/200][120/346], lr: 0.00000019 	 loss = 0.4230(0.6359)
2023/09/28 12:05:59 - INFO - root -   Epoch: [65/200][140/346], lr: 0.00000019 	 loss = 0.3110(0.6284)
2023/09/28 12:07:06 - INFO - root -   Epoch: [65/200][160/346], lr: 0.00000019 	 loss = 0.5918(0.6364)
2023/09/28 12:08:10 - INFO - root -   Epoch: [65/200][180/346], lr: 0.00000019 	 loss = 0.5357(0.6387)
2023/09/28 12:09:20 - INFO - root -   Epoch: [65/200][200/346], lr: 0.00000019 	 loss = 0.3064(0.6435)
2023/09/28 12:10:22 - INFO - root -   Epoch: [65/200][220/346], lr: 0.00000019 	 loss = 0.4468(0.6540)
2023/09/28 12:11:34 - INFO - root -   Epoch: [65/200][240/346], lr: 0.00000019 	 loss = 0.2240(0.6560)
2023/09/28 12:12:35 - INFO - root -   Epoch: [65/200][260/346], lr: 0.00000019 	 loss = 0.6865(0.6507)
2023/09/28 12:13:33 - INFO - root -   Epoch: [65/200][280/346], lr: 0.00000019 	 loss = 0.6956(0.6547)
2023/09/28 12:14:32 - INFO - root -   Epoch: [65/200][300/346], lr: 0.00000019 	 loss = 0.6075(0.6485)
2023/09/28 12:15:55 - INFO - root -   Epoch: [65/200][320/346], lr: 0.00000019 	 loss = 0.5145(0.6480)
2023/09/28 12:16:36 - INFO - root -   Epoch: [65/200][340/346], lr: 0.00000019 	 loss = 0.8527(0.6463)
2023/09/28 12:16:42 - INFO - root -   Epoch: [65/200] 	 loss = 0.6467
2023/09/28 12:16:42 - INFO - root -   train_accuracy = 0.6402
2023/09/28 12:17:03 - INFO - root -   Epoch: [66/200][0/346], lr: 0.00000019 	 loss = 0.2680(0.2680)
2023/09/28 12:18:03 - INFO - root -   Epoch: [66/200][20/346], lr: 0.00000019 	 loss = 0.5029(0.5682)
2023/09/28 12:19:27 - INFO - root -   Epoch: [66/200][40/346], lr: 0.00000019 	 loss = 0.6676(0.6249)
2023/09/28 12:20:18 - INFO - root -   Epoch: [66/200][60/346], lr: 0.00000019 	 loss = 0.4869(0.5882)
2023/09/28 12:21:35 - INFO - root -   Epoch: [66/200][80/346], lr: 0.00000019 	 loss = 0.7792(0.5968)
2023/09/28 12:22:33 - INFO - root -   Epoch: [66/200][100/346], lr: 0.00000019 	 loss = 0.6984(0.6306)
2023/09/28 12:23:38 - INFO - root -   Epoch: [66/200][120/346], lr: 0.00000019 	 loss = 0.9476(0.6535)
2023/09/28 12:25:00 - INFO - root -   Epoch: [66/200][140/346], lr: 0.00000019 	 loss = 1.3060(0.6367)
2023/09/28 12:25:51 - INFO - root -   Epoch: [66/200][160/346], lr: 0.00000019 	 loss = 0.3124(0.6463)
2023/09/28 12:27:11 - INFO - root -   Epoch: [66/200][180/346], lr: 0.00000019 	 loss = 0.9891(0.6458)
2023/09/28 12:27:57 - INFO - root -   Epoch: [66/200][200/346], lr: 0.00000019 	 loss = 0.4725(0.6494)
2023/09/28 12:29:12 - INFO - root -   Epoch: [66/200][220/346], lr: 0.00000019 	 loss = 0.8785(0.6561)
2023/09/28 12:30:17 - INFO - root -   Epoch: [66/200][240/346], lr: 0.00000019 	 loss = 0.4465(0.6501)
2023/09/28 12:31:31 - INFO - root -   Epoch: [66/200][260/346], lr: 0.00000019 	 loss = 0.4464(0.6469)
2023/09/28 12:32:26 - INFO - root -   Epoch: [66/200][280/346], lr: 0.00000019 	 loss = 0.4461(0.6545)
2023/09/28 12:33:41 - INFO - root -   Epoch: [66/200][300/346], lr: 0.00000019 	 loss = 0.2626(0.6519)
2023/09/28 12:35:27 - INFO - root -   Epoch: [66/200][320/346], lr: 0.00000019 	 loss = 0.5467(0.6503)
2023/09/28 12:35:57 - INFO - root -   Epoch: [66/200][340/346], lr: 0.00000019 	 loss = 0.6583(0.6492)
2023/09/28 12:36:01 - INFO - root -   Epoch: [66/200] 	 loss = 0.6478
2023/09/28 12:36:01 - INFO - root -   train_accuracy = 0.6474
2023/09/28 12:36:47 - INFO - root -   Epoch: [67/200][0/346], lr: 0.00000020 	 loss = 0.5632(0.5632)
2023/09/28 12:37:46 - INFO - root -   Epoch: [67/200][20/346], lr: 0.00000020 	 loss = 0.4908(0.5547)
2023/09/28 12:39:09 - INFO - root -   Epoch: [67/200][40/346], lr: 0.00000020 	 loss = 1.2711(0.6534)
2023/09/28 12:40:07 - INFO - root -   Epoch: [67/200][60/346], lr: 0.00000020 	 loss = 0.4935(0.6214)
2023/09/28 12:41:31 - INFO - root -   Epoch: [67/200][80/346], lr: 0.00000020 	 loss = 0.6768(0.6435)
2023/09/28 12:42:26 - INFO - root -   Epoch: [67/200][100/346], lr: 0.00000020 	 loss = 0.9998(0.6419)
2023/09/28 12:43:34 - INFO - root -   Epoch: [67/200][120/346], lr: 0.00000020 	 loss = 0.8580(0.6561)
2023/09/28 12:44:18 - INFO - root -   Epoch: [67/200][140/346], lr: 0.00000020 	 loss = 1.0082(0.6570)
2023/09/28 12:45:36 - INFO - root -   Epoch: [67/200][160/346], lr: 0.00000020 	 loss = 0.5500(0.6677)
2023/09/28 12:46:37 - INFO - root -   Epoch: [67/200][180/346], lr: 0.00000020 	 loss = 1.1761(0.6758)
2023/09/28 12:47:55 - INFO - root -   Epoch: [67/200][200/346], lr: 0.00000020 	 loss = 0.5169(0.6675)
2023/09/28 12:49:00 - INFO - root -   Epoch: [67/200][220/346], lr: 0.00000020 	 loss = 0.2266(0.6644)
2023/09/28 12:49:50 - INFO - root -   Epoch: [67/200][240/346], lr: 0.00000020 	 loss = 0.3447(0.6627)
2023/09/28 12:51:00 - INFO - root -   Epoch: [67/200][260/346], lr: 0.00000020 	 loss = 0.6003(0.6621)
2023/09/28 12:52:05 - INFO - root -   Epoch: [67/200][280/346], lr: 0.00000020 	 loss = 0.7201(0.6691)
2023/09/28 12:53:24 - INFO - root -   Epoch: [67/200][300/346], lr: 0.00000020 	 loss = 0.6278(0.6672)
2023/09/28 12:54:09 - INFO - root -   Epoch: [67/200][320/346], lr: 0.00000020 	 loss = 0.7815(0.6640)
2023/09/28 12:55:20 - INFO - root -   Epoch: [67/200][340/346], lr: 0.00000020 	 loss = 0.6015(0.6578)
2023/09/28 12:55:22 - INFO - root -   Epoch: [67/200] 	 loss = 0.6562
2023/09/28 12:55:22 - INFO - root -   train_accuracy = 0.6214
2023/09/28 12:55:44 - INFO - root -   Epoch: [68/200][0/346], lr: 0.00000020 	 loss = 0.1713(0.1713)
2023/09/28 12:56:53 - INFO - root -   Epoch: [68/200][20/346], lr: 0.00000020 	 loss = 0.5725(0.5493)
2023/09/28 12:57:59 - INFO - root -   Epoch: [68/200][40/346], lr: 0.00000020 	 loss = 0.7712(0.5914)
2023/09/28 12:58:59 - INFO - root -   Epoch: [68/200][60/346], lr: 0.00000020 	 loss = 0.2862(0.5932)
2023/09/28 13:00:12 - INFO - root -   Epoch: [68/200][80/346], lr: 0.00000020 	 loss = 1.3364(0.6053)
2023/09/28 13:01:22 - INFO - root -   Epoch: [68/200][100/346], lr: 0.00000020 	 loss = 0.6536(0.6162)
2023/09/28 13:02:36 - INFO - root -   Epoch: [68/200][120/346], lr: 0.00000020 	 loss = 0.5563(0.6429)
2023/09/28 13:03:36 - INFO - root -   Epoch: [68/200][140/346], lr: 0.00000020 	 loss = 0.9048(0.6323)
2023/09/28 13:04:53 - INFO - root -   Epoch: [68/200][160/346], lr: 0.00000020 	 loss = 0.2147(0.6384)
2023/09/28 13:05:48 - INFO - root -   Epoch: [68/200][180/346], lr: 0.00000020 	 loss = 1.2486(0.6314)
2023/09/28 13:06:54 - INFO - root -   Epoch: [68/200][200/346], lr: 0.00000020 	 loss = 0.2220(0.6361)
2023/09/28 13:07:45 - INFO - root -   Epoch: [68/200][220/346], lr: 0.00000020 	 loss = 0.2115(0.6461)
2023/09/28 13:09:11 - INFO - root -   Epoch: [68/200][240/346], lr: 0.00000020 	 loss = 0.3661(0.6481)
2023/09/28 13:09:58 - INFO - root -   Epoch: [68/200][260/346], lr: 0.00000020 	 loss = 0.4597(0.6443)
2023/09/28 13:11:21 - INFO - root -   Epoch: [68/200][280/346], lr: 0.00000020 	 loss = 0.7621(0.6542)
2023/09/28 13:12:18 - INFO - root -   Epoch: [68/200][300/346], lr: 0.00000020 	 loss = 0.1860(0.6482)
2023/09/28 13:13:27 - INFO - root -   Epoch: [68/200][320/346], lr: 0.00000020 	 loss = 0.8701(0.6467)
2023/09/28 13:14:24 - INFO - root -   Epoch: [68/200][340/346], lr: 0.00000020 	 loss = 1.3022(0.6415)
2023/09/28 13:14:29 - INFO - root -   Epoch: [68/200] 	 loss = 0.6429
2023/09/28 13:14:29 - INFO - root -   train_accuracy = 0.6199
2023/09/28 13:14:51 - INFO - root -   Epoch: [69/200][0/346], lr: 0.00000020 	 loss = 0.2336(0.2336)
2023/09/28 13:15:59 - INFO - root -   Epoch: [69/200][20/346], lr: 0.00000020 	 loss = 0.4899(0.6050)
2023/09/28 13:17:03 - INFO - root -   Epoch: [69/200][40/346], lr: 0.00000020 	 loss = 1.0126(0.6361)
2023/09/28 13:18:11 - INFO - root -   Epoch: [69/200][60/346], lr: 0.00000020 	 loss = 0.5117(0.5925)
2023/09/28 13:19:23 - INFO - root -   Epoch: [69/200][80/346], lr: 0.00000020 	 loss = 0.6879(0.6377)
2023/09/28 13:20:18 - INFO - root -   Epoch: [69/200][100/346], lr: 0.00000020 	 loss = 0.5251(0.6373)
2023/09/28 13:21:20 - INFO - root -   Epoch: [69/200][120/346], lr: 0.00000020 	 loss = 0.4649(0.6575)
2023/09/28 13:22:26 - INFO - root -   Epoch: [69/200][140/346], lr: 0.00000020 	 loss = 0.7833(0.6580)
2023/09/28 13:23:49 - INFO - root -   Epoch: [69/200][160/346], lr: 0.00000020 	 loss = 0.1539(0.6644)
2023/09/28 13:24:45 - INFO - root -   Epoch: [69/200][180/346], lr: 0.00000020 	 loss = 0.8070(0.6556)
2023/09/28 13:26:08 - INFO - root -   Epoch: [69/200][200/346], lr: 0.00000020 	 loss = 0.4800(0.6526)
2023/09/28 13:26:56 - INFO - root -   Epoch: [69/200][220/346], lr: 0.00000020 	 loss = 0.3066(0.6515)
2023/09/28 13:28:13 - INFO - root -   Epoch: [69/200][240/346], lr: 0.00000020 	 loss = 0.3833(0.6467)
2023/09/28 13:29:16 - INFO - root -   Epoch: [69/200][260/346], lr: 0.00000020 	 loss = 0.7263(0.6506)
2023/09/28 13:30:39 - INFO - root -   Epoch: [69/200][280/346], lr: 0.00000020 	 loss = 0.4717(0.6602)
2023/09/28 13:31:47 - INFO - root -   Epoch: [69/200][300/346], lr: 0.00000020 	 loss = 0.3807(0.6637)
2023/09/28 13:33:00 - INFO - root -   Epoch: [69/200][320/346], lr: 0.00000020 	 loss = 0.5011(0.6627)
2023/09/28 13:33:46 - INFO - root -   Epoch: [69/200][340/346], lr: 0.00000020 	 loss = 0.9202(0.6599)
2023/09/28 13:33:48 - INFO - root -   Epoch: [69/200] 	 loss = 0.6578
2023/09/28 13:37:40 - INFO - root -   precision = 0.6552
2023/09/28 13:37:40 - INFO - root -   eval_loss = 0.6278
2023/09/28 13:37:41 - INFO - root -   train_accuracy = 0.6344
2023/09/28 13:38:19 - INFO - root -   Epoch: [70/200][0/346], lr: 0.00000020 	 loss = 0.7389(0.7389)
2023/09/28 13:39:18 - INFO - root -   Epoch: [70/200][20/346], lr: 0.00000020 	 loss = 0.6734(0.7101)
2023/09/28 13:40:34 - INFO - root -   Epoch: [70/200][40/346], lr: 0.00000020 	 loss = 0.9723(0.7303)
2023/09/28 13:41:35 - INFO - root -   Epoch: [70/200][60/346], lr: 0.00000020 	 loss = 0.2692(0.6997)
2023/09/28 13:42:48 - INFO - root -   Epoch: [70/200][80/346], lr: 0.00000020 	 loss = 0.3005(0.6887)
2023/09/28 13:43:57 - INFO - root -   Epoch: [70/200][100/346], lr: 0.00000020 	 loss = 0.6556(0.6824)
2023/09/28 13:44:50 - INFO - root -   Epoch: [70/200][120/346], lr: 0.00000020 	 loss = 0.4428(0.6798)
2023/09/28 13:46:18 - INFO - root -   Epoch: [70/200][140/346], lr: 0.00000020 	 loss = 0.7012(0.6694)
2023/09/28 13:47:21 - INFO - root -   Epoch: [70/200][160/346], lr: 0.00000020 	 loss = 0.1811(0.6747)
2023/09/28 13:48:30 - INFO - root -   Epoch: [70/200][180/346], lr: 0.00000020 	 loss = 1.2090(0.6608)
2023/09/28 13:49:39 - INFO - root -   Epoch: [70/200][200/346], lr: 0.00000020 	 loss = 0.3313(0.6580)
2023/09/28 13:50:46 - INFO - root -   Epoch: [70/200][220/346], lr: 0.00000020 	 loss = 0.4687(0.6686)
2023/09/28 13:51:52 - INFO - root -   Epoch: [70/200][240/346], lr: 0.00000020 	 loss = 0.3205(0.6632)
2023/09/28 13:52:50 - INFO - root -   Epoch: [70/200][260/346], lr: 0.00000020 	 loss = 0.7871(0.6730)
2023/09/28 13:53:52 - INFO - root -   Epoch: [70/200][280/346], lr: 0.00000020 	 loss = 0.5144(0.6797)
2023/09/28 13:54:57 - INFO - root -   Epoch: [70/200][300/346], lr: 0.00000020 	 loss = 0.4956(0.6808)
2023/09/28 13:56:08 - INFO - root -   Epoch: [70/200][320/346], lr: 0.00000020 	 loss = 0.6120(0.6872)
2023/09/28 13:57:06 - INFO - root -   Epoch: [70/200][340/346], lr: 0.00000020 	 loss = 1.1732(0.6822)
2023/09/28 13:57:08 - INFO - root -   Epoch: [70/200] 	 loss = 0.6817
2023/09/28 13:57:08 - INFO - root -   train_accuracy = 0.6026
2023/09/28 13:57:40 - INFO - root -   Epoch: [71/200][0/346], lr: 0.00000020 	 loss = 0.7476(0.7476)
2023/09/28 13:58:44 - INFO - root -   Epoch: [71/200][20/346], lr: 0.00000020 	 loss = 0.5243(0.5721)
2023/09/28 14:00:03 - INFO - root -   Epoch: [71/200][40/346], lr: 0.00000020 	 loss = 0.3318(0.6093)
2023/09/28 14:01:02 - INFO - root -   Epoch: [71/200][60/346], lr: 0.00000020 	 loss = 0.4753(0.5993)
2023/09/28 14:02:02 - INFO - root -   Epoch: [71/200][80/346], lr: 0.00000020 	 loss = 0.5170(0.6172)
2023/09/28 14:03:05 - INFO - root -   Epoch: [71/200][100/346], lr: 0.00000020 	 loss = 0.5859(0.6138)
2023/09/28 14:04:12 - INFO - root -   Epoch: [71/200][120/346], lr: 0.00000020 	 loss = 0.5339(0.6268)
2023/09/28 14:05:25 - INFO - root -   Epoch: [71/200][140/346], lr: 0.00000020 	 loss = 0.5125(0.6209)
2023/09/28 14:06:30 - INFO - root -   Epoch: [71/200][160/346], lr: 0.00000020 	 loss = 0.8147(0.6300)
2023/09/28 14:07:38 - INFO - root -   Epoch: [71/200][180/346], lr: 0.00000020 	 loss = 0.7570(0.6279)
2023/09/28 14:08:37 - INFO - root -   Epoch: [71/200][200/346], lr: 0.00000020 	 loss = 0.3542(0.6337)
2023/09/28 14:09:53 - INFO - root -   Epoch: [71/200][220/346], lr: 0.00000020 	 loss = 0.2753(0.6401)
2023/09/28 14:10:44 - INFO - root -   Epoch: [71/200][240/346], lr: 0.00000020 	 loss = 0.2892(0.6390)
2023/09/28 14:11:53 - INFO - root -   Epoch: [71/200][260/346], lr: 0.00000020 	 loss = 0.7679(0.6448)
2023/09/28 14:12:49 - INFO - root -   Epoch: [71/200][280/346], lr: 0.00000020 	 loss = 0.6021(0.6515)
2023/09/28 14:14:07 - INFO - root -   Epoch: [71/200][300/346], lr: 0.00000020 	 loss = 0.7001(0.6548)
2023/09/28 14:15:08 - INFO - root -   Epoch: [71/200][320/346], lr: 0.00000020 	 loss = 0.3464(0.6519)
2023/09/28 14:16:10 - INFO - root -   Epoch: [71/200][340/346], lr: 0.00000020 	 loss = 1.0081(0.6460)
2023/09/28 14:16:15 - INFO - root -   Epoch: [71/200] 	 loss = 0.6461
2023/09/28 14:16:15 - INFO - root -   train_accuracy = 0.6228
2023/09/28 14:16:37 - INFO - root -   Epoch: [72/200][0/346], lr: 0.00000020 	 loss = 0.3738(0.3738)
2023/09/28 14:17:46 - INFO - root -   Epoch: [72/200][20/346], lr: 0.00000020 	 loss = 0.4102(0.6033)
2023/09/28 14:18:53 - INFO - root -   Epoch: [72/200][40/346], lr: 0.00000020 	 loss = 0.4739(0.5920)
2023/09/28 14:20:06 - INFO - root -   Epoch: [72/200][60/346], lr: 0.00000020 	 loss = 0.5106(0.6008)
2023/09/28 14:21:14 - INFO - root -   Epoch: [72/200][80/346], lr: 0.00000020 	 loss = 0.9620(0.6288)
2023/09/28 14:22:21 - INFO - root -   Epoch: [72/200][100/346], lr: 0.00000020 	 loss = 0.3792(0.6138)
2023/09/28 14:23:20 - INFO - root -   Epoch: [72/200][120/346], lr: 0.00000020 	 loss = 0.5086(0.6241)
2023/09/28 14:24:28 - INFO - root -   Epoch: [72/200][140/346], lr: 0.00000020 	 loss = 0.5984(0.6163)
2023/09/28 14:25:43 - INFO - root -   Epoch: [72/200][160/346], lr: 0.00000020 	 loss = 0.3591(0.6197)
2023/09/28 14:26:42 - INFO - root -   Epoch: [72/200][180/346], lr: 0.00000020 	 loss = 0.3121(0.6125)
2023/09/28 14:27:55 - INFO - root -   Epoch: [72/200][200/346], lr: 0.00000020 	 loss = 0.3304(0.6231)
2023/09/28 14:28:56 - INFO - root -   Epoch: [72/200][220/346], lr: 0.00000020 	 loss = 0.5893(0.6333)
2023/09/28 14:30:08 - INFO - root -   Epoch: [72/200][240/346], lr: 0.00000020 	 loss = 0.4949(0.6340)
2023/09/28 14:31:22 - INFO - root -   Epoch: [72/200][260/346], lr: 0.00000020 	 loss = 0.7222(0.6384)
2023/09/28 14:32:24 - INFO - root -   Epoch: [72/200][280/346], lr: 0.00000020 	 loss = 0.7600(0.6493)
2023/09/28 14:33:35 - INFO - root -   Epoch: [72/200][300/346], lr: 0.00000020 	 loss = 0.2740(0.6480)
2023/09/28 14:34:37 - INFO - root -   Epoch: [72/200][320/346], lr: 0.00000020 	 loss = 0.3327(0.6458)
2023/09/28 14:35:45 - INFO - root -   Epoch: [72/200][340/346], lr: 0.00000020 	 loss = 0.7165(0.6457)
2023/09/28 14:35:47 - INFO - root -   Epoch: [72/200] 	 loss = 0.6449
2023/09/28 14:35:47 - INFO - root -   train_accuracy = 0.6171
2023/09/28 14:36:34 - INFO - root -   Epoch: [73/200][0/346], lr: 0.00000020 	 loss = 0.6075(0.6075)
2023/09/28 14:37:17 - INFO - root -   Epoch: [73/200][20/346], lr: 0.00000020 	 loss = 0.4966(0.5360)
2023/09/28 14:38:27 - INFO - root -   Epoch: [73/200][40/346], lr: 0.00000020 	 loss = 0.8501(0.6847)
2023/09/28 14:40:01 - INFO - root -   Epoch: [73/200][60/346], lr: 0.00000020 	 loss = 0.8480(0.6572)
2023/09/28 14:40:59 - INFO - root -   Epoch: [73/200][80/346], lr: 0.00000020 	 loss = 0.4162(0.6475)
2023/09/28 14:42:14 - INFO - root -   Epoch: [73/200][100/346], lr: 0.00000020 	 loss = 0.7018(0.6319)
2023/09/28 14:43:05 - INFO - root -   Epoch: [73/200][120/346], lr: 0.00000020 	 loss = 0.2590(0.6328)
2023/09/28 14:44:23 - INFO - root -   Epoch: [73/200][140/346], lr: 0.00000020 	 loss = 0.8932(0.6322)
2023/09/28 14:45:23 - INFO - root -   Epoch: [73/200][160/346], lr: 0.00000020 	 loss = 0.1278(0.6344)
2023/09/28 14:46:29 - INFO - root -   Epoch: [73/200][180/346], lr: 0.00000020 	 loss = 0.5648(0.6338)
2023/09/28 14:47:37 - INFO - root -   Epoch: [73/200][200/346], lr: 0.00000020 	 loss = 0.1883(0.6307)
2023/09/28 14:48:33 - INFO - root -   Epoch: [73/200][220/346], lr: 0.00000020 	 loss = 0.3998(0.6357)
2023/09/28 14:49:36 - INFO - root -   Epoch: [73/200][240/346], lr: 0.00000020 	 loss = 0.3026(0.6384)
2023/09/28 14:50:42 - INFO - root -   Epoch: [73/200][260/346], lr: 0.00000020 	 loss = 0.4472(0.6406)
2023/09/28 14:52:01 - INFO - root -   Epoch: [73/200][280/346], lr: 0.00000020 	 loss = 1.3122(0.6473)
2023/09/28 14:52:51 - INFO - root -   Epoch: [73/200][300/346], lr: 0.00000020 	 loss = 0.5008(0.6531)
2023/09/28 14:54:22 - INFO - root -   Epoch: [73/200][320/346], lr: 0.00000020 	 loss = 0.8923(0.6537)
2023/09/28 14:55:00 - INFO - root -   Epoch: [73/200][340/346], lr: 0.00000020 	 loss = 0.7639(0.6486)
2023/09/28 14:55:05 - INFO - root -   Epoch: [73/200] 	 loss = 0.6486
2023/09/28 14:55:05 - INFO - root -   train_accuracy = 0.6315
2023/09/28 14:55:34 - INFO - root -   Epoch: [74/200][0/346], lr: 0.00000021 	 loss = 0.6361(0.6361)
2023/09/28 14:56:25 - INFO - root -   Epoch: [74/200][20/346], lr: 0.00000021 	 loss = 0.6186(0.6102)
2023/09/28 14:57:34 - INFO - root -   Epoch: [74/200][40/346], lr: 0.00000021 	 loss = 0.9399(0.6861)
2023/09/28 14:58:51 - INFO - root -   Epoch: [74/200][60/346], lr: 0.00000021 	 loss = 0.3529(0.6546)
2023/09/28 14:59:43 - INFO - root -   Epoch: [74/200][80/346], lr: 0.00000021 	 loss = 0.9116(0.6687)
2023/09/28 15:01:00 - INFO - root -   Epoch: [74/200][100/346], lr: 0.00000021 	 loss = 0.6193(0.6834)
2023/09/28 15:01:59 - INFO - root -   Epoch: [74/200][120/346], lr: 0.00000021 	 loss = 0.2729(0.6864)
2023/09/28 15:03:13 - INFO - root -   Epoch: [74/200][140/346], lr: 0.00000021 	 loss = 0.5555(0.6892)
2023/09/28 15:04:20 - INFO - root -   Epoch: [74/200][160/346], lr: 0.00000021 	 loss = 0.2148(0.6843)
2023/09/28 15:05:32 - INFO - root -   Epoch: [74/200][180/346], lr: 0.00000021 	 loss = 0.4636(0.6735)
2023/09/28 15:06:41 - INFO - root -   Epoch: [74/200][200/346], lr: 0.00000021 	 loss = 0.2178(0.6674)
2023/09/28 15:07:40 - INFO - root -   Epoch: [74/200][220/346], lr: 0.00000021 	 loss = 0.4699(0.6696)
2023/09/28 15:08:59 - INFO - root -   Epoch: [74/200][240/346], lr: 0.00000021 	 loss = 0.5536(0.6660)
2023/09/28 15:09:59 - INFO - root -   Epoch: [74/200][260/346], lr: 0.00000021 	 loss = 0.6827(0.6638)
2023/09/28 15:11:08 - INFO - root -   Epoch: [74/200][280/346], lr: 0.00000021 	 loss = 0.6767(0.6711)
2023/09/28 15:12:26 - INFO - root -   Epoch: [74/200][300/346], lr: 0.00000021 	 loss = 0.4996(0.6686)
2023/09/28 15:13:22 - INFO - root -   Epoch: [74/200][320/346], lr: 0.00000021 	 loss = 0.4991(0.6660)
2023/09/28 15:14:28 - INFO - root -   Epoch: [74/200][340/346], lr: 0.00000021 	 loss = 0.8260(0.6652)
2023/09/28 15:14:30 - INFO - root -   Epoch: [74/200] 	 loss = 0.6624
2023/09/28 15:18:22 - INFO - root -   precision = 0.6667
2023/09/28 15:18:22 - INFO - root -   eval_loss = 0.6197
2023/09/28 15:18:23 - INFO - root -   train_accuracy = 0.6156
2023/09/28 15:18:45 - INFO - root -   Epoch: [75/200][0/346], lr: 0.00000021 	 loss = 0.1426(0.1426)
2023/09/28 15:19:53 - INFO - root -   Epoch: [75/200][20/346], lr: 0.00000021 	 loss = 0.5276(0.5765)
2023/09/28 15:20:58 - INFO - root -   Epoch: [75/200][40/346], lr: 0.00000021 	 loss = 0.8582(0.6052)
2023/09/28 15:22:22 - INFO - root -   Epoch: [75/200][60/346], lr: 0.00000021 	 loss = 0.5360(0.5848)
2023/09/28 15:23:13 - INFO - root -   Epoch: [75/200][80/346], lr: 0.00000021 	 loss = 0.6039(0.5772)
2023/09/28 15:24:30 - INFO - root -   Epoch: [75/200][100/346], lr: 0.00000021 	 loss = 0.6416(0.5858)
2023/09/28 15:25:28 - INFO - root -   Epoch: [75/200][120/346], lr: 0.00000021 	 loss = 1.1013(0.6175)
2023/09/28 15:26:52 - INFO - root -   Epoch: [75/200][140/346], lr: 0.00000021 	 loss = 0.9712(0.6085)
2023/09/28 15:27:51 - INFO - root -   Epoch: [75/200][160/346], lr: 0.00000021 	 loss = 0.2357(0.6243)
2023/09/28 15:29:08 - INFO - root -   Epoch: [75/200][180/346], lr: 0.00000021 	 loss = 0.3822(0.6157)
2023/09/28 15:30:14 - INFO - root -   Epoch: [75/200][200/346], lr: 0.00000021 	 loss = 0.7151(0.6232)
2023/09/28 15:31:41 - INFO - root -   Epoch: [75/200][220/346], lr: 0.00000021 	 loss = 0.3277(0.6294)
2023/09/28 15:32:46 - INFO - root -   Epoch: [75/200][240/346], lr: 0.00000021 	 loss = 0.3325(0.6277)
2023/09/28 15:33:31 - INFO - root -   Epoch: [75/200][260/346], lr: 0.00000021 	 loss = 0.6747(0.6305)
2023/09/28 15:34:51 - INFO - root -   Epoch: [75/200][280/346], lr: 0.00000021 	 loss = 0.5632(0.6417)
2023/09/28 15:35:51 - INFO - root -   Epoch: [75/200][300/346], lr: 0.00000021 	 loss = 0.7522(0.6449)
2023/09/28 15:37:12 - INFO - root -   Epoch: [75/200][320/346], lr: 0.00000021 	 loss = 0.5856(0.6468)
2023/09/28 15:37:57 - INFO - root -   Epoch: [75/200][340/346], lr: 0.00000021 	 loss = 1.3091(0.6470)
2023/09/28 15:38:05 - INFO - root -   Epoch: [75/200] 	 loss = 0.6445
2023/09/28 15:38:05 - INFO - root -   train_accuracy = 0.6402
2023/09/28 15:38:36 - INFO - root -   Epoch: [76/200][0/346], lr: 0.00000021 	 loss = 0.3725(0.3725)
2023/09/28 15:39:56 - INFO - root -   Epoch: [76/200][20/346], lr: 0.00000021 	 loss = 0.4457(0.6251)
2023/09/28 15:40:47 - INFO - root -   Epoch: [76/200][40/346], lr: 0.00000021 	 loss = 0.5742(0.5683)
2023/09/28 15:42:05 - INFO - root -   Epoch: [76/200][60/346], lr: 0.00000021 	 loss = 0.5621(0.5496)
2023/09/28 15:42:53 - INFO - root -   Epoch: [76/200][80/346], lr: 0.00000021 	 loss = 1.0910(0.5844)
2023/09/28 15:43:59 - INFO - root -   Epoch: [76/200][100/346], lr: 0.00000021 	 loss = 0.5492(0.5982)
2023/09/28 15:45:10 - INFO - root -   Epoch: [76/200][120/346], lr: 0.00000021 	 loss = 0.4485(0.6215)
2023/09/28 15:46:04 - INFO - root -   Epoch: [76/200][140/346], lr: 0.00000021 	 loss = 0.6757(0.6224)
2023/09/28 15:47:21 - INFO - root -   Epoch: [76/200][160/346], lr: 0.00000021 	 loss = 0.4717(0.6288)
2023/09/28 15:48:12 - INFO - root -   Epoch: [76/200][180/346], lr: 0.00000021 	 loss = 0.6264(0.6322)
2023/09/28 15:49:37 - INFO - root -   Epoch: [76/200][200/346], lr: 0.00000021 	 loss = 0.4275(0.6429)
2023/09/28 15:50:30 - INFO - root -   Epoch: [76/200][220/346], lr: 0.00000021 	 loss = 0.2125(0.6512)
2023/09/28 15:51:46 - INFO - root -   Epoch: [76/200][240/346], lr: 0.00000021 	 loss = 0.9081(0.6546)
2023/09/28 15:52:44 - INFO - root -   Epoch: [76/200][260/346], lr: 0.00000021 	 loss = 0.5998(0.6503)
2023/09/28 15:54:01 - INFO - root -   Epoch: [76/200][280/346], lr: 0.00000021 	 loss = 0.7971(0.6554)
2023/09/28 15:55:00 - INFO - root -   Epoch: [76/200][300/346], lr: 0.00000021 	 loss = 0.2976(0.6543)
2023/09/28 15:56:08 - INFO - root -   Epoch: [76/200][320/346], lr: 0.00000021 	 loss = 0.5573(0.6576)
2023/09/28 15:56:50 - INFO - root -   Epoch: [76/200][340/346], lr: 0.00000021 	 loss = 1.1930(0.6522)
2023/09/28 15:57:00 - INFO - root -   Epoch: [76/200] 	 loss = 0.6529
2023/09/28 15:57:00 - INFO - root -   train_accuracy = 0.6286
2023/09/28 15:57:38 - INFO - root -   Epoch: [77/200][0/346], lr: 0.00000021 	 loss = 0.6731(0.6731)
2023/09/28 15:58:23 - INFO - root -   Epoch: [77/200][20/346], lr: 0.00000021 	 loss = 0.3508(0.6503)
2023/09/28 15:59:47 - INFO - root -   Epoch: [77/200][40/346], lr: 0.00000021 	 loss = 0.7021(0.6631)
2023/09/28 16:00:43 - INFO - root -   Epoch: [77/200][60/346], lr: 0.00000021 	 loss = 0.2754(0.6261)
2023/09/28 16:01:53 - INFO - root -   Epoch: [77/200][80/346], lr: 0.00000021 	 loss = 0.5025(0.6446)
2023/09/28 16:03:02 - INFO - root -   Epoch: [77/200][100/346], lr: 0.00000021 	 loss = 0.9334(0.6555)
2023/09/28 16:04:18 - INFO - root -   Epoch: [77/200][120/346], lr: 0.00000021 	 loss = 0.3901(0.6536)
2023/09/28 16:05:16 - INFO - root -   Epoch: [77/200][140/346], lr: 0.00000021 	 loss = 0.3830(0.6492)
2023/09/28 16:06:26 - INFO - root -   Epoch: [77/200][160/346], lr: 0.00000021 	 loss = 0.2149(0.6503)
2023/09/28 16:07:37 - INFO - root -   Epoch: [77/200][180/346], lr: 0.00000021 	 loss = 0.3544(0.6485)
2023/09/28 16:08:51 - INFO - root -   Epoch: [77/200][200/346], lr: 0.00000021 	 loss = 0.2062(0.6541)
2023/09/28 16:09:44 - INFO - root -   Epoch: [77/200][220/346], lr: 0.00000021 	 loss = 0.3983(0.6550)
2023/09/28 16:11:12 - INFO - root -   Epoch: [77/200][240/346], lr: 0.00000021 	 loss = 0.3849(0.6591)
2023/09/28 16:11:59 - INFO - root -   Epoch: [77/200][260/346], lr: 0.00000021 	 loss = 0.9184(0.6617)
2023/09/28 16:13:30 - INFO - root -   Epoch: [77/200][280/346], lr: 0.00000021 	 loss = 0.8084(0.6669)
2023/09/28 16:14:27 - INFO - root -   Epoch: [77/200][300/346], lr: 0.00000021 	 loss = 0.5336(0.6698)
2023/09/28 16:15:28 - INFO - root -   Epoch: [77/200][320/346], lr: 0.00000021 	 loss = 0.7511(0.6691)
2023/09/28 16:16:18 - INFO - root -   Epoch: [77/200][340/346], lr: 0.00000021 	 loss = 0.7224(0.6623)
2023/09/28 16:16:28 - INFO - root -   Epoch: [77/200] 	 loss = 0.6599
2023/09/28 16:16:28 - INFO - root -   train_accuracy = 0.6142
2023/09/28 16:16:50 - INFO - root -   Epoch: [78/200][0/346], lr: 0.00000021 	 loss = 0.3825(0.3825)
2023/09/28 16:18:06 - INFO - root -   Epoch: [78/200][20/346], lr: 0.00000021 	 loss = 0.1406(0.5105)
2023/09/28 16:19:02 - INFO - root -   Epoch: [78/200][40/346], lr: 0.00000021 	 loss = 0.5397(0.5546)
2023/09/28 16:20:25 - INFO - root -   Epoch: [78/200][60/346], lr: 0.00000021 	 loss = 0.6557(0.5882)
2023/09/28 16:21:14 - INFO - root -   Epoch: [78/200][80/346], lr: 0.00000021 	 loss = 0.5464(0.5887)
2023/09/28 16:22:25 - INFO - root -   Epoch: [78/200][100/346], lr: 0.00000021 	 loss = 0.5814(0.6118)
2023/09/28 16:23:40 - INFO - root -   Epoch: [78/200][120/346], lr: 0.00000021 	 loss = 0.4248(0.6317)
2023/09/28 16:24:39 - INFO - root -   Epoch: [78/200][140/346], lr: 0.00000021 	 loss = 0.3664(0.6253)
2023/09/28 16:25:31 - INFO - root -   Epoch: [78/200][160/346], lr: 0.00000021 	 loss = 0.2230(0.6341)
2023/09/28 16:26:47 - INFO - root -   Epoch: [78/200][180/346], lr: 0.00000021 	 loss = 0.8840(0.6297)
2023/09/28 16:27:53 - INFO - root -   Epoch: [78/200][200/346], lr: 0.00000021 	 loss = 0.5206(0.6254)
2023/09/28 16:29:09 - INFO - root -   Epoch: [78/200][220/346], lr: 0.00000021 	 loss = 0.1459(0.6346)
2023/09/28 16:29:53 - INFO - root -   Epoch: [78/200][240/346], lr: 0.00000021 	 loss = 0.7787(0.6322)
2023/09/28 16:31:09 - INFO - root -   Epoch: [78/200][260/346], lr: 0.00000021 	 loss = 0.2914(0.6280)
2023/09/28 16:32:08 - INFO - root -   Epoch: [78/200][280/346], lr: 0.00000021 	 loss = 0.5844(0.6375)
2023/09/28 16:33:27 - INFO - root -   Epoch: [78/200][300/346], lr: 0.00000021 	 loss = 0.6516(0.6323)
2023/09/28 16:34:17 - INFO - root -   Epoch: [78/200][320/346], lr: 0.00000021 	 loss = 0.3687(0.6332)
2023/09/28 16:35:23 - INFO - root -   Epoch: [78/200][340/346], lr: 0.00000021 	 loss = 1.1823(0.6315)
2023/09/28 16:35:25 - INFO - root -   Epoch: [78/200] 	 loss = 0.6313
2023/09/28 16:35:25 - INFO - root -   train_accuracy = 0.6329
2023/09/28 16:35:47 - INFO - root -   Epoch: [79/200][0/346], lr: 0.00000021 	 loss = 0.3237(0.3237)
2023/09/28 16:37:11 - INFO - root -   Epoch: [79/200][20/346], lr: 0.00000021 	 loss = 0.5378(0.6713)
2023/09/28 16:38:20 - INFO - root -   Epoch: [79/200][40/346], lr: 0.00000021 	 loss = 0.6014(0.6428)
2023/09/28 16:39:24 - INFO - root -   Epoch: [79/200][60/346], lr: 0.00000021 	 loss = 0.6821(0.6031)
2023/09/28 16:40:39 - INFO - root -   Epoch: [79/200][80/346], lr: 0.00000021 	 loss = 0.6897(0.6222)
2023/09/28 16:41:45 - INFO - root -   Epoch: [79/200][100/346], lr: 0.00000021 	 loss = 0.8380(0.6166)
2023/09/28 16:42:44 - INFO - root -   Epoch: [79/200][120/346], lr: 0.00000021 	 loss = 0.4971(0.6396)
2023/09/28 16:43:41 - INFO - root -   Epoch: [79/200][140/346], lr: 0.00000021 	 loss = 0.7672(0.6326)
2023/09/28 16:44:47 - INFO - root -   Epoch: [79/200][160/346], lr: 0.00000021 	 loss = 0.1630(0.6408)
2023/09/28 16:45:59 - INFO - root -   Epoch: [79/200][180/346], lr: 0.00000021 	 loss = 0.9312(0.6378)
2023/09/28 16:47:15 - INFO - root -   Epoch: [79/200][200/346], lr: 0.00000021 	 loss = 0.2255(0.6398)
2023/09/28 16:48:26 - INFO - root -   Epoch: [79/200][220/346], lr: 0.00000021 	 loss = 0.4108(0.6496)
2023/09/28 16:49:33 - INFO - root -   Epoch: [79/200][240/346], lr: 0.00000021 	 loss = 0.2097(0.6483)
2023/09/28 16:50:33 - INFO - root -   Epoch: [79/200][260/346], lr: 0.00000021 	 loss = 0.5176(0.6537)
2023/09/28 16:51:48 - INFO - root -   Epoch: [79/200][280/346], lr: 0.00000021 	 loss = 1.1902(0.6565)
2023/09/28 16:52:35 - INFO - root -   Epoch: [79/200][300/346], lr: 0.00000021 	 loss = 0.3257(0.6532)
2023/09/28 16:53:56 - INFO - root -   Epoch: [79/200][320/346], lr: 0.00000021 	 loss = 0.8972(0.6533)
2023/09/28 16:54:50 - INFO - root -   Epoch: [79/200][340/346], lr: 0.00000021 	 loss = 1.3888(0.6524)
2023/09/28 16:54:52 - INFO - root -   Epoch: [79/200] 	 loss = 0.6522
2023/09/28 16:58:42 - INFO - root -   precision = 0.6724
2023/09/28 16:58:42 - INFO - root -   eval_loss = 0.6083
2023/09/28 16:58:44 - INFO - root -   train_accuracy = 0.6286
2023/09/28 16:59:14 - INFO - root -   Epoch: [80/200][0/346], lr: 0.00000021 	 loss = 0.3701(0.3701)
2023/09/28 17:00:26 - INFO - root -   Epoch: [80/200][20/346], lr: 0.00000021 	 loss = 0.3734(0.5918)
2023/09/28 17:01:30 - INFO - root -   Epoch: [80/200][40/346], lr: 0.00000021 	 loss = 0.9657(0.6200)
2023/09/28 17:02:43 - INFO - root -   Epoch: [80/200][60/346], lr: 0.00000021 	 loss = 0.4313(0.6285)
2023/09/28 17:03:40 - INFO - root -   Epoch: [80/200][80/346], lr: 0.00000021 	 loss = 0.5970(0.6343)
2023/09/28 17:04:50 - INFO - root -   Epoch: [80/200][100/346], lr: 0.00000021 	 loss = 0.8010(0.6639)
2023/09/28 17:06:17 - INFO - root -   Epoch: [80/200][120/346], lr: 0.00000021 	 loss = 0.9497(0.6680)
2023/09/28 17:07:15 - INFO - root -   Epoch: [80/200][140/346], lr: 0.00000021 	 loss = 0.2798(0.6494)
2023/09/28 17:08:41 - INFO - root -   Epoch: [80/200][160/346], lr: 0.00000021 	 loss = 0.3883(0.6549)
2023/09/28 17:09:42 - INFO - root -   Epoch: [80/200][180/346], lr: 0.00000021 	 loss = 0.7440(0.6478)
2023/09/28 17:11:14 - INFO - root -   Epoch: [80/200][200/346], lr: 0.00000021 	 loss = 0.2458(0.6387)
2023/09/28 17:12:04 - INFO - root -   Epoch: [80/200][220/346], lr: 0.00000021 	 loss = 0.3307(0.6456)
2023/09/28 17:13:21 - INFO - root -   Epoch: [80/200][240/346], lr: 0.00000021 	 loss = 0.4428(0.6433)
2023/09/28 17:14:05 - INFO - root -   Epoch: [80/200][260/346], lr: 0.00000021 	 loss = 0.4038(0.6458)
2023/09/28 17:15:16 - INFO - root -   Epoch: [80/200][280/346], lr: 0.00000021 	 loss = 0.2584(0.6481)
2023/09/28 17:16:31 - INFO - root -   Epoch: [80/200][300/346], lr: 0.00000021 	 loss = 0.4043(0.6567)
2023/09/28 17:17:36 - INFO - root -   Epoch: [80/200][320/346], lr: 0.00000021 	 loss = 0.2891(0.6579)
2023/09/28 17:18:29 - INFO - root -   Epoch: [80/200][340/346], lr: 0.00000021 	 loss = 1.7466(0.6572)
2023/09/28 17:18:36 - INFO - root -   Epoch: [80/200] 	 loss = 0.6582
2023/09/28 17:18:36 - INFO - root -   train_accuracy = 0.6286
2023/09/28 17:18:59 - INFO - root -   Epoch: [81/200][0/346], lr: 0.00000022 	 loss = 0.3994(0.3994)
2023/09/28 17:20:11 - INFO - root -   Epoch: [81/200][20/346], lr: 0.00000022 	 loss = 0.5116(0.5018)
2023/09/28 17:21:16 - INFO - root -   Epoch: [81/200][40/346], lr: 0.00000022 	 loss = 0.5194(0.5710)
2023/09/28 17:22:29 - INFO - root -   Epoch: [81/200][60/346], lr: 0.00000022 	 loss = 1.0048(0.5611)
2023/09/28 17:23:52 - INFO - root -   Epoch: [81/200][80/346], lr: 0.00000022 	 loss = 0.7007(0.5686)
2023/09/28 17:24:49 - INFO - root -   Epoch: [81/200][100/346], lr: 0.00000022 	 loss = 1.5513(0.5932)
2023/09/28 17:25:55 - INFO - root -   Epoch: [81/200][120/346], lr: 0.00000022 	 loss = 0.5589(0.6187)
2023/09/28 17:27:08 - INFO - root -   Epoch: [81/200][140/346], lr: 0.00000022 	 loss = 0.5694(0.6112)
2023/09/28 17:28:07 - INFO - root -   Epoch: [81/200][160/346], lr: 0.00000022 	 loss = 0.3506(0.6280)
2023/09/28 17:29:20 - INFO - root -   Epoch: [81/200][180/346], lr: 0.00000022 	 loss = 0.8681(0.6385)
2023/09/28 17:30:24 - INFO - root -   Epoch: [81/200][200/346], lr: 0.00000022 	 loss = 0.1820(0.6375)
2023/09/28 17:31:22 - INFO - root -   Epoch: [81/200][220/346], lr: 0.00000022 	 loss = 0.4448(0.6521)
2023/09/28 17:32:52 - INFO - root -   Epoch: [81/200][240/346], lr: 0.00000022 	 loss = 0.6596(0.6604)
2023/09/28 17:33:48 - INFO - root -   Epoch: [81/200][260/346], lr: 0.00000022 	 loss = 1.1268(0.6653)
2023/09/28 17:34:48 - INFO - root -   Epoch: [81/200][280/346], lr: 0.00000022 	 loss = 0.7696(0.6662)
2023/09/28 17:36:05 - INFO - root -   Epoch: [81/200][300/346], lr: 0.00000022 	 loss = 0.4819(0.6725)
2023/09/28 17:36:57 - INFO - root -   Epoch: [81/200][320/346], lr: 0.00000022 	 loss = 0.6950(0.6797)
2023/09/28 17:38:06 - INFO - root -   Epoch: [81/200][340/346], lr: 0.00000022 	 loss = 1.0352(0.6714)
2023/09/28 17:38:08 - INFO - root -   Epoch: [81/200] 	 loss = 0.6718
2023/09/28 17:38:08 - INFO - root -   train_accuracy = 0.6069
2023/09/28 17:38:38 - INFO - root -   Epoch: [82/200][0/346], lr: 0.00000022 	 loss = 0.6674(0.6674)
2023/09/28 17:39:30 - INFO - root -   Epoch: [82/200][20/346], lr: 0.00000022 	 loss = 0.5660(0.5783)
2023/09/28 17:40:50 - INFO - root -   Epoch: [82/200][40/346], lr: 0.00000022 	 loss = 0.6274(0.6705)
2023/09/28 17:41:55 - INFO - root -   Epoch: [82/200][60/346], lr: 0.00000022 	 loss = 0.3964(0.6202)
2023/09/28 17:43:22 - INFO - root -   Epoch: [82/200][80/346], lr: 0.00000022 	 loss = 0.4974(0.6309)
2023/09/28 17:44:24 - INFO - root -   Epoch: [82/200][100/346], lr: 0.00000022 	 loss = 0.8199(0.6464)
2023/09/28 17:45:39 - INFO - root -   Epoch: [82/200][120/346], lr: 0.00000022 	 loss = 0.4707(0.6683)
2023/09/28 17:46:45 - INFO - root -   Epoch: [82/200][140/346], lr: 0.00000022 	 loss = 0.6828(0.6638)
2023/09/28 17:47:51 - INFO - root -   Epoch: [82/200][160/346], lr: 0.00000022 	 loss = 0.2819(0.6569)
2023/09/28 17:48:56 - INFO - root -   Epoch: [82/200][180/346], lr: 0.00000022 	 loss = 0.5261(0.6448)
2023/09/28 17:49:58 - INFO - root -   Epoch: [82/200][200/346], lr: 0.00000022 	 loss = 0.2136(0.6523)
2023/09/28 17:51:10 - INFO - root -   Epoch: [82/200][220/346], lr: 0.00000022 	 loss = 0.5106(0.6530)
2023/09/28 17:52:12 - INFO - root -   Epoch: [82/200][240/346], lr: 0.00000022 	 loss = 0.5760(0.6471)
2023/09/28 17:53:21 - INFO - root -   Epoch: [82/200][260/346], lr: 0.00000022 	 loss = 0.5063(0.6482)
2023/09/28 17:54:53 - INFO - root -   Epoch: [82/200][280/346], lr: 0.00000022 	 loss = 0.8707(0.6622)
2023/09/28 17:55:45 - INFO - root -   Epoch: [82/200][300/346], lr: 0.00000022 	 loss = 0.4773(0.6688)
2023/09/28 17:56:57 - INFO - root -   Epoch: [82/200][320/346], lr: 0.00000022 	 loss = 0.5097(0.6645)
2023/09/28 17:57:49 - INFO - root -   Epoch: [82/200][340/346], lr: 0.00000022 	 loss = 1.0377(0.6591)
2023/09/28 17:57:58 - INFO - root -   Epoch: [82/200] 	 loss = 0.6600
2023/09/28 17:57:58 - INFO - root -   train_accuracy = 0.6228
2023/09/28 17:58:27 - INFO - root -   Epoch: [83/200][0/346], lr: 0.00000022 	 loss = 0.3544(0.3544)
2023/09/28 17:59:41 - INFO - root -   Epoch: [83/200][20/346], lr: 0.00000022 	 loss = 0.9546(0.6910)
2023/09/28 18:00:48 - INFO - root -   Epoch: [83/200][40/346], lr: 0.00000022 	 loss = 0.5663(0.6655)
2023/09/28 18:02:01 - INFO - root -   Epoch: [83/200][60/346], lr: 0.00000022 	 loss = 0.3705(0.6166)
2023/09/28 18:02:55 - INFO - root -   Epoch: [83/200][80/346], lr: 0.00000022 	 loss = 0.8677(0.6636)
2023/09/28 18:04:07 - INFO - root -   Epoch: [83/200][100/346], lr: 0.00000022 	 loss = 0.6422(0.6673)
2023/09/28 18:05:05 - INFO - root -   Epoch: [83/200][120/346], lr: 0.00000022 	 loss = 0.6225(0.6631)
2023/09/28 18:06:15 - INFO - root -   Epoch: [83/200][140/346], lr: 0.00000022 	 loss = 0.7067(0.6510)
2023/09/28 18:07:24 - INFO - root -   Epoch: [83/200][160/346], lr: 0.00000022 	 loss = 0.3961(0.6486)
2023/09/28 18:08:35 - INFO - root -   Epoch: [83/200][180/346], lr: 0.00000022 	 loss = 0.4980(0.6514)
2023/09/28 18:09:37 - INFO - root -   Epoch: [83/200][200/346], lr: 0.00000022 	 loss = 0.1898(0.6452)
2023/09/28 18:11:06 - INFO - root -   Epoch: [83/200][220/346], lr: 0.00000022 	 loss = 0.4476(0.6609)
2023/09/28 18:12:03 - INFO - root -   Epoch: [83/200][240/346], lr: 0.00000022 	 loss = 0.4620(0.6620)
2023/09/28 18:12:59 - INFO - root -   Epoch: [83/200][260/346], lr: 0.00000022 	 loss = 0.8072(0.6581)
2023/09/28 18:14:18 - INFO - root -   Epoch: [83/200][280/346], lr: 0.00000022 	 loss = 0.9697(0.6692)
2023/09/28 18:15:27 - INFO - root -   Epoch: [83/200][300/346], lr: 0.00000022 	 loss = 0.3248(0.6714)
2023/09/28 18:16:45 - INFO - root -   Epoch: [83/200][320/346], lr: 0.00000022 	 loss = 0.7823(0.6699)
2023/09/28 18:17:31 - INFO - root -   Epoch: [83/200][340/346], lr: 0.00000022 	 loss = 0.7835(0.6679)
2023/09/28 18:17:38 - INFO - root -   Epoch: [83/200] 	 loss = 0.6670
2023/09/28 18:17:38 - INFO - root -   train_accuracy = 0.6113
2023/09/28 18:18:16 - INFO - root -   Epoch: [84/200][0/346], lr: 0.00000022 	 loss = 0.5401(0.5401)
2023/09/28 18:19:14 - INFO - root -   Epoch: [84/200][20/346], lr: 0.00000022 	 loss = 0.7384(0.6788)
2023/09/28 18:20:33 - INFO - root -   Epoch: [84/200][40/346], lr: 0.00000022 	 loss = 0.5557(0.6409)
2023/09/28 18:21:38 - INFO - root -   Epoch: [84/200][60/346], lr: 0.00000022 	 loss = 0.4121(0.6392)
2023/09/28 18:22:45 - INFO - root -   Epoch: [84/200][80/346], lr: 0.00000022 	 loss = 0.6152(0.6324)
2023/09/28 18:23:47 - INFO - root -   Epoch: [84/200][100/346], lr: 0.00000022 	 loss = 0.4911(0.6353)
2023/09/28 18:25:17 - INFO - root -   Epoch: [84/200][120/346], lr: 0.00000022 	 loss = 0.5336(0.6534)
2023/09/28 18:26:22 - INFO - root -   Epoch: [84/200][140/346], lr: 0.00000022 	 loss = 0.8118(0.6531)
2023/09/28 18:27:40 - INFO - root -   Epoch: [84/200][160/346], lr: 0.00000022 	 loss = 0.3873(0.6524)
2023/09/28 18:28:34 - INFO - root -   Epoch: [84/200][180/346], lr: 0.00000022 	 loss = 0.6298(0.6547)
2023/09/28 18:29:45 - INFO - root -   Epoch: [84/200][200/346], lr: 0.00000022 	 loss = 0.1648(0.6527)
2023/09/28 18:30:34 - INFO - root -   Epoch: [84/200][220/346], lr: 0.00000022 	 loss = 0.2652(0.6635)
2023/09/28 18:31:35 - INFO - root -   Epoch: [84/200][240/346], lr: 0.00000022 	 loss = 0.3083(0.6573)
2023/09/28 18:32:59 - INFO - root -   Epoch: [84/200][260/346], lr: 0.00000022 	 loss = 0.4780(0.6628)
2023/09/28 18:34:00 - INFO - root -   Epoch: [84/200][280/346], lr: 0.00000022 	 loss = 0.8474(0.6682)
2023/09/28 18:35:13 - INFO - root -   Epoch: [84/200][300/346], lr: 0.00000022 	 loss = 0.3476(0.6674)
2023/09/28 18:36:05 - INFO - root -   Epoch: [84/200][320/346], lr: 0.00000022 	 loss = 0.4195(0.6676)
2023/09/28 18:37:08 - INFO - root -   Epoch: [84/200][340/346], lr: 0.00000022 	 loss = 0.8488(0.6643)
2023/09/28 18:37:12 - INFO - root -   Epoch: [84/200] 	 loss = 0.6646
2023/09/28 18:41:04 - INFO - root -   precision = 0.6839
2023/09/28 18:41:04 - INFO - root -   eval_loss = 0.5939
2023/09/28 18:41:05 - INFO - root -   train_accuracy = 0.6098
2023/09/28 18:41:35 - INFO - root -   Epoch: [85/200][0/346], lr: 0.00000022 	 loss = 0.4840(0.4840)
2023/09/28 18:42:33 - INFO - root -   Epoch: [85/200][20/346], lr: 0.00000022 	 loss = 0.5572(0.5755)
2023/09/28 18:43:52 - INFO - root -   Epoch: [85/200][40/346], lr: 0.00000022 	 loss = 0.6239(0.5961)
2023/09/28 18:44:46 - INFO - root -   Epoch: [85/200][60/346], lr: 0.00000022 	 loss = 1.0136(0.5810)
2023/09/28 18:45:50 - INFO - root -   Epoch: [85/200][80/346], lr: 0.00000022 	 loss = 0.8074(0.6111)
2023/09/28 18:46:58 - INFO - root -   Epoch: [85/200][100/346], lr: 0.00000022 	 loss = 0.6994(0.6096)
2023/09/28 18:47:55 - INFO - root -   Epoch: [85/200][120/346], lr: 0.00000022 	 loss = 1.0802(0.6287)
2023/09/28 18:48:57 - INFO - root -   Epoch: [85/200][140/346], lr: 0.00000022 	 loss = 0.9015(0.6167)
2023/09/28 18:49:59 - INFO - root -   Epoch: [85/200][160/346], lr: 0.00000022 	 loss = 0.2956(0.6211)
2023/09/28 18:51:11 - INFO - root -   Epoch: [85/200][180/346], lr: 0.00000022 	 loss = 0.5342(0.6333)
2023/09/28 18:52:10 - INFO - root -   Epoch: [85/200][200/346], lr: 0.00000022 	 loss = 0.5595(0.6305)
2023/09/28 18:53:36 - INFO - root -   Epoch: [85/200][220/346], lr: 0.00000022 	 loss = 0.3420(0.6362)
2023/09/28 18:54:40 - INFO - root -   Epoch: [85/200][240/346], lr: 0.00000022 	 loss = 0.7502(0.6393)
2023/09/28 18:55:51 - INFO - root -   Epoch: [85/200][260/346], lr: 0.00000022 	 loss = 0.7085(0.6384)
2023/09/28 18:56:58 - INFO - root -   Epoch: [85/200][280/346], lr: 0.00000022 	 loss = 0.5744(0.6459)
2023/09/28 18:58:13 - INFO - root -   Epoch: [85/200][300/346], lr: 0.00000022 	 loss = 0.5611(0.6474)
2023/09/28 18:59:05 - INFO - root -   Epoch: [85/200][320/346], lr: 0.00000022 	 loss = 0.6301(0.6531)
2023/09/28 19:00:06 - INFO - root -   Epoch: [85/200][340/346], lr: 0.00000022 	 loss = 0.5702(0.6480)
2023/09/28 19:00:08 - INFO - root -   Epoch: [85/200] 	 loss = 0.6472
2023/09/28 19:00:08 - INFO - root -   train_accuracy = 0.6185
2023/09/28 19:00:38 - INFO - root -   Epoch: [86/200][0/346], lr: 0.00000022 	 loss = 0.4056(0.4056)
2023/09/28 19:01:41 - INFO - root -   Epoch: [86/200][20/346], lr: 0.00000022 	 loss = 0.9779(0.7004)
2023/09/28 19:02:45 - INFO - root -   Epoch: [86/200][40/346], lr: 0.00000022 	 loss = 0.4349(0.6449)
2023/09/28 19:03:54 - INFO - root -   Epoch: [86/200][60/346], lr: 0.00000022 	 loss = 0.6348(0.5907)
2023/09/28 19:04:55 - INFO - root -   Epoch: [86/200][80/346], lr: 0.00000022 	 loss = 0.6126(0.5981)
2023/09/28 19:06:07 - INFO - root -   Epoch: [86/200][100/346], lr: 0.00000022 	 loss = 0.8403(0.6102)
2023/09/28 19:07:23 - INFO - root -   Epoch: [86/200][120/346], lr: 0.00000022 	 loss = 0.6353(0.6219)
2023/09/28 19:08:17 - INFO - root -   Epoch: [86/200][140/346], lr: 0.00000022 	 loss = 1.1498(0.6148)
2023/09/28 19:09:25 - INFO - root -   Epoch: [86/200][160/346], lr: 0.00000022 	 loss = 0.2028(0.6281)
2023/09/28 19:10:22 - INFO - root -   Epoch: [86/200][180/346], lr: 0.00000022 	 loss = 0.7475(0.6232)
2023/09/28 19:11:20 - INFO - root -   Epoch: [86/200][200/346], lr: 0.00000022 	 loss = 0.2818(0.6229)
2023/09/28 19:12:44 - INFO - root -   Epoch: [86/200][220/346], lr: 0.00000022 	 loss = 0.3732(0.6340)
2023/09/28 19:13:39 - INFO - root -   Epoch: [86/200][240/346], lr: 0.00000022 	 loss = 0.3130(0.6282)
2023/09/28 19:15:01 - INFO - root -   Epoch: [86/200][260/346], lr: 0.00000022 	 loss = 0.4052(0.6168)
2023/09/28 19:15:56 - INFO - root -   Epoch: [86/200][280/346], lr: 0.00000022 	 loss = 1.0641(0.6297)
2023/09/28 19:17:08 - INFO - root -   Epoch: [86/200][300/346], lr: 0.00000022 	 loss = 0.2023(0.6356)
2023/09/28 19:18:15 - INFO - root -   Epoch: [86/200][320/346], lr: 0.00000022 	 loss = 0.2517(0.6350)
2023/09/28 19:19:21 - INFO - root -   Epoch: [86/200][340/346], lr: 0.00000022 	 loss = 0.5728(0.6314)
2023/09/28 19:19:23 - INFO - root -   Epoch: [86/200] 	 loss = 0.6309
2023/09/28 19:19:23 - INFO - root -   train_accuracy = 0.6488
2023/09/28 19:19:53 - INFO - root -   Epoch: [87/200][0/346], lr: 0.00000022 	 loss = 0.3502(0.3502)
2023/09/28 19:20:53 - INFO - root -   Epoch: [87/200][20/346], lr: 0.00000022 	 loss = 0.8050(0.5656)
2023/09/28 19:21:54 - INFO - root -   Epoch: [87/200][40/346], lr: 0.00000022 	 loss = 0.7208(0.5964)
2023/09/28 19:23:15 - INFO - root -   Epoch: [87/200][60/346], lr: 0.00000022 	 loss = 0.5936(0.5894)
2023/09/28 19:24:13 - INFO - root -   Epoch: [87/200][80/346], lr: 0.00000022 	 loss = 0.8941(0.5936)
2023/09/28 19:25:27 - INFO - root -   Epoch: [87/200][100/346], lr: 0.00000022 	 loss = 1.5067(0.6148)
2023/09/28 19:26:27 - INFO - root -   Epoch: [87/200][120/346], lr: 0.00000022 	 loss = 0.6423(0.6293)
2023/09/28 19:27:44 - INFO - root -   Epoch: [87/200][140/346], lr: 0.00000022 	 loss = 1.0170(0.6264)
2023/09/28 19:28:47 - INFO - root -   Epoch: [87/200][160/346], lr: 0.00000022 	 loss = 0.2852(0.6324)
2023/09/28 19:29:56 - INFO - root -   Epoch: [87/200][180/346], lr: 0.00000022 	 loss = 0.7315(0.6345)
2023/09/28 19:31:12 - INFO - root -   Epoch: [87/200][200/346], lr: 0.00000022 	 loss = 0.4667(0.6254)
2023/09/28 19:32:05 - INFO - root -   Epoch: [87/200][220/346], lr: 0.00000022 	 loss = 0.4079(0.6283)
2023/09/28 19:33:31 - INFO - root -   Epoch: [87/200][240/346], lr: 0.00000022 	 loss = 0.3511(0.6235)
2023/09/28 19:34:12 - INFO - root -   Epoch: [87/200][260/346], lr: 0.00000022 	 loss = 0.3533(0.6220)
2023/09/28 19:35:29 - INFO - root -   Epoch: [87/200][280/346], lr: 0.00000022 	 loss = 0.6623(0.6320)
2023/09/28 19:36:32 - INFO - root -   Epoch: [87/200][300/346], lr: 0.00000022 	 loss = 0.1613(0.6360)
2023/09/28 19:37:46 - INFO - root -   Epoch: [87/200][320/346], lr: 0.00000022 	 loss = 0.4063(0.6379)
2023/09/28 19:38:31 - INFO - root -   Epoch: [87/200][340/346], lr: 0.00000022 	 loss = 1.0224(0.6334)
2023/09/28 19:38:41 - INFO - root -   Epoch: [87/200] 	 loss = 0.6324
2023/09/28 19:38:41 - INFO - root -   train_accuracy = 0.6561
2023/09/28 19:39:18 - INFO - root -   Epoch: [88/200][0/346], lr: 0.00000023 	 loss = 0.6406(0.6406)
2023/09/28 19:40:15 - INFO - root -   Epoch: [88/200][20/346], lr: 0.00000023 	 loss = 0.2798(0.6031)
2023/09/28 19:41:23 - INFO - root -   Epoch: [88/200][40/346], lr: 0.00000023 	 loss = 0.3329(0.5810)
2023/09/28 19:42:17 - INFO - root -   Epoch: [88/200][60/346], lr: 0.00000023 	 loss = 0.7587(0.5476)
2023/09/28 19:43:29 - INFO - root -   Epoch: [88/200][80/346], lr: 0.00000023 	 loss = 0.8241(0.5859)
2023/09/28 19:44:34 - INFO - root -   Epoch: [88/200][100/346], lr: 0.00000023 	 loss = 0.6977(0.6058)
2023/09/28 19:45:47 - INFO - root -   Epoch: [88/200][120/346], lr: 0.00000023 	 loss = 0.7242(0.6418)
2023/09/28 19:46:48 - INFO - root -   Epoch: [88/200][140/346], lr: 0.00000023 	 loss = 0.9943(0.6408)
2023/09/28 19:48:03 - INFO - root -   Epoch: [88/200][160/346], lr: 0.00000023 	 loss = 0.3348(0.6447)
2023/09/28 19:49:02 - INFO - root -   Epoch: [88/200][180/346], lr: 0.00000023 	 loss = 0.7150(0.6371)
2023/09/28 19:50:15 - INFO - root -   Epoch: [88/200][200/346], lr: 0.00000023 	 loss = 0.2732(0.6307)
2023/09/28 19:51:32 - INFO - root -   Epoch: [88/200][220/346], lr: 0.00000023 	 loss = 0.3493(0.6433)
2023/09/28 19:52:33 - INFO - root -   Epoch: [88/200][240/346], lr: 0.00000023 	 loss = 0.5437(0.6427)
2023/09/28 19:53:41 - INFO - root -   Epoch: [88/200][260/346], lr: 0.00000023 	 loss = 1.1210(0.6379)
2023/09/28 19:54:50 - INFO - root -   Epoch: [88/200][280/346], lr: 0.00000023 	 loss = 0.2978(0.6460)
2023/09/28 19:55:59 - INFO - root -   Epoch: [88/200][300/346], lr: 0.00000023 	 loss = 0.5728(0.6457)
2023/09/28 19:56:57 - INFO - root -   Epoch: [88/200][320/346], lr: 0.00000023 	 loss = 0.4118(0.6403)
2023/09/28 19:57:55 - INFO - root -   Epoch: [88/200][340/346], lr: 0.00000023 	 loss = 0.8975(0.6406)
2023/09/28 19:58:02 - INFO - root -   Epoch: [88/200] 	 loss = 0.6425
2023/09/28 19:58:02 - INFO - root -   train_accuracy = 0.6474
2023/09/28 19:58:24 - INFO - root -   Epoch: [89/200][0/346], lr: 0.00000023 	 loss = 0.2551(0.2551)
2023/09/28 19:59:28 - INFO - root -   Epoch: [89/200][20/346], lr: 0.00000023 	 loss = 0.5442(0.6099)
2023/09/28 20:00:40 - INFO - root -   Epoch: [89/200][40/346], lr: 0.00000023 	 loss = 0.8312(0.6059)
2023/09/28 20:01:39 - INFO - root -   Epoch: [89/200][60/346], lr: 0.00000023 	 loss = 0.3539(0.5941)
2023/09/28 20:02:45 - INFO - root -   Epoch: [89/200][80/346], lr: 0.00000023 	 loss = 1.0608(0.6196)
2023/09/28 20:04:14 - INFO - root -   Epoch: [89/200][100/346], lr: 0.00000023 	 loss = 0.7526(0.6104)
2023/09/28 20:05:06 - INFO - root -   Epoch: [89/200][120/346], lr: 0.00000023 	 loss = 0.4699(0.6325)
2023/09/28 20:06:24 - INFO - root -   Epoch: [89/200][140/346], lr: 0.00000023 	 loss = 1.0319(0.6160)
2023/09/28 20:07:15 - INFO - root -   Epoch: [89/200][160/346], lr: 0.00000023 	 loss = 0.2056(0.6183)
2023/09/28 20:08:43 - INFO - root -   Epoch: [89/200][180/346], lr: 0.00000023 	 loss = 0.4676(0.6158)
2023/09/28 20:09:27 - INFO - root -   Epoch: [89/200][200/346], lr: 0.00000023 	 loss = 0.5547(0.6211)
2023/09/28 20:10:34 - INFO - root -   Epoch: [89/200][220/346], lr: 0.00000023 	 loss = 0.2621(0.6215)
2023/09/28 20:11:50 - INFO - root -   Epoch: [89/200][240/346], lr: 0.00000023 	 loss = 0.3454(0.6232)
2023/09/28 20:12:57 - INFO - root -   Epoch: [89/200][260/346], lr: 0.00000023 	 loss = 0.4020(0.6220)
2023/09/28 20:13:57 - INFO - root -   Epoch: [89/200][280/346], lr: 0.00000023 	 loss = 0.6879(0.6230)
2023/09/28 20:15:15 - INFO - root -   Epoch: [89/200][300/346], lr: 0.00000023 	 loss = 0.2898(0.6271)
2023/09/28 20:16:04 - INFO - root -   Epoch: [89/200][320/346], lr: 0.00000023 	 loss = 0.7655(0.6233)
2023/09/28 20:17:09 - INFO - root -   Epoch: [89/200][340/346], lr: 0.00000023 	 loss = 0.7330(0.6195)
2023/09/28 20:17:12 - INFO - root -   Epoch: [89/200] 	 loss = 0.6182
2023/09/28 20:21:02 - INFO - root -   precision = 0.6782
2023/09/28 20:21:02 - INFO - root -   eval_loss = 0.6085
2023/09/28 20:21:03 - INFO - root -   train_accuracy = 0.6618
2023/09/28 20:21:34 - INFO - root -   Epoch: [90/200][0/346], lr: 0.00000023 	 loss = 0.3746(0.3746)
2023/09/28 20:22:42 - INFO - root -   Epoch: [90/200][20/346], lr: 0.00000023 	 loss = 0.2342(0.5024)
2023/09/28 20:23:31 - INFO - root -   Epoch: [90/200][40/346], lr: 0.00000023 	 loss = 0.7611(0.5293)
2023/09/28 20:24:53 - INFO - root -   Epoch: [90/200][60/346], lr: 0.00000023 	 loss = 0.3551(0.4902)
2023/09/28 20:25:46 - INFO - root -   Epoch: [90/200][80/346], lr: 0.00000023 	 loss = 0.5740(0.5375)
2023/09/28 20:27:02 - INFO - root -   Epoch: [90/200][100/346], lr: 0.00000023 	 loss = 0.5979(0.5457)
2023/09/28 20:27:57 - INFO - root -   Epoch: [90/200][120/346], lr: 0.00000023 	 loss = 0.7074(0.5876)
2023/09/28 20:29:16 - INFO - root -   Epoch: [90/200][140/346], lr: 0.00000023 	 loss = 0.4247(0.5813)
2023/09/28 20:30:17 - INFO - root -   Epoch: [90/200][160/346], lr: 0.00000023 	 loss = 0.4121(0.5928)
2023/09/28 20:31:30 - INFO - root -   Epoch: [90/200][180/346], lr: 0.00000023 	 loss = 0.8315(0.5918)
2023/09/28 20:32:27 - INFO - root -   Epoch: [90/200][200/346], lr: 0.00000023 	 loss = 0.8388(0.5950)
2023/09/28 20:33:43 - INFO - root -   Epoch: [90/200][220/346], lr: 0.00000023 	 loss = 0.1122(0.6107)
2023/09/28 20:34:39 - INFO - root -   Epoch: [90/200][240/346], lr: 0.00000023 	 loss = 0.1461(0.6086)
2023/09/28 20:35:54 - INFO - root -   Epoch: [90/200][260/346], lr: 0.00000023 	 loss = 1.1322(0.6134)
2023/09/28 20:36:43 - INFO - root -   Epoch: [90/200][280/346], lr: 0.00000023 	 loss = 0.9993(0.6219)
2023/09/28 20:37:57 - INFO - root -   Epoch: [90/200][300/346], lr: 0.00000023 	 loss = 0.3436(0.6268)
2023/09/28 20:39:20 - INFO - root -   Epoch: [90/200][320/346], lr: 0.00000023 	 loss = 0.5812(0.6247)
2023/09/28 20:40:06 - INFO - root -   Epoch: [90/200][340/346], lr: 0.00000023 	 loss = 0.9800(0.6264)
2023/09/28 20:40:11 - INFO - root -   Epoch: [90/200] 	 loss = 0.6252
2023/09/28 20:40:11 - INFO - root -   train_accuracy = 0.6445
2023/09/28 20:40:41 - INFO - root -   Epoch: [91/200][0/346], lr: 0.00000023 	 loss = 0.8921(0.8921)
2023/09/28 20:41:54 - INFO - root -   Epoch: [91/200][20/346], lr: 0.00000023 	 loss = 0.6042(0.6253)
2023/09/28 20:43:08 - INFO - root -   Epoch: [91/200][40/346], lr: 0.00000023 	 loss = 0.7772(0.6132)
2023/09/28 20:44:10 - INFO - root -   Epoch: [91/200][60/346], lr: 0.00000023 	 loss = 0.7403(0.5970)
2023/09/28 20:45:16 - INFO - root -   Epoch: [91/200][80/346], lr: 0.00000023 	 loss = 0.7667(0.5992)
2023/09/28 20:46:28 - INFO - root -   Epoch: [91/200][100/346], lr: 0.00000023 	 loss = 0.9864(0.6120)
2023/09/28 20:47:32 - INFO - root -   Epoch: [91/200][120/346], lr: 0.00000023 	 loss = 0.7673(0.6385)
2023/09/28 20:48:46 - INFO - root -   Epoch: [91/200][140/346], lr: 0.00000023 	 loss = 0.8208(0.6329)
2023/09/28 20:49:51 - INFO - root -   Epoch: [91/200][160/346], lr: 0.00000023 	 loss = 0.2228(0.6413)
2023/09/28 20:50:59 - INFO - root -   Epoch: [91/200][180/346], lr: 0.00000023 	 loss = 0.6883(0.6362)
2023/09/28 20:52:17 - INFO - root -   Epoch: [91/200][200/346], lr: 0.00000023 	 loss = 0.4271(0.6336)
2023/09/28 20:53:07 - INFO - root -   Epoch: [91/200][220/346], lr: 0.00000023 	 loss = 0.3195(0.6408)
2023/09/28 20:54:37 - INFO - root -   Epoch: [91/200][240/346], lr: 0.00000023 	 loss = 0.8962(0.6385)
2023/09/28 20:55:27 - INFO - root -   Epoch: [91/200][260/346], lr: 0.00000023 	 loss = 0.4777(0.6315)
2023/09/28 20:56:44 - INFO - root -   Epoch: [91/200][280/346], lr: 0.00000023 	 loss = 1.1861(0.6429)
2023/09/28 20:57:42 - INFO - root -   Epoch: [91/200][300/346], lr: 0.00000023 	 loss = 0.2893(0.6460)
2023/09/28 20:59:01 - INFO - root -   Epoch: [91/200][320/346], lr: 0.00000023 	 loss = 0.3269(0.6513)
2023/09/28 20:59:48 - INFO - root -   Epoch: [91/200][340/346], lr: 0.00000023 	 loss = 0.8053(0.6469)
2023/09/28 20:59:55 - INFO - root -   Epoch: [91/200] 	 loss = 0.6492
2023/09/28 20:59:55 - INFO - root -   train_accuracy = 0.6329
2023/09/28 21:00:26 - INFO - root -   Epoch: [92/200][0/346], lr: 0.00000023 	 loss = 0.6091(0.6091)
2023/09/28 21:01:28 - INFO - root -   Epoch: [92/200][20/346], lr: 0.00000023 	 loss = 0.5947(0.5239)
2023/09/28 21:02:59 - INFO - root -   Epoch: [92/200][40/346], lr: 0.00000023 	 loss = 0.9219(0.6118)
2023/09/28 21:03:52 - INFO - root -   Epoch: [92/200][60/346], lr: 0.00000023 	 loss = 0.5614(0.6043)
2023/09/28 21:05:17 - INFO - root -   Epoch: [92/200][80/346], lr: 0.00000023 	 loss = 0.5262(0.6108)
2023/09/28 21:06:01 - INFO - root -   Epoch: [92/200][100/346], lr: 0.00000023 	 loss = 0.8772(0.6237)
2023/09/28 21:07:14 - INFO - root -   Epoch: [92/200][120/346], lr: 0.00000023 	 loss = 0.2570(0.6446)
2023/09/28 21:08:28 - INFO - root -   Epoch: [92/200][140/346], lr: 0.00000023 	 loss = 0.9442(0.6393)
2023/09/28 21:09:26 - INFO - root -   Epoch: [92/200][160/346], lr: 0.00000023 	 loss = 0.2392(0.6300)
2023/09/28 21:10:38 - INFO - root -   Epoch: [92/200][180/346], lr: 0.00000023 	 loss = 0.8239(0.6282)
2023/09/28 21:11:30 - INFO - root -   Epoch: [92/200][200/346], lr: 0.00000023 	 loss = 0.2110(0.6225)
2023/09/28 21:12:55 - INFO - root -   Epoch: [92/200][220/346], lr: 0.00000023 	 loss = 0.2868(0.6341)
2023/09/28 21:13:55 - INFO - root -   Epoch: [92/200][240/346], lr: 0.00000023 	 loss = 0.3192(0.6357)
2023/09/28 21:14:56 - INFO - root -   Epoch: [92/200][260/346], lr: 0.00000023 	 loss = 0.8909(0.6316)
2023/09/28 21:16:19 - INFO - root -   Epoch: [92/200][280/346], lr: 0.00000023 	 loss = 0.5081(0.6322)
2023/09/28 21:17:18 - INFO - root -   Epoch: [92/200][300/346], lr: 0.00000023 	 loss = 0.7153(0.6425)
2023/09/28 21:18:33 - INFO - root -   Epoch: [92/200][320/346], lr: 0.00000023 	 loss = 0.2905(0.6433)
2023/09/28 21:19:21 - INFO - root -   Epoch: [92/200][340/346], lr: 0.00000023 	 loss = 1.1100(0.6466)
2023/09/28 21:19:30 - INFO - root -   Epoch: [92/200] 	 loss = 0.6451
2023/09/28 21:19:30 - INFO - root -   train_accuracy = 0.6460
2023/09/28 21:20:01 - INFO - root -   Epoch: [93/200][0/346], lr: 0.00000023 	 loss = 0.8780(0.8780)
2023/09/28 21:21:00 - INFO - root -   Epoch: [93/200][20/346], lr: 0.00000023 	 loss = 0.8230(0.5538)
2023/09/28 21:22:23 - INFO - root -   Epoch: [93/200][40/346], lr: 0.00000023 	 loss = 0.2887(0.5903)
2023/09/28 21:23:19 - INFO - root -   Epoch: [93/200][60/346], lr: 0.00000023 	 loss = 0.7525(0.6138)
2023/09/28 21:24:32 - INFO - root -   Epoch: [93/200][80/346], lr: 0.00000023 	 loss = 0.7757(0.6179)
2023/09/28 21:25:36 - INFO - root -   Epoch: [93/200][100/346], lr: 0.00000023 	 loss = 0.2417(0.6097)
2023/09/28 21:26:51 - INFO - root -   Epoch: [93/200][120/346], lr: 0.00000023 	 loss = 0.5891(0.6380)
2023/09/28 21:27:49 - INFO - root -   Epoch: [93/200][140/346], lr: 0.00000023 	 loss = 0.8924(0.6289)
2023/09/28 21:29:00 - INFO - root -   Epoch: [93/200][160/346], lr: 0.00000023 	 loss = 0.2472(0.6366)
2023/09/28 21:29:52 - INFO - root -   Epoch: [93/200][180/346], lr: 0.00000023 	 loss = 0.7029(0.6279)
2023/09/28 21:31:13 - INFO - root -   Epoch: [93/200][200/346], lr: 0.00000023 	 loss = 0.2579(0.6273)
2023/09/28 21:32:15 - INFO - root -   Epoch: [93/200][220/346], lr: 0.00000023 	 loss = 0.1970(0.6341)
2023/09/28 21:33:25 - INFO - root -   Epoch: [93/200][240/346], lr: 0.00000023 	 loss = 0.5023(0.6366)
2023/09/28 21:34:21 - INFO - root -   Epoch: [93/200][260/346], lr: 0.00000023 	 loss = 0.4119(0.6344)
2023/09/28 21:35:46 - INFO - root -   Epoch: [93/200][280/346], lr: 0.00000023 	 loss = 0.9901(0.6425)
2023/09/28 21:36:45 - INFO - root -   Epoch: [93/200][300/346], lr: 0.00000023 	 loss = 0.4873(0.6423)
2023/09/28 21:38:03 - INFO - root -   Epoch: [93/200][320/346], lr: 0.00000023 	 loss = 0.4418(0.6346)
2023/09/28 21:38:59 - INFO - root -   Epoch: [93/200][340/346], lr: 0.00000023 	 loss = 0.6669(0.6335)
2023/09/28 21:39:03 - INFO - root -   Epoch: [93/200] 	 loss = 0.6354
2023/09/28 21:39:03 - INFO - root -   train_accuracy = 0.6460
2023/09/28 21:39:26 - INFO - root -   Epoch: [94/200][0/346], lr: 0.00000023 	 loss = 0.4076(0.4076)
2023/09/28 21:40:47 - INFO - root -   Epoch: [94/200][20/346], lr: 0.00000023 	 loss = 0.2901(0.6316)
2023/09/28 21:41:46 - INFO - root -   Epoch: [94/200][40/346], lr: 0.00000023 	 loss = 0.9611(0.6503)
2023/09/28 21:43:05 - INFO - root -   Epoch: [94/200][60/346], lr: 0.00000023 	 loss = 0.2477(0.6327)
2023/09/28 21:44:07 - INFO - root -   Epoch: [94/200][80/346], lr: 0.00000023 	 loss = 0.4642(0.6363)
2023/09/28 21:45:04 - INFO - root -   Epoch: [94/200][100/346], lr: 0.00000023 	 loss = 0.5780(0.6507)
2023/09/28 21:46:11 - INFO - root -   Epoch: [94/200][120/346], lr: 0.00000023 	 loss = 1.0452(0.6587)
2023/09/28 21:47:25 - INFO - root -   Epoch: [94/200][140/346], lr: 0.00000023 	 loss = 0.7266(0.6535)
2023/09/28 21:48:31 - INFO - root -   Epoch: [94/200][160/346], lr: 0.00000023 	 loss = 0.8973(0.6618)
2023/09/28 21:49:48 - INFO - root -   Epoch: [94/200][180/346], lr: 0.00000023 	 loss = 0.9094(0.6679)
2023/09/28 21:50:45 - INFO - root -   Epoch: [94/200][200/346], lr: 0.00000023 	 loss = 0.2060(0.6597)
2023/09/28 21:51:56 - INFO - root -   Epoch: [94/200][220/346], lr: 0.00000023 	 loss = 0.2729(0.6602)
2023/09/28 21:52:58 - INFO - root -   Epoch: [94/200][240/346], lr: 0.00000023 	 loss = 0.3104(0.6549)
2023/09/28 21:54:18 - INFO - root -   Epoch: [94/200][260/346], lr: 0.00000023 	 loss = 0.6385(0.6496)
2023/09/28 21:55:13 - INFO - root -   Epoch: [94/200][280/346], lr: 0.00000023 	 loss = 0.8054(0.6584)
2023/09/28 21:56:35 - INFO - root -   Epoch: [94/200][300/346], lr: 0.00000023 	 loss = 0.5676(0.6538)
2023/09/28 21:57:29 - INFO - root -   Epoch: [94/200][320/346], lr: 0.00000023 	 loss = 0.3576(0.6538)
2023/09/28 21:58:30 - INFO - root -   Epoch: [94/200][340/346], lr: 0.00000023 	 loss = 0.9267(0.6475)
2023/09/28 21:58:32 - INFO - root -   Epoch: [94/200] 	 loss = 0.6465
2023/09/28 22:02:25 - INFO - root -   precision = 0.6897
2023/09/28 22:02:25 - INFO - root -   eval_loss = 0.5906
2023/09/28 22:02:26 - INFO - root -   train_accuracy = 0.6214
2023/09/28 22:02:49 - INFO - root -   Epoch: [95/200][0/346], lr: 0.00000024 	 loss = 0.2090(0.2090)
2023/09/28 22:03:48 - INFO - root -   Epoch: [95/200][20/346], lr: 0.00000024 	 loss = 0.1828(0.5129)
2023/09/28 22:05:04 - INFO - root -   Epoch: [95/200][40/346], lr: 0.00000024 	 loss = 0.7458(0.5401)
2023/09/28 22:06:15 - INFO - root -   Epoch: [95/200][60/346], lr: 0.00000024 	 loss = 0.2904(0.5422)
2023/09/28 22:07:20 - INFO - root -   Epoch: [95/200][80/346], lr: 0.00000024 	 loss = 0.4940(0.5705)
2023/09/28 22:08:28 - INFO - root -   Epoch: [95/200][100/346], lr: 0.00000024 	 loss = 0.5072(0.5773)
2023/09/28 22:09:30 - INFO - root -   Epoch: [95/200][120/346], lr: 0.00000024 	 loss = 0.5725(0.5967)
2023/09/28 22:10:33 - INFO - root -   Epoch: [95/200][140/346], lr: 0.00000024 	 loss = 0.5923(0.5837)
2023/09/28 22:11:25 - INFO - root -   Epoch: [95/200][160/346], lr: 0.00000024 	 loss = 0.1850(0.5912)
2023/09/28 22:12:41 - INFO - root -   Epoch: [95/200][180/346], lr: 0.00000024 	 loss = 0.5057(0.5818)
2023/09/28 22:13:53 - INFO - root -   Epoch: [95/200][200/346], lr: 0.00000024 	 loss = 0.0997(0.5852)
2023/09/28 22:15:05 - INFO - root -   Epoch: [95/200][220/346], lr: 0.00000024 	 loss = 0.4096(0.6033)
2023/09/28 22:16:08 - INFO - root -   Epoch: [95/200][240/346], lr: 0.00000024 	 loss = 0.3897(0.6100)
2023/09/28 22:17:21 - INFO - root -   Epoch: [95/200][260/346], lr: 0.00000024 	 loss = 0.3936(0.6113)
2023/09/28 22:18:11 - INFO - root -   Epoch: [95/200][280/346], lr: 0.00000024 	 loss = 0.6725(0.6193)
2023/09/28 22:19:29 - INFO - root -   Epoch: [95/200][300/346], lr: 0.00000024 	 loss = 0.5276(0.6188)
2023/09/28 22:20:24 - INFO - root -   Epoch: [95/200][320/346], lr: 0.00000024 	 loss = 0.3941(0.6196)
2023/09/28 22:21:19 - INFO - root -   Epoch: [95/200][340/346], lr: 0.00000024 	 loss = 0.5411(0.6186)
2023/09/28 22:21:23 - INFO - root -   Epoch: [95/200] 	 loss = 0.6180
2023/09/28 22:21:23 - INFO - root -   train_accuracy = 0.6561
2023/09/28 22:21:54 - INFO - root -   Epoch: [96/200][0/346], lr: 0.00000024 	 loss = 0.3651(0.3651)
2023/09/28 22:22:57 - INFO - root -   Epoch: [96/200][20/346], lr: 0.00000024 	 loss = 0.7321(0.5999)
2023/09/28 22:24:05 - INFO - root -   Epoch: [96/200][40/346], lr: 0.00000024 	 loss = 0.1194(0.5946)
2023/09/28 22:25:24 - INFO - root -   Epoch: [96/200][60/346], lr: 0.00000024 	 loss = 0.3552(0.5683)
2023/09/28 22:26:18 - INFO - root -   Epoch: [96/200][80/346], lr: 0.00000024 	 loss = 0.2817(0.5867)
2023/09/28 22:27:47 - INFO - root -   Epoch: [96/200][100/346], lr: 0.00000024 	 loss = 0.6992(0.6175)
2023/09/28 22:28:43 - INFO - root -   Epoch: [96/200][120/346], lr: 0.00000024 	 loss = 0.9606(0.6339)
2023/09/28 22:29:55 - INFO - root -   Epoch: [96/200][140/346], lr: 0.00000024 	 loss = 0.4598(0.6413)
2023/09/28 22:30:57 - INFO - root -   Epoch: [96/200][160/346], lr: 0.00000024 	 loss = 0.4144(0.6482)
2023/09/28 22:32:23 - INFO - root -   Epoch: [96/200][180/346], lr: 0.00000024 	 loss = 1.3187(0.6495)
2023/09/28 22:33:10 - INFO - root -   Epoch: [96/200][200/346], lr: 0.00000024 	 loss = 0.3849(0.6457)
2023/09/28 22:34:43 - INFO - root -   Epoch: [96/200][220/346], lr: 0.00000024 	 loss = 0.3007(0.6469)
2023/09/28 22:35:31 - INFO - root -   Epoch: [96/200][240/346], lr: 0.00000024 	 loss = 0.5817(0.6371)
2023/09/28 22:36:45 - INFO - root -   Epoch: [96/200][260/346], lr: 0.00000024 	 loss = 1.0138(0.6404)
2023/09/28 22:37:37 - INFO - root -   Epoch: [96/200][280/346], lr: 0.00000024 	 loss = 0.6863(0.6453)
2023/09/28 22:39:00 - INFO - root -   Epoch: [96/200][300/346], lr: 0.00000024 	 loss = 0.2828(0.6483)
2023/09/28 22:40:07 - INFO - root -   Epoch: [96/200][320/346], lr: 0.00000024 	 loss = 0.3405(0.6510)
2023/09/28 22:41:08 - INFO - root -   Epoch: [96/200][340/346], lr: 0.00000024 	 loss = 0.3947(0.6421)
2023/09/28 22:41:10 - INFO - root -   Epoch: [96/200] 	 loss = 0.6418
2023/09/28 22:41:10 - INFO - root -   train_accuracy = 0.6488
2023/09/28 22:41:32 - INFO - root -   Epoch: [97/200][0/346], lr: 0.00000024 	 loss = 0.5552(0.5552)
2023/09/28 22:42:40 - INFO - root -   Epoch: [97/200][20/346], lr: 0.00000024 	 loss = 0.3562(0.5608)
2023/09/28 22:43:52 - INFO - root -   Epoch: [97/200][40/346], lr: 0.00000024 	 loss = 0.7394(0.6011)
2023/09/28 22:44:49 - INFO - root -   Epoch: [97/200][60/346], lr: 0.00000024 	 loss = 0.4166(0.5798)
2023/09/28 22:46:14 - INFO - root -   Epoch: [97/200][80/346], lr: 0.00000024 	 loss = 0.7661(0.5962)
2023/09/28 22:47:13 - INFO - root -   Epoch: [97/200][100/346], lr: 0.00000024 	 loss = 0.6573(0.6040)
2023/09/28 22:48:20 - INFO - root -   Epoch: [97/200][120/346], lr: 0.00000024 	 loss = 0.8560(0.6184)
2023/09/28 22:49:20 - INFO - root -   Epoch: [97/200][140/346], lr: 0.00000024 	 loss = 0.7109(0.6137)
2023/09/28 22:50:18 - INFO - root -   Epoch: [97/200][160/346], lr: 0.00000024 	 loss = 0.1907(0.6202)
2023/09/28 22:51:26 - INFO - root -   Epoch: [97/200][180/346], lr: 0.00000024 	 loss = 0.5569(0.6216)
2023/09/28 22:52:35 - INFO - root -   Epoch: [97/200][200/346], lr: 0.00000024 	 loss = 0.1764(0.6301)
2023/09/28 22:53:44 - INFO - root -   Epoch: [97/200][220/346], lr: 0.00000024 	 loss = 0.2321(0.6421)
2023/09/28 22:54:49 - INFO - root -   Epoch: [97/200][240/346], lr: 0.00000024 	 loss = 0.5388(0.6459)
2023/09/28 22:55:50 - INFO - root -   Epoch: [97/200][260/346], lr: 0.00000024 	 loss = 0.6277(0.6438)
2023/09/28 22:56:57 - INFO - root -   Epoch: [97/200][280/346], lr: 0.00000024 	 loss = 0.3376(0.6517)
2023/09/28 22:58:17 - INFO - root -   Epoch: [97/200][300/346], lr: 0.00000024 	 loss = 0.1733(0.6479)
2023/09/28 22:59:08 - INFO - root -   Epoch: [97/200][320/346], lr: 0.00000024 	 loss = 0.4121(0.6501)
2023/09/28 23:00:13 - INFO - root -   Epoch: [97/200][340/346], lr: 0.00000024 	 loss = 0.9124(0.6445)
2023/09/28 23:00:20 - INFO - root -   Epoch: [97/200] 	 loss = 0.6447
2023/09/28 23:00:20 - INFO - root -   train_accuracy = 0.6329
2023/09/28 23:00:50 - INFO - root -   Epoch: [98/200][0/346], lr: 0.00000024 	 loss = 0.2886(0.2886)
2023/09/28 23:01:43 - INFO - root -   Epoch: [98/200][20/346], lr: 0.00000024 	 loss = 0.4902(0.5133)
2023/09/28 23:02:52 - INFO - root -   Epoch: [98/200][40/346], lr: 0.00000024 	 loss = 0.5274(0.5784)
2023/09/28 23:04:10 - INFO - root -   Epoch: [98/200][60/346], lr: 0.00000024 	 loss = 0.3477(0.5726)
2023/09/28 23:05:07 - INFO - root -   Epoch: [98/200][80/346], lr: 0.00000024 	 loss = 0.5703(0.6004)
2023/09/28 23:06:22 - INFO - root -   Epoch: [98/200][100/346], lr: 0.00000024 	 loss = 1.0350(0.6113)
2023/09/28 23:07:34 - INFO - root -   Epoch: [98/200][120/346], lr: 0.00000024 	 loss = 0.4139(0.6361)
2023/09/28 23:08:47 - INFO - root -   Epoch: [98/200][140/346], lr: 0.00000024 	 loss = 1.1218(0.6352)
2023/09/28 23:09:56 - INFO - root -   Epoch: [98/200][160/346], lr: 0.00000024 	 loss = 0.3523(0.6305)
2023/09/28 23:10:58 - INFO - root -   Epoch: [98/200][180/346], lr: 0.00000024 	 loss = 0.9107(0.6299)
2023/09/28 23:11:56 - INFO - root -   Epoch: [98/200][200/346], lr: 0.00000024 	 loss = 0.1105(0.6249)
2023/09/28 23:13:21 - INFO - root -   Epoch: [98/200][220/346], lr: 0.00000024 	 loss = 0.2180(0.6271)
2023/09/28 23:14:17 - INFO - root -   Epoch: [98/200][240/346], lr: 0.00000024 	 loss = 0.1558(0.6184)
2023/09/28 23:15:31 - INFO - root -   Epoch: [98/200][260/346], lr: 0.00000024 	 loss = 0.2238(0.6158)
2023/09/28 23:16:27 - INFO - root -   Epoch: [98/200][280/346], lr: 0.00000024 	 loss = 0.7128(0.6215)
2023/09/28 23:17:47 - INFO - root -   Epoch: [98/200][300/346], lr: 0.00000024 	 loss = 0.5631(0.6232)
2023/09/28 23:18:40 - INFO - root -   Epoch: [98/200][320/346], lr: 0.00000024 	 loss = 0.4309(0.6264)
2023/09/28 23:19:45 - INFO - root -   Epoch: [98/200][340/346], lr: 0.00000024 	 loss = 0.9529(0.6268)
2023/09/28 23:19:49 - INFO - root -   Epoch: [98/200] 	 loss = 0.6297
2023/09/28 23:19:49 - INFO - root -   train_accuracy = 0.6517
2023/09/28 23:20:19 - INFO - root -   Epoch: [99/200][0/346], lr: 0.00000024 	 loss = 0.6644(0.6644)
2023/09/28 23:21:19 - INFO - root -   Epoch: [99/200][20/346], lr: 0.00000024 	 loss = 0.3929(0.6686)
2023/09/28 23:22:35 - INFO - root -   Epoch: [99/200][40/346], lr: 0.00000024 	 loss = 0.2652(0.6381)
2023/09/28 23:23:58 - INFO - root -   Epoch: [99/200][60/346], lr: 0.00000024 	 loss = 0.6648(0.6155)
2023/09/28 23:24:53 - INFO - root -   Epoch: [99/200][80/346], lr: 0.00000024 	 loss = 0.5682(0.6016)
2023/09/28 23:26:20 - INFO - root -   Epoch: [99/200][100/346], lr: 0.00000024 	 loss = 0.3715(0.6103)
2023/09/28 23:27:22 - INFO - root -   Epoch: [99/200][120/346], lr: 0.00000024 	 loss = 0.7000(0.6223)
2023/09/28 23:28:29 - INFO - root -   Epoch: [99/200][140/346], lr: 0.00000024 	 loss = 0.7852(0.6214)
2023/09/28 23:29:19 - INFO - root -   Epoch: [99/200][160/346], lr: 0.00000024 	 loss = 0.6759(0.6259)
2023/09/28 23:30:41 - INFO - root -   Epoch: [99/200][180/346], lr: 0.00000024 	 loss = 0.7923(0.6287)
2023/09/28 23:32:09 - INFO - root -   Epoch: [99/200][200/346], lr: 0.00000024 	 loss = 0.2661(0.6208)
2023/09/28 23:33:05 - INFO - root -   Epoch: [99/200][220/346], lr: 0.00000024 	 loss = 0.2833(0.6255)
2023/09/28 23:34:17 - INFO - root -   Epoch: [99/200][240/346], lr: 0.00000024 	 loss = 0.7188(0.6250)
2023/09/28 23:35:01 - INFO - root -   Epoch: [99/200][260/346], lr: 0.00000024 	 loss = 1.1078(0.6326)
2023/09/28 23:36:11 - INFO - root -   Epoch: [99/200][280/346], lr: 0.00000024 	 loss = 0.8251(0.6346)
2023/09/28 23:37:22 - INFO - root -   Epoch: [99/200][300/346], lr: 0.00000024 	 loss = 0.4426(0.6257)
2023/09/28 23:38:23 - INFO - root -   Epoch: [99/200][320/346], lr: 0.00000024 	 loss = 0.2817(0.6284)
2023/09/28 23:39:28 - INFO - root -   Epoch: [99/200][340/346], lr: 0.00000024 	 loss = 1.0077(0.6251)
2023/09/28 23:39:31 - INFO - root -   Epoch: [99/200] 	 loss = 0.6233
2023/09/28 23:43:22 - INFO - root -   precision = 0.6609
2023/09/28 23:43:22 - INFO - root -   eval_loss = 0.6413
2023/09/28 23:43:23 - INFO - root -   train_accuracy = 0.6561
2023/09/28 23:43:52 - INFO - root -   Epoch: [100/200][0/346], lr: 0.00000024 	 loss = 0.2470(0.2470)
2023/09/28 23:44:56 - INFO - root -   Epoch: [100/200][20/346], lr: 0.00000024 	 loss = 0.4785(0.5362)
2023/09/28 23:45:59 - INFO - root -   Epoch: [100/200][40/346], lr: 0.00000024 	 loss = 0.3782(0.6067)
2023/09/28 23:47:21 - INFO - root -   Epoch: [100/200][60/346], lr: 0.00000024 	 loss = 0.6931(0.5879)
2023/09/28 23:48:20 - INFO - root -   Epoch: [100/200][80/346], lr: 0.00000024 	 loss = 0.9353(0.6189)
2023/09/28 23:49:39 - INFO - root -   Epoch: [100/200][100/346], lr: 0.00000024 	 loss = 0.7007(0.6256)
2023/09/28 23:50:32 - INFO - root -   Epoch: [100/200][120/346], lr: 0.00000024 	 loss = 0.4106(0.6336)
2023/09/28 23:51:52 - INFO - root -   Epoch: [100/200][140/346], lr: 0.00000024 	 loss = 0.4933(0.6329)
2023/09/28 23:52:51 - INFO - root -   Epoch: [100/200][160/346], lr: 0.00000024 	 loss = 0.3473(0.6369)
2023/09/28 23:54:00 - INFO - root -   Epoch: [100/200][180/346], lr: 0.00000024 	 loss = 0.6311(0.6328)
2023/09/28 23:54:51 - INFO - root -   Epoch: [100/200][200/346], lr: 0.00000024 	 loss = 0.2383(0.6310)
2023/09/28 23:56:05 - INFO - root -   Epoch: [100/200][220/346], lr: 0.00000024 	 loss = 0.1736(0.6397)
2023/09/28 23:57:23 - INFO - root -   Epoch: [100/200][240/346], lr: 0.00000024 	 loss = 0.2466(0.6375)
2023/09/28 23:58:22 - INFO - root -   Epoch: [100/200][260/346], lr: 0.00000024 	 loss = 0.7265(0.6357)
2023/09/28 23:59:27 - INFO - root -   Epoch: [100/200][280/346], lr: 0.00000024 	 loss = 0.4736(0.6374)
2023/09/29 00:00:19 - INFO - root -   Epoch: [100/200][300/346], lr: 0.00000024 	 loss = 0.5941(0.6321)
2023/09/29 00:01:23 - INFO - root -   Epoch: [100/200][320/346], lr: 0.00000024 	 loss = 0.5953(0.6328)
2023/09/29 00:02:26 - INFO - root -   Epoch: [100/200][340/346], lr: 0.00000024 	 loss = 0.8175(0.6245)
2023/09/29 00:02:28 - INFO - root -   Epoch: [100/200] 	 loss = 0.6227
2023/09/29 00:02:28 - INFO - root -   train_accuracy = 0.6590
2023/09/29 00:02:59 - INFO - root -   Epoch: [101/200][0/346], lr: 0.00000024 	 loss = 0.3988(0.3988)
2023/09/29 00:03:59 - INFO - root -   Epoch: [101/200][20/346], lr: 0.00000024 	 loss = 0.3721(0.5917)
2023/09/29 00:05:10 - INFO - root -   Epoch: [101/200][40/346], lr: 0.00000024 	 loss = 0.5856(0.6060)
2023/09/29 00:06:15 - INFO - root -   Epoch: [101/200][60/346], lr: 0.00000024 	 loss = 0.5953(0.5908)
2023/09/29 00:07:33 - INFO - root -   Epoch: [101/200][80/346], lr: 0.00000024 	 loss = 0.4088(0.6106)
2023/09/29 00:08:55 - INFO - root -   Epoch: [101/200][100/346], lr: 0.00000024 	 loss = 1.1102(0.6289)
2023/09/29 00:09:45 - INFO - root -   Epoch: [101/200][120/346], lr: 0.00000024 	 loss = 0.5398(0.6542)
2023/09/29 00:11:12 - INFO - root -   Epoch: [101/200][140/346], lr: 0.00000024 	 loss = 0.2550(0.6315)
2023/09/29 00:12:05 - INFO - root -   Epoch: [101/200][160/346], lr: 0.00000024 	 loss = 0.1967(0.6341)
2023/09/29 00:13:20 - INFO - root -   Epoch: [101/200][180/346], lr: 0.00000024 	 loss = 0.7327(0.6311)
2023/09/29 00:14:08 - INFO - root -   Epoch: [101/200][200/346], lr: 0.00000024 	 loss = 0.2247(0.6327)
2023/09/29 00:15:34 - INFO - root -   Epoch: [101/200][220/346], lr: 0.00000024 	 loss = 0.4480(0.6389)
2023/09/29 00:16:21 - INFO - root -   Epoch: [101/200][240/346], lr: 0.00000024 	 loss = 0.4739(0.6327)
2023/09/29 00:17:53 - INFO - root -   Epoch: [101/200][260/346], lr: 0.00000024 	 loss = 0.8339(0.6264)
2023/09/29 00:18:49 - INFO - root -   Epoch: [101/200][280/346], lr: 0.00000024 	 loss = 1.3011(0.6353)
2023/09/29 00:19:53 - INFO - root -   Epoch: [101/200][300/346], lr: 0.00000024 	 loss = 0.2503(0.6336)
2023/09/29 00:21:12 - INFO - root -   Epoch: [101/200][320/346], lr: 0.00000024 	 loss = 0.1488(0.6327)
2023/09/29 00:22:01 - INFO - root -   Epoch: [101/200][340/346], lr: 0.00000024 	 loss = 1.1163(0.6346)
2023/09/29 00:22:04 - INFO - root -   Epoch: [101/200] 	 loss = 0.6368
2023/09/29 00:22:04 - INFO - root -   train_accuracy = 0.6445
2023/09/29 00:22:27 - INFO - root -   Epoch: [102/200][0/346], lr: 0.00000025 	 loss = 1.0702(1.0702)
2023/09/29 00:23:32 - INFO - root -   Epoch: [102/200][20/346], lr: 0.00000025 	 loss = 0.3461(0.5888)
2023/09/29 00:24:50 - INFO - root -   Epoch: [102/200][40/346], lr: 0.00000025 	 loss = 0.5484(0.5981)
2023/09/29 00:25:38 - INFO - root -   Epoch: [102/200][60/346], lr: 0.00000025 	 loss = 0.6075(0.5701)
2023/09/29 00:26:59 - INFO - root -   Epoch: [102/200][80/346], lr: 0.00000025 	 loss = 0.5965(0.5669)
2023/09/29 00:27:53 - INFO - root -   Epoch: [102/200][100/346], lr: 0.00000025 	 loss = 0.6549(0.5966)
2023/09/29 00:29:02 - INFO - root -   Epoch: [102/200][120/346], lr: 0.00000025 	 loss = 0.3805(0.6039)
2023/09/29 00:30:19 - INFO - root -   Epoch: [102/200][140/346], lr: 0.00000025 	 loss = 1.2025(0.6266)
2023/09/29 00:31:25 - INFO - root -   Epoch: [102/200][160/346], lr: 0.00000025 	 loss = 0.2512(0.6228)
2023/09/29 00:32:27 - INFO - root -   Epoch: [102/200][180/346], lr: 0.00000025 	 loss = 1.1486(0.6278)
2023/09/29 00:33:43 - INFO - root -   Epoch: [102/200][200/346], lr: 0.00000025 	 loss = 0.3731(0.6258)
2023/09/29 00:34:43 - INFO - root -   Epoch: [102/200][220/346], lr: 0.00000025 	 loss = 0.1128(0.6324)
2023/09/29 00:36:12 - INFO - root -   Epoch: [102/200][240/346], lr: 0.00000025 	 loss = 0.4370(0.6257)
2023/09/29 00:37:11 - INFO - root -   Epoch: [102/200][260/346], lr: 0.00000025 	 loss = 0.6173(0.6200)
2023/09/29 00:38:34 - INFO - root -   Epoch: [102/200][280/346], lr: 0.00000025 	 loss = 0.6742(0.6385)
2023/09/29 00:39:33 - INFO - root -   Epoch: [102/200][300/346], lr: 0.00000025 	 loss = 0.2996(0.6399)
2023/09/29 00:40:42 - INFO - root -   Epoch: [102/200][320/346], lr: 0.00000025 	 loss = 0.4908(0.6412)
2023/09/29 00:41:40 - INFO - root -   Epoch: [102/200][340/346], lr: 0.00000025 	 loss = 1.0095(0.6414)
2023/09/29 00:41:42 - INFO - root -   Epoch: [102/200] 	 loss = 0.6397
2023/09/29 00:41:42 - INFO - root -   train_accuracy = 0.6402
2023/09/29 00:42:03 - INFO - root -   Epoch: [103/200][0/346], lr: 0.00000025 	 loss = 0.3481(0.3481)
2023/09/29 00:43:25 - INFO - root -   Epoch: [103/200][20/346], lr: 0.00000025 	 loss = 0.4795(0.5598)
2023/09/29 00:44:12 - INFO - root -   Epoch: [103/200][40/346], lr: 0.00000025 	 loss = 0.5384(0.6233)
2023/09/29 00:45:37 - INFO - root -   Epoch: [103/200][60/346], lr: 0.00000025 	 loss = 0.4059(0.5648)
2023/09/29 00:46:27 - INFO - root -   Epoch: [103/200][80/346], lr: 0.00000025 	 loss = 0.5207(0.5654)
2023/09/29 00:47:36 - INFO - root -   Epoch: [103/200][100/346], lr: 0.00000025 	 loss = 0.7960(0.5968)
2023/09/29 00:48:33 - INFO - root -   Epoch: [103/200][120/346], lr: 0.00000025 	 loss = 0.6097(0.5947)
2023/09/29 00:49:51 - INFO - root -   Epoch: [103/200][140/346], lr: 0.00000025 	 loss = 0.3141(0.5893)
2023/09/29 00:50:55 - INFO - root -   Epoch: [103/200][160/346], lr: 0.00000025 	 loss = 0.1760(0.5874)
2023/09/29 00:52:12 - INFO - root -   Epoch: [103/200][180/346], lr: 0.00000025 	 loss = 0.8835(0.5911)
2023/09/29 00:53:13 - INFO - root -   Epoch: [103/200][200/346], lr: 0.00000025 	 loss = 0.3523(0.5912)
2023/09/29 00:54:30 - INFO - root -   Epoch: [103/200][220/346], lr: 0.00000025 	 loss = 0.1669(0.6004)
2023/09/29 00:55:22 - INFO - root -   Epoch: [103/200][240/346], lr: 0.00000025 	 loss = 0.6132(0.5976)
2023/09/29 00:56:42 - INFO - root -   Epoch: [103/200][260/346], lr: 0.00000025 	 loss = 0.3272(0.6069)
2023/09/29 00:57:45 - INFO - root -   Epoch: [103/200][280/346], lr: 0.00000025 	 loss = 1.2455(0.6150)
2023/09/29 00:58:58 - INFO - root -   Epoch: [103/200][300/346], lr: 0.00000025 	 loss = 0.2783(0.6178)
2023/09/29 00:59:50 - INFO - root -   Epoch: [103/200][320/346], lr: 0.00000025 	 loss = 0.3332(0.6199)
2023/09/29 01:00:54 - INFO - root -   Epoch: [103/200][340/346], lr: 0.00000025 	 loss = 1.1072(0.6189)
2023/09/29 01:00:58 - INFO - root -   Epoch: [103/200] 	 loss = 0.6175
2023/09/29 01:00:58 - INFO - root -   train_accuracy = 0.6618
2023/09/29 01:01:20 - INFO - root -   Epoch: [104/200][0/346], lr: 0.00000025 	 loss = 0.3846(0.3846)
2023/09/29 01:02:27 - INFO - root -   Epoch: [104/200][20/346], lr: 0.00000025 	 loss = 0.4544(0.5596)
2023/09/29 01:03:38 - INFO - root -   Epoch: [104/200][40/346], lr: 0.00000025 	 loss = 0.5374(0.6094)
2023/09/29 01:04:27 - INFO - root -   Epoch: [104/200][60/346], lr: 0.00000025 	 loss = 0.5203(0.5761)
2023/09/29 01:05:37 - INFO - root -   Epoch: [104/200][80/346], lr: 0.00000025 	 loss = 0.5211(0.5865)
2023/09/29 01:06:42 - INFO - root -   Epoch: [104/200][100/346], lr: 0.00000025 	 loss = 0.9122(0.6111)
2023/09/29 01:08:08 - INFO - root -   Epoch: [104/200][120/346], lr: 0.00000025 	 loss = 0.8252(0.6382)
2023/09/29 01:09:05 - INFO - root -   Epoch: [104/200][140/346], lr: 0.00000025 	 loss = 0.7827(0.6390)
2023/09/29 01:10:01 - INFO - root -   Epoch: [104/200][160/346], lr: 0.00000025 	 loss = 0.3005(0.6436)
2023/09/29 01:11:23 - INFO - root -   Epoch: [104/200][180/346], lr: 0.00000025 	 loss = 1.1717(0.6486)
2023/09/29 01:12:46 - INFO - root -   Epoch: [104/200][200/346], lr: 0.00000025 	 loss = 0.4262(0.6427)
2023/09/29 01:13:53 - INFO - root -   Epoch: [104/200][220/346], lr: 0.00000025 	 loss = 0.2875(0.6514)
2023/09/29 01:15:04 - INFO - root -   Epoch: [104/200][240/346], lr: 0.00000025 	 loss = 0.4434(0.6428)
2023/09/29 01:16:12 - INFO - root -   Epoch: [104/200][260/346], lr: 0.00000025 	 loss = 0.6400(0.6418)
2023/09/29 01:17:03 - INFO - root -   Epoch: [104/200][280/346], lr: 0.00000025 	 loss = 0.3458(0.6471)
2023/09/29 01:18:25 - INFO - root -   Epoch: [104/200][300/346], lr: 0.00000025 	 loss = 0.5378(0.6487)
2023/09/29 01:19:25 - INFO - root -   Epoch: [104/200][320/346], lr: 0.00000025 	 loss = 0.2710(0.6488)
2023/09/29 01:20:25 - INFO - root -   Epoch: [104/200][340/346], lr: 0.00000025 	 loss = 0.7511(0.6471)
2023/09/29 01:20:27 - INFO - root -   Epoch: [104/200] 	 loss = 0.6490
2023/09/29 01:24:19 - INFO - root -   precision = 0.6954
2023/09/29 01:24:19 - INFO - root -   eval_loss = 0.5804
2023/09/29 01:24:20 - INFO - root -   train_accuracy = 0.6272
2023/09/29 01:24:50 - INFO - root -   Epoch: [105/200][0/346], lr: 0.00000025 	 loss = 0.3294(0.3294)
2023/09/29 01:25:50 - INFO - root -   Epoch: [105/200][20/346], lr: 0.00000025 	 loss = 0.5901(0.5624)
2023/09/29 01:27:07 - INFO - root -   Epoch: [105/200][40/346], lr: 0.00000025 	 loss = 1.0370(0.5610)
2023/09/29 01:28:11 - INFO - root -   Epoch: [105/200][60/346], lr: 0.00000025 	 loss = 0.9774(0.5590)
2023/09/29 01:29:09 - INFO - root -   Epoch: [105/200][80/346], lr: 0.00000025 	 loss = 0.6637(0.5268)
2023/09/29 01:30:13 - INFO - root -   Epoch: [105/200][100/346], lr: 0.00000025 	 loss = 0.8615(0.5695)
2023/09/29 01:31:23 - INFO - root -   Epoch: [105/200][120/346], lr: 0.00000025 	 loss = 1.0593(0.6101)
2023/09/29 01:32:43 - INFO - root -   Epoch: [105/200][140/346], lr: 0.00000025 	 loss = 0.6404(0.6205)
2023/09/29 01:33:31 - INFO - root -   Epoch: [105/200][160/346], lr: 0.00000025 	 loss = 0.3306(0.6235)
2023/09/29 01:34:48 - INFO - root -   Epoch: [105/200][180/346], lr: 0.00000025 	 loss = 0.6129(0.6244)
2023/09/29 01:35:45 - INFO - root -   Epoch: [105/200][200/346], lr: 0.00000025 	 loss = 0.2331(0.6238)
2023/09/29 01:36:55 - INFO - root -   Epoch: [105/200][220/346], lr: 0.00000025 	 loss = 0.2814(0.6321)
2023/09/29 01:38:08 - INFO - root -   Epoch: [105/200][240/346], lr: 0.00000025 	 loss = 1.3686(0.6331)
2023/09/29 01:39:17 - INFO - root -   Epoch: [105/200][260/346], lr: 0.00000025 	 loss = 0.6101(0.6286)
2023/09/29 01:40:29 - INFO - root -   Epoch: [105/200][280/346], lr: 0.00000025 	 loss = 0.5048(0.6349)
2023/09/29 01:41:30 - INFO - root -   Epoch: [105/200][300/346], lr: 0.00000025 	 loss = 0.8082(0.6322)
2023/09/29 01:42:55 - INFO - root -   Epoch: [105/200][320/346], lr: 0.00000025 	 loss = 0.2618(0.6300)
2023/09/29 01:43:32 - INFO - root -   Epoch: [105/200][340/346], lr: 0.00000025 	 loss = 0.8307(0.6248)
2023/09/29 01:43:40 - INFO - root -   Epoch: [105/200] 	 loss = 0.6242
2023/09/29 01:43:40 - INFO - root -   train_accuracy = 0.6734
2023/09/29 01:44:02 - INFO - root -   Epoch: [106/200][0/346], lr: 0.00000025 	 loss = 0.2860(0.2860)
2023/09/29 01:45:24 - INFO - root -   Epoch: [106/200][20/346], lr: 0.00000025 	 loss = 0.2311(0.5175)
2023/09/29 01:46:22 - INFO - root -   Epoch: [106/200][40/346], lr: 0.00000025 	 loss = 0.8312(0.5834)
2023/09/29 01:47:43 - INFO - root -   Epoch: [106/200][60/346], lr: 0.00000025 	 loss = 0.4694(0.5848)
2023/09/29 01:48:39 - INFO - root -   Epoch: [106/200][80/346], lr: 0.00000025 	 loss = 0.4504(0.6221)
2023/09/29 01:49:54 - INFO - root -   Epoch: [106/200][100/346], lr: 0.00000025 	 loss = 0.4228(0.6152)
2023/09/29 01:50:57 - INFO - root -   Epoch: [106/200][120/346], lr: 0.00000025 	 loss = 0.3095(0.6499)
2023/09/29 01:52:08 - INFO - root -   Epoch: [106/200][140/346], lr: 0.00000025 	 loss = 0.5381(0.6275)
2023/09/29 01:52:52 - INFO - root -   Epoch: [106/200][160/346], lr: 0.00000025 	 loss = 0.6080(0.6264)
2023/09/29 01:54:13 - INFO - root -   Epoch: [106/200][180/346], lr: 0.00000025 	 loss = 0.3567(0.6225)
2023/09/29 01:55:09 - INFO - root -   Epoch: [106/200][200/346], lr: 0.00000025 	 loss = 0.2610(0.6262)
2023/09/29 01:56:17 - INFO - root -   Epoch: [106/200][220/346], lr: 0.00000025 	 loss = 0.2504(0.6354)
2023/09/29 01:57:06 - INFO - root -   Epoch: [106/200][240/346], lr: 0.00000025 	 loss = 0.2089(0.6279)
2023/09/29 01:58:26 - INFO - root -   Epoch: [106/200][260/346], lr: 0.00000025 	 loss = 0.5392(0.6257)
2023/09/29 01:59:22 - INFO - root -   Epoch: [106/200][280/346], lr: 0.00000025 	 loss = 0.8873(0.6324)
2023/09/29 02:00:41 - INFO - root -   Epoch: [106/200][300/346], lr: 0.00000025 	 loss = 0.4113(0.6330)
2023/09/29 02:01:45 - INFO - root -   Epoch: [106/200][320/346], lr: 0.00000025 	 loss = 0.3156(0.6265)
2023/09/29 02:02:47 - INFO - root -   Epoch: [106/200][340/346], lr: 0.00000025 	 loss = 0.6279(0.6241)
2023/09/29 02:02:50 - INFO - root -   Epoch: [106/200] 	 loss = 0.6242
2023/09/29 02:02:50 - INFO - root -   train_accuracy = 0.6460
2023/09/29 02:03:13 - INFO - root -   Epoch: [107/200][0/346], lr: 0.00000025 	 loss = 0.2315(0.2315)
2023/09/29 02:04:43 - INFO - root -   Epoch: [107/200][20/346], lr: 0.00000025 	 loss = 0.7329(0.6220)
2023/09/29 02:05:28 - INFO - root -   Epoch: [107/200][40/346], lr: 0.00000025 	 loss = 0.5402(0.6476)
2023/09/29 02:06:46 - INFO - root -   Epoch: [107/200][60/346], lr: 0.00000025 	 loss = 0.5085(0.6134)
2023/09/29 02:07:43 - INFO - root -   Epoch: [107/200][80/346], lr: 0.00000025 	 loss = 0.3855(0.6527)
2023/09/29 02:08:53 - INFO - root -   Epoch: [107/200][100/346], lr: 0.00000025 	 loss = 0.8311(0.6601)
2023/09/29 02:10:01 - INFO - root -   Epoch: [107/200][120/346], lr: 0.00000025 	 loss = 0.3528(0.6762)
2023/09/29 02:11:23 - INFO - root -   Epoch: [107/200][140/346], lr: 0.00000025 	 loss = 0.3476(0.6616)
2023/09/29 02:12:14 - INFO - root -   Epoch: [107/200][160/346], lr: 0.00000025 	 loss = 0.3507(0.6593)
2023/09/29 02:13:46 - INFO - root -   Epoch: [107/200][180/346], lr: 0.00000025 	 loss = 1.0231(0.6492)
2023/09/29 02:14:39 - INFO - root -   Epoch: [107/200][200/346], lr: 0.00000025 	 loss = 0.1167(0.6387)
2023/09/29 02:16:09 - INFO - root -   Epoch: [107/200][220/346], lr: 0.00000025 	 loss = 0.3775(0.6435)
2023/09/29 02:16:52 - INFO - root -   Epoch: [107/200][240/346], lr: 0.00000025 	 loss = 0.3659(0.6481)
2023/09/29 02:18:10 - INFO - root -   Epoch: [107/200][260/346], lr: 0.00000025 	 loss = 0.5089(0.6413)
2023/09/29 02:19:16 - INFO - root -   Epoch: [107/200][280/346], lr: 0.00000025 	 loss = 0.5703(0.6444)
2023/09/29 02:20:14 - INFO - root -   Epoch: [107/200][300/346], lr: 0.00000025 	 loss = 0.2200(0.6428)
2023/09/29 02:21:22 - INFO - root -   Epoch: [107/200][320/346], lr: 0.00000025 	 loss = 0.2672(0.6352)
2023/09/29 02:22:27 - INFO - root -   Epoch: [107/200][340/346], lr: 0.00000025 	 loss = 1.3478(0.6346)
2023/09/29 02:22:31 - INFO - root -   Epoch: [107/200] 	 loss = 0.6338
2023/09/29 02:22:31 - INFO - root -   train_accuracy = 0.6387
2023/09/29 02:23:03 - INFO - root -   Epoch: [108/200][0/346], lr: 0.00000025 	 loss = 0.4523(0.4523)
2023/09/29 02:24:11 - INFO - root -   Epoch: [108/200][20/346], lr: 0.00000025 	 loss = 0.3604(0.5428)
2023/09/29 02:25:15 - INFO - root -   Epoch: [108/200][40/346], lr: 0.00000025 	 loss = 0.6939(0.5825)
2023/09/29 02:26:35 - INFO - root -   Epoch: [108/200][60/346], lr: 0.00000025 	 loss = 0.4715(0.5954)
2023/09/29 02:27:36 - INFO - root -   Epoch: [108/200][80/346], lr: 0.00000025 	 loss = 0.4311(0.6043)
2023/09/29 02:28:59 - INFO - root -   Epoch: [108/200][100/346], lr: 0.00000025 	 loss = 0.9173(0.6168)
2023/09/29 02:29:49 - INFO - root -   Epoch: [108/200][120/346], lr: 0.00000025 	 loss = 0.6172(0.6310)
2023/09/29 02:30:52 - INFO - root -   Epoch: [108/200][140/346], lr: 0.00000025 	 loss = 0.6040(0.6256)
2023/09/29 02:32:03 - INFO - root -   Epoch: [108/200][160/346], lr: 0.00000025 	 loss = 0.6199(0.6299)
2023/09/29 02:33:05 - INFO - root -   Epoch: [108/200][180/346], lr: 0.00000025 	 loss = 0.5663(0.6323)
2023/09/29 02:34:21 - INFO - root -   Epoch: [108/200][200/346], lr: 0.00000025 	 loss = 0.3837(0.6317)
2023/09/29 02:35:26 - INFO - root -   Epoch: [108/200][220/346], lr: 0.00000025 	 loss = 0.6343(0.6415)
2023/09/29 02:36:48 - INFO - root -   Epoch: [108/200][240/346], lr: 0.00000025 	 loss = 0.7256(0.6373)
2023/09/29 02:37:47 - INFO - root -   Epoch: [108/200][260/346], lr: 0.00000025 	 loss = 0.6119(0.6392)
2023/09/29 02:39:00 - INFO - root -   Epoch: [108/200][280/346], lr: 0.00000025 	 loss = 0.5630(0.6423)
2023/09/29 02:39:43 - INFO - root -   Epoch: [108/200][300/346], lr: 0.00000025 	 loss = 1.1352(0.6380)
2023/09/29 02:41:08 - INFO - root -   Epoch: [108/200][320/346], lr: 0.00000025 	 loss = 0.7734(0.6422)
2023/09/29 02:41:57 - INFO - root -   Epoch: [108/200][340/346], lr: 0.00000025 	 loss = 1.0627(0.6371)
2023/09/29 02:41:59 - INFO - root -   Epoch: [108/200] 	 loss = 0.6360
2023/09/29 02:41:59 - INFO - root -   train_accuracy = 0.6460
2023/09/29 02:42:39 - INFO - root -   Epoch: [109/200][0/346], lr: 0.00000026 	 loss = 0.4280(0.4280)
2023/09/29 02:43:23 - INFO - root -   Epoch: [109/200][20/346], lr: 0.00000026 	 loss = 0.5813(0.5174)
2023/09/29 02:44:43 - INFO - root -   Epoch: [109/200][40/346], lr: 0.00000026 	 loss = 0.3120(0.5520)
2023/09/29 02:45:45 - INFO - root -   Epoch: [109/200][60/346], lr: 0.00000026 	 loss = 0.6224(0.5403)
2023/09/29 02:46:45 - INFO - root -   Epoch: [109/200][80/346], lr: 0.00000026 	 loss = 0.3952(0.5680)
2023/09/29 02:47:59 - INFO - root -   Epoch: [109/200][100/346], lr: 0.00000026 	 loss = 0.7605(0.5730)
2023/09/29 02:49:14 - INFO - root -   Epoch: [109/200][120/346], lr: 0.00000026 	 loss = 0.4962(0.5930)
2023/09/29 02:50:11 - INFO - root -   Epoch: [109/200][140/346], lr: 0.00000026 	 loss = 0.7033(0.5834)
2023/09/29 02:51:21 - INFO - root -   Epoch: [109/200][160/346], lr: 0.00000026 	 loss = 0.2139(0.5938)
2023/09/29 02:52:27 - INFO - root -   Epoch: [109/200][180/346], lr: 0.00000026 	 loss = 1.2525(0.6077)
2023/09/29 02:53:38 - INFO - root -   Epoch: [109/200][200/346], lr: 0.00000026 	 loss = 0.2235(0.6042)
2023/09/29 02:54:53 - INFO - root -   Epoch: [109/200][220/346], lr: 0.00000026 	 loss = 0.1771(0.6132)
2023/09/29 02:55:53 - INFO - root -   Epoch: [109/200][240/346], lr: 0.00000026 	 loss = 0.4049(0.6115)
2023/09/29 02:56:53 - INFO - root -   Epoch: [109/200][260/346], lr: 0.00000026 	 loss = 0.6569(0.6098)
2023/09/29 02:57:54 - INFO - root -   Epoch: [109/200][280/346], lr: 0.00000026 	 loss = 1.0328(0.6154)
2023/09/29 02:59:07 - INFO - root -   Epoch: [109/200][300/346], lr: 0.00000026 	 loss = 0.2742(0.6181)
2023/09/29 03:00:26 - INFO - root -   Epoch: [109/200][320/346], lr: 0.00000026 	 loss = 0.3095(0.6109)
2023/09/29 03:01:27 - INFO - root -   Epoch: [109/200][340/346], lr: 0.00000026 	 loss = 0.5150(0.6132)
2023/09/29 03:01:31 - INFO - root -   Epoch: [109/200] 	 loss = 0.6119
2023/09/29 03:05:22 - INFO - root -   precision = 0.6782
2023/09/29 03:05:22 - INFO - root -   eval_loss = 0.5911
2023/09/29 03:05:23 - INFO - root -   train_accuracy = 0.6879
2023/09/29 03:05:45 - INFO - root -   Epoch: [110/200][0/346], lr: 0.00000026 	 loss = 0.2705(0.2705)
2023/09/29 03:06:54 - INFO - root -   Epoch: [110/200][20/346], lr: 0.00000026 	 loss = 0.3839(0.5931)
2023/09/29 03:08:01 - INFO - root -   Epoch: [110/200][40/346], lr: 0.00000026 	 loss = 0.3875(0.5802)
2023/09/29 03:09:21 - INFO - root -   Epoch: [110/200][60/346], lr: 0.00000026 	 loss = 0.3315(0.5849)
2023/09/29 03:10:21 - INFO - root -   Epoch: [110/200][80/346], lr: 0.00000026 	 loss = 0.1403(0.5969)
2023/09/29 03:11:38 - INFO - root -   Epoch: [110/200][100/346], lr: 0.00000026 	 loss = 0.4544(0.5968)
2023/09/29 03:12:36 - INFO - root -   Epoch: [110/200][120/346], lr: 0.00000026 	 loss = 0.3877(0.6129)
2023/09/29 03:13:47 - INFO - root -   Epoch: [110/200][140/346], lr: 0.00000026 	 loss = 0.4734(0.6091)
2023/09/29 03:14:45 - INFO - root -   Epoch: [110/200][160/346], lr: 0.00000026 	 loss = 0.4693(0.6153)
2023/09/29 03:16:09 - INFO - root -   Epoch: [110/200][180/346], lr: 0.00000026 	 loss = 0.7662(0.6108)
2023/09/29 03:17:11 - INFO - root -   Epoch: [110/200][200/346], lr: 0.00000026 	 loss = 0.3057(0.6129)
2023/09/29 03:18:32 - INFO - root -   Epoch: [110/200][220/346], lr: 0.00000026 	 loss = 0.7155(0.6102)
2023/09/29 03:19:23 - INFO - root -   Epoch: [110/200][240/346], lr: 0.00000026 	 loss = 0.2086(0.6062)
2023/09/29 03:20:30 - INFO - root -   Epoch: [110/200][260/346], lr: 0.00000026 	 loss = 0.5102(0.6093)
2023/09/29 03:21:41 - INFO - root -   Epoch: [110/200][280/346], lr: 0.00000026 	 loss = 0.4701(0.6115)
2023/09/29 03:22:34 - INFO - root -   Epoch: [110/200][300/346], lr: 0.00000026 	 loss = 0.3303(0.6028)
2023/09/29 03:23:55 - INFO - root -   Epoch: [110/200][320/346], lr: 0.00000026 	 loss = 0.8266(0.6023)
2023/09/29 03:24:44 - INFO - root -   Epoch: [110/200][340/346], lr: 0.00000026 	 loss = 0.5346(0.6051)
2023/09/29 03:24:50 - INFO - root -   Epoch: [110/200] 	 loss = 0.6041
2023/09/29 03:24:50 - INFO - root -   train_accuracy = 0.6618
2023/09/29 03:25:20 - INFO - root -   Epoch: [111/200][0/346], lr: 0.00000026 	 loss = 0.8915(0.8915)
2023/09/29 03:26:28 - INFO - root -   Epoch: [111/200][20/346], lr: 0.00000026 	 loss = 0.4097(0.5601)
2023/09/29 03:27:35 - INFO - root -   Epoch: [111/200][40/346], lr: 0.00000026 	 loss = 0.6477(0.5793)
2023/09/29 03:28:46 - INFO - root -   Epoch: [111/200][60/346], lr: 0.00000026 	 loss = 0.4117(0.5613)
2023/09/29 03:29:41 - INFO - root -   Epoch: [111/200][80/346], lr: 0.00000026 	 loss = 0.4095(0.5666)
2023/09/29 03:30:53 - INFO - root -   Epoch: [111/200][100/346], lr: 0.00000026 	 loss = 0.5579(0.5734)
2023/09/29 03:31:50 - INFO - root -   Epoch: [111/200][120/346], lr: 0.00000026 	 loss = 0.8679(0.6024)
2023/09/29 03:33:10 - INFO - root -   Epoch: [111/200][140/346], lr: 0.00000026 	 loss = 1.5015(0.6061)
2023/09/29 03:34:06 - INFO - root -   Epoch: [111/200][160/346], lr: 0.00000026 	 loss = 0.3492(0.6060)
2023/09/29 03:35:23 - INFO - root -   Epoch: [111/200][180/346], lr: 0.00000026 	 loss = 0.5843(0.5972)
2023/09/29 03:36:25 - INFO - root -   Epoch: [111/200][200/346], lr: 0.00000026 	 loss = 0.3756(0.5998)
2023/09/29 03:37:31 - INFO - root -   Epoch: [111/200][220/346], lr: 0.00000026 	 loss = 0.1203(0.6031)
2023/09/29 03:38:34 - INFO - root -   Epoch: [111/200][240/346], lr: 0.00000026 	 loss = 0.1800(0.6037)
2023/09/29 03:39:46 - INFO - root -   Epoch: [111/200][260/346], lr: 0.00000026 	 loss = 0.6623(0.6003)
2023/09/29 03:40:57 - INFO - root -   Epoch: [111/200][280/346], lr: 0.00000026 	 loss = 0.9077(0.6053)
2023/09/29 03:42:15 - INFO - root -   Epoch: [111/200][300/346], lr: 0.00000026 	 loss = 0.2790(0.6062)
2023/09/29 03:43:08 - INFO - root -   Epoch: [111/200][320/346], lr: 0.00000026 	 loss = 0.3422(0.6072)
2023/09/29 03:44:08 - INFO - root -   Epoch: [111/200][340/346], lr: 0.00000026 	 loss = 0.8952(0.6120)
2023/09/29 03:44:12 - INFO - root -   Epoch: [111/200] 	 loss = 0.6108
2023/09/29 03:44:12 - INFO - root -   train_accuracy = 0.6517
2023/09/29 03:44:33 - INFO - root -   Epoch: [112/200][0/346], lr: 0.00000026 	 loss = 0.2879(0.2879)
2023/09/29 03:45:34 - INFO - root -   Epoch: [112/200][20/346], lr: 0.00000026 	 loss = 0.0715(0.4586)
2023/09/29 03:46:48 - INFO - root -   Epoch: [112/200][40/346], lr: 0.00000026 	 loss = 0.4808(0.5481)
2023/09/29 03:48:11 - INFO - root -   Epoch: [112/200][60/346], lr: 0.00000026 	 loss = 0.7145(0.5412)
2023/09/29 03:48:59 - INFO - root -   Epoch: [112/200][80/346], lr: 0.00000026 	 loss = 0.4696(0.5732)
2023/09/29 03:50:17 - INFO - root -   Epoch: [112/200][100/346], lr: 0.00000026 	 loss = 0.5652(0.5783)
2023/09/29 03:51:16 - INFO - root -   Epoch: [112/200][120/346], lr: 0.00000026 	 loss = 0.6107(0.6261)
2023/09/29 03:52:32 - INFO - root -   Epoch: [112/200][140/346], lr: 0.00000026 	 loss = 0.8809(0.6143)
2023/09/29 03:53:27 - INFO - root -   Epoch: [112/200][160/346], lr: 0.00000026 	 loss = 0.4225(0.6262)
2023/09/29 03:54:55 - INFO - root -   Epoch: [112/200][180/346], lr: 0.00000026 	 loss = 0.7573(0.6298)
2023/09/29 03:55:47 - INFO - root -   Epoch: [112/200][200/346], lr: 0.00000026 	 loss = 0.2938(0.6270)
2023/09/29 03:56:56 - INFO - root -   Epoch: [112/200][220/346], lr: 0.00000026 	 loss = 0.2596(0.6301)
2023/09/29 03:58:02 - INFO - root -   Epoch: [112/200][240/346], lr: 0.00000026 	 loss = 0.6920(0.6248)
2023/09/29 03:59:26 - INFO - root -   Epoch: [112/200][260/346], lr: 0.00000026 	 loss = 0.4071(0.6258)
2023/09/29 04:00:24 - INFO - root -   Epoch: [112/200][280/346], lr: 0.00000026 	 loss = 0.5579(0.6364)
2023/09/29 04:01:34 - INFO - root -   Epoch: [112/200][300/346], lr: 0.00000026 	 loss = 0.3131(0.6453)
2023/09/29 04:02:40 - INFO - root -   Epoch: [112/200][320/346], lr: 0.00000026 	 loss = 0.2646(0.6433)
2023/09/29 04:03:38 - INFO - root -   Epoch: [112/200][340/346], lr: 0.00000026 	 loss = 1.1283(0.6411)
2023/09/29 04:03:40 - INFO - root -   Epoch: [112/200] 	 loss = 0.6405
2023/09/29 04:03:41 - INFO - root -   train_accuracy = 0.6301
2023/09/29 04:04:03 - INFO - root -   Epoch: [113/200][0/346], lr: 0.00000026 	 loss = 0.1207(0.1207)
2023/09/29 04:05:20 - INFO - root -   Epoch: [113/200][20/346], lr: 0.00000026 	 loss = 0.7859(0.5322)
2023/09/29 04:06:18 - INFO - root -   Epoch: [113/200][40/346], lr: 0.00000026 	 loss = 0.3630(0.5468)
2023/09/29 04:07:37 - INFO - root -   Epoch: [113/200][60/346], lr: 0.00000026 	 loss = 0.3619(0.5105)
2023/09/29 04:08:37 - INFO - root -   Epoch: [113/200][80/346], lr: 0.00000026 	 loss = 0.3606(0.5549)
2023/09/29 04:10:13 - INFO - root -   Epoch: [113/200][100/346], lr: 0.00000026 	 loss = 0.8619(0.5954)
2023/09/29 04:11:04 - INFO - root -   Epoch: [113/200][120/346], lr: 0.00000026 	 loss = 0.7781(0.6203)
2023/09/29 04:12:30 - INFO - root -   Epoch: [113/200][140/346], lr: 0.00000026 	 loss = 0.7478(0.6227)
2023/09/29 04:13:17 - INFO - root -   Epoch: [113/200][160/346], lr: 0.00000026 	 loss = 0.4541(0.6286)
2023/09/29 04:14:35 - INFO - root -   Epoch: [113/200][180/346], lr: 0.00000026 	 loss = 0.5588(0.6251)
2023/09/29 04:15:32 - INFO - root -   Epoch: [113/200][200/346], lr: 0.00000026 	 loss = 0.4155(0.6169)
2023/09/29 04:16:38 - INFO - root -   Epoch: [113/200][220/346], lr: 0.00000026 	 loss = 0.1231(0.6220)
2023/09/29 04:17:38 - INFO - root -   Epoch: [113/200][240/346], lr: 0.00000026 	 loss = 0.2090(0.6100)
2023/09/29 04:19:07 - INFO - root -   Epoch: [113/200][260/346], lr: 0.00000026 	 loss = 1.1582(0.6253)
2023/09/29 04:19:59 - INFO - root -   Epoch: [113/200][280/346], lr: 0.00000026 	 loss = 0.4199(0.6345)
2023/09/29 04:21:08 - INFO - root -   Epoch: [113/200][300/346], lr: 0.00000026 	 loss = 0.4205(0.6326)
2023/09/29 04:22:22 - INFO - root -   Epoch: [113/200][320/346], lr: 0.00000026 	 loss = 0.2768(0.6333)
2023/09/29 04:23:02 - INFO - root -   Epoch: [113/200][340/346], lr: 0.00000026 	 loss = 0.4425(0.6268)
2023/09/29 04:23:09 - INFO - root -   Epoch: [113/200] 	 loss = 0.6266
2023/09/29 04:23:09 - INFO - root -   train_accuracy = 0.6676
2023/09/29 04:23:31 - INFO - root -   Epoch: [114/200][0/346], lr: 0.00000026 	 loss = 0.4753(0.4753)
2023/09/29 04:24:31 - INFO - root -   Epoch: [114/200][20/346], lr: 0.00000026 	 loss = 0.9839(0.5898)
2023/09/29 04:25:50 - INFO - root -   Epoch: [114/200][40/346], lr: 0.00000026 	 loss = 0.2670(0.5964)
2023/09/29 04:26:49 - INFO - root -   Epoch: [114/200][60/346], lr: 0.00000026 	 loss = 0.8473(0.5494)
2023/09/29 04:28:06 - INFO - root -   Epoch: [114/200][80/346], lr: 0.00000026 	 loss = 0.4751(0.5343)
2023/09/29 04:29:05 - INFO - root -   Epoch: [114/200][100/346], lr: 0.00000026 	 loss = 0.9208(0.5662)
2023/09/29 04:30:25 - INFO - root -   Epoch: [114/200][120/346], lr: 0.00000026 	 loss = 0.7185(0.5903)
2023/09/29 04:31:30 - INFO - root -   Epoch: [114/200][140/346], lr: 0.00000026 	 loss = 0.9187(0.6050)
2023/09/29 04:32:38 - INFO - root -   Epoch: [114/200][160/346], lr: 0.00000026 	 loss = 0.2215(0.6093)
2023/09/29 04:33:44 - INFO - root -   Epoch: [114/200][180/346], lr: 0.00000026 	 loss = 0.8345(0.6102)
2023/09/29 04:34:52 - INFO - root -   Epoch: [114/200][200/346], lr: 0.00000026 	 loss = 0.7556(0.6010)
2023/09/29 04:35:46 - INFO - root -   Epoch: [114/200][220/346], lr: 0.00000026 	 loss = 0.2635(0.6099)
2023/09/29 04:37:00 - INFO - root -   Epoch: [114/200][240/346], lr: 0.00000026 	 loss = 0.7562(0.6078)
2023/09/29 04:38:09 - INFO - root -   Epoch: [114/200][260/346], lr: 0.00000026 	 loss = 1.0988(0.6086)
2023/09/29 04:39:06 - INFO - root -   Epoch: [114/200][280/346], lr: 0.00000026 	 loss = 0.7261(0.6138)
2023/09/29 04:40:30 - INFO - root -   Epoch: [114/200][300/346], lr: 0.00000026 	 loss = 0.8793(0.6167)
2023/09/29 04:41:26 - INFO - root -   Epoch: [114/200][320/346], lr: 0.00000026 	 loss = 0.5308(0.6183)
2023/09/29 04:42:22 - INFO - root -   Epoch: [114/200][340/346], lr: 0.00000026 	 loss = 1.1150(0.6186)
2023/09/29 04:42:24 - INFO - root -   Epoch: [114/200] 	 loss = 0.6183
2023/09/29 04:46:15 - INFO - root -   precision = 0.7126
2023/09/29 04:46:15 - INFO - root -   eval_loss = 0.5844
2023/09/29 04:46:16 - INFO - root -   train_accuracy = 0.6806
2023/09/29 04:46:38 - INFO - root -   Epoch: [115/200][0/346], lr: 0.00000026 	 loss = 0.4080(0.4080)
2023/09/29 04:47:45 - INFO - root -   Epoch: [115/200][20/346], lr: 0.00000026 	 loss = 0.3072(0.5251)
2023/09/29 04:48:49 - INFO - root -   Epoch: [115/200][40/346], lr: 0.00000026 	 loss = 0.4280(0.5645)
2023/09/29 04:50:10 - INFO - root -   Epoch: [115/200][60/346], lr: 0.00000026 	 loss = 0.4555(0.5452)
2023/09/29 04:51:12 - INFO - root -   Epoch: [115/200][80/346], lr: 0.00000026 	 loss = 0.9305(0.5713)
2023/09/29 04:52:31 - INFO - root -   Epoch: [115/200][100/346], lr: 0.00000026 	 loss = 0.4042(0.5788)
2023/09/29 04:53:19 - INFO - root -   Epoch: [115/200][120/346], lr: 0.00000026 	 loss = 0.3037(0.5866)
2023/09/29 04:54:37 - INFO - root -   Epoch: [115/200][140/346], lr: 0.00000026 	 loss = 0.5257(0.5948)
2023/09/29 04:55:36 - INFO - root -   Epoch: [115/200][160/346], lr: 0.00000026 	 loss = 0.3330(0.6026)
2023/09/29 04:56:40 - INFO - root -   Epoch: [115/200][180/346], lr: 0.00000026 	 loss = 0.5919(0.6043)
2023/09/29 04:57:48 - INFO - root -   Epoch: [115/200][200/346], lr: 0.00000026 	 loss = 0.5510(0.5996)
2023/09/29 04:59:06 - INFO - root -   Epoch: [115/200][220/346], lr: 0.00000026 	 loss = 0.5071(0.6185)
2023/09/29 05:00:06 - INFO - root -   Epoch: [115/200][240/346], lr: 0.00000026 	 loss = 0.4395(0.6241)
2023/09/29 05:01:09 - INFO - root -   Epoch: [115/200][260/346], lr: 0.00000026 	 loss = 0.5752(0.6198)
2023/09/29 05:02:27 - INFO - root -   Epoch: [115/200][280/346], lr: 0.00000026 	 loss = 0.8651(0.6259)
2023/09/29 05:03:18 - INFO - root -   Epoch: [115/200][300/346], lr: 0.00000026 	 loss = 0.6023(0.6257)
2023/09/29 05:04:46 - INFO - root -   Epoch: [115/200][320/346], lr: 0.00000026 	 loss = 0.6767(0.6259)
2023/09/29 05:05:26 - INFO - root -   Epoch: [115/200][340/346], lr: 0.00000026 	 loss = 0.9935(0.6216)
2023/09/29 05:05:34 - INFO - root -   Epoch: [115/200] 	 loss = 0.6205
2023/09/29 05:05:34 - INFO - root -   train_accuracy = 0.6575
2023/09/29 05:05:56 - INFO - root -   Epoch: [116/200][0/346], lr: 0.00000027 	 loss = 0.1656(0.1656)
2023/09/29 05:07:03 - INFO - root -   Epoch: [116/200][20/346], lr: 0.00000027 	 loss = 0.8021(0.6322)
2023/09/29 05:08:04 - INFO - root -   Epoch: [116/200][40/346], lr: 0.00000027 	 loss = 0.5768(0.6310)
2023/09/29 05:09:15 - INFO - root -   Epoch: [116/200][60/346], lr: 0.00000027 	 loss = 0.3271(0.6269)
2023/09/29 05:10:20 - INFO - root -   Epoch: [116/200][80/346], lr: 0.00000027 	 loss = 1.0142(0.6119)
2023/09/29 05:11:25 - INFO - root -   Epoch: [116/200][100/346], lr: 0.00000027 	 loss = 0.8710(0.6147)
2023/09/29 05:12:47 - INFO - root -   Epoch: [116/200][120/346], lr: 0.00000027 	 loss = 0.3108(0.6253)
2023/09/29 05:13:51 - INFO - root -   Epoch: [116/200][140/346], lr: 0.00000027 	 loss = 1.0469(0.6185)
2023/09/29 05:15:07 - INFO - root -   Epoch: [116/200][160/346], lr: 0.00000027 	 loss = 0.1827(0.6147)
2023/09/29 05:15:59 - INFO - root -   Epoch: [116/200][180/346], lr: 0.00000027 	 loss = 1.0061(0.6136)
2023/09/29 05:17:03 - INFO - root -   Epoch: [116/200][200/346], lr: 0.00000027 	 loss = 0.1662(0.6115)
2023/09/29 05:17:55 - INFO - root -   Epoch: [116/200][220/346], lr: 0.00000027 	 loss = 0.3697(0.6206)
2023/09/29 05:19:12 - INFO - root -   Epoch: [116/200][240/346], lr: 0.00000027 	 loss = 0.4565(0.6198)
2023/09/29 05:20:03 - INFO - root -   Epoch: [116/200][260/346], lr: 0.00000027 	 loss = 0.5446(0.6128)
2023/09/29 05:21:20 - INFO - root -   Epoch: [116/200][280/346], lr: 0.00000027 	 loss = 0.3437(0.6213)
2023/09/29 05:22:20 - INFO - root -   Epoch: [116/200][300/346], lr: 0.00000027 	 loss = 0.6417(0.6218)
2023/09/29 05:23:36 - INFO - root -   Epoch: [116/200][320/346], lr: 0.00000027 	 loss = 0.6133(0.6233)
2023/09/29 05:24:29 - INFO - root -   Epoch: [116/200][340/346], lr: 0.00000027 	 loss = 1.0400(0.6197)
2023/09/29 05:24:34 - INFO - root -   Epoch: [116/200] 	 loss = 0.6180
2023/09/29 05:24:34 - INFO - root -   train_accuracy = 0.6734
2023/09/29 05:24:55 - INFO - root -   Epoch: [117/200][0/346], lr: 0.00000027 	 loss = 0.1041(0.1041)
2023/09/29 05:25:57 - INFO - root -   Epoch: [117/200][20/346], lr: 0.00000027 	 loss = 0.2354(0.5486)
2023/09/29 05:27:11 - INFO - root -   Epoch: [117/200][40/346], lr: 0.00000027 	 loss = 1.0608(0.5652)
2023/09/29 05:28:03 - INFO - root -   Epoch: [117/200][60/346], lr: 0.00000027 	 loss = 0.3947(0.5551)
2023/09/29 05:29:40 - INFO - root -   Epoch: [117/200][80/346], lr: 0.00000027 	 loss = 0.6307(0.5807)
2023/09/29 05:30:35 - INFO - root -   Epoch: [117/200][100/346], lr: 0.00000027 	 loss = 1.0606(0.5937)
2023/09/29 05:32:14 - INFO - root -   Epoch: [117/200][120/346], lr: 0.00000027 	 loss = 0.6697(0.6171)
2023/09/29 05:33:02 - INFO - root -   Epoch: [117/200][140/346], lr: 0.00000027 	 loss = 0.4605(0.5986)
2023/09/29 05:34:27 - INFO - root -   Epoch: [117/200][160/346], lr: 0.00000027 	 loss = 0.6804(0.6027)
2023/09/29 05:35:10 - INFO - root -   Epoch: [117/200][180/346], lr: 0.00000027 	 loss = 0.7977(0.5987)
2023/09/29 05:36:43 - INFO - root -   Epoch: [117/200][200/346], lr: 0.00000027 	 loss = 0.3591(0.6042)
2023/09/29 05:37:34 - INFO - root -   Epoch: [117/200][220/346], lr: 0.00000027 	 loss = 0.6976(0.6113)
2023/09/29 05:38:52 - INFO - root -   Epoch: [117/200][240/346], lr: 0.00000027 	 loss = 0.4153(0.6146)
2023/09/29 05:39:34 - INFO - root -   Epoch: [117/200][260/346], lr: 0.00000027 	 loss = 0.8356(0.6225)
2023/09/29 05:40:56 - INFO - root -   Epoch: [117/200][280/346], lr: 0.00000027 	 loss = 0.5953(0.6255)
2023/09/29 05:41:55 - INFO - root -   Epoch: [117/200][300/346], lr: 0.00000027 	 loss = 0.6508(0.6213)
2023/09/29 05:43:09 - INFO - root -   Epoch: [117/200][320/346], lr: 0.00000027 	 loss = 0.3535(0.6263)
2023/09/29 05:43:51 - INFO - root -   Epoch: [117/200][340/346], lr: 0.00000027 	 loss = 0.9763(0.6214)
2023/09/29 05:43:55 - INFO - root -   Epoch: [117/200] 	 loss = 0.6204
2023/09/29 05:43:55 - INFO - root -   train_accuracy = 0.6488
2023/09/29 05:44:24 - INFO - root -   Epoch: [118/200][0/346], lr: 0.00000027 	 loss = 0.4383(0.4383)
2023/09/29 05:45:17 - INFO - root -   Epoch: [118/200][20/346], lr: 0.00000027 	 loss = 0.9583(0.4990)
2023/09/29 05:46:36 - INFO - root -   Epoch: [118/200][40/346], lr: 0.00000027 	 loss = 0.3846(0.5558)
2023/09/29 05:47:56 - INFO - root -   Epoch: [118/200][60/346], lr: 0.00000027 	 loss = 0.3522(0.5677)
2023/09/29 05:48:57 - INFO - root -   Epoch: [118/200][80/346], lr: 0.00000027 	 loss = 0.4387(0.5567)
2023/09/29 05:50:05 - INFO - root -   Epoch: [118/200][100/346], lr: 0.00000027 	 loss = 0.6626(0.5588)
2023/09/29 05:51:14 - INFO - root -   Epoch: [118/200][120/346], lr: 0.00000027 	 loss = 1.1710(0.6174)
2023/09/29 05:52:09 - INFO - root -   Epoch: [118/200][140/346], lr: 0.00000027 	 loss = 1.2494(0.6055)
2023/09/29 05:53:29 - INFO - root -   Epoch: [118/200][160/346], lr: 0.00000027 	 loss = 0.2571(0.6079)
2023/09/29 05:54:23 - INFO - root -   Epoch: [118/200][180/346], lr: 0.00000027 	 loss = 0.9701(0.6100)
2023/09/29 05:55:37 - INFO - root -   Epoch: [118/200][200/346], lr: 0.00000027 	 loss = 0.2521(0.6120)
2023/09/29 05:56:40 - INFO - root -   Epoch: [118/200][220/346], lr: 0.00000027 	 loss = 0.1114(0.6150)
2023/09/29 05:57:51 - INFO - root -   Epoch: [118/200][240/346], lr: 0.00000027 	 loss = 0.1723(0.6101)
2023/09/29 05:58:43 - INFO - root -   Epoch: [118/200][260/346], lr: 0.00000027 	 loss = 0.2835(0.6089)
2023/09/29 05:59:59 - INFO - root -   Epoch: [118/200][280/346], lr: 0.00000027 	 loss = 0.9282(0.6221)
2023/09/29 06:01:07 - INFO - root -   Epoch: [118/200][300/346], lr: 0.00000027 	 loss = 0.4092(0.6196)
2023/09/29 06:02:15 - INFO - root -   Epoch: [118/200][320/346], lr: 0.00000027 	 loss = 0.9178(0.6269)
2023/09/29 06:03:18 - INFO - root -   Epoch: [118/200][340/346], lr: 0.00000027 	 loss = 0.8972(0.6199)
2023/09/29 06:03:20 - INFO - root -   Epoch: [118/200] 	 loss = 0.6211
2023/09/29 06:03:20 - INFO - root -   train_accuracy = 0.6532
2023/09/29 06:03:50 - INFO - root -   Epoch: [119/200][0/346], lr: 0.00000027 	 loss = 0.5123(0.5123)
2023/09/29 06:04:58 - INFO - root -   Epoch: [119/200][20/346], lr: 0.00000027 	 loss = 0.5215(0.6588)
2023/09/29 06:05:57 - INFO - root -   Epoch: [119/200][40/346], lr: 0.00000027 	 loss = 0.6045(0.6759)
2023/09/29 06:07:26 - INFO - root -   Epoch: [119/200][60/346], lr: 0.00000027 	 loss = 0.2623(0.6224)
2023/09/29 06:08:12 - INFO - root -   Epoch: [119/200][80/346], lr: 0.00000027 	 loss = 1.2136(0.6268)
2023/09/29 06:09:43 - INFO - root -   Epoch: [119/200][100/346], lr: 0.00000027 	 loss = 0.5654(0.6334)
2023/09/29 06:10:30 - INFO - root -   Epoch: [119/200][120/346], lr: 0.00000027 	 loss = 0.2293(0.6454)
2023/09/29 06:11:50 - INFO - root -   Epoch: [119/200][140/346], lr: 0.00000027 	 loss = 0.4848(0.6259)
2023/09/29 06:12:41 - INFO - root -   Epoch: [119/200][160/346], lr: 0.00000027 	 loss = 0.1818(0.6297)
2023/09/29 06:14:02 - INFO - root -   Epoch: [119/200][180/346], lr: 0.00000027 	 loss = 0.4961(0.6279)
2023/09/29 06:15:09 - INFO - root -   Epoch: [119/200][200/346], lr: 0.00000027 	 loss = 0.2529(0.6267)
2023/09/29 06:16:15 - INFO - root -   Epoch: [119/200][220/346], lr: 0.00000027 	 loss = 0.1735(0.6381)
2023/09/29 06:17:22 - INFO - root -   Epoch: [119/200][240/346], lr: 0.00000027 	 loss = 0.2621(0.6316)
2023/09/29 06:18:40 - INFO - root -   Epoch: [119/200][260/346], lr: 0.00000027 	 loss = 0.3932(0.6235)
2023/09/29 06:19:22 - INFO - root -   Epoch: [119/200][280/346], lr: 0.00000027 	 loss = 0.7290(0.6262)
2023/09/29 06:20:35 - INFO - root -   Epoch: [119/200][300/346], lr: 0.00000027 	 loss = 0.4497(0.6227)
2023/09/29 06:21:33 - INFO - root -   Epoch: [119/200][320/346], lr: 0.00000027 	 loss = 0.3630(0.6208)
2023/09/29 06:22:36 - INFO - root -   Epoch: [119/200][340/346], lr: 0.00000027 	 loss = 1.1020(0.6184)
2023/09/29 06:22:40 - INFO - root -   Epoch: [119/200] 	 loss = 0.6207
2023/09/29 06:26:30 - INFO - root -   precision = 0.6897
2023/09/29 06:26:30 - INFO - root -   eval_loss = 0.5874
2023/09/29 06:26:31 - INFO - root -   train_accuracy = 0.6575
2023/09/29 06:27:01 - INFO - root -   Epoch: [120/200][0/346], lr: 0.00000027 	 loss = 0.2908(0.2908)
2023/09/29 06:28:09 - INFO - root -   Epoch: [120/200][20/346], lr: 0.00000027 	 loss = 0.5548(0.5641)
2023/09/29 06:29:09 - INFO - root -   Epoch: [120/200][40/346], lr: 0.00000027 	 loss = 0.6130(0.6025)
2023/09/29 06:30:06 - INFO - root -   Epoch: [120/200][60/346], lr: 0.00000027 	 loss = 0.3944(0.5570)
2023/09/29 06:31:19 - INFO - root -   Epoch: [120/200][80/346], lr: 0.00000027 	 loss = 0.3304(0.5472)
2023/09/29 06:32:14 - INFO - root -   Epoch: [120/200][100/346], lr: 0.00000027 	 loss = 1.3777(0.5657)
2023/09/29 06:33:37 - INFO - root -   Epoch: [120/200][120/346], lr: 0.00000027 	 loss = 0.5241(0.5799)
2023/09/29 06:34:23 - INFO - root -   Epoch: [120/200][140/346], lr: 0.00000027 	 loss = 0.6444(0.5801)
2023/09/29 06:35:38 - INFO - root -   Epoch: [120/200][160/346], lr: 0.00000027 	 loss = 0.3347(0.5798)
2023/09/29 06:36:39 - INFO - root -   Epoch: [120/200][180/346], lr: 0.00000027 	 loss = 0.6087(0.5803)
2023/09/29 06:37:47 - INFO - root -   Epoch: [120/200][200/346], lr: 0.00000027 	 loss = 0.3897(0.5860)
2023/09/29 06:38:44 - INFO - root -   Epoch: [120/200][220/346], lr: 0.00000027 	 loss = 0.1629(0.5945)
2023/09/29 06:39:52 - INFO - root -   Epoch: [120/200][240/346], lr: 0.00000027 	 loss = 0.3389(0.5822)
2023/09/29 06:41:07 - INFO - root -   Epoch: [120/200][260/346], lr: 0.00000027 	 loss = 0.6621(0.5890)
2023/09/29 06:42:06 - INFO - root -   Epoch: [120/200][280/346], lr: 0.00000027 	 loss = 0.1337(0.6015)
2023/09/29 06:43:13 - INFO - root -   Epoch: [120/200][300/346], lr: 0.00000027 	 loss = 0.3677(0.6047)
2023/09/29 06:44:11 - INFO - root -   Epoch: [120/200][320/346], lr: 0.00000027 	 loss = 0.1997(0.5984)
2023/09/29 06:45:08 - INFO - root -   Epoch: [120/200][340/346], lr: 0.00000027 	 loss = 1.3386(0.6032)
2023/09/29 06:45:19 - INFO - root -   Epoch: [120/200] 	 loss = 0.6027
2023/09/29 06:45:19 - INFO - root -   train_accuracy = 0.6618
2023/09/29 06:45:41 - INFO - root -   Epoch: [121/200][0/346], lr: 0.00000027 	 loss = 0.4592(0.4592)
2023/09/29 06:46:49 - INFO - root -   Epoch: [121/200][20/346], lr: 0.00000027 	 loss = 0.6557(0.6527)
2023/09/29 06:48:03 - INFO - root -   Epoch: [121/200][40/346], lr: 0.00000027 	 loss = 0.5047(0.6649)
2023/09/29 06:49:01 - INFO - root -   Epoch: [121/200][60/346], lr: 0.00000027 	 loss = 0.6513(0.6103)
2023/09/29 06:50:21 - INFO - root -   Epoch: [121/200][80/346], lr: 0.00000027 	 loss = 0.6405(0.5999)
2023/09/29 06:51:28 - INFO - root -   Epoch: [121/200][100/346], lr: 0.00000027 	 loss = 1.5456(0.6028)
2023/09/29 06:52:52 - INFO - root -   Epoch: [121/200][120/346], lr: 0.00000027 	 loss = 0.4069(0.6009)
2023/09/29 06:53:51 - INFO - root -   Epoch: [121/200][140/346], lr: 0.00000027 	 loss = 0.8790(0.6094)
2023/09/29 06:55:20 - INFO - root -   Epoch: [121/200][160/346], lr: 0.00000027 	 loss = 0.3218(0.6309)
2023/09/29 06:56:05 - INFO - root -   Epoch: [121/200][180/346], lr: 0.00000027 	 loss = 0.6413(0.6309)
2023/09/29 06:57:25 - INFO - root -   Epoch: [121/200][200/346], lr: 0.00000027 	 loss = 0.3398(0.6202)
2023/09/29 06:58:16 - INFO - root -   Epoch: [121/200][220/346], lr: 0.00000027 	 loss = 0.4092(0.6266)
2023/09/29 06:59:47 - INFO - root -   Epoch: [121/200][240/346], lr: 0.00000027 	 loss = 0.2577(0.6259)
2023/09/29 07:00:37 - INFO - root -   Epoch: [121/200][260/346], lr: 0.00000027 	 loss = 0.5771(0.6275)
2023/09/29 07:01:46 - INFO - root -   Epoch: [121/200][280/346], lr: 0.00000027 	 loss = 0.5376(0.6318)
2023/09/29 07:02:43 - INFO - root -   Epoch: [121/200][300/346], lr: 0.00000027 	 loss = 0.4858(0.6313)
2023/09/29 07:03:50 - INFO - root -   Epoch: [121/200][320/346], lr: 0.00000027 	 loss = 0.6492(0.6337)
2023/09/29 07:04:48 - INFO - root -   Epoch: [121/200][340/346], lr: 0.00000027 	 loss = 0.5003(0.6342)
2023/09/29 07:04:52 - INFO - root -   Epoch: [121/200] 	 loss = 0.6359
2023/09/29 07:04:52 - INFO - root -   train_accuracy = 0.6517
2023/09/29 07:05:23 - INFO - root -   Epoch: [122/200][0/346], lr: 0.00000027 	 loss = 0.5131(0.5131)
2023/09/29 07:06:30 - INFO - root -   Epoch: [122/200][20/346], lr: 0.00000027 	 loss = 0.4410(0.5395)
2023/09/29 07:07:46 - INFO - root -   Epoch: [122/200][40/346], lr: 0.00000027 	 loss = 0.4231(0.5582)
2023/09/29 07:08:36 - INFO - root -   Epoch: [122/200][60/346], lr: 0.00000027 	 loss = 0.7103(0.5581)
2023/09/29 07:09:49 - INFO - root -   Epoch: [122/200][80/346], lr: 0.00000027 	 loss = 0.5833(0.5712)
2023/09/29 07:10:48 - INFO - root -   Epoch: [122/200][100/346], lr: 0.00000027 	 loss = 0.7401(0.5844)
2023/09/29 07:12:10 - INFO - root -   Epoch: [122/200][120/346], lr: 0.00000027 	 loss = 0.5603(0.6127)
2023/09/29 07:13:10 - INFO - root -   Epoch: [122/200][140/346], lr: 0.00000027 	 loss = 0.9635(0.6112)
2023/09/29 07:14:23 - INFO - root -   Epoch: [122/200][160/346], lr: 0.00000027 	 loss = 0.2178(0.6233)
2023/09/29 07:15:05 - INFO - root -   Epoch: [122/200][180/346], lr: 0.00000027 	 loss = 0.6837(0.6266)
2023/09/29 07:16:31 - INFO - root -   Epoch: [122/200][200/346], lr: 0.00000027 	 loss = 0.3973(0.6292)
2023/09/29 07:17:16 - INFO - root -   Epoch: [122/200][220/346], lr: 0.00000027 	 loss = 0.5096(0.6366)
2023/09/29 07:18:28 - INFO - root -   Epoch: [122/200][240/346], lr: 0.00000027 	 loss = 0.3926(0.6273)
2023/09/29 07:19:29 - INFO - root -   Epoch: [122/200][260/346], lr: 0.00000027 	 loss = 0.6985(0.6186)
2023/09/29 07:20:42 - INFO - root -   Epoch: [122/200][280/346], lr: 0.00000027 	 loss = 0.8086(0.6283)
2023/09/29 07:21:41 - INFO - root -   Epoch: [122/200][300/346], lr: 0.00000027 	 loss = 0.4944(0.6309)
2023/09/29 07:22:54 - INFO - root -   Epoch: [122/200][320/346], lr: 0.00000027 	 loss = 0.5899(0.6321)
2023/09/29 07:23:43 - INFO - root -   Epoch: [122/200][340/346], lr: 0.00000027 	 loss = 0.8705(0.6256)
2023/09/29 07:23:46 - INFO - root -   Epoch: [122/200] 	 loss = 0.6259
2023/09/29 07:23:46 - INFO - root -   train_accuracy = 0.6416
2023/09/29 07:24:17 - INFO - root -   Epoch: [123/200][0/346], lr: 0.00000028 	 loss = 0.5079(0.5079)
2023/09/29 07:25:17 - INFO - root -   Epoch: [123/200][20/346], lr: 0.00000028 	 loss = 0.1857(0.5516)
2023/09/29 07:26:47 - INFO - root -   Epoch: [123/200][40/346], lr: 0.00000028 	 loss = 0.9610(0.6088)
2023/09/29 07:27:39 - INFO - root -   Epoch: [123/200][60/346], lr: 0.00000028 	 loss = 0.3133(0.5792)
2023/09/29 07:28:51 - INFO - root -   Epoch: [123/200][80/346], lr: 0.00000028 	 loss = 0.6363(0.6033)
2023/09/29 07:29:54 - INFO - root -   Epoch: [123/200][100/346], lr: 0.00000028 	 loss = 0.7146(0.6080)
2023/09/29 07:31:02 - INFO - root -   Epoch: [123/200][120/346], lr: 0.00000028 	 loss = 0.3961(0.6179)
2023/09/29 07:32:16 - INFO - root -   Epoch: [123/200][140/346], lr: 0.00000028 	 loss = 0.7780(0.6122)
2023/09/29 07:33:15 - INFO - root -   Epoch: [123/200][160/346], lr: 0.00000028 	 loss = 0.1567(0.6133)
2023/09/29 07:34:42 - INFO - root -   Epoch: [123/200][180/346], lr: 0.00000028 	 loss = 0.6960(0.6115)
2023/09/29 07:35:34 - INFO - root -   Epoch: [123/200][200/346], lr: 0.00000028 	 loss = 0.6535(0.6144)
2023/09/29 07:37:04 - INFO - root -   Epoch: [123/200][220/346], lr: 0.00000028 	 loss = 0.4694(0.6212)
2023/09/29 07:38:06 - INFO - root -   Epoch: [123/200][240/346], lr: 0.00000028 	 loss = 0.1910(0.6225)
2023/09/29 07:39:20 - INFO - root -   Epoch: [123/200][260/346], lr: 0.00000028 	 loss = 0.5915(0.6219)
2023/09/29 07:40:13 - INFO - root -   Epoch: [123/200][280/346], lr: 0.00000028 	 loss = 1.0880(0.6307)
2023/09/29 07:41:30 - INFO - root -   Epoch: [123/200][300/346], lr: 0.00000028 	 loss = 0.5987(0.6276)
2023/09/29 07:42:23 - INFO - root -   Epoch: [123/200][320/346], lr: 0.00000028 	 loss = 0.5333(0.6355)
2023/09/29 07:43:29 - INFO - root -   Epoch: [123/200][340/346], lr: 0.00000028 	 loss = 0.9625(0.6400)
2023/09/29 07:43:31 - INFO - root -   Epoch: [123/200] 	 loss = 0.6395
2023/09/29 07:43:31 - INFO - root -   train_accuracy = 0.6431
2023/09/29 07:43:55 - INFO - root -   Epoch: [124/200][0/346], lr: 0.00000028 	 loss = 0.3465(0.3465)
2023/09/29 07:45:08 - INFO - root -   Epoch: [124/200][20/346], lr: 0.00000028 	 loss = 0.1552(0.6123)
2023/09/29 07:46:03 - INFO - root -   Epoch: [124/200][40/346], lr: 0.00000028 	 loss = 0.4491(0.6270)
2023/09/29 07:47:12 - INFO - root -   Epoch: [124/200][60/346], lr: 0.00000028 	 loss = 0.2417(0.5572)
2023/09/29 07:48:08 - INFO - root -   Epoch: [124/200][80/346], lr: 0.00000028 	 loss = 0.4862(0.5545)
2023/09/29 07:49:36 - INFO - root -   Epoch: [124/200][100/346], lr: 0.00000028 	 loss = 1.0666(0.5852)
2023/09/29 07:50:43 - INFO - root -   Epoch: [124/200][120/346], lr: 0.00000028 	 loss = 0.9549(0.6261)
2023/09/29 07:51:50 - INFO - root -   Epoch: [124/200][140/346], lr: 0.00000028 	 loss = 0.6136(0.6244)
2023/09/29 07:53:02 - INFO - root -   Epoch: [124/200][160/346], lr: 0.00000028 	 loss = 0.3628(0.6334)
2023/09/29 07:53:50 - INFO - root -   Epoch: [124/200][180/346], lr: 0.00000028 	 loss = 0.3995(0.6283)
2023/09/29 07:55:19 - INFO - root -   Epoch: [124/200][200/346], lr: 0.00000028 	 loss = 0.5193(0.6215)
2023/09/29 07:56:16 - INFO - root -   Epoch: [124/200][220/346], lr: 0.00000028 	 loss = 0.2977(0.6243)
2023/09/29 07:57:32 - INFO - root -   Epoch: [124/200][240/346], lr: 0.00000028 	 loss = 0.2702(0.6158)
2023/09/29 07:58:35 - INFO - root -   Epoch: [124/200][260/346], lr: 0.00000028 	 loss = 0.8822(0.6134)
2023/09/29 07:59:35 - INFO - root -   Epoch: [124/200][280/346], lr: 0.00000028 	 loss = 0.6085(0.6169)
2023/09/29 08:00:45 - INFO - root -   Epoch: [124/200][300/346], lr: 0.00000028 	 loss = 0.4534(0.6157)
2023/09/29 08:01:49 - INFO - root -   Epoch: [124/200][320/346], lr: 0.00000028 	 loss = 0.4873(0.6164)
2023/09/29 08:02:59 - INFO - root -   Epoch: [124/200][340/346], lr: 0.00000028 	 loss = 0.3486(0.6168)
2023/09/29 08:03:01 - INFO - root -   Epoch: [124/200] 	 loss = 0.6188
2023/09/29 08:06:51 - INFO - root -   precision = 0.7011
2023/09/29 08:06:51 - INFO - root -   eval_loss = 0.5720
2023/09/29 08:06:52 - INFO - root -   train_accuracy = 0.6517
2023/09/29 08:07:14 - INFO - root -   Epoch: [125/200][0/346], lr: 0.00000028 	 loss = 0.3338(0.3338)
2023/09/29 08:08:26 - INFO - root -   Epoch: [125/200][20/346], lr: 0.00000028 	 loss = 0.3346(0.5283)
2023/09/29 08:09:50 - INFO - root -   Epoch: [125/200][40/346], lr: 0.00000028 	 loss = 0.2721(0.5740)
2023/09/29 08:11:01 - INFO - root -   Epoch: [125/200][60/346], lr: 0.00000028 	 loss = 0.5543(0.5427)
2023/09/29 08:11:58 - INFO - root -   Epoch: [125/200][80/346], lr: 0.00000028 	 loss = 0.9282(0.6014)
2023/09/29 08:13:14 - INFO - root -   Epoch: [125/200][100/346], lr: 0.00000028 	 loss = 0.5571(0.5939)
2023/09/29 08:14:11 - INFO - root -   Epoch: [125/200][120/346], lr: 0.00000028 	 loss = 0.6128(0.6083)
2023/09/29 08:15:26 - INFO - root -   Epoch: [125/200][140/346], lr: 0.00000028 	 loss = 0.5552(0.5962)
2023/09/29 08:16:43 - INFO - root -   Epoch: [125/200][160/346], lr: 0.00000028 	 loss = 0.1653(0.5870)
2023/09/29 08:18:04 - INFO - root -   Epoch: [125/200][180/346], lr: 0.00000028 	 loss = 1.2068(0.5985)
2023/09/29 08:18:54 - INFO - root -   Epoch: [125/200][200/346], lr: 0.00000028 	 loss = 0.2608(0.6099)
2023/09/29 08:20:03 - INFO - root -   Epoch: [125/200][220/346], lr: 0.00000028 	 loss = 0.2666(0.6236)
2023/09/29 08:21:12 - INFO - root -   Epoch: [125/200][240/346], lr: 0.00000028 	 loss = 0.4802(0.6257)
2023/09/29 08:22:10 - INFO - root -   Epoch: [125/200][260/346], lr: 0.00000028 	 loss = 0.4886(0.6156)
2023/09/29 08:23:16 - INFO - root -   Epoch: [125/200][280/346], lr: 0.00000028 	 loss = 0.3877(0.6215)
2023/09/29 08:24:23 - INFO - root -   Epoch: [125/200][300/346], lr: 0.00000028 	 loss = 0.5572(0.6258)
2023/09/29 08:25:40 - INFO - root -   Epoch: [125/200][320/346], lr: 0.00000028 	 loss = 0.2837(0.6245)
2023/09/29 08:26:29 - INFO - root -   Epoch: [125/200][340/346], lr: 0.00000028 	 loss = 0.9137(0.6264)
2023/09/29 08:26:34 - INFO - root -   Epoch: [125/200] 	 loss = 0.6288
2023/09/29 08:26:34 - INFO - root -   train_accuracy = 0.6431
2023/09/29 08:26:56 - INFO - root -   Epoch: [126/200][0/346], lr: 0.00000028 	 loss = 0.2878(0.2878)
2023/09/29 08:28:09 - INFO - root -   Epoch: [126/200][20/346], lr: 0.00000028 	 loss = 0.5558(0.5106)
2023/09/29 08:29:14 - INFO - root -   Epoch: [126/200][40/346], lr: 0.00000028 	 loss = 0.6263(0.5855)
2023/09/29 08:30:27 - INFO - root -   Epoch: [126/200][60/346], lr: 0.00000028 	 loss = 0.5636(0.5721)
2023/09/29 08:31:22 - INFO - root -   Epoch: [126/200][80/346], lr: 0.00000028 	 loss = 0.3565(0.5822)
2023/09/29 08:32:52 - INFO - root -   Epoch: [126/200][100/346], lr: 0.00000028 	 loss = 0.8246(0.5941)
2023/09/29 08:33:42 - INFO - root -   Epoch: [126/200][120/346], lr: 0.00000028 	 loss = 0.3282(0.5853)
2023/09/29 08:35:09 - INFO - root -   Epoch: [126/200][140/346], lr: 0.00000028 	 loss = 0.6363(0.5964)
2023/09/29 08:36:10 - INFO - root -   Epoch: [126/200][160/346], lr: 0.00000028 	 loss = 0.1794(0.5924)
2023/09/29 08:37:14 - INFO - root -   Epoch: [126/200][180/346], lr: 0.00000028 	 loss = 0.6071(0.5985)
2023/09/29 08:38:12 - INFO - root -   Epoch: [126/200][200/346], lr: 0.00000028 	 loss = 0.1551(0.5877)
2023/09/29 08:39:13 - INFO - root -   Epoch: [126/200][220/346], lr: 0.00000028 	 loss = 0.2107(0.6025)
2023/09/29 08:40:30 - INFO - root -   Epoch: [126/200][240/346], lr: 0.00000028 	 loss = 0.6487(0.6007)
2023/09/29 08:41:28 - INFO - root -   Epoch: [126/200][260/346], lr: 0.00000028 	 loss = 0.4412(0.5981)
2023/09/29 08:42:37 - INFO - root -   Epoch: [126/200][280/346], lr: 0.00000028 	 loss = 0.2779(0.6043)
2023/09/29 08:43:37 - INFO - root -   Epoch: [126/200][300/346], lr: 0.00000028 	 loss = 0.6270(0.6035)
2023/09/29 08:44:46 - INFO - root -   Epoch: [126/200][320/346], lr: 0.00000028 	 loss = 0.4244(0.6021)
2023/09/29 08:45:50 - INFO - root -   Epoch: [126/200][340/346], lr: 0.00000028 	 loss = 1.0327(0.6015)
2023/09/29 08:45:52 - INFO - root -   Epoch: [126/200] 	 loss = 0.6012
2023/09/29 08:45:52 - INFO - root -   train_accuracy = 0.6734
2023/09/29 08:46:31 - INFO - root -   Epoch: [127/200][0/346], lr: 0.00000028 	 loss = 0.5857(0.5857)
2023/09/29 08:47:23 - INFO - root -   Epoch: [127/200][20/346], lr: 0.00000028 	 loss = 0.4520(0.5595)
2023/09/29 08:48:58 - INFO - root -   Epoch: [127/200][40/346], lr: 0.00000028 	 loss = 0.7341(0.6012)
2023/09/29 08:49:51 - INFO - root -   Epoch: [127/200][60/346], lr: 0.00000028 	 loss = 0.4911(0.5781)
2023/09/29 08:50:52 - INFO - root -   Epoch: [127/200][80/346], lr: 0.00000028 	 loss = 0.2600(0.5753)
2023/09/29 08:52:06 - INFO - root -   Epoch: [127/200][100/346], lr: 0.00000028 	 loss = 0.5233(0.6028)
2023/09/29 08:53:16 - INFO - root -   Epoch: [127/200][120/346], lr: 0.00000028 	 loss = 0.7268(0.6096)
2023/09/29 08:54:25 - INFO - root -   Epoch: [127/200][140/346], lr: 0.00000028 	 loss = 0.6871(0.6032)
2023/09/29 08:55:29 - INFO - root -   Epoch: [127/200][160/346], lr: 0.00000028 	 loss = 0.5192(0.6062)
2023/09/29 08:56:50 - INFO - root -   Epoch: [127/200][180/346], lr: 0.00000028 	 loss = 0.6279(0.6009)
2023/09/29 08:57:41 - INFO - root -   Epoch: [127/200][200/346], lr: 0.00000028 	 loss = 0.4815(0.6051)
2023/09/29 08:59:04 - INFO - root -   Epoch: [127/200][220/346], lr: 0.00000028 	 loss = 0.2949(0.6142)
2023/09/29 08:59:47 - INFO - root -   Epoch: [127/200][240/346], lr: 0.00000028 	 loss = 0.7481(0.6100)
2023/09/29 09:01:11 - INFO - root -   Epoch: [127/200][260/346], lr: 0.00000028 	 loss = 0.5080(0.6115)
2023/09/29 09:02:06 - INFO - root -   Epoch: [127/200][280/346], lr: 0.00000028 	 loss = 0.4908(0.6240)
2023/09/29 09:03:15 - INFO - root -   Epoch: [127/200][300/346], lr: 0.00000028 	 loss = 0.3388(0.6211)
2023/09/29 09:04:32 - INFO - root -   Epoch: [127/200][320/346], lr: 0.00000028 	 loss = 0.3409(0.6240)
2023/09/29 09:05:17 - INFO - root -   Epoch: [127/200][340/346], lr: 0.00000028 	 loss = 1.0160(0.6222)
2023/09/29 09:05:23 - INFO - root -   Epoch: [127/200] 	 loss = 0.6222
2023/09/29 09:05:23 - INFO - root -   train_accuracy = 0.6662
2023/09/29 09:06:00 - INFO - root -   Epoch: [128/200][0/346], lr: 0.00000028 	 loss = 0.7752(0.7752)
2023/09/29 09:07:05 - INFO - root -   Epoch: [128/200][20/346], lr: 0.00000028 	 loss = 0.5107(0.5700)
2023/09/29 09:08:06 - INFO - root -   Epoch: [128/200][40/346], lr: 0.00000028 	 loss = 0.7339(0.5752)
2023/09/29 09:09:16 - INFO - root -   Epoch: [128/200][60/346], lr: 0.00000028 	 loss = 0.6849(0.5700)
2023/09/29 09:10:31 - INFO - root -   Epoch: [128/200][80/346], lr: 0.00000028 	 loss = 0.3316(0.5992)
2023/09/29 09:11:28 - INFO - root -   Epoch: [128/200][100/346], lr: 0.00000028 	 loss = 0.4532(0.6106)
2023/09/29 09:12:46 - INFO - root -   Epoch: [128/200][120/346], lr: 0.00000028 	 loss = 0.9519(0.6196)
2023/09/29 09:13:36 - INFO - root -   Epoch: [128/200][140/346], lr: 0.00000028 	 loss = 0.4403(0.6092)
2023/09/29 09:14:50 - INFO - root -   Epoch: [128/200][160/346], lr: 0.00000028 	 loss = 0.1455(0.6146)
2023/09/29 09:16:00 - INFO - root -   Epoch: [128/200][180/346], lr: 0.00000028 	 loss = 1.0845(0.6123)
2023/09/29 09:17:01 - INFO - root -   Epoch: [128/200][200/346], lr: 0.00000028 	 loss = 0.9477(0.6194)
2023/09/29 09:18:03 - INFO - root -   Epoch: [128/200][220/346], lr: 0.00000028 	 loss = 0.2807(0.6257)
2023/09/29 09:19:32 - INFO - root -   Epoch: [128/200][240/346], lr: 0.00000028 	 loss = 0.4391(0.6261)
2023/09/29 09:20:30 - INFO - root -   Epoch: [128/200][260/346], lr: 0.00000028 	 loss = 0.4501(0.6196)
2023/09/29 09:21:44 - INFO - root -   Epoch: [128/200][280/346], lr: 0.00000028 	 loss = 0.7540(0.6217)
2023/09/29 09:22:26 - INFO - root -   Epoch: [128/200][300/346], lr: 0.00000028 	 loss = 0.6352(0.6162)
2023/09/29 09:23:34 - INFO - root -   Epoch: [128/200][320/346], lr: 0.00000028 	 loss = 0.3995(0.6181)
2023/09/29 09:24:31 - INFO - root -   Epoch: [128/200][340/346], lr: 0.00000028 	 loss = 0.8492(0.6153)
2023/09/29 09:24:35 - INFO - root -   Epoch: [128/200] 	 loss = 0.6155
2023/09/29 09:24:35 - INFO - root -   train_accuracy = 0.6705
2023/09/29 09:25:13 - INFO - root -   Epoch: [129/200][0/346], lr: 0.00000028 	 loss = 0.6043(0.6043)
2023/09/29 09:26:05 - INFO - root -   Epoch: [129/200][20/346], lr: 0.00000028 	 loss = 0.8295(0.5080)
2023/09/29 09:27:20 - INFO - root -   Epoch: [129/200][40/346], lr: 0.00000028 	 loss = 0.4228(0.5599)
2023/09/29 09:28:28 - INFO - root -   Epoch: [129/200][60/346], lr: 0.00000028 	 loss = 0.8793(0.5268)
2023/09/29 09:29:35 - INFO - root -   Epoch: [129/200][80/346], lr: 0.00000028 	 loss = 0.4817(0.5415)
2023/09/29 09:30:49 - INFO - root -   Epoch: [129/200][100/346], lr: 0.00000028 	 loss = 0.5288(0.5526)
2023/09/29 09:31:43 - INFO - root -   Epoch: [129/200][120/346], lr: 0.00000028 	 loss = 0.2787(0.5879)
2023/09/29 09:33:06 - INFO - root -   Epoch: [129/200][140/346], lr: 0.00000028 	 loss = 0.3426(0.5837)
2023/09/29 09:34:05 - INFO - root -   Epoch: [129/200][160/346], lr: 0.00000028 	 loss = 0.1987(0.5887)
2023/09/29 09:35:20 - INFO - root -   Epoch: [129/200][180/346], lr: 0.00000028 	 loss = 1.0838(0.5927)
2023/09/29 09:36:24 - INFO - root -   Epoch: [129/200][200/346], lr: 0.00000028 	 loss = 0.1619(0.6014)
2023/09/29 09:37:21 - INFO - root -   Epoch: [129/200][220/346], lr: 0.00000028 	 loss = 0.3702(0.6148)
2023/09/29 09:38:44 - INFO - root -   Epoch: [129/200][240/346], lr: 0.00000028 	 loss = 0.2713(0.6051)
2023/09/29 09:39:31 - INFO - root -   Epoch: [129/200][260/346], lr: 0.00000028 	 loss = 0.5980(0.5965)
2023/09/29 09:40:47 - INFO - root -   Epoch: [129/200][280/346], lr: 0.00000028 	 loss = 0.2751(0.6055)
2023/09/29 09:41:49 - INFO - root -   Epoch: [129/200][300/346], lr: 0.00000028 	 loss = 0.1901(0.6049)
2023/09/29 09:42:56 - INFO - root -   Epoch: [129/200][320/346], lr: 0.00000028 	 loss = 0.3858(0.6062)
2023/09/29 09:43:53 - INFO - root -   Epoch: [129/200][340/346], lr: 0.00000028 	 loss = 1.1317(0.6114)
2023/09/29 09:43:59 - INFO - root -   Epoch: [129/200] 	 loss = 0.6121
2023/09/29 09:47:53 - INFO - root -   precision = 0.7184
2023/09/29 09:47:53 - INFO - root -   eval_loss = 0.5675
2023/09/29 09:47:54 - INFO - root -   train_accuracy = 0.6546
2023/09/29 09:48:25 - INFO - root -   Epoch: [130/200][0/346], lr: 0.00000029 	 loss = 0.6795(0.6795)
2023/09/29 09:49:24 - INFO - root -   Epoch: [130/200][20/346], lr: 0.00000029 	 loss = 0.8157(0.5506)
2023/09/29 09:50:38 - INFO - root -   Epoch: [130/200][40/346], lr: 0.00000029 	 loss = 0.7857(0.5723)
2023/09/29 09:51:48 - INFO - root -   Epoch: [130/200][60/346], lr: 0.00000029 	 loss = 0.4971(0.5353)
2023/09/29 09:52:53 - INFO - root -   Epoch: [130/200][80/346], lr: 0.00000029 	 loss = 0.5576(0.5685)
2023/09/29 09:54:15 - INFO - root -   Epoch: [130/200][100/346], lr: 0.00000029 	 loss = 0.8262(0.5854)
2023/09/29 09:55:07 - INFO - root -   Epoch: [130/200][120/346], lr: 0.00000029 	 loss = 0.4290(0.5976)
2023/09/29 09:56:28 - INFO - root -   Epoch: [130/200][140/346], lr: 0.00000029 	 loss = 0.7070(0.6012)
2023/09/29 09:57:22 - INFO - root -   Epoch: [130/200][160/346], lr: 0.00000029 	 loss = 0.1022(0.6063)
2023/09/29 09:58:41 - INFO - root -   Epoch: [130/200][180/346], lr: 0.00000029 	 loss = 0.5967(0.5981)
2023/09/29 09:59:36 - INFO - root -   Epoch: [130/200][200/346], lr: 0.00000029 	 loss = 0.2991(0.6022)
2023/09/29 10:00:59 - INFO - root -   Epoch: [130/200][220/346], lr: 0.00000029 	 loss = 0.2812(0.6127)
2023/09/29 10:01:58 - INFO - root -   Epoch: [130/200][240/346], lr: 0.00000029 	 loss = 0.4931(0.6053)
2023/09/29 10:03:17 - INFO - root -   Epoch: [130/200][260/346], lr: 0.00000029 	 loss = 0.5164(0.6008)
2023/09/29 10:04:05 - INFO - root -   Epoch: [130/200][280/346], lr: 0.00000029 	 loss = 0.5121(0.5994)
2023/09/29 10:05:13 - INFO - root -   Epoch: [130/200][300/346], lr: 0.00000029 	 loss = 0.2844(0.5999)
2023/09/29 10:06:21 - INFO - root -   Epoch: [130/200][320/346], lr: 0.00000029 	 loss = 0.4859(0.6005)
2023/09/29 10:07:29 - INFO - root -   Epoch: [130/200][340/346], lr: 0.00000029 	 loss = 1.0698(0.5987)
2023/09/29 10:07:32 - INFO - root -   Epoch: [130/200] 	 loss = 0.6004
2023/09/29 10:07:32 - INFO - root -   train_accuracy = 0.6763
2023/09/29 10:08:02 - INFO - root -   Epoch: [131/200][0/346], lr: 0.00000029 	 loss = 0.6283(0.6283)
2023/09/29 10:09:08 - INFO - root -   Epoch: [131/200][20/346], lr: 0.00000029 	 loss = 0.7709(0.6086)
2023/09/29 10:10:06 - INFO - root -   Epoch: [131/200][40/346], lr: 0.00000029 	 loss = 0.8429(0.6225)
2023/09/29 10:11:21 - INFO - root -   Epoch: [131/200][60/346], lr: 0.00000029 	 loss = 0.4010(0.5809)
2023/09/29 10:12:26 - INFO - root -   Epoch: [131/200][80/346], lr: 0.00000029 	 loss = 0.8771(0.5815)
2023/09/29 10:13:36 - INFO - root -   Epoch: [131/200][100/346], lr: 0.00000029 	 loss = 0.8725(0.5713)
2023/09/29 10:14:28 - INFO - root -   Epoch: [131/200][120/346], lr: 0.00000029 	 loss = 0.2914(0.5903)
2023/09/29 10:15:49 - INFO - root -   Epoch: [131/200][140/346], lr: 0.00000029 	 loss = 0.4335(0.5861)
2023/09/29 10:16:47 - INFO - root -   Epoch: [131/200][160/346], lr: 0.00000029 	 loss = 0.1651(0.5920)
2023/09/29 10:18:08 - INFO - root -   Epoch: [131/200][180/346], lr: 0.00000029 	 loss = 0.7376(0.5959)
2023/09/29 10:19:01 - INFO - root -   Epoch: [131/200][200/346], lr: 0.00000029 	 loss = 0.3227(0.5966)
2023/09/29 10:20:16 - INFO - root -   Epoch: [131/200][220/346], lr: 0.00000029 	 loss = 0.4210(0.6117)
2023/09/29 10:21:20 - INFO - root -   Epoch: [131/200][240/346], lr: 0.00000029 	 loss = 0.6504(0.6076)
2023/09/29 10:22:41 - INFO - root -   Epoch: [131/200][260/346], lr: 0.00000029 	 loss = 0.5883(0.6053)
2023/09/29 10:23:38 - INFO - root -   Epoch: [131/200][280/346], lr: 0.00000029 	 loss = 0.3901(0.6097)
2023/09/29 10:24:57 - INFO - root -   Epoch: [131/200][300/346], lr: 0.00000029 	 loss = 0.2069(0.6104)
2023/09/29 10:26:07 - INFO - root -   Epoch: [131/200][320/346], lr: 0.00000029 	 loss = 0.2831(0.6093)
2023/09/29 10:26:53 - INFO - root -   Epoch: [131/200][340/346], lr: 0.00000029 	 loss = 0.9493(0.6098)
2023/09/29 10:27:04 - INFO - root -   Epoch: [131/200] 	 loss = 0.6099
2023/09/29 10:27:04 - INFO - root -   train_accuracy = 0.6662
2023/09/29 10:27:26 - INFO - root -   Epoch: [132/200][0/346], lr: 0.00000029 	 loss = 0.2909(0.2909)
2023/09/29 10:28:34 - INFO - root -   Epoch: [132/200][20/346], lr: 0.00000029 	 loss = 0.2243(0.5159)
2023/09/29 10:29:50 - INFO - root -   Epoch: [132/200][40/346], lr: 0.00000029 	 loss = 0.8157(0.5618)
2023/09/29 10:31:05 - INFO - root -   Epoch: [132/200][60/346], lr: 0.00000029 	 loss = 0.5137(0.5292)
2023/09/29 10:32:19 - INFO - root -   Epoch: [132/200][80/346], lr: 0.00000029 	 loss = 0.5018(0.5336)
2023/09/29 10:33:24 - INFO - root -   Epoch: [132/200][100/346], lr: 0.00000029 	 loss = 1.1897(0.5534)
2023/09/29 10:34:38 - INFO - root -   Epoch: [132/200][120/346], lr: 0.00000029 	 loss = 0.2585(0.5679)
2023/09/29 10:35:39 - INFO - root -   Epoch: [132/200][140/346], lr: 0.00000029 	 loss = 1.0549(0.5836)
2023/09/29 10:36:38 - INFO - root -   Epoch: [132/200][160/346], lr: 0.00000029 	 loss = 0.0819(0.5882)
2023/09/29 10:37:57 - INFO - root -   Epoch: [132/200][180/346], lr: 0.00000029 	 loss = 0.6738(0.5961)
2023/09/29 10:38:53 - INFO - root -   Epoch: [132/200][200/346], lr: 0.00000029 	 loss = 0.1475(0.5907)
2023/09/29 10:40:08 - INFO - root -   Epoch: [132/200][220/346], lr: 0.00000029 	 loss = 0.7922(0.6031)
2023/09/29 10:41:15 - INFO - root -   Epoch: [132/200][240/346], lr: 0.00000029 	 loss = 0.5500(0.5945)
2023/09/29 10:42:25 - INFO - root -   Epoch: [132/200][260/346], lr: 0.00000029 	 loss = 0.4613(0.5877)
2023/09/29 10:43:27 - INFO - root -   Epoch: [132/200][280/346], lr: 0.00000029 	 loss = 0.2762(0.5951)
2023/09/29 10:44:40 - INFO - root -   Epoch: [132/200][300/346], lr: 0.00000029 	 loss = 0.1975(0.5921)
2023/09/29 10:45:56 - INFO - root -   Epoch: [132/200][320/346], lr: 0.00000029 	 loss = 0.7857(0.5976)
2023/09/29 10:46:50 - INFO - root -   Epoch: [132/200][340/346], lr: 0.00000029 	 loss = 0.9854(0.6033)
2023/09/29 10:46:57 - INFO - root -   Epoch: [132/200] 	 loss = 0.6020
2023/09/29 10:46:57 - INFO - root -   train_accuracy = 0.6647
2023/09/29 10:47:35 - INFO - root -   Epoch: [133/200][0/346], lr: 0.00000029 	 loss = 0.4120(0.4120)
2023/09/29 10:48:32 - INFO - root -   Epoch: [133/200][20/346], lr: 0.00000029 	 loss = 0.5635(0.5771)
2023/09/29 10:49:36 - INFO - root -   Epoch: [133/200][40/346], lr: 0.00000029 	 loss = 0.2835(0.5780)
2023/09/29 10:50:43 - INFO - root -   Epoch: [133/200][60/346], lr: 0.00000029 	 loss = 0.3969(0.5598)
2023/09/29 10:51:52 - INFO - root -   Epoch: [133/200][80/346], lr: 0.00000029 	 loss = 0.5198(0.5754)
2023/09/29 10:53:13 - INFO - root -   Epoch: [133/200][100/346], lr: 0.00000029 	 loss = 0.9613(0.5999)
2023/09/29 10:54:04 - INFO - root -   Epoch: [133/200][120/346], lr: 0.00000029 	 loss = 0.5089(0.5891)
2023/09/29 10:55:14 - INFO - root -   Epoch: [133/200][140/346], lr: 0.00000029 	 loss = 1.0813(0.6026)
2023/09/29 10:56:18 - INFO - root -   Epoch: [133/200][160/346], lr: 0.00000029 	 loss = 0.1380(0.5992)
2023/09/29 10:57:24 - INFO - root -   Epoch: [133/200][180/346], lr: 0.00000029 	 loss = 0.6756(0.6095)
2023/09/29 10:58:17 - INFO - root -   Epoch: [133/200][200/346], lr: 0.00000029 	 loss = 0.3593(0.6126)
2023/09/29 10:59:36 - INFO - root -   Epoch: [133/200][220/346], lr: 0.00000029 	 loss = 0.4193(0.6229)
2023/09/29 11:00:40 - INFO - root -   Epoch: [133/200][240/346], lr: 0.00000029 	 loss = 0.0766(0.6169)
2023/09/29 11:01:51 - INFO - root -   Epoch: [133/200][260/346], lr: 0.00000029 	 loss = 0.5984(0.6179)
2023/09/29 11:02:49 - INFO - root -   Epoch: [133/200][280/346], lr: 0.00000029 	 loss = 0.4747(0.6145)
2023/09/29 11:04:15 - INFO - root -   Epoch: [133/200][300/346], lr: 0.00000029 	 loss = 0.3041(0.6156)
2023/09/29 11:05:06 - INFO - root -   Epoch: [133/200][320/346], lr: 0.00000029 	 loss = 0.6020(0.6158)
2023/09/29 11:06:05 - INFO - root -   Epoch: [133/200][340/346], lr: 0.00000029 	 loss = 0.7974(0.6065)
2023/09/29 11:06:07 - INFO - root -   Epoch: [133/200] 	 loss = 0.6063
2023/09/29 11:06:07 - INFO - root -   train_accuracy = 0.6705
2023/09/29 11:06:38 - INFO - root -   Epoch: [134/200][0/346], lr: 0.00000029 	 loss = 0.5623(0.5623)
2023/09/29 11:07:49 - INFO - root -   Epoch: [134/200][20/346], lr: 0.00000029 	 loss = 0.0854(0.5156)
2023/09/29 11:08:55 - INFO - root -   Epoch: [134/200][40/346], lr: 0.00000029 	 loss = 0.8977(0.5839)
2023/09/29 11:10:00 - INFO - root -   Epoch: [134/200][60/346], lr: 0.00000029 	 loss = 0.5223(0.5619)
2023/09/29 11:10:59 - INFO - root -   Epoch: [134/200][80/346], lr: 0.00000029 	 loss = 0.8648(0.5953)
2023/09/29 11:12:18 - INFO - root -   Epoch: [134/200][100/346], lr: 0.00000029 	 loss = 0.8768(0.6077)
2023/09/29 11:13:08 - INFO - root -   Epoch: [134/200][120/346], lr: 0.00000029 	 loss = 0.7280(0.6293)
2023/09/29 11:14:35 - INFO - root -   Epoch: [134/200][140/346], lr: 0.00000029 	 loss = 0.5293(0.6218)
2023/09/29 11:15:24 - INFO - root -   Epoch: [134/200][160/346], lr: 0.00000029 	 loss = 0.1287(0.6232)
2023/09/29 11:16:51 - INFO - root -   Epoch: [134/200][180/346], lr: 0.00000029 	 loss = 1.8246(0.6335)
2023/09/29 11:17:52 - INFO - root -   Epoch: [134/200][200/346], lr: 0.00000029 	 loss = 0.2304(0.6302)
2023/09/29 11:19:08 - INFO - root -   Epoch: [134/200][220/346], lr: 0.00000029 	 loss = 0.2123(0.6316)
2023/09/29 11:19:57 - INFO - root -   Epoch: [134/200][240/346], lr: 0.00000029 	 loss = 0.1682(0.6274)
2023/09/29 11:21:00 - INFO - root -   Epoch: [134/200][260/346], lr: 0.00000029 	 loss = 0.2723(0.6202)
2023/09/29 11:22:28 - INFO - root -   Epoch: [134/200][280/346], lr: 0.00000029 	 loss = 0.4903(0.6176)
2023/09/29 11:23:29 - INFO - root -   Epoch: [134/200][300/346], lr: 0.00000029 	 loss = 0.1482(0.6200)
2023/09/29 11:24:45 - INFO - root -   Epoch: [134/200][320/346], lr: 0.00000029 	 loss = 0.8495(0.6177)
2023/09/29 11:25:36 - INFO - root -   Epoch: [134/200][340/346], lr: 0.00000029 	 loss = 0.7766(0.6154)
2023/09/29 11:25:42 - INFO - root -   Epoch: [134/200] 	 loss = 0.6186
2023/09/29 11:29:33 - INFO - root -   precision = 0.7011
2023/09/29 11:29:33 - INFO - root -   eval_loss = 0.5856
2023/09/29 11:29:34 - INFO - root -   train_accuracy = 0.6532
2023/09/29 11:29:57 - INFO - root -   Epoch: [135/200][0/346], lr: 0.00000029 	 loss = 0.2527(0.2527)
2023/09/29 11:31:11 - INFO - root -   Epoch: [135/200][20/346], lr: 0.00000029 	 loss = 0.5917(0.5660)
2023/09/29 11:32:13 - INFO - root -   Epoch: [135/200][40/346], lr: 0.00000029 	 loss = 0.7070(0.5711)
2023/09/29 11:33:40 - INFO - root -   Epoch: [135/200][60/346], lr: 0.00000029 	 loss = 1.0838(0.5773)
2023/09/29 11:34:32 - INFO - root -   Epoch: [135/200][80/346], lr: 0.00000029 	 loss = 0.6392(0.5821)
2023/09/29 11:35:41 - INFO - root -   Epoch: [135/200][100/346], lr: 0.00000029 	 loss = 0.9950(0.5984)
2023/09/29 11:37:07 - INFO - root -   Epoch: [135/200][120/346], lr: 0.00000029 	 loss = 0.7775(0.6023)
2023/09/29 11:38:12 - INFO - root -   Epoch: [135/200][140/346], lr: 0.00000029 	 loss = 0.6480(0.5980)
2023/09/29 11:39:13 - INFO - root -   Epoch: [135/200][160/346], lr: 0.00000029 	 loss = 0.3356(0.5973)
2023/09/29 11:40:19 - INFO - root -   Epoch: [135/200][180/346], lr: 0.00000029 	 loss = 0.6813(0.6030)
2023/09/29 11:41:34 - INFO - root -   Epoch: [135/200][200/346], lr: 0.00000029 	 loss = 0.4761(0.6015)
2023/09/29 11:42:30 - INFO - root -   Epoch: [135/200][220/346], lr: 0.00000029 	 loss = 0.1534(0.6088)
2023/09/29 11:43:35 - INFO - root -   Epoch: [135/200][240/346], lr: 0.00000029 	 loss = 0.2254(0.5990)
2023/09/29 11:44:44 - INFO - root -   Epoch: [135/200][260/346], lr: 0.00000029 	 loss = 0.4435(0.5896)
2023/09/29 11:45:55 - INFO - root -   Epoch: [135/200][280/346], lr: 0.00000029 	 loss = 0.4124(0.5977)
2023/09/29 11:47:04 - INFO - root -   Epoch: [135/200][300/346], lr: 0.00000029 	 loss = 0.4182(0.6000)
2023/09/29 11:48:17 - INFO - root -   Epoch: [135/200][320/346], lr: 0.00000029 	 loss = 0.2592(0.5943)
2023/09/29 11:48:58 - INFO - root -   Epoch: [135/200][340/346], lr: 0.00000029 	 loss = 0.8336(0.5944)
2023/09/29 11:49:01 - INFO - root -   Epoch: [135/200] 	 loss = 0.5940
2023/09/29 11:49:01 - INFO - root -   train_accuracy = 0.6864
2023/09/29 11:49:34 - INFO - root -   Epoch: [136/200][0/346], lr: 0.00000029 	 loss = 0.5111(0.5111)
2023/09/29 11:50:41 - INFO - root -   Epoch: [136/200][20/346], lr: 0.00000029 	 loss = 0.2004(0.5002)
2023/09/29 11:51:42 - INFO - root -   Epoch: [136/200][40/346], lr: 0.00000029 	 loss = 0.7158(0.5720)
2023/09/29 11:52:38 - INFO - root -   Epoch: [136/200][60/346], lr: 0.00000029 	 loss = 0.4599(0.5387)
2023/09/29 11:53:45 - INFO - root -   Epoch: [136/200][80/346], lr: 0.00000029 	 loss = 0.3275(0.5521)
2023/09/29 11:55:07 - INFO - root -   Epoch: [136/200][100/346], lr: 0.00000029 	 loss = 1.2610(0.5693)
2023/09/29 11:55:58 - INFO - root -   Epoch: [136/200][120/346], lr: 0.00000029 	 loss = 0.3345(0.5933)
2023/09/29 11:57:11 - INFO - root -   Epoch: [136/200][140/346], lr: 0.00000029 	 loss = 0.6689(0.5783)
2023/09/29 11:58:25 - INFO - root -   Epoch: [136/200][160/346], lr: 0.00000029 	 loss = 0.2845(0.5897)
2023/09/29 11:59:31 - INFO - root -   Epoch: [136/200][180/346], lr: 0.00000029 	 loss = 0.4000(0.5882)
2023/09/29 12:00:37 - INFO - root -   Epoch: [136/200][200/346], lr: 0.00000029 	 loss = 0.3666(0.5827)
2023/09/29 12:01:46 - INFO - root -   Epoch: [136/200][220/346], lr: 0.00000029 	 loss = 0.5530(0.5952)
2023/09/29 12:02:58 - INFO - root -   Epoch: [136/200][240/346], lr: 0.00000029 	 loss = 0.4882(0.5907)
2023/09/29 12:04:07 - INFO - root -   Epoch: [136/200][260/346], lr: 0.00000029 	 loss = 0.6145(0.5895)
2023/09/29 12:05:16 - INFO - root -   Epoch: [136/200][280/346], lr: 0.00000029 	 loss = 0.4716(0.5981)
2023/09/29 12:06:19 - INFO - root -   Epoch: [136/200][300/346], lr: 0.00000029 	 loss = 0.4444(0.5952)
2023/09/29 12:07:34 - INFO - root -   Epoch: [136/200][320/346], lr: 0.00000029 	 loss = 0.5913(0.5971)
2023/09/29 12:08:29 - INFO - root -   Epoch: [136/200][340/346], lr: 0.00000029 	 loss = 0.5528(0.5984)
2023/09/29 12:08:36 - INFO - root -   Epoch: [136/200] 	 loss = 0.6038
2023/09/29 12:08:36 - INFO - root -   train_accuracy = 0.6763
2023/09/29 12:09:05 - INFO - root -   Epoch: [137/200][0/346], lr: 0.00000030 	 loss = 0.2584(0.2584)
2023/09/29 12:10:13 - INFO - root -   Epoch: [137/200][20/346], lr: 0.00000030 	 loss = 0.3147(0.5047)
2023/09/29 12:11:19 - INFO - root -   Epoch: [137/200][40/346], lr: 0.00000030 	 loss = 0.7838(0.5399)
2023/09/29 12:12:28 - INFO - root -   Epoch: [137/200][60/346], lr: 0.00000030 	 loss = 0.7155(0.4978)
2023/09/29 12:13:34 - INFO - root -   Epoch: [137/200][80/346], lr: 0.00000030 	 loss = 0.2163(0.5305)
2023/09/29 12:14:46 - INFO - root -   Epoch: [137/200][100/346], lr: 0.00000030 	 loss = 0.6625(0.5560)
2023/09/29 12:15:45 - INFO - root -   Epoch: [137/200][120/346], lr: 0.00000030 	 loss = 0.6680(0.5746)
2023/09/29 12:17:02 - INFO - root -   Epoch: [137/200][140/346], lr: 0.00000030 	 loss = 0.4838(0.5747)
2023/09/29 12:17:58 - INFO - root -   Epoch: [137/200][160/346], lr: 0.00000030 	 loss = 0.3022(0.5676)
2023/09/29 12:19:14 - INFO - root -   Epoch: [137/200][180/346], lr: 0.00000030 	 loss = 0.6180(0.5708)
2023/09/29 12:20:14 - INFO - root -   Epoch: [137/200][200/346], lr: 0.00000030 	 loss = 0.1456(0.5796)
2023/09/29 12:21:37 - INFO - root -   Epoch: [137/200][220/346], lr: 0.00000030 	 loss = 0.3038(0.5867)
2023/09/29 12:22:29 - INFO - root -   Epoch: [137/200][240/346], lr: 0.00000030 	 loss = 0.4753(0.5819)
2023/09/29 12:23:42 - INFO - root -   Epoch: [137/200][260/346], lr: 0.00000030 	 loss = 0.5631(0.5766)
2023/09/29 12:24:30 - INFO - root -   Epoch: [137/200][280/346], lr: 0.00000030 	 loss = 0.3500(0.5835)
2023/09/29 12:25:56 - INFO - root -   Epoch: [137/200][300/346], lr: 0.00000030 	 loss = 0.8354(0.5897)
2023/09/29 12:26:45 - INFO - root -   Epoch: [137/200][320/346], lr: 0.00000030 	 loss = 0.5270(0.5931)
2023/09/29 12:27:43 - INFO - root -   Epoch: [137/200][340/346], lr: 0.00000030 	 loss = 0.6489(0.5900)
2023/09/29 12:27:45 - INFO - root -   Epoch: [137/200] 	 loss = 0.5903
2023/09/29 12:27:45 - INFO - root -   train_accuracy = 0.6763
2023/09/29 12:28:15 - INFO - root -   Epoch: [138/200][0/346], lr: 0.00000030 	 loss = 0.3309(0.3309)
2023/09/29 12:29:28 - INFO - root -   Epoch: [138/200][20/346], lr: 0.00000030 	 loss = 0.4030(0.5822)
2023/09/29 12:30:14 - INFO - root -   Epoch: [138/200][40/346], lr: 0.00000030 	 loss = 0.5042(0.5855)
2023/09/29 12:31:30 - INFO - root -   Epoch: [138/200][60/346], lr: 0.00000030 	 loss = 0.2381(0.5467)
2023/09/29 12:32:27 - INFO - root -   Epoch: [138/200][80/346], lr: 0.00000030 	 loss = 0.3122(0.5306)
2023/09/29 12:33:32 - INFO - root -   Epoch: [138/200][100/346], lr: 0.00000030 	 loss = 0.1716(0.5411)
2023/09/29 12:34:27 - INFO - root -   Epoch: [138/200][120/346], lr: 0.00000030 	 loss = 0.2376(0.5733)
2023/09/29 12:35:40 - INFO - root -   Epoch: [138/200][140/346], lr: 0.00000030 	 loss = 0.4260(0.5632)
2023/09/29 12:36:39 - INFO - root -   Epoch: [138/200][160/346], lr: 0.00000030 	 loss = 0.2489(0.5703)
2023/09/29 12:38:09 - INFO - root -   Epoch: [138/200][180/346], lr: 0.00000030 	 loss = 0.5356(0.5710)
2023/09/29 12:39:07 - INFO - root -   Epoch: [138/200][200/346], lr: 0.00000030 	 loss = 0.1253(0.5709)
2023/09/29 12:40:20 - INFO - root -   Epoch: [138/200][220/346], lr: 0.00000030 	 loss = 0.2379(0.5791)
2023/09/29 12:41:18 - INFO - root -   Epoch: [138/200][240/346], lr: 0.00000030 	 loss = 0.3072(0.5716)
2023/09/29 12:42:29 - INFO - root -   Epoch: [138/200][260/346], lr: 0.00000030 	 loss = 0.4296(0.5682)
2023/09/29 12:43:31 - INFO - root -   Epoch: [138/200][280/346], lr: 0.00000030 	 loss = 0.2185(0.5718)
2023/09/29 12:44:36 - INFO - root -   Epoch: [138/200][300/346], lr: 0.00000030 	 loss = 0.5347(0.5691)
2023/09/29 12:45:49 - INFO - root -   Epoch: [138/200][320/346], lr: 0.00000030 	 loss = 0.1869(0.5685)
2023/09/29 12:46:34 - INFO - root -   Epoch: [138/200][340/346], lr: 0.00000030 	 loss = 0.6225(0.5664)
2023/09/29 12:46:43 - INFO - root -   Epoch: [138/200] 	 loss = 0.5672
2023/09/29 12:46:43 - INFO - root -   train_accuracy = 0.7095
2023/09/29 12:47:12 - INFO - root -   Epoch: [139/200][0/346], lr: 0.00000030 	 loss = 0.7193(0.7193)
2023/09/29 12:48:28 - INFO - root -   Epoch: [139/200][20/346], lr: 0.00000030 	 loss = 0.6320(0.6383)
2023/09/29 12:49:44 - INFO - root -   Epoch: [139/200][40/346], lr: 0.00000030 	 loss = 0.8359(0.6163)
2023/09/29 12:50:32 - INFO - root -   Epoch: [139/200][60/346], lr: 0.00000030 	 loss = 0.4601(0.5699)
2023/09/29 12:51:43 - INFO - root -   Epoch: [139/200][80/346], lr: 0.00000030 	 loss = 0.8048(0.5862)
2023/09/29 12:52:42 - INFO - root -   Epoch: [139/200][100/346], lr: 0.00000030 	 loss = 0.9057(0.5828)
2023/09/29 12:53:58 - INFO - root -   Epoch: [139/200][120/346], lr: 0.00000030 	 loss = 0.3463(0.5892)
2023/09/29 12:54:57 - INFO - root -   Epoch: [139/200][140/346], lr: 0.00000030 	 loss = 0.7078(0.5864)
2023/09/29 12:56:04 - INFO - root -   Epoch: [139/200][160/346], lr: 0.00000030 	 loss = 0.1555(0.5967)
2023/09/29 12:57:20 - INFO - root -   Epoch: [139/200][180/346], lr: 0.00000030 	 loss = 0.5278(0.6038)
2023/09/29 12:58:25 - INFO - root -   Epoch: [139/200][200/346], lr: 0.00000030 	 loss = 0.1478(0.5958)
2023/09/29 12:59:33 - INFO - root -   Epoch: [139/200][220/346], lr: 0.00000030 	 loss = 0.5474(0.6000)
2023/09/29 13:00:36 - INFO - root -   Epoch: [139/200][240/346], lr: 0.00000030 	 loss = 0.2113(0.5926)
2023/09/29 13:01:36 - INFO - root -   Epoch: [139/200][260/346], lr: 0.00000030 	 loss = 0.4488(0.5897)
2023/09/29 13:02:41 - INFO - root -   Epoch: [139/200][280/346], lr: 0.00000030 	 loss = 0.7126(0.6040)
2023/09/29 13:03:52 - INFO - root -   Epoch: [139/200][300/346], lr: 0.00000030 	 loss = 0.2760(0.5980)
2023/09/29 13:04:53 - INFO - root -   Epoch: [139/200][320/346], lr: 0.00000030 	 loss = 0.5421(0.6010)
2023/09/29 13:05:54 - INFO - root -   Epoch: [139/200][340/346], lr: 0.00000030 	 loss = 0.7942(0.5985)
2023/09/29 13:05:56 - INFO - root -   Epoch: [139/200] 	 loss = 0.5996
2023/09/29 13:09:46 - INFO - root -   precision = 0.6897
2023/09/29 13:09:46 - INFO - root -   eval_loss = 0.5822
2023/09/29 13:09:47 - INFO - root -   train_accuracy = 0.6546
2023/09/29 13:10:09 - INFO - root -   Epoch: [140/200][0/346], lr: 0.00000030 	 loss = 0.2393(0.2393)
2023/09/29 13:11:16 - INFO - root -   Epoch: [140/200][20/346], lr: 0.00000030 	 loss = 0.3538(0.4872)
2023/09/29 13:12:33 - INFO - root -   Epoch: [140/200][40/346], lr: 0.00000030 	 loss = 0.7842(0.5220)
2023/09/29 13:13:37 - INFO - root -   Epoch: [140/200][60/346], lr: 0.00000030 	 loss = 0.4840(0.5164)
2023/09/29 13:14:37 - INFO - root -   Epoch: [140/200][80/346], lr: 0.00000030 	 loss = 0.3905(0.5316)
2023/09/29 13:15:54 - INFO - root -   Epoch: [140/200][100/346], lr: 0.00000030 	 loss = 0.4172(0.5301)
2023/09/29 13:16:48 - INFO - root -   Epoch: [140/200][120/346], lr: 0.00000030 	 loss = 0.3138(0.5459)
2023/09/29 13:18:09 - INFO - root -   Epoch: [140/200][140/346], lr: 0.00000030 	 loss = 0.6708(0.5448)
2023/09/29 13:19:05 - INFO - root -   Epoch: [140/200][160/346], lr: 0.00000030 	 loss = 0.5187(0.5481)
2023/09/29 13:20:13 - INFO - root -   Epoch: [140/200][180/346], lr: 0.00000030 	 loss = 1.2169(0.5625)
2023/09/29 13:21:14 - INFO - root -   Epoch: [140/200][200/346], lr: 0.00000030 	 loss = 0.2771(0.5519)
2023/09/29 13:22:27 - INFO - root -   Epoch: [140/200][220/346], lr: 0.00000030 	 loss = 0.2274(0.5650)
2023/09/29 13:23:41 - INFO - root -   Epoch: [140/200][240/346], lr: 0.00000030 	 loss = 0.2113(0.5623)
2023/09/29 13:24:40 - INFO - root -   Epoch: [140/200][260/346], lr: 0.00000030 	 loss = 0.9966(0.5645)
2023/09/29 13:25:50 - INFO - root -   Epoch: [140/200][280/346], lr: 0.00000030 	 loss = 1.0669(0.5744)
2023/09/29 13:26:44 - INFO - root -   Epoch: [140/200][300/346], lr: 0.00000030 	 loss = 0.2550(0.5746)
2023/09/29 13:27:58 - INFO - root -   Epoch: [140/200][320/346], lr: 0.00000030 	 loss = 0.3795(0.5765)
2023/09/29 13:28:42 - INFO - root -   Epoch: [140/200][340/346], lr: 0.00000030 	 loss = 1.2558(0.5774)
2023/09/29 13:28:50 - INFO - root -   Epoch: [140/200] 	 loss = 0.5789
2023/09/29 13:28:50 - INFO - root -   train_accuracy = 0.6922
2023/09/29 13:29:12 - INFO - root -   Epoch: [141/200][0/346], lr: 0.00000030 	 loss = 0.3445(0.3445)
2023/09/29 13:30:13 - INFO - root -   Epoch: [141/200][20/346], lr: 0.00000030 	 loss = 0.0911(0.5183)
2023/09/29 13:31:26 - INFO - root -   Epoch: [141/200][40/346], lr: 0.00000030 	 loss = 0.6751(0.5784)
2023/09/29 13:32:36 - INFO - root -   Epoch: [141/200][60/346], lr: 0.00000030 	 loss = 0.4086(0.5114)
2023/09/29 13:33:43 - INFO - root -   Epoch: [141/200][80/346], lr: 0.00000030 	 loss = 0.3546(0.5184)
2023/09/29 13:34:58 - INFO - root -   Epoch: [141/200][100/346], lr: 0.00000030 	 loss = 0.3258(0.5396)
2023/09/29 13:35:53 - INFO - root -   Epoch: [141/200][120/346], lr: 0.00000030 	 loss = 0.4564(0.5557)
2023/09/29 13:37:17 - INFO - root -   Epoch: [141/200][140/346], lr: 0.00000030 	 loss = 0.9501(0.5607)
2023/09/29 13:38:15 - INFO - root -   Epoch: [141/200][160/346], lr: 0.00000030 	 loss = 0.1988(0.5871)
2023/09/29 13:39:24 - INFO - root -   Epoch: [141/200][180/346], lr: 0.00000030 	 loss = 1.4001(0.5958)
2023/09/29 13:40:18 - INFO - root -   Epoch: [141/200][200/346], lr: 0.00000030 	 loss = 0.1479(0.5949)
2023/09/29 13:41:25 - INFO - root -   Epoch: [141/200][220/346], lr: 0.00000030 	 loss = 0.2871(0.5983)
2023/09/29 13:42:41 - INFO - root -   Epoch: [141/200][240/346], lr: 0.00000030 	 loss = 0.3614(0.5955)
2023/09/29 13:43:54 - INFO - root -   Epoch: [141/200][260/346], lr: 0.00000030 	 loss = 0.9297(0.5952)
2023/09/29 13:44:58 - INFO - root -   Epoch: [141/200][280/346], lr: 0.00000030 	 loss = 0.8484(0.6005)
2023/09/29 13:46:17 - INFO - root -   Epoch: [141/200][300/346], lr: 0.00000030 	 loss = 0.3391(0.5992)
2023/09/29 13:47:08 - INFO - root -   Epoch: [141/200][320/346], lr: 0.00000030 	 loss = 0.2605(0.5998)
2023/09/29 13:48:08 - INFO - root -   Epoch: [141/200][340/346], lr: 0.00000030 	 loss = 1.0296(0.5967)
2023/09/29 13:48:12 - INFO - root -   Epoch: [141/200] 	 loss = 0.6018
2023/09/29 13:48:12 - INFO - root -   train_accuracy = 0.6691
2023/09/29 13:48:43 - INFO - root -   Epoch: [142/200][0/346], lr: 0.00000030 	 loss = 0.3493(0.3493)
2023/09/29 13:49:41 - INFO - root -   Epoch: [142/200][20/346], lr: 0.00000030 	 loss = 0.6236(0.5149)
2023/09/29 13:51:13 - INFO - root -   Epoch: [142/200][40/346], lr: 0.00000030 	 loss = 0.3237(0.5289)
2023/09/29 13:52:02 - INFO - root -   Epoch: [142/200][60/346], lr: 0.00000030 	 loss = 0.1954(0.5127)
2023/09/29 13:53:24 - INFO - root -   Epoch: [142/200][80/346], lr: 0.00000030 	 loss = 0.6231(0.5269)
2023/09/29 13:54:22 - INFO - root -   Epoch: [142/200][100/346], lr: 0.00000030 	 loss = 0.5175(0.5303)
2023/09/29 13:55:39 - INFO - root -   Epoch: [142/200][120/346], lr: 0.00000030 	 loss = 0.4361(0.5621)
2023/09/29 13:56:40 - INFO - root -   Epoch: [142/200][140/346], lr: 0.00000030 	 loss = 0.5688(0.5690)
2023/09/29 13:57:47 - INFO - root -   Epoch: [142/200][160/346], lr: 0.00000030 	 loss = 0.3623(0.5816)
2023/09/29 13:58:51 - INFO - root -   Epoch: [142/200][180/346], lr: 0.00000030 	 loss = 0.8893(0.5786)
2023/09/29 14:00:15 - INFO - root -   Epoch: [142/200][200/346], lr: 0.00000030 	 loss = 0.2433(0.5807)
2023/09/29 14:01:06 - INFO - root -   Epoch: [142/200][220/346], lr: 0.00000030 	 loss = 0.2093(0.5921)
2023/09/29 14:02:21 - INFO - root -   Epoch: [142/200][240/346], lr: 0.00000030 	 loss = 0.4919(0.5917)
2023/09/29 14:03:33 - INFO - root -   Epoch: [142/200][260/346], lr: 0.00000030 	 loss = 0.7364(0.5969)
2023/09/29 14:04:31 - INFO - root -   Epoch: [142/200][280/346], lr: 0.00000030 	 loss = 0.2868(0.5960)
2023/09/29 14:05:52 - INFO - root -   Epoch: [142/200][300/346], lr: 0.00000030 	 loss = 0.1576(0.5929)
2023/09/29 14:07:01 - INFO - root -   Epoch: [142/200][320/346], lr: 0.00000030 	 loss = 0.3205(0.5881)
2023/09/29 14:07:57 - INFO - root -   Epoch: [142/200][340/346], lr: 0.00000030 	 loss = 1.0384(0.5874)
2023/09/29 14:07:59 - INFO - root -   Epoch: [142/200] 	 loss = 0.5869
2023/09/29 14:07:59 - INFO - root -   train_accuracy = 0.6864
2023/09/29 14:08:21 - INFO - root -   Epoch: [143/200][0/346], lr: 0.00000030 	 loss = 0.2485(0.2485)
2023/09/29 14:09:48 - INFO - root -   Epoch: [143/200][20/346], lr: 0.00000030 	 loss = 1.0504(0.5472)
2023/09/29 14:10:45 - INFO - root -   Epoch: [143/200][40/346], lr: 0.00000030 	 loss = 0.5478(0.5735)
2023/09/29 14:11:51 - INFO - root -   Epoch: [143/200][60/346], lr: 0.00000030 	 loss = 0.9089(0.5535)
2023/09/29 14:12:56 - INFO - root -   Epoch: [143/200][80/346], lr: 0.00000030 	 loss = 0.2986(0.5586)
2023/09/29 14:14:17 - INFO - root -   Epoch: [143/200][100/346], lr: 0.00000030 	 loss = 0.2989(0.5396)
2023/09/29 14:15:18 - INFO - root -   Epoch: [143/200][120/346], lr: 0.00000030 	 loss = 0.4028(0.5649)
2023/09/29 14:16:33 - INFO - root -   Epoch: [143/200][140/346], lr: 0.00000030 	 loss = 1.2367(0.5626)
2023/09/29 14:17:35 - INFO - root -   Epoch: [143/200][160/346], lr: 0.00000030 	 loss = 0.5381(0.5807)
2023/09/29 14:18:38 - INFO - root -   Epoch: [143/200][180/346], lr: 0.00000030 	 loss = 0.2470(0.5772)
2023/09/29 14:19:39 - INFO - root -   Epoch: [143/200][200/346], lr: 0.00000030 	 loss = 0.3369(0.5785)
2023/09/29 14:20:38 - INFO - root -   Epoch: [143/200][220/346], lr: 0.00000030 	 loss = 0.3839(0.5951)
2023/09/29 14:21:46 - INFO - root -   Epoch: [143/200][240/346], lr: 0.00000030 	 loss = 0.3720(0.5859)
2023/09/29 14:22:44 - INFO - root -   Epoch: [143/200][260/346], lr: 0.00000030 	 loss = 0.5456(0.5854)
2023/09/29 14:24:01 - INFO - root -   Epoch: [143/200][280/346], lr: 0.00000030 	 loss = 0.4588(0.5911)
2023/09/29 14:24:58 - INFO - root -   Epoch: [143/200][300/346], lr: 0.00000030 	 loss = 0.4092(0.5918)
2023/09/29 14:26:16 - INFO - root -   Epoch: [143/200][320/346], lr: 0.00000030 	 loss = 0.4929(0.5967)
2023/09/29 14:27:03 - INFO - root -   Epoch: [143/200][340/346], lr: 0.00000030 	 loss = 0.6665(0.5936)
2023/09/29 14:27:10 - INFO - root -   Epoch: [143/200] 	 loss = 0.5935
2023/09/29 14:27:10 - INFO - root -   train_accuracy = 0.6720
2023/09/29 14:27:32 - INFO - root -   Epoch: [144/200][0/346], lr: 0.00000031 	 loss = 0.4003(0.4003)
2023/09/29 14:28:45 - INFO - root -   Epoch: [144/200][20/346], lr: 0.00000031 	 loss = 0.1422(0.5855)
2023/09/29 14:29:35 - INFO - root -   Epoch: [144/200][40/346], lr: 0.00000031 	 loss = 1.1268(0.6039)
2023/09/29 14:30:53 - INFO - root -   Epoch: [144/200][60/346], lr: 0.00000031 	 loss = 0.4518(0.6242)
2023/09/29 14:31:52 - INFO - root -   Epoch: [144/200][80/346], lr: 0.00000031 	 loss = 0.5912(0.6205)
2023/09/29 14:33:18 - INFO - root -   Epoch: [144/200][100/346], lr: 0.00000031 	 loss = 0.9081(0.6266)
2023/09/29 14:34:19 - INFO - root -   Epoch: [144/200][120/346], lr: 0.00000031 	 loss = 0.4675(0.6530)
2023/09/29 14:35:25 - INFO - root -   Epoch: [144/200][140/346], lr: 0.00000031 	 loss = 1.0561(0.6528)
2023/09/29 14:36:52 - INFO - root -   Epoch: [144/200][160/346], lr: 0.00000031 	 loss = 0.5220(0.6567)
2023/09/29 14:37:35 - INFO - root -   Epoch: [144/200][180/346], lr: 0.00000031 	 loss = 0.7401(0.6510)
2023/09/29 14:38:52 - INFO - root -   Epoch: [144/200][200/346], lr: 0.00000031 	 loss = 0.2649(0.6297)
2023/09/29 14:39:40 - INFO - root -   Epoch: [144/200][220/346], lr: 0.00000031 	 loss = 0.5263(0.6325)
2023/09/29 14:40:52 - INFO - root -   Epoch: [144/200][240/346], lr: 0.00000031 	 loss = 0.1406(0.6340)
2023/09/29 14:41:42 - INFO - root -   Epoch: [144/200][260/346], lr: 0.00000031 	 loss = 0.6054(0.6329)
2023/09/29 14:43:00 - INFO - root -   Epoch: [144/200][280/346], lr: 0.00000031 	 loss = 0.7844(0.6417)
2023/09/29 14:44:08 - INFO - root -   Epoch: [144/200][300/346], lr: 0.00000031 	 loss = 0.3430(0.6401)
2023/09/29 14:45:08 - INFO - root -   Epoch: [144/200][320/346], lr: 0.00000031 	 loss = 0.4950(0.6367)
2023/09/29 14:46:12 - INFO - root -   Epoch: [144/200][340/346], lr: 0.00000031 	 loss = 0.8854(0.6291)
2023/09/29 14:46:14 - INFO - root -   Epoch: [144/200] 	 loss = 0.6294
2023/09/29 14:50:06 - INFO - root -   precision = 0.6782
2023/09/29 14:50:06 - INFO - root -   eval_loss = 0.5850
2023/09/29 14:50:07 - INFO - root -   train_accuracy = 0.6561
2023/09/29 14:50:39 - INFO - root -   Epoch: [145/200][0/346], lr: 0.00000031 	 loss = 0.5378(0.5378)
2023/09/29 14:51:38 - INFO - root -   Epoch: [145/200][20/346], lr: 0.00000031 	 loss = 0.6917(0.5117)
2023/09/29 14:53:03 - INFO - root -   Epoch: [145/200][40/346], lr: 0.00000031 	 loss = 0.8714(0.5942)
2023/09/29 14:53:56 - INFO - root -   Epoch: [145/200][60/346], lr: 0.00000031 	 loss = 0.8467(0.5741)
2023/09/29 14:55:08 - INFO - root -   Epoch: [145/200][80/346], lr: 0.00000031 	 loss = 0.3405(0.5704)
2023/09/29 14:56:06 - INFO - root -   Epoch: [145/200][100/346], lr: 0.00000031 	 loss = 0.9034(0.5800)
2023/09/29 14:57:24 - INFO - root -   Epoch: [145/200][120/346], lr: 0.00000031 	 loss = 0.5783(0.5937)
2023/09/29 14:58:34 - INFO - root -   Epoch: [145/200][140/346], lr: 0.00000031 	 loss = 0.3213(0.5761)
2023/09/29 14:59:40 - INFO - root -   Epoch: [145/200][160/346], lr: 0.00000031 	 loss = 0.0921(0.5959)
2023/09/29 15:01:03 - INFO - root -   Epoch: [145/200][180/346], lr: 0.00000031 	 loss = 0.6856(0.5960)
2023/09/29 15:01:53 - INFO - root -   Epoch: [145/200][200/346], lr: 0.00000031 	 loss = 0.6360(0.5955)
2023/09/29 15:03:00 - INFO - root -   Epoch: [145/200][220/346], lr: 0.00000031 	 loss = 0.4136(0.6010)
2023/09/29 15:04:04 - INFO - root -   Epoch: [145/200][240/346], lr: 0.00000031 	 loss = 0.3168(0.5923)
2023/09/29 15:05:10 - INFO - root -   Epoch: [145/200][260/346], lr: 0.00000031 	 loss = 0.4606(0.5958)
2023/09/29 15:06:04 - INFO - root -   Epoch: [145/200][280/346], lr: 0.00000031 	 loss = 0.5611(0.6056)
2023/09/29 15:07:22 - INFO - root -   Epoch: [145/200][300/346], lr: 0.00000031 	 loss = 0.4927(0.6043)
2023/09/29 15:08:32 - INFO - root -   Epoch: [145/200][320/346], lr: 0.00000031 	 loss = 0.3252(0.6013)
2023/09/29 15:09:26 - INFO - root -   Epoch: [145/200][340/346], lr: 0.00000031 	 loss = 1.1805(0.6050)
2023/09/29 15:09:28 - INFO - root -   Epoch: [145/200] 	 loss = 0.6065
2023/09/29 15:09:28 - INFO - root -   train_accuracy = 0.6777
2023/09/29 15:09:49 - INFO - root -   Epoch: [146/200][0/346], lr: 0.00000031 	 loss = 0.3565(0.3565)
2023/09/29 15:10:56 - INFO - root -   Epoch: [146/200][20/346], lr: 0.00000031 	 loss = 0.1141(0.5812)
2023/09/29 15:12:02 - INFO - root -   Epoch: [146/200][40/346], lr: 0.00000031 	 loss = 0.3166(0.5963)
2023/09/29 15:13:24 - INFO - root -   Epoch: [146/200][60/346], lr: 0.00000031 	 loss = 0.7159(0.5915)
2023/09/29 15:14:23 - INFO - root -   Epoch: [146/200][80/346], lr: 0.00000031 	 loss = 0.3433(0.5943)
2023/09/29 15:15:32 - INFO - root -   Epoch: [146/200][100/346], lr: 0.00000031 	 loss = 0.4598(0.5926)
2023/09/29 15:16:38 - INFO - root -   Epoch: [146/200][120/346], lr: 0.00000031 	 loss = 0.9090(0.6124)
2023/09/29 15:17:48 - INFO - root -   Epoch: [146/200][140/346], lr: 0.00000031 	 loss = 1.0746(0.6019)
2023/09/29 15:18:44 - INFO - root -   Epoch: [146/200][160/346], lr: 0.00000031 	 loss = 0.3406(0.5995)
2023/09/29 15:19:57 - INFO - root -   Epoch: [146/200][180/346], lr: 0.00000031 	 loss = 0.4636(0.5902)
2023/09/29 15:21:03 - INFO - root -   Epoch: [146/200][200/346], lr: 0.00000031 	 loss = 0.2666(0.5890)
2023/09/29 15:22:23 - INFO - root -   Epoch: [146/200][220/346], lr: 0.00000031 	 loss = 0.2814(0.5994)
2023/09/29 15:23:15 - INFO - root -   Epoch: [146/200][240/346], lr: 0.00000031 	 loss = 0.3446(0.5933)
2023/09/29 15:24:46 - INFO - root -   Epoch: [146/200][260/346], lr: 0.00000031 	 loss = 0.9075(0.5968)
2023/09/29 15:25:34 - INFO - root -   Epoch: [146/200][280/346], lr: 0.00000031 	 loss = 0.4112(0.6074)
2023/09/29 15:26:55 - INFO - root -   Epoch: [146/200][300/346], lr: 0.00000031 	 loss = 0.2024(0.5975)
2023/09/29 15:27:39 - INFO - root -   Epoch: [146/200][320/346], lr: 0.00000031 	 loss = 0.5270(0.5933)
2023/09/29 15:28:37 - INFO - root -   Epoch: [146/200][340/346], lr: 0.00000031 	 loss = 0.7955(0.5924)
2023/09/29 15:28:39 - INFO - root -   Epoch: [146/200] 	 loss = 0.5942
2023/09/29 15:28:39 - INFO - root -   train_accuracy = 0.6720
2023/09/29 15:29:18 - INFO - root -   Epoch: [147/200][0/346], lr: 0.00000031 	 loss = 0.8028(0.8028)
2023/09/29 15:30:15 - INFO - root -   Epoch: [147/200][20/346], lr: 0.00000031 	 loss = 0.1753(0.4867)
2023/09/29 15:31:27 - INFO - root -   Epoch: [147/200][40/346], lr: 0.00000031 	 loss = 0.4126(0.5212)
2023/09/29 15:32:28 - INFO - root -   Epoch: [147/200][60/346], lr: 0.00000031 	 loss = 0.4385(0.4994)
2023/09/29 15:33:43 - INFO - root -   Epoch: [147/200][80/346], lr: 0.00000031 	 loss = 0.3859(0.5219)
2023/09/29 15:34:57 - INFO - root -   Epoch: [147/200][100/346], lr: 0.00000031 	 loss = 0.7207(0.5499)
2023/09/29 15:36:02 - INFO - root -   Epoch: [147/200][120/346], lr: 0.00000031 	 loss = 0.3725(0.5643)
2023/09/29 15:37:08 - INFO - root -   Epoch: [147/200][140/346], lr: 0.00000031 	 loss = 0.4136(0.5641)
2023/09/29 15:38:26 - INFO - root -   Epoch: [147/200][160/346], lr: 0.00000031 	 loss = 0.3621(0.5702)
2023/09/29 15:39:41 - INFO - root -   Epoch: [147/200][180/346], lr: 0.00000031 	 loss = 0.3282(0.5684)
2023/09/29 15:40:39 - INFO - root -   Epoch: [147/200][200/346], lr: 0.00000031 	 loss = 0.2612(0.5643)
2023/09/29 15:41:59 - INFO - root -   Epoch: [147/200][220/346], lr: 0.00000031 	 loss = 0.1915(0.5722)
2023/09/29 15:42:51 - INFO - root -   Epoch: [147/200][240/346], lr: 0.00000031 	 loss = 0.2614(0.5706)
2023/09/29 15:44:12 - INFO - root -   Epoch: [147/200][260/346], lr: 0.00000031 	 loss = 0.3166(0.5796)
2023/09/29 15:45:00 - INFO - root -   Epoch: [147/200][280/346], lr: 0.00000031 	 loss = 0.7695(0.5835)
2023/09/29 15:46:09 - INFO - root -   Epoch: [147/200][300/346], lr: 0.00000031 	 loss = 0.5226(0.5878)
2023/09/29 15:47:32 - INFO - root -   Epoch: [147/200][320/346], lr: 0.00000031 	 loss = 0.6602(0.5912)
2023/09/29 15:48:13 - INFO - root -   Epoch: [147/200][340/346], lr: 0.00000031 	 loss = 0.7581(0.5896)
2023/09/29 15:48:22 - INFO - root -   Epoch: [147/200] 	 loss = 0.5883
2023/09/29 15:48:22 - INFO - root -   train_accuracy = 0.7009
2023/09/29 15:48:44 - INFO - root -   Epoch: [148/200][0/346], lr: 0.00000031 	 loss = 0.4861(0.4861)
2023/09/29 15:49:59 - INFO - root -   Epoch: [148/200][20/346], lr: 0.00000031 	 loss = 0.3202(0.5227)
2023/09/29 15:51:03 - INFO - root -   Epoch: [148/200][40/346], lr: 0.00000031 	 loss = 0.7519(0.5727)
2023/09/29 15:52:18 - INFO - root -   Epoch: [148/200][60/346], lr: 0.00000031 	 loss = 0.8929(0.5533)
2023/09/29 15:53:01 - INFO - root -   Epoch: [148/200][80/346], lr: 0.00000031 	 loss = 0.4167(0.5872)
2023/09/29 15:54:19 - INFO - root -   Epoch: [148/200][100/346], lr: 0.00000031 	 loss = 0.3611(0.5692)
2023/09/29 15:55:13 - INFO - root -   Epoch: [148/200][120/346], lr: 0.00000031 	 loss = 0.3486(0.5799)
2023/09/29 15:56:31 - INFO - root -   Epoch: [148/200][140/346], lr: 0.00000031 	 loss = 0.3191(0.5742)
2023/09/29 15:57:22 - INFO - root -   Epoch: [148/200][160/346], lr: 0.00000031 	 loss = 0.1387(0.5708)
2023/09/29 15:58:43 - INFO - root -   Epoch: [148/200][180/346], lr: 0.00000031 	 loss = 0.4040(0.5670)
2023/09/29 15:59:33 - INFO - root -   Epoch: [148/200][200/346], lr: 0.00000031 	 loss = 0.2823(0.5654)
2023/09/29 16:00:58 - INFO - root -   Epoch: [148/200][220/346], lr: 0.00000031 	 loss = 0.7244(0.5687)
2023/09/29 16:02:00 - INFO - root -   Epoch: [148/200][240/346], lr: 0.00000031 	 loss = 0.2021(0.5651)
2023/09/29 16:03:05 - INFO - root -   Epoch: [148/200][260/346], lr: 0.00000031 	 loss = 0.6562(0.5611)
2023/09/29 16:04:06 - INFO - root -   Epoch: [148/200][280/346], lr: 0.00000031 	 loss = 0.3280(0.5753)
2023/09/29 16:05:28 - INFO - root -   Epoch: [148/200][300/346], lr: 0.00000031 	 loss = 0.5737(0.5834)
2023/09/29 16:06:24 - INFO - root -   Epoch: [148/200][320/346], lr: 0.00000031 	 loss = 0.2408(0.5827)
2023/09/29 16:07:32 - INFO - root -   Epoch: [148/200][340/346], lr: 0.00000031 	 loss = 0.5230(0.5792)
2023/09/29 16:07:34 - INFO - root -   Epoch: [148/200] 	 loss = 0.5803
2023/09/29 16:07:34 - INFO - root -   train_accuracy = 0.6908
2023/09/29 16:07:57 - INFO - root -   Epoch: [149/200][0/346], lr: 0.00000031 	 loss = 0.1872(0.1872)
2023/09/29 16:08:57 - INFO - root -   Epoch: [149/200][20/346], lr: 0.00000031 	 loss = 0.1445(0.5184)
2023/09/29 16:10:13 - INFO - root -   Epoch: [149/200][40/346], lr: 0.00000031 	 loss = 0.8190(0.5797)
2023/09/29 16:11:24 - INFO - root -   Epoch: [149/200][60/346], lr: 0.00000031 	 loss = 0.5780(0.5370)
2023/09/29 16:12:25 - INFO - root -   Epoch: [149/200][80/346], lr: 0.00000031 	 loss = 0.2998(0.5247)
2023/09/29 16:13:41 - INFO - root -   Epoch: [149/200][100/346], lr: 0.00000031 	 loss = 0.3323(0.5396)
2023/09/29 16:14:53 - INFO - root -   Epoch: [149/200][120/346], lr: 0.00000031 	 loss = 0.2426(0.5606)
2023/09/29 16:15:51 - INFO - root -   Epoch: [149/200][140/346], lr: 0.00000031 	 loss = 0.7356(0.5599)
2023/09/29 16:16:52 - INFO - root -   Epoch: [149/200][160/346], lr: 0.00000031 	 loss = 0.1299(0.5827)
2023/09/29 16:17:59 - INFO - root -   Epoch: [149/200][180/346], lr: 0.00000031 	 loss = 0.4666(0.5828)
2023/09/29 16:19:03 - INFO - root -   Epoch: [149/200][200/346], lr: 0.00000031 	 loss = 0.1458(0.5783)
2023/09/29 16:20:15 - INFO - root -   Epoch: [149/200][220/346], lr: 0.00000031 	 loss = 0.2862(0.5851)
2023/09/29 16:21:16 - INFO - root -   Epoch: [149/200][240/346], lr: 0.00000031 	 loss = 0.3679(0.5825)
2023/09/29 16:22:20 - INFO - root -   Epoch: [149/200][260/346], lr: 0.00000031 	 loss = 0.9546(0.5776)
2023/09/29 16:23:26 - INFO - root -   Epoch: [149/200][280/346], lr: 0.00000031 	 loss = 0.6728(0.5858)
2023/09/29 16:24:32 - INFO - root -   Epoch: [149/200][300/346], lr: 0.00000031 	 loss = 0.6279(0.5917)
2023/09/29 16:25:52 - INFO - root -   Epoch: [149/200][320/346], lr: 0.00000031 	 loss = 0.2594(0.5894)
2023/09/29 16:26:45 - INFO - root -   Epoch: [149/200][340/346], lr: 0.00000031 	 loss = 0.5205(0.5865)
2023/09/29 16:26:48 - INFO - root -   Epoch: [149/200] 	 loss = 0.5842
2023/09/29 16:30:38 - INFO - root -   precision = 0.6839
2023/09/29 16:30:38 - INFO - root -   eval_loss = 0.5762
2023/09/29 16:30:39 - INFO - root -   train_accuracy = 0.6662
2023/09/29 16:31:01 - INFO - root -   Epoch: [150/200][0/346], lr: 0.00000031 	 loss = 0.4486(0.4486)
2023/09/29 16:32:25 - INFO - root -   Epoch: [150/200][20/346], lr: 0.00000031 	 loss = 0.8553(0.6530)
2023/09/29 16:33:19 - INFO - root -   Epoch: [150/200][40/346], lr: 0.00000031 	 loss = 0.6903(0.6279)
2023/09/29 16:34:35 - INFO - root -   Epoch: [150/200][60/346], lr: 0.00000031 	 loss = 0.7736(0.5850)
2023/09/29 16:35:31 - INFO - root -   Epoch: [150/200][80/346], lr: 0.00000031 	 loss = 0.1933(0.5869)
2023/09/29 16:36:48 - INFO - root -   Epoch: [150/200][100/346], lr: 0.00000031 	 loss = 0.9306(0.5861)
2023/09/29 16:37:43 - INFO - root -   Epoch: [150/200][120/346], lr: 0.00000031 	 loss = 0.6698(0.6079)
2023/09/29 16:39:08 - INFO - root -   Epoch: [150/200][140/346], lr: 0.00000031 	 loss = 0.3817(0.6105)
2023/09/29 16:39:57 - INFO - root -   Epoch: [150/200][160/346], lr: 0.00000031 	 loss = 0.6903(0.6160)
2023/09/29 16:41:06 - INFO - root -   Epoch: [150/200][180/346], lr: 0.00000031 	 loss = 0.5021(0.6056)
2023/09/29 16:42:17 - INFO - root -   Epoch: [150/200][200/346], lr: 0.00000031 	 loss = 0.1075(0.6013)
2023/09/29 16:43:10 - INFO - root -   Epoch: [150/200][220/346], lr: 0.00000031 	 loss = 0.1841(0.6111)
2023/09/29 16:44:41 - INFO - root -   Epoch: [150/200][240/346], lr: 0.00000031 	 loss = 0.4733(0.6100)
2023/09/29 16:45:24 - INFO - root -   Epoch: [150/200][260/346], lr: 0.00000031 	 loss = 0.6553(0.6063)
2023/09/29 16:46:47 - INFO - root -   Epoch: [150/200][280/346], lr: 0.00000031 	 loss = 0.1628(0.6117)
2023/09/29 16:47:35 - INFO - root -   Epoch: [150/200][300/346], lr: 0.00000031 	 loss = 0.2257(0.6054)
2023/09/29 16:48:51 - INFO - root -   Epoch: [150/200][320/346], lr: 0.00000031 	 loss = 0.4531(0.6052)
2023/09/29 16:49:40 - INFO - root -   Epoch: [150/200][340/346], lr: 0.00000031 	 loss = 0.6910(0.6027)
2023/09/29 16:49:46 - INFO - root -   Epoch: [150/200] 	 loss = 0.6022
2023/09/29 16:49:46 - INFO - root -   train_accuracy = 0.6662
2023/09/29 16:50:18 - INFO - root -   Epoch: [151/200][0/346], lr: 0.00000032 	 loss = 0.4867(0.4867)
2023/09/29 16:51:10 - INFO - root -   Epoch: [151/200][20/346], lr: 0.00000032 	 loss = 0.4342(0.5177)
2023/09/29 16:52:26 - INFO - root -   Epoch: [151/200][40/346], lr: 0.00000032 	 loss = 0.8299(0.5712)
2023/09/29 16:53:40 - INFO - root -   Epoch: [151/200][60/346], lr: 0.00000032 	 loss = 0.4916(0.5571)
2023/09/29 16:54:39 - INFO - root -   Epoch: [151/200][80/346], lr: 0.00000032 	 loss = 0.4082(0.5775)
2023/09/29 16:56:05 - INFO - root -   Epoch: [151/200][100/346], lr: 0.00000032 	 loss = 0.6289(0.6020)
2023/09/29 16:56:57 - INFO - root -   Epoch: [151/200][120/346], lr: 0.00000032 	 loss = 0.4064(0.6169)
2023/09/29 16:58:13 - INFO - root -   Epoch: [151/200][140/346], lr: 0.00000032 	 loss = 0.5567(0.6042)
2023/09/29 16:59:11 - INFO - root -   Epoch: [151/200][160/346], lr: 0.00000032 	 loss = 0.1601(0.6078)
2023/09/29 17:00:42 - INFO - root -   Epoch: [151/200][180/346], lr: 0.00000032 	 loss = 1.0629(0.5959)
2023/09/29 17:01:36 - INFO - root -   Epoch: [151/200][200/346], lr: 0.00000032 	 loss = 0.1612(0.5857)
2023/09/29 17:02:48 - INFO - root -   Epoch: [151/200][220/346], lr: 0.00000032 	 loss = 0.4424(0.6004)
2023/09/29 17:03:46 - INFO - root -   Epoch: [151/200][240/346], lr: 0.00000032 	 loss = 0.2170(0.5948)
2023/09/29 17:04:49 - INFO - root -   Epoch: [151/200][260/346], lr: 0.00000032 	 loss = 0.8219(0.5934)
2023/09/29 17:05:38 - INFO - root -   Epoch: [151/200][280/346], lr: 0.00000032 	 loss = 0.2577(0.5971)
2023/09/29 17:06:54 - INFO - root -   Epoch: [151/200][300/346], lr: 0.00000032 	 loss = 0.2534(0.5990)
2023/09/29 17:07:57 - INFO - root -   Epoch: [151/200][320/346], lr: 0.00000032 	 loss = 0.1972(0.6022)
2023/09/29 17:08:59 - INFO - root -   Epoch: [151/200][340/346], lr: 0.00000032 	 loss = 1.3158(0.6073)
2023/09/29 17:09:02 - INFO - root -   Epoch: [151/200] 	 loss = 0.6086
2023/09/29 17:09:02 - INFO - root -   train_accuracy = 0.6777
2023/09/29 17:09:24 - INFO - root -   Epoch: [152/200][0/346], lr: 0.00000032 	 loss = 0.1548(0.1548)
2023/09/29 17:10:39 - INFO - root -   Epoch: [152/200][20/346], lr: 0.00000032 	 loss = 0.3967(0.4770)
2023/09/29 17:11:41 - INFO - root -   Epoch: [152/200][40/346], lr: 0.00000032 	 loss = 0.8278(0.5440)
2023/09/29 17:12:44 - INFO - root -   Epoch: [152/200][60/346], lr: 0.00000032 	 loss = 0.2451(0.5065)
2023/09/29 17:13:52 - INFO - root -   Epoch: [152/200][80/346], lr: 0.00000032 	 loss = 0.9841(0.5086)
2023/09/29 17:14:59 - INFO - root -   Epoch: [152/200][100/346], lr: 0.00000032 	 loss = 0.5822(0.5164)
2023/09/29 17:16:28 - INFO - root -   Epoch: [152/200][120/346], lr: 0.00000032 	 loss = 1.1634(0.5522)
2023/09/29 17:17:20 - INFO - root -   Epoch: [152/200][140/346], lr: 0.00000032 	 loss = 1.0227(0.5505)
2023/09/29 17:18:44 - INFO - root -   Epoch: [152/200][160/346], lr: 0.00000032 	 loss = 0.9360(0.5598)
2023/09/29 17:19:34 - INFO - root -   Epoch: [152/200][180/346], lr: 0.00000032 	 loss = 0.2592(0.5640)
2023/09/29 17:21:10 - INFO - root -   Epoch: [152/200][200/346], lr: 0.00000032 	 loss = 0.2287(0.5587)
2023/09/29 17:22:06 - INFO - root -   Epoch: [152/200][220/346], lr: 0.00000032 	 loss = 0.2737(0.5702)
2023/09/29 17:23:20 - INFO - root -   Epoch: [152/200][240/346], lr: 0.00000032 	 loss = 0.1731(0.5720)
2023/09/29 17:24:11 - INFO - root -   Epoch: [152/200][260/346], lr: 0.00000032 	 loss = 0.8762(0.5719)
2023/09/29 17:25:23 - INFO - root -   Epoch: [152/200][280/346], lr: 0.00000032 	 loss = 0.5712(0.5853)
2023/09/29 17:26:24 - INFO - root -   Epoch: [152/200][300/346], lr: 0.00000032 	 loss = 0.5489(0.5806)
2023/09/29 17:27:18 - INFO - root -   Epoch: [152/200][320/346], lr: 0.00000032 	 loss = 0.2127(0.5801)
2023/09/29 17:28:17 - INFO - root -   Epoch: [152/200][340/346], lr: 0.00000032 	 loss = 1.0692(0.5855)
2023/09/29 17:28:19 - INFO - root -   Epoch: [152/200] 	 loss = 0.5826
2023/09/29 17:28:19 - INFO - root -   train_accuracy = 0.6777
2023/09/29 17:28:49 - INFO - root -   Epoch: [153/200][0/346], lr: 0.00000032 	 loss = 0.5797(0.5797)
2023/09/29 17:29:55 - INFO - root -   Epoch: [153/200][20/346], lr: 0.00000032 	 loss = 0.3278(0.5231)
2023/09/29 17:30:56 - INFO - root -   Epoch: [153/200][40/346], lr: 0.00000032 	 loss = 0.4421(0.5063)
2023/09/29 17:32:14 - INFO - root -   Epoch: [153/200][60/346], lr: 0.00000032 	 loss = 0.8769(0.4885)
2023/09/29 17:33:05 - INFO - root -   Epoch: [153/200][80/346], lr: 0.00000032 	 loss = 0.5786(0.5207)
2023/09/29 17:34:23 - INFO - root -   Epoch: [153/200][100/346], lr: 0.00000032 	 loss = 0.1055(0.5409)
2023/09/29 17:35:15 - INFO - root -   Epoch: [153/200][120/346], lr: 0.00000032 	 loss = 0.2050(0.5621)
2023/09/29 17:36:43 - INFO - root -   Epoch: [153/200][140/346], lr: 0.00000032 	 loss = 0.5950(0.5463)
2023/09/29 17:37:31 - INFO - root -   Epoch: [153/200][160/346], lr: 0.00000032 	 loss = 0.3634(0.5444)
2023/09/29 17:38:51 - INFO - root -   Epoch: [153/200][180/346], lr: 0.00000032 	 loss = 0.6639(0.5561)
2023/09/29 17:39:39 - INFO - root -   Epoch: [153/200][200/346], lr: 0.00000032 	 loss = 0.1328(0.5576)
2023/09/29 17:40:53 - INFO - root -   Epoch: [153/200][220/346], lr: 0.00000032 	 loss = 0.2000(0.5715)
2023/09/29 17:41:44 - INFO - root -   Epoch: [153/200][240/346], lr: 0.00000032 	 loss = 0.2937(0.5734)
2023/09/29 17:43:00 - INFO - root -   Epoch: [153/200][260/346], lr: 0.00000032 	 loss = 0.6685(0.5768)
2023/09/29 17:43:56 - INFO - root -   Epoch: [153/200][280/346], lr: 0.00000032 	 loss = 0.2959(0.5841)
2023/09/29 17:45:12 - INFO - root -   Epoch: [153/200][300/346], lr: 0.00000032 	 loss = 0.6203(0.5799)
2023/09/29 17:46:02 - INFO - root -   Epoch: [153/200][320/346], lr: 0.00000032 	 loss = 0.2186(0.5787)
2023/09/29 17:47:05 - INFO - root -   Epoch: [153/200][340/346], lr: 0.00000032 	 loss = 1.5589(0.5836)
2023/09/29 17:47:07 - INFO - root -   Epoch: [153/200] 	 loss = 0.5844
2023/09/29 17:47:07 - INFO - root -   train_accuracy = 0.6734
2023/09/29 17:47:28 - INFO - root -   Epoch: [154/200][0/346], lr: 0.00000032 	 loss = 0.3494(0.3494)
2023/09/29 17:48:37 - INFO - root -   Epoch: [154/200][20/346], lr: 0.00000032 	 loss = 0.1477(0.3906)
2023/09/29 17:49:44 - INFO - root -   Epoch: [154/200][40/346], lr: 0.00000032 	 loss = 1.5112(0.5138)
2023/09/29 17:51:03 - INFO - root -   Epoch: [154/200][60/346], lr: 0.00000032 	 loss = 0.8191(0.5305)
2023/09/29 17:51:56 - INFO - root -   Epoch: [154/200][80/346], lr: 0.00000032 	 loss = 0.7957(0.5264)
2023/09/29 17:53:10 - INFO - root -   Epoch: [154/200][100/346], lr: 0.00000032 	 loss = 0.8447(0.5262)
2023/09/29 17:54:04 - INFO - root -   Epoch: [154/200][120/346], lr: 0.00000032 	 loss = 0.3244(0.5369)
2023/09/29 17:55:17 - INFO - root -   Epoch: [154/200][140/346], lr: 0.00000032 	 loss = 1.0587(0.5510)
2023/09/29 17:56:47 - INFO - root -   Epoch: [154/200][160/346], lr: 0.00000032 	 loss = 0.4229(0.5707)
2023/09/29 17:57:25 - INFO - root -   Epoch: [154/200][180/346], lr: 0.00000032 	 loss = 0.4066(0.5626)
2023/09/29 17:58:47 - INFO - root -   Epoch: [154/200][200/346], lr: 0.00000032 	 loss = 0.5498(0.5582)
2023/09/29 17:59:44 - INFO - root -   Epoch: [154/200][220/346], lr: 0.00000032 	 loss = 0.2798(0.5673)
2023/09/29 18:01:13 - INFO - root -   Epoch: [154/200][240/346], lr: 0.00000032 	 loss = 0.3024(0.5654)
2023/09/29 18:02:04 - INFO - root -   Epoch: [154/200][260/346], lr: 0.00000032 	 loss = 0.9147(0.5658)
2023/09/29 18:03:14 - INFO - root -   Epoch: [154/200][280/346], lr: 0.00000032 	 loss = 0.3494(0.5669)
2023/09/29 18:04:04 - INFO - root -   Epoch: [154/200][300/346], lr: 0.00000032 	 loss = 0.4558(0.5669)
2023/09/29 18:05:13 - INFO - root -   Epoch: [154/200][320/346], lr: 0.00000032 	 loss = 0.5835(0.5620)
2023/09/29 18:06:14 - INFO - root -   Epoch: [154/200][340/346], lr: 0.00000032 	 loss = 0.5099(0.5606)
2023/09/29 18:06:20 - INFO - root -   Epoch: [154/200] 	 loss = 0.5601
2023/09/29 18:10:12 - INFO - root -   precision = 0.6954
2023/09/29 18:10:12 - INFO - root -   eval_loss = 0.5970
2023/09/29 18:10:13 - INFO - root -   train_accuracy = 0.7312
2023/09/29 18:10:35 - INFO - root -   Epoch: [155/200][0/346], lr: 0.00000032 	 loss = 0.1075(0.1075)
2023/09/29 18:11:44 - INFO - root -   Epoch: [155/200][20/346], lr: 0.00000032 	 loss = 0.1441(0.5341)
2023/09/29 18:13:05 - INFO - root -   Epoch: [155/200][40/346], lr: 0.00000032 	 loss = 0.7596(0.5296)
2023/09/29 18:14:07 - INFO - root -   Epoch: [155/200][60/346], lr: 0.00000032 	 loss = 0.6066(0.5154)
2023/09/29 18:15:18 - INFO - root -   Epoch: [155/200][80/346], lr: 0.00000032 	 loss = 0.7448(0.5213)
2023/09/29 18:16:32 - INFO - root -   Epoch: [155/200][100/346], lr: 0.00000032 	 loss = 0.6532(0.5500)
2023/09/29 18:17:33 - INFO - root -   Epoch: [155/200][120/346], lr: 0.00000032 	 loss = 0.4933(0.5908)
2023/09/29 18:18:48 - INFO - root -   Epoch: [155/200][140/346], lr: 0.00000032 	 loss = 0.8706(0.5890)
2023/09/29 18:19:50 - INFO - root -   Epoch: [155/200][160/346], lr: 0.00000032 	 loss = 0.1675(0.5864)
2023/09/29 18:21:06 - INFO - root -   Epoch: [155/200][180/346], lr: 0.00000032 	 loss = 1.2356(0.5889)
2023/09/29 18:21:59 - INFO - root -   Epoch: [155/200][200/346], lr: 0.00000032 	 loss = 0.2430(0.5953)
2023/09/29 18:23:16 - INFO - root -   Epoch: [155/200][220/346], lr: 0.00000032 	 loss = 0.2099(0.6017)
2023/09/29 18:24:21 - INFO - root -   Epoch: [155/200][240/346], lr: 0.00000032 	 loss = 0.2890(0.5994)
2023/09/29 18:25:39 - INFO - root -   Epoch: [155/200][260/346], lr: 0.00000032 	 loss = 0.5662(0.6034)
2023/09/29 18:26:51 - INFO - root -   Epoch: [155/200][280/346], lr: 0.00000032 	 loss = 0.5736(0.6091)
2023/09/29 18:27:53 - INFO - root -   Epoch: [155/200][300/346], lr: 0.00000032 	 loss = 0.3805(0.6062)
2023/09/29 18:28:55 - INFO - root -   Epoch: [155/200][320/346], lr: 0.00000032 	 loss = 0.2575(0.6083)
2023/09/29 18:29:59 - INFO - root -   Epoch: [155/200][340/346], lr: 0.00000032 	 loss = 0.7050(0.6033)
2023/09/29 18:30:05 - INFO - root -   Epoch: [155/200] 	 loss = 0.6053
2023/09/29 18:30:05 - INFO - root -   train_accuracy = 0.6792
2023/09/29 18:30:26 - INFO - root -   Epoch: [156/200][0/346], lr: 0.00000032 	 loss = 0.4078(0.4078)
2023/09/29 18:31:41 - INFO - root -   Epoch: [156/200][20/346], lr: 0.00000032 	 loss = 0.3073(0.5120)
2023/09/29 18:32:34 - INFO - root -   Epoch: [156/200][40/346], lr: 0.00000032 	 loss = 0.9409(0.5621)
2023/09/29 18:33:58 - INFO - root -   Epoch: [156/200][60/346], lr: 0.00000032 	 loss = 0.9489(0.5133)
2023/09/29 18:34:50 - INFO - root -   Epoch: [156/200][80/346], lr: 0.00000032 	 loss = 0.2313(0.5229)
2023/09/29 18:35:52 - INFO - root -   Epoch: [156/200][100/346], lr: 0.00000032 	 loss = 0.5885(0.5241)
2023/09/29 18:37:07 - INFO - root -   Epoch: [156/200][120/346], lr: 0.00000032 	 loss = 0.6784(0.5698)
2023/09/29 18:38:06 - INFO - root -   Epoch: [156/200][140/346], lr: 0.00000032 	 loss = 1.2587(0.5663)
2023/09/29 18:39:26 - INFO - root -   Epoch: [156/200][160/346], lr: 0.00000032 	 loss = 0.5164(0.5667)
2023/09/29 18:40:47 - INFO - root -   Epoch: [156/200][180/346], lr: 0.00000032 	 loss = 0.5285(0.5757)
2023/09/29 18:41:41 - INFO - root -   Epoch: [156/200][200/346], lr: 0.00000032 	 loss = 0.0967(0.5768)
2023/09/29 18:42:56 - INFO - root -   Epoch: [156/200][220/346], lr: 0.00000032 	 loss = 0.7173(0.5892)
2023/09/29 18:44:01 - INFO - root -   Epoch: [156/200][240/346], lr: 0.00000032 	 loss = 0.3773(0.5815)
2023/09/29 18:45:00 - INFO - root -   Epoch: [156/200][260/346], lr: 0.00000032 	 loss = 0.9692(0.5904)
2023/09/29 18:46:03 - INFO - root -   Epoch: [156/200][280/346], lr: 0.00000032 	 loss = 1.1118(0.5962)
2023/09/29 18:47:27 - INFO - root -   Epoch: [156/200][300/346], lr: 0.00000032 	 loss = 0.7259(0.5947)
2023/09/29 18:48:28 - INFO - root -   Epoch: [156/200][320/346], lr: 0.00000032 	 loss = 0.5194(0.5895)
2023/09/29 18:49:20 - INFO - root -   Epoch: [156/200][340/346], lr: 0.00000032 	 loss = 0.5890(0.5799)
2023/09/29 18:49:22 - INFO - root -   Epoch: [156/200] 	 loss = 0.5846
2023/09/29 18:49:22 - INFO - root -   train_accuracy = 0.6821
2023/09/29 18:50:02 - INFO - root -   Epoch: [157/200][0/346], lr: 0.00000032 	 loss = 0.4422(0.4422)
2023/09/29 18:50:59 - INFO - root -   Epoch: [157/200][20/346], lr: 0.00000032 	 loss = 0.1720(0.4700)
2023/09/29 18:52:10 - INFO - root -   Epoch: [157/200][40/346], lr: 0.00000032 	 loss = 0.6705(0.5480)
2023/09/29 18:53:00 - INFO - root -   Epoch: [157/200][60/346], lr: 0.00000032 	 loss = 0.4267(0.5058)
2023/09/29 18:54:28 - INFO - root -   Epoch: [157/200][80/346], lr: 0.00000032 	 loss = 0.3004(0.5172)
2023/09/29 18:55:26 - INFO - root -   Epoch: [157/200][100/346], lr: 0.00000032 	 loss = 0.2863(0.5306)
2023/09/29 18:56:26 - INFO - root -   Epoch: [157/200][120/346], lr: 0.00000032 	 loss = 0.4003(0.5526)
2023/09/29 18:57:33 - INFO - root -   Epoch: [157/200][140/346], lr: 0.00000032 	 loss = 0.7904(0.5568)
2023/09/29 18:58:51 - INFO - root -   Epoch: [157/200][160/346], lr: 0.00000032 	 loss = 0.8503(0.5641)
2023/09/29 19:00:01 - INFO - root -   Epoch: [157/200][180/346], lr: 0.00000032 	 loss = 0.8614(0.5651)
2023/09/29 19:01:05 - INFO - root -   Epoch: [157/200][200/346], lr: 0.00000032 	 loss = 0.1985(0.5683)
2023/09/29 19:02:16 - INFO - root -   Epoch: [157/200][220/346], lr: 0.00000032 	 loss = 0.1696(0.5819)
2023/09/29 19:03:20 - INFO - root -   Epoch: [157/200][240/346], lr: 0.00000032 	 loss = 0.2070(0.5787)
2023/09/29 19:04:17 - INFO - root -   Epoch: [157/200][260/346], lr: 0.00000032 	 loss = 0.6908(0.5760)
2023/09/29 19:05:46 - INFO - root -   Epoch: [157/200][280/346], lr: 0.00000032 	 loss = 0.4226(0.5842)
2023/09/29 19:06:29 - INFO - root -   Epoch: [157/200][300/346], lr: 0.00000032 	 loss = 0.4619(0.5817)
2023/09/29 19:07:56 - INFO - root -   Epoch: [157/200][320/346], lr: 0.00000032 	 loss = 0.4119(0.5844)
2023/09/29 19:08:42 - INFO - root -   Epoch: [157/200][340/346], lr: 0.00000032 	 loss = 1.4019(0.5860)
2023/09/29 19:08:44 - INFO - root -   Epoch: [157/200] 	 loss = 0.5849
2023/09/29 19:08:44 - INFO - root -   train_accuracy = 0.6864
2023/09/29 19:09:23 - INFO - root -   Epoch: [158/200][0/346], lr: 0.00000033 	 loss = 0.7760(0.7760)
2023/09/29 19:10:27 - INFO - root -   Epoch: [158/200][20/346], lr: 0.00000033 	 loss = 0.3367(0.5055)
2023/09/29 19:11:29 - INFO - root -   Epoch: [158/200][40/346], lr: 0.00000033 	 loss = 0.7942(0.5838)
2023/09/29 19:12:47 - INFO - root -   Epoch: [158/200][60/346], lr: 0.00000033 	 loss = 0.9969(0.5569)
2023/09/29 19:13:51 - INFO - root -   Epoch: [158/200][80/346], lr: 0.00000033 	 loss = 0.2329(0.5791)
2023/09/29 19:15:09 - INFO - root -   Epoch: [158/200][100/346], lr: 0.00000033 	 loss = 0.9202(0.5722)
2023/09/29 19:16:00 - INFO - root -   Epoch: [158/200][120/346], lr: 0.00000033 	 loss = 0.1156(0.5705)
2023/09/29 19:17:19 - INFO - root -   Epoch: [158/200][140/346], lr: 0.00000033 	 loss = 0.4524(0.5680)
2023/09/29 19:18:15 - INFO - root -   Epoch: [158/200][160/346], lr: 0.00000033 	 loss = 0.3515(0.5813)
2023/09/29 19:19:31 - INFO - root -   Epoch: [158/200][180/346], lr: 0.00000033 	 loss = 0.6119(0.5858)
2023/09/29 19:20:32 - INFO - root -   Epoch: [158/200][200/346], lr: 0.00000033 	 loss = 0.1933(0.5830)
2023/09/29 19:21:58 - INFO - root -   Epoch: [158/200][220/346], lr: 0.00000033 	 loss = 0.5346(0.5919)
2023/09/29 19:22:41 - INFO - root -   Epoch: [158/200][240/346], lr: 0.00000033 	 loss = 0.1235(0.5896)
2023/09/29 19:24:00 - INFO - root -   Epoch: [158/200][260/346], lr: 0.00000033 	 loss = 0.5168(0.5880)
2023/09/29 19:24:47 - INFO - root -   Epoch: [158/200][280/346], lr: 0.00000033 	 loss = 1.2367(0.5945)
2023/09/29 19:26:08 - INFO - root -   Epoch: [158/200][300/346], lr: 0.00000033 	 loss = 0.3455(0.5876)
2023/09/29 19:26:58 - INFO - root -   Epoch: [158/200][320/346], lr: 0.00000033 	 loss = 0.5603(0.5824)
2023/09/29 19:28:13 - INFO - root -   Epoch: [158/200][340/346], lr: 0.00000033 	 loss = 0.5072(0.5834)
2023/09/29 19:28:15 - INFO - root -   Epoch: [158/200] 	 loss = 0.5850
2023/09/29 19:28:15 - INFO - root -   train_accuracy = 0.6806
2023/09/29 19:28:37 - INFO - root -   Epoch: [159/200][0/346], lr: 0.00000033 	 loss = 0.3464(0.3464)
2023/09/29 19:29:43 - INFO - root -   Epoch: [159/200][20/346], lr: 0.00000033 	 loss = 0.4522(0.4664)
2023/09/29 19:30:52 - INFO - root -   Epoch: [159/200][40/346], lr: 0.00000033 	 loss = 0.4241(0.5852)
2023/09/29 19:31:59 - INFO - root -   Epoch: [159/200][60/346], lr: 0.00000033 	 loss = 0.6503(0.5501)
2023/09/29 19:33:01 - INFO - root -   Epoch: [159/200][80/346], lr: 0.00000033 	 loss = 0.5092(0.5147)
2023/09/29 19:34:11 - INFO - root -   Epoch: [159/200][100/346], lr: 0.00000033 	 loss = 1.1139(0.5421)
2023/09/29 19:35:23 - INFO - root -   Epoch: [159/200][120/346], lr: 0.00000033 	 loss = 0.8963(0.5573)
2023/09/29 19:36:31 - INFO - root -   Epoch: [159/200][140/346], lr: 0.00000033 	 loss = 0.3851(0.5662)
2023/09/29 19:37:32 - INFO - root -   Epoch: [159/200][160/346], lr: 0.00000033 	 loss = 0.5910(0.5699)
2023/09/29 19:38:36 - INFO - root -   Epoch: [159/200][180/346], lr: 0.00000033 	 loss = 0.4985(0.5725)
2023/09/29 19:40:03 - INFO - root -   Epoch: [159/200][200/346], lr: 0.00000033 	 loss = 0.6650(0.5862)
2023/09/29 19:40:52 - INFO - root -   Epoch: [159/200][220/346], lr: 0.00000033 	 loss = 0.0981(0.6035)
2023/09/29 19:42:14 - INFO - root -   Epoch: [159/200][240/346], lr: 0.00000033 	 loss = 0.6178(0.5931)
2023/09/29 19:43:03 - INFO - root -   Epoch: [159/200][260/346], lr: 0.00000033 	 loss = 0.3372(0.5879)
2023/09/29 19:44:14 - INFO - root -   Epoch: [159/200][280/346], lr: 0.00000033 	 loss = 1.0897(0.5957)
2023/09/29 19:45:13 - INFO - root -   Epoch: [159/200][300/346], lr: 0.00000033 	 loss = 0.3211(0.5879)
2023/09/29 19:46:31 - INFO - root -   Epoch: [159/200][320/346], lr: 0.00000033 	 loss = 0.7599(0.5926)
2023/09/29 19:47:30 - INFO - root -   Epoch: [159/200][340/346], lr: 0.00000033 	 loss = 0.6284(0.5913)
2023/09/29 19:47:33 - INFO - root -   Epoch: [159/200] 	 loss = 0.5902
2023/09/29 19:51:26 - INFO - root -   precision = 0.7184
2023/09/29 19:51:26 - INFO - root -   eval_loss = 0.5757
2023/09/29 19:51:27 - INFO - root -   train_accuracy = 0.6835
2023/09/29 19:51:49 - INFO - root -   Epoch: [160/200][0/346], lr: 0.00000033 	 loss = 0.5454(0.5454)
2023/09/29 19:52:48 - INFO - root -   Epoch: [160/200][20/346], lr: 0.00000033 	 loss = 0.1554(0.4963)
2023/09/29 19:53:57 - INFO - root -   Epoch: [160/200][40/346], lr: 0.00000033 	 loss = 0.4967(0.5166)
2023/09/29 19:55:15 - INFO - root -   Epoch: [160/200][60/346], lr: 0.00000033 	 loss = 0.2959(0.5040)
2023/09/29 19:56:11 - INFO - root -   Epoch: [160/200][80/346], lr: 0.00000033 	 loss = 0.6849(0.5076)
2023/09/29 19:57:29 - INFO - root -   Epoch: [160/200][100/346], lr: 0.00000033 	 loss = 0.5052(0.5063)
2023/09/29 19:58:19 - INFO - root -   Epoch: [160/200][120/346], lr: 0.00000033 	 loss = 0.5394(0.5357)
2023/09/29 19:59:37 - INFO - root -   Epoch: [160/200][140/346], lr: 0.00000033 	 loss = 0.8640(0.5386)
2023/09/29 20:00:54 - INFO - root -   Epoch: [160/200][160/346], lr: 0.00000033 	 loss = 0.1552(0.5532)
2023/09/29 20:01:49 - INFO - root -   Epoch: [160/200][180/346], lr: 0.00000033 	 loss = 0.8705(0.5552)
2023/09/29 20:02:56 - INFO - root -   Epoch: [160/200][200/346], lr: 0.00000033 	 loss = 0.2708(0.5644)
2023/09/29 20:04:18 - INFO - root -   Epoch: [160/200][220/346], lr: 0.00000033 	 loss = 0.4116(0.5737)
2023/09/29 20:05:08 - INFO - root -   Epoch: [160/200][240/346], lr: 0.00000033 	 loss = 0.3033(0.5746)
2023/09/29 20:06:23 - INFO - root -   Epoch: [160/200][260/346], lr: 0.00000033 	 loss = 0.7690(0.5747)
2023/09/29 20:07:15 - INFO - root -   Epoch: [160/200][280/346], lr: 0.00000033 	 loss = 0.6442(0.5788)
2023/09/29 20:08:38 - INFO - root -   Epoch: [160/200][300/346], lr: 0.00000033 	 loss = 0.2241(0.5837)
2023/09/29 20:09:38 - INFO - root -   Epoch: [160/200][320/346], lr: 0.00000033 	 loss = 0.4147(0.5842)
2023/09/29 20:10:38 - INFO - root -   Epoch: [160/200][340/346], lr: 0.00000033 	 loss = 0.6661(0.5850)
2023/09/29 20:10:40 - INFO - root -   Epoch: [160/200] 	 loss = 0.5909
2023/09/29 20:10:40 - INFO - root -   train_accuracy = 0.6864
2023/09/29 20:11:02 - INFO - root -   Epoch: [161/200][0/346], lr: 0.00000033 	 loss = 0.3903(0.3903)
2023/09/29 20:12:19 - INFO - root -   Epoch: [161/200][20/346], lr: 0.00000033 	 loss = 0.5407(0.5556)
2023/09/29 20:13:25 - INFO - root -   Epoch: [161/200][40/346], lr: 0.00000033 	 loss = 0.8615(0.5895)
2023/09/29 20:14:25 - INFO - root -   Epoch: [161/200][60/346], lr: 0.00000033 	 loss = 0.9595(0.5625)
2023/09/29 20:15:45 - INFO - root -   Epoch: [161/200][80/346], lr: 0.00000033 	 loss = 0.3264(0.5595)
2023/09/29 20:16:54 - INFO - root -   Epoch: [161/200][100/346], lr: 0.00000033 	 loss = 1.0747(0.5596)
2023/09/29 20:17:55 - INFO - root -   Epoch: [161/200][120/346], lr: 0.00000033 	 loss = 0.2526(0.5837)
2023/09/29 20:19:09 - INFO - root -   Epoch: [161/200][140/346], lr: 0.00000033 	 loss = 0.5167(0.5927)
2023/09/29 20:20:00 - INFO - root -   Epoch: [161/200][160/346], lr: 0.00000033 	 loss = 0.3676(0.5980)
2023/09/29 20:21:24 - INFO - root -   Epoch: [161/200][180/346], lr: 0.00000033 	 loss = 0.2041(0.5954)
2023/09/29 20:22:22 - INFO - root -   Epoch: [161/200][200/346], lr: 0.00000033 	 loss = 0.1653(0.5985)
2023/09/29 20:23:29 - INFO - root -   Epoch: [161/200][220/346], lr: 0.00000033 	 loss = 0.4308(0.6003)
2023/09/29 20:24:31 - INFO - root -   Epoch: [161/200][240/346], lr: 0.00000033 	 loss = 0.5815(0.6021)
2023/09/29 20:25:52 - INFO - root -   Epoch: [161/200][260/346], lr: 0.00000033 	 loss = 0.4826(0.6003)
2023/09/29 20:26:43 - INFO - root -   Epoch: [161/200][280/346], lr: 0.00000033 	 loss = 0.8509(0.6131)
2023/09/29 20:28:08 - INFO - root -   Epoch: [161/200][300/346], lr: 0.00000033 	 loss = 0.2061(0.6099)
2023/09/29 20:29:04 - INFO - root -   Epoch: [161/200][320/346], lr: 0.00000033 	 loss = 0.1984(0.6061)
2023/09/29 20:30:00 - INFO - root -   Epoch: [161/200][340/346], lr: 0.00000033 	 loss = 0.5004(0.6040)
2023/09/29 20:30:04 - INFO - root -   Epoch: [161/200] 	 loss = 0.6019
2023/09/29 20:30:04 - INFO - root -   train_accuracy = 0.6705
2023/09/29 20:30:26 - INFO - root -   Epoch: [162/200][0/346], lr: 0.00000033 	 loss = 0.1702(0.1702)
2023/09/29 20:31:25 - INFO - root -   Epoch: [162/200][20/346], lr: 0.00000033 	 loss = 0.6290(0.4304)
2023/09/29 20:32:49 - INFO - root -   Epoch: [162/200][40/346], lr: 0.00000033 	 loss = 0.8605(0.4849)
2023/09/29 20:33:43 - INFO - root -   Epoch: [162/200][60/346], lr: 0.00000033 	 loss = 0.3083(0.4739)
2023/09/29 20:34:54 - INFO - root -   Epoch: [162/200][80/346], lr: 0.00000033 	 loss = 0.4208(0.5114)
2023/09/29 20:36:03 - INFO - root -   Epoch: [162/200][100/346], lr: 0.00000033 	 loss = 1.4057(0.5332)
2023/09/29 20:37:05 - INFO - root -   Epoch: [162/200][120/346], lr: 0.00000033 	 loss = 0.5466(0.5451)
2023/09/29 20:38:07 - INFO - root -   Epoch: [162/200][140/346], lr: 0.00000033 	 loss = 0.3674(0.5486)
2023/09/29 20:39:30 - INFO - root -   Epoch: [162/200][160/346], lr: 0.00000033 	 loss = 0.5831(0.5621)
2023/09/29 20:40:16 - INFO - root -   Epoch: [162/200][180/346], lr: 0.00000033 	 loss = 0.3083(0.5623)
2023/09/29 20:41:30 - INFO - root -   Epoch: [162/200][200/346], lr: 0.00000033 	 loss = 0.0982(0.5634)
2023/09/29 20:42:20 - INFO - root -   Epoch: [162/200][220/346], lr: 0.00000033 	 loss = 0.1112(0.5770)
2023/09/29 20:43:30 - INFO - root -   Epoch: [162/200][240/346], lr: 0.00000033 	 loss = 0.3419(0.5753)
2023/09/29 20:44:41 - INFO - root -   Epoch: [162/200][260/346], lr: 0.00000033 	 loss = 0.4476(0.5740)
2023/09/29 20:45:51 - INFO - root -   Epoch: [162/200][280/346], lr: 0.00000033 	 loss = 0.6346(0.5824)
2023/09/29 20:46:43 - INFO - root -   Epoch: [162/200][300/346], lr: 0.00000033 	 loss = 0.2642(0.5830)
2023/09/29 20:47:58 - INFO - root -   Epoch: [162/200][320/346], lr: 0.00000033 	 loss = 0.1316(0.5844)
2023/09/29 20:48:42 - INFO - root -   Epoch: [162/200][340/346], lr: 0.00000033 	 loss = 0.8344(0.5871)
2023/09/29 20:48:46 - INFO - root -   Epoch: [162/200] 	 loss = 0.5877
2023/09/29 20:48:46 - INFO - root -   train_accuracy = 0.6821
2023/09/29 20:49:15 - INFO - root -   Epoch: [163/200][0/346], lr: 0.00000033 	 loss = 0.6862(0.6862)
2023/09/29 20:50:24 - INFO - root -   Epoch: [163/200][20/346], lr: 0.00000033 	 loss = 0.3951(0.5758)
2023/09/29 20:51:25 - INFO - root -   Epoch: [163/200][40/346], lr: 0.00000033 	 loss = 0.5662(0.5857)
2023/09/29 20:52:43 - INFO - root -   Epoch: [163/200][60/346], lr: 0.00000033 	 loss = 0.4489(0.5830)
2023/09/29 20:53:42 - INFO - root -   Epoch: [163/200][80/346], lr: 0.00000033 	 loss = 0.4583(0.5653)
2023/09/29 20:54:52 - INFO - root -   Epoch: [163/200][100/346], lr: 0.00000033 	 loss = 0.4381(0.5769)
2023/09/29 20:55:52 - INFO - root -   Epoch: [163/200][120/346], lr: 0.00000033 	 loss = 0.4161(0.6039)
2023/09/29 20:57:03 - INFO - root -   Epoch: [163/200][140/346], lr: 0.00000033 	 loss = 0.7910(0.5951)
2023/09/29 20:58:01 - INFO - root -   Epoch: [163/200][160/346], lr: 0.00000033 	 loss = 0.4994(0.5998)
2023/09/29 20:59:17 - INFO - root -   Epoch: [163/200][180/346], lr: 0.00000033 	 loss = 0.6479(0.6070)
2023/09/29 21:00:17 - INFO - root -   Epoch: [163/200][200/346], lr: 0.00000033 	 loss = 0.1970(0.6078)
2023/09/29 21:01:47 - INFO - root -   Epoch: [163/200][220/346], lr: 0.00000033 	 loss = 0.2580(0.6172)
2023/09/29 21:02:41 - INFO - root -   Epoch: [163/200][240/346], lr: 0.00000033 	 loss = 0.1424(0.6126)
2023/09/29 21:03:45 - INFO - root -   Epoch: [163/200][260/346], lr: 0.00000033 	 loss = 0.8570(0.6117)
2023/09/29 21:05:03 - INFO - root -   Epoch: [163/200][280/346], lr: 0.00000033 	 loss = 0.7378(0.6100)
2023/09/29 21:06:11 - INFO - root -   Epoch: [163/200][300/346], lr: 0.00000033 	 loss = 0.5080(0.6105)
2023/09/29 21:07:08 - INFO - root -   Epoch: [163/200][320/346], lr: 0.00000033 	 loss = 0.2554(0.6059)
2023/09/29 21:08:14 - INFO - root -   Epoch: [163/200][340/346], lr: 0.00000033 	 loss = 0.3467(0.6050)
2023/09/29 21:08:16 - INFO - root -   Epoch: [163/200] 	 loss = 0.6043
2023/09/29 21:08:16 - INFO - root -   train_accuracy = 0.6691
2023/09/29 21:08:46 - INFO - root -   Epoch: [164/200][0/346], lr: 0.00000033 	 loss = 0.4025(0.4025)
2023/09/29 21:09:55 - INFO - root -   Epoch: [164/200][20/346], lr: 0.00000033 	 loss = 0.5626(0.4603)
2023/09/29 21:10:58 - INFO - root -   Epoch: [164/200][40/346], lr: 0.00000033 	 loss = 0.5912(0.5401)
2023/09/29 21:12:13 - INFO - root -   Epoch: [164/200][60/346], lr: 0.00000033 	 loss = 1.2731(0.5252)
2023/09/29 21:13:04 - INFO - root -   Epoch: [164/200][80/346], lr: 0.00000033 	 loss = 0.2349(0.5307)
2023/09/29 21:14:28 - INFO - root -   Epoch: [164/200][100/346], lr: 0.00000033 	 loss = 0.6157(0.5356)
2023/09/29 21:15:31 - INFO - root -   Epoch: [164/200][120/346], lr: 0.00000033 	 loss = 0.6635(0.5563)
2023/09/29 21:16:51 - INFO - root -   Epoch: [164/200][140/346], lr: 0.00000033 	 loss = 0.7793(0.5597)
2023/09/29 21:17:47 - INFO - root -   Epoch: [164/200][160/346], lr: 0.00000033 	 loss = 0.1813(0.5655)
2023/09/29 21:19:00 - INFO - root -   Epoch: [164/200][180/346], lr: 0.00000033 	 loss = 0.4074(0.5673)
2023/09/29 21:19:57 - INFO - root -   Epoch: [164/200][200/346], lr: 0.00000033 	 loss = 0.1460(0.5737)
2023/09/29 21:21:17 - INFO - root -   Epoch: [164/200][220/346], lr: 0.00000033 	 loss = 0.1752(0.5685)
2023/09/29 21:22:14 - INFO - root -   Epoch: [164/200][240/346], lr: 0.00000033 	 loss = 0.1053(0.5724)
2023/09/29 21:23:38 - INFO - root -   Epoch: [164/200][260/346], lr: 0.00000033 	 loss = 0.9109(0.5752)
2023/09/29 21:24:34 - INFO - root -   Epoch: [164/200][280/346], lr: 0.00000033 	 loss = 0.5131(0.5832)
2023/09/29 21:25:35 - INFO - root -   Epoch: [164/200][300/346], lr: 0.00000033 	 loss = 0.3127(0.5831)
2023/09/29 21:26:44 - INFO - root -   Epoch: [164/200][320/346], lr: 0.00000033 	 loss = 0.7200(0.5836)
2023/09/29 21:27:49 - INFO - root -   Epoch: [164/200][340/346], lr: 0.00000033 	 loss = 0.9667(0.5807)
2023/09/29 21:27:53 - INFO - root -   Epoch: [164/200] 	 loss = 0.5821
2023/09/29 21:31:43 - INFO - root -   precision = 0.6724
2023/09/29 21:31:43 - INFO - root -   eval_loss = 0.5964
2023/09/29 21:31:44 - INFO - root -   train_accuracy = 0.6864
2023/09/29 21:32:15 - INFO - root -   Epoch: [165/200][0/346], lr: 0.00000034 	 loss = 0.6056(0.6056)
2023/09/29 21:33:25 - INFO - root -   Epoch: [165/200][20/346], lr: 0.00000034 	 loss = 0.1887(0.4447)
2023/09/29 21:34:32 - INFO - root -   Epoch: [165/200][40/346], lr: 0.00000034 	 loss = 1.2438(0.5070)
2023/09/29 21:35:31 - INFO - root -   Epoch: [165/200][60/346], lr: 0.00000034 	 loss = 0.2360(0.4790)
2023/09/29 21:36:49 - INFO - root -   Epoch: [165/200][80/346], lr: 0.00000034 	 loss = 0.5891(0.5282)
2023/09/29 21:37:38 - INFO - root -   Epoch: [165/200][100/346], lr: 0.00000034 	 loss = 0.4032(0.5265)
2023/09/29 21:39:07 - INFO - root -   Epoch: [165/200][120/346], lr: 0.00000034 	 loss = 1.2711(0.5710)
2023/09/29 21:40:00 - INFO - root -   Epoch: [165/200][140/346], lr: 0.00000034 	 loss = 0.3748(0.5658)
2023/09/29 21:41:01 - INFO - root -   Epoch: [165/200][160/346], lr: 0.00000034 	 loss = 0.0981(0.5698)
2023/09/29 21:42:11 - INFO - root -   Epoch: [165/200][180/346], lr: 0.00000034 	 loss = 0.7250(0.5605)
2023/09/29 21:43:06 - INFO - root -   Epoch: [165/200][200/346], lr: 0.00000034 	 loss = 0.2749(0.5613)
2023/09/29 21:44:06 - INFO - root -   Epoch: [165/200][220/346], lr: 0.00000034 	 loss = 0.3257(0.5685)
2023/09/29 21:45:27 - INFO - root -   Epoch: [165/200][240/346], lr: 0.00000034 	 loss = 0.2892(0.5692)
2023/09/29 21:46:23 - INFO - root -   Epoch: [165/200][260/346], lr: 0.00000034 	 loss = 0.6267(0.5692)
2023/09/29 21:47:31 - INFO - root -   Epoch: [165/200][280/346], lr: 0.00000034 	 loss = 0.2693(0.5810)
2023/09/29 21:48:50 - INFO - root -   Epoch: [165/200][300/346], lr: 0.00000034 	 loss = 0.1495(0.5778)
2023/09/29 21:49:45 - INFO - root -   Epoch: [165/200][320/346], lr: 0.00000034 	 loss = 0.5358(0.5814)
2023/09/29 21:50:41 - INFO - root -   Epoch: [165/200][340/346], lr: 0.00000034 	 loss = 0.7129(0.5831)
2023/09/29 21:50:43 - INFO - root -   Epoch: [165/200] 	 loss = 0.5836
2023/09/29 21:50:43 - INFO - root -   train_accuracy = 0.6749
2023/09/29 21:51:22 - INFO - root -   Epoch: [166/200][0/346], lr: 0.00000034 	 loss = 0.3040(0.3040)
2023/09/29 21:52:20 - INFO - root -   Epoch: [166/200][20/346], lr: 0.00000034 	 loss = 0.1685(0.6202)
2023/09/29 21:53:32 - INFO - root -   Epoch: [166/200][40/346], lr: 0.00000034 	 loss = 0.5282(0.6156)
2023/09/29 21:54:37 - INFO - root -   Epoch: [166/200][60/346], lr: 0.00000034 	 loss = 0.4534(0.5804)
2023/09/29 21:55:36 - INFO - root -   Epoch: [166/200][80/346], lr: 0.00000034 	 loss = 0.5511(0.6023)
2023/09/29 21:56:40 - INFO - root -   Epoch: [166/200][100/346], lr: 0.00000034 	 loss = 0.7933(0.5987)
2023/09/29 21:57:54 - INFO - root -   Epoch: [166/200][120/346], lr: 0.00000034 	 loss = 0.5089(0.6063)
2023/09/29 21:58:50 - INFO - root -   Epoch: [166/200][140/346], lr: 0.00000034 	 loss = 0.4679(0.6129)
2023/09/29 22:00:02 - INFO - root -   Epoch: [166/200][160/346], lr: 0.00000034 	 loss = 0.1387(0.6107)
2023/09/29 22:01:18 - INFO - root -   Epoch: [166/200][180/346], lr: 0.00000034 	 loss = 0.6406(0.6113)
2023/09/29 22:02:18 - INFO - root -   Epoch: [166/200][200/346], lr: 0.00000034 	 loss = 0.1562(0.6012)
2023/09/29 22:03:32 - INFO - root -   Epoch: [166/200][220/346], lr: 0.00000034 	 loss = 0.2334(0.6021)
2023/09/29 22:04:32 - INFO - root -   Epoch: [166/200][240/346], lr: 0.00000034 	 loss = 0.4086(0.5981)
2023/09/29 22:05:46 - INFO - root -   Epoch: [166/200][260/346], lr: 0.00000034 	 loss = 0.7216(0.5939)
2023/09/29 22:06:37 - INFO - root -   Epoch: [166/200][280/346], lr: 0.00000034 	 loss = 0.5715(0.6015)
2023/09/29 22:07:51 - INFO - root -   Epoch: [166/200][300/346], lr: 0.00000034 	 loss = 0.3028(0.5984)
2023/09/29 22:08:53 - INFO - root -   Epoch: [166/200][320/346], lr: 0.00000034 	 loss = 0.1831(0.6011)
2023/09/29 22:09:55 - INFO - root -   Epoch: [166/200][340/346], lr: 0.00000034 	 loss = 0.5654(0.5948)
2023/09/29 22:09:57 - INFO - root -   Epoch: [166/200] 	 loss = 0.5955
2023/09/29 22:09:57 - INFO - root -   train_accuracy = 0.6821
2023/09/29 22:10:20 - INFO - root -   Epoch: [167/200][0/346], lr: 0.00000034 	 loss = 0.5107(0.5107)
2023/09/29 22:11:31 - INFO - root -   Epoch: [167/200][20/346], lr: 0.00000034 	 loss = 0.4635(0.4878)
2023/09/29 22:12:39 - INFO - root -   Epoch: [167/200][40/346], lr: 0.00000034 	 loss = 0.2720(0.4963)
2023/09/29 22:13:58 - INFO - root -   Epoch: [167/200][60/346], lr: 0.00000034 	 loss = 0.5060(0.4991)
2023/09/29 22:15:02 - INFO - root -   Epoch: [167/200][80/346], lr: 0.00000034 	 loss = 0.6509(0.5090)
2023/09/29 22:16:07 - INFO - root -   Epoch: [167/200][100/346], lr: 0.00000034 	 loss = 0.3994(0.5308)
2023/09/29 22:17:16 - INFO - root -   Epoch: [167/200][120/346], lr: 0.00000034 	 loss = 0.3172(0.5517)
2023/09/29 22:18:17 - INFO - root -   Epoch: [167/200][140/346], lr: 0.00000034 	 loss = 0.4086(0.5468)
2023/09/29 22:19:41 - INFO - root -   Epoch: [167/200][160/346], lr: 0.00000034 	 loss = 0.5150(0.5523)
2023/09/29 22:20:25 - INFO - root -   Epoch: [167/200][180/346], lr: 0.00000034 	 loss = 0.2506(0.5483)
2023/09/29 22:21:48 - INFO - root -   Epoch: [167/200][200/346], lr: 0.00000034 	 loss = 0.2628(0.5517)
2023/09/29 22:22:55 - INFO - root -   Epoch: [167/200][220/346], lr: 0.00000034 	 loss = 0.2744(0.5670)
2023/09/29 22:24:12 - INFO - root -   Epoch: [167/200][240/346], lr: 0.00000034 	 loss = 0.5037(0.5667)
2023/09/29 22:25:13 - INFO - root -   Epoch: [167/200][260/346], lr: 0.00000034 	 loss = 0.4424(0.5684)
2023/09/29 22:26:14 - INFO - root -   Epoch: [167/200][280/346], lr: 0.00000034 	 loss = 0.4207(0.5754)
2023/09/29 22:27:23 - INFO - root -   Epoch: [167/200][300/346], lr: 0.00000034 	 loss = 0.3897(0.5780)
2023/09/29 22:28:40 - INFO - root -   Epoch: [167/200][320/346], lr: 0.00000034 	 loss = 0.6682(0.5822)
2023/09/29 22:29:18 - INFO - root -   Epoch: [167/200][340/346], lr: 0.00000034 	 loss = 0.9914(0.5845)
2023/09/29 22:29:25 - INFO - root -   Epoch: [167/200] 	 loss = 0.5845
2023/09/29 22:29:25 - INFO - root -   train_accuracy = 0.6821
2023/09/29 22:29:47 - INFO - root -   Epoch: [168/200][0/346], lr: 0.00000034 	 loss = 0.4680(0.4680)
2023/09/29 22:30:57 - INFO - root -   Epoch: [168/200][20/346], lr: 0.00000034 	 loss = 1.0222(0.4987)
2023/09/29 22:32:20 - INFO - root -   Epoch: [168/200][40/346], lr: 0.00000034 	 loss = 0.8552(0.5897)
2023/09/29 22:33:12 - INFO - root -   Epoch: [168/200][60/346], lr: 0.00000034 	 loss = 0.5925(0.5413)
2023/09/29 22:34:17 - INFO - root -   Epoch: [168/200][80/346], lr: 0.00000034 	 loss = 0.4455(0.5446)
2023/09/29 22:35:52 - INFO - root -   Epoch: [168/200][100/346], lr: 0.00000034 	 loss = 0.9418(0.5503)
2023/09/29 22:36:33 - INFO - root -   Epoch: [168/200][120/346], lr: 0.00000034 	 loss = 0.4129(0.5698)
2023/09/29 22:38:02 - INFO - root -   Epoch: [168/200][140/346], lr: 0.00000034 	 loss = 0.5493(0.5638)
2023/09/29 22:38:53 - INFO - root -   Epoch: [168/200][160/346], lr: 0.00000034 	 loss = 0.3905(0.5673)
2023/09/29 22:40:17 - INFO - root -   Epoch: [168/200][180/346], lr: 0.00000034 	 loss = 0.8448(0.5697)
2023/09/29 22:41:04 - INFO - root -   Epoch: [168/200][200/346], lr: 0.00000034 	 loss = 0.1262(0.5647)
2023/09/29 22:42:17 - INFO - root -   Epoch: [168/200][220/346], lr: 0.00000034 	 loss = 0.3332(0.5849)
2023/09/29 22:43:21 - INFO - root -   Epoch: [168/200][240/346], lr: 0.00000034 	 loss = 0.5875(0.5837)
2023/09/29 22:44:33 - INFO - root -   Epoch: [168/200][260/346], lr: 0.00000034 	 loss = 0.6564(0.5849)
2023/09/29 22:45:32 - INFO - root -   Epoch: [168/200][280/346], lr: 0.00000034 	 loss = 0.5647(0.5977)
2023/09/29 22:46:37 - INFO - root -   Epoch: [168/200][300/346], lr: 0.00000034 	 loss = 0.8889(0.5968)
2023/09/29 22:47:46 - INFO - root -   Epoch: [168/200][320/346], lr: 0.00000034 	 loss = 0.2824(0.6036)
2023/09/29 22:48:46 - INFO - root -   Epoch: [168/200][340/346], lr: 0.00000034 	 loss = 0.5675(0.5993)
2023/09/29 22:48:48 - INFO - root -   Epoch: [168/200] 	 loss = 0.5995
2023/09/29 22:48:48 - INFO - root -   train_accuracy = 0.6792
2023/09/29 22:49:18 - INFO - root -   Epoch: [169/200][0/346], lr: 0.00000034 	 loss = 0.3721(0.3721)
2023/09/29 22:50:22 - INFO - root -   Epoch: [169/200][20/346], lr: 0.00000034 	 loss = 0.2504(0.4922)
2023/09/29 22:51:23 - INFO - root -   Epoch: [169/200][40/346], lr: 0.00000034 	 loss = 0.3127(0.4956)
2023/09/29 22:52:39 - INFO - root -   Epoch: [169/200][60/346], lr: 0.00000034 	 loss = 0.5736(0.4709)
2023/09/29 22:53:43 - INFO - root -   Epoch: [169/200][80/346], lr: 0.00000034 	 loss = 0.4133(0.5006)
2023/09/29 22:54:43 - INFO - root -   Epoch: [169/200][100/346], lr: 0.00000034 	 loss = 0.4593(0.5262)
2023/09/29 22:55:50 - INFO - root -   Epoch: [169/200][120/346], lr: 0.00000034 	 loss = 0.4009(0.5462)
2023/09/29 22:56:58 - INFO - root -   Epoch: [169/200][140/346], lr: 0.00000034 	 loss = 0.2658(0.5410)
2023/09/29 22:58:05 - INFO - root -   Epoch: [169/200][160/346], lr: 0.00000034 	 loss = 0.2059(0.5573)
2023/09/29 22:59:22 - INFO - root -   Epoch: [169/200][180/346], lr: 0.00000034 	 loss = 0.9385(0.5577)
2023/09/29 23:00:18 - INFO - root -   Epoch: [169/200][200/346], lr: 0.00000034 	 loss = 0.5288(0.5512)
2023/09/29 23:01:15 - INFO - root -   Epoch: [169/200][220/346], lr: 0.00000034 	 loss = 0.1842(0.5662)
2023/09/29 23:02:24 - INFO - root -   Epoch: [169/200][240/346], lr: 0.00000034 	 loss = 0.4483(0.5607)
2023/09/29 23:03:34 - INFO - root -   Epoch: [169/200][260/346], lr: 0.00000034 	 loss = 0.8596(0.5694)
2023/09/29 23:04:38 - INFO - root -   Epoch: [169/200][280/346], lr: 0.00000034 	 loss = 1.1326(0.5848)
2023/09/29 23:05:49 - INFO - root -   Epoch: [169/200][300/346], lr: 0.00000034 	 loss = 0.2089(0.5869)
2023/09/29 23:06:49 - INFO - root -   Epoch: [169/200][320/346], lr: 0.00000034 	 loss = 0.5986(0.5888)
2023/09/29 23:07:45 - INFO - root -   Epoch: [169/200][340/346], lr: 0.00000034 	 loss = 1.0223(0.5867)
2023/09/29 23:07:48 - INFO - root -   Epoch: [169/200] 	 loss = 0.5888
2023/09/29 23:11:39 - INFO - root -   precision = 0.6897
2023/09/29 23:11:39 - INFO - root -   eval_loss = 0.5864
2023/09/29 23:11:40 - INFO - root -   train_accuracy = 0.6763
2023/09/29 23:12:18 - INFO - root -   Epoch: [170/200][0/346], lr: 0.00000034 	 loss = 0.4991(0.4991)
2023/09/29 23:13:09 - INFO - root -   Epoch: [170/200][20/346], lr: 0.00000034 	 loss = 0.8659(0.5704)
2023/09/29 23:14:17 - INFO - root -   Epoch: [170/200][40/346], lr: 0.00000034 	 loss = 0.6429(0.5785)
2023/09/29 23:15:45 - INFO - root -   Epoch: [170/200][60/346], lr: 0.00000034 	 loss = 0.3484(0.5191)
2023/09/29 23:16:38 - INFO - root -   Epoch: [170/200][80/346], lr: 0.00000034 	 loss = 0.3589(0.5243)
2023/09/29 23:17:50 - INFO - root -   Epoch: [170/200][100/346], lr: 0.00000034 	 loss = 1.0984(0.5322)
2023/09/29 23:18:48 - INFO - root -   Epoch: [170/200][120/346], lr: 0.00000034 	 loss = 0.1158(0.5630)
2023/09/29 23:19:48 - INFO - root -   Epoch: [170/200][140/346], lr: 0.00000034 	 loss = 0.3473(0.5627)
2023/09/29 23:21:01 - INFO - root -   Epoch: [170/200][160/346], lr: 0.00000034 	 loss = 0.2975(0.5719)
2023/09/29 23:21:58 - INFO - root -   Epoch: [170/200][180/346], lr: 0.00000034 	 loss = 0.8244(0.5754)
2023/09/29 23:23:05 - INFO - root -   Epoch: [170/200][200/346], lr: 0.00000034 	 loss = 0.1376(0.5772)
2023/09/29 23:24:03 - INFO - root -   Epoch: [170/200][220/346], lr: 0.00000034 	 loss = 0.6835(0.5919)
2023/09/29 23:25:18 - INFO - root -   Epoch: [170/200][240/346], lr: 0.00000034 	 loss = 0.5119(0.5920)
2023/09/29 23:26:22 - INFO - root -   Epoch: [170/200][260/346], lr: 0.00000034 	 loss = 0.4374(0.5884)
2023/09/29 23:27:24 - INFO - root -   Epoch: [170/200][280/346], lr: 0.00000034 	 loss = 0.3188(0.5931)
2023/09/29 23:28:41 - INFO - root -   Epoch: [170/200][300/346], lr: 0.00000034 	 loss = 0.6779(0.5955)
2023/09/29 23:29:39 - INFO - root -   Epoch: [170/200][320/346], lr: 0.00000034 	 loss = 0.3096(0.5974)
2023/09/29 23:30:37 - INFO - root -   Epoch: [170/200][340/346], lr: 0.00000034 	 loss = 0.7971(0.5989)
2023/09/29 23:30:43 - INFO - root -   Epoch: [170/200] 	 loss = 0.5986
2023/09/29 23:30:44 - INFO - root -   train_accuracy = 0.6691
2023/09/29 23:31:13 - INFO - root -   Epoch: [171/200][0/346], lr: 0.00000034 	 loss = 0.9545(0.9545)
2023/09/29 23:32:23 - INFO - root -   Epoch: [171/200][20/346], lr: 0.00000034 	 loss = 0.0576(0.5278)
2023/09/29 23:33:35 - INFO - root -   Epoch: [171/200][40/346], lr: 0.00000034 	 loss = 0.8301(0.5722)
2023/09/29 23:34:32 - INFO - root -   Epoch: [171/200][60/346], lr: 0.00000034 	 loss = 0.3473(0.5338)
2023/09/29 23:35:53 - INFO - root -   Epoch: [171/200][80/346], lr: 0.00000034 	 loss = 0.6880(0.5421)
2023/09/29 23:36:47 - INFO - root -   Epoch: [171/200][100/346], lr: 0.00000034 	 loss = 0.5668(0.5282)
2023/09/29 23:37:59 - INFO - root -   Epoch: [171/200][120/346], lr: 0.00000034 	 loss = 0.2590(0.5589)
2023/09/29 23:39:21 - INFO - root -   Epoch: [171/200][140/346], lr: 0.00000034 	 loss = 0.8520(0.5575)
2023/09/29 23:40:11 - INFO - root -   Epoch: [171/200][160/346], lr: 0.00000034 	 loss = 0.1741(0.5636)
2023/09/29 23:41:33 - INFO - root -   Epoch: [171/200][180/346], lr: 0.00000034 	 loss = 1.0051(0.5707)
2023/09/29 23:42:24 - INFO - root -   Epoch: [171/200][200/346], lr: 0.00000034 	 loss = 0.1795(0.5718)
2023/09/29 23:43:35 - INFO - root -   Epoch: [171/200][220/346], lr: 0.00000034 	 loss = 0.2779(0.5857)
2023/09/29 23:44:42 - INFO - root -   Epoch: [171/200][240/346], lr: 0.00000034 	 loss = 0.5216(0.5775)
2023/09/29 23:45:50 - INFO - root -   Epoch: [171/200][260/346], lr: 0.00000034 	 loss = 0.6268(0.5747)
2023/09/29 23:46:46 - INFO - root -   Epoch: [171/200][280/346], lr: 0.00000034 	 loss = 0.4191(0.5757)
2023/09/29 23:48:07 - INFO - root -   Epoch: [171/200][300/346], lr: 0.00000034 	 loss = 0.4906(0.5843)
2023/09/29 23:49:04 - INFO - root -   Epoch: [171/200][320/346], lr: 0.00000034 	 loss = 0.4469(0.5858)
2023/09/29 23:50:04 - INFO - root -   Epoch: [171/200][340/346], lr: 0.00000034 	 loss = 0.7993(0.5821)
2023/09/29 23:50:08 - INFO - root -   Epoch: [171/200] 	 loss = 0.5821
2023/09/29 23:50:08 - INFO - root -   train_accuracy = 0.6908
2023/09/29 23:50:38 - INFO - root -   Epoch: [172/200][0/346], lr: 0.00000035 	 loss = 0.8665(0.8665)
2023/09/29 23:51:45 - INFO - root -   Epoch: [172/200][20/346], lr: 0.00000035 	 loss = 0.4214(0.5152)
2023/09/29 23:52:51 - INFO - root -   Epoch: [172/200][40/346], lr: 0.00000035 	 loss = 0.7553(0.5300)
2023/09/29 23:53:58 - INFO - root -   Epoch: [172/200][60/346], lr: 0.00000035 	 loss = 0.3529(0.5095)
2023/09/29 23:54:58 - INFO - root -   Epoch: [172/200][80/346], lr: 0.00000035 	 loss = 0.7261(0.5369)
2023/09/29 23:56:22 - INFO - root -   Epoch: [172/200][100/346], lr: 0.00000035 	 loss = 0.3154(0.5565)
2023/09/29 23:57:13 - INFO - root -   Epoch: [172/200][120/346], lr: 0.00000035 	 loss = 0.2567(0.5622)
2023/09/29 23:58:34 - INFO - root -   Epoch: [172/200][140/346], lr: 0.00000035 	 loss = 0.4238(0.5510)
2023/09/29 23:59:29 - INFO - root -   Epoch: [172/200][160/346], lr: 0.00000035 	 loss = 0.4868(0.5521)
2023/09/30 00:00:45 - INFO - root -   Epoch: [172/200][180/346], lr: 0.00000035 	 loss = 0.3037(0.5586)
2023/09/30 00:01:51 - INFO - root -   Epoch: [172/200][200/346], lr: 0.00000035 	 loss = 0.4079(0.5756)
2023/09/30 00:03:05 - INFO - root -   Epoch: [172/200][220/346], lr: 0.00000035 	 loss = 0.4021(0.5764)
2023/09/30 00:04:17 - INFO - root -   Epoch: [172/200][240/346], lr: 0.00000035 	 loss = 0.4309(0.5755)
2023/09/30 00:05:13 - INFO - root -   Epoch: [172/200][260/346], lr: 0.00000035 	 loss = 0.6187(0.5727)
2023/09/30 00:06:21 - INFO - root -   Epoch: [172/200][280/346], lr: 0.00000035 	 loss = 0.4638(0.5710)
2023/09/30 00:07:24 - INFO - root -   Epoch: [172/200][300/346], lr: 0.00000035 	 loss = 0.1499(0.5658)
2023/09/30 00:08:27 - INFO - root -   Epoch: [172/200][320/346], lr: 0.00000035 	 loss = 1.0345(0.5710)
2023/09/30 00:09:20 - INFO - root -   Epoch: [172/200][340/346], lr: 0.00000035 	 loss = 0.3926(0.5698)
2023/09/30 00:09:22 - INFO - root -   Epoch: [172/200] 	 loss = 0.5691
2023/09/30 00:09:22 - INFO - root -   train_accuracy = 0.7168
2023/09/30 00:09:52 - INFO - root -   Epoch: [173/200][0/346], lr: 0.00000035 	 loss = 0.4096(0.4096)
2023/09/30 00:10:51 - INFO - root -   Epoch: [173/200][20/346], lr: 0.00000035 	 loss = 0.6551(0.5446)
2023/09/30 00:11:52 - INFO - root -   Epoch: [173/200][40/346], lr: 0.00000035 	 loss = 0.5983(0.5373)
2023/09/30 00:13:02 - INFO - root -   Epoch: [173/200][60/346], lr: 0.00000035 	 loss = 0.4279(0.5067)
2023/09/30 00:14:15 - INFO - root -   Epoch: [173/200][80/346], lr: 0.00000035 	 loss = 0.1844(0.5089)
2023/09/30 00:15:37 - INFO - root -   Epoch: [173/200][100/346], lr: 0.00000035 	 loss = 0.7535(0.5293)
2023/09/30 00:16:35 - INFO - root -   Epoch: [173/200][120/346], lr: 0.00000035 	 loss = 0.7880(0.5615)
2023/09/30 00:17:45 - INFO - root -   Epoch: [173/200][140/346], lr: 0.00000035 	 loss = 0.6778(0.5467)
2023/09/30 00:18:45 - INFO - root -   Epoch: [173/200][160/346], lr: 0.00000035 	 loss = 0.0737(0.5530)
2023/09/30 00:20:00 - INFO - root -   Epoch: [173/200][180/346], lr: 0.00000035 	 loss = 0.2189(0.5571)
2023/09/30 00:20:58 - INFO - root -   Epoch: [173/200][200/346], lr: 0.00000035 	 loss = 0.6011(0.5591)
2023/09/30 00:22:18 - INFO - root -   Epoch: [173/200][220/346], lr: 0.00000035 	 loss = 0.2733(0.5701)
2023/09/30 00:23:01 - INFO - root -   Epoch: [173/200][240/346], lr: 0.00000035 	 loss = 0.6169(0.5692)
2023/09/30 00:24:14 - INFO - root -   Epoch: [173/200][260/346], lr: 0.00000035 	 loss = 0.6934(0.5713)
2023/09/30 00:25:13 - INFO - root -   Epoch: [173/200][280/346], lr: 0.00000035 	 loss = 0.2395(0.5708)
2023/09/30 00:26:10 - INFO - root -   Epoch: [173/200][300/346], lr: 0.00000035 	 loss = 0.2221(0.5702)
2023/09/30 00:27:11 - INFO - root -   Epoch: [173/200][320/346], lr: 0.00000035 	 loss = 0.3531(0.5715)
2023/09/30 00:28:18 - INFO - root -   Epoch: [173/200][340/346], lr: 0.00000035 	 loss = 0.8422(0.5678)
2023/09/30 00:28:26 - INFO - root -   Epoch: [173/200] 	 loss = 0.5679
2023/09/30 00:28:26 - INFO - root -   train_accuracy = 0.7095
2023/09/30 00:28:56 - INFO - root -   Epoch: [174/200][0/346], lr: 0.00000035 	 loss = 0.8070(0.8070)
2023/09/30 00:30:03 - INFO - root -   Epoch: [174/200][20/346], lr: 0.00000035 	 loss = 0.4238(0.6496)
2023/09/30 00:31:14 - INFO - root -   Epoch: [174/200][40/346], lr: 0.00000035 	 loss = 0.5386(0.6091)
2023/09/30 00:32:15 - INFO - root -   Epoch: [174/200][60/346], lr: 0.00000035 	 loss = 0.2715(0.5566)
2023/09/30 00:33:17 - INFO - root -   Epoch: [174/200][80/346], lr: 0.00000035 	 loss = 0.1302(0.5459)
2023/09/30 00:34:46 - INFO - root -   Epoch: [174/200][100/346], lr: 0.00000035 	 loss = 0.4341(0.5580)
2023/09/30 00:35:39 - INFO - root -   Epoch: [174/200][120/346], lr: 0.00000035 	 loss = 0.1812(0.5779)
2023/09/30 00:36:48 - INFO - root -   Epoch: [174/200][140/346], lr: 0.00000035 	 loss = 0.3314(0.5687)
2023/09/30 00:38:06 - INFO - root -   Epoch: [174/200][160/346], lr: 0.00000035 	 loss = 0.1000(0.5687)
2023/09/30 00:39:01 - INFO - root -   Epoch: [174/200][180/346], lr: 0.00000035 	 loss = 0.5766(0.5694)
2023/09/30 00:40:09 - INFO - root -   Epoch: [174/200][200/346], lr: 0.00000035 	 loss = 0.2764(0.5657)
2023/09/30 00:41:12 - INFO - root -   Epoch: [174/200][220/346], lr: 0.00000035 	 loss = 0.5503(0.5704)
2023/09/30 00:42:30 - INFO - root -   Epoch: [174/200][240/346], lr: 0.00000035 	 loss = 0.2499(0.5692)
2023/09/30 00:43:14 - INFO - root -   Epoch: [174/200][260/346], lr: 0.00000035 	 loss = 0.4561(0.5667)
2023/09/30 00:44:22 - INFO - root -   Epoch: [174/200][280/346], lr: 0.00000035 	 loss = 0.3340(0.5695)
2023/09/30 00:45:24 - INFO - root -   Epoch: [174/200][300/346], lr: 0.00000035 	 loss = 0.6684(0.5678)
2023/09/30 00:46:49 - INFO - root -   Epoch: [174/200][320/346], lr: 0.00000035 	 loss = 0.4784(0.5739)
2023/09/30 00:47:31 - INFO - root -   Epoch: [174/200][340/346], lr: 0.00000035 	 loss = 0.8391(0.5712)
2023/09/30 00:47:36 - INFO - root -   Epoch: [174/200] 	 loss = 0.5736
2023/09/30 00:51:29 - INFO - root -   precision = 0.6897
2023/09/30 00:51:29 - INFO - root -   eval_loss = 0.5832
2023/09/30 00:51:30 - INFO - root -   train_accuracy = 0.6965
2023/09/30 00:51:51 - INFO - root -   Epoch: [175/200][0/346], lr: 0.00000035 	 loss = 0.2905(0.2905)
2023/09/30 00:53:14 - INFO - root -   Epoch: [175/200][20/346], lr: 0.00000035 	 loss = 0.2107(0.6863)
2023/09/30 00:54:18 - INFO - root -   Epoch: [175/200][40/346], lr: 0.00000035 	 loss = 1.1432(0.6395)
2023/09/30 00:55:32 - INFO - root -   Epoch: [175/200][60/346], lr: 0.00000035 	 loss = 0.6188(0.5848)
2023/09/30 00:56:25 - INFO - root -   Epoch: [175/200][80/346], lr: 0.00000035 	 loss = 0.2979(0.5675)
2023/09/30 00:57:42 - INFO - root -   Epoch: [175/200][100/346], lr: 0.00000035 	 loss = 0.8442(0.6147)
2023/09/30 00:58:38 - INFO - root -   Epoch: [175/200][120/346], lr: 0.00000035 	 loss = 0.6676(0.6163)
2023/09/30 01:00:08 - INFO - root -   Epoch: [175/200][140/346], lr: 0.00000035 	 loss = 0.8296(0.6131)
2023/09/30 01:01:06 - INFO - root -   Epoch: [175/200][160/346], lr: 0.00000035 	 loss = 0.2928(0.5948)
2023/09/30 01:02:22 - INFO - root -   Epoch: [175/200][180/346], lr: 0.00000035 	 loss = 0.5087(0.5898)
2023/09/30 01:03:21 - INFO - root -   Epoch: [175/200][200/346], lr: 0.00000035 	 loss = 0.4857(0.5873)
2023/09/30 01:04:46 - INFO - root -   Epoch: [175/200][220/346], lr: 0.00000035 	 loss = 0.2937(0.5941)
2023/09/30 01:05:48 - INFO - root -   Epoch: [175/200][240/346], lr: 0.00000035 	 loss = 0.1562(0.5876)
2023/09/30 01:06:50 - INFO - root -   Epoch: [175/200][260/346], lr: 0.00000035 	 loss = 0.6569(0.5804)
2023/09/30 01:07:45 - INFO - root -   Epoch: [175/200][280/346], lr: 0.00000035 	 loss = 1.1342(0.5960)
2023/09/30 01:08:48 - INFO - root -   Epoch: [175/200][300/346], lr: 0.00000035 	 loss = 0.2080(0.5945)
2023/09/30 01:09:55 - INFO - root -   Epoch: [175/200][320/346], lr: 0.00000035 	 loss = 0.4308(0.5839)
2023/09/30 01:11:01 - INFO - root -   Epoch: [175/200][340/346], lr: 0.00000035 	 loss = 0.8407(0.5834)
2023/09/30 01:11:03 - INFO - root -   Epoch: [175/200] 	 loss = 0.5845
2023/09/30 01:11:03 - INFO - root -   train_accuracy = 0.6893
2023/09/30 01:11:41 - INFO - root -   Epoch: [176/200][0/346], lr: 0.00000035 	 loss = 0.2742(0.2742)
2023/09/30 01:12:45 - INFO - root -   Epoch: [176/200][20/346], lr: 0.00000035 	 loss = 0.5872(0.5242)
2023/09/30 01:14:04 - INFO - root -   Epoch: [176/200][40/346], lr: 0.00000035 	 loss = 0.2851(0.5458)
2023/09/30 01:15:01 - INFO - root -   Epoch: [176/200][60/346], lr: 0.00000035 	 loss = 0.2026(0.4948)
2023/09/30 01:16:08 - INFO - root -   Epoch: [176/200][80/346], lr: 0.00000035 	 loss = 0.1352(0.5398)
2023/09/30 01:17:01 - INFO - root -   Epoch: [176/200][100/346], lr: 0.00000035 	 loss = 0.2298(0.5635)
2023/09/30 01:18:11 - INFO - root -   Epoch: [176/200][120/346], lr: 0.00000035 	 loss = 0.5943(0.5743)
2023/09/30 01:19:06 - INFO - root -   Epoch: [176/200][140/346], lr: 0.00000035 	 loss = 0.9586(0.5729)
2023/09/30 01:20:14 - INFO - root -   Epoch: [176/200][160/346], lr: 0.00000035 	 loss = 0.2710(0.5803)
2023/09/30 01:21:21 - INFO - root -   Epoch: [176/200][180/346], lr: 0.00000035 	 loss = 0.2653(0.5806)
2023/09/30 01:22:27 - INFO - root -   Epoch: [176/200][200/346], lr: 0.00000035 	 loss = 0.4786(0.5744)
2023/09/30 01:23:51 - INFO - root -   Epoch: [176/200][220/346], lr: 0.00000035 	 loss = 0.2230(0.5822)
2023/09/30 01:24:48 - INFO - root -   Epoch: [176/200][240/346], lr: 0.00000035 	 loss = 0.4101(0.5719)
2023/09/30 01:25:54 - INFO - root -   Epoch: [176/200][260/346], lr: 0.00000035 	 loss = 0.5373(0.5671)
2023/09/30 01:26:59 - INFO - root -   Epoch: [176/200][280/346], lr: 0.00000035 	 loss = 0.4792(0.5749)
2023/09/30 01:28:17 - INFO - root -   Epoch: [176/200][300/346], lr: 0.00000035 	 loss = 0.6283(0.5794)
2023/09/30 01:29:16 - INFO - root -   Epoch: [176/200][320/346], lr: 0.00000035 	 loss = 0.3763(0.5739)
2023/09/30 01:30:27 - INFO - root -   Epoch: [176/200][340/346], lr: 0.00000035 	 loss = 0.5924(0.5744)
2023/09/30 01:30:29 - INFO - root -   Epoch: [176/200] 	 loss = 0.5745
2023/09/30 01:30:29 - INFO - root -   train_accuracy = 0.6922
2023/09/30 01:31:17 - INFO - root -   Epoch: [177/200][0/346], lr: 0.00000035 	 loss = 0.8222(0.8222)
2023/09/30 01:32:09 - INFO - root -   Epoch: [177/200][20/346], lr: 0.00000035 	 loss = 0.2404(0.4688)
2023/09/30 01:33:36 - INFO - root -   Epoch: [177/200][40/346], lr: 0.00000035 	 loss = 0.5028(0.4949)
2023/09/30 01:34:29 - INFO - root -   Epoch: [177/200][60/346], lr: 0.00000035 	 loss = 0.3564(0.4840)
2023/09/30 01:35:59 - INFO - root -   Epoch: [177/200][80/346], lr: 0.00000035 	 loss = 0.3954(0.5122)
2023/09/30 01:36:41 - INFO - root -   Epoch: [177/200][100/346], lr: 0.00000035 	 loss = 0.8670(0.5134)
2023/09/30 01:37:55 - INFO - root -   Epoch: [177/200][120/346], lr: 0.00000035 	 loss = 0.3187(0.5265)
2023/09/30 01:38:52 - INFO - root -   Epoch: [177/200][140/346], lr: 0.00000035 	 loss = 0.7044(0.5187)
2023/09/30 01:40:09 - INFO - root -   Epoch: [177/200][160/346], lr: 0.00000035 	 loss = 0.0771(0.5263)
2023/09/30 01:41:07 - INFO - root -   Epoch: [177/200][180/346], lr: 0.00000035 	 loss = 0.6236(0.5343)
2023/09/30 01:42:31 - INFO - root -   Epoch: [177/200][200/346], lr: 0.00000035 	 loss = 0.1618(0.5317)
2023/09/30 01:43:28 - INFO - root -   Epoch: [177/200][220/346], lr: 0.00000035 	 loss = 0.2339(0.5362)
2023/09/30 01:44:49 - INFO - root -   Epoch: [177/200][240/346], lr: 0.00000035 	 loss = 0.1896(0.5381)
2023/09/30 01:45:45 - INFO - root -   Epoch: [177/200][260/346], lr: 0.00000035 	 loss = 1.8874(0.5417)
2023/09/30 01:47:10 - INFO - root -   Epoch: [177/200][280/346], lr: 0.00000035 	 loss = 0.2921(0.5521)
2023/09/30 01:48:14 - INFO - root -   Epoch: [177/200][300/346], lr: 0.00000035 	 loss = 0.5450(0.5557)
2023/09/30 01:49:20 - INFO - root -   Epoch: [177/200][320/346], lr: 0.00000035 	 loss = 0.5550(0.5581)
2023/09/30 01:50:15 - INFO - root -   Epoch: [177/200][340/346], lr: 0.00000035 	 loss = 1.2343(0.5595)
2023/09/30 01:50:18 - INFO - root -   Epoch: [177/200] 	 loss = 0.5581
2023/09/30 01:50:18 - INFO - root -   train_accuracy = 0.7023
2023/09/30 01:50:56 - INFO - root -   Epoch: [178/200][0/346], lr: 0.00000035 	 loss = 0.1711(0.1711)
2023/09/30 01:51:55 - INFO - root -   Epoch: [178/200][20/346], lr: 0.00000035 	 loss = 0.8776(0.5691)
2023/09/30 01:53:01 - INFO - root -   Epoch: [178/200][40/346], lr: 0.00000035 	 loss = 0.5061(0.5110)
2023/09/30 01:54:32 - INFO - root -   Epoch: [178/200][60/346], lr: 0.00000035 	 loss = 0.2574(0.5158)
2023/09/30 01:55:22 - INFO - root -   Epoch: [178/200][80/346], lr: 0.00000035 	 loss = 0.1941(0.5173)
2023/09/30 01:56:42 - INFO - root -   Epoch: [178/200][100/346], lr: 0.00000035 	 loss = 0.3051(0.5081)
2023/09/30 01:57:39 - INFO - root -   Epoch: [178/200][120/346], lr: 0.00000035 	 loss = 0.1420(0.5520)
2023/09/30 01:58:57 - INFO - root -   Epoch: [178/200][140/346], lr: 0.00000035 	 loss = 0.3016(0.5438)
2023/09/30 01:59:48 - INFO - root -   Epoch: [178/200][160/346], lr: 0.00000035 	 loss = 0.1269(0.5442)
2023/09/30 02:00:54 - INFO - root -   Epoch: [178/200][180/346], lr: 0.00000035 	 loss = 0.5794(0.5537)
2023/09/30 02:02:03 - INFO - root -   Epoch: [178/200][200/346], lr: 0.00000035 	 loss = 0.7465(0.5559)
2023/09/30 02:02:54 - INFO - root -   Epoch: [178/200][220/346], lr: 0.00000035 	 loss = 0.2309(0.5615)
2023/09/30 02:04:00 - INFO - root -   Epoch: [178/200][240/346], lr: 0.00000035 	 loss = 0.2693(0.5558)
2023/09/30 02:05:17 - INFO - root -   Epoch: [178/200][260/346], lr: 0.00000035 	 loss = 0.3984(0.5603)
2023/09/30 02:06:28 - INFO - root -   Epoch: [178/200][280/346], lr: 0.00000035 	 loss = 0.6047(0.5670)
2023/09/30 02:07:51 - INFO - root -   Epoch: [178/200][300/346], lr: 0.00000035 	 loss = 0.2822(0.5706)
2023/09/30 02:08:49 - INFO - root -   Epoch: [178/200][320/346], lr: 0.00000035 	 loss = 0.2399(0.5662)
2023/09/30 02:09:51 - INFO - root -   Epoch: [178/200][340/346], lr: 0.00000035 	 loss = 0.7987(0.5693)
2023/09/30 02:09:53 - INFO - root -   Epoch: [178/200] 	 loss = 0.5695
2023/09/30 02:09:53 - INFO - root -   train_accuracy = 0.6850
2023/09/30 02:10:22 - INFO - root -   Epoch: [179/200][0/346], lr: 0.00000036 	 loss = 0.8549(0.8549)
2023/09/30 02:11:27 - INFO - root -   Epoch: [179/200][20/346], lr: 0.00000036 	 loss = 0.1027(0.5419)
2023/09/30 02:12:36 - INFO - root -   Epoch: [179/200][40/346], lr: 0.00000036 	 loss = 1.5377(0.5508)
2023/09/30 02:13:44 - INFO - root -   Epoch: [179/200][60/346], lr: 0.00000036 	 loss = 0.2974(0.5490)
2023/09/30 02:14:56 - INFO - root -   Epoch: [179/200][80/346], lr: 0.00000036 	 loss = 0.7123(0.5658)
2023/09/30 02:16:06 - INFO - root -   Epoch: [179/200][100/346], lr: 0.00000036 	 loss = 0.4892(0.5703)
2023/09/30 02:17:14 - INFO - root -   Epoch: [179/200][120/346], lr: 0.00000036 	 loss = 0.7529(0.5965)
2023/09/30 02:18:27 - INFO - root -   Epoch: [179/200][140/346], lr: 0.00000036 	 loss = 0.2155(0.5904)
2023/09/30 02:19:19 - INFO - root -   Epoch: [179/200][160/346], lr: 0.00000036 	 loss = 0.1858(0.5834)
2023/09/30 02:20:45 - INFO - root -   Epoch: [179/200][180/346], lr: 0.00000036 	 loss = 0.6103(0.5831)
2023/09/30 02:21:26 - INFO - root -   Epoch: [179/200][200/346], lr: 0.00000036 	 loss = 0.2348(0.5785)
2023/09/30 02:22:55 - INFO - root -   Epoch: [179/200][220/346], lr: 0.00000036 	 loss = 0.1433(0.5814)
2023/09/30 02:23:49 - INFO - root -   Epoch: [179/200][240/346], lr: 0.00000036 	 loss = 0.3234(0.5814)
2023/09/30 02:24:58 - INFO - root -   Epoch: [179/200][260/346], lr: 0.00000036 	 loss = 0.6107(0.5744)
2023/09/30 02:26:24 - INFO - root -   Epoch: [179/200][280/346], lr: 0.00000036 	 loss = 0.4252(0.5825)
2023/09/30 02:27:14 - INFO - root -   Epoch: [179/200][300/346], lr: 0.00000036 	 loss = 0.2780(0.5782)
2023/09/30 02:28:31 - INFO - root -   Epoch: [179/200][320/346], lr: 0.00000036 	 loss = 0.2623(0.5785)
2023/09/30 02:29:20 - INFO - root -   Epoch: [179/200][340/346], lr: 0.00000036 	 loss = 0.3776(0.5763)
2023/09/30 02:29:28 - INFO - root -   Epoch: [179/200] 	 loss = 0.5797
2023/09/30 02:33:20 - INFO - root -   precision = 0.6782
2023/09/30 02:33:20 - INFO - root -   eval_loss = 0.6329
2023/09/30 02:33:21 - INFO - root -   train_accuracy = 0.6879
2023/09/30 02:33:43 - INFO - root -   Epoch: [180/200][0/346], lr: 0.00000036 	 loss = 0.1533(0.1533)
2023/09/30 02:35:07 - INFO - root -   Epoch: [180/200][20/346], lr: 0.00000036 	 loss = 0.2004(0.5721)
2023/09/30 02:36:02 - INFO - root -   Epoch: [180/200][40/346], lr: 0.00000036 	 loss = 0.9265(0.6086)
2023/09/30 02:37:27 - INFO - root -   Epoch: [180/200][60/346], lr: 0.00000036 	 loss = 0.7965(0.5584)
2023/09/30 02:38:17 - INFO - root -   Epoch: [180/200][80/346], lr: 0.00000036 	 loss = 0.6554(0.5480)
2023/09/30 02:39:38 - INFO - root -   Epoch: [180/200][100/346], lr: 0.00000036 	 loss = 1.1524(0.5672)
2023/09/30 02:40:32 - INFO - root -   Epoch: [180/200][120/346], lr: 0.00000036 	 loss = 0.4978(0.5723)
2023/09/30 02:41:41 - INFO - root -   Epoch: [180/200][140/346], lr: 0.00000036 	 loss = 0.5586(0.5775)
2023/09/30 02:42:41 - INFO - root -   Epoch: [180/200][160/346], lr: 0.00000036 	 loss = 0.1421(0.5809)
2023/09/30 02:44:08 - INFO - root -   Epoch: [180/200][180/346], lr: 0.00000036 	 loss = 0.6042(0.5783)
2023/09/30 02:45:00 - INFO - root -   Epoch: [180/200][200/346], lr: 0.00000036 	 loss = 0.3393(0.5783)
2023/09/30 02:46:18 - INFO - root -   Epoch: [180/200][220/346], lr: 0.00000036 	 loss = 0.2218(0.5776)
2023/09/30 02:47:13 - INFO - root -   Epoch: [180/200][240/346], lr: 0.00000036 	 loss = 1.4832(0.5811)
2023/09/30 02:48:32 - INFO - root -   Epoch: [180/200][260/346], lr: 0.00000036 	 loss = 0.9793(0.5817)
2023/09/30 02:49:47 - INFO - root -   Epoch: [180/200][280/346], lr: 0.00000036 	 loss = 0.6572(0.5882)
2023/09/30 02:50:48 - INFO - root -   Epoch: [180/200][300/346], lr: 0.00000036 	 loss = 0.2874(0.5868)
2023/09/30 02:52:11 - INFO - root -   Epoch: [180/200][320/346], lr: 0.00000036 	 loss = 0.2412(0.5832)
2023/09/30 02:52:51 - INFO - root -   Epoch: [180/200][340/346], lr: 0.00000036 	 loss = 0.9890(0.5876)
2023/09/30 02:52:53 - INFO - root -   Epoch: [180/200] 	 loss = 0.5933
2023/09/30 02:52:53 - INFO - root -   train_accuracy = 0.6893
2023/09/30 02:53:15 - INFO - root -   Epoch: [181/200][0/346], lr: 0.00000036 	 loss = 0.1456(0.1456)
2023/09/30 02:54:30 - INFO - root -   Epoch: [181/200][20/346], lr: 0.00000036 	 loss = 0.2808(0.5393)
2023/09/30 02:55:28 - INFO - root -   Epoch: [181/200][40/346], lr: 0.00000036 	 loss = 0.2434(0.5603)
2023/09/30 02:56:39 - INFO - root -   Epoch: [181/200][60/346], lr: 0.00000036 	 loss = 0.6501(0.5322)
2023/09/30 02:57:32 - INFO - root -   Epoch: [181/200][80/346], lr: 0.00000036 	 loss = 0.5688(0.5677)
2023/09/30 02:58:48 - INFO - root -   Epoch: [181/200][100/346], lr: 0.00000036 	 loss = 0.3884(0.5530)
2023/09/30 02:59:39 - INFO - root -   Epoch: [181/200][120/346], lr: 0.00000036 	 loss = 0.3129(0.5760)
2023/09/30 03:00:58 - INFO - root -   Epoch: [181/200][140/346], lr: 0.00000036 	 loss = 0.6163(0.5619)
2023/09/30 03:01:52 - INFO - root -   Epoch: [181/200][160/346], lr: 0.00000036 	 loss = 0.5662(0.5731)
2023/09/30 03:03:10 - INFO - root -   Epoch: [181/200][180/346], lr: 0.00000036 	 loss = 0.1974(0.5685)
2023/09/30 03:04:14 - INFO - root -   Epoch: [181/200][200/346], lr: 0.00000036 	 loss = 0.1861(0.5655)
2023/09/30 03:05:23 - INFO - root -   Epoch: [181/200][220/346], lr: 0.00000036 	 loss = 0.6340(0.5695)
2023/09/30 03:06:17 - INFO - root -   Epoch: [181/200][240/346], lr: 0.00000036 	 loss = 0.5786(0.5680)
2023/09/30 03:07:31 - INFO - root -   Epoch: [181/200][260/346], lr: 0.00000036 	 loss = 0.5268(0.5695)
2023/09/30 03:08:28 - INFO - root -   Epoch: [181/200][280/346], lr: 0.00000036 	 loss = 0.4864(0.5694)
2023/09/30 03:09:46 - INFO - root -   Epoch: [181/200][300/346], lr: 0.00000036 	 loss = 0.0724(0.5732)
2023/09/30 03:10:55 - INFO - root -   Epoch: [181/200][320/346], lr: 0.00000036 	 loss = 0.2711(0.5725)
2023/09/30 03:12:01 - INFO - root -   Epoch: [181/200][340/346], lr: 0.00000036 	 loss = 0.7830(0.5737)
2023/09/30 03:12:03 - INFO - root -   Epoch: [181/200] 	 loss = 0.5735
2023/09/30 03:12:03 - INFO - root -   train_accuracy = 0.6879
2023/09/30 03:12:25 - INFO - root -   Epoch: [182/200][0/346], lr: 0.00000036 	 loss = 0.2103(0.2103)
2023/09/30 03:13:35 - INFO - root -   Epoch: [182/200][20/346], lr: 0.00000036 	 loss = 0.3286(0.4673)
2023/09/30 03:14:46 - INFO - root -   Epoch: [182/200][40/346], lr: 0.00000036 	 loss = 1.0515(0.5563)
2023/09/30 03:15:40 - INFO - root -   Epoch: [182/200][60/346], lr: 0.00000036 	 loss = 0.6162(0.5122)
2023/09/30 03:17:16 - INFO - root -   Epoch: [182/200][80/346], lr: 0.00000036 	 loss = 0.5695(0.5315)
2023/09/30 03:18:12 - INFO - root -   Epoch: [182/200][100/346], lr: 0.00000036 	 loss = 0.6375(0.5620)
2023/09/30 03:19:27 - INFO - root -   Epoch: [182/200][120/346], lr: 0.00000036 	 loss = 0.5247(0.5641)
2023/09/30 03:20:18 - INFO - root -   Epoch: [182/200][140/346], lr: 0.00000036 	 loss = 0.6563(0.5640)
2023/09/30 03:21:39 - INFO - root -   Epoch: [182/200][160/346], lr: 0.00000036 	 loss = 0.1858(0.5607)
2023/09/30 03:22:38 - INFO - root -   Epoch: [182/200][180/346], lr: 0.00000036 	 loss = 0.1343(0.5652)
2023/09/30 03:23:46 - INFO - root -   Epoch: [182/200][200/346], lr: 0.00000036 	 loss = 0.7162(0.5697)
2023/09/30 03:24:38 - INFO - root -   Epoch: [182/200][220/346], lr: 0.00000036 	 loss = 0.5697(0.5733)
2023/09/30 03:25:46 - INFO - root -   Epoch: [182/200][240/346], lr: 0.00000036 	 loss = 0.2767(0.5734)
2023/09/30 03:26:37 - INFO - root -   Epoch: [182/200][260/346], lr: 0.00000036 	 loss = 0.9933(0.5680)
2023/09/30 03:27:59 - INFO - root -   Epoch: [182/200][280/346], lr: 0.00000036 	 loss = 0.8026(0.5679)
2023/09/30 03:28:48 - INFO - root -   Epoch: [182/200][300/346], lr: 0.00000036 	 loss = 0.5792(0.5752)
2023/09/30 03:30:07 - INFO - root -   Epoch: [182/200][320/346], lr: 0.00000036 	 loss = 0.4332(0.5759)
2023/09/30 03:30:46 - INFO - root -   Epoch: [182/200][340/346], lr: 0.00000036 	 loss = 0.2908(0.5680)
2023/09/30 03:30:59 - INFO - root -   Epoch: [182/200] 	 loss = 0.5678
2023/09/30 03:30:59 - INFO - root -   train_accuracy = 0.6965
2023/09/30 03:31:36 - INFO - root -   Epoch: [183/200][0/346], lr: 0.00000036 	 loss = 0.4445(0.4445)
2023/09/30 03:32:35 - INFO - root -   Epoch: [183/200][20/346], lr: 0.00000036 	 loss = 0.0736(0.5244)
2023/09/30 03:33:43 - INFO - root -   Epoch: [183/200][40/346], lr: 0.00000036 	 loss = 0.1209(0.5459)
2023/09/30 03:34:44 - INFO - root -   Epoch: [183/200][60/346], lr: 0.00000036 	 loss = 0.4952(0.5110)
2023/09/30 03:36:03 - INFO - root -   Epoch: [183/200][80/346], lr: 0.00000036 	 loss = 0.4018(0.5117)
2023/09/30 03:37:11 - INFO - root -   Epoch: [183/200][100/346], lr: 0.00000036 	 loss = 0.8017(0.5267)
2023/09/30 03:38:10 - INFO - root -   Epoch: [183/200][120/346], lr: 0.00000036 	 loss = 0.4383(0.5403)
2023/09/30 03:39:03 - INFO - root -   Epoch: [183/200][140/346], lr: 0.00000036 	 loss = 0.3515(0.5332)
2023/09/30 03:40:19 - INFO - root -   Epoch: [183/200][160/346], lr: 0.00000036 	 loss = 0.1890(0.5404)
2023/09/30 03:41:16 - INFO - root -   Epoch: [183/200][180/346], lr: 0.00000036 	 loss = 0.8142(0.5480)
2023/09/30 03:42:32 - INFO - root -   Epoch: [183/200][200/346], lr: 0.00000036 	 loss = 0.3758(0.5479)
2023/09/30 03:43:37 - INFO - root -   Epoch: [183/200][220/346], lr: 0.00000036 	 loss = 0.3843(0.5587)
2023/09/30 03:44:52 - INFO - root -   Epoch: [183/200][240/346], lr: 0.00000036 	 loss = 0.2934(0.5533)
2023/09/30 03:45:35 - INFO - root -   Epoch: [183/200][260/346], lr: 0.00000036 	 loss = 0.6292(0.5529)
2023/09/30 03:47:04 - INFO - root -   Epoch: [183/200][280/346], lr: 0.00000036 	 loss = 0.3598(0.5600)
2023/09/30 03:48:02 - INFO - root -   Epoch: [183/200][300/346], lr: 0.00000036 	 loss = 0.4478(0.5590)
2023/09/30 03:49:15 - INFO - root -   Epoch: [183/200][320/346], lr: 0.00000036 	 loss = 0.2986(0.5551)
2023/09/30 03:50:06 - INFO - root -   Epoch: [183/200][340/346], lr: 0.00000036 	 loss = 0.5641(0.5588)
2023/09/30 03:50:12 - INFO - root -   Epoch: [183/200] 	 loss = 0.5593
2023/09/30 03:50:12 - INFO - root -   train_accuracy = 0.6850
2023/09/30 03:50:35 - INFO - root -   Epoch: [184/200][0/346], lr: 0.00000036 	 loss = 0.3894(0.3894)
2023/09/30 03:51:57 - INFO - root -   Epoch: [184/200][20/346], lr: 0.00000036 	 loss = 0.3771(0.5849)
2023/09/30 03:52:52 - INFO - root -   Epoch: [184/200][40/346], lr: 0.00000036 	 loss = 0.6535(0.5487)
2023/09/30 03:53:57 - INFO - root -   Epoch: [184/200][60/346], lr: 0.00000036 	 loss = 0.5924(0.5156)
2023/09/30 03:55:16 - INFO - root -   Epoch: [184/200][80/346], lr: 0.00000036 	 loss = 0.5041(0.5158)
2023/09/30 03:56:16 - INFO - root -   Epoch: [184/200][100/346], lr: 0.00000036 	 loss = 0.4310(0.5247)
2023/09/30 03:57:47 - INFO - root -   Epoch: [184/200][120/346], lr: 0.00000036 	 loss = 0.3794(0.5464)
2023/09/30 03:58:27 - INFO - root -   Epoch: [184/200][140/346], lr: 0.00000036 	 loss = 0.4527(0.5359)
2023/09/30 03:59:44 - INFO - root -   Epoch: [184/200][160/346], lr: 0.00000036 	 loss = 0.3641(0.5484)
2023/09/30 04:00:57 - INFO - root -   Epoch: [184/200][180/346], lr: 0.00000036 	 loss = 0.6734(0.5454)
2023/09/30 04:02:04 - INFO - root -   Epoch: [184/200][200/346], lr: 0.00000036 	 loss = 0.1462(0.5430)
2023/09/30 04:03:08 - INFO - root -   Epoch: [184/200][220/346], lr: 0.00000036 	 loss = 0.4515(0.5446)
2023/09/30 04:04:17 - INFO - root -   Epoch: [184/200][240/346], lr: 0.00000036 	 loss = 0.4934(0.5503)
2023/09/30 04:05:31 - INFO - root -   Epoch: [184/200][260/346], lr: 0.00000036 	 loss = 1.0456(0.5477)
2023/09/30 04:06:25 - INFO - root -   Epoch: [184/200][280/346], lr: 0.00000036 	 loss = 0.6025(0.5648)
2023/09/30 04:07:41 - INFO - root -   Epoch: [184/200][300/346], lr: 0.00000036 	 loss = 0.2756(0.5601)
2023/09/30 04:08:52 - INFO - root -   Epoch: [184/200][320/346], lr: 0.00000036 	 loss = 0.3348(0.5621)
2023/09/30 04:09:31 - INFO - root -   Epoch: [184/200][340/346], lr: 0.00000036 	 loss = 0.7303(0.5588)
2023/09/30 04:09:38 - INFO - root -   Epoch: [184/200] 	 loss = 0.5629
2023/09/30 04:13:28 - INFO - root -   precision = 0.6724
2023/09/30 04:13:28 - INFO - root -   eval_loss = 0.6152
2023/09/30 04:13:29 - INFO - root -   train_accuracy = 0.7023
2023/09/30 04:13:58 - INFO - root -   Epoch: [185/200][0/346], lr: 0.00000036 	 loss = 0.4483(0.4483)
2023/09/30 04:15:06 - INFO - root -   Epoch: [185/200][20/346], lr: 0.00000036 	 loss = 0.0580(0.5087)
2023/09/30 04:16:14 - INFO - root -   Epoch: [185/200][40/346], lr: 0.00000036 	 loss = 0.8736(0.5339)
2023/09/30 04:17:28 - INFO - root -   Epoch: [185/200][60/346], lr: 0.00000036 	 loss = 0.7378(0.5438)
2023/09/30 04:18:38 - INFO - root -   Epoch: [185/200][80/346], lr: 0.00000036 	 loss = 0.2558(0.5025)
2023/09/30 04:19:34 - INFO - root -   Epoch: [185/200][100/346], lr: 0.00000036 	 loss = 0.9427(0.5125)
2023/09/30 04:20:55 - INFO - root -   Epoch: [185/200][120/346], lr: 0.00000036 	 loss = 0.3543(0.5332)
2023/09/30 04:21:45 - INFO - root -   Epoch: [185/200][140/346], lr: 0.00000036 	 loss = 0.5997(0.5351)
2023/09/30 04:23:19 - INFO - root -   Epoch: [185/200][160/346], lr: 0.00000036 	 loss = 0.2479(0.5300)
2023/09/30 04:24:14 - INFO - root -   Epoch: [185/200][180/346], lr: 0.00000036 	 loss = 0.0670(0.5230)
2023/09/30 04:25:15 - INFO - root -   Epoch: [185/200][200/346], lr: 0.00000036 	 loss = 0.1200(0.5152)
2023/09/30 04:26:14 - INFO - root -   Epoch: [185/200][220/346], lr: 0.00000036 	 loss = 0.3519(0.5295)
2023/09/30 04:27:41 - INFO - root -   Epoch: [185/200][240/346], lr: 0.00000036 	 loss = 0.2397(0.5348)
2023/09/30 04:28:39 - INFO - root -   Epoch: [185/200][260/346], lr: 0.00000036 	 loss = 0.3600(0.5378)
2023/09/30 04:29:43 - INFO - root -   Epoch: [185/200][280/346], lr: 0.00000036 	 loss = 0.5823(0.5454)
2023/09/30 04:30:48 - INFO - root -   Epoch: [185/200][300/346], lr: 0.00000036 	 loss = 0.4260(0.5503)
2023/09/30 04:31:56 - INFO - root -   Epoch: [185/200][320/346], lr: 0.00000036 	 loss = 0.1390(0.5553)
2023/09/30 04:32:52 - INFO - root -   Epoch: [185/200][340/346], lr: 0.00000036 	 loss = 0.8316(0.5576)
2023/09/30 04:32:55 - INFO - root -   Epoch: [185/200] 	 loss = 0.5557
2023/09/30 04:32:55 - INFO - root -   train_accuracy = 0.7038
2023/09/30 04:33:19 - INFO - root -   Epoch: [186/200][0/346], lr: 0.00000037 	 loss = 0.4385(0.4385)
2023/09/30 04:34:26 - INFO - root -   Epoch: [186/200][20/346], lr: 0.00000037 	 loss = 0.8796(0.5037)
2023/09/30 04:35:43 - INFO - root -   Epoch: [186/200][40/346], lr: 0.00000037 	 loss = 0.8275(0.4844)
2023/09/30 04:36:56 - INFO - root -   Epoch: [186/200][60/346], lr: 0.00000037 	 loss = 0.5811(0.4772)
2023/09/30 04:38:00 - INFO - root -   Epoch: [186/200][80/346], lr: 0.00000037 	 loss = 0.1519(0.5095)
2023/09/30 04:39:21 - INFO - root -   Epoch: [186/200][100/346], lr: 0.00000037 	 loss = 1.0833(0.5083)
2023/09/30 04:40:12 - INFO - root -   Epoch: [186/200][120/346], lr: 0.00000037 	 loss = 0.4177(0.5231)
2023/09/30 04:41:28 - INFO - root -   Epoch: [186/200][140/346], lr: 0.00000037 	 loss = 0.5791(0.5227)
2023/09/30 04:42:20 - INFO - root -   Epoch: [186/200][160/346], lr: 0.00000037 	 loss = 0.3512(0.5270)
2023/09/30 04:43:25 - INFO - root -   Epoch: [186/200][180/346], lr: 0.00000037 	 loss = 0.9505(0.5330)
2023/09/30 04:44:26 - INFO - root -   Epoch: [186/200][200/346], lr: 0.00000037 	 loss = 0.1854(0.5321)
2023/09/30 04:45:55 - INFO - root -   Epoch: [186/200][220/346], lr: 0.00000037 	 loss = 0.1920(0.5428)
2023/09/30 04:46:38 - INFO - root -   Epoch: [186/200][240/346], lr: 0.00000037 	 loss = 0.2828(0.5431)
2023/09/30 04:48:04 - INFO - root -   Epoch: [186/200][260/346], lr: 0.00000037 	 loss = 0.5803(0.5451)
2023/09/30 04:49:05 - INFO - root -   Epoch: [186/200][280/346], lr: 0.00000037 	 loss = 0.5683(0.5502)
2023/09/30 04:50:21 - INFO - root -   Epoch: [186/200][300/346], lr: 0.00000037 	 loss = 0.4402(0.5595)
2023/09/30 04:51:33 - INFO - root -   Epoch: [186/200][320/346], lr: 0.00000037 	 loss = 0.1763(0.5652)
2023/09/30 04:52:22 - INFO - root -   Epoch: [186/200][340/346], lr: 0.00000037 	 loss = 0.7105(0.5597)
2023/09/30 04:52:26 - INFO - root -   Epoch: [186/200] 	 loss = 0.5663
2023/09/30 04:52:26 - INFO - root -   train_accuracy = 0.7023
2023/09/30 04:52:48 - INFO - root -   Epoch: [187/200][0/346], lr: 0.00000037 	 loss = 0.2403(0.2403)
2023/09/30 04:53:47 - INFO - root -   Epoch: [187/200][20/346], lr: 0.00000037 	 loss = 0.1588(0.4734)
2023/09/30 04:54:55 - INFO - root -   Epoch: [187/200][40/346], lr: 0.00000037 	 loss = 1.0388(0.5375)
2023/09/30 04:56:22 - INFO - root -   Epoch: [187/200][60/346], lr: 0.00000037 	 loss = 0.3822(0.5059)
2023/09/30 04:57:19 - INFO - root -   Epoch: [187/200][80/346], lr: 0.00000037 	 loss = 0.3214(0.5319)
2023/09/30 04:58:28 - INFO - root -   Epoch: [187/200][100/346], lr: 0.00000037 	 loss = 0.2510(0.5204)
2023/09/30 04:59:21 - INFO - root -   Epoch: [187/200][120/346], lr: 0.00000037 	 loss = 0.2365(0.5375)
2023/09/30 05:00:34 - INFO - root -   Epoch: [187/200][140/346], lr: 0.00000037 	 loss = 0.5188(0.5482)
2023/09/30 05:01:40 - INFO - root -   Epoch: [187/200][160/346], lr: 0.00000037 	 loss = 0.4394(0.5633)
2023/09/30 05:02:54 - INFO - root -   Epoch: [187/200][180/346], lr: 0.00000037 	 loss = 0.2988(0.5529)
2023/09/30 05:03:53 - INFO - root -   Epoch: [187/200][200/346], lr: 0.00000037 	 loss = 0.2054(0.5531)
2023/09/30 05:05:07 - INFO - root -   Epoch: [187/200][220/346], lr: 0.00000037 	 loss = 0.3383(0.5639)
2023/09/30 05:06:05 - INFO - root -   Epoch: [187/200][240/346], lr: 0.00000037 	 loss = 0.2851(0.5602)
2023/09/30 05:07:20 - INFO - root -   Epoch: [187/200][260/346], lr: 0.00000037 	 loss = 0.3992(0.5508)
2023/09/30 05:08:20 - INFO - root -   Epoch: [187/200][280/346], lr: 0.00000037 	 loss = 0.5861(0.5535)
2023/09/30 05:09:41 - INFO - root -   Epoch: [187/200][300/346], lr: 0.00000037 	 loss = 0.1882(0.5583)
2023/09/30 05:10:36 - INFO - root -   Epoch: [187/200][320/346], lr: 0.00000037 	 loss = 0.4622(0.5553)
2023/09/30 05:11:34 - INFO - root -   Epoch: [187/200][340/346], lr: 0.00000037 	 loss = 0.6675(0.5603)
2023/09/30 05:11:36 - INFO - root -   Epoch: [187/200] 	 loss = 0.5604
2023/09/30 05:11:36 - INFO - root -   train_accuracy = 0.7110
2023/09/30 05:12:06 - INFO - root -   Epoch: [188/200][0/346], lr: 0.00000037 	 loss = 0.2806(0.2806)
2023/09/30 05:13:17 - INFO - root -   Epoch: [188/200][20/346], lr: 0.00000037 	 loss = 0.3390(0.5661)
2023/09/30 05:14:23 - INFO - root -   Epoch: [188/200][40/346], lr: 0.00000037 	 loss = 0.3169(0.5731)
2023/09/30 05:15:36 - INFO - root -   Epoch: [188/200][60/346], lr: 0.00000037 	 loss = 0.2235(0.5388)
2023/09/30 05:16:47 - INFO - root -   Epoch: [188/200][80/346], lr: 0.00000037 	 loss = 0.4921(0.5513)
2023/09/30 05:17:57 - INFO - root -   Epoch: [188/200][100/346], lr: 0.00000037 	 loss = 0.1738(0.5273)
2023/09/30 05:18:56 - INFO - root -   Epoch: [188/200][120/346], lr: 0.00000037 	 loss = 0.6638(0.5555)
2023/09/30 05:20:11 - INFO - root -   Epoch: [188/200][140/346], lr: 0.00000037 	 loss = 0.3328(0.5557)
2023/09/30 05:21:08 - INFO - root -   Epoch: [188/200][160/346], lr: 0.00000037 	 loss = 0.4325(0.5585)
2023/09/30 05:22:29 - INFO - root -   Epoch: [188/200][180/346], lr: 0.00000037 	 loss = 0.4287(0.5546)
2023/09/30 05:23:20 - INFO - root -   Epoch: [188/200][200/346], lr: 0.00000037 	 loss = 0.4045(0.5547)
2023/09/30 05:24:36 - INFO - root -   Epoch: [188/200][220/346], lr: 0.00000037 	 loss = 0.2769(0.5678)
2023/09/30 05:25:34 - INFO - root -   Epoch: [188/200][240/346], lr: 0.00000037 	 loss = 0.3138(0.5609)
2023/09/30 05:26:48 - INFO - root -   Epoch: [188/200][260/346], lr: 0.00000037 	 loss = 0.8010(0.5615)
2023/09/30 05:27:47 - INFO - root -   Epoch: [188/200][280/346], lr: 0.00000037 	 loss = 0.3112(0.5665)
2023/09/30 05:28:51 - INFO - root -   Epoch: [188/200][300/346], lr: 0.00000037 	 loss = 0.7385(0.5713)
2023/09/30 05:29:58 - INFO - root -   Epoch: [188/200][320/346], lr: 0.00000037 	 loss = 0.3592(0.5709)
2023/09/30 05:31:02 - INFO - root -   Epoch: [188/200][340/346], lr: 0.00000037 	 loss = 0.8095(0.5785)
2023/09/30 05:31:04 - INFO - root -   Epoch: [188/200] 	 loss = 0.5784
2023/09/30 05:31:04 - INFO - root -   train_accuracy = 0.6936
2023/09/30 05:31:33 - INFO - root -   Epoch: [189/200][0/346], lr: 0.00000037 	 loss = 0.1920(0.1920)
2023/09/30 05:32:35 - INFO - root -   Epoch: [189/200][20/346], lr: 0.00000037 	 loss = 0.2063(0.4525)
2023/09/30 05:33:56 - INFO - root -   Epoch: [189/200][40/346], lr: 0.00000037 	 loss = 0.6550(0.5507)
2023/09/30 05:34:49 - INFO - root -   Epoch: [189/200][60/346], lr: 0.00000037 	 loss = 0.4235(0.5306)
2023/09/30 05:35:51 - INFO - root -   Epoch: [189/200][80/346], lr: 0.00000037 	 loss = 0.0886(0.5203)
2023/09/30 05:37:09 - INFO - root -   Epoch: [189/200][100/346], lr: 0.00000037 	 loss = 0.5438(0.5384)
2023/09/30 05:37:59 - INFO - root -   Epoch: [189/200][120/346], lr: 0.00000037 	 loss = 0.7328(0.5769)
2023/09/30 05:39:17 - INFO - root -   Epoch: [189/200][140/346], lr: 0.00000037 	 loss = 0.4167(0.5876)
2023/09/30 05:40:14 - INFO - root -   Epoch: [189/200][160/346], lr: 0.00000037 	 loss = 0.1162(0.5732)
2023/09/30 05:41:22 - INFO - root -   Epoch: [189/200][180/346], lr: 0.00000037 	 loss = 0.4095(0.5702)
2023/09/30 05:42:39 - INFO - root -   Epoch: [189/200][200/346], lr: 0.00000037 	 loss = 0.1472(0.5673)
2023/09/30 05:43:34 - INFO - root -   Epoch: [189/200][220/346], lr: 0.00000037 	 loss = 0.3968(0.5808)
2023/09/30 05:44:57 - INFO - root -   Epoch: [189/200][240/346], lr: 0.00000037 	 loss = 0.2830(0.5765)
2023/09/30 05:46:13 - INFO - root -   Epoch: [189/200][260/346], lr: 0.00000037 	 loss = 0.8272(0.5840)
2023/09/30 05:47:17 - INFO - root -   Epoch: [189/200][280/346], lr: 0.00000037 	 loss = 1.0377(0.5880)
2023/09/30 05:48:27 - INFO - root -   Epoch: [189/200][300/346], lr: 0.00000037 	 loss = 0.1100(0.5835)
2023/09/30 05:49:40 - INFO - root -   Epoch: [189/200][320/346], lr: 0.00000037 	 loss = 0.3932(0.5887)
2023/09/30 05:50:29 - INFO - root -   Epoch: [189/200][340/346], lr: 0.00000037 	 loss = 0.6182(0.5865)
2023/09/30 05:50:31 - INFO - root -   Epoch: [189/200] 	 loss = 0.5863
2023/09/30 05:54:21 - INFO - root -   precision = 0.6552
2023/09/30 05:54:21 - INFO - root -   eval_loss = 0.6260
2023/09/30 05:54:22 - INFO - root -   train_accuracy = 0.6705
2023/09/30 05:54:52 - INFO - root -   Epoch: [190/200][0/346], lr: 0.00000037 	 loss = 0.1941(0.1941)
2023/09/30 05:55:59 - INFO - root -   Epoch: [190/200][20/346], lr: 0.00000037 	 loss = 0.4610(0.5564)
2023/09/30 05:57:19 - INFO - root -   Epoch: [190/200][40/346], lr: 0.00000037 	 loss = 1.0461(0.5560)
2023/09/30 05:58:11 - INFO - root -   Epoch: [190/200][60/346], lr: 0.00000037 	 loss = 0.2809(0.5217)
2023/09/30 05:59:20 - INFO - root -   Epoch: [190/200][80/346], lr: 0.00000037 	 loss = 0.3587(0.5379)
2023/09/30 06:00:23 - INFO - root -   Epoch: [190/200][100/346], lr: 0.00000037 	 loss = 0.9181(0.5607)
2023/09/30 06:01:35 - INFO - root -   Epoch: [190/200][120/346], lr: 0.00000037 	 loss = 0.6218(0.5771)
2023/09/30 06:02:34 - INFO - root -   Epoch: [190/200][140/346], lr: 0.00000037 	 loss = 0.3860(0.5745)
2023/09/30 06:03:56 - INFO - root -   Epoch: [190/200][160/346], lr: 0.00000037 	 loss = 0.4308(0.5595)
2023/09/30 06:04:51 - INFO - root -   Epoch: [190/200][180/346], lr: 0.00000037 	 loss = 0.1867(0.5531)
2023/09/30 06:05:59 - INFO - root -   Epoch: [190/200][200/346], lr: 0.00000037 	 loss = 0.1438(0.5488)
2023/09/30 06:06:55 - INFO - root -   Epoch: [190/200][220/346], lr: 0.00000037 	 loss = 0.3445(0.5592)
2023/09/30 06:08:08 - INFO - root -   Epoch: [190/200][240/346], lr: 0.00000037 	 loss = 0.3453(0.5427)
2023/09/30 06:09:04 - INFO - root -   Epoch: [190/200][260/346], lr: 0.00000037 	 loss = 0.8347(0.5413)
2023/09/30 06:10:12 - INFO - root -   Epoch: [190/200][280/346], lr: 0.00000037 	 loss = 0.3597(0.5486)
2023/09/30 06:11:17 - INFO - root -   Epoch: [190/200][300/346], lr: 0.00000037 	 loss = 0.2106(0.5483)
2023/09/30 06:12:23 - INFO - root -   Epoch: [190/200][320/346], lr: 0.00000037 	 loss = 0.2432(0.5552)
2023/09/30 06:13:24 - INFO - root -   Epoch: [190/200][340/346], lr: 0.00000037 	 loss = 1.2510(0.5551)
2023/09/30 06:13:27 - INFO - root -   Epoch: [190/200] 	 loss = 0.5542
2023/09/30 06:13:27 - INFO - root -   train_accuracy = 0.7110
2023/09/30 06:13:57 - INFO - root -   Epoch: [191/200][0/346], lr: 0.00000037 	 loss = 0.4620(0.4620)
2023/09/30 06:15:04 - INFO - root -   Epoch: [191/200][20/346], lr: 0.00000037 	 loss = 0.2110(0.5331)
2023/09/30 06:16:14 - INFO - root -   Epoch: [191/200][40/346], lr: 0.00000037 	 loss = 0.3995(0.5412)
2023/09/30 06:17:27 - INFO - root -   Epoch: [191/200][60/346], lr: 0.00000037 	 loss = 0.6023(0.5125)
2023/09/30 06:18:24 - INFO - root -   Epoch: [191/200][80/346], lr: 0.00000037 	 loss = 0.4306(0.5138)
2023/09/30 06:19:27 - INFO - root -   Epoch: [191/200][100/346], lr: 0.00000037 	 loss = 0.5847(0.5099)
2023/09/30 06:20:55 - INFO - root -   Epoch: [191/200][120/346], lr: 0.00000037 	 loss = 0.7142(0.5537)
2023/09/30 06:21:54 - INFO - root -   Epoch: [191/200][140/346], lr: 0.00000037 	 loss = 0.2667(0.5428)
2023/09/30 06:23:22 - INFO - root -   Epoch: [191/200][160/346], lr: 0.00000037 	 loss = 0.1013(0.5539)
2023/09/30 06:24:14 - INFO - root -   Epoch: [191/200][180/346], lr: 0.00000037 	 loss = 0.8071(0.5626)
2023/09/30 06:25:26 - INFO - root -   Epoch: [191/200][200/346], lr: 0.00000037 	 loss = 0.1295(0.5697)
2023/09/30 06:26:25 - INFO - root -   Epoch: [191/200][220/346], lr: 0.00000037 	 loss = 0.2557(0.5745)
2023/09/30 06:27:31 - INFO - root -   Epoch: [191/200][240/346], lr: 0.00000037 	 loss = 0.2928(0.5663)
2023/09/30 06:28:54 - INFO - root -   Epoch: [191/200][260/346], lr: 0.00000037 	 loss = 0.3518(0.5597)
2023/09/30 06:29:45 - INFO - root -   Epoch: [191/200][280/346], lr: 0.00000037 	 loss = 0.4357(0.5573)
2023/09/30 06:31:09 - INFO - root -   Epoch: [191/200][300/346], lr: 0.00000037 	 loss = 0.4032(0.5616)
2023/09/30 06:32:08 - INFO - root -   Epoch: [191/200][320/346], lr: 0.00000037 	 loss = 0.4172(0.5657)
2023/09/30 06:32:57 - INFO - root -   Epoch: [191/200][340/346], lr: 0.00000037 	 loss = 0.8893(0.5630)
2023/09/30 06:32:59 - INFO - root -   Epoch: [191/200] 	 loss = 0.5634
2023/09/30 06:32:59 - INFO - root -   train_accuracy = 0.6908
2023/09/30 06:33:22 - INFO - root -   Epoch: [192/200][0/346], lr: 0.00000037 	 loss = 0.2764(0.2764)
2023/09/30 06:34:28 - INFO - root -   Epoch: [192/200][20/346], lr: 0.00000037 	 loss = 0.0964(0.5330)
2023/09/30 06:35:46 - INFO - root -   Epoch: [192/200][40/346], lr: 0.00000037 	 loss = 0.6850(0.5181)
2023/09/30 06:36:40 - INFO - root -   Epoch: [192/200][60/346], lr: 0.00000037 	 loss = 0.3359(0.5061)
2023/09/30 06:38:02 - INFO - root -   Epoch: [192/200][80/346], lr: 0.00000037 	 loss = 0.3224(0.5090)
2023/09/30 06:38:45 - INFO - root -   Epoch: [192/200][100/346], lr: 0.00000037 	 loss = 0.2395(0.5326)
2023/09/30 06:40:04 - INFO - root -   Epoch: [192/200][120/346], lr: 0.00000037 	 loss = 0.3050(0.5600)
2023/09/30 06:41:05 - INFO - root -   Epoch: [192/200][140/346], lr: 0.00000037 	 loss = 0.3114(0.5543)
2023/09/30 06:42:09 - INFO - root -   Epoch: [192/200][160/346], lr: 0.00000037 	 loss = 0.0719(0.5471)
2023/09/30 06:43:19 - INFO - root -   Epoch: [192/200][180/346], lr: 0.00000037 	 loss = 0.7033(0.5475)
2023/09/30 06:44:26 - INFO - root -   Epoch: [192/200][200/346], lr: 0.00000037 	 loss = 0.2148(0.5460)
2023/09/30 06:45:28 - INFO - root -   Epoch: [192/200][220/346], lr: 0.00000037 	 loss = 0.1420(0.5612)
2023/09/30 06:46:39 - INFO - root -   Epoch: [192/200][240/346], lr: 0.00000037 	 loss = 0.4084(0.5588)
2023/09/30 06:47:46 - INFO - root -   Epoch: [192/200][260/346], lr: 0.00000037 	 loss = 0.9362(0.5561)
2023/09/30 06:49:13 - INFO - root -   Epoch: [192/200][280/346], lr: 0.00000037 	 loss = 0.3480(0.5547)
2023/09/30 06:50:12 - INFO - root -   Epoch: [192/200][300/346], lr: 0.00000037 	 loss = 0.3673(0.5567)
2023/09/30 06:51:29 - INFO - root -   Epoch: [192/200][320/346], lr: 0.00000037 	 loss = 0.2808(0.5563)
2023/09/30 06:52:11 - INFO - root -   Epoch: [192/200][340/346], lr: 0.00000037 	 loss = 0.7326(0.5525)
2023/09/30 06:52:20 - INFO - root -   Epoch: [192/200] 	 loss = 0.5536
2023/09/30 06:52:20 - INFO - root -   train_accuracy = 0.7153
2023/09/30 06:52:52 - INFO - root -   Epoch: [193/200][0/346], lr: 0.00000038 	 loss = 0.4996(0.4996)
2023/09/30 06:53:51 - INFO - root -   Epoch: [193/200][20/346], lr: 0.00000038 	 loss = 0.3290(0.4689)
2023/09/30 06:55:03 - INFO - root -   Epoch: [193/200][40/346], lr: 0.00000038 	 loss = 0.5162(0.5395)
2023/09/30 06:56:02 - INFO - root -   Epoch: [193/200][60/346], lr: 0.00000038 	 loss = 0.6612(0.4731)
2023/09/30 06:57:15 - INFO - root -   Epoch: [193/200][80/346], lr: 0.00000038 	 loss = 0.2884(0.5195)
2023/09/30 06:58:22 - INFO - root -   Epoch: [193/200][100/346], lr: 0.00000038 	 loss = 0.4364(0.5532)
2023/09/30 06:59:36 - INFO - root -   Epoch: [193/200][120/346], lr: 0.00000038 	 loss = 0.4016(0.5799)
2023/09/30 07:00:31 - INFO - root -   Epoch: [193/200][140/346], lr: 0.00000038 	 loss = 0.2695(0.5802)
2023/09/30 07:01:48 - INFO - root -   Epoch: [193/200][160/346], lr: 0.00000038 	 loss = 0.0741(0.5652)
2023/09/30 07:02:58 - INFO - root -   Epoch: [193/200][180/346], lr: 0.00000038 	 loss = 0.6437(0.5571)
2023/09/30 07:04:13 - INFO - root -   Epoch: [193/200][200/346], lr: 0.00000038 	 loss = 0.1852(0.5490)
2023/09/30 07:05:12 - INFO - root -   Epoch: [193/200][220/346], lr: 0.00000038 	 loss = 0.5033(0.5586)
2023/09/30 07:06:20 - INFO - root -   Epoch: [193/200][240/346], lr: 0.00000038 	 loss = 0.2242(0.5484)
2023/09/30 07:07:27 - INFO - root -   Epoch: [193/200][260/346], lr: 0.00000038 	 loss = 0.7459(0.5481)
2023/09/30 07:08:47 - INFO - root -   Epoch: [193/200][280/346], lr: 0.00000038 	 loss = 0.3008(0.5565)
2023/09/30 07:09:41 - INFO - root -   Epoch: [193/200][300/346], lr: 0.00000038 	 loss = 0.3256(0.5536)
2023/09/30 07:11:00 - INFO - root -   Epoch: [193/200][320/346], lr: 0.00000038 	 loss = 0.3719(0.5584)
2023/09/30 07:11:49 - INFO - root -   Epoch: [193/200][340/346], lr: 0.00000038 	 loss = 0.6890(0.5543)
2023/09/30 07:11:55 - INFO - root -   Epoch: [193/200] 	 loss = 0.5518
2023/09/30 07:11:55 - INFO - root -   train_accuracy = 0.7066
2023/09/30 07:12:24 - INFO - root -   Epoch: [194/200][0/346], lr: 0.00000038 	 loss = 0.5544(0.5544)
2023/09/30 07:13:26 - INFO - root -   Epoch: [194/200][20/346], lr: 0.00000038 	 loss = 0.1313(0.5041)
2023/09/30 07:14:25 - INFO - root -   Epoch: [194/200][40/346], lr: 0.00000038 	 loss = 0.5545(0.5197)
2023/09/30 07:15:25 - INFO - root -   Epoch: [194/200][60/346], lr: 0.00000038 	 loss = 0.3783(0.5000)
2023/09/30 07:16:42 - INFO - root -   Epoch: [194/200][80/346], lr: 0.00000038 	 loss = 0.6517(0.5012)
2023/09/30 07:17:43 - INFO - root -   Epoch: [194/200][100/346], lr: 0.00000038 	 loss = 0.4971(0.5036)
2023/09/30 07:18:55 - INFO - root -   Epoch: [194/200][120/346], lr: 0.00000038 	 loss = 0.3666(0.5178)
2023/09/30 07:19:54 - INFO - root -   Epoch: [194/200][140/346], lr: 0.00000038 	 loss = 0.3695(0.5268)
2023/09/30 07:21:15 - INFO - root -   Epoch: [194/200][160/346], lr: 0.00000038 	 loss = 0.3342(0.5409)
2023/09/30 07:22:17 - INFO - root -   Epoch: [194/200][180/346], lr: 0.00000038 	 loss = 0.6788(0.5461)
2023/09/30 07:23:21 - INFO - root -   Epoch: [194/200][200/346], lr: 0.00000038 	 loss = 0.2187(0.5438)
2023/09/30 07:24:31 - INFO - root -   Epoch: [194/200][220/346], lr: 0.00000038 	 loss = 0.1598(0.5515)
2023/09/30 07:25:29 - INFO - root -   Epoch: [194/200][240/346], lr: 0.00000038 	 loss = 0.1654(0.5405)
2023/09/30 07:26:45 - INFO - root -   Epoch: [194/200][260/346], lr: 0.00000038 	 loss = 1.0345(0.5446)
2023/09/30 07:27:40 - INFO - root -   Epoch: [194/200][280/346], lr: 0.00000038 	 loss = 0.2161(0.5377)
2023/09/30 07:29:09 - INFO - root -   Epoch: [194/200][300/346], lr: 0.00000038 	 loss = 0.1784(0.5431)
2023/09/30 07:29:59 - INFO - root -   Epoch: [194/200][320/346], lr: 0.00000038 	 loss = 0.1398(0.5442)
2023/09/30 07:30:59 - INFO - root -   Epoch: [194/200][340/346], lr: 0.00000038 	 loss = 1.0419(0.5441)
2023/09/30 07:31:01 - INFO - root -   Epoch: [194/200] 	 loss = 0.5437
2023/09/30 07:34:57 - INFO - root -   precision = 0.7069
2023/09/30 07:34:57 - INFO - root -   eval_loss = 0.6220
2023/09/30 07:34:58 - INFO - root -   train_accuracy = 0.7168
2023/09/30 07:35:29 - INFO - root -   Epoch: [195/200][0/346], lr: 0.00000038 	 loss = 0.3726(0.3726)
2023/09/30 07:36:35 - INFO - root -   Epoch: [195/200][20/346], lr: 0.00000038 	 loss = 0.0369(0.4654)
2023/09/30 07:37:38 - INFO - root -   Epoch: [195/200][40/346], lr: 0.00000038 	 loss = 0.8028(0.5265)
2023/09/30 07:38:51 - INFO - root -   Epoch: [195/200][60/346], lr: 0.00000038 	 loss = 0.2661(0.4705)
2023/09/30 07:39:55 - INFO - root -   Epoch: [195/200][80/346], lr: 0.00000038 	 loss = 0.3350(0.4879)
2023/09/30 07:41:15 - INFO - root -   Epoch: [195/200][100/346], lr: 0.00000038 	 loss = 0.5095(0.4853)
2023/09/30 07:42:13 - INFO - root -   Epoch: [195/200][120/346], lr: 0.00000038 	 loss = 0.2375(0.5225)
2023/09/30 07:43:29 - INFO - root -   Epoch: [195/200][140/346], lr: 0.00000038 	 loss = 0.4549(0.5197)
2023/09/30 07:44:23 - INFO - root -   Epoch: [195/200][160/346], lr: 0.00000038 	 loss = 0.1771(0.5218)
2023/09/30 07:45:26 - INFO - root -   Epoch: [195/200][180/346], lr: 0.00000038 	 loss = 0.6852(0.5303)
2023/09/30 07:46:43 - INFO - root -   Epoch: [195/200][200/346], lr: 0.00000038 	 loss = 0.4453(0.5313)
2023/09/30 07:47:57 - INFO - root -   Epoch: [195/200][220/346], lr: 0.00000038 	 loss = 0.4193(0.5490)
2023/09/30 07:49:06 - INFO - root -   Epoch: [195/200][240/346], lr: 0.00000038 	 loss = 0.2037(0.5433)
2023/09/30 07:50:13 - INFO - root -   Epoch: [195/200][260/346], lr: 0.00000038 	 loss = 0.2041(0.5383)
2023/09/30 07:51:15 - INFO - root -   Epoch: [195/200][280/346], lr: 0.00000038 	 loss = 0.2847(0.5387)
2023/09/30 07:52:29 - INFO - root -   Epoch: [195/200][300/346], lr: 0.00000038 	 loss = 0.1291(0.5349)
2023/09/30 07:53:22 - INFO - root -   Epoch: [195/200][320/346], lr: 0.00000038 	 loss = 0.2310(0.5343)
2023/09/30 07:54:32 - INFO - root -   Epoch: [195/200][340/346], lr: 0.00000038 	 loss = 0.4172(0.5366)
2023/09/30 07:54:34 - INFO - root -   Epoch: [195/200] 	 loss = 0.5364
2023/09/30 07:54:34 - INFO - root -   train_accuracy = 0.7225
2023/09/30 07:55:13 - INFO - root -   Epoch: [196/200][0/346], lr: 0.00000038 	 loss = 0.2004(0.2004)
2023/09/30 07:56:11 - INFO - root -   Epoch: [196/200][20/346], lr: 0.00000038 	 loss = 0.0616(0.3953)
2023/09/30 07:57:17 - INFO - root -   Epoch: [196/200][40/346], lr: 0.00000038 	 loss = 0.4489(0.4774)
2023/09/30 07:58:29 - INFO - root -   Epoch: [196/200][60/346], lr: 0.00000038 	 loss = 0.7764(0.4497)
2023/09/30 07:59:27 - INFO - root -   Epoch: [196/200][80/346], lr: 0.00000038 	 loss = 0.2327(0.4419)
2023/09/30 08:00:31 - INFO - root -   Epoch: [196/200][100/346], lr: 0.00000038 	 loss = 0.3249(0.4664)
2023/09/30 08:01:47 - INFO - root -   Epoch: [196/200][120/346], lr: 0.00000038 	 loss = 0.0965(0.4976)
2023/09/30 08:02:47 - INFO - root -   Epoch: [196/200][140/346], lr: 0.00000038 	 loss = 0.4003(0.5019)
2023/09/30 08:03:51 - INFO - root -   Epoch: [196/200][160/346], lr: 0.00000038 	 loss = 0.2300(0.5075)
2023/09/30 08:05:06 - INFO - root -   Epoch: [196/200][180/346], lr: 0.00000038 	 loss = 0.5093(0.5106)
2023/09/30 08:05:57 - INFO - root -   Epoch: [196/200][200/346], lr: 0.00000038 	 loss = 0.2063(0.5094)
2023/09/30 08:06:59 - INFO - root -   Epoch: [196/200][220/346], lr: 0.00000038 	 loss = 0.4826(0.5220)
2023/09/30 08:08:18 - INFO - root -   Epoch: [196/200][240/346], lr: 0.00000038 	 loss = 0.2342(0.5337)
2023/09/30 08:09:26 - INFO - root -   Epoch: [196/200][260/346], lr: 0.00000038 	 loss = 0.7901(0.5340)
2023/09/30 08:10:54 - INFO - root -   Epoch: [196/200][280/346], lr: 0.00000038 	 loss = 1.4024(0.5484)
2023/09/30 08:11:44 - INFO - root -   Epoch: [196/200][300/346], lr: 0.00000038 	 loss = 0.3684(0.5508)
2023/09/30 08:12:49 - INFO - root -   Epoch: [196/200][320/346], lr: 0.00000038 	 loss = 0.4185(0.5592)
2023/09/30 08:13:38 - INFO - root -   Epoch: [196/200][340/346], lr: 0.00000038 	 loss = 0.3664(0.5594)
2023/09/30 08:13:43 - INFO - root -   Epoch: [196/200] 	 loss = 0.5576
2023/09/30 08:13:43 - INFO - root -   train_accuracy = 0.7023
2023/09/30 08:14:05 - INFO - root -   Epoch: [197/200][0/346], lr: 0.00000038 	 loss = 0.0990(0.0990)
2023/09/30 08:15:13 - INFO - root -   Epoch: [197/200][20/346], lr: 0.00000038 	 loss = 0.1788(0.4958)
2023/09/30 08:16:41 - INFO - root -   Epoch: [197/200][40/346], lr: 0.00000038 	 loss = 0.6316(0.5611)
2023/09/30 08:17:30 - INFO - root -   Epoch: [197/200][60/346], lr: 0.00000038 	 loss = 0.3904(0.5001)
2023/09/30 08:18:45 - INFO - root -   Epoch: [197/200][80/346], lr: 0.00000038 	 loss = 0.1745(0.5011)
2023/09/30 08:19:41 - INFO - root -   Epoch: [197/200][100/346], lr: 0.00000038 	 loss = 0.3579(0.5134)
2023/09/30 08:20:47 - INFO - root -   Epoch: [197/200][120/346], lr: 0.00000038 	 loss = 0.6031(0.5445)
2023/09/30 08:21:51 - INFO - root -   Epoch: [197/200][140/346], lr: 0.00000038 	 loss = 0.2330(0.5382)
2023/09/30 08:23:03 - INFO - root -   Epoch: [197/200][160/346], lr: 0.00000038 	 loss = 0.0756(0.5284)
2023/09/30 08:24:08 - INFO - root -   Epoch: [197/200][180/346], lr: 0.00000038 	 loss = 0.3040(0.5291)
2023/09/30 08:25:25 - INFO - root -   Epoch: [197/200][200/346], lr: 0.00000038 	 loss = 0.2338(0.5251)
2023/09/30 08:26:21 - INFO - root -   Epoch: [197/200][220/346], lr: 0.00000038 	 loss = 0.1618(0.5268)
2023/09/30 08:27:15 - INFO - root -   Epoch: [197/200][240/346], lr: 0.00000038 	 loss = 0.1218(0.5159)
2023/09/30 08:28:33 - INFO - root -   Epoch: [197/200][260/346], lr: 0.00000038 	 loss = 0.4787(0.5130)
2023/09/30 08:29:32 - INFO - root -   Epoch: [197/200][280/346], lr: 0.00000038 	 loss = 0.7998(0.5211)
2023/09/30 08:30:52 - INFO - root -   Epoch: [197/200][300/346], lr: 0.00000038 	 loss = 0.2804(0.5218)
2023/09/30 08:31:44 - INFO - root -   Epoch: [197/200][320/346], lr: 0.00000038 	 loss = 0.1661(0.5196)
2023/09/30 08:32:42 - INFO - root -   Epoch: [197/200][340/346], lr: 0.00000038 	 loss = 0.8891(0.5272)
2023/09/30 08:32:47 - INFO - root -   Epoch: [197/200] 	 loss = 0.5301
2023/09/30 08:32:47 - INFO - root -   train_accuracy = 0.7240
2023/09/30 08:33:16 - INFO - root -   Epoch: [198/200][0/346], lr: 0.00000038 	 loss = 0.0994(0.0994)
2023/09/30 08:34:18 - INFO - root -   Epoch: [198/200][20/346], lr: 0.00000038 	 loss = 0.2358(0.4381)
2023/09/30 08:35:27 - INFO - root -   Epoch: [198/200][40/346], lr: 0.00000038 	 loss = 0.6588(0.4463)
2023/09/30 08:36:26 - INFO - root -   Epoch: [198/200][60/346], lr: 0.00000038 	 loss = 0.1954(0.4227)
2023/09/30 08:38:03 - INFO - root -   Epoch: [198/200][80/346], lr: 0.00000038 	 loss = 0.3304(0.4369)
2023/09/30 08:38:57 - INFO - root -   Epoch: [198/200][100/346], lr: 0.00000038 	 loss = 0.9383(0.4862)
2023/09/30 08:40:15 - INFO - root -   Epoch: [198/200][120/346], lr: 0.00000038 	 loss = 0.1718(0.5079)
2023/09/30 08:41:16 - INFO - root -   Epoch: [198/200][140/346], lr: 0.00000038 	 loss = 0.4832(0.5076)
2023/09/30 08:42:18 - INFO - root -   Epoch: [198/200][160/346], lr: 0.00000038 	 loss = 0.1098(0.5164)
2023/09/30 08:43:41 - INFO - root -   Epoch: [198/200][180/346], lr: 0.00000038 	 loss = 0.3138(0.5135)
2023/09/30 08:44:27 - INFO - root -   Epoch: [198/200][200/346], lr: 0.00000038 	 loss = 0.2434(0.5135)
2023/09/30 08:45:37 - INFO - root -   Epoch: [198/200][220/346], lr: 0.00000038 	 loss = 0.2615(0.5253)
2023/09/30 08:46:52 - INFO - root -   Epoch: [198/200][240/346], lr: 0.00000038 	 loss = 0.2916(0.5217)
2023/09/30 08:47:42 - INFO - root -   Epoch: [198/200][260/346], lr: 0.00000038 	 loss = 0.9142(0.5353)
2023/09/30 08:48:57 - INFO - root -   Epoch: [198/200][280/346], lr: 0.00000038 	 loss = 0.3345(0.5445)
2023/09/30 08:49:47 - INFO - root -   Epoch: [198/200][300/346], lr: 0.00000038 	 loss = 0.1886(0.5430)
2023/09/30 08:50:59 - INFO - root -   Epoch: [198/200][320/346], lr: 0.00000038 	 loss = 0.4193(0.5423)
2023/09/30 08:51:51 - INFO - root -   Epoch: [198/200][340/346], lr: 0.00000038 	 loss = 0.6219(0.5383)
2023/09/30 08:51:53 - INFO - root -   Epoch: [198/200] 	 loss = 0.5367
2023/09/30 08:51:53 - INFO - root -   train_accuracy = 0.7384
2023/09/30 08:52:15 - INFO - root -   Epoch: [199/200][0/346], lr: 0.00000038 	 loss = 0.1725(0.1725)
2023/09/30 08:53:22 - INFO - root -   Epoch: [199/200][20/346], lr: 0.00000038 	 loss = 0.1464(0.4729)
2023/09/30 08:54:24 - INFO - root -   Epoch: [199/200][40/346], lr: 0.00000038 	 loss = 1.1479(0.4962)
2023/09/30 08:55:38 - INFO - root -   Epoch: [199/200][60/346], lr: 0.00000038 	 loss = 0.5089(0.4782)
2023/09/30 08:56:44 - INFO - root -   Epoch: [199/200][80/346], lr: 0.00000038 	 loss = 0.3668(0.5228)
2023/09/30 08:57:54 - INFO - root -   Epoch: [199/200][100/346], lr: 0.00000038 	 loss = 0.3936(0.5523)
2023/09/30 08:58:49 - INFO - root -   Epoch: [199/200][120/346], lr: 0.00000038 	 loss = 0.6202(0.5801)
2023/09/30 09:00:04 - INFO - root -   Epoch: [199/200][140/346], lr: 0.00000038 	 loss = 0.3975(0.5672)
2023/09/30 09:01:24 - INFO - root -   Epoch: [199/200][160/346], lr: 0.00000038 	 loss = 0.0722(0.5655)
2023/09/30 09:02:20 - INFO - root -   Epoch: [199/200][180/346], lr: 0.00000038 	 loss = 0.3681(0.5605)
2023/09/30 09:03:37 - INFO - root -   Epoch: [199/200][200/346], lr: 0.00000038 	 loss = 0.0613(0.5658)
2023/09/30 09:04:36 - INFO - root -   Epoch: [199/200][220/346], lr: 0.00000038 	 loss = 0.2309(0.5761)
2023/09/30 09:05:46 - INFO - root -   Epoch: [199/200][240/346], lr: 0.00000038 	 loss = 0.2509(0.5715)
2023/09/30 09:06:51 - INFO - root -   Epoch: [199/200][260/346], lr: 0.00000038 	 loss = 0.7835(0.5773)
2023/09/30 09:07:49 - INFO - root -   Epoch: [199/200][280/346], lr: 0.00000038 	 loss = 0.3618(0.5771)
2023/09/30 09:08:58 - INFO - root -   Epoch: [199/200][300/346], lr: 0.00000038 	 loss = 0.2613(0.5766)
2023/09/30 09:10:17 - INFO - root -   Epoch: [199/200][320/346], lr: 0.00000038 	 loss = 0.3729(0.5703)
2023/09/30 09:11:10 - INFO - root -   Epoch: [199/200][340/346], lr: 0.00000038 	 loss = 0.3647(0.5668)
2023/09/30 09:11:15 - INFO - root -   Epoch: [199/200] 	 loss = 0.5676
2023/09/30 09:15:06 - INFO - root -   precision = 0.6667
2023/09/30 09:15:06 - INFO - root -   eval_loss = 0.6069
2023/09/30 09:15:07 - INFO - root -   train_accuracy = 0.6994
