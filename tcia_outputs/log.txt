2023/10/16 23:13:16 - INFO - root -   Num train examples = 169
2023/10/16 23:13:16 - INFO - root -   Num val examples = 43
2023/10/16 23:13:17 - INFO - root -   backend = nccl
2023/10/16 23:13:17 - INFO - root -   batch_size = 1
2023/10/16 23:13:17 - INFO - root -   dropout = 0.5
2023/10/16 23:13:17 - INFO - root -   epochs = 300
2023/10/16 23:13:17 - INFO - root -   eval_freq = 5
2023/10/16 23:13:17 - INFO - root -   focal_loss = False
2023/10/16 23:13:17 - INFO - root -   input_size = 224
2023/10/16 23:13:17 - INFO - root -   is_pretrained = False
2023/10/16 23:13:17 - INFO - root -   label_smooth = False
2023/10/16 23:13:17 - INFO - root -   local_rank = -1
2023/10/16 23:13:17 - INFO - root -   lr = 1e-05
2023/10/16 23:13:17 - INFO - root -   lr_decay_rate = 0.1
2023/10/16 23:13:17 - INFO - root -   lr_steps = [50, 100]
2023/10/16 23:13:17 - INFO - root -   lr_type = cosine
2023/10/16 23:13:17 - INFO - root -   model_depth = 34
2023/10/16 23:13:17 - INFO - root -   model_name = resnet50
2023/10/16 23:13:17 - INFO - root -   momentum = 0.9
2023/10/16 23:13:17 - INFO - root -   num_classes = 8
2023/10/16 23:13:17 - INFO - root -   output = ./tcia_outputs
2023/10/16 23:13:17 - INFO - root -   print_freq = 20
2023/10/16 23:13:17 - INFO - root -   resume = 
2023/10/16 23:13:17 - INFO - root -   start_epoch = 0
2023/10/16 23:13:17 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/16 23:13:17 - INFO - root -   tune_from = 
2023/10/16 23:13:17 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/16 23:13:17 - INFO - root -   warmup_epoch = 20
2023/10/16 23:13:17 - INFO - root -   warmup_multiplier = 100
2023/10/16 23:13:17 - INFO - root -   weight_decay = 0.0005
2023/10/16 23:13:17 - INFO - root -   workers = 8
2023/10/16 23:15:57 - INFO - root -   Epoch: [0/300][0/169], lr: 0.00000010 	 loss = 2.4941(2.4941)
2023/10/16 23:16:25 - INFO - root -   Epoch: [0/300][20/169], lr: 0.00000010 	 loss = 2.4478(2.2612)
2023/10/16 23:17:13 - INFO - root -   Epoch: [0/300][40/169], lr: 0.00000010 	 loss = 2.3924(2.2525)
2023/10/16 23:17:57 - INFO - root -   Epoch: [0/300][60/169], lr: 0.00000010 	 loss = 2.0414(2.2338)
2023/10/16 23:18:32 - INFO - root -   Epoch: [0/300][80/169], lr: 0.00000010 	 loss = 2.4603(2.2495)
2023/10/16 23:19:16 - INFO - root -   Epoch: [0/300][100/169], lr: 0.00000010 	 loss = 2.1086(2.2488)
2023/10/16 23:20:16 - INFO - root -   Num train examples = 169
2023/10/16 23:20:16 - INFO - root -   Num val examples = 43
2023/10/16 23:20:16 - INFO - root -   backend = nccl
2023/10/16 23:20:16 - INFO - root -   batch_size = 1
2023/10/16 23:20:16 - INFO - root -   dropout = 0.5
2023/10/16 23:20:16 - INFO - root -   epochs = 300
2023/10/16 23:20:16 - INFO - root -   eval_freq = 5
2023/10/16 23:20:16 - INFO - root -   focal_loss = False
2023/10/16 23:20:16 - INFO - root -   input_size = 224
2023/10/16 23:20:16 - INFO - root -   is_pretrained = False
2023/10/16 23:20:16 - INFO - root -   label_smooth = False
2023/10/16 23:20:16 - INFO - root -   local_rank = -1
2023/10/16 23:20:16 - INFO - root -   lr = 1e-05
2023/10/16 23:20:16 - INFO - root -   lr_decay_rate = 0.1
2023/10/16 23:20:16 - INFO - root -   lr_steps = [50, 100]
2023/10/16 23:20:16 - INFO - root -   lr_type = cosine
2023/10/16 23:20:16 - INFO - root -   model_depth = 34
2023/10/16 23:20:16 - INFO - root -   model_name = resnet50
2023/10/16 23:20:16 - INFO - root -   momentum = 0.9
2023/10/16 23:20:16 - INFO - root -   num_classes = 2
2023/10/16 23:20:16 - INFO - root -   output = ./tcia_outputs
2023/10/16 23:20:16 - INFO - root -   print_freq = 20
2023/10/16 23:20:16 - INFO - root -   resume = 
2023/10/16 23:20:16 - INFO - root -   start_epoch = 0
2023/10/16 23:20:16 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/16 23:20:16 - INFO - root -   tune_from = 
2023/10/16 23:20:16 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/16 23:20:16 - INFO - root -   warmup_epoch = 20
2023/10/16 23:20:16 - INFO - root -   warmup_multiplier = 100
2023/10/16 23:20:16 - INFO - root -   weight_decay = 0.0005
2023/10/16 23:20:16 - INFO - root -   workers = 8
2023/10/16 23:20:32 - INFO - root -   Epoch: [0/300][0/169], lr: 0.00000010 	 loss = 0.6210(0.6210)
2023/10/16 23:20:49 - INFO - root -   Num train examples = 169
2023/10/16 23:20:49 - INFO - root -   Num val examples = 43
2023/10/16 23:20:49 - INFO - root -   backend = nccl
2023/10/16 23:20:49 - INFO - root -   batch_size = 8
2023/10/16 23:20:49 - INFO - root -   dropout = 0.5
2023/10/16 23:20:49 - INFO - root -   epochs = 300
2023/10/16 23:20:49 - INFO - root -   eval_freq = 5
2023/10/16 23:20:49 - INFO - root -   focal_loss = False
2023/10/16 23:20:49 - INFO - root -   input_size = 224
2023/10/16 23:20:49 - INFO - root -   is_pretrained = False
2023/10/16 23:20:49 - INFO - root -   label_smooth = False
2023/10/16 23:20:49 - INFO - root -   local_rank = -1
2023/10/16 23:20:49 - INFO - root -   lr = 1e-05
2023/10/16 23:20:49 - INFO - root -   lr_decay_rate = 0.1
2023/10/16 23:20:49 - INFO - root -   lr_steps = [50, 100]
2023/10/16 23:20:49 - INFO - root -   lr_type = cosine
2023/10/16 23:20:49 - INFO - root -   model_depth = 34
2023/10/16 23:20:49 - INFO - root -   model_name = resnet50
2023/10/16 23:20:49 - INFO - root -   momentum = 0.9
2023/10/16 23:20:49 - INFO - root -   num_classes = 2
2023/10/16 23:20:49 - INFO - root -   output = ./tcia_outputs
2023/10/16 23:20:49 - INFO - root -   print_freq = 20
2023/10/16 23:20:49 - INFO - root -   resume = 
2023/10/16 23:20:49 - INFO - root -   start_epoch = 0
2023/10/16 23:20:49 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/16 23:20:49 - INFO - root -   tune_from = 
2023/10/16 23:20:49 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/16 23:20:49 - INFO - root -   warmup_epoch = 20
2023/10/16 23:20:49 - INFO - root -   warmup_multiplier = 100
2023/10/16 23:20:49 - INFO - root -   weight_decay = 0.0005
2023/10/16 23:20:49 - INFO - root -   workers = 8
2023/10/16 23:24:19 - INFO - root -   Epoch: [0/300][0/21], lr: 0.00000010 	 loss = 0.5913(0.5913)
2023/10/16 23:27:18 - INFO - root -   Epoch: [0/300][20/21], lr: 0.00000010 	 loss = 0.6998(0.7391)
2023/10/16 23:27:19 - INFO - root -   Epoch: [0/300] 	 loss = 0.7391
2023/10/16 23:27:19 - INFO - root -   train_accuracy = 0.5813
2023/10/16 23:29:30 - INFO - root -   Num train examples = 169
2023/10/16 23:29:30 - INFO - root -   Num val examples = 43
2023/10/16 23:29:30 - INFO - root -   backend = nccl
2023/10/16 23:29:30 - INFO - root -   batch_size = 2
2023/10/16 23:29:30 - INFO - root -   dropout = 0.5
2023/10/16 23:29:30 - INFO - root -   epochs = 300
2023/10/16 23:29:30 - INFO - root -   eval_freq = 5
2023/10/16 23:29:30 - INFO - root -   focal_loss = False
2023/10/16 23:29:30 - INFO - root -   input_size = 224
2023/10/16 23:29:30 - INFO - root -   is_pretrained = False
2023/10/16 23:29:30 - INFO - root -   label_smooth = False
2023/10/16 23:29:30 - INFO - root -   local_rank = -1
2023/10/16 23:29:30 - INFO - root -   lr = 1e-05
2023/10/16 23:29:30 - INFO - root -   lr_decay_rate = 0.1
2023/10/16 23:29:30 - INFO - root -   lr_steps = [50, 100]
2023/10/16 23:29:30 - INFO - root -   lr_type = cosine
2023/10/16 23:29:30 - INFO - root -   model_depth = 34
2023/10/16 23:29:30 - INFO - root -   model_name = resnet50
2023/10/16 23:29:30 - INFO - root -   momentum = 0.9
2023/10/16 23:29:30 - INFO - root -   num_classes = 2
2023/10/16 23:29:30 - INFO - root -   output = ./tcia_outputs
2023/10/16 23:29:30 - INFO - root -   print_freq = 20
2023/10/16 23:29:30 - INFO - root -   resume = 
2023/10/16 23:29:30 - INFO - root -   start_epoch = 0
2023/10/16 23:29:30 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/16 23:29:30 - INFO - root -   tune_from = 
2023/10/16 23:29:30 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/16 23:29:30 - INFO - root -   warmup_epoch = 20
2023/10/16 23:29:30 - INFO - root -   warmup_multiplier = 100
2023/10/16 23:29:30 - INFO - root -   weight_decay = 0.0005
2023/10/16 23:29:30 - INFO - root -   workers = 8
2023/10/16 23:30:17 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 0.4999(0.4999)
2023/10/16 23:31:30 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 0.8380(0.6378)
2023/10/16 23:32:47 - INFO - root -   Epoch: [0/300][40/84], lr: 0.00000010 	 loss = 0.6663(0.6518)
2023/10/16 23:34:39 - INFO - root -   Epoch: [0/300][60/84], lr: 0.00000010 	 loss = 0.6890(0.6608)
2023/10/16 23:35:25 - INFO - root -   Epoch: [0/300][80/84], lr: 0.00000010 	 loss = 0.6457(0.6606)
2023/10/16 23:35:30 - INFO - root -   Epoch: [0/300] 	 loss = 0.6586
2023/10/16 23:35:30 - INFO - root -   train_accuracy = 0.6528
2023/10/16 23:36:08 - INFO - root -   Epoch: [1/300][0/84], lr: 0.00000011 	 loss = 0.5291(0.5291)
2023/10/16 23:37:29 - INFO - root -   Epoch: [1/300][20/84], lr: 0.00000011 	 loss = 0.8539(0.6339)
2023/10/16 23:39:19 - INFO - root -   Epoch: [1/300][40/84], lr: 0.00000011 	 loss = 0.6638(0.6464)
2023/10/16 23:40:24 - INFO - root -   Epoch: [1/300][60/84], lr: 0.00000011 	 loss = 0.6995(0.6541)
2023/10/16 23:41:31 - INFO - root -   Epoch: [1/300][80/84], lr: 0.00000011 	 loss = 0.6323(0.6543)
2023/10/16 23:41:32 - INFO - root -   Epoch: [1/300] 	 loss = 0.6524
2023/10/16 23:41:32 - INFO - root -   train_accuracy = 0.6528
2023/10/16 23:42:01 - INFO - root -   Epoch: [2/300][0/84], lr: 0.00000011 	 loss = 0.4930(0.4930)
2023/10/16 23:43:24 - INFO - root -   Epoch: [2/300][20/84], lr: 0.00000011 	 loss = 0.8344(0.6348)
2023/10/16 23:44:51 - INFO - root -   Epoch: [2/300][40/84], lr: 0.00000011 	 loss = 0.7066(0.6463)
2023/10/16 23:46:18 - INFO - root -   Epoch: [2/300][60/84], lr: 0.00000011 	 loss = 0.6944(0.6519)
2023/10/16 23:47:19 - INFO - root -   Epoch: [2/300][80/84], lr: 0.00000011 	 loss = 0.6139(0.6508)
2023/10/16 23:47:29 - INFO - root -   Epoch: [2/300] 	 loss = 0.6486
2023/10/16 23:47:29 - INFO - root -   train_accuracy = 0.6528
2023/10/16 23:47:58 - INFO - root -   Epoch: [3/300][0/84], lr: 0.00000012 	 loss = 0.4979(0.4979)
2023/10/16 23:49:20 - INFO - root -   Epoch: [3/300][20/84], lr: 0.00000012 	 loss = 0.8539(0.6273)
2023/10/16 23:51:00 - INFO - root -   Epoch: [3/300][40/84], lr: 0.00000012 	 loss = 0.7035(0.6454)
2023/10/16 23:52:20 - INFO - root -   Epoch: [3/300][60/84], lr: 0.00000012 	 loss = 0.6919(0.6528)
2023/10/16 23:53:35 - INFO - root -   Epoch: [3/300][80/84], lr: 0.00000012 	 loss = 0.6512(0.6523)
2023/10/16 23:53:37 - INFO - root -   Epoch: [3/300] 	 loss = 0.6499
2023/10/16 23:53:37 - INFO - root -   train_accuracy = 0.6528
2023/10/16 23:54:14 - INFO - root -   Epoch: [4/300][0/84], lr: 0.00000012 	 loss = 0.5103(0.5103)
2023/10/16 23:55:42 - INFO - root -   Epoch: [4/300][20/84], lr: 0.00000012 	 loss = 0.8458(0.6260)
2023/10/16 23:57:28 - INFO - root -   Epoch: [4/300][40/84], lr: 0.00000012 	 loss = 0.6467(0.6444)
2023/10/16 23:58:42 - INFO - root -   Epoch: [4/300][60/84], lr: 0.00000012 	 loss = 0.7000(0.6487)
2023/10/16 23:59:46 - INFO - root -   Epoch: [4/300][80/84], lr: 0.00000012 	 loss = 0.6074(0.6483)
2023/10/16 23:59:48 - INFO - root -   Epoch: [4/300] 	 loss = 0.6464
2023/10/17 14:03:46 - INFO - root -   Num train examples = 169
2023/10/17 14:03:46 - INFO - root -   Num val examples = 43
2023/10/17 14:03:47 - INFO - root -   backend = nccl
2023/10/17 14:03:47 - INFO - root -   batch_size = 2
2023/10/17 14:03:47 - INFO - root -   dropout = 0.5
2023/10/17 14:03:47 - INFO - root -   epochs = 300
2023/10/17 14:03:47 - INFO - root -   eval_freq = 5
2023/10/17 14:03:47 - INFO - root -   focal_loss = False
2023/10/17 14:03:47 - INFO - root -   input_size = 224
2023/10/17 14:03:47 - INFO - root -   is_pretrained = False
2023/10/17 14:03:47 - INFO - root -   label_smooth = False
2023/10/17 14:03:47 - INFO - root -   local_rank = -1
2023/10/17 14:03:47 - INFO - root -   lr = 1e-05
2023/10/17 14:03:47 - INFO - root -   lr_decay_rate = 0.1
2023/10/17 14:03:47 - INFO - root -   lr_steps = [50, 100]
2023/10/17 14:03:47 - INFO - root -   lr_type = cosine
2023/10/17 14:03:47 - INFO - root -   model_depth = 34
2023/10/17 14:03:47 - INFO - root -   model_name = resnet50
2023/10/17 14:03:47 - INFO - root -   momentum = 0.9
2023/10/17 14:03:47 - INFO - root -   num_classes = 2
2023/10/17 14:03:47 - INFO - root -   output = ./tcia_outputs
2023/10/17 14:03:47 - INFO - root -   print_freq = 20
2023/10/17 14:03:47 - INFO - root -   resume = 
2023/10/17 14:03:47 - INFO - root -   start_epoch = 0
2023/10/17 14:03:47 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/17 14:03:47 - INFO - root -   tune_from = 
2023/10/17 14:03:47 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/17 14:03:47 - INFO - root -   warmup_epoch = 20
2023/10/17 14:03:47 - INFO - root -   warmup_multiplier = 100
2023/10/17 14:03:47 - INFO - root -   weight_decay = 0.0005
2023/10/17 14:03:47 - INFO - root -   workers = 8
2023/10/17 14:04:25 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 0.9195(0.9195)
2023/10/17 14:06:05 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 1.0973(0.8821)
2023/10/17 14:07:22 - INFO - root -   Num train examples = 169
2023/10/17 14:07:22 - INFO - root -   Num val examples = 43
2023/10/17 14:07:23 - INFO - root -   backend = nccl
2023/10/17 14:07:23 - INFO - root -   batch_size = 16
2023/10/17 14:07:23 - INFO - root -   dropout = 0.5
2023/10/17 14:07:23 - INFO - root -   epochs = 300
2023/10/17 14:07:23 - INFO - root -   eval_freq = 5
2023/10/17 14:07:23 - INFO - root -   focal_loss = False
2023/10/17 14:07:23 - INFO - root -   input_size = 224
2023/10/17 14:07:23 - INFO - root -   is_pretrained = False
2023/10/17 14:07:23 - INFO - root -   label_smooth = False
2023/10/17 14:07:23 - INFO - root -   local_rank = -1
2023/10/17 14:07:23 - INFO - root -   lr = 1e-05
2023/10/17 14:07:23 - INFO - root -   lr_decay_rate = 0.1
2023/10/17 14:07:23 - INFO - root -   lr_steps = [50, 100]
2023/10/17 14:07:23 - INFO - root -   lr_type = cosine
2023/10/17 14:07:23 - INFO - root -   model_depth = 34
2023/10/17 14:07:23 - INFO - root -   model_name = resnet50
2023/10/17 14:07:23 - INFO - root -   momentum = 0.9
2023/10/17 14:07:23 - INFO - root -   num_classes = 2
2023/10/17 14:07:23 - INFO - root -   output = ./tcia_outputs
2023/10/17 14:07:23 - INFO - root -   print_freq = 20
2023/10/17 14:07:23 - INFO - root -   resume = 
2023/10/17 14:07:23 - INFO - root -   start_epoch = 0
2023/10/17 14:07:23 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/17 14:07:23 - INFO - root -   tune_from = 
2023/10/17 14:07:23 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/17 14:07:23 - INFO - root -   warmup_epoch = 20
2023/10/17 14:07:23 - INFO - root -   warmup_multiplier = 100
2023/10/17 14:07:23 - INFO - root -   weight_decay = 0.0005
2023/10/17 14:07:23 - INFO - root -   workers = 8
2023/10/17 14:12:10 - INFO - root -   Epoch: [0/300][0/10], lr: 0.00000010 	 loss = 0.7108(0.7108)
2023/10/17 14:13:08 - INFO - root -   Epoch: [0/300] 	 loss = 0.7338
2023/10/17 14:13:08 - INFO - root -   train_accuracy = 0.5292
2023/10/17 14:17:43 - INFO - root -   Epoch: [1/300][0/10], lr: 0.00000015 	 loss = 0.7069(0.7069)
2023/10/17 14:18:57 - INFO - root -   Epoch: [1/300] 	 loss = 0.7283
2023/10/17 14:18:57 - INFO - root -   train_accuracy = 0.5229
2023/10/17 14:23:19 - INFO - root -   Epoch: [2/300][0/10], lr: 0.00000020 	 loss = 0.6960(0.6960)
2023/10/17 14:24:30 - INFO - root -   Epoch: [2/300] 	 loss = 0.7180
2023/10/17 14:24:30 - INFO - root -   train_accuracy = 0.5479
2023/10/17 14:28:42 - INFO - root -   Epoch: [3/300][0/10], lr: 0.00000025 	 loss = 0.6857(0.6857)
2023/10/17 14:30:13 - INFO - root -   Epoch: [3/300] 	 loss = 0.7132
2023/10/17 14:30:13 - INFO - root -   train_accuracy = 0.5479
2023/10/17 14:34:52 - INFO - root -   Epoch: [4/300][0/10], lr: 0.00000030 	 loss = 0.6664(0.6664)
2023/10/17 14:35:52 - INFO - root -   Epoch: [4/300] 	 loss = 0.7051
2023/10/17 14:37:12 - INFO - root -   precision = 0.5814
2023/10/17 14:37:12 - INFO - root -   eval_loss = 0.6770
2023/10/17 14:37:13 - INFO - root -   train_accuracy = 0.5292
2023/10/17 14:41:35 - INFO - root -   Epoch: [5/300][0/10], lr: 0.00000035 	 loss = 0.6660(0.6660)
2023/10/17 14:42:50 - INFO - root -   Epoch: [5/300] 	 loss = 0.6972
2023/10/17 14:42:50 - INFO - root -   train_accuracy = 0.5479
2023/10/17 14:47:17 - INFO - root -   Epoch: [6/300][0/10], lr: 0.00000040 	 loss = 0.6565(0.6565)
2023/10/17 14:48:40 - INFO - root -   Epoch: [6/300] 	 loss = 0.6903
2023/10/17 14:48:40 - INFO - root -   train_accuracy = 0.5417
2023/10/17 14:53:28 - INFO - root -   Epoch: [7/300][0/10], lr: 0.00000045 	 loss = 0.6487(0.6487)
2023/10/17 14:54:35 - INFO - root -   Epoch: [7/300] 	 loss = 0.6883
2023/10/17 14:54:35 - INFO - root -   train_accuracy = 0.5542
2023/10/17 14:58:40 - INFO - root -   Epoch: [8/300][0/10], lr: 0.00000050 	 loss = 0.6627(0.6627)
2023/10/17 15:00:22 - INFO - root -   Epoch: [8/300] 	 loss = 0.6814
2023/10/17 15:00:22 - INFO - root -   train_accuracy = 0.5917
2023/10/17 15:04:52 - INFO - root -   Epoch: [9/300][0/10], lr: 0.00000055 	 loss = 0.6536(0.6536)
2023/10/17 15:06:18 - INFO - root -   Epoch: [9/300] 	 loss = 0.6773
2023/10/17 15:07:34 - INFO - root -   precision = 0.6512
2023/10/17 15:07:34 - INFO - root -   eval_loss = 0.6546
2023/10/17 15:07:34 - INFO - root -   train_accuracy = 0.5896
2023/10/17 15:12:03 - INFO - root -   Epoch: [10/300][0/10], lr: 0.00000060 	 loss = 0.6427(0.6427)
2023/10/17 15:13:14 - INFO - root -   Epoch: [10/300] 	 loss = 0.6760
2023/10/17 15:13:14 - INFO - root -   train_accuracy = 0.5938
2023/10/17 15:17:45 - INFO - root -   Epoch: [11/300][0/10], lr: 0.00000064 	 loss = 0.6541(0.6541)
2023/10/17 15:18:56 - INFO - root -   Epoch: [11/300] 	 loss = 0.6714
2023/10/17 15:18:56 - INFO - root -   train_accuracy = 0.6125
2023/10/17 15:23:25 - INFO - root -   Epoch: [12/300][0/10], lr: 0.00000069 	 loss = 0.6406(0.6406)
2023/10/17 15:24:48 - INFO - root -   Epoch: [12/300] 	 loss = 0.6667
2023/10/17 15:24:48 - INFO - root -   train_accuracy = 0.6125
2023/10/17 15:29:36 - INFO - root -   Epoch: [13/300][0/10], lr: 0.00000074 	 loss = 0.6363(0.6363)
2023/10/17 15:30:47 - INFO - root -   Epoch: [13/300] 	 loss = 0.6642
2023/10/17 15:30:47 - INFO - root -   train_accuracy = 0.6375
2023/10/17 15:35:02 - INFO - root -   Epoch: [14/300][0/10], lr: 0.00000079 	 loss = 0.6395(0.6395)
2023/10/17 15:36:27 - INFO - root -   Epoch: [14/300] 	 loss = 0.6589
2023/10/17 15:37:42 - INFO - root -   precision = 0.6899
2023/10/17 15:37:42 - INFO - root -   eval_loss = 0.6445
2023/10/17 15:37:43 - INFO - root -   train_accuracy = 0.6333
2023/10/17 15:41:53 - INFO - root -   Epoch: [15/300][0/10], lr: 0.00000084 	 loss = 0.6532(0.6532)
2023/10/17 15:43:07 - INFO - root -   Epoch: [15/300] 	 loss = 0.6624
2023/10/17 15:43:07 - INFO - root -   train_accuracy = 0.6333
2023/10/17 15:47:25 - INFO - root -   Epoch: [16/300][0/10], lr: 0.00000089 	 loss = 0.6379(0.6379)
2023/10/17 15:48:58 - INFO - root -   Epoch: [16/300] 	 loss = 0.6589
2023/10/17 15:48:58 - INFO - root -   train_accuracy = 0.6250
2023/10/17 15:53:33 - INFO - root -   Epoch: [17/300][0/10], lr: 0.00000094 	 loss = 0.6245(0.6245)
2023/10/17 15:54:40 - INFO - root -   Epoch: [17/300] 	 loss = 0.6489
2023/10/17 15:54:40 - INFO - root -   train_accuracy = 0.6479
2023/10/17 15:59:20 - INFO - root -   Epoch: [18/300][0/10], lr: 0.00000099 	 loss = 0.6339(0.6339)
2023/10/17 16:00:22 - INFO - root -   Epoch: [18/300] 	 loss = 0.6514
2023/10/17 16:00:22 - INFO - root -   train_accuracy = 0.6562
2023/10/17 16:05:01 - INFO - root -   Epoch: [19/300][0/10], lr: 0.00000104 	 loss = 0.6352(0.6352)
2023/10/17 16:06:14 - INFO - root -   Epoch: [19/300] 	 loss = 0.6494
2023/10/17 16:07:30 - INFO - root -   precision = 0.6512
2023/10/17 16:07:30 - INFO - root -   eval_loss = 0.6398
2023/10/17 16:07:30 - INFO - root -   train_accuracy = 0.6583
2023/10/17 16:12:15 - INFO - root -   Epoch: [20/300][0/10], lr: 0.00000109 	 loss = 0.6305(0.6305)
2023/10/17 16:13:16 - INFO - root -   Epoch: [20/300] 	 loss = 0.6444
2023/10/17 16:13:16 - INFO - root -   train_accuracy = 0.6500
2023/10/17 16:17:59 - INFO - root -   Epoch: [21/300][0/10], lr: 0.00000114 	 loss = 0.6329(0.6329)
2023/10/17 16:19:06 - INFO - root -   Epoch: [21/300] 	 loss = 0.6432
2023/10/17 16:19:06 - INFO - root -   train_accuracy = 0.6542
2023/10/17 16:23:29 - INFO - root -   Epoch: [22/300][0/10], lr: 0.00000119 	 loss = 0.6369(0.6369)
2023/10/17 16:24:44 - INFO - root -   Epoch: [22/300] 	 loss = 0.6417
2023/10/17 16:24:44 - INFO - root -   train_accuracy = 0.6542
2023/10/17 16:28:57 - INFO - root -   Epoch: [23/300][0/10], lr: 0.00000124 	 loss = 0.6191(0.6191)
2023/10/17 16:30:13 - INFO - root -   Epoch: [23/300] 	 loss = 0.6356
2023/10/17 16:30:13 - INFO - root -   train_accuracy = 0.6604
2023/10/17 16:34:58 - INFO - root -   Epoch: [24/300][0/10], lr: 0.00000129 	 loss = 0.6203(0.6203)
2023/10/17 16:36:06 - INFO - root -   Epoch: [24/300] 	 loss = 0.6444
2023/10/17 16:37:22 - INFO - root -   precision = 0.6512
2023/10/17 16:37:22 - INFO - root -   eval_loss = 0.6428
2023/10/17 16:37:22 - INFO - root -   train_accuracy = 0.6583
2023/10/17 16:41:45 - INFO - root -   Epoch: [25/300][0/10], lr: 0.00000134 	 loss = 0.6150(0.6150)
2023/10/17 16:42:48 - INFO - root -   Epoch: [25/300] 	 loss = 0.6343
2023/10/17 16:42:48 - INFO - root -   train_accuracy = 0.6604
2023/10/17 16:47:36 - INFO - root -   Epoch: [26/300][0/10], lr: 0.00000139 	 loss = 0.6102(0.6102)
2023/10/17 16:48:44 - INFO - root -   Epoch: [26/300] 	 loss = 0.6352
2023/10/17 16:48:44 - INFO - root -   train_accuracy = 0.6583
2023/10/17 16:53:02 - INFO - root -   Epoch: [27/300][0/10], lr: 0.00000144 	 loss = 0.6128(0.6128)
2023/10/17 16:54:23 - INFO - root -   Epoch: [27/300] 	 loss = 0.6300
2023/10/17 16:54:23 - INFO - root -   train_accuracy = 0.6542
2023/10/17 16:58:52 - INFO - root -   Epoch: [28/300][0/10], lr: 0.00000149 	 loss = 0.6232(0.6232)
2023/10/17 17:00:08 - INFO - root -   Epoch: [28/300] 	 loss = 0.6330
2023/10/17 17:00:08 - INFO - root -   train_accuracy = 0.6583
2023/10/17 17:04:23 - INFO - root -   Epoch: [29/300][0/10], lr: 0.00000154 	 loss = 0.6227(0.6227)
2023/10/17 17:05:53 - INFO - root -   Epoch: [29/300] 	 loss = 0.6247
2023/10/17 17:07:09 - INFO - root -   precision = 0.6512
2023/10/17 17:07:09 - INFO - root -   eval_loss = 0.6436
2023/10/17 17:07:09 - INFO - root -   train_accuracy = 0.6625
2023/10/17 17:11:34 - INFO - root -   Epoch: [30/300][0/10], lr: 0.00000159 	 loss = 0.6236(0.6236)
2023/10/17 17:12:57 - INFO - root -   Epoch: [30/300] 	 loss = 0.6264
2023/10/17 17:12:57 - INFO - root -   train_accuracy = 0.6604
2023/10/17 17:17:29 - INFO - root -   Epoch: [31/300][0/10], lr: 0.00000163 	 loss = 0.5997(0.5997)
2023/10/17 17:18:39 - INFO - root -   Epoch: [31/300] 	 loss = 0.6273
2023/10/17 17:18:39 - INFO - root -   train_accuracy = 0.6604
2023/10/17 17:23:24 - INFO - root -   Epoch: [32/300][0/10], lr: 0.00000168 	 loss = 0.6234(0.6234)
2023/10/17 17:24:34 - INFO - root -   Epoch: [32/300] 	 loss = 0.6303
2023/10/17 17:24:34 - INFO - root -   train_accuracy = 0.6583
2023/10/17 17:29:21 - INFO - root -   Epoch: [33/300][0/10], lr: 0.00000173 	 loss = 0.6123(0.6123)
2023/10/17 17:30:27 - INFO - root -   Epoch: [33/300] 	 loss = 0.6271
2023/10/17 17:30:27 - INFO - root -   train_accuracy = 0.6583
2023/10/17 17:35:05 - INFO - root -   Epoch: [34/300][0/10], lr: 0.00000178 	 loss = 0.6232(0.6232)
2023/10/17 17:36:06 - INFO - root -   Epoch: [34/300] 	 loss = 0.6237
2023/10/17 17:37:22 - INFO - root -   precision = 0.6512
2023/10/17 17:37:22 - INFO - root -   eval_loss = 0.6428
2023/10/17 17:37:23 - INFO - root -   train_accuracy = 0.6646
2023/10/17 17:41:57 - INFO - root -   Epoch: [35/300][0/10], lr: 0.00000183 	 loss = 0.6238(0.6238)
2023/10/17 17:43:08 - INFO - root -   Epoch: [35/300] 	 loss = 0.6229
2023/10/17 17:43:08 - INFO - root -   train_accuracy = 0.6646
2023/10/17 17:47:43 - INFO - root -   Epoch: [36/300][0/10], lr: 0.00000188 	 loss = 0.6014(0.6014)
2023/10/17 17:48:53 - INFO - root -   Epoch: [36/300] 	 loss = 0.6237
2023/10/17 17:48:53 - INFO - root -   train_accuracy = 0.6562
2023/10/17 17:53:19 - INFO - root -   Epoch: [37/300][0/10], lr: 0.00000193 	 loss = 0.6031(0.6031)
2023/10/17 17:54:34 - INFO - root -   Epoch: [37/300] 	 loss = 0.6115
2023/10/17 17:54:34 - INFO - root -   train_accuracy = 0.6729
2023/10/17 17:59:16 - INFO - root -   Epoch: [38/300][0/10], lr: 0.00000198 	 loss = 0.5980(0.5980)
2023/10/17 18:00:30 - INFO - root -   Epoch: [38/300] 	 loss = 0.6245
2023/10/17 18:00:30 - INFO - root -   train_accuracy = 0.6667
2023/10/17 18:05:06 - INFO - root -   Epoch: [39/300][0/10], lr: 0.00000203 	 loss = 0.6398(0.6398)
2023/10/17 18:06:28 - INFO - root -   Epoch: [39/300] 	 loss = 0.6249
2023/10/17 18:07:43 - INFO - root -   precision = 0.6512
2023/10/17 18:07:43 - INFO - root -   eval_loss = 0.6486
2023/10/17 18:07:44 - INFO - root -   train_accuracy = 0.6625
2023/10/17 18:12:13 - INFO - root -   Epoch: [40/300][0/10], lr: 0.00000208 	 loss = 0.6222(0.6222)
2023/10/17 18:13:18 - INFO - root -   Epoch: [40/300] 	 loss = 0.6007
2023/10/17 18:13:18 - INFO - root -   train_accuracy = 0.6833
2023/10/17 18:17:40 - INFO - root -   Epoch: [41/300][0/10], lr: 0.00000213 	 loss = 0.5913(0.5913)
2023/10/17 18:18:55 - INFO - root -   Epoch: [41/300] 	 loss = 0.6177
2023/10/17 18:18:55 - INFO - root -   train_accuracy = 0.6604
2023/10/17 18:23:08 - INFO - root -   Epoch: [42/300][0/10], lr: 0.00000218 	 loss = 0.6053(0.6053)
2023/10/17 18:24:39 - INFO - root -   Epoch: [42/300] 	 loss = 0.6086
2023/10/17 18:24:39 - INFO - root -   train_accuracy = 0.6708
2023/10/17 18:28:50 - INFO - root -   Epoch: [43/300][0/10], lr: 0.00000223 	 loss = 0.5833(0.5833)
2023/10/17 18:30:23 - INFO - root -   Epoch: [43/300] 	 loss = 0.6173
2023/10/17 18:30:23 - INFO - root -   train_accuracy = 0.6646
2023/10/17 18:35:02 - INFO - root -   Epoch: [44/300][0/10], lr: 0.00000228 	 loss = 0.5806(0.5806)
2023/10/17 18:36:15 - INFO - root -   Epoch: [44/300] 	 loss = 0.6144
2023/10/17 18:37:30 - INFO - root -   precision = 0.6512
2023/10/17 18:37:30 - INFO - root -   eval_loss = 0.6467
2023/10/17 18:37:31 - INFO - root -   train_accuracy = 0.6687
2023/10/17 18:42:00 - INFO - root -   Epoch: [45/300][0/10], lr: 0.00000233 	 loss = 0.6344(0.6344)
2023/10/17 18:43:20 - INFO - root -   Epoch: [45/300] 	 loss = 0.6085
2023/10/17 18:43:20 - INFO - root -   train_accuracy = 0.6687
2023/10/17 18:47:54 - INFO - root -   Epoch: [46/300][0/10], lr: 0.00000238 	 loss = 0.5994(0.5994)
2023/10/17 18:49:07 - INFO - root -   Epoch: [46/300] 	 loss = 0.6001
2023/10/17 18:49:07 - INFO - root -   train_accuracy = 0.6792
2023/10/17 18:53:26 - INFO - root -   Epoch: [47/300][0/10], lr: 0.00000243 	 loss = 0.5768(0.5768)
2023/10/17 18:54:57 - INFO - root -   Epoch: [47/300] 	 loss = 0.6138
2023/10/17 18:54:57 - INFO - root -   train_accuracy = 0.6667
2023/10/17 18:59:02 - INFO - root -   Epoch: [48/300][0/10], lr: 0.00000248 	 loss = 0.6092(0.6092)
2023/10/17 19:00:34 - INFO - root -   Epoch: [48/300] 	 loss = 0.6228
2023/10/17 19:00:34 - INFO - root -   train_accuracy = 0.6646
2023/10/17 19:04:57 - INFO - root -   Epoch: [49/300][0/10], lr: 0.00000253 	 loss = 0.5744(0.5744)
2023/10/17 19:06:10 - INFO - root -   Epoch: [49/300] 	 loss = 0.5991
2023/10/17 19:07:26 - INFO - root -   precision = 0.6512
2023/10/17 19:07:26 - INFO - root -   eval_loss = 0.6510
2023/10/17 19:07:26 - INFO - root -   train_accuracy = 0.6750
2023/10/17 19:11:54 - INFO - root -   Epoch: [50/300][0/10], lr: 0.00000258 	 loss = 0.6133(0.6133)
2023/10/17 19:13:20 - INFO - root -   Epoch: [50/300] 	 loss = 0.6121
2023/10/17 19:13:20 - INFO - root -   train_accuracy = 0.6708
2023/10/17 19:17:45 - INFO - root -   Epoch: [51/300][0/10], lr: 0.00000262 	 loss = 0.5837(0.5837)
2023/10/17 19:18:49 - INFO - root -   Epoch: [51/300] 	 loss = 0.5795
2023/10/17 19:18:49 - INFO - root -   train_accuracy = 0.6854
2023/10/17 19:23:24 - INFO - root -   Epoch: [52/300][0/10], lr: 0.00000267 	 loss = 0.5902(0.5902)
2023/10/17 19:24:35 - INFO - root -   Epoch: [52/300] 	 loss = 0.6026
2023/10/17 19:24:35 - INFO - root -   train_accuracy = 0.6708
2023/10/17 19:29:10 - INFO - root -   Epoch: [53/300][0/10], lr: 0.00000272 	 loss = 0.5730(0.5730)
2023/10/17 19:30:18 - INFO - root -   Epoch: [53/300] 	 loss = 0.5937
2023/10/17 19:30:18 - INFO - root -   train_accuracy = 0.6896
2023/10/17 19:34:40 - INFO - root -   Epoch: [54/300][0/10], lr: 0.00000277 	 loss = 0.6000(0.6000)
2023/10/17 19:36:03 - INFO - root -   Epoch: [54/300] 	 loss = 0.6026
2023/10/17 19:37:18 - INFO - root -   precision = 0.6512
2023/10/17 19:37:18 - INFO - root -   eval_loss = 0.6513
2023/10/17 19:37:19 - INFO - root -   train_accuracy = 0.6750
2023/10/17 19:41:41 - INFO - root -   Epoch: [55/300][0/10], lr: 0.00000282 	 loss = 0.5626(0.5626)
2023/10/17 19:43:03 - INFO - root -   Epoch: [55/300] 	 loss = 0.5996
2023/10/17 19:43:03 - INFO - root -   train_accuracy = 0.6875
2023/10/17 19:47:31 - INFO - root -   Epoch: [56/300][0/10], lr: 0.00000287 	 loss = 0.5849(0.5849)
2023/10/17 19:48:48 - INFO - root -   Epoch: [56/300] 	 loss = 0.5885
2023/10/17 19:48:48 - INFO - root -   train_accuracy = 0.6833
2023/10/17 19:53:24 - INFO - root -   Epoch: [57/300][0/10], lr: 0.00000292 	 loss = 0.6063(0.6063)
2023/10/17 19:54:36 - INFO - root -   Epoch: [57/300] 	 loss = 0.6015
2023/10/17 19:54:36 - INFO - root -   train_accuracy = 0.6792
2023/10/17 19:59:17 - INFO - root -   Epoch: [58/300][0/10], lr: 0.00000297 	 loss = 0.6142(0.6142)
2023/10/17 20:00:27 - INFO - root -   Epoch: [58/300] 	 loss = 0.5826
2023/10/17 20:00:27 - INFO - root -   train_accuracy = 0.6896
2023/10/17 20:04:50 - INFO - root -   Epoch: [59/300][0/10], lr: 0.00000302 	 loss = 0.5733(0.5733)
2023/10/17 20:05:56 - INFO - root -   Epoch: [59/300] 	 loss = 0.5828
2023/10/17 20:07:12 - INFO - root -   precision = 0.6512
2023/10/17 20:07:12 - INFO - root -   eval_loss = 0.6494
2023/10/17 20:07:12 - INFO - root -   train_accuracy = 0.6958
2023/10/17 20:11:24 - INFO - root -   Epoch: [60/300][0/10], lr: 0.00000307 	 loss = 0.5950(0.5950)
2023/10/17 20:12:56 - INFO - root -   Epoch: [60/300] 	 loss = 0.5930
2023/10/17 20:12:56 - INFO - root -   train_accuracy = 0.6833
2023/10/17 20:17:23 - INFO - root -   Epoch: [61/300][0/10], lr: 0.00000312 	 loss = 0.5926(0.5926)
2023/10/17 20:18:42 - INFO - root -   Epoch: [61/300] 	 loss = 0.5919
2023/10/17 20:18:42 - INFO - root -   train_accuracy = 0.6750
2023/10/17 20:23:19 - INFO - root -   Epoch: [62/300][0/10], lr: 0.00000317 	 loss = 0.5707(0.5707)
2023/10/17 20:24:30 - INFO - root -   Epoch: [62/300] 	 loss = 0.5840
2023/10/17 20:24:30 - INFO - root -   train_accuracy = 0.7000
2023/10/17 20:29:02 - INFO - root -   Epoch: [63/300][0/10], lr: 0.00000322 	 loss = 0.5654(0.5654)
2023/10/17 20:30:15 - INFO - root -   Epoch: [63/300] 	 loss = 0.5832
2023/10/17 20:30:15 - INFO - root -   train_accuracy = 0.6958
2023/10/17 20:34:45 - INFO - root -   Epoch: [64/300][0/10], lr: 0.00000327 	 loss = 0.5927(0.5927)
2023/10/17 20:35:59 - INFO - root -   Epoch: [64/300] 	 loss = 0.5945
2023/10/17 20:37:14 - INFO - root -   precision = 0.6434
2023/10/17 20:37:14 - INFO - root -   eval_loss = 0.6554
2023/10/17 20:37:15 - INFO - root -   train_accuracy = 0.6854
2023/10/17 20:41:58 - INFO - root -   Epoch: [65/300][0/10], lr: 0.00000332 	 loss = 0.5574(0.5574)
2023/10/17 20:43:12 - INFO - root -   Epoch: [65/300] 	 loss = 0.5844
2023/10/17 20:43:12 - INFO - root -   train_accuracy = 0.6958
2023/10/17 20:47:41 - INFO - root -   Epoch: [66/300][0/10], lr: 0.00000337 	 loss = 0.5757(0.5757)
2023/10/17 20:49:01 - INFO - root -   Epoch: [66/300] 	 loss = 0.5854
2023/10/17 20:49:01 - INFO - root -   train_accuracy = 0.6958
2023/10/17 20:53:32 - INFO - root -   Epoch: [67/300][0/10], lr: 0.00000342 	 loss = 0.6012(0.6012)
2023/10/17 20:54:47 - INFO - root -   Epoch: [67/300] 	 loss = 0.5731
2023/10/17 20:54:47 - INFO - root -   train_accuracy = 0.7104
2023/10/17 20:59:15 - INFO - root -   Epoch: [68/300][0/10], lr: 0.00000347 	 loss = 0.5211(0.5211)
2023/10/17 21:00:46 - INFO - root -   Epoch: [68/300] 	 loss = 0.5691
2023/10/17 21:00:46 - INFO - root -   train_accuracy = 0.6979
2023/10/17 21:05:18 - INFO - root -   Epoch: [69/300][0/10], lr: 0.00000352 	 loss = 0.5721(0.5721)
2023/10/17 21:06:32 - INFO - root -   Epoch: [69/300] 	 loss = 0.5803
2023/10/17 21:07:49 - INFO - root -   precision = 0.5969
2023/10/17 21:07:49 - INFO - root -   eval_loss = 0.6811
2023/10/17 21:07:49 - INFO - root -   train_accuracy = 0.6875
2023/10/17 21:12:39 - INFO - root -   Epoch: [70/300][0/10], lr: 0.00000357 	 loss = 0.5578(0.5578)
2023/10/17 21:13:43 - INFO - root -   Epoch: [70/300] 	 loss = 0.5820
2023/10/17 21:13:43 - INFO - root -   train_accuracy = 0.7000
2023/10/17 21:17:59 - INFO - root -   Epoch: [71/300][0/10], lr: 0.00000361 	 loss = 0.5943(0.5943)
2023/10/17 21:19:44 - INFO - root -   Epoch: [71/300] 	 loss = 0.5671
2023/10/17 21:19:44 - INFO - root -   train_accuracy = 0.6979
2023/10/17 21:24:35 - INFO - root -   Epoch: [72/300][0/10], lr: 0.00000366 	 loss = 0.5836(0.5836)
2023/10/17 21:25:48 - INFO - root -   Epoch: [72/300] 	 loss = 0.5704
2023/10/17 21:25:48 - INFO - root -   train_accuracy = 0.7271
2023/10/17 21:30:15 - INFO - root -   Epoch: [73/300][0/10], lr: 0.00000371 	 loss = 0.5397(0.5397)
2023/10/17 21:31:25 - INFO - root -   Epoch: [73/300] 	 loss = 0.5497
2023/10/17 21:31:25 - INFO - root -   train_accuracy = 0.7208
2023/10/17 21:35:45 - INFO - root -   Epoch: [74/300][0/10], lr: 0.00000376 	 loss = 0.5255(0.5255)
2023/10/17 21:36:54 - INFO - root -   Epoch: [74/300] 	 loss = 0.5481
2023/10/17 21:38:10 - INFO - root -   precision = 0.5814
2023/10/17 21:38:10 - INFO - root -   eval_loss = 0.6606
2023/10/17 21:38:11 - INFO - root -   train_accuracy = 0.7167
2023/10/17 21:42:51 - INFO - root -   Epoch: [75/300][0/10], lr: 0.00000381 	 loss = 0.5306(0.5306)
2023/10/17 21:43:54 - INFO - root -   Epoch: [75/300] 	 loss = 0.5452
2023/10/17 21:43:54 - INFO - root -   train_accuracy = 0.7104
2023/10/17 21:48:27 - INFO - root -   Epoch: [76/300][0/10], lr: 0.00000386 	 loss = 0.5033(0.5033)
2023/10/17 21:49:35 - INFO - root -   Epoch: [76/300] 	 loss = 0.5726
2023/10/17 21:49:35 - INFO - root -   train_accuracy = 0.7021
2023/10/17 21:54:11 - INFO - root -   Epoch: [77/300][0/10], lr: 0.00000391 	 loss = 0.5834(0.5834)
2023/10/17 21:55:25 - INFO - root -   Epoch: [77/300] 	 loss = 0.5563
2023/10/17 21:55:25 - INFO - root -   train_accuracy = 0.7021
2023/10/17 21:59:43 - INFO - root -   Epoch: [78/300][0/10], lr: 0.00000396 	 loss = 0.5051(0.5051)
2023/10/17 22:01:13 - INFO - root -   Epoch: [78/300] 	 loss = 0.5550
2023/10/17 22:01:13 - INFO - root -   train_accuracy = 0.7146
2023/10/17 22:05:29 - INFO - root -   Epoch: [79/300][0/10], lr: 0.00000401 	 loss = 0.5460(0.5460)
2023/10/17 22:07:07 - INFO - root -   Epoch: [79/300] 	 loss = 0.5723
2023/10/17 22:08:23 - INFO - root -   precision = 0.5891
2023/10/17 22:08:23 - INFO - root -   eval_loss = 0.7012
2023/10/17 22:08:24 - INFO - root -   train_accuracy = 0.6896
2023/10/17 22:12:39 - INFO - root -   Epoch: [80/300][0/10], lr: 0.00000406 	 loss = 0.5268(0.5268)
2023/10/17 22:13:59 - INFO - root -   Epoch: [80/300] 	 loss = 0.5342
2023/10/17 22:13:59 - INFO - root -   train_accuracy = 0.7438
2023/10/17 22:18:20 - INFO - root -   Epoch: [81/300][0/10], lr: 0.00000411 	 loss = 0.4837(0.4837)
2023/10/17 22:19:43 - INFO - root -   Epoch: [81/300] 	 loss = 0.5384
2023/10/17 22:19:43 - INFO - root -   train_accuracy = 0.7625
2023/10/17 22:24:19 - INFO - root -   Epoch: [82/300][0/10], lr: 0.00000416 	 loss = 0.4992(0.4992)
2023/10/17 22:25:25 - INFO - root -   Epoch: [82/300] 	 loss = 0.5491
2023/10/17 22:25:25 - INFO - root -   train_accuracy = 0.7250
2023/10/17 22:29:39 - INFO - root -   Epoch: [83/300][0/10], lr: 0.00000421 	 loss = 0.5282(0.5282)
2023/10/17 22:31:20 - INFO - root -   Epoch: [83/300] 	 loss = 0.5478
2023/10/17 22:31:20 - INFO - root -   train_accuracy = 0.7271
2023/10/17 22:35:38 - INFO - root -   Epoch: [84/300][0/10], lr: 0.00000426 	 loss = 0.4856(0.4856)
2023/10/17 22:36:51 - INFO - root -   Epoch: [84/300] 	 loss = 0.5059
2023/10/17 22:38:07 - INFO - root -   precision = 0.5969
2023/10/17 22:38:07 - INFO - root -   eval_loss = 0.6862
2023/10/17 22:38:07 - INFO - root -   train_accuracy = 0.7458
2023/10/17 22:42:19 - INFO - root -   Epoch: [85/300][0/10], lr: 0.00000431 	 loss = 0.4250(0.4250)
2023/10/17 22:43:55 - INFO - root -   Epoch: [85/300] 	 loss = 0.5171
2023/10/17 22:43:55 - INFO - root -   train_accuracy = 0.7583
2023/10/17 22:48:17 - INFO - root -   Epoch: [86/300][0/10], lr: 0.00000436 	 loss = 0.4593(0.4593)
2023/10/17 22:49:31 - INFO - root -   Epoch: [86/300] 	 loss = 0.5139
2023/10/17 22:49:31 - INFO - root -   train_accuracy = 0.7542
2023/10/17 22:54:01 - INFO - root -   Epoch: [87/300][0/10], lr: 0.00000441 	 loss = 0.4552(0.4552)
2023/10/17 22:55:15 - INFO - root -   Epoch: [87/300] 	 loss = 0.5041
2023/10/17 22:55:15 - INFO - root -   train_accuracy = 0.7375
2023/10/17 22:59:50 - INFO - root -   Epoch: [88/300][0/10], lr: 0.00000446 	 loss = 0.5623(0.5623)
2023/10/17 23:00:59 - INFO - root -   Epoch: [88/300] 	 loss = 0.5085
2023/10/17 23:00:59 - INFO - root -   train_accuracy = 0.7625
2023/10/17 23:05:18 - INFO - root -   Epoch: [89/300][0/10], lr: 0.00000451 	 loss = 0.4021(0.4021)
2023/10/17 23:06:46 - INFO - root -   Epoch: [89/300] 	 loss = 0.5199
2023/10/17 23:08:01 - INFO - root -   precision = 0.5736
2023/10/17 23:08:01 - INFO - root -   eval_loss = 0.7111
2023/10/17 23:08:02 - INFO - root -   train_accuracy = 0.7417
2023/10/17 23:12:23 - INFO - root -   Epoch: [90/300][0/10], lr: 0.00000456 	 loss = 0.4604(0.4604)
2023/10/17 23:13:42 - INFO - root -   Epoch: [90/300] 	 loss = 0.5233
2023/10/17 23:13:42 - INFO - root -   train_accuracy = 0.7417
2023/10/17 23:18:24 - INFO - root -   Epoch: [91/300][0/10], lr: 0.00000460 	 loss = 0.4714(0.4714)
2023/10/17 23:19:25 - INFO - root -   Epoch: [91/300] 	 loss = 0.5148
2023/10/17 23:19:25 - INFO - root -   train_accuracy = 0.7333
2023/10/17 23:23:51 - INFO - root -   Epoch: [92/300][0/10], lr: 0.00000465 	 loss = 0.4884(0.4884)
2023/10/17 23:25:03 - INFO - root -   Epoch: [92/300] 	 loss = 0.4945
2023/10/17 23:25:03 - INFO - root -   train_accuracy = 0.7708
2023/10/17 23:29:24 - INFO - root -   Epoch: [93/300][0/10], lr: 0.00000470 	 loss = 0.3639(0.3639)
2023/10/17 23:30:43 - INFO - root -   Epoch: [93/300] 	 loss = 0.4707
2023/10/17 23:30:43 - INFO - root -   train_accuracy = 0.7750
2023/10/17 23:34:54 - INFO - root -   Epoch: [94/300][0/10], lr: 0.00000475 	 loss = 0.4697(0.4697)
2023/10/17 23:36:18 - INFO - root -   Epoch: [94/300] 	 loss = 0.4708
2023/10/17 23:37:34 - INFO - root -   precision = 0.5891
2023/10/17 23:37:34 - INFO - root -   eval_loss = 0.7433
2023/10/17 23:37:34 - INFO - root -   train_accuracy = 0.7771
2023/10/17 23:41:45 - INFO - root -   Epoch: [95/300][0/10], lr: 0.00000480 	 loss = 0.4153(0.4153)
2023/10/17 23:43:16 - INFO - root -   Epoch: [95/300] 	 loss = 0.4982
2023/10/17 23:43:16 - INFO - root -   train_accuracy = 0.7292
2023/10/17 23:47:53 - INFO - root -   Epoch: [96/300][0/10], lr: 0.00000485 	 loss = 0.4524(0.4524)
2023/10/17 23:48:53 - INFO - root -   Epoch: [96/300] 	 loss = 0.5071
2023/10/17 23:48:53 - INFO - root -   train_accuracy = 0.7562
2023/10/17 23:53:24 - INFO - root -   Epoch: [97/300][0/10], lr: 0.00000490 	 loss = 0.5603(0.5603)
2023/10/17 23:54:29 - INFO - root -   Epoch: [97/300] 	 loss = 0.4910
2023/10/17 23:54:29 - INFO - root -   train_accuracy = 0.7708
2023/10/17 23:59:09 - INFO - root -   Epoch: [98/300][0/10], lr: 0.00000495 	 loss = 0.3961(0.3961)
2023/10/18 00:00:15 - INFO - root -   Epoch: [98/300] 	 loss = 0.4541
2023/10/18 00:00:15 - INFO - root -   train_accuracy = 0.7812
2023/10/18 00:04:51 - INFO - root -   Epoch: [99/300][0/10], lr: 0.00000500 	 loss = 0.5999(0.5999)
2023/10/18 00:05:57 - INFO - root -   Epoch: [99/300] 	 loss = 0.4457
2023/10/18 00:07:12 - INFO - root -   precision = 0.5814
2023/10/18 00:07:12 - INFO - root -   eval_loss = 0.7719
2023/10/18 00:07:12 - INFO - root -   train_accuracy = 0.7812
2023/10/18 00:11:29 - INFO - root -   Epoch: [100/300][0/10], lr: 0.00000505 	 loss = 0.4397(0.4397)
2023/10/18 00:13:04 - INFO - root -   Epoch: [100/300] 	 loss = 0.4687
2023/10/18 00:13:04 - INFO - root -   train_accuracy = 0.7479
2023/10/18 00:17:17 - INFO - root -   Epoch: [101/300][0/10], lr: 0.00000510 	 loss = 0.2957(0.2957)
2023/10/18 00:18:42 - INFO - root -   Epoch: [101/300] 	 loss = 0.4419
2023/10/18 00:18:42 - INFO - root -   train_accuracy = 0.7896
2023/10/18 00:23:04 - INFO - root -   Epoch: [102/300][0/10], lr: 0.00000515 	 loss = 0.3929(0.3929)
2023/10/18 00:24:25 - INFO - root -   Epoch: [102/300] 	 loss = 0.4462
2023/10/18 00:24:25 - INFO - root -   train_accuracy = 0.8021
2023/10/18 00:28:39 - INFO - root -   Epoch: [103/300][0/10], lr: 0.00000520 	 loss = 0.5231(0.5231)
2023/10/18 00:30:03 - INFO - root -   Epoch: [103/300] 	 loss = 0.4565
2023/10/18 00:30:03 - INFO - root -   train_accuracy = 0.8000
2023/10/18 00:34:39 - INFO - root -   Epoch: [104/300][0/10], lr: 0.00000525 	 loss = 0.4283(0.4283)
2023/10/18 00:35:55 - INFO - root -   Epoch: [104/300] 	 loss = 0.4893
2023/10/18 00:37:10 - INFO - root -   precision = 0.5271
2023/10/18 00:37:10 - INFO - root -   eval_loss = 0.7616
2023/10/18 00:37:11 - INFO - root -   train_accuracy = 0.7521
2023/10/18 00:41:59 - INFO - root -   Epoch: [105/300][0/10], lr: 0.00000530 	 loss = 0.4146(0.4146)
2023/10/18 00:42:59 - INFO - root -   Epoch: [105/300] 	 loss = 0.4413
2023/10/18 00:42:59 - INFO - root -   train_accuracy = 0.7958
2023/10/18 00:47:18 - INFO - root -   Epoch: [106/300][0/10], lr: 0.00000535 	 loss = 0.3521(0.3521)
2023/10/18 00:48:36 - INFO - root -   Epoch: [106/300] 	 loss = 0.4013
2023/10/18 00:48:36 - INFO - root -   train_accuracy = 0.8125
2023/10/18 00:53:01 - INFO - root -   Epoch: [107/300][0/10], lr: 0.00000540 	 loss = 0.3277(0.3277)
2023/10/18 00:54:16 - INFO - root -   Epoch: [107/300] 	 loss = 0.4445
2023/10/18 00:54:16 - INFO - root -   train_accuracy = 0.8104
2023/10/18 00:58:36 - INFO - root -   Epoch: [108/300][0/10], lr: 0.00000545 	 loss = 0.3278(0.3278)
2023/10/18 00:59:50 - INFO - root -   Epoch: [108/300] 	 loss = 0.4120
2023/10/18 00:59:50 - INFO - root -   train_accuracy = 0.7875
2023/10/18 01:04:16 - INFO - root -   Epoch: [109/300][0/10], lr: 0.00000550 	 loss = 0.5711(0.5711)
2023/10/18 01:05:29 - INFO - root -   Epoch: [109/300] 	 loss = 0.4149
2023/10/18 01:06:45 - INFO - root -   precision = 0.5349
2023/10/18 01:06:45 - INFO - root -   eval_loss = 0.9136
2023/10/18 01:06:46 - INFO - root -   train_accuracy = 0.8063
2023/10/18 01:11:17 - INFO - root -   Epoch: [110/300][0/10], lr: 0.00000555 	 loss = 0.3309(0.3309)
2023/10/18 01:12:29 - INFO - root -   Epoch: [110/300] 	 loss = 0.4317
2023/10/18 01:12:29 - INFO - root -   train_accuracy = 0.8042
2023/10/18 01:17:00 - INFO - root -   Epoch: [111/300][0/10], lr: 0.00000559 	 loss = 0.3237(0.3237)
2023/10/18 01:18:14 - INFO - root -   Epoch: [111/300] 	 loss = 0.4483
2023/10/18 01:18:14 - INFO - root -   train_accuracy = 0.7771
2023/10/18 01:22:53 - INFO - root -   Epoch: [112/300][0/10], lr: 0.00000564 	 loss = 0.3287(0.3287)
2023/10/18 01:23:53 - INFO - root -   Epoch: [112/300] 	 loss = 0.4247
2023/10/18 01:23:53 - INFO - root -   train_accuracy = 0.7750
2023/10/18 01:28:28 - INFO - root -   Epoch: [113/300][0/10], lr: 0.00000569 	 loss = 0.2778(0.2778)
2023/10/18 01:29:36 - INFO - root -   Epoch: [113/300] 	 loss = 0.3551
2023/10/18 01:29:36 - INFO - root -   train_accuracy = 0.8500
2023/10/18 01:34:01 - INFO - root -   Epoch: [114/300][0/10], lr: 0.00000574 	 loss = 0.2669(0.2669)
2023/10/18 01:35:21 - INFO - root -   Epoch: [114/300] 	 loss = 0.3817
2023/10/18 01:36:37 - INFO - root -   precision = 0.5581
2023/10/18 01:36:37 - INFO - root -   eval_loss = 0.8643
2023/10/18 01:36:37 - INFO - root -   train_accuracy = 0.8229
2023/10/18 01:41:07 - INFO - root -   Epoch: [115/300][0/10], lr: 0.00000579 	 loss = 0.2904(0.2904)
2023/10/18 01:42:26 - INFO - root -   Epoch: [115/300] 	 loss = 0.4557
2023/10/18 01:42:26 - INFO - root -   train_accuracy = 0.7729
2023/10/18 01:46:54 - INFO - root -   Epoch: [116/300][0/10], lr: 0.00000584 	 loss = 0.3609(0.3609)
2023/10/18 01:48:19 - INFO - root -   Epoch: [116/300] 	 loss = 0.4457
2023/10/18 01:48:19 - INFO - root -   train_accuracy = 0.8021
2023/10/18 01:52:24 - INFO - root -   Epoch: [117/300][0/10], lr: 0.00000589 	 loss = 0.2959(0.2959)
2023/10/18 01:53:52 - INFO - root -   Epoch: [117/300] 	 loss = 0.3296
2023/10/18 01:53:52 - INFO - root -   train_accuracy = 0.8688
2023/10/18 01:58:15 - INFO - root -   Epoch: [118/300][0/10], lr: 0.00000594 	 loss = 0.3172(0.3172)
2023/10/18 01:59:41 - INFO - root -   Epoch: [118/300] 	 loss = 0.3686
2023/10/18 01:59:41 - INFO - root -   train_accuracy = 0.8250
2023/10/18 02:03:43 - INFO - root -   Epoch: [119/300][0/10], lr: 0.00000599 	 loss = 0.3122(0.3122)
2023/10/18 02:05:09 - INFO - root -   Epoch: [119/300] 	 loss = 0.3606
2023/10/18 02:06:24 - INFO - root -   precision = 0.5736
2023/10/18 02:06:24 - INFO - root -   eval_loss = 0.9070
2023/10/18 02:06:25 - INFO - root -   train_accuracy = 0.8542
2023/10/18 02:11:04 - INFO - root -   Epoch: [120/300][0/10], lr: 0.00000604 	 loss = 0.6171(0.6171)
2023/10/18 02:12:18 - INFO - root -   Epoch: [120/300] 	 loss = 0.4228
2023/10/18 02:12:18 - INFO - root -   train_accuracy = 0.7792
2023/10/18 02:16:47 - INFO - root -   Epoch: [121/300][0/10], lr: 0.00000609 	 loss = 0.3310(0.3310)
2023/10/18 02:17:51 - INFO - root -   Epoch: [121/300] 	 loss = 0.3113
2023/10/18 02:17:51 - INFO - root -   train_accuracy = 0.8625
2023/10/18 02:22:17 - INFO - root -   Epoch: [122/300][0/10], lr: 0.00000614 	 loss = 0.2796(0.2796)
2023/10/18 02:23:34 - INFO - root -   Epoch: [122/300] 	 loss = 0.3369
2023/10/18 02:23:34 - INFO - root -   train_accuracy = 0.8458
2023/10/18 02:28:09 - INFO - root -   Epoch: [123/300][0/10], lr: 0.00000619 	 loss = 0.2178(0.2178)
2023/10/18 02:29:33 - INFO - root -   Epoch: [123/300] 	 loss = 0.4611
2023/10/18 02:29:33 - INFO - root -   train_accuracy = 0.7875
2023/10/18 02:33:50 - INFO - root -   Epoch: [124/300][0/10], lr: 0.00000624 	 loss = 0.2563(0.2563)
2023/10/18 02:35:20 - INFO - root -   Epoch: [124/300] 	 loss = 0.4308
2023/10/18 02:36:35 - INFO - root -   precision = 0.6589
2023/10/18 02:36:35 - INFO - root -   eval_loss = 0.9421
2023/10/18 02:36:36 - INFO - root -   train_accuracy = 0.7937
2023/10/18 02:41:10 - INFO - root -   Epoch: [125/300][0/10], lr: 0.00000629 	 loss = 0.4549(0.4549)
2023/10/18 02:42:30 - INFO - root -   Epoch: [125/300] 	 loss = 0.3897
2023/10/18 02:42:30 - INFO - root -   train_accuracy = 0.8187
2023/10/18 02:47:10 - INFO - root -   Epoch: [126/300][0/10], lr: 0.00000634 	 loss = 0.4458(0.4458)
2023/10/18 02:48:17 - INFO - root -   Epoch: [126/300] 	 loss = 0.3760
2023/10/18 02:48:17 - INFO - root -   train_accuracy = 0.8250
2023/10/18 02:52:52 - INFO - root -   Epoch: [127/300][0/10], lr: 0.00000639 	 loss = 0.3320(0.3320)
2023/10/18 02:54:00 - INFO - root -   Epoch: [127/300] 	 loss = 0.3773
2023/10/18 02:54:00 - INFO - root -   train_accuracy = 0.8187
2023/10/18 02:58:22 - INFO - root -   Epoch: [128/300][0/10], lr: 0.00000644 	 loss = 0.1952(0.1952)
2023/10/18 02:59:37 - INFO - root -   Epoch: [128/300] 	 loss = 0.3390
2023/10/18 02:59:37 - INFO - root -   train_accuracy = 0.8542
2023/10/18 03:04:16 - INFO - root -   Epoch: [129/300][0/10], lr: 0.00000649 	 loss = 0.5902(0.5902)
2023/10/18 03:05:25 - INFO - root -   Epoch: [129/300] 	 loss = 0.4105
2023/10/18 03:06:41 - INFO - root -   precision = 0.5814
2023/10/18 03:06:41 - INFO - root -   eval_loss = 1.0474
2023/10/18 03:06:42 - INFO - root -   train_accuracy = 0.8042
2023/10/18 03:10:51 - INFO - root -   Epoch: [130/300][0/10], lr: 0.00000654 	 loss = 0.2467(0.2467)
2023/10/18 03:12:11 - INFO - root -   Epoch: [130/300] 	 loss = 0.3172
2023/10/18 03:12:11 - INFO - root -   train_accuracy = 0.8583
2023/10/18 03:16:50 - INFO - root -   Epoch: [131/300][0/10], lr: 0.00000658 	 loss = 0.3875(0.3875)
2023/10/18 03:17:55 - INFO - root -   Epoch: [131/300] 	 loss = 0.3404
2023/10/18 03:17:55 - INFO - root -   train_accuracy = 0.8479
2023/10/18 03:22:23 - INFO - root -   Epoch: [132/300][0/10], lr: 0.00000663 	 loss = 0.2798(0.2798)
2023/10/18 03:23:35 - INFO - root -   Epoch: [132/300] 	 loss = 0.3396
2023/10/18 03:23:35 - INFO - root -   train_accuracy = 0.8500
2023/10/18 03:28:15 - INFO - root -   Epoch: [133/300][0/10], lr: 0.00000668 	 loss = 0.2575(0.2575)
2023/10/18 03:29:20 - INFO - root -   Epoch: [133/300] 	 loss = 0.3482
2023/10/18 03:29:20 - INFO - root -   train_accuracy = 0.8333
2023/10/18 03:33:54 - INFO - root -   Epoch: [134/300][0/10], lr: 0.00000673 	 loss = 0.3914(0.3914)
2023/10/18 03:35:01 - INFO - root -   Epoch: [134/300] 	 loss = 0.3494
2023/10/18 03:36:16 - INFO - root -   precision = 0.6357
2023/10/18 03:36:16 - INFO - root -   eval_loss = 0.8497
2023/10/18 03:36:17 - INFO - root -   train_accuracy = 0.8292
2023/10/18 03:40:20 - INFO - root -   Epoch: [135/300][0/10], lr: 0.00000678 	 loss = 0.2066(0.2066)
2023/10/18 03:41:45 - INFO - root -   Epoch: [135/300] 	 loss = 0.2654
2023/10/18 03:41:45 - INFO - root -   train_accuracy = 0.8812
2023/10/18 03:46:30 - INFO - root -   Epoch: [136/300][0/10], lr: 0.00000683 	 loss = 0.6612(0.6612)
2023/10/18 03:47:38 - INFO - root -   Epoch: [136/300] 	 loss = 0.3922
2023/10/18 03:47:38 - INFO - root -   train_accuracy = 0.8083
2023/10/18 03:52:02 - INFO - root -   Epoch: [137/300][0/10], lr: 0.00000688 	 loss = 0.2320(0.2320)
2023/10/18 03:53:26 - INFO - root -   Epoch: [137/300] 	 loss = 0.4057
2023/10/18 03:53:26 - INFO - root -   train_accuracy = 0.8167
2023/10/18 03:57:56 - INFO - root -   Epoch: [138/300][0/10], lr: 0.00000693 	 loss = 0.3392(0.3392)
2023/10/18 03:59:16 - INFO - root -   Epoch: [138/300] 	 loss = 0.3416
2023/10/18 03:59:16 - INFO - root -   train_accuracy = 0.8375
2023/10/18 04:03:37 - INFO - root -   Epoch: [139/300][0/10], lr: 0.00000698 	 loss = 0.2588(0.2588)
2023/10/18 04:05:00 - INFO - root -   Epoch: [139/300] 	 loss = 0.3762
2023/10/18 04:06:16 - INFO - root -   precision = 0.4961
2023/10/18 04:06:16 - INFO - root -   eval_loss = 1.0322
2023/10/18 04:06:17 - INFO - root -   train_accuracy = 0.8021
2023/10/18 04:10:56 - INFO - root -   Epoch: [140/300][0/10], lr: 0.00000703 	 loss = 0.7260(0.7260)
2023/10/18 04:12:06 - INFO - root -   Epoch: [140/300] 	 loss = 0.4325
2023/10/18 04:12:06 - INFO - root -   train_accuracy = 0.7750
2023/10/18 04:16:49 - INFO - root -   Epoch: [141/300][0/10], lr: 0.00000708 	 loss = 0.7633(0.7633)
2023/10/18 04:17:59 - INFO - root -   Epoch: [141/300] 	 loss = 0.4352
2023/10/18 04:17:59 - INFO - root -   train_accuracy = 0.7771
2023/10/18 04:22:18 - INFO - root -   Epoch: [142/300][0/10], lr: 0.00000713 	 loss = 0.2338(0.2338)
2023/10/18 04:23:45 - INFO - root -   Epoch: [142/300] 	 loss = 0.3521
2023/10/18 04:23:45 - INFO - root -   train_accuracy = 0.8396
2023/10/18 04:28:21 - INFO - root -   Epoch: [143/300][0/10], lr: 0.00000718 	 loss = 0.3197(0.3197)
2023/10/18 04:29:23 - INFO - root -   Epoch: [143/300] 	 loss = 0.3854
2023/10/18 04:29:23 - INFO - root -   train_accuracy = 0.8104
2023/10/18 04:33:32 - INFO - root -   Epoch: [144/300][0/10], lr: 0.00000723 	 loss = 0.1861(0.1861)
2023/10/18 04:34:52 - INFO - root -   Epoch: [144/300] 	 loss = 0.3251
2023/10/18 04:36:07 - INFO - root -   precision = 0.6279
2023/10/18 04:36:07 - INFO - root -   eval_loss = 0.9925
2023/10/18 04:36:08 - INFO - root -   train_accuracy = 0.8562
2023/10/18 04:40:38 - INFO - root -   Epoch: [145/300][0/10], lr: 0.00000728 	 loss = 0.4603(0.4603)
2023/10/18 04:41:48 - INFO - root -   Epoch: [145/300] 	 loss = 0.3834
2023/10/18 04:41:48 - INFO - root -   train_accuracy = 0.8187
2023/10/18 04:46:22 - INFO - root -   Epoch: [146/300][0/10], lr: 0.00000733 	 loss = 0.1707(0.1707)
2023/10/18 04:47:39 - INFO - root -   Epoch: [146/300] 	 loss = 0.3263
2023/10/18 04:47:39 - INFO - root -   train_accuracy = 0.8438
2023/10/18 04:51:54 - INFO - root -   Epoch: [147/300][0/10], lr: 0.00000738 	 loss = 0.4003(0.4003)
2023/10/18 04:53:19 - INFO - root -   Epoch: [147/300] 	 loss = 0.3424
2023/10/18 04:53:19 - INFO - root -   train_accuracy = 0.8417
2023/10/18 04:57:47 - INFO - root -   Epoch: [148/300][0/10], lr: 0.00000743 	 loss = 0.1879(0.1879)
2023/10/18 04:59:02 - INFO - root -   Epoch: [148/300] 	 loss = 0.2770
2023/10/18 04:59:02 - INFO - root -   train_accuracy = 0.8833
2023/10/18 05:03:29 - INFO - root -   Epoch: [149/300][0/10], lr: 0.00000748 	 loss = 0.3254(0.3254)
2023/10/18 05:04:43 - INFO - root -   Epoch: [149/300] 	 loss = 0.3255
2023/10/18 05:05:59 - INFO - root -   precision = 0.5504
2023/10/18 05:05:59 - INFO - root -   eval_loss = 1.1063
2023/10/18 05:05:59 - INFO - root -   train_accuracy = 0.8438
2023/10/18 05:10:29 - INFO - root -   Epoch: [150/300][0/10], lr: 0.00000753 	 loss = 0.2680(0.2680)
2023/10/18 05:11:47 - INFO - root -   Epoch: [150/300] 	 loss = 0.3320
2023/10/18 05:11:47 - INFO - root -   train_accuracy = 0.8354
2023/10/18 05:16:19 - INFO - root -   Epoch: [151/300][0/10], lr: 0.00000757 	 loss = 0.1892(0.1892)
2023/10/18 05:17:31 - INFO - root -   Epoch: [151/300] 	 loss = 0.3084
2023/10/18 05:17:31 - INFO - root -   train_accuracy = 0.8625
2023/10/18 05:22:00 - INFO - root -   Epoch: [152/300][0/10], lr: 0.00000762 	 loss = 0.2212(0.2212)
2023/10/18 05:23:20 - INFO - root -   Epoch: [152/300] 	 loss = 0.3135
2023/10/18 05:23:20 - INFO - root -   train_accuracy = 0.8479
2023/10/18 05:27:46 - INFO - root -   Epoch: [153/300][0/10], lr: 0.00000767 	 loss = 0.2477(0.2477)
2023/10/18 05:29:08 - INFO - root -   Epoch: [153/300] 	 loss = 0.3293
2023/10/18 05:29:08 - INFO - root -   train_accuracy = 0.8417
2023/10/18 05:33:48 - INFO - root -   Epoch: [154/300][0/10], lr: 0.00000772 	 loss = 0.3927(0.3927)
2023/10/18 05:35:02 - INFO - root -   Epoch: [154/300] 	 loss = 0.3571
2023/10/18 05:36:17 - INFO - root -   precision = 0.5891
2023/10/18 05:36:17 - INFO - root -   eval_loss = 0.9808
2023/10/18 05:36:18 - INFO - root -   train_accuracy = 0.8333
2023/10/18 05:40:45 - INFO - root -   Epoch: [155/300][0/10], lr: 0.00000777 	 loss = 0.2765(0.2765)
2023/10/18 05:42:14 - INFO - root -   Epoch: [155/300] 	 loss = 0.3956
2023/10/18 05:42:14 - INFO - root -   train_accuracy = 0.8000
2023/10/18 05:46:46 - INFO - root -   Epoch: [156/300][0/10], lr: 0.00000782 	 loss = 0.2994(0.2994)
2023/10/18 05:47:57 - INFO - root -   Epoch: [156/300] 	 loss = 0.3519
2023/10/18 05:47:57 - INFO - root -   train_accuracy = 0.8250
2023/10/18 05:52:10 - INFO - root -   Epoch: [157/300][0/10], lr: 0.00000787 	 loss = 0.4249(0.4249)
2023/10/18 05:53:30 - INFO - root -   Epoch: [157/300] 	 loss = 0.2870
2023/10/18 05:53:30 - INFO - root -   train_accuracy = 0.8708
2023/10/18 05:57:56 - INFO - root -   Epoch: [158/300][0/10], lr: 0.00000792 	 loss = 0.1952(0.1952)
2023/10/18 05:59:20 - INFO - root -   Epoch: [158/300] 	 loss = 0.3229
2023/10/18 05:59:20 - INFO - root -   train_accuracy = 0.8458
2023/10/18 06:03:47 - INFO - root -   Epoch: [159/300][0/10], lr: 0.00000797 	 loss = 0.2686(0.2686)
2023/10/18 06:05:02 - INFO - root -   Epoch: [159/300] 	 loss = 0.3102
2023/10/18 06:06:18 - INFO - root -   precision = 0.6047
2023/10/18 06:06:18 - INFO - root -   eval_loss = 1.2005
2023/10/18 06:06:19 - INFO - root -   train_accuracy = 0.8604
2023/10/18 06:10:49 - INFO - root -   Epoch: [160/300][0/10], lr: 0.00000802 	 loss = 0.2756(0.2756)
2023/10/18 06:12:00 - INFO - root -   Epoch: [160/300] 	 loss = 0.2303
2023/10/18 06:12:00 - INFO - root -   train_accuracy = 0.9021
2023/10/18 06:16:34 - INFO - root -   Epoch: [161/300][0/10], lr: 0.00000807 	 loss = 0.2800(0.2800)
2023/10/18 06:17:46 - INFO - root -   Epoch: [161/300] 	 loss = 0.2694
2023/10/18 06:17:46 - INFO - root -   train_accuracy = 0.8792
2023/10/18 06:22:02 - INFO - root -   Epoch: [162/300][0/10], lr: 0.00000812 	 loss = 0.2508(0.2508)
2023/10/18 06:23:23 - INFO - root -   Epoch: [162/300] 	 loss = 0.2906
2023/10/18 06:23:23 - INFO - root -   train_accuracy = 0.8604
2023/10/18 06:28:02 - INFO - root -   Epoch: [163/300][0/10], lr: 0.00000817 	 loss = 0.3376(0.3376)
2023/10/18 06:29:13 - INFO - root -   Epoch: [163/300] 	 loss = 0.3217
2023/10/18 06:29:13 - INFO - root -   train_accuracy = 0.8458
2023/10/18 06:33:16 - INFO - root -   Epoch: [164/300][0/10], lr: 0.00000822 	 loss = 0.1824(0.1824)
2023/10/18 06:34:55 - INFO - root -   Epoch: [164/300] 	 loss = 0.3100
2023/10/18 06:36:10 - INFO - root -   precision = 0.5194
2023/10/18 06:36:10 - INFO - root -   eval_loss = 1.3585
2023/10/18 06:36:11 - INFO - root -   train_accuracy = 0.8500
2023/10/18 06:40:26 - INFO - root -   Epoch: [165/300][0/10], lr: 0.00000827 	 loss = 0.2640(0.2640)
2023/10/18 06:41:46 - INFO - root -   Epoch: [165/300] 	 loss = 0.3179
2023/10/18 06:41:46 - INFO - root -   train_accuracy = 0.8458
2023/10/18 06:46:02 - INFO - root -   Epoch: [166/300][0/10], lr: 0.00000832 	 loss = 0.2092(0.2092)
2023/10/18 06:47:22 - INFO - root -   Epoch: [166/300] 	 loss = 0.3081
2023/10/18 06:47:22 - INFO - root -   train_accuracy = 0.8521
2023/10/18 06:51:33 - INFO - root -   Epoch: [167/300][0/10], lr: 0.00000837 	 loss = 0.1598(0.1598)
2023/10/18 06:52:54 - INFO - root -   Epoch: [167/300] 	 loss = 0.2467
2023/10/18 06:52:54 - INFO - root -   train_accuracy = 0.8854
2023/10/18 06:57:13 - INFO - root -   Epoch: [168/300][0/10], lr: 0.00000842 	 loss = 0.2066(0.2066)
2023/10/18 06:58:42 - INFO - root -   Epoch: [168/300] 	 loss = 0.3171
2023/10/18 06:58:42 - INFO - root -   train_accuracy = 0.8438
2023/10/18 07:03:14 - INFO - root -   Epoch: [169/300][0/10], lr: 0.00000847 	 loss = 0.4186(0.4186)
2023/10/18 07:04:24 - INFO - root -   Epoch: [169/300] 	 loss = 0.3815
2023/10/18 07:05:39 - INFO - root -   precision = 0.5271
2023/10/18 07:05:39 - INFO - root -   eval_loss = 1.2445
2023/10/18 07:05:40 - INFO - root -   train_accuracy = 0.8271
2023/10/18 07:10:09 - INFO - root -   Epoch: [170/300][0/10], lr: 0.00000852 	 loss = 0.2177(0.2177)
2023/10/18 07:11:31 - INFO - root -   Epoch: [170/300] 	 loss = 0.3682
2023/10/18 07:11:31 - INFO - root -   train_accuracy = 0.8250
2023/10/18 07:16:00 - INFO - root -   Epoch: [171/300][0/10], lr: 0.00000856 	 loss = 0.3592(0.3592)
2023/10/18 07:17:08 - INFO - root -   Epoch: [171/300] 	 loss = 0.2928
2023/10/18 07:17:08 - INFO - root -   train_accuracy = 0.8604
2023/10/18 07:21:38 - INFO - root -   Epoch: [172/300][0/10], lr: 0.00000861 	 loss = 0.3378(0.3378)
2023/10/18 07:22:40 - INFO - root -   Epoch: [172/300] 	 loss = 0.2910
2023/10/18 07:22:40 - INFO - root -   train_accuracy = 0.8542
2023/10/18 07:26:52 - INFO - root -   Epoch: [173/300][0/10], lr: 0.00000866 	 loss = 0.1146(0.1146)
2023/10/18 07:28:22 - INFO - root -   Epoch: [173/300] 	 loss = 0.3029
2023/10/18 07:28:22 - INFO - root -   train_accuracy = 0.8646
2023/10/18 07:33:03 - INFO - root -   Epoch: [174/300][0/10], lr: 0.00000871 	 loss = 0.2014(0.2014)
2023/10/18 07:34:11 - INFO - root -   Epoch: [174/300] 	 loss = 0.2910
2023/10/18 07:35:27 - INFO - root -   precision = 0.5659
2023/10/18 07:35:27 - INFO - root -   eval_loss = 1.3750
2023/10/18 07:35:27 - INFO - root -   train_accuracy = 0.8625
2023/10/18 07:39:47 - INFO - root -   Epoch: [175/300][0/10], lr: 0.00000876 	 loss = 0.1408(0.1408)
2023/10/18 07:41:09 - INFO - root -   Epoch: [175/300] 	 loss = 0.2836
2023/10/18 07:41:09 - INFO - root -   train_accuracy = 0.8729
2023/10/18 07:45:30 - INFO - root -   Epoch: [176/300][0/10], lr: 0.00000881 	 loss = 0.1238(0.1238)
2023/10/18 07:46:49 - INFO - root -   Epoch: [176/300] 	 loss = 0.2544
2023/10/18 07:46:49 - INFO - root -   train_accuracy = 0.8833
2023/10/18 07:51:25 - INFO - root -   Epoch: [177/300][0/10], lr: 0.00000886 	 loss = 0.3112(0.3112)
2023/10/18 07:52:42 - INFO - root -   Epoch: [177/300] 	 loss = 0.3961
2023/10/18 07:52:42 - INFO - root -   train_accuracy = 0.8083
2023/10/18 07:57:24 - INFO - root -   Epoch: [178/300][0/10], lr: 0.00000891 	 loss = 0.4323(0.4323)
2023/10/18 07:58:24 - INFO - root -   Epoch: [178/300] 	 loss = 0.3059
2023/10/18 07:58:24 - INFO - root -   train_accuracy = 0.8792
2023/10/18 08:02:56 - INFO - root -   Epoch: [179/300][0/10], lr: 0.00000896 	 loss = 0.1652(0.1652)
2023/10/18 08:04:06 - INFO - root -   Epoch: [179/300] 	 loss = 0.2975
2023/10/18 08:05:22 - INFO - root -   precision = 0.5426
2023/10/18 08:05:22 - INFO - root -   eval_loss = 1.2008
2023/10/18 08:05:23 - INFO - root -   train_accuracy = 0.8625
2023/10/18 08:09:50 - INFO - root -   Epoch: [180/300][0/10], lr: 0.00000901 	 loss = 0.1925(0.1925)
2023/10/18 08:11:08 - INFO - root -   Epoch: [180/300] 	 loss = 0.3356
2023/10/18 08:11:08 - INFO - root -   train_accuracy = 0.8458
2023/10/18 08:15:50 - INFO - root -   Epoch: [181/300][0/10], lr: 0.00000906 	 loss = 0.4515(0.4515)
2023/10/18 08:16:55 - INFO - root -   Epoch: [181/300] 	 loss = 0.2907
2023/10/18 08:16:55 - INFO - root -   train_accuracy = 0.8667
2023/10/18 08:21:37 - INFO - root -   Epoch: [182/300][0/10], lr: 0.00000911 	 loss = 0.6000(0.6000)
2023/10/18 08:22:47 - INFO - root -   Epoch: [182/300] 	 loss = 0.3433
2023/10/18 08:22:47 - INFO - root -   train_accuracy = 0.8417
2023/10/18 08:27:10 - INFO - root -   Epoch: [183/300][0/10], lr: 0.00000916 	 loss = 0.2159(0.2159)
2023/10/18 08:28:33 - INFO - root -   Epoch: [183/300] 	 loss = 0.3174
2023/10/18 08:28:33 - INFO - root -   train_accuracy = 0.8583
2023/10/18 08:32:54 - INFO - root -   Epoch: [184/300][0/10], lr: 0.00000921 	 loss = 0.2418(0.2418)
2023/10/18 08:34:02 - INFO - root -   Epoch: [184/300] 	 loss = 0.2453
2023/10/18 08:35:18 - INFO - root -   precision = 0.5581
2023/10/18 08:35:18 - INFO - root -   eval_loss = 1.3631
2023/10/18 08:35:18 - INFO - root -   train_accuracy = 0.8771
2023/10/18 08:40:08 - INFO - root -   Epoch: [185/300][0/10], lr: 0.00000926 	 loss = 0.3638(0.3638)
2023/10/18 08:41:14 - INFO - root -   Epoch: [185/300] 	 loss = 0.3578
2023/10/18 08:41:14 - INFO - root -   train_accuracy = 0.8292
2023/10/18 08:45:39 - INFO - root -   Epoch: [186/300][0/10], lr: 0.00000931 	 loss = 0.2297(0.2297)
2023/10/18 08:46:56 - INFO - root -   Epoch: [186/300] 	 loss = 0.2685
2023/10/18 08:46:56 - INFO - root -   train_accuracy = 0.8812
2023/10/18 08:51:20 - INFO - root -   Epoch: [187/300][0/10], lr: 0.00000936 	 loss = 0.2522(0.2522)
2023/10/18 08:52:25 - INFO - root -   Epoch: [187/300] 	 loss = 0.2899
2023/10/18 08:52:25 - INFO - root -   train_accuracy = 0.8812
2023/10/18 08:56:36 - INFO - root -   Epoch: [188/300][0/10], lr: 0.00000941 	 loss = 0.1729(0.1729)
2023/10/18 08:58:13 - INFO - root -   Epoch: [188/300] 	 loss = 0.3210
2023/10/18 08:58:13 - INFO - root -   train_accuracy = 0.8458
2023/10/18 09:02:48 - INFO - root -   Epoch: [189/300][0/10], lr: 0.00000946 	 loss = 0.4613(0.4613)
2023/10/18 09:04:00 - INFO - root -   Epoch: [189/300] 	 loss = 0.3504
2023/10/18 09:05:16 - INFO - root -   precision = 0.5039
2023/10/18 09:05:16 - INFO - root -   eval_loss = 1.6760
2023/10/18 09:05:17 - INFO - root -   train_accuracy = 0.8438
2023/10/18 09:09:47 - INFO - root -   Epoch: [190/300][0/10], lr: 0.00000951 	 loss = 0.2313(0.2313)
2023/10/18 09:10:56 - INFO - root -   Epoch: [190/300] 	 loss = 0.2549
2023/10/18 09:10:56 - INFO - root -   train_accuracy = 0.8979
2023/10/18 09:15:21 - INFO - root -   Epoch: [191/300][0/10], lr: 0.00000955 	 loss = 0.1401(0.1401)
2023/10/18 09:16:40 - INFO - root -   Epoch: [191/300] 	 loss = 0.2802
2023/10/18 09:16:40 - INFO - root -   train_accuracy = 0.8688
2023/10/18 09:20:51 - INFO - root -   Epoch: [192/300][0/10], lr: 0.00000960 	 loss = 0.2049(0.2049)
2023/10/18 09:22:41 - INFO - root -   Epoch: [192/300] 	 loss = 0.3648
2023/10/18 09:22:41 - INFO - root -   train_accuracy = 0.8292
2023/10/18 09:27:15 - INFO - root -   Epoch: [193/300][0/10], lr: 0.00000965 	 loss = 0.2856(0.2856)
2023/10/18 09:28:33 - INFO - root -   Epoch: [193/300] 	 loss = 0.3399
2023/10/18 09:28:33 - INFO - root -   train_accuracy = 0.8417
2023/10/18 09:33:01 - INFO - root -   Epoch: [194/300][0/10], lr: 0.00000970 	 loss = 0.3643(0.3643)
2023/10/18 09:34:20 - INFO - root -   Epoch: [194/300] 	 loss = 0.3139
2023/10/18 09:35:35 - INFO - root -   precision = 0.5194
2023/10/18 09:35:35 - INFO - root -   eval_loss = 1.6598
2023/10/18 09:35:36 - INFO - root -   train_accuracy = 0.8521
2023/10/18 09:40:05 - INFO - root -   Epoch: [195/300][0/10], lr: 0.00000975 	 loss = 0.1994(0.1994)
2023/10/18 09:41:14 - INFO - root -   Epoch: [195/300] 	 loss = 0.2803
2023/10/18 09:41:14 - INFO - root -   train_accuracy = 0.8667
2023/10/18 09:45:46 - INFO - root -   Epoch: [196/300][0/10], lr: 0.00000980 	 loss = 0.2318(0.2318)
2023/10/18 09:47:08 - INFO - root -   Epoch: [196/300] 	 loss = 0.3070
2023/10/18 09:47:08 - INFO - root -   train_accuracy = 0.8667
2023/10/18 09:51:39 - INFO - root -   Epoch: [197/300][0/10], lr: 0.00000985 	 loss = 0.4414(0.4414)
2023/10/18 09:52:56 - INFO - root -   Epoch: [197/300] 	 loss = 0.3763
2023/10/18 09:52:56 - INFO - root -   train_accuracy = 0.8083
2023/10/18 09:57:19 - INFO - root -   Epoch: [198/300][0/10], lr: 0.00000990 	 loss = 0.3020(0.3020)
2023/10/18 09:58:36 - INFO - root -   Epoch: [198/300] 	 loss = 0.2996
2023/10/18 09:58:36 - INFO - root -   train_accuracy = 0.8646
2023/10/18 10:03:15 - INFO - root -   Epoch: [199/300][0/10], lr: 0.00000995 	 loss = 0.1968(0.1968)
2023/10/18 10:04:26 - INFO - root -   Epoch: [199/300] 	 loss = 0.3356
2023/10/18 10:05:42 - INFO - root -   precision = 0.5891
2023/10/18 10:05:42 - INFO - root -   eval_loss = 1.3031
2023/10/18 10:05:43 - INFO - root -   train_accuracy = 0.8313
2023/10/18 10:10:01 - INFO - root -   Epoch: [200/300][0/10], lr: 0.00001000 	 loss = 0.2137(0.2137)
2023/10/18 10:11:27 - INFO - root -   Epoch: [200/300] 	 loss = 0.2851
2023/10/18 10:11:27 - INFO - root -   train_accuracy = 0.8500
2023/10/18 10:15:33 - INFO - root -   Epoch: [201/300][0/10], lr: 0.00001000 	 loss = 0.2151(0.2151)
2023/10/18 10:17:10 - INFO - root -   Epoch: [201/300] 	 loss = 0.2773
2023/10/18 10:17:10 - INFO - root -   train_accuracy = 0.8729
2023/10/18 10:21:42 - INFO - root -   Epoch: [202/300][0/10], lr: 0.00001000 	 loss = 0.2206(0.2206)
2023/10/18 10:22:57 - INFO - root -   Epoch: [202/300] 	 loss = 0.2830
2023/10/18 10:22:57 - INFO - root -   train_accuracy = 0.8708
2023/10/18 10:27:37 - INFO - root -   Epoch: [203/300][0/10], lr: 0.00001000 	 loss = 0.2548(0.2548)
2023/10/18 10:28:48 - INFO - root -   Epoch: [203/300] 	 loss = 0.3310
2023/10/18 10:28:48 - INFO - root -   train_accuracy = 0.8396
2023/10/18 10:33:22 - INFO - root -   Epoch: [204/300][0/10], lr: 0.00001000 	 loss = 0.2529(0.2529)
2023/10/18 10:34:29 - INFO - root -   Epoch: [204/300] 	 loss = 0.2605
2023/10/18 10:35:44 - INFO - root -   precision = 0.5426
2023/10/18 10:35:44 - INFO - root -   eval_loss = 1.4221
2023/10/18 10:35:44 - INFO - root -   train_accuracy = 0.8854
2023/10/18 10:40:08 - INFO - root -   Epoch: [205/300][0/10], lr: 0.00001000 	 loss = 0.2290(0.2290)
2023/10/18 10:41:13 - INFO - root -   Epoch: [205/300] 	 loss = 0.2418
2023/10/18 10:41:13 - INFO - root -   train_accuracy = 0.8958
2023/10/18 10:45:41 - INFO - root -   Epoch: [206/300][0/10], lr: 0.00001000 	 loss = 0.2186(0.2186)
2023/10/18 10:46:48 - INFO - root -   Epoch: [206/300] 	 loss = 0.2363
2023/10/18 10:46:48 - INFO - root -   train_accuracy = 0.8833
2023/10/18 10:51:21 - INFO - root -   Epoch: [207/300][0/10], lr: 0.00001000 	 loss = 0.2860(0.2860)
2023/10/18 10:52:25 - INFO - root -   Epoch: [207/300] 	 loss = 0.2969
2023/10/18 10:52:25 - INFO - root -   train_accuracy = 0.8667
2023/10/18 10:56:48 - INFO - root -   Epoch: [208/300][0/10], lr: 0.00001000 	 loss = 0.1917(0.1917)
2023/10/18 10:58:10 - INFO - root -   Epoch: [208/300] 	 loss = 0.3274
2023/10/18 10:58:10 - INFO - root -   train_accuracy = 0.8646
2023/10/18 11:02:33 - INFO - root -   Epoch: [209/300][0/10], lr: 0.00001000 	 loss = 0.1960(0.1960)
2023/10/18 11:03:57 - INFO - root -   Epoch: [209/300] 	 loss = 0.3066
2023/10/18 11:05:13 - INFO - root -   precision = 0.4574
2023/10/18 11:05:13 - INFO - root -   eval_loss = 1.3965
2023/10/18 11:05:13 - INFO - root -   train_accuracy = 0.8625
2023/10/18 11:09:41 - INFO - root -   Epoch: [210/300][0/10], lr: 0.00001000 	 loss = 0.2030(0.2030)
2023/10/18 11:10:54 - INFO - root -   Epoch: [210/300] 	 loss = 0.2874
2023/10/18 11:10:54 - INFO - root -   train_accuracy = 0.8688
2023/10/18 11:15:03 - INFO - root -   Epoch: [211/300][0/10], lr: 0.00001000 	 loss = 0.1092(0.1092)
2023/10/18 11:16:25 - INFO - root -   Epoch: [211/300] 	 loss = 0.2598
2023/10/18 11:16:25 - INFO - root -   train_accuracy = 0.8729
2023/10/18 11:20:44 - INFO - root -   Epoch: [212/300][0/10], lr: 0.00001000 	 loss = 0.1700(0.1700)
2023/10/18 11:22:07 - INFO - root -   Epoch: [212/300] 	 loss = 0.3060
2023/10/18 11:22:07 - INFO - root -   train_accuracy = 0.8458
2023/10/18 11:26:12 - INFO - root -   Epoch: [213/300][0/10], lr: 0.00001000 	 loss = 0.1145(0.1145)
2023/10/18 11:27:49 - INFO - root -   Epoch: [213/300] 	 loss = 0.2948
2023/10/18 11:27:49 - INFO - root -   train_accuracy = 0.8542
2023/10/18 11:32:07 - INFO - root -   Epoch: [214/300][0/10], lr: 0.00001000 	 loss = 0.1323(0.1323)
2023/10/18 11:33:22 - INFO - root -   Epoch: [214/300] 	 loss = 0.2638
2023/10/18 11:34:38 - INFO - root -   precision = 0.5891
2023/10/18 11:34:38 - INFO - root -   eval_loss = 1.5312
2023/10/18 11:34:39 - INFO - root -   train_accuracy = 0.8771
2023/10/18 11:39:18 - INFO - root -   Epoch: [215/300][0/10], lr: 0.00001000 	 loss = 0.4300(0.4300)
2023/10/18 11:40:24 - INFO - root -   Epoch: [215/300] 	 loss = 0.2741
2023/10/18 11:40:24 - INFO - root -   train_accuracy = 0.8583
2023/10/18 11:45:03 - INFO - root -   Epoch: [216/300][0/10], lr: 0.00001000 	 loss = 0.1789(0.1789)
2023/10/18 11:46:15 - INFO - root -   Epoch: [216/300] 	 loss = 0.3284
2023/10/18 11:46:15 - INFO - root -   train_accuracy = 0.8542
2023/10/18 11:50:28 - INFO - root -   Epoch: [217/300][0/10], lr: 0.00001000 	 loss = 0.1611(0.1611)
2023/10/18 11:51:59 - INFO - root -   Epoch: [217/300] 	 loss = 0.3121
2023/10/18 11:51:59 - INFO - root -   train_accuracy = 0.8625
2023/10/18 11:56:10 - INFO - root -   Epoch: [218/300][0/10], lr: 0.00001000 	 loss = 0.1437(0.1437)
2023/10/18 11:57:41 - INFO - root -   Epoch: [218/300] 	 loss = 0.2547
2023/10/18 11:57:41 - INFO - root -   train_accuracy = 0.8875
2023/10/18 12:02:26 - INFO - root -   Epoch: [219/300][0/10], lr: 0.00001000 	 loss = 0.3521(0.3521)
2023/10/18 12:03:37 - INFO - root -   Epoch: [219/300] 	 loss = 0.3157
2023/10/18 12:04:52 - INFO - root -   precision = 0.5581
2023/10/18 12:04:52 - INFO - root -   eval_loss = 1.3891
2023/10/18 12:04:53 - INFO - root -   train_accuracy = 0.8625
2023/10/18 12:09:20 - INFO - root -   Epoch: [220/300][0/10], lr: 0.00001000 	 loss = 0.0884(0.0884)
2023/10/18 12:10:32 - INFO - root -   Epoch: [220/300] 	 loss = 0.2454
2023/10/18 12:10:32 - INFO - root -   train_accuracy = 0.8833
2023/10/18 12:14:58 - INFO - root -   Epoch: [221/300][0/10], lr: 0.00001000 	 loss = 0.1868(0.1868)
2023/10/18 12:16:16 - INFO - root -   Epoch: [221/300] 	 loss = 0.2665
2023/10/18 12:16:16 - INFO - root -   train_accuracy = 0.8771
2023/10/18 12:20:47 - INFO - root -   Epoch: [222/300][0/10], lr: 0.00001000 	 loss = 0.1541(0.1541)
2023/10/18 12:21:59 - INFO - root -   Epoch: [222/300] 	 loss = 0.3006
2023/10/18 12:21:59 - INFO - root -   train_accuracy = 0.8438
2023/10/18 12:26:11 - INFO - root -   Epoch: [223/300][0/10], lr: 0.00001000 	 loss = 0.1524(0.1524)
2023/10/18 12:27:41 - INFO - root -   Epoch: [223/300] 	 loss = 0.2627
2023/10/18 12:27:41 - INFO - root -   train_accuracy = 0.8917
2023/10/18 12:32:16 - INFO - root -   Epoch: [224/300][0/10], lr: 0.00001000 	 loss = 0.2045(0.2045)
2023/10/18 12:33:17 - INFO - root -   Epoch: [224/300] 	 loss = 0.2580
2023/10/18 12:34:33 - INFO - root -   precision = 0.6047
2023/10/18 12:34:33 - INFO - root -   eval_loss = 1.2868
2023/10/18 12:34:34 - INFO - root -   train_accuracy = 0.8854
2023/10/18 12:39:12 - INFO - root -   Epoch: [225/300][0/10], lr: 0.00001000 	 loss = 0.2400(0.2400)
2023/10/18 12:40:28 - INFO - root -   Epoch: [225/300] 	 loss = 0.3115
2023/10/18 12:40:28 - INFO - root -   train_accuracy = 0.8375
2023/10/18 12:44:57 - INFO - root -   Epoch: [226/300][0/10], lr: 0.00001000 	 loss = 0.2111(0.2111)
2023/10/18 12:46:13 - INFO - root -   Epoch: [226/300] 	 loss = 0.2824
2023/10/18 12:46:13 - INFO - root -   train_accuracy = 0.8750
2023/10/18 12:50:45 - INFO - root -   Epoch: [227/300][0/10], lr: 0.00001000 	 loss = 0.3780(0.3780)
2023/10/18 12:51:51 - INFO - root -   Epoch: [227/300] 	 loss = 0.2490
2023/10/18 12:51:51 - INFO - root -   train_accuracy = 0.8854
2023/10/18 12:56:31 - INFO - root -   Epoch: [228/300][0/10], lr: 0.00001000 	 loss = 0.2043(0.2043)
2023/10/18 12:57:38 - INFO - root -   Epoch: [228/300] 	 loss = 0.2439
2023/10/18 12:57:38 - INFO - root -   train_accuracy = 0.8896
2023/10/18 13:02:20 - INFO - root -   Epoch: [229/300][0/10], lr: 0.00001000 	 loss = 0.2141(0.2141)
2023/10/18 13:03:25 - INFO - root -   Epoch: [229/300] 	 loss = 0.2428
2023/10/18 13:04:41 - INFO - root -   precision = 0.5891
2023/10/18 13:04:41 - INFO - root -   eval_loss = 1.5726
2023/10/18 13:04:41 - INFO - root -   train_accuracy = 0.8750
2023/10/18 13:09:19 - INFO - root -   Epoch: [230/300][0/10], lr: 0.00001000 	 loss = 0.6539(0.6539)
2023/10/18 13:10:30 - INFO - root -   Epoch: [230/300] 	 loss = 0.2782
2023/10/18 13:10:30 - INFO - root -   train_accuracy = 0.8729
2023/10/18 13:15:00 - INFO - root -   Epoch: [231/300][0/10], lr: 0.00001000 	 loss = 0.2176(0.2176)
2023/10/18 13:16:04 - INFO - root -   Epoch: [231/300] 	 loss = 0.2224
2023/10/18 13:16:04 - INFO - root -   train_accuracy = 0.8854
2023/10/18 13:20:57 - INFO - root -   Epoch: [232/300][0/10], lr: 0.00001000 	 loss = 0.5798(0.5798)
2023/10/18 13:22:08 - INFO - root -   Epoch: [232/300] 	 loss = 0.3594
2023/10/18 13:22:08 - INFO - root -   train_accuracy = 0.8083
2023/10/18 13:26:34 - INFO - root -   Epoch: [233/300][0/10], lr: 0.00001000 	 loss = 0.2231(0.2231)
2023/10/18 13:27:48 - INFO - root -   Epoch: [233/300] 	 loss = 0.2460
2023/10/18 13:27:48 - INFO - root -   train_accuracy = 0.8750
2023/10/18 13:32:16 - INFO - root -   Epoch: [234/300][0/10], lr: 0.00001000 	 loss = 0.1229(0.1229)
2023/10/18 13:33:32 - INFO - root -   Epoch: [234/300] 	 loss = 0.2702
2023/10/18 13:34:48 - INFO - root -   precision = 0.5116
2023/10/18 13:34:48 - INFO - root -   eval_loss = 1.5524
2023/10/18 13:34:49 - INFO - root -   train_accuracy = 0.8896
2023/10/18 13:39:22 - INFO - root -   Epoch: [235/300][0/10], lr: 0.00001000 	 loss = 0.1707(0.1707)
2023/10/18 13:40:38 - INFO - root -   Epoch: [235/300] 	 loss = 0.2719
2023/10/18 13:40:38 - INFO - root -   train_accuracy = 0.8646
2023/10/18 13:45:11 - INFO - root -   Epoch: [236/300][0/10], lr: 0.00001000 	 loss = 0.1718(0.1718)
2023/10/18 13:46:31 - INFO - root -   Epoch: [236/300] 	 loss = 0.2656
2023/10/18 13:46:31 - INFO - root -   train_accuracy = 0.8875
2023/10/18 13:51:03 - INFO - root -   Epoch: [237/300][0/10], lr: 0.00001000 	 loss = 0.4937(0.4937)
2023/10/18 13:52:16 - INFO - root -   Epoch: [237/300] 	 loss = 0.2906
2023/10/18 13:52:16 - INFO - root -   train_accuracy = 0.8479
2023/10/18 13:56:35 - INFO - root -   Epoch: [238/300][0/10], lr: 0.00001000 	 loss = 0.1641(0.1641)
2023/10/18 13:57:58 - INFO - root -   Epoch: [238/300] 	 loss = 0.2663
2023/10/18 13:57:58 - INFO - root -   train_accuracy = 0.8812
2023/10/18 14:02:36 - INFO - root -   Epoch: [239/300][0/10], lr: 0.00001000 	 loss = 0.4021(0.4021)
2023/10/18 14:03:47 - INFO - root -   Epoch: [239/300] 	 loss = 0.2785
2023/10/18 14:05:03 - INFO - root -   precision = 0.5581
2023/10/18 14:05:03 - INFO - root -   eval_loss = 1.3367
2023/10/18 14:05:03 - INFO - root -   train_accuracy = 0.8688
2023/10/18 14:09:15 - INFO - root -   Epoch: [240/300][0/10], lr: 0.00001000 	 loss = 0.2226(0.2226)
2023/10/18 14:10:48 - INFO - root -   Epoch: [240/300] 	 loss = 0.2708
2023/10/18 14:10:48 - INFO - root -   train_accuracy = 0.8708
2023/10/18 14:15:24 - INFO - root -   Epoch: [241/300][0/10], lr: 0.00001000 	 loss = 0.2851(0.2851)
2023/10/18 14:16:41 - INFO - root -   Epoch: [241/300] 	 loss = 0.2972
2023/10/18 14:16:41 - INFO - root -   train_accuracy = 0.8458
2023/10/18 14:21:02 - INFO - root -   Epoch: [242/300][0/10], lr: 0.00001000 	 loss = 0.1940(0.1940)
2023/10/18 14:22:23 - INFO - root -   Epoch: [242/300] 	 loss = 0.2571
2023/10/18 14:22:23 - INFO - root -   train_accuracy = 0.8875
2023/10/18 14:26:39 - INFO - root -   Epoch: [243/300][0/10], lr: 0.00001000 	 loss = 0.1954(0.1954)
2023/10/18 14:27:55 - INFO - root -   Epoch: [243/300] 	 loss = 0.2573
2023/10/18 14:27:55 - INFO - root -   train_accuracy = 0.8792
2023/10/18 14:32:21 - INFO - root -   Epoch: [244/300][0/10], lr: 0.00001000 	 loss = 0.3543(0.3543)
2023/10/18 14:33:35 - INFO - root -   Epoch: [244/300] 	 loss = 0.2817
2023/10/18 14:34:50 - INFO - root -   precision = 0.5039
2023/10/18 14:34:50 - INFO - root -   eval_loss = 1.5525
2023/10/18 14:34:51 - INFO - root -   train_accuracy = 0.8771
2023/10/18 14:39:07 - INFO - root -   Epoch: [245/300][0/10], lr: 0.00001000 	 loss = 0.2383(0.2383)
2023/10/18 14:40:23 - INFO - root -   Epoch: [245/300] 	 loss = 0.2615
2023/10/18 14:40:23 - INFO - root -   train_accuracy = 0.8771
2023/10/18 14:45:01 - INFO - root -   Epoch: [246/300][0/10], lr: 0.00001000 	 loss = 0.2252(0.2252)
2023/10/18 14:46:06 - INFO - root -   Epoch: [246/300] 	 loss = 0.2255
2023/10/18 14:46:06 - INFO - root -   train_accuracy = 0.8896
2023/10/18 14:50:38 - INFO - root -   Epoch: [247/300][0/10], lr: 0.00001000 	 loss = 0.4250(0.4250)
2023/10/18 14:51:46 - INFO - root -   Epoch: [247/300] 	 loss = 0.2531
2023/10/18 14:51:46 - INFO - root -   train_accuracy = 0.8729
2023/10/18 14:56:13 - INFO - root -   Epoch: [248/300][0/10], lr: 0.00001000 	 loss = 0.1963(0.1963)
2023/10/18 14:57:22 - INFO - root -   Epoch: [248/300] 	 loss = 0.2070
2023/10/18 14:57:22 - INFO - root -   train_accuracy = 0.9021
2023/10/18 15:01:25 - INFO - root -   Epoch: [249/300][0/10], lr: 0.00001000 	 loss = 0.1290(0.1290)
2023/10/18 15:02:56 - INFO - root -   Epoch: [249/300] 	 loss = 0.2010
2023/10/18 15:04:11 - INFO - root -   precision = 0.5426
2023/10/18 15:04:11 - INFO - root -   eval_loss = 1.4042
2023/10/18 15:04:12 - INFO - root -   train_accuracy = 0.9021
2023/10/18 15:08:30 - INFO - root -   Epoch: [250/300][0/10], lr: 0.00001000 	 loss = 0.0846(0.0846)
2023/10/18 15:09:48 - INFO - root -   Epoch: [250/300] 	 loss = 0.2310
2023/10/18 15:09:48 - INFO - root -   train_accuracy = 0.9146
2023/10/18 15:14:35 - INFO - root -   Epoch: [251/300][0/10], lr: 0.00001000 	 loss = 0.4920(0.4920)
2023/10/18 15:15:41 - INFO - root -   Epoch: [251/300] 	 loss = 0.3082
2023/10/18 15:15:41 - INFO - root -   train_accuracy = 0.8583
2023/10/18 15:20:17 - INFO - root -   Epoch: [252/300][0/10], lr: 0.00001000 	 loss = 0.1590(0.1590)
2023/10/18 15:21:36 - INFO - root -   Epoch: [252/300] 	 loss = 0.2826
2023/10/18 15:21:36 - INFO - root -   train_accuracy = 0.8667
2023/10/18 15:26:12 - INFO - root -   Epoch: [253/300][0/10], lr: 0.00001000 	 loss = 0.3187(0.3187)
2023/10/18 15:27:20 - INFO - root -   Epoch: [253/300] 	 loss = 0.2463
2023/10/18 15:27:20 - INFO - root -   train_accuracy = 0.8792
2023/10/18 15:32:02 - INFO - root -   Epoch: [254/300][0/10], lr: 0.00001000 	 loss = 0.3190(0.3190)
2023/10/18 15:33:06 - INFO - root -   Epoch: [254/300] 	 loss = 0.2501
2023/10/18 15:34:22 - INFO - root -   precision = 0.5349
2023/10/18 15:34:22 - INFO - root -   eval_loss = 1.8156
2023/10/18 15:34:22 - INFO - root -   train_accuracy = 0.8771
2023/10/18 15:38:47 - INFO - root -   Epoch: [255/300][0/10], lr: 0.00001000 	 loss = 0.1882(0.1882)
2023/10/18 15:40:12 - INFO - root -   Epoch: [255/300] 	 loss = 0.3236
2023/10/18 15:40:12 - INFO - root -   train_accuracy = 0.8625
2023/10/18 15:44:34 - INFO - root -   Epoch: [256/300][0/10], lr: 0.00001000 	 loss = 0.1519(0.1519)
2023/10/18 15:45:50 - INFO - root -   Epoch: [256/300] 	 loss = 0.2372
2023/10/18 15:45:50 - INFO - root -   train_accuracy = 0.8812
2023/10/18 15:50:28 - INFO - root -   Epoch: [257/300][0/10], lr: 0.00001000 	 loss = 0.5218(0.5218)
2023/10/18 15:51:34 - INFO - root -   Epoch: [257/300] 	 loss = 0.2687
2023/10/18 15:51:34 - INFO - root -   train_accuracy = 0.8812
2023/10/18 15:56:04 - INFO - root -   Epoch: [258/300][0/10], lr: 0.00001000 	 loss = 0.2096(0.2096)
2023/10/18 15:57:11 - INFO - root -   Epoch: [258/300] 	 loss = 0.2117
2023/10/18 15:57:11 - INFO - root -   train_accuracy = 0.8979
2023/10/18 16:01:40 - INFO - root -   Epoch: [259/300][0/10], lr: 0.00001000 	 loss = 0.1824(0.1824)
2023/10/18 16:02:45 - INFO - root -   Epoch: [259/300] 	 loss = 0.2179
2023/10/18 16:04:01 - INFO - root -   precision = 0.5581
2023/10/18 16:04:01 - INFO - root -   eval_loss = 1.3195
2023/10/18 16:04:01 - INFO - root -   train_accuracy = 0.9062
