2023/11/22 16:52:38 - INFO - root -   Num train examples = 566
2023/11/22 16:52:38 - INFO - root -   Num val examples = 143
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/22 16:52:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/22 16:52:38 - INFO - root -   backend = nccl
2023/11/22 16:52:38 - INFO - root -   batch_size = 2
2023/11/22 16:52:38 - INFO - root -   dropout = 0.5
2023/11/22 16:52:38 - INFO - root -   epochs = 300
2023/11/22 16:52:38 - INFO - root -   eval_freq = 5
2023/11/22 16:52:38 - INFO - root -   focal_loss = False
2023/11/22 16:52:38 - INFO - root -   input_size = 128
2023/11/22 16:52:38 - INFO - root -   is_pretrained = False
2023/11/22 16:52:38 - INFO - root -   label_smooth = False
2023/11/22 16:52:38 - INFO - root -   local_rank = -1
2023/11/22 16:52:38 - INFO - root -   lr = 1e-05
2023/11/22 16:52:38 - INFO - root -   lr_decay_rate = 0.1
2023/11/22 16:52:38 - INFO - root -   lr_steps = [50, 100]
2023/11/22 16:52:38 - INFO - root -   lr_type = cosine
2023/11/22 16:52:38 - INFO - root -   model_depth = 34
2023/11/22 16:52:38 - INFO - root -   model_name = resnet50
2023/11/22 16:52:38 - INFO - root -   momentum = 0.9
2023/11/22 16:52:38 - INFO - root -   num_classes = 3
2023/11/22 16:52:38 - INFO - root -   output = ./tcia_ucsf_roi_gene_outputs
2023/11/22 16:52:38 - INFO - root -   print_freq = 20
2023/11/22 16:52:38 - INFO - root -   resume = 
2023/11/22 16:52:38 - INFO - root -   start_epoch = 0
2023/11/22 16:52:38 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_ucsf_train_patients.txt
2023/11/22 16:52:38 - INFO - root -   tune_from = 
2023/11/22 16:52:38 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_ucsf_test_patients.txt
2023/11/22 16:52:38 - INFO - root -   warmup_epoch = 20
2023/11/22 16:52:38 - INFO - root -   warmup_multiplier = 100
2023/11/22 16:52:38 - INFO - root -   weight_decay = 0.0005
2023/11/22 16:52:38 - INFO - root -   workers = 8
2023/11/22 16:52:41 - INFO - root -   Epoch: [0/300][0/283], lr: 0.00000010 	 loss = 1.3997(1.3997)
2023/11/22 16:53:01 - INFO - root -   Epoch: [0/300][20/283], lr: 0.00000010 	 loss = 1.7952(1.5638)
2023/11/22 16:53:16 - INFO - root -   Epoch: [0/300][40/283], lr: 0.00000010 	 loss = 0.8113(1.4084)
2023/11/22 16:53:37 - INFO - root -   Epoch: [0/300][60/283], lr: 0.00000010 	 loss = 0.9404(1.3502)
2023/11/22 16:53:52 - INFO - root -   Epoch: [0/300][80/283], lr: 0.00000010 	 loss = 0.9859(1.3352)
2023/11/22 16:54:16 - INFO - root -   Epoch: [0/300][100/283], lr: 0.00000010 	 loss = 1.6982(1.3037)
2023/11/22 16:54:50 - INFO - root -   Epoch: [0/300][120/283], lr: 0.00000010 	 loss = 1.7865(1.2599)
2023/11/22 16:55:12 - INFO - root -   Epoch: [0/300][140/283], lr: 0.00000010 	 loss = 0.7824(1.2273)
2023/11/22 16:55:43 - INFO - root -   Epoch: [0/300][160/283], lr: 0.00000010 	 loss = 1.7555(1.1906)
2023/11/22 16:56:12 - INFO - root -   Epoch: [0/300][180/283], lr: 0.00000010 	 loss = 0.5772(1.1780)
2023/11/22 16:56:45 - INFO - root -   Epoch: [0/300][200/283], lr: 0.00000010 	 loss = 1.3119(1.1669)
2023/11/22 16:57:05 - INFO - root -   Epoch: [0/300][220/283], lr: 0.00000010 	 loss = 1.0085(1.1378)
2023/11/22 16:57:33 - INFO - root -   Epoch: [0/300][240/283], lr: 0.00000010 	 loss = 0.2617(1.1173)
2023/11/22 16:58:03 - INFO - root -   Epoch: [0/300][260/283], lr: 0.00000010 	 loss = 0.8413(1.0845)
2023/11/22 16:58:24 - INFO - root -   Epoch: [0/300][280/283], lr: 0.00000010 	 loss = 1.5594(1.0662)
2023/11/22 16:58:25 - INFO - root -   Epoch: [0/300] 	 loss = 1.0605
2023/11/22 16:58:25 - INFO - root -   train_accuracy = 0.5212
2023/11/22 16:58:34 - INFO - root -   Epoch: [1/300][0/283], lr: 0.00000010 	 loss = 0.3138(0.3138)
2023/11/22 16:58:50 - INFO - root -   Epoch: [1/300][20/283], lr: 0.00000010 	 loss = 2.3064(1.0193)
2023/11/22 16:59:11 - INFO - root -   Epoch: [1/300][40/283], lr: 0.00000010 	 loss = 0.5049(1.1055)
2023/11/22 16:59:24 - INFO - root -   Epoch: [1/300][60/283], lr: 0.00000010 	 loss = 0.2931(1.1053)
2023/11/22 16:59:41 - INFO - root -   Epoch: [1/300][80/283], lr: 0.00000010 	 loss = 0.4277(1.1043)
2023/11/22 16:59:56 - INFO - root -   Epoch: [1/300][100/283], lr: 0.00000010 	 loss = 0.2913(1.0477)
2023/11/22 17:00:18 - INFO - root -   Epoch: [1/300][120/283], lr: 0.00000010 	 loss = 0.2420(0.9761)
2023/11/22 17:00:42 - INFO - root -   Epoch: [1/300][140/283], lr: 0.00000010 	 loss = 0.3101(0.9230)
2023/11/22 17:01:10 - INFO - root -   Epoch: [1/300][160/283], lr: 0.00000010 	 loss = 1.0499(0.8943)
2023/11/22 17:01:34 - INFO - root -   Epoch: [1/300][180/283], lr: 0.00000010 	 loss = 0.7370(0.8891)
2023/11/22 17:02:01 - INFO - root -   Epoch: [1/300][200/283], lr: 0.00000010 	 loss = 0.3383(0.8672)
2023/11/22 17:02:26 - INFO - root -   Epoch: [1/300][220/283], lr: 0.00000010 	 loss = 0.1020(0.8441)
2023/11/22 17:02:52 - INFO - root -   Epoch: [1/300][240/283], lr: 0.00000010 	 loss = 0.5809(0.8359)
2023/11/22 17:03:19 - INFO - root -   Epoch: [1/300][260/283], lr: 0.00000010 	 loss = 2.2601(0.8215)
2023/11/22 17:03:42 - INFO - root -   Epoch: [1/300][280/283], lr: 0.00000010 	 loss = 1.3810(0.8128)
2023/11/22 17:03:43 - INFO - root -   Epoch: [1/300] 	 loss = 0.8105
2023/11/22 17:03:43 - INFO - root -   train_accuracy = 0.6943
2023/11/22 17:03:52 - INFO - root -   Epoch: [2/300][0/283], lr: 0.00000010 	 loss = 0.2895(0.2895)
2023/11/22 17:04:03 - INFO - root -   Epoch: [2/300][20/283], lr: 0.00000010 	 loss = 2.3715(1.0784)
2023/11/22 17:04:22 - INFO - root -   Epoch: [2/300][40/283], lr: 0.00000010 	 loss = 0.5854(1.0276)
2023/11/22 17:04:35 - INFO - root -   Epoch: [2/300][60/283], lr: 0.00000010 	 loss = 0.2745(1.0039)
2023/11/22 17:04:48 - INFO - root -   Epoch: [2/300][80/283], lr: 0.00000010 	 loss = 0.4075(0.9855)
2023/11/22 17:05:06 - INFO - root -   Epoch: [2/300][100/283], lr: 0.00000010 	 loss = 0.2378(0.9279)
2023/11/22 17:05:22 - INFO - root -   Epoch: [2/300][120/283], lr: 0.00000010 	 loss = 0.2327(0.8670)
2023/11/22 17:05:37 - INFO - root -   Epoch: [2/300][140/283], lr: 0.00000010 	 loss = 0.3726(0.8214)
2023/11/22 17:05:53 - INFO - root -   Epoch: [2/300][160/283], lr: 0.00000010 	 loss = 1.4562(0.8079)
2023/11/22 17:06:10 - INFO - root -   Epoch: [2/300][180/283], lr: 0.00000010 	 loss = 0.5337(0.8119)
2023/11/22 17:06:26 - INFO - root -   Epoch: [2/300][200/283], lr: 0.00000010 	 loss = 0.3113(0.7973)
2023/11/22 17:06:50 - INFO - root -   Epoch: [2/300][220/283], lr: 0.00000010 	 loss = 0.5041(0.7886)
2023/11/22 17:07:07 - INFO - root -   Epoch: [2/300][240/283], lr: 0.00000010 	 loss = 0.6083(0.7911)
2023/11/22 17:07:25 - INFO - root -   Epoch: [2/300][260/283], lr: 0.00000010 	 loss = 1.6482(0.7791)
2023/11/22 17:07:35 - INFO - root -   Epoch: [2/300][280/283], lr: 0.00000010 	 loss = 1.9325(0.7706)
2023/11/22 17:07:36 - INFO - root -   Epoch: [2/300] 	 loss = 0.7690
2023/11/22 17:07:36 - INFO - root -   train_accuracy = 0.7067
2023/11/22 17:07:38 - INFO - root -   Epoch: [3/300][0/283], lr: 0.00000011 	 loss = 0.3643(0.3643)
2023/11/22 17:08:01 - INFO - root -   Epoch: [3/300][20/283], lr: 0.00000011 	 loss = 2.6145(1.1087)
2023/11/22 17:08:19 - INFO - root -   Epoch: [3/300][40/283], lr: 0.00000011 	 loss = 0.2793(1.0490)
2023/11/22 17:08:39 - INFO - root -   Epoch: [3/300][60/283], lr: 0.00000011 	 loss = 0.2798(1.0268)
2023/11/22 17:08:54 - INFO - root -   Epoch: [3/300][80/283], lr: 0.00000011 	 loss = 0.3002(1.0224)
2023/11/22 17:09:09 - INFO - root -   Epoch: [3/300][100/283], lr: 0.00000011 	 loss = 0.3361(0.9561)
2023/11/22 17:09:22 - INFO - root -   Epoch: [3/300][120/283], lr: 0.00000011 	 loss = 0.7122(0.8839)
2023/11/22 17:09:34 - INFO - root -   Epoch: [3/300][140/283], lr: 0.00000011 	 loss = 0.4830(0.8326)
2023/11/22 17:09:49 - INFO - root -   Epoch: [3/300][160/283], lr: 0.00000011 	 loss = 1.5284(0.8133)
2023/11/22 17:10:02 - INFO - root -   Epoch: [3/300][180/283], lr: 0.00000011 	 loss = 0.2097(0.8031)
2023/11/22 17:10:16 - INFO - root -   Epoch: [3/300][200/283], lr: 0.00000011 	 loss = 0.4089(0.7902)
2023/11/22 17:10:24 - INFO - root -   Epoch: [3/300][220/283], lr: 0.00000011 	 loss = 0.3702(0.7777)
2023/11/22 17:10:41 - INFO - root -   Epoch: [3/300][240/283], lr: 0.00000011 	 loss = 0.8611(0.7761)
2023/11/22 17:11:02 - INFO - root -   Epoch: [3/300][260/283], lr: 0.00000011 	 loss = 1.7025(0.7643)
2023/11/22 17:11:14 - INFO - root -   Epoch: [3/300][280/283], lr: 0.00000011 	 loss = 2.5255(0.7602)
2023/11/22 17:11:15 - INFO - root -   Epoch: [3/300] 	 loss = 0.7574
2023/11/22 17:11:15 - INFO - root -   train_accuracy = 0.7226
2023/11/22 17:11:24 - INFO - root -   Epoch: [4/300][0/283], lr: 0.00000011 	 loss = 0.2345(0.2345)
2023/11/22 17:11:43 - INFO - root -   Epoch: [4/300][20/283], lr: 0.00000011 	 loss = 2.8648(1.0738)
2023/11/22 17:12:00 - INFO - root -   Epoch: [4/300][40/283], lr: 0.00000011 	 loss = 0.6068(1.1045)
2023/11/22 17:12:23 - INFO - root -   Epoch: [4/300][60/283], lr: 0.00000011 	 loss = 0.2524(1.0682)
2023/11/22 17:12:31 - INFO - root -   Epoch: [4/300][80/283], lr: 0.00000011 	 loss = 0.1595(1.0796)
2023/11/22 17:12:52 - INFO - root -   Epoch: [4/300][100/283], lr: 0.00000011 	 loss = 0.3595(1.0233)
2023/11/22 17:13:10 - INFO - root -   Epoch: [4/300][120/283], lr: 0.00000011 	 loss = 0.6895(0.9362)
2023/11/22 17:13:33 - INFO - root -   Epoch: [4/300][140/283], lr: 0.00000011 	 loss = 0.1167(0.8658)
2023/11/22 17:13:49 - INFO - root -   Epoch: [4/300][160/283], lr: 0.00000011 	 loss = 2.0932(0.8358)
2023/11/22 17:14:03 - INFO - root -   Epoch: [4/300][180/283], lr: 0.00000011 	 loss = 0.2263(0.8460)
2023/11/22 17:14:15 - INFO - root -   Epoch: [4/300][200/283], lr: 0.00000011 	 loss = 0.1870(0.8180)
2023/11/22 17:14:34 - INFO - root -   Epoch: [4/300][220/283], lr: 0.00000011 	 loss = 0.3966(0.7990)
2023/11/22 17:14:44 - INFO - root -   Epoch: [4/300][240/283], lr: 0.00000011 	 loss = 0.1590(0.8002)
2023/11/22 17:15:02 - INFO - root -   Epoch: [4/300][260/283], lr: 0.00000011 	 loss = 1.8826(0.7876)
2023/11/22 17:15:16 - INFO - root -   Epoch: [4/300][280/283], lr: 0.00000011 	 loss = 1.6879(0.7700)
2023/11/22 17:15:17 - INFO - root -   Epoch: [4/300] 	 loss = 0.7672
2023/11/22 17:18:18 - INFO - root -   precision = 0.7343
2023/11/22 17:18:18 - INFO - root -   eval_loss = 0.6525
2023/11/22 17:18:18 - INFO - root -   eval_acc = 0.7343
2023/11/22 17:18:19 - INFO - root -   train_accuracy = 0.7226
2023/11/22 17:18:26 - INFO - root -   Epoch: [5/300][0/283], lr: 0.00000011 	 loss = 0.3823(0.3823)
2023/11/22 17:18:44 - INFO - root -   Epoch: [5/300][20/283], lr: 0.00000011 	 loss = 2.4783(1.0158)
2023/11/22 17:18:57 - INFO - root -   Epoch: [5/300][40/283], lr: 0.00000011 	 loss = 0.1690(1.0101)
2023/11/22 17:19:14 - INFO - root -   Epoch: [5/300][60/283], lr: 0.00000011 	 loss = 0.3531(0.9873)
2023/11/22 17:19:31 - INFO - root -   Epoch: [5/300][80/283], lr: 0.00000011 	 loss = 0.3159(0.9726)
2023/11/22 17:19:43 - INFO - root -   Epoch: [5/300][100/283], lr: 0.00000011 	 loss = 0.5694(0.9241)
2023/11/22 17:19:57 - INFO - root -   Epoch: [5/300][120/283], lr: 0.00000011 	 loss = 0.4433(0.8442)
2023/11/22 17:20:12 - INFO - root -   Epoch: [5/300][140/283], lr: 0.00000011 	 loss = 0.1296(0.7841)
2023/11/22 17:20:29 - INFO - root -   Epoch: [5/300][160/283], lr: 0.00000011 	 loss = 1.0460(0.7626)
2023/11/22 17:20:50 - INFO - root -   Epoch: [5/300][180/283], lr: 0.00000011 	 loss = 0.4061(0.7686)
2023/11/22 17:21:06 - INFO - root -   Epoch: [5/300][200/283], lr: 0.00000011 	 loss = 0.2326(0.7466)
2023/11/22 17:21:17 - INFO - root -   Epoch: [5/300][220/283], lr: 0.00000011 	 loss = 0.1914(0.7292)
2023/11/22 17:21:28 - INFO - root -   Epoch: [5/300][240/283], lr: 0.00000011 	 loss = 0.1945(0.7355)
2023/11/22 17:21:43 - INFO - root -   Epoch: [5/300][260/283], lr: 0.00000011 	 loss = 1.5601(0.7211)
2023/11/22 17:22:01 - INFO - root -   Epoch: [5/300][280/283], lr: 0.00000011 	 loss = 1.9619(0.7199)
2023/11/22 17:22:02 - INFO - root -   Epoch: [5/300] 	 loss = 0.7154
2023/11/22 17:22:02 - INFO - root -   train_accuracy = 0.7403
2023/11/22 17:22:03 - INFO - root -   Epoch: [6/300][0/283], lr: 0.00000011 	 loss = 0.2174(0.2174)
2023/11/22 17:22:25 - INFO - root -   Epoch: [6/300][20/283], lr: 0.00000011 	 loss = 1.7481(1.0105)
2023/11/22 17:22:36 - INFO - root -   Epoch: [6/300][40/283], lr: 0.00000011 	 loss = 0.2883(0.9234)
2023/11/22 17:22:49 - INFO - root -   Epoch: [6/300][60/283], lr: 0.00000011 	 loss = 0.1905(0.9372)
2023/11/22 17:23:01 - INFO - root -   Epoch: [6/300][80/283], lr: 0.00000011 	 loss = 0.1970(0.9471)
2023/11/22 17:23:14 - INFO - root -   Epoch: [6/300][100/283], lr: 0.00000011 	 loss = 0.2040(0.8909)
2023/11/22 17:23:30 - INFO - root -   Epoch: [6/300][120/283], lr: 0.00000011 	 loss = 0.6023(0.8402)
2023/11/22 17:23:43 - INFO - root -   Epoch: [6/300][140/283], lr: 0.00000011 	 loss = 0.0879(0.7850)
2023/11/22 17:24:07 - INFO - root -   Epoch: [6/300][160/283], lr: 0.00000011 	 loss = 1.4710(0.7675)
2023/11/22 17:24:34 - INFO - root -   Epoch: [6/300][180/283], lr: 0.00000011 	 loss = 0.4567(0.7636)
2023/11/22 17:24:47 - INFO - root -   Epoch: [6/300][200/283], lr: 0.00000011 	 loss = 0.3915(0.7430)
2023/11/22 17:25:08 - INFO - root -   Epoch: [6/300][220/283], lr: 0.00000011 	 loss = 0.1274(0.7295)
2023/11/22 17:25:22 - INFO - root -   Epoch: [6/300][240/283], lr: 0.00000011 	 loss = 0.8240(0.7281)
2023/11/22 17:25:44 - INFO - root -   Epoch: [6/300][260/283], lr: 0.00000011 	 loss = 1.6682(0.7177)
2023/11/22 17:25:58 - INFO - root -   Epoch: [6/300][280/283], lr: 0.00000011 	 loss = 1.8821(0.7070)
2023/11/22 17:25:59 - INFO - root -   Epoch: [6/300] 	 loss = 0.7038
2023/11/22 17:25:59 - INFO - root -   train_accuracy = 0.7279
2023/11/22 17:26:00 - INFO - root -   Epoch: [7/300][0/283], lr: 0.00000011 	 loss = 0.4118(0.4118)
2023/11/22 17:26:23 - INFO - root -   Epoch: [7/300][20/283], lr: 0.00000011 	 loss = 2.1988(1.0047)
2023/11/22 17:26:39 - INFO - root -   Epoch: [7/300][40/283], lr: 0.00000011 	 loss = 0.2539(0.9751)
2023/11/22 17:26:51 - INFO - root -   Epoch: [7/300][60/283], lr: 0.00000011 	 loss = 0.4189(0.9799)
2023/11/22 17:27:06 - INFO - root -   Epoch: [7/300][80/283], lr: 0.00000011 	 loss = 0.2594(1.0207)
2023/11/22 17:27:15 - INFO - root -   Epoch: [7/300][100/283], lr: 0.00000011 	 loss = 0.2206(0.9496)
2023/11/22 17:27:30 - INFO - root -   Epoch: [7/300][120/283], lr: 0.00000011 	 loss = 0.3481(0.8628)
2023/11/22 17:27:47 - INFO - root -   Epoch: [7/300][140/283], lr: 0.00000011 	 loss = 0.2114(0.8105)
2023/11/22 17:28:00 - INFO - root -   Epoch: [7/300][160/283], lr: 0.00000011 	 loss = 1.8673(0.7929)
2023/11/22 17:28:15 - INFO - root -   Epoch: [7/300][180/283], lr: 0.00000011 	 loss = 0.2628(0.7841)
2023/11/22 17:28:23 - INFO - root -   Epoch: [7/300][200/283], lr: 0.00000011 	 loss = 0.6695(0.7563)
2023/11/22 17:28:44 - INFO - root -   Epoch: [7/300][220/283], lr: 0.00000011 	 loss = 0.1566(0.7477)
2023/11/22 17:28:56 - INFO - root -   Epoch: [7/300][240/283], lr: 0.00000011 	 loss = 0.1225(0.7337)
2023/11/22 17:29:10 - INFO - root -   Epoch: [7/300][260/283], lr: 0.00000011 	 loss = 1.7837(0.7212)
2023/11/22 17:29:30 - INFO - root -   Epoch: [7/300][280/283], lr: 0.00000011 	 loss = 1.6358(0.7028)
2023/11/22 17:29:31 - INFO - root -   Epoch: [7/300] 	 loss = 0.6998
2023/11/22 17:29:31 - INFO - root -   train_accuracy = 0.7297
2023/11/22 17:29:33 - INFO - root -   Epoch: [8/300][0/283], lr: 0.00000011 	 loss = 0.4835(0.4835)
2023/11/22 17:29:55 - INFO - root -   Epoch: [8/300][20/283], lr: 0.00000011 	 loss = 2.7175(0.8407)
2023/11/22 17:30:04 - INFO - root -   Epoch: [8/300][40/283], lr: 0.00000011 	 loss = 0.0576(0.8644)
2023/11/22 17:30:12 - INFO - root -   Epoch: [8/300][60/283], lr: 0.00000011 	 loss = 0.2157(0.8994)
2023/11/22 17:30:24 - INFO - root -   Epoch: [8/300][80/283], lr: 0.00000011 	 loss = 0.0618(0.9125)
2023/11/22 17:30:41 - INFO - root -   Epoch: [8/300][100/283], lr: 0.00000011 	 loss = 0.1237(0.8482)
2023/11/22 17:30:49 - INFO - root -   Epoch: [8/300][120/283], lr: 0.00000011 	 loss = 0.3436(0.7954)
2023/11/22 17:31:06 - INFO - root -   Epoch: [8/300][140/283], lr: 0.00000011 	 loss = 0.1692(0.7351)
2023/11/22 17:31:26 - INFO - root -   Epoch: [8/300][160/283], lr: 0.00000011 	 loss = 1.2752(0.7158)
2023/11/22 17:31:36 - INFO - root -   Epoch: [8/300][180/283], lr: 0.00000011 	 loss = 0.1932(0.7308)
2023/11/22 17:31:50 - INFO - root -   Epoch: [8/300][200/283], lr: 0.00000011 	 loss = 0.2942(0.7124)
2023/11/22 17:32:03 - INFO - root -   Epoch: [8/300][220/283], lr: 0.00000011 	 loss = 0.1268(0.6957)
2023/11/22 17:32:14 - INFO - root -   Epoch: [8/300][240/283], lr: 0.00000011 	 loss = 0.1422(0.6871)
2023/11/22 17:32:24 - INFO - root -   Epoch: [8/300][260/283], lr: 0.00000011 	 loss = 1.6190(0.6746)
2023/11/22 17:32:41 - INFO - root -   Epoch: [8/300][280/283], lr: 0.00000011 	 loss = 1.6506(0.6674)
2023/11/22 17:32:42 - INFO - root -   Epoch: [8/300] 	 loss = 0.6641
2023/11/22 17:32:42 - INFO - root -   train_accuracy = 0.7438
2023/11/22 17:32:44 - INFO - root -   Epoch: [9/300][0/283], lr: 0.00000012 	 loss = 0.1191(0.1191)
2023/11/22 17:33:06 - INFO - root -   Epoch: [9/300][20/283], lr: 0.00000012 	 loss = 1.7565(0.9737)
2023/11/22 17:33:17 - INFO - root -   Epoch: [9/300][40/283], lr: 0.00000012 	 loss = 0.1467(0.9908)
2023/11/22 17:33:27 - INFO - root -   Epoch: [9/300][60/283], lr: 0.00000012 	 loss = 0.1535(0.9679)
2023/11/22 17:33:37 - INFO - root -   Epoch: [9/300][80/283], lr: 0.00000012 	 loss = 0.1793(0.9573)
2023/11/22 17:33:51 - INFO - root -   Epoch: [9/300][100/283], lr: 0.00000012 	 loss = 0.4704(0.9031)
2023/11/22 17:34:07 - INFO - root -   Epoch: [9/300][120/283], lr: 0.00000012 	 loss = 0.0528(0.8345)
2023/11/22 17:34:24 - INFO - root -   Epoch: [9/300][140/283], lr: 0.00000012 	 loss = 0.1397(0.7789)
2023/11/22 17:34:41 - INFO - root -   Epoch: [9/300][160/283], lr: 0.00000012 	 loss = 1.2969(0.7580)
2023/11/22 17:35:02 - INFO - root -   Epoch: [9/300][180/283], lr: 0.00000012 	 loss = 0.2061(0.7587)
2023/11/22 17:35:16 - INFO - root -   Epoch: [9/300][200/283], lr: 0.00000012 	 loss = 0.1514(0.7408)
2023/11/22 17:35:28 - INFO - root -   Epoch: [9/300][220/283], lr: 0.00000012 	 loss = 0.0930(0.7219)
2023/11/22 17:35:39 - INFO - root -   Epoch: [9/300][240/283], lr: 0.00000012 	 loss = 0.6659(0.7107)
2023/11/22 17:35:51 - INFO - root -   Epoch: [9/300][260/283], lr: 0.00000012 	 loss = 1.0475(0.6916)
2023/11/22 17:36:07 - INFO - root -   Epoch: [9/300][280/283], lr: 0.00000012 	 loss = 1.6434(0.6792)
2023/11/22 17:36:08 - INFO - root -   Epoch: [9/300] 	 loss = 0.6763
2023/11/22 17:39:02 - INFO - root -   precision = 0.7413
2023/11/22 17:39:02 - INFO - root -   eval_loss = 0.6292
2023/11/22 17:39:02 - INFO - root -   eval_acc = 0.7413
2023/11/22 17:39:03 - INFO - root -   train_accuracy = 0.7403
2023/11/22 17:39:11 - INFO - root -   Epoch: [10/300][0/283], lr: 0.00000012 	 loss = 0.2794(0.2794)
2023/11/22 17:39:22 - INFO - root -   Epoch: [10/300][20/283], lr: 0.00000012 	 loss = 2.6555(0.9525)
2023/11/22 17:39:38 - INFO - root -   Epoch: [10/300][40/283], lr: 0.00000012 	 loss = 0.5866(0.8994)
2023/11/22 17:39:58 - INFO - root -   Epoch: [10/300][60/283], lr: 0.00000012 	 loss = 0.2419(0.8994)
2023/11/22 17:40:10 - INFO - root -   Epoch: [10/300][80/283], lr: 0.00000012 	 loss = 0.1999(0.8989)
2023/11/22 17:40:29 - INFO - root -   Epoch: [10/300][100/283], lr: 0.00000012 	 loss = 0.2205(0.8477)
2023/11/22 17:40:39 - INFO - root -   Epoch: [10/300][120/283], lr: 0.00000012 	 loss = 0.2900(0.7734)
2023/11/22 17:40:54 - INFO - root -   Epoch: [10/300][140/283], lr: 0.00000012 	 loss = 0.2050(0.7280)
2023/11/22 17:41:08 - INFO - root -   Epoch: [10/300][160/283], lr: 0.00000012 	 loss = 2.0768(0.7113)
2023/11/22 17:41:26 - INFO - root -   Epoch: [10/300][180/283], lr: 0.00000012 	 loss = 0.1737(0.7021)
2023/11/22 17:41:39 - INFO - root -   Epoch: [10/300][200/283], lr: 0.00000012 	 loss = 0.2879(0.6787)
2023/11/22 17:41:52 - INFO - root -   Epoch: [10/300][220/283], lr: 0.00000012 	 loss = 0.3964(0.6706)
2023/11/22 17:42:02 - INFO - root -   Epoch: [10/300][240/283], lr: 0.00000012 	 loss = 0.7599(0.6769)
2023/11/22 17:42:17 - INFO - root -   Epoch: [10/300][260/283], lr: 0.00000012 	 loss = 2.7147(0.6752)
2023/11/22 17:42:33 - INFO - root -   Epoch: [10/300][280/283], lr: 0.00000012 	 loss = 2.2861(0.6722)
2023/11/22 17:42:34 - INFO - root -   Epoch: [10/300] 	 loss = 0.6685
2023/11/22 17:42:34 - INFO - root -   train_accuracy = 0.7650
2023/11/22 17:42:36 - INFO - root -   Epoch: [11/300][0/283], lr: 0.00000012 	 loss = 0.0818(0.0818)
2023/11/22 17:42:56 - INFO - root -   Epoch: [11/300][20/283], lr: 0.00000012 	 loss = 2.2629(1.1563)
2023/11/22 17:43:16 - INFO - root -   Epoch: [11/300][40/283], lr: 0.00000012 	 loss = 0.1380(1.0683)
2023/11/22 17:43:34 - INFO - root -   Epoch: [11/300][60/283], lr: 0.00000012 	 loss = 0.1928(1.0556)
2023/11/22 17:43:42 - INFO - root -   Epoch: [11/300][80/283], lr: 0.00000012 	 loss = 0.3082(1.0328)
2023/11/22 17:43:57 - INFO - root -   Epoch: [11/300][100/283], lr: 0.00000012 	 loss = 0.2521(0.9282)
2023/11/22 17:44:19 - INFO - root -   Epoch: [11/300][120/283], lr: 0.00000012 	 loss = 0.1591(0.8452)
2023/11/22 17:44:31 - INFO - root -   Epoch: [11/300][140/283], lr: 0.00000012 	 loss = 0.5349(0.7983)
2023/11/22 17:44:49 - INFO - root -   Epoch: [11/300][160/283], lr: 0.00000012 	 loss = 2.1389(0.7775)
2023/11/22 17:45:07 - INFO - root -   Epoch: [11/300][180/283], lr: 0.00000012 	 loss = 0.2228(0.7730)
2023/11/22 17:45:17 - INFO - root -   Epoch: [11/300][200/283], lr: 0.00000012 	 loss = 0.1791(0.7371)
2023/11/22 17:45:31 - INFO - root -   Epoch: [11/300][220/283], lr: 0.00000012 	 loss = 0.5241(0.7293)
2023/11/22 17:45:45 - INFO - root -   Epoch: [11/300][240/283], lr: 0.00000012 	 loss = 0.7769(0.7150)
2023/11/22 17:45:56 - INFO - root -   Epoch: [11/300][260/283], lr: 0.00000012 	 loss = 1.7872(0.7032)
2023/11/22 17:46:09 - INFO - root -   Epoch: [11/300][280/283], lr: 0.00000012 	 loss = 2.2406(0.6852)
2023/11/22 17:46:10 - INFO - root -   Epoch: [11/300] 	 loss = 0.6815
2023/11/22 17:46:10 - INFO - root -   train_accuracy = 0.7597
2023/11/22 17:46:16 - INFO - root -   Epoch: [12/300][0/283], lr: 0.00000012 	 loss = 0.1553(0.1553)
2023/11/22 17:46:24 - INFO - root -   Epoch: [12/300][20/283], lr: 0.00000012 	 loss = 1.4028(0.9278)
2023/11/22 17:46:37 - INFO - root -   Epoch: [12/300][40/283], lr: 0.00000012 	 loss = 0.1003(0.8510)
2023/11/22 17:46:51 - INFO - root -   Epoch: [12/300][60/283], lr: 0.00000012 	 loss = 0.1784(0.8747)
2023/11/22 17:47:09 - INFO - root -   Epoch: [12/300][80/283], lr: 0.00000012 	 loss = 0.3826(0.9065)
2023/11/22 17:47:20 - INFO - root -   Epoch: [12/300][100/283], lr: 0.00000012 	 loss = 0.5067(0.8454)
2023/11/22 17:47:41 - INFO - root -   Epoch: [12/300][120/283], lr: 0.00000012 	 loss = 0.4959(0.7741)
2023/11/22 17:47:51 - INFO - root -   Epoch: [12/300][140/283], lr: 0.00000012 	 loss = 0.0569(0.7134)
2023/11/22 17:48:08 - INFO - root -   Epoch: [12/300][160/283], lr: 0.00000012 	 loss = 1.9476(0.7030)
2023/11/22 17:48:16 - INFO - root -   Epoch: [12/300][180/283], lr: 0.00000012 	 loss = 0.4563(0.7148)
2023/11/22 17:48:25 - INFO - root -   Epoch: [12/300][200/283], lr: 0.00000012 	 loss = 0.1789(0.6965)
2023/11/22 17:48:41 - INFO - root -   Epoch: [12/300][220/283], lr: 0.00000012 	 loss = 0.4458(0.6791)
2023/11/22 17:48:54 - INFO - root -   Epoch: [12/300][240/283], lr: 0.00000012 	 loss = 0.6057(0.6750)
2023/11/22 17:49:08 - INFO - root -   Epoch: [12/300][260/283], lr: 0.00000012 	 loss = 2.0697(0.6594)
2023/11/22 17:49:18 - INFO - root -   Epoch: [12/300][280/283], lr: 0.00000012 	 loss = 2.1910(0.6462)
2023/11/22 17:49:19 - INFO - root -   Epoch: [12/300] 	 loss = 0.6434
2023/11/22 17:49:19 - INFO - root -   train_accuracy = 0.7668
2023/11/22 17:49:26 - INFO - root -   Epoch: [13/300][0/283], lr: 0.00000012 	 loss = 0.1039(0.1039)
2023/11/22 17:49:36 - INFO - root -   Epoch: [13/300][20/283], lr: 0.00000012 	 loss = 2.3417(0.8037)
2023/11/22 17:49:52 - INFO - root -   Epoch: [13/300][40/283], lr: 0.00000012 	 loss = 0.3044(0.8627)
2023/11/22 17:50:04 - INFO - root -   Epoch: [13/300][60/283], lr: 0.00000012 	 loss = 0.4223(0.8937)
2023/11/22 17:50:21 - INFO - root -   Epoch: [13/300][80/283], lr: 0.00000012 	 loss = 0.4413(0.9070)
2023/11/22 17:50:33 - INFO - root -   Epoch: [13/300][100/283], lr: 0.00000012 	 loss = 0.3817(0.8827)
2023/11/22 17:50:53 - INFO - root -   Epoch: [13/300][120/283], lr: 0.00000012 	 loss = 0.7583(0.8160)
2023/11/22 17:51:01 - INFO - root -   Epoch: [13/300][140/283], lr: 0.00000012 	 loss = 0.0979(0.7498)
2023/11/22 17:51:22 - INFO - root -   Epoch: [13/300][160/283], lr: 0.00000012 	 loss = 0.7965(0.7244)
2023/11/22 17:51:30 - INFO - root -   Epoch: [13/300][180/283], lr: 0.00000012 	 loss = 0.2781(0.7254)
2023/11/22 17:51:50 - INFO - root -   Epoch: [13/300][200/283], lr: 0.00000012 	 loss = 0.0868(0.7057)
2023/11/22 17:52:09 - INFO - root -   Epoch: [13/300][220/283], lr: 0.00000012 	 loss = 0.1606(0.6922)
2023/11/22 17:52:23 - INFO - root -   Epoch: [13/300][240/283], lr: 0.00000012 	 loss = 0.2979(0.6799)
2023/11/22 17:52:44 - INFO - root -   Epoch: [13/300][260/283], lr: 0.00000012 	 loss = 1.6504(0.6676)
2023/11/22 17:52:52 - INFO - root -   Epoch: [13/300][280/283], lr: 0.00000012 	 loss = 2.2851(0.6514)
2023/11/22 17:52:53 - INFO - root -   Epoch: [13/300] 	 loss = 0.6478
2023/11/22 17:52:53 - INFO - root -   train_accuracy = 0.7633
2023/11/22 17:52:54 - INFO - root -   Epoch: [14/300][0/283], lr: 0.00000012 	 loss = 0.2055(0.2055)
2023/11/22 17:53:20 - INFO - root -   Epoch: [14/300][20/283], lr: 0.00000012 	 loss = 2.6862(0.9153)
2023/11/22 17:53:39 - INFO - root -   Epoch: [14/300][40/283], lr: 0.00000012 	 loss = 0.1814(0.9371)
2023/11/22 17:53:53 - INFO - root -   Epoch: [14/300][60/283], lr: 0.00000012 	 loss = 0.1605(0.8826)
2023/11/22 17:54:09 - INFO - root -   Epoch: [14/300][80/283], lr: 0.00000012 	 loss = 0.2016(0.8823)
2023/11/22 17:54:24 - INFO - root -   Epoch: [14/300][100/283], lr: 0.00000012 	 loss = 0.8732(0.8254)
2023/11/22 17:54:35 - INFO - root -   Epoch: [14/300][120/283], lr: 0.00000012 	 loss = 0.8109(0.7843)
2023/11/22 17:54:44 - INFO - root -   Epoch: [14/300][140/283], lr: 0.00000012 	 loss = 0.2860(0.7380)
2023/11/22 17:55:06 - INFO - root -   Epoch: [14/300][160/283], lr: 0.00000012 	 loss = 0.3099(0.7247)
2023/11/22 17:55:15 - INFO - root -   Epoch: [14/300][180/283], lr: 0.00000012 	 loss = 0.2946(0.7231)
2023/11/22 17:55:26 - INFO - root -   Epoch: [14/300][200/283], lr: 0.00000012 	 loss = 0.2371(0.6906)
2023/11/22 17:55:50 - INFO - root -   Epoch: [14/300][220/283], lr: 0.00000012 	 loss = 0.4385(0.6719)
2023/11/22 17:56:11 - INFO - root -   Epoch: [14/300][240/283], lr: 0.00000012 	 loss = 0.4941(0.6641)
2023/11/22 17:56:25 - INFO - root -   Epoch: [14/300][260/283], lr: 0.00000012 	 loss = 1.9250(0.6578)
2023/11/22 17:56:37 - INFO - root -   Epoch: [14/300][280/283], lr: 0.00000012 	 loss = 1.7639(0.6460)
2023/11/22 17:56:38 - INFO - root -   Epoch: [14/300] 	 loss = 0.6418
2023/11/22 17:59:32 - INFO - root -   precision = 0.7413
2023/11/22 17:59:32 - INFO - root -   eval_loss = 0.5948
2023/11/22 17:59:32 - INFO - root -   eval_acc = 0.7413
2023/11/22 17:59:33 - INFO - root -   train_accuracy = 0.7527
2023/11/22 17:59:34 - INFO - root -   Epoch: [15/300][0/283], lr: 0.00000013 	 loss = 0.1298(0.1298)
2023/11/22 17:59:56 - INFO - root -   Epoch: [15/300][20/283], lr: 0.00000013 	 loss = 2.1853(1.0142)
2023/11/22 18:00:08 - INFO - root -   Epoch: [15/300][40/283], lr: 0.00000013 	 loss = 0.1265(0.9078)
2023/11/22 18:00:23 - INFO - root -   Epoch: [15/300][60/283], lr: 0.00000013 	 loss = 0.1062(0.8859)
2023/11/22 18:00:42 - INFO - root -   Epoch: [15/300][80/283], lr: 0.00000013 	 loss = 0.1626(0.8931)
2023/11/22 18:00:54 - INFO - root -   Epoch: [15/300][100/283], lr: 0.00000013 	 loss = 0.9612(0.8591)
2023/11/22 18:01:14 - INFO - root -   Epoch: [15/300][120/283], lr: 0.00000013 	 loss = 0.8987(0.8139)
2023/11/22 18:01:28 - INFO - root -   Epoch: [15/300][140/283], lr: 0.00000013 	 loss = 0.1482(0.7620)
2023/11/22 18:01:38 - INFO - root -   Epoch: [15/300][160/283], lr: 0.00000013 	 loss = 1.1845(0.7515)
2023/11/22 18:01:56 - INFO - root -   Epoch: [15/300][180/283], lr: 0.00000013 	 loss = 0.0517(0.7492)
2023/11/22 18:02:09 - INFO - root -   Epoch: [15/300][200/283], lr: 0.00000013 	 loss = 0.3031(0.7169)
2023/11/22 18:02:21 - INFO - root -   Epoch: [15/300][220/283], lr: 0.00000013 	 loss = 0.8050(0.7063)
2023/11/22 18:02:41 - INFO - root -   Epoch: [15/300][240/283], lr: 0.00000013 	 loss = 0.6755(0.6933)
2023/11/22 18:02:49 - INFO - root -   Epoch: [15/300][260/283], lr: 0.00000013 	 loss = 2.5547(0.6755)
2023/11/22 18:03:07 - INFO - root -   Epoch: [15/300][280/283], lr: 0.00000013 	 loss = 2.4330(0.6659)
2023/11/22 18:03:08 - INFO - root -   Epoch: [15/300] 	 loss = 0.6627
2023/11/22 18:03:08 - INFO - root -   train_accuracy = 0.7456
2023/11/22 18:03:09 - INFO - root -   Epoch: [16/300][0/283], lr: 0.00000013 	 loss = 0.0968(0.0968)
2023/11/22 18:03:24 - INFO - root -   Epoch: [16/300][20/283], lr: 0.00000013 	 loss = 2.2114(0.9975)
2023/11/22 18:03:40 - INFO - root -   Epoch: [16/300][40/283], lr: 0.00000013 	 loss = 0.5160(0.8874)
2023/11/22 18:03:52 - INFO - root -   Epoch: [16/300][60/283], lr: 0.00000013 	 loss = 0.2958(0.8825)
2023/11/22 18:04:09 - INFO - root -   Epoch: [16/300][80/283], lr: 0.00000013 	 loss = 0.1076(0.8624)
2023/11/22 18:04:25 - INFO - root -   Epoch: [16/300][100/283], lr: 0.00000013 	 loss = 0.2249(0.8432)
2023/11/22 18:04:39 - INFO - root -   Epoch: [16/300][120/283], lr: 0.00000013 	 loss = 0.4522(0.7935)
2023/11/22 18:04:54 - INFO - root -   Epoch: [16/300][140/283], lr: 0.00000013 	 loss = 0.1009(0.7407)
2023/11/22 18:05:07 - INFO - root -   Epoch: [16/300][160/283], lr: 0.00000013 	 loss = 1.4476(0.7133)
2023/11/22 18:05:22 - INFO - root -   Epoch: [16/300][180/283], lr: 0.00000013 	 loss = 0.1302(0.7168)
2023/11/22 18:05:33 - INFO - root -   Epoch: [16/300][200/283], lr: 0.00000013 	 loss = 0.0576(0.6923)
2023/11/22 18:05:49 - INFO - root -   Epoch: [16/300][220/283], lr: 0.00000013 	 loss = 0.1330(0.6711)
2023/11/22 18:06:03 - INFO - root -   Epoch: [16/300][240/283], lr: 0.00000013 	 loss = 0.8092(0.6648)
2023/11/22 18:06:15 - INFO - root -   Epoch: [16/300][260/283], lr: 0.00000013 	 loss = 2.0934(0.6535)
2023/11/22 18:06:29 - INFO - root -   Epoch: [16/300][280/283], lr: 0.00000013 	 loss = 1.5475(0.6359)
2023/11/22 18:06:30 - INFO - root -   Epoch: [16/300] 	 loss = 0.6327
2023/11/22 18:06:30 - INFO - root -   train_accuracy = 0.7527
2023/11/22 18:06:32 - INFO - root -   Epoch: [17/300][0/283], lr: 0.00000013 	 loss = 0.2159(0.2159)
2023/11/22 18:06:50 - INFO - root -   Epoch: [17/300][20/283], lr: 0.00000013 	 loss = 2.6478(0.9896)
2023/11/22 18:06:58 - INFO - root -   Epoch: [17/300][40/283], lr: 0.00000013 	 loss = 0.1900(0.9095)
2023/11/22 18:07:21 - INFO - root -   Epoch: [17/300][60/283], lr: 0.00000013 	 loss = 0.3048(0.9017)
2023/11/22 18:07:31 - INFO - root -   Epoch: [17/300][80/283], lr: 0.00000013 	 loss = 0.3361(0.8917)
2023/11/22 18:07:51 - INFO - root -   Epoch: [17/300][100/283], lr: 0.00000013 	 loss = 0.5145(0.8370)
2023/11/22 18:08:06 - INFO - root -   Epoch: [17/300][120/283], lr: 0.00000013 	 loss = 0.2348(0.7717)
2023/11/22 18:08:20 - INFO - root -   Epoch: [17/300][140/283], lr: 0.00000013 	 loss = 0.0762(0.7028)
2023/11/22 18:08:30 - INFO - root -   Epoch: [17/300][160/283], lr: 0.00000013 	 loss = 1.5563(0.6944)
2023/11/22 18:08:47 - INFO - root -   Epoch: [17/300][180/283], lr: 0.00000013 	 loss = 0.2990(0.6997)
2023/11/22 18:09:10 - INFO - root -   Epoch: [17/300][200/283], lr: 0.00000013 	 loss = 0.5144(0.6745)
2023/11/22 18:09:20 - INFO - root -   Epoch: [17/300][220/283], lr: 0.00000013 	 loss = 0.1096(0.6523)
2023/11/22 18:09:35 - INFO - root -   Epoch: [17/300][240/283], lr: 0.00000013 	 loss = 0.1845(0.6520)
2023/11/22 18:09:49 - INFO - root -   Epoch: [17/300][260/283], lr: 0.00000013 	 loss = 1.4609(0.6371)
2023/11/22 18:10:00 - INFO - root -   Epoch: [17/300][280/283], lr: 0.00000013 	 loss = 2.1547(0.6249)
2023/11/22 18:10:01 - INFO - root -   Epoch: [17/300] 	 loss = 0.6221
2023/11/22 18:10:01 - INFO - root -   train_accuracy = 0.7933
2023/11/22 18:10:07 - INFO - root -   Epoch: [18/300][0/283], lr: 0.00000013 	 loss = 0.1358(0.1358)
2023/11/22 18:10:21 - INFO - root -   Epoch: [18/300][20/283], lr: 0.00000013 	 loss = 3.1993(0.9327)
2023/11/22 18:10:31 - INFO - root -   Epoch: [18/300][40/283], lr: 0.00000013 	 loss = 0.3471(0.8614)
2023/11/22 18:10:43 - INFO - root -   Epoch: [18/300][60/283], lr: 0.00000013 	 loss = 0.0561(0.8288)
2023/11/22 18:10:54 - INFO - root -   Epoch: [18/300][80/283], lr: 0.00000013 	 loss = 0.1121(0.8504)
2023/11/22 18:11:10 - INFO - root -   Epoch: [18/300][100/283], lr: 0.00000013 	 loss = 0.3415(0.8120)
2023/11/22 18:11:25 - INFO - root -   Epoch: [18/300][120/283], lr: 0.00000013 	 loss = 0.3047(0.7309)
2023/11/22 18:11:43 - INFO - root -   Epoch: [18/300][140/283], lr: 0.00000013 	 loss = 0.1973(0.6795)
2023/11/22 18:11:56 - INFO - root -   Epoch: [18/300][160/283], lr: 0.00000013 	 loss = 1.1358(0.6699)
2023/11/22 18:12:07 - INFO - root -   Epoch: [18/300][180/283], lr: 0.00000013 	 loss = 0.0401(0.6618)
2023/11/22 18:12:25 - INFO - root -   Epoch: [18/300][200/283], lr: 0.00000013 	 loss = 0.1146(0.6478)
2023/11/22 18:12:39 - INFO - root -   Epoch: [18/300][220/283], lr: 0.00000013 	 loss = 0.1699(0.6253)
2023/11/22 18:12:53 - INFO - root -   Epoch: [18/300][240/283], lr: 0.00000013 	 loss = 0.8042(0.6320)
2023/11/22 18:13:06 - INFO - root -   Epoch: [18/300][260/283], lr: 0.00000013 	 loss = 1.1776(0.6310)
2023/11/22 18:13:22 - INFO - root -   Epoch: [18/300][280/283], lr: 0.00000013 	 loss = 2.1957(0.6228)
2023/11/22 18:13:23 - INFO - root -   Epoch: [18/300] 	 loss = 0.6190
2023/11/22 18:13:23 - INFO - root -   train_accuracy = 0.7986
2023/11/22 18:13:33 - INFO - root -   Epoch: [19/300][0/283], lr: 0.00000013 	 loss = 0.1123(0.1123)
2023/11/22 18:13:41 - INFO - root -   Epoch: [19/300][20/283], lr: 0.00000013 	 loss = 2.3223(0.8847)
2023/11/22 18:14:01 - INFO - root -   Epoch: [19/300][40/283], lr: 0.00000013 	 loss = 0.1761(0.8075)
2023/11/22 18:14:17 - INFO - root -   Epoch: [19/300][60/283], lr: 0.00000013 	 loss = 0.1464(0.8465)
2023/11/22 18:14:25 - INFO - root -   Epoch: [19/300][80/283], lr: 0.00000013 	 loss = 0.1334(0.8753)
2023/11/22 18:14:44 - INFO - root -   Epoch: [19/300][100/283], lr: 0.00000013 	 loss = 0.2271(0.8092)
2023/11/22 18:14:58 - INFO - root -   Epoch: [19/300][120/283], lr: 0.00000013 	 loss = 0.1195(0.7455)
2023/11/22 18:15:18 - INFO - root -   Epoch: [19/300][140/283], lr: 0.00000013 	 loss = 0.3205(0.6987)
2023/11/22 18:15:27 - INFO - root -   Epoch: [19/300][160/283], lr: 0.00000013 	 loss = 1.5392(0.6910)
2023/11/22 18:15:38 - INFO - root -   Epoch: [19/300][180/283], lr: 0.00000013 	 loss = 0.2577(0.6725)
2023/11/22 18:15:52 - INFO - root -   Epoch: [19/300][200/283], lr: 0.00000013 	 loss = 0.5141(0.6633)
2023/11/22 18:16:11 - INFO - root -   Epoch: [19/300][220/283], lr: 0.00000013 	 loss = 0.1769(0.6458)
2023/11/22 18:16:29 - INFO - root -   Epoch: [19/300][240/283], lr: 0.00000013 	 loss = 0.2466(0.6402)
2023/11/22 18:16:42 - INFO - root -   Epoch: [19/300][260/283], lr: 0.00000013 	 loss = 0.8617(0.6244)
2023/11/22 18:16:50 - INFO - root -   Epoch: [19/300][280/283], lr: 0.00000013 	 loss = 1.9709(0.6174)
2023/11/22 18:16:51 - INFO - root -   Epoch: [19/300] 	 loss = 0.6139
2023/11/22 18:19:44 - INFO - root -   precision = 0.7762
2023/11/22 18:19:44 - INFO - root -   eval_loss = 0.5327
2023/11/22 18:19:44 - INFO - root -   eval_acc = 0.7762
2023/11/22 18:19:45 - INFO - root -   train_accuracy = 0.7880
2023/11/22 18:19:51 - INFO - root -   Epoch: [20/300][0/283], lr: 0.00000013 	 loss = 0.2174(0.2174)
2023/11/22 18:20:12 - INFO - root -   Epoch: [20/300][20/283], lr: 0.00000013 	 loss = 1.6581(0.7860)
2023/11/22 18:20:28 - INFO - root -   Epoch: [20/300][40/283], lr: 0.00000013 	 loss = 0.2642(0.8280)
2023/11/22 18:20:44 - INFO - root -   Epoch: [20/300][60/283], lr: 0.00000013 	 loss = 0.3233(0.8178)
2023/11/22 18:20:57 - INFO - root -   Epoch: [20/300][80/283], lr: 0.00000013 	 loss = 0.0568(0.8293)
2023/11/22 18:21:11 - INFO - root -   Epoch: [20/300][100/283], lr: 0.00000013 	 loss = 0.1947(0.7824)
2023/11/22 18:21:30 - INFO - root -   Epoch: [20/300][120/283], lr: 0.00000013 	 loss = 0.2052(0.7177)
2023/11/22 18:21:42 - INFO - root -   Epoch: [20/300][140/283], lr: 0.00000013 	 loss = 0.0736(0.6722)
2023/11/22 18:21:51 - INFO - root -   Epoch: [20/300][160/283], lr: 0.00000013 	 loss = 0.9150(0.6384)
2023/11/22 18:22:05 - INFO - root -   Epoch: [20/300][180/283], lr: 0.00000013 	 loss = 0.1127(0.6480)
2023/11/22 18:22:16 - INFO - root -   Epoch: [20/300][200/283], lr: 0.00000013 	 loss = 0.1981(0.6232)
2023/11/22 18:22:32 - INFO - root -   Epoch: [20/300][220/283], lr: 0.00000013 	 loss = 0.1563(0.6074)
2023/11/22 18:22:53 - INFO - root -   Epoch: [20/300][240/283], lr: 0.00000013 	 loss = 0.0927(0.6009)
2023/11/22 18:23:12 - INFO - root -   Epoch: [20/300][260/283], lr: 0.00000013 	 loss = 1.9392(0.5958)
2023/11/22 18:23:24 - INFO - root -   Epoch: [20/300][280/283], lr: 0.00000013 	 loss = 1.1207(0.5828)
2023/11/22 18:23:25 - INFO - root -   Epoch: [20/300] 	 loss = 0.5797
2023/11/22 18:23:25 - INFO - root -   train_accuracy = 0.7933
2023/11/22 18:23:26 - INFO - root -   Epoch: [21/300][0/283], lr: 0.00000014 	 loss = 0.1677(0.1677)
2023/11/22 18:23:56 - INFO - root -   Epoch: [21/300][20/283], lr: 0.00000014 	 loss = 2.1150(0.9429)
2023/11/22 18:24:07 - INFO - root -   Epoch: [21/300][40/283], lr: 0.00000014 	 loss = 0.6880(0.9630)
2023/11/22 18:24:30 - INFO - root -   Epoch: [21/300][60/283], lr: 0.00000014 	 loss = 0.3838(0.9137)
2023/11/22 18:24:48 - INFO - root -   Epoch: [21/300][80/283], lr: 0.00000014 	 loss = 1.1508(0.8933)
2023/11/22 18:25:01 - INFO - root -   Epoch: [21/300][100/283], lr: 0.00000014 	 loss = 0.6948(0.8387)
2023/11/22 18:25:12 - INFO - root -   Epoch: [21/300][120/283], lr: 0.00000014 	 loss = 0.5897(0.7578)
2023/11/22 18:25:31 - INFO - root -   Epoch: [21/300][140/283], lr: 0.00000014 	 loss = 0.1252(0.6975)
2023/11/22 18:25:47 - INFO - root -   Epoch: [21/300][160/283], lr: 0.00000014 	 loss = 1.5329(0.6928)
2023/11/22 18:25:56 - INFO - root -   Epoch: [21/300][180/283], lr: 0.00000014 	 loss = 0.0329(0.6991)
2023/11/22 18:26:13 - INFO - root -   Epoch: [21/300][200/283], lr: 0.00000014 	 loss = 0.2727(0.6763)
2023/11/22 18:26:25 - INFO - root -   Epoch: [21/300][220/283], lr: 0.00000014 	 loss = 0.3617(0.6756)
2023/11/22 18:26:39 - INFO - root -   Epoch: [21/300][240/283], lr: 0.00000014 	 loss = 1.2391(0.6667)
2023/11/22 18:26:52 - INFO - root -   Epoch: [21/300][260/283], lr: 0.00000014 	 loss = 1.6663(0.6475)
2023/11/22 18:27:06 - INFO - root -   Epoch: [21/300][280/283], lr: 0.00000014 	 loss = 2.3093(0.6374)
2023/11/22 18:27:07 - INFO - root -   Epoch: [21/300] 	 loss = 0.6350
2023/11/22 18:27:07 - INFO - root -   train_accuracy = 0.7792
2023/11/22 18:27:16 - INFO - root -   Epoch: [22/300][0/283], lr: 0.00000014 	 loss = 0.3674(0.3674)
2023/11/22 18:27:32 - INFO - root -   Epoch: [22/300][20/283], lr: 0.00000014 	 loss = 2.1852(0.7730)
2023/11/22 18:27:52 - INFO - root -   Epoch: [22/300][40/283], lr: 0.00000014 	 loss = 0.2566(0.8573)
2023/11/22 18:28:06 - INFO - root -   Epoch: [22/300][60/283], lr: 0.00000014 	 loss = 0.1982(0.8253)
2023/11/22 18:28:22 - INFO - root -   Epoch: [22/300][80/283], lr: 0.00000014 	 loss = 0.1762(0.8490)
2023/11/22 18:28:40 - INFO - root -   Epoch: [22/300][100/283], lr: 0.00000014 	 loss = 0.1051(0.7736)
2023/11/22 18:28:54 - INFO - root -   Epoch: [22/300][120/283], lr: 0.00000014 	 loss = 0.1402(0.7006)
2023/11/22 18:29:09 - INFO - root -   Epoch: [22/300][140/283], lr: 0.00000014 	 loss = 0.4765(0.6612)
2023/11/22 18:29:32 - INFO - root -   Epoch: [22/300][160/283], lr: 0.00000014 	 loss = 1.7963(0.6515)
2023/11/22 18:29:52 - INFO - root -   Epoch: [22/300][180/283], lr: 0.00000014 	 loss = 0.2754(0.6703)
2023/11/22 18:30:14 - INFO - root -   Epoch: [22/300][200/283], lr: 0.00000014 	 loss = 0.0295(0.6524)
2023/11/22 18:30:23 - INFO - root -   Epoch: [22/300][220/283], lr: 0.00000014 	 loss = 0.2706(0.6369)
2023/11/22 18:30:51 - INFO - root -   Epoch: [22/300][240/283], lr: 0.00000014 	 loss = 0.8787(0.6298)
2023/11/22 18:30:59 - INFO - root -   Epoch: [22/300][260/283], lr: 0.00000014 	 loss = 1.3105(0.6179)
2023/11/22 18:31:07 - INFO - root -   Epoch: [22/300][280/283], lr: 0.00000014 	 loss = 2.8223(0.6065)
2023/11/22 18:31:08 - INFO - root -   Epoch: [22/300] 	 loss = 0.6041
2023/11/22 18:31:08 - INFO - root -   train_accuracy = 0.7668
2023/11/22 18:31:17 - INFO - root -   Epoch: [23/300][0/283], lr: 0.00000014 	 loss = 0.1933(0.1933)
2023/11/22 18:31:29 - INFO - root -   Epoch: [23/300][20/283], lr: 0.00000014 	 loss = 1.2734(0.7959)
2023/11/22 18:31:40 - INFO - root -   Epoch: [23/300][40/283], lr: 0.00000014 	 loss = 0.7952(0.7730)
2023/11/22 18:31:58 - INFO - root -   Epoch: [23/300][60/283], lr: 0.00000014 	 loss = 0.2226(0.7527)
2023/11/22 18:32:17 - INFO - root -   Epoch: [23/300][80/283], lr: 0.00000014 	 loss = 0.1258(0.7841)
2023/11/22 18:32:32 - INFO - root -   Epoch: [23/300][100/283], lr: 0.00000014 	 loss = 0.5390(0.7403)
2023/11/22 18:32:44 - INFO - root -   Epoch: [23/300][120/283], lr: 0.00000014 	 loss = 0.1158(0.6673)
2023/11/22 18:33:00 - INFO - root -   Epoch: [23/300][140/283], lr: 0.00000014 	 loss = 0.1287(0.6102)
2023/11/22 18:33:24 - INFO - root -   Epoch: [23/300][160/283], lr: 0.00000014 	 loss = 2.1818(0.6172)
2023/11/22 18:33:37 - INFO - root -   Epoch: [23/300][180/283], lr: 0.00000014 	 loss = 0.5022(0.6217)
2023/11/22 18:33:54 - INFO - root -   Epoch: [23/300][200/283], lr: 0.00000014 	 loss = 0.6348(0.6069)
2023/11/22 18:34:10 - INFO - root -   Epoch: [23/300][220/283], lr: 0.00000014 	 loss = 0.0950(0.5958)
2023/11/22 18:34:33 - INFO - root -   Epoch: [23/300][240/283], lr: 0.00000014 	 loss = 1.3631(0.5984)
2023/11/22 18:34:46 - INFO - root -   Epoch: [23/300][260/283], lr: 0.00000014 	 loss = 1.2940(0.5898)
2023/11/22 18:34:58 - INFO - root -   Epoch: [23/300][280/283], lr: 0.00000014 	 loss = 2.0729(0.5916)
2023/11/22 18:34:59 - INFO - root -   Epoch: [23/300] 	 loss = 0.5879
2023/11/22 18:34:59 - INFO - root -   train_accuracy = 0.7809
2023/11/22 18:35:07 - INFO - root -   Epoch: [24/300][0/283], lr: 0.00000014 	 loss = 0.2071(0.2071)
2023/11/22 18:35:28 - INFO - root -   Epoch: [24/300][20/283], lr: 0.00000014 	 loss = 2.1499(1.0211)
2023/11/22 18:35:46 - INFO - root -   Epoch: [24/300][40/283], lr: 0.00000014 	 loss = 0.1039(0.9630)
2023/11/22 18:35:58 - INFO - root -   Epoch: [24/300][60/283], lr: 0.00000014 	 loss = 0.3526(0.9409)
2023/11/22 18:36:12 - INFO - root -   Epoch: [24/300][80/283], lr: 0.00000014 	 loss = 0.2001(0.9330)
2023/11/22 18:36:36 - INFO - root -   Epoch: [24/300][100/283], lr: 0.00000014 	 loss = 1.2393(0.8509)
2023/11/22 18:36:52 - INFO - root -   Epoch: [24/300][120/283], lr: 0.00000014 	 loss = 0.3082(0.7768)
2023/11/22 18:37:04 - INFO - root -   Epoch: [24/300][140/283], lr: 0.00000014 	 loss = 0.1023(0.7195)
2023/11/22 18:37:21 - INFO - root -   Epoch: [24/300][160/283], lr: 0.00000014 	 loss = 1.4732(0.6885)
2023/11/22 18:37:37 - INFO - root -   Epoch: [24/300][180/283], lr: 0.00000014 	 loss = 0.0892(0.6809)
2023/11/22 18:37:49 - INFO - root -   Epoch: [24/300][200/283], lr: 0.00000014 	 loss = 0.0835(0.6563)
2023/11/22 18:38:04 - INFO - root -   Epoch: [24/300][220/283], lr: 0.00000014 	 loss = 0.0578(0.6355)
2023/11/22 18:38:19 - INFO - root -   Epoch: [24/300][240/283], lr: 0.00000014 	 loss = 0.9803(0.6312)
2023/11/22 18:38:38 - INFO - root -   Epoch: [24/300][260/283], lr: 0.00000014 	 loss = 2.6945(0.6274)
2023/11/22 18:38:48 - INFO - root -   Epoch: [24/300][280/283], lr: 0.00000014 	 loss = 2.7917(0.6163)
2023/11/22 18:38:49 - INFO - root -   Epoch: [24/300] 	 loss = 0.6129
2023/11/22 18:41:40 - INFO - root -   precision = 0.7902
2023/11/22 18:41:40 - INFO - root -   eval_loss = 0.5550
2023/11/22 18:41:40 - INFO - root -   eval_acc = 0.7902
2023/11/22 18:41:41 - INFO - root -   train_accuracy = 0.7986
2023/11/22 18:41:50 - INFO - root -   Epoch: [25/300][0/283], lr: 0.00000014 	 loss = 0.0828(0.0828)
2023/11/22 18:42:05 - INFO - root -   Epoch: [25/300][20/283], lr: 0.00000014 	 loss = 1.5197(0.6985)
2023/11/22 18:42:18 - INFO - root -   Epoch: [25/300][40/283], lr: 0.00000014 	 loss = 0.1082(0.8526)
2023/11/22 18:42:37 - INFO - root -   Epoch: [25/300][60/283], lr: 0.00000014 	 loss = 0.0524(0.7926)
2023/11/22 18:42:50 - INFO - root -   Epoch: [25/300][80/283], lr: 0.00000014 	 loss = 0.2459(0.8170)
2023/11/22 18:43:09 - INFO - root -   Epoch: [25/300][100/283], lr: 0.00000014 	 loss = 0.4151(0.7668)
2023/11/22 18:43:20 - INFO - root -   Epoch: [25/300][120/283], lr: 0.00000014 	 loss = 0.4796(0.7032)
2023/11/22 18:43:33 - INFO - root -   Epoch: [25/300][140/283], lr: 0.00000014 	 loss = 0.1222(0.6430)
2023/11/22 18:43:55 - INFO - root -   Epoch: [25/300][160/283], lr: 0.00000014 	 loss = 1.1638(0.6401)
2023/11/22 18:44:09 - INFO - root -   Epoch: [25/300][180/283], lr: 0.00000014 	 loss = 0.1211(0.6382)
2023/11/22 18:44:37 - INFO - root -   Epoch: [25/300][200/283], lr: 0.00000014 	 loss = 0.0771(0.6224)
2023/11/22 18:44:46 - INFO - root -   Epoch: [25/300][220/283], lr: 0.00000014 	 loss = 0.0310(0.6064)
2023/11/22 18:45:10 - INFO - root -   Epoch: [25/300][240/283], lr: 0.00000014 	 loss = 0.1239(0.6024)
2023/11/22 18:45:23 - INFO - root -   Epoch: [25/300][260/283], lr: 0.00000014 	 loss = 1.9067(0.5888)
2023/11/22 18:45:40 - INFO - root -   Epoch: [25/300][280/283], lr: 0.00000014 	 loss = 1.9519(0.5835)
2023/11/22 18:45:41 - INFO - root -   Epoch: [25/300] 	 loss = 0.5805
2023/11/22 18:45:41 - INFO - root -   train_accuracy = 0.7880
2023/11/22 18:45:42 - INFO - root -   Epoch: [26/300][0/283], lr: 0.00000015 	 loss = 0.2302(0.2302)
2023/11/22 18:46:04 - INFO - root -   Epoch: [26/300][20/283], lr: 0.00000015 	 loss = 2.2186(0.9083)
2023/11/22 18:46:21 - INFO - root -   Epoch: [26/300][40/283], lr: 0.00000015 	 loss = 0.0977(0.8632)
2023/11/22 18:46:30 - INFO - root -   Epoch: [26/300][60/283], lr: 0.00000015 	 loss = 0.2400(0.7978)
2023/11/22 18:46:51 - INFO - root -   Epoch: [26/300][80/283], lr: 0.00000015 	 loss = 0.7148(0.8018)
2023/11/22 18:47:04 - INFO - root -   Epoch: [26/300][100/283], lr: 0.00000015 	 loss = 0.2640(0.7516)
2023/11/22 18:47:14 - INFO - root -   Epoch: [26/300][120/283], lr: 0.00000015 	 loss = 0.5836(0.6778)
2023/11/22 18:47:34 - INFO - root -   Epoch: [26/300][140/283], lr: 0.00000015 	 loss = 0.1155(0.6421)
2023/11/22 18:47:52 - INFO - root -   Epoch: [26/300][160/283], lr: 0.00000015 	 loss = 2.1243(0.6364)
2023/11/22 18:48:11 - INFO - root -   Epoch: [26/300][180/283], lr: 0.00000015 	 loss = 0.0423(0.6465)
2023/11/22 18:48:24 - INFO - root -   Epoch: [26/300][200/283], lr: 0.00000015 	 loss = 0.6234(0.6217)
2023/11/22 18:48:36 - INFO - root -   Epoch: [26/300][220/283], lr: 0.00000015 	 loss = 0.2729(0.6053)
2023/11/22 18:48:59 - INFO - root -   Epoch: [26/300][240/283], lr: 0.00000015 	 loss = 1.0259(0.6013)
2023/11/22 18:49:13 - INFO - root -   Epoch: [26/300][260/283], lr: 0.00000015 	 loss = 1.4530(0.5907)
2023/11/22 18:49:28 - INFO - root -   Epoch: [26/300][280/283], lr: 0.00000015 	 loss = 2.2248(0.5812)
2023/11/22 18:49:29 - INFO - root -   Epoch: [26/300] 	 loss = 0.5787
2023/11/22 18:49:29 - INFO - root -   train_accuracy = 0.8021
2023/11/22 18:49:38 - INFO - root -   Epoch: [27/300][0/283], lr: 0.00000015 	 loss = 0.2603(0.2603)
2023/11/22 18:49:54 - INFO - root -   Epoch: [27/300][20/283], lr: 0.00000015 	 loss = 2.6997(0.9008)
2023/11/22 18:50:02 - INFO - root -   Epoch: [27/300][40/283], lr: 0.00000015 	 loss = 0.7217(0.8408)
2023/11/22 18:50:19 - INFO - root -   Epoch: [27/300][60/283], lr: 0.00000015 	 loss = 0.0299(0.8755)
2023/11/22 18:50:32 - INFO - root -   Epoch: [27/300][80/283], lr: 0.00000015 	 loss = 0.2716(0.8746)
2023/11/22 18:50:47 - INFO - root -   Epoch: [27/300][100/283], lr: 0.00000015 	 loss = 0.7072(0.8059)
2023/11/22 18:51:03 - INFO - root -   Epoch: [27/300][120/283], lr: 0.00000015 	 loss = 0.0728(0.7361)
2023/11/22 18:51:26 - INFO - root -   Epoch: [27/300][140/283], lr: 0.00000015 	 loss = 0.0930(0.6890)
2023/11/22 18:51:45 - INFO - root -   Epoch: [27/300][160/283], lr: 0.00000015 	 loss = 1.6390(0.6960)
2023/11/22 18:52:01 - INFO - root -   Epoch: [27/300][180/283], lr: 0.00000015 	 loss = 0.4876(0.7033)
2023/11/22 18:52:24 - INFO - root -   Epoch: [27/300][200/283], lr: 0.00000015 	 loss = 0.5702(0.7021)
2023/11/22 18:52:35 - INFO - root -   Epoch: [27/300][220/283], lr: 0.00000015 	 loss = 0.1087(0.6839)
2023/11/22 18:52:51 - INFO - root -   Epoch: [27/300][240/283], lr: 0.00000015 	 loss = 1.4740(0.6761)
2023/11/22 18:53:04 - INFO - root -   Epoch: [27/300][260/283], lr: 0.00000015 	 loss = 0.8683(0.6514)
2023/11/22 18:53:16 - INFO - root -   Epoch: [27/300][280/283], lr: 0.00000015 	 loss = 1.9981(0.6387)
2023/11/22 18:53:17 - INFO - root -   Epoch: [27/300] 	 loss = 0.6348
2023/11/22 18:53:17 - INFO - root -   train_accuracy = 0.7845
2023/11/22 18:53:25 - INFO - root -   Epoch: [28/300][0/283], lr: 0.00000015 	 loss = 0.0807(0.0807)
2023/11/22 18:53:43 - INFO - root -   Epoch: [28/300][20/283], lr: 0.00000015 	 loss = 1.1178(0.6317)
2023/11/22 18:53:53 - INFO - root -   Epoch: [28/300][40/283], lr: 0.00000015 	 loss = 0.1745(0.7182)
2023/11/22 18:54:07 - INFO - root -   Epoch: [28/300][60/283], lr: 0.00000015 	 loss = 0.1250(0.6876)
2023/11/22 18:54:22 - INFO - root -   Epoch: [28/300][80/283], lr: 0.00000015 	 loss = 0.5878(0.7233)
2023/11/22 18:54:38 - INFO - root -   Epoch: [28/300][100/283], lr: 0.00000015 	 loss = 0.1873(0.6742)
2023/11/22 18:54:50 - INFO - root -   Epoch: [28/300][120/283], lr: 0.00000015 	 loss = 0.3067(0.6286)
2023/11/22 18:55:04 - INFO - root -   Epoch: [28/300][140/283], lr: 0.00000015 	 loss = 0.0674(0.5894)
2023/11/22 18:55:21 - INFO - root -   Epoch: [28/300][160/283], lr: 0.00000015 	 loss = 1.6711(0.5835)
2023/11/22 18:55:43 - INFO - root -   Epoch: [28/300][180/283], lr: 0.00000015 	 loss = 0.0782(0.6021)
2023/11/22 18:56:06 - INFO - root -   Epoch: [28/300][200/283], lr: 0.00000015 	 loss = 0.0920(0.5856)
2023/11/22 18:56:16 - INFO - root -   Epoch: [28/300][220/283], lr: 0.00000015 	 loss = 0.1254(0.5730)
2023/11/22 18:56:33 - INFO - root -   Epoch: [28/300][240/283], lr: 0.00000015 	 loss = 0.3225(0.5869)
2023/11/22 18:56:42 - INFO - root -   Epoch: [28/300][260/283], lr: 0.00000015 	 loss = 1.9508(0.5814)
2023/11/22 18:57:00 - INFO - root -   Epoch: [28/300][280/283], lr: 0.00000015 	 loss = 1.4279(0.5725)
2023/11/22 18:57:01 - INFO - root -   Epoch: [28/300] 	 loss = 0.5695
2023/11/22 18:57:01 - INFO - root -   train_accuracy = 0.7933
2023/11/22 18:57:02 - INFO - root -   Epoch: [29/300][0/283], lr: 0.00000015 	 loss = 0.1389(0.1389)
2023/11/22 18:57:23 - INFO - root -   Epoch: [29/300][20/283], lr: 0.00000015 	 loss = 1.5443(0.8581)
2023/11/22 18:57:36 - INFO - root -   Epoch: [29/300][40/283], lr: 0.00000015 	 loss = 0.2815(0.8269)
2023/11/22 18:57:46 - INFO - root -   Epoch: [29/300][60/283], lr: 0.00000015 	 loss = 0.5459(0.7801)
2023/11/22 18:57:54 - INFO - root -   Epoch: [29/300][80/283], lr: 0.00000015 	 loss = 0.1510(0.8008)
2023/11/22 18:58:03 - INFO - root -   Epoch: [29/300][100/283], lr: 0.00000015 	 loss = 0.4568(0.7834)
2023/11/22 18:58:15 - INFO - root -   Epoch: [29/300][120/283], lr: 0.00000015 	 loss = 0.4218(0.7242)
2023/11/22 18:58:35 - INFO - root -   Epoch: [29/300][140/283], lr: 0.00000015 	 loss = 0.2100(0.6733)
2023/11/22 18:58:48 - INFO - root -   Epoch: [29/300][160/283], lr: 0.00000015 	 loss = 1.2133(0.6662)
2023/11/22 18:59:03 - INFO - root -   Epoch: [29/300][180/283], lr: 0.00000015 	 loss = 0.3412(0.6568)
2023/11/22 18:59:21 - INFO - root -   Epoch: [29/300][200/283], lr: 0.00000015 	 loss = 0.9565(0.6518)
2023/11/22 18:59:47 - INFO - root -   Epoch: [29/300][220/283], lr: 0.00000015 	 loss = 0.1680(0.6303)
2023/11/22 18:59:58 - INFO - root -   Epoch: [29/300][240/283], lr: 0.00000015 	 loss = 0.9349(0.6405)
2023/11/22 19:00:10 - INFO - root -   Epoch: [29/300][260/283], lr: 0.00000015 	 loss = 1.3999(0.6276)
2023/11/22 19:00:19 - INFO - root -   Epoch: [29/300][280/283], lr: 0.00000015 	 loss = 1.0669(0.6031)
2023/11/22 19:00:20 - INFO - root -   Epoch: [29/300] 	 loss = 0.6000
2023/11/22 19:03:12 - INFO - root -   precision = 0.7832
2023/11/22 19:03:12 - INFO - root -   eval_loss = 0.5128
2023/11/22 19:03:12 - INFO - root -   eval_acc = 0.7832
2023/11/22 19:03:13 - INFO - root -   train_accuracy = 0.7880
2023/11/22 19:03:14 - INFO - root -   Epoch: [30/300][0/283], lr: 0.00000015 	 loss = 0.2608(0.2608)
2023/11/22 19:03:35 - INFO - root -   Epoch: [30/300][20/283], lr: 0.00000015 	 loss = 1.5809(0.8764)
2023/11/22 19:03:55 - INFO - root -   Epoch: [30/300][40/283], lr: 0.00000015 	 loss = 0.1850(0.8449)
2023/11/22 19:04:06 - INFO - root -   Epoch: [30/300][60/283], lr: 0.00000015 	 loss = 0.1613(0.8199)
2023/11/22 19:04:36 - INFO - root -   Epoch: [30/300][80/283], lr: 0.00000015 	 loss = 0.0565(0.8310)
2023/11/22 19:04:44 - INFO - root -   Epoch: [30/300][100/283], lr: 0.00000015 	 loss = 0.4082(0.7730)
2023/11/22 19:05:00 - INFO - root -   Epoch: [30/300][120/283], lr: 0.00000015 	 loss = 0.2349(0.7165)
2023/11/22 19:05:18 - INFO - root -   Epoch: [30/300][140/283], lr: 0.00000015 	 loss = 0.2515(0.6600)
2023/11/22 19:05:36 - INFO - root -   Epoch: [30/300][160/283], lr: 0.00000015 	 loss = 1.8774(0.6462)
2023/11/22 19:05:45 - INFO - root -   Epoch: [30/300][180/283], lr: 0.00000015 	 loss = 0.1336(0.6460)
2023/11/22 19:06:03 - INFO - root -   Epoch: [30/300][200/283], lr: 0.00000015 	 loss = 0.2279(0.6248)
2023/11/22 19:06:23 - INFO - root -   Epoch: [30/300][220/283], lr: 0.00000015 	 loss = 0.5273(0.6123)
2023/11/22 19:06:42 - INFO - root -   Epoch: [30/300][240/283], lr: 0.00000015 	 loss = 1.3011(0.6202)
2023/11/22 19:06:52 - INFO - root -   Epoch: [30/300][260/283], lr: 0.00000015 	 loss = 1.2419(0.6123)
2023/11/22 19:07:06 - INFO - root -   Epoch: [30/300][280/283], lr: 0.00000015 	 loss = 2.2606(0.6010)
2023/11/22 19:07:06 - INFO - root -   Epoch: [30/300] 	 loss = 0.5983
2023/11/22 19:07:06 - INFO - root -   train_accuracy = 0.7862
2023/11/22 19:07:07 - INFO - root -   Epoch: [31/300][0/283], lr: 0.00000015 	 loss = 0.2136(0.2136)
2023/11/22 19:07:23 - INFO - root -   Epoch: [31/300][20/283], lr: 0.00000015 	 loss = 2.1033(0.7994)
2023/11/22 19:07:37 - INFO - root -   Epoch: [31/300][40/283], lr: 0.00000015 	 loss = 0.3020(0.8810)
2023/11/22 19:07:48 - INFO - root -   Epoch: [31/300][60/283], lr: 0.00000015 	 loss = 0.0888(0.8399)
2023/11/22 19:08:14 - INFO - root -   Epoch: [31/300][80/283], lr: 0.00000015 	 loss = 0.2615(0.8542)
2023/11/22 19:08:22 - INFO - root -   Epoch: [31/300][100/283], lr: 0.00000015 	 loss = 0.4308(0.8067)
2023/11/22 19:08:40 - INFO - root -   Epoch: [31/300][120/283], lr: 0.00000015 	 loss = 0.2933(0.7507)
2023/11/22 19:08:58 - INFO - root -   Epoch: [31/300][140/283], lr: 0.00000015 	 loss = 0.1025(0.6959)
2023/11/22 19:09:20 - INFO - root -   Epoch: [31/300][160/283], lr: 0.00000015 	 loss = 1.6324(0.6778)
2023/11/22 19:09:39 - INFO - root -   Epoch: [31/300][180/283], lr: 0.00000015 	 loss = 1.1522(0.6730)
2023/11/22 19:09:57 - INFO - root -   Epoch: [31/300][200/283], lr: 0.00000015 	 loss = 0.2523(0.6588)
2023/11/22 19:10:13 - INFO - root -   Epoch: [31/300][220/283], lr: 0.00000015 	 loss = 0.1800(0.6464)
2023/11/22 19:10:26 - INFO - root -   Epoch: [31/300][240/283], lr: 0.00000015 	 loss = 0.0415(0.6469)
2023/11/22 19:10:46 - INFO - root -   Epoch: [31/300][260/283], lr: 0.00000015 	 loss = 1.1306(0.6368)
2023/11/22 19:10:54 - INFO - root -   Epoch: [31/300][280/283], lr: 0.00000015 	 loss = 1.5170(0.6247)
2023/11/22 19:10:54 - INFO - root -   Epoch: [31/300] 	 loss = 0.6212
2023/11/22 19:10:54 - INFO - root -   train_accuracy = 0.7774
2023/11/22 19:11:08 - INFO - root -   Epoch: [32/300][0/283], lr: 0.00000016 	 loss = 0.0991(0.0991)
2023/11/22 19:11:29 - INFO - root -   Epoch: [32/300][20/283], lr: 0.00000016 	 loss = 1.6555(0.9872)
2023/11/22 19:11:45 - INFO - root -   Epoch: [32/300][40/283], lr: 0.00000016 	 loss = 0.2305(0.9355)
2023/11/22 19:11:58 - INFO - root -   Epoch: [32/300][60/283], lr: 0.00000016 	 loss = 0.1105(0.8890)
2023/11/22 19:12:12 - INFO - root -   Epoch: [32/300][80/283], lr: 0.00000016 	 loss = 0.3075(0.8748)
2023/11/22 19:12:30 - INFO - root -   Epoch: [32/300][100/283], lr: 0.00000016 	 loss = 0.3875(0.8144)
2023/11/22 19:12:44 - INFO - root -   Epoch: [32/300][120/283], lr: 0.00000016 	 loss = 1.0419(0.7473)
2023/11/22 19:12:56 - INFO - root -   Epoch: [32/300][140/283], lr: 0.00000016 	 loss = 0.1603(0.6807)
2023/11/22 19:13:11 - INFO - root -   Epoch: [32/300][160/283], lr: 0.00000016 	 loss = 1.0844(0.6627)
2023/11/22 19:13:24 - INFO - root -   Epoch: [32/300][180/283], lr: 0.00000016 	 loss = 0.1037(0.6536)
2023/11/22 19:13:42 - INFO - root -   Epoch: [32/300][200/283], lr: 0.00000016 	 loss = 0.7817(0.6442)
2023/11/22 19:14:01 - INFO - root -   Epoch: [32/300][220/283], lr: 0.00000016 	 loss = 0.0637(0.6253)
2023/11/22 19:14:21 - INFO - root -   Epoch: [32/300][240/283], lr: 0.00000016 	 loss = 0.8493(0.6266)
2023/11/22 19:14:36 - INFO - root -   Epoch: [32/300][260/283], lr: 0.00000016 	 loss = 0.7912(0.6079)
2023/11/22 19:14:51 - INFO - root -   Epoch: [32/300][280/283], lr: 0.00000016 	 loss = 2.4692(0.6014)
2023/11/22 19:14:52 - INFO - root -   Epoch: [32/300] 	 loss = 0.5979
2023/11/22 19:14:52 - INFO - root -   train_accuracy = 0.7774
2023/11/22 19:14:53 - INFO - root -   Epoch: [33/300][0/283], lr: 0.00000016 	 loss = 0.0977(0.0977)
2023/11/22 19:15:19 - INFO - root -   Epoch: [33/300][20/283], lr: 0.00000016 	 loss = 2.8966(1.0522)
2023/11/22 19:15:44 - INFO - root -   Epoch: [33/300][40/283], lr: 0.00000016 	 loss = 0.1831(0.9206)
2023/11/22 19:15:54 - INFO - root -   Epoch: [33/300][60/283], lr: 0.00000016 	 loss = 0.0982(0.8689)
2023/11/22 19:16:08 - INFO - root -   Epoch: [33/300][80/283], lr: 0.00000016 	 loss = 0.0663(0.8351)
2023/11/22 19:16:24 - INFO - root -   Epoch: [33/300][100/283], lr: 0.00000016 	 loss = 0.2261(0.7688)
2023/11/22 19:16:46 - INFO - root -   Epoch: [33/300][120/283], lr: 0.00000016 	 loss = 0.0981(0.7061)
2023/11/22 19:16:58 - INFO - root -   Epoch: [33/300][140/283], lr: 0.00000016 	 loss = 0.2220(0.6664)
2023/11/22 19:17:16 - INFO - root -   Epoch: [33/300][160/283], lr: 0.00000016 	 loss = 1.6511(0.6497)
2023/11/22 19:17:40 - INFO - root -   Epoch: [33/300][180/283], lr: 0.00000016 	 loss = 0.1167(0.6673)
2023/11/22 19:17:54 - INFO - root -   Epoch: [33/300][200/283], lr: 0.00000016 	 loss = 0.7527(0.6467)
2023/11/22 19:18:14 - INFO - root -   Epoch: [33/300][220/283], lr: 0.00000016 	 loss = 0.1193(0.6355)
2023/11/22 19:18:29 - INFO - root -   Epoch: [33/300][240/283], lr: 0.00000016 	 loss = 0.1528(0.6289)
2023/11/22 19:18:50 - INFO - root -   Epoch: [33/300][260/283], lr: 0.00000016 	 loss = 1.7793(0.6158)
2023/11/22 19:19:03 - INFO - root -   Epoch: [33/300][280/283], lr: 0.00000016 	 loss = 1.6709(0.6042)
2023/11/22 19:19:04 - INFO - root -   Epoch: [33/300] 	 loss = 0.6008
2023/11/22 19:19:04 - INFO - root -   train_accuracy = 0.7809
2023/11/22 19:19:06 - INFO - root -   Epoch: [34/300][0/283], lr: 0.00000016 	 loss = 0.4857(0.4857)
2023/11/22 19:19:30 - INFO - root -   Epoch: [34/300][20/283], lr: 0.00000016 	 loss = 1.7623(0.8520)
2023/11/22 19:19:43 - INFO - root -   Epoch: [34/300][40/283], lr: 0.00000016 	 loss = 0.4320(0.9318)
2023/11/22 19:20:00 - INFO - root -   Epoch: [34/300][60/283], lr: 0.00000016 	 loss = 0.2312(0.8395)
2023/11/22 19:20:15 - INFO - root -   Epoch: [34/300][80/283], lr: 0.00000016 	 loss = 0.2517(0.8324)
2023/11/22 19:20:23 - INFO - root -   Epoch: [34/300][100/283], lr: 0.00000016 	 loss = 0.7546(0.7777)
2023/11/22 19:20:46 - INFO - root -   Epoch: [34/300][120/283], lr: 0.00000016 	 loss = 0.6807(0.7192)
2023/11/22 19:20:58 - INFO - root -   Epoch: [34/300][140/283], lr: 0.00000016 	 loss = 0.0416(0.6788)
2023/11/22 19:21:19 - INFO - root -   Epoch: [34/300][160/283], lr: 0.00000016 	 loss = 1.1384(0.6640)
2023/11/22 19:21:40 - INFO - root -   Epoch: [34/300][180/283], lr: 0.00000016 	 loss = 0.2527(0.6744)
2023/11/22 19:21:49 - INFO - root -   Epoch: [34/300][200/283], lr: 0.00000016 	 loss = 0.3147(0.6543)
2023/11/22 19:22:06 - INFO - root -   Epoch: [34/300][220/283], lr: 0.00000016 	 loss = 0.0611(0.6334)
2023/11/22 19:22:19 - INFO - root -   Epoch: [34/300][240/283], lr: 0.00000016 	 loss = 0.1744(0.6206)
2023/11/22 19:22:29 - INFO - root -   Epoch: [34/300][260/283], lr: 0.00000016 	 loss = 1.4069(0.6074)
2023/11/22 19:22:43 - INFO - root -   Epoch: [34/300][280/283], lr: 0.00000016 	 loss = 3.0467(0.5898)
2023/11/22 19:22:43 - INFO - root -   Epoch: [34/300] 	 loss = 0.5873
2023/11/22 19:25:36 - INFO - root -   precision = 0.8042
2023/11/22 19:25:36 - INFO - root -   eval_loss = 0.5119
2023/11/22 19:25:36 - INFO - root -   eval_acc = 0.8042
2023/11/22 19:25:37 - INFO - root -   train_accuracy = 0.7792
2023/11/22 19:25:45 - INFO - root -   Epoch: [35/300][0/283], lr: 0.00000016 	 loss = 0.0542(0.0542)
2023/11/22 19:25:59 - INFO - root -   Epoch: [35/300][20/283], lr: 0.00000016 	 loss = 2.4002(0.8926)
2023/11/22 19:26:07 - INFO - root -   Epoch: [35/300][40/283], lr: 0.00000016 	 loss = 0.7968(0.8911)
2023/11/22 19:26:17 - INFO - root -   Epoch: [35/300][60/283], lr: 0.00000016 	 loss = 0.2340(0.7696)
2023/11/22 19:26:30 - INFO - root -   Epoch: [35/300][80/283], lr: 0.00000016 	 loss = 0.1768(0.7775)
2023/11/22 19:26:42 - INFO - root -   Epoch: [35/300][100/283], lr: 0.00000016 	 loss = 0.2478(0.7101)
2023/11/22 19:26:53 - INFO - root -   Epoch: [35/300][120/283], lr: 0.00000016 	 loss = 0.5718(0.6501)
2023/11/22 19:27:09 - INFO - root -   Epoch: [35/300][140/283], lr: 0.00000016 	 loss = 0.0458(0.5969)
2023/11/22 19:27:22 - INFO - root -   Epoch: [35/300][160/283], lr: 0.00000016 	 loss = 2.0143(0.5924)
2023/11/22 19:27:36 - INFO - root -   Epoch: [35/300][180/283], lr: 0.00000016 	 loss = 0.0351(0.5904)
2023/11/22 19:27:44 - INFO - root -   Epoch: [35/300][200/283], lr: 0.00000016 	 loss = 0.1557(0.5821)
2023/11/22 19:28:04 - INFO - root -   Epoch: [35/300][220/283], lr: 0.00000016 	 loss = 0.1717(0.5763)
2023/11/22 19:28:17 - INFO - root -   Epoch: [35/300][240/283], lr: 0.00000016 	 loss = 1.2705(0.5721)
2023/11/22 19:28:39 - INFO - root -   Epoch: [35/300][260/283], lr: 0.00000016 	 loss = 1.3614(0.5693)
2023/11/22 19:28:50 - INFO - root -   Epoch: [35/300][280/283], lr: 0.00000016 	 loss = 3.1922(0.5621)
2023/11/22 19:28:51 - INFO - root -   Epoch: [35/300] 	 loss = 0.5594
2023/11/22 19:28:51 - INFO - root -   train_accuracy = 0.8057
2023/11/22 19:28:53 - INFO - root -   Epoch: [36/300][0/283], lr: 0.00000016 	 loss = 0.0724(0.0724)
2023/11/22 19:29:16 - INFO - root -   Epoch: [36/300][20/283], lr: 0.00000016 	 loss = 2.0336(0.6600)
2023/11/22 19:29:24 - INFO - root -   Epoch: [36/300][40/283], lr: 0.00000016 	 loss = 0.4659(0.7999)
2023/11/22 19:29:37 - INFO - root -   Epoch: [36/300][60/283], lr: 0.00000016 	 loss = 0.1466(0.8285)
2023/11/22 19:29:56 - INFO - root -   Epoch: [36/300][80/283], lr: 0.00000016 	 loss = 0.2922(0.8268)
2023/11/22 19:30:24 - INFO - root -   Epoch: [36/300][100/283], lr: 0.00000016 	 loss = 0.5779(0.7581)
2023/11/22 19:30:41 - INFO - root -   Epoch: [36/300][120/283], lr: 0.00000016 	 loss = 0.2687(0.7071)
2023/11/22 19:30:58 - INFO - root -   Epoch: [36/300][140/283], lr: 0.00000016 	 loss = 0.1504(0.6630)
2023/11/22 19:31:10 - INFO - root -   Epoch: [36/300][160/283], lr: 0.00000016 	 loss = 1.9424(0.6588)
2023/11/22 19:31:19 - INFO - root -   Epoch: [36/300][180/283], lr: 0.00000016 	 loss = 0.0794(0.6539)
2023/11/22 19:31:32 - INFO - root -   Epoch: [36/300][200/283], lr: 0.00000016 	 loss = 0.1707(0.6343)
2023/11/22 19:31:44 - INFO - root -   Epoch: [36/300][220/283], lr: 0.00000016 	 loss = 0.1586(0.6178)
2023/11/22 19:31:52 - INFO - root -   Epoch: [36/300][240/283], lr: 0.00000016 	 loss = 0.9583(0.6128)
2023/11/22 19:32:02 - INFO - root -   Epoch: [36/300][260/283], lr: 0.00000016 	 loss = 1.9139(0.6130)
2023/11/22 19:32:16 - INFO - root -   Epoch: [36/300][280/283], lr: 0.00000016 	 loss = 2.6148(0.6001)
2023/11/22 19:32:17 - INFO - root -   Epoch: [36/300] 	 loss = 0.5968
2023/11/22 19:32:17 - INFO - root -   train_accuracy = 0.7862
2023/11/22 19:32:18 - INFO - root -   Epoch: [37/300][0/283], lr: 0.00000016 	 loss = 0.0785(0.0785)
2023/11/22 19:32:39 - INFO - root -   Epoch: [37/300][20/283], lr: 0.00000016 	 loss = 1.3461(0.8936)
2023/11/22 19:32:55 - INFO - root -   Epoch: [37/300][40/283], lr: 0.00000016 	 loss = 0.1676(0.8522)
2023/11/22 19:33:08 - INFO - root -   Epoch: [37/300][60/283], lr: 0.00000016 	 loss = 0.3713(0.8580)
2023/11/22 19:33:24 - INFO - root -   Epoch: [37/300][80/283], lr: 0.00000016 	 loss = 0.1720(0.8072)
2023/11/22 19:33:40 - INFO - root -   Epoch: [37/300][100/283], lr: 0.00000016 	 loss = 0.6364(0.7426)
2023/11/22 19:33:48 - INFO - root -   Epoch: [37/300][120/283], lr: 0.00000016 	 loss = 0.3983(0.6765)
2023/11/22 19:33:57 - INFO - root -   Epoch: [37/300][140/283], lr: 0.00000016 	 loss = 0.0836(0.6153)
2023/11/22 19:34:13 - INFO - root -   Epoch: [37/300][160/283], lr: 0.00000016 	 loss = 1.1366(0.5983)
2023/11/22 19:34:29 - INFO - root -   Epoch: [37/300][180/283], lr: 0.00000016 	 loss = 0.1220(0.5981)
2023/11/22 19:34:41 - INFO - root -   Epoch: [37/300][200/283], lr: 0.00000016 	 loss = 0.4921(0.5891)
2023/11/22 19:34:55 - INFO - root -   Epoch: [37/300][220/283], lr: 0.00000016 	 loss = 0.1278(0.5813)
2023/11/22 19:35:14 - INFO - root -   Epoch: [37/300][240/283], lr: 0.00000016 	 loss = 0.1058(0.5831)
2023/11/22 19:35:30 - INFO - root -   Epoch: [37/300][260/283], lr: 0.00000016 	 loss = 2.0804(0.5790)
2023/11/22 19:35:46 - INFO - root -   Epoch: [37/300][280/283], lr: 0.00000016 	 loss = 2.6668(0.5689)
2023/11/22 19:35:47 - INFO - root -   Epoch: [37/300] 	 loss = 0.5656
2023/11/22 19:35:47 - INFO - root -   train_accuracy = 0.8004
2023/11/22 19:35:48 - INFO - root -   Epoch: [38/300][0/283], lr: 0.00000017 	 loss = 0.1211(0.1211)
2023/11/22 19:36:11 - INFO - root -   Epoch: [38/300][20/283], lr: 0.00000017 	 loss = 1.4263(0.7307)
2023/11/22 19:36:28 - INFO - root -   Epoch: [38/300][40/283], lr: 0.00000017 	 loss = 0.3636(0.8469)
2023/11/22 19:36:41 - INFO - root -   Epoch: [38/300][60/283], lr: 0.00000017 	 loss = 0.1866(0.8407)
2023/11/22 19:36:59 - INFO - root -   Epoch: [38/300][80/283], lr: 0.00000017 	 loss = 0.1574(0.8300)
2023/11/22 19:37:10 - INFO - root -   Epoch: [38/300][100/283], lr: 0.00000017 	 loss = 0.4086(0.7690)
2023/11/22 19:37:24 - INFO - root -   Epoch: [38/300][120/283], lr: 0.00000017 	 loss = 0.4982(0.7198)
2023/11/22 19:37:34 - INFO - root -   Epoch: [38/300][140/283], lr: 0.00000017 	 loss = 0.0305(0.6637)
2023/11/22 19:37:47 - INFO - root -   Epoch: [38/300][160/283], lr: 0.00000017 	 loss = 1.1516(0.6524)
2023/11/22 19:38:01 - INFO - root -   Epoch: [38/300][180/283], lr: 0.00000017 	 loss = 0.3842(0.6529)
2023/11/22 19:38:14 - INFO - root -   Epoch: [38/300][200/283], lr: 0.00000017 	 loss = 0.0955(0.6314)
2023/11/22 19:38:30 - INFO - root -   Epoch: [38/300][220/283], lr: 0.00000017 	 loss = 0.0547(0.6138)
2023/11/22 19:38:48 - INFO - root -   Epoch: [38/300][240/283], lr: 0.00000017 	 loss = 1.1230(0.6195)
2023/11/22 19:39:04 - INFO - root -   Epoch: [38/300][260/283], lr: 0.00000017 	 loss = 1.8790(0.6089)
2023/11/22 19:39:17 - INFO - root -   Epoch: [38/300][280/283], lr: 0.00000017 	 loss = 2.8699(0.6023)
2023/11/22 19:39:17 - INFO - root -   Epoch: [38/300] 	 loss = 0.5990
2023/11/22 19:39:17 - INFO - root -   train_accuracy = 0.7845
2023/11/22 19:39:25 - INFO - root -   Epoch: [39/300][0/283], lr: 0.00000017 	 loss = 0.1982(0.1982)
2023/11/22 19:39:36 - INFO - root -   Epoch: [39/300][20/283], lr: 0.00000017 	 loss = 1.1358(0.8075)
2023/11/22 19:39:53 - INFO - root -   Epoch: [39/300][40/283], lr: 0.00000017 	 loss = 0.2332(0.8500)
2023/11/22 19:40:08 - INFO - root -   Epoch: [39/300][60/283], lr: 0.00000017 	 loss = 0.2657(0.8437)
2023/11/22 19:40:16 - INFO - root -   Epoch: [39/300][80/283], lr: 0.00000017 	 loss = 0.3163(0.8443)
2023/11/22 19:40:29 - INFO - root -   Epoch: [39/300][100/283], lr: 0.00000017 	 loss = 0.3918(0.8128)
2023/11/22 19:40:45 - INFO - root -   Epoch: [39/300][120/283], lr: 0.00000017 	 loss = 0.1427(0.7679)
2023/11/22 19:40:56 - INFO - root -   Epoch: [39/300][140/283], lr: 0.00000017 	 loss = 0.1309(0.7036)
2023/11/22 19:41:12 - INFO - root -   Epoch: [39/300][160/283], lr: 0.00000017 	 loss = 1.3033(0.6734)
2023/11/22 19:41:28 - INFO - root -   Epoch: [39/300][180/283], lr: 0.00000017 	 loss = 0.0727(0.6613)
2023/11/22 19:41:36 - INFO - root -   Epoch: [39/300][200/283], lr: 0.00000017 	 loss = 0.2790(0.6481)
2023/11/22 19:41:48 - INFO - root -   Epoch: [39/300][220/283], lr: 0.00000017 	 loss = 0.0706(0.6309)
2023/11/22 19:42:03 - INFO - root -   Epoch: [39/300][240/283], lr: 0.00000017 	 loss = 0.1411(0.6252)
2023/11/22 19:42:18 - INFO - root -   Epoch: [39/300][260/283], lr: 0.00000017 	 loss = 2.1176(0.6112)
2023/11/22 19:42:33 - INFO - root -   Epoch: [39/300][280/283], lr: 0.00000017 	 loss = 3.2144(0.5982)
2023/11/22 19:42:34 - INFO - root -   Epoch: [39/300] 	 loss = 0.5956
2023/11/22 19:45:28 - INFO - root -   precision = 0.7902
2023/11/22 19:45:28 - INFO - root -   eval_loss = 0.5234
2023/11/22 19:45:28 - INFO - root -   eval_acc = 0.7902
2023/11/22 19:45:29 - INFO - root -   train_accuracy = 0.7721
2023/11/22 19:45:37 - INFO - root -   Epoch: [40/300][0/283], lr: 0.00000017 	 loss = 0.4439(0.4439)
2023/11/22 19:45:51 - INFO - root -   Epoch: [40/300][20/283], lr: 0.00000017 	 loss = 2.2638(0.6898)
2023/11/22 19:46:02 - INFO - root -   Epoch: [40/300][40/283], lr: 0.00000017 	 loss = 0.1660(0.7260)
2023/11/22 19:46:18 - INFO - root -   Epoch: [40/300][60/283], lr: 0.00000017 	 loss = 0.1498(0.7221)
2023/11/22 19:46:38 - INFO - root -   Epoch: [40/300][80/283], lr: 0.00000017 	 loss = 0.2013(0.7011)
2023/11/22 19:46:50 - INFO - root -   Epoch: [40/300][100/283], lr: 0.00000017 	 loss = 0.1665(0.6761)
2023/11/22 19:47:09 - INFO - root -   Epoch: [40/300][120/283], lr: 0.00000017 	 loss = 0.2040(0.6452)
2023/11/22 19:47:27 - INFO - root -   Epoch: [40/300][140/283], lr: 0.00000017 	 loss = 0.0171(0.6004)
2023/11/22 19:47:40 - INFO - root -   Epoch: [40/300][160/283], lr: 0.00000017 	 loss = 1.3615(0.5992)
2023/11/22 19:47:57 - INFO - root -   Epoch: [40/300][180/283], lr: 0.00000017 	 loss = 0.2046(0.5940)
2023/11/22 19:48:13 - INFO - root -   Epoch: [40/300][200/283], lr: 0.00000017 	 loss = 1.3473(0.5825)
2023/11/22 19:48:27 - INFO - root -   Epoch: [40/300][220/283], lr: 0.00000017 	 loss = 0.1802(0.5687)
2023/11/22 19:48:43 - INFO - root -   Epoch: [40/300][240/283], lr: 0.00000017 	 loss = 1.1260(0.5658)
2023/11/22 19:49:03 - INFO - root -   Epoch: [40/300][260/283], lr: 0.00000017 	 loss = 1.5754(0.5622)
2023/11/22 19:49:17 - INFO - root -   Epoch: [40/300][280/283], lr: 0.00000017 	 loss = 1.2927(0.5478)
2023/11/22 19:49:18 - INFO - root -   Epoch: [40/300] 	 loss = 0.5451
2023/11/22 19:49:18 - INFO - root -   train_accuracy = 0.7880
2023/11/22 19:49:20 - INFO - root -   Epoch: [41/300][0/283], lr: 0.00000017 	 loss = 0.0653(0.0653)
2023/11/22 19:49:39 - INFO - root -   Epoch: [41/300][20/283], lr: 0.00000017 	 loss = 1.0848(0.8106)
2023/11/22 19:49:50 - INFO - root -   Epoch: [41/300][40/283], lr: 0.00000017 	 loss = 0.1621(0.8242)
2023/11/22 19:50:06 - INFO - root -   Epoch: [41/300][60/283], lr: 0.00000017 	 loss = 0.1689(0.7583)
2023/11/22 19:50:21 - INFO - root -   Epoch: [41/300][80/283], lr: 0.00000017 	 loss = 0.0671(0.7344)
2023/11/22 19:50:33 - INFO - root -   Epoch: [41/300][100/283], lr: 0.00000017 	 loss = 0.1190(0.6866)
2023/11/22 19:50:52 - INFO - root -   Epoch: [41/300][120/283], lr: 0.00000017 	 loss = 0.1448(0.6396)
2023/11/22 19:51:09 - INFO - root -   Epoch: [41/300][140/283], lr: 0.00000017 	 loss = 0.1285(0.6038)
2023/11/22 19:51:27 - INFO - root -   Epoch: [41/300][160/283], lr: 0.00000017 	 loss = 1.7772(0.6024)
2023/11/22 19:51:42 - INFO - root -   Epoch: [41/300][180/283], lr: 0.00000017 	 loss = 0.1336(0.6135)
2023/11/22 19:51:59 - INFO - root -   Epoch: [41/300][200/283], lr: 0.00000017 	 loss = 0.3293(0.6044)
2023/11/22 19:52:15 - INFO - root -   Epoch: [41/300][220/283], lr: 0.00000017 	 loss = 0.3006(0.5840)
2023/11/22 19:52:25 - INFO - root -   Epoch: [41/300][240/283], lr: 0.00000017 	 loss = 0.9006(0.5979)
2023/11/22 19:52:43 - INFO - root -   Epoch: [41/300][260/283], lr: 0.00000017 	 loss = 0.6820(0.5788)
2023/11/22 19:52:56 - INFO - root -   Epoch: [41/300][280/283], lr: 0.00000017 	 loss = 2.5438(0.5765)
2023/11/22 19:52:57 - INFO - root -   Epoch: [41/300] 	 loss = 0.5742
2023/11/22 19:52:57 - INFO - root -   train_accuracy = 0.7827
2023/11/22 19:53:03 - INFO - root -   Epoch: [42/300][0/283], lr: 0.00000017 	 loss = 0.0673(0.0673)
2023/11/22 19:53:18 - INFO - root -   Epoch: [42/300][20/283], lr: 0.00000017 	 loss = 1.5208(0.7794)
2023/11/22 19:53:40 - INFO - root -   Epoch: [42/300][40/283], lr: 0.00000017 	 loss = 0.3035(0.7965)
2023/11/22 19:53:48 - INFO - root -   Epoch: [42/300][60/283], lr: 0.00000017 	 loss = 0.1399(0.7901)
2023/11/22 19:54:02 - INFO - root -   Epoch: [42/300][80/283], lr: 0.00000017 	 loss = 0.0532(0.7797)
2023/11/22 19:54:11 - INFO - root -   Epoch: [42/300][100/283], lr: 0.00000017 	 loss = 0.5329(0.7555)
2023/11/22 19:54:33 - INFO - root -   Epoch: [42/300][120/283], lr: 0.00000017 	 loss = 0.1554(0.6917)
2023/11/22 19:54:49 - INFO - root -   Epoch: [42/300][140/283], lr: 0.00000017 	 loss = 0.0939(0.6358)
2023/11/22 19:55:02 - INFO - root -   Epoch: [42/300][160/283], lr: 0.00000017 	 loss = 1.0092(0.6201)
2023/11/22 19:55:16 - INFO - root -   Epoch: [42/300][180/283], lr: 0.00000017 	 loss = 0.8275(0.6225)
2023/11/22 19:55:30 - INFO - root -   Epoch: [42/300][200/283], lr: 0.00000017 	 loss = 0.4070(0.6078)
2023/11/22 19:55:46 - INFO - root -   Epoch: [42/300][220/283], lr: 0.00000017 	 loss = 0.2126(0.5744)
2023/11/22 19:55:59 - INFO - root -   Epoch: [42/300][240/283], lr: 0.00000017 	 loss = 0.8133(0.5923)
2023/11/22 19:56:13 - INFO - root -   Epoch: [42/300][260/283], lr: 0.00000017 	 loss = 1.6076(0.5771)
2023/11/22 19:56:22 - INFO - root -   Epoch: [42/300][280/283], lr: 0.00000017 	 loss = 2.6713(0.5701)
2023/11/22 19:56:23 - INFO - root -   Epoch: [42/300] 	 loss = 0.5665
2023/11/22 19:56:23 - INFO - root -   train_accuracy = 0.7809
2023/11/22 19:56:24 - INFO - root -   Epoch: [43/300][0/283], lr: 0.00000018 	 loss = 0.0943(0.0943)
2023/11/22 19:56:47 - INFO - root -   Epoch: [43/300][20/283], lr: 0.00000018 	 loss = 1.5888(0.7397)
2023/11/22 19:57:03 - INFO - root -   Epoch: [43/300][40/283], lr: 0.00000018 	 loss = 1.3321(0.8332)
2023/11/22 19:57:12 - INFO - root -   Epoch: [43/300][60/283], lr: 0.00000018 	 loss = 0.0631(0.7749)
2023/11/22 19:57:28 - INFO - root -   Epoch: [43/300][80/283], lr: 0.00000018 	 loss = 0.2221(0.8045)
2023/11/22 19:57:48 - INFO - root -   Epoch: [43/300][100/283], lr: 0.00000018 	 loss = 0.6851(0.7554)
2023/11/22 19:57:58 - INFO - root -   Epoch: [43/300][120/283], lr: 0.00000018 	 loss = 0.3604(0.7020)
2023/11/22 19:58:21 - INFO - root -   Epoch: [43/300][140/283], lr: 0.00000018 	 loss = 0.0566(0.6599)
2023/11/22 19:58:38 - INFO - root -   Epoch: [43/300][160/283], lr: 0.00000018 	 loss = 2.1867(0.6479)
2023/11/22 19:58:53 - INFO - root -   Epoch: [43/300][180/283], lr: 0.00000018 	 loss = 0.1810(0.6585)
2023/11/22 19:59:08 - INFO - root -   Epoch: [43/300][200/283], lr: 0.00000018 	 loss = 0.1897(0.6366)
2023/11/22 19:59:19 - INFO - root -   Epoch: [43/300][220/283], lr: 0.00000018 	 loss = 0.1076(0.6115)
2023/11/22 19:59:29 - INFO - root -   Epoch: [43/300][240/283], lr: 0.00000018 	 loss = 1.5213(0.6097)
2023/11/22 19:59:52 - INFO - root -   Epoch: [43/300][260/283], lr: 0.00000018 	 loss = 1.7239(0.6009)
2023/11/22 20:00:03 - INFO - root -   Epoch: [43/300][280/283], lr: 0.00000018 	 loss = 1.4782(0.5832)
2023/11/22 20:00:04 - INFO - root -   Epoch: [43/300] 	 loss = 0.5806
2023/11/22 20:00:04 - INFO - root -   train_accuracy = 0.7986
2023/11/22 20:00:15 - INFO - root -   Epoch: [44/300][0/283], lr: 0.00000018 	 loss = 0.2821(0.2821)
2023/11/22 20:00:31 - INFO - root -   Epoch: [44/300][20/283], lr: 0.00000018 	 loss = 2.0834(0.8347)
2023/11/22 20:00:43 - INFO - root -   Epoch: [44/300][40/283], lr: 0.00000018 	 loss = 1.2360(0.8180)
2023/11/22 20:00:59 - INFO - root -   Epoch: [44/300][60/283], lr: 0.00000018 	 loss = 0.1963(0.7583)
2023/11/22 20:01:13 - INFO - root -   Epoch: [44/300][80/283], lr: 0.00000018 	 loss = 0.1081(0.7971)
2023/11/22 20:01:31 - INFO - root -   Epoch: [44/300][100/283], lr: 0.00000018 	 loss = 0.2460(0.7394)
2023/11/22 20:01:47 - INFO - root -   Epoch: [44/300][120/283], lr: 0.00000018 	 loss = 0.2750(0.6666)
2023/11/22 20:02:07 - INFO - root -   Epoch: [44/300][140/283], lr: 0.00000018 	 loss = 0.2320(0.6361)
2023/11/22 20:02:25 - INFO - root -   Epoch: [44/300][160/283], lr: 0.00000018 	 loss = 1.7555(0.6316)
2023/11/22 20:02:37 - INFO - root -   Epoch: [44/300][180/283], lr: 0.00000018 	 loss = 0.0700(0.6270)
2023/11/22 20:02:51 - INFO - root -   Epoch: [44/300][200/283], lr: 0.00000018 	 loss = 0.3429(0.6036)
2023/11/22 20:02:59 - INFO - root -   Epoch: [44/300][220/283], lr: 0.00000018 	 loss = 0.5047(0.5798)
2023/11/22 20:03:17 - INFO - root -   Epoch: [44/300][240/283], lr: 0.00000018 	 loss = 0.9039(0.5748)
2023/11/22 20:03:25 - INFO - root -   Epoch: [44/300][260/283], lr: 0.00000018 	 loss = 1.3966(0.5652)
2023/11/22 20:03:39 - INFO - root -   Epoch: [44/300][280/283], lr: 0.00000018 	 loss = 2.7749(0.5657)
2023/11/22 20:03:39 - INFO - root -   Epoch: [44/300] 	 loss = 0.5629
2023/11/22 20:06:32 - INFO - root -   precision = 0.7972
2023/11/22 20:06:32 - INFO - root -   eval_loss = 0.5124
2023/11/22 20:06:32 - INFO - root -   eval_acc = 0.7972
2023/11/22 20:06:33 - INFO - root -   train_accuracy = 0.8004
2023/11/22 20:06:34 - INFO - root -   Epoch: [45/300][0/283], lr: 0.00000018 	 loss = 0.2712(0.2712)
2023/11/22 20:06:53 - INFO - root -   Epoch: [45/300][20/283], lr: 0.00000018 	 loss = 2.1510(0.7314)
2023/11/22 20:07:08 - INFO - root -   Epoch: [45/300][40/283], lr: 0.00000018 	 loss = 0.1385(0.8784)
2023/11/22 20:07:18 - INFO - root -   Epoch: [45/300][60/283], lr: 0.00000018 	 loss = 0.1471(0.7859)
2023/11/22 20:07:29 - INFO - root -   Epoch: [45/300][80/283], lr: 0.00000018 	 loss = 0.2247(0.7620)
2023/11/22 20:07:44 - INFO - root -   Epoch: [45/300][100/283], lr: 0.00000018 	 loss = 0.2622(0.7081)
2023/11/22 20:07:54 - INFO - root -   Epoch: [45/300][120/283], lr: 0.00000018 	 loss = 0.2670(0.6553)
2023/11/22 20:08:10 - INFO - root -   Epoch: [45/300][140/283], lr: 0.00000018 	 loss = 0.1288(0.6109)
2023/11/22 20:08:24 - INFO - root -   Epoch: [45/300][160/283], lr: 0.00000018 	 loss = 1.6839(0.6068)
2023/11/22 20:08:32 - INFO - root -   Epoch: [45/300][180/283], lr: 0.00000018 	 loss = 0.1559(0.5882)
2023/11/22 20:08:50 - INFO - root -   Epoch: [45/300][200/283], lr: 0.00000018 	 loss = 0.4073(0.5811)
2023/11/22 20:09:03 - INFO - root -   Epoch: [45/300][220/283], lr: 0.00000018 	 loss = 0.2121(0.5606)
2023/11/22 20:09:15 - INFO - root -   Epoch: [45/300][240/283], lr: 0.00000018 	 loss = 1.4138(0.5711)
2023/11/22 20:09:27 - INFO - root -   Epoch: [45/300][260/283], lr: 0.00000018 	 loss = 1.2528(0.5588)
2023/11/22 20:09:41 - INFO - root -   Epoch: [45/300][280/283], lr: 0.00000018 	 loss = 2.4194(0.5502)
2023/11/22 20:09:42 - INFO - root -   Epoch: [45/300] 	 loss = 0.5480
2023/11/22 20:09:42 - INFO - root -   train_accuracy = 0.8092
2023/11/22 20:09:57 - INFO - root -   Epoch: [46/300][0/283], lr: 0.00000018 	 loss = 0.3996(0.3996)
2023/11/22 20:10:05 - INFO - root -   Epoch: [46/300][20/283], lr: 0.00000018 	 loss = 2.3473(0.8405)
2023/11/22 20:10:18 - INFO - root -   Epoch: [46/300][40/283], lr: 0.00000018 	 loss = 0.1797(0.8109)
2023/11/22 20:10:34 - INFO - root -   Epoch: [46/300][60/283], lr: 0.00000018 	 loss = 0.0493(0.7360)
2023/11/22 20:10:52 - INFO - root -   Epoch: [46/300][80/283], lr: 0.00000018 	 loss = 0.1641(0.7847)
2023/11/22 20:11:04 - INFO - root -   Epoch: [46/300][100/283], lr: 0.00000018 	 loss = 0.3516(0.7695)
2023/11/22 20:11:15 - INFO - root -   Epoch: [46/300][120/283], lr: 0.00000018 	 loss = 0.0836(0.6985)
2023/11/22 20:11:31 - INFO - root -   Epoch: [46/300][140/283], lr: 0.00000018 	 loss = 0.4555(0.6483)
2023/11/22 20:11:40 - INFO - root -   Epoch: [46/300][160/283], lr: 0.00000018 	 loss = 1.1217(0.6292)
2023/11/22 20:11:52 - INFO - root -   Epoch: [46/300][180/283], lr: 0.00000018 	 loss = 0.1469(0.6075)
2023/11/22 20:12:13 - INFO - root -   Epoch: [46/300][200/283], lr: 0.00000018 	 loss = 0.3335(0.6040)
2023/11/22 20:12:25 - INFO - root -   Epoch: [46/300][220/283], lr: 0.00000018 	 loss = 0.0946(0.5951)
2023/11/22 20:12:45 - INFO - root -   Epoch: [46/300][240/283], lr: 0.00000018 	 loss = 0.7057(0.5954)
2023/11/22 20:12:54 - INFO - root -   Epoch: [46/300][260/283], lr: 0.00000018 	 loss = 0.9041(0.5844)
2023/11/22 20:13:11 - INFO - root -   Epoch: [46/300][280/283], lr: 0.00000018 	 loss = 1.2917(0.5682)
2023/11/22 20:13:12 - INFO - root -   Epoch: [46/300] 	 loss = 0.5654
2023/11/22 20:13:12 - INFO - root -   train_accuracy = 0.8004
2023/11/22 20:13:21 - INFO - root -   Epoch: [47/300][0/283], lr: 0.00000018 	 loss = 0.4745(0.4745)
2023/11/22 20:13:36 - INFO - root -   Epoch: [47/300][20/283], lr: 0.00000018 	 loss = 1.0993(0.6057)
2023/11/22 20:13:49 - INFO - root -   Epoch: [47/300][40/283], lr: 0.00000018 	 loss = 0.5999(0.7770)
2023/11/22 20:14:09 - INFO - root -   Epoch: [47/300][60/283], lr: 0.00000018 	 loss = 0.1771(0.7608)
2023/11/22 20:14:26 - INFO - root -   Epoch: [47/300][80/283], lr: 0.00000018 	 loss = 0.0638(0.7454)
2023/11/22 20:14:47 - INFO - root -   Epoch: [47/300][100/283], lr: 0.00000018 	 loss = 0.2130(0.6830)
2023/11/22 20:15:05 - INFO - root -   Epoch: [47/300][120/283], lr: 0.00000018 	 loss = 0.1487(0.6377)
2023/11/22 20:15:17 - INFO - root -   Epoch: [47/300][140/283], lr: 0.00000018 	 loss = 0.0377(0.5985)
2023/11/22 20:15:30 - INFO - root -   Epoch: [47/300][160/283], lr: 0.00000018 	 loss = 0.9643(0.5887)
2023/11/22 20:15:45 - INFO - root -   Epoch: [47/300][180/283], lr: 0.00000018 	 loss = 0.1269(0.6038)
2023/11/22 20:15:57 - INFO - root -   Epoch: [47/300][200/283], lr: 0.00000018 	 loss = 0.1751(0.5905)
2023/11/22 20:16:12 - INFO - root -   Epoch: [47/300][220/283], lr: 0.00000018 	 loss = 0.2867(0.5782)
2023/11/22 20:16:21 - INFO - root -   Epoch: [47/300][240/283], lr: 0.00000018 	 loss = 0.1354(0.5809)
2023/11/22 20:16:38 - INFO - root -   Epoch: [47/300][260/283], lr: 0.00000018 	 loss = 1.7143(0.5719)
2023/11/22 20:16:52 - INFO - root -   Epoch: [47/300][280/283], lr: 0.00000018 	 loss = 2.1828(0.5598)
2023/11/22 20:16:53 - INFO - root -   Epoch: [47/300] 	 loss = 0.5565
2023/11/22 20:16:53 - INFO - root -   train_accuracy = 0.8092
2023/11/22 20:16:55 - INFO - root -   Epoch: [48/300][0/283], lr: 0.00000018 	 loss = 0.0323(0.0323)
2023/11/22 20:17:18 - INFO - root -   Epoch: [48/300][20/283], lr: 0.00000018 	 loss = 2.4800(0.8210)
2023/11/22 20:17:35 - INFO - root -   Epoch: [48/300][40/283], lr: 0.00000018 	 loss = 0.2090(0.8014)
2023/11/22 20:17:54 - INFO - root -   Epoch: [48/300][60/283], lr: 0.00000018 	 loss = 0.0592(0.7731)
2023/11/22 20:18:15 - INFO - root -   Epoch: [48/300][80/283], lr: 0.00000018 	 loss = 0.1049(0.7694)
2023/11/22 20:18:25 - INFO - root -   Epoch: [48/300][100/283], lr: 0.00000018 	 loss = 1.1006(0.7305)
2023/11/22 20:18:45 - INFO - root -   Epoch: [48/300][120/283], lr: 0.00000018 	 loss = 0.7571(0.7243)
2023/11/22 20:18:54 - INFO - root -   Epoch: [48/300][140/283], lr: 0.00000018 	 loss = 0.2340(0.6616)
2023/11/22 20:19:11 - INFO - root -   Epoch: [48/300][160/283], lr: 0.00000018 	 loss = 0.7462(0.6462)
2023/11/22 20:19:19 - INFO - root -   Epoch: [48/300][180/283], lr: 0.00000018 	 loss = 0.3499(0.6442)
2023/11/22 20:19:36 - INFO - root -   Epoch: [48/300][200/283], lr: 0.00000018 	 loss = 0.4560(0.6308)
2023/11/22 20:19:50 - INFO - root -   Epoch: [48/300][220/283], lr: 0.00000018 	 loss = 0.0896(0.6102)
2023/11/22 20:20:11 - INFO - root -   Epoch: [48/300][240/283], lr: 0.00000018 	 loss = 1.1365(0.6126)
2023/11/22 20:20:33 - INFO - root -   Epoch: [48/300][260/283], lr: 0.00000018 	 loss = 1.5367(0.6041)
2023/11/22 20:20:42 - INFO - root -   Epoch: [48/300][280/283], lr: 0.00000018 	 loss = 2.4686(0.5882)
2023/11/22 20:20:43 - INFO - root -   Epoch: [48/300] 	 loss = 0.5845
2023/11/22 20:20:43 - INFO - root -   train_accuracy = 0.8004
2023/11/22 20:20:44 - INFO - root -   Epoch: [49/300][0/283], lr: 0.00000019 	 loss = 0.1073(0.1073)
2023/11/22 20:21:13 - INFO - root -   Epoch: [49/300][20/283], lr: 0.00000019 	 loss = 0.9030(0.8112)
2023/11/22 20:21:27 - INFO - root -   Epoch: [49/300][40/283], lr: 0.00000019 	 loss = 0.2621(0.7548)
2023/11/22 20:21:44 - INFO - root -   Epoch: [49/300][60/283], lr: 0.00000019 	 loss = 0.1207(0.7783)
2023/11/22 20:21:56 - INFO - root -   Epoch: [49/300][80/283], lr: 0.00000019 	 loss = 0.1158(0.7836)
2023/11/22 20:22:21 - INFO - root -   Epoch: [49/300][100/283], lr: 0.00000019 	 loss = 0.6851(0.7434)
2023/11/22 20:22:33 - INFO - root -   Epoch: [49/300][120/283], lr: 0.00000019 	 loss = 0.3565(0.6933)
2023/11/22 20:22:53 - INFO - root -   Epoch: [49/300][140/283], lr: 0.00000019 	 loss = 0.0253(0.6427)
2023/11/22 20:23:12 - INFO - root -   Epoch: [49/300][160/283], lr: 0.00000019 	 loss = 0.9524(0.6490)
2023/11/22 20:23:30 - INFO - root -   Epoch: [49/300][180/283], lr: 0.00000019 	 loss = 0.1782(0.6506)
2023/11/22 20:23:51 - INFO - root -   Epoch: [49/300][200/283], lr: 0.00000019 	 loss = 0.2245(0.6266)
2023/11/22 20:24:05 - INFO - root -   Epoch: [49/300][220/283], lr: 0.00000019 	 loss = 0.5215(0.6098)
2023/11/22 20:24:20 - INFO - root -   Epoch: [49/300][240/283], lr: 0.00000019 	 loss = 0.9995(0.6090)
2023/11/22 20:24:28 - INFO - root -   Epoch: [49/300][260/283], lr: 0.00000019 	 loss = 2.0797(0.5975)
2023/11/22 20:24:43 - INFO - root -   Epoch: [49/300][280/283], lr: 0.00000019 	 loss = 1.9553(0.5758)
2023/11/22 20:24:44 - INFO - root -   Epoch: [49/300] 	 loss = 0.5721
2023/11/22 20:27:39 - INFO - root -   precision = 0.7832
2023/11/22 20:27:39 - INFO - root -   eval_loss = 0.5061
2023/11/22 20:27:39 - INFO - root -   eval_acc = 0.7832
2023/11/22 20:27:40 - INFO - root -   train_accuracy = 0.8057
2023/11/22 20:27:42 - INFO - root -   Epoch: [50/300][0/283], lr: 0.00000019 	 loss = 0.0851(0.0851)
2023/11/22 20:27:58 - INFO - root -   Epoch: [50/300][20/283], lr: 0.00000019 	 loss = 1.9192(0.8899)
2023/11/22 20:28:17 - INFO - root -   Epoch: [50/300][40/283], lr: 0.00000019 	 loss = 0.2242(0.8157)
2023/11/22 20:28:31 - INFO - root -   Epoch: [50/300][60/283], lr: 0.00000019 	 loss = 0.1902(0.7890)
2023/11/22 20:28:44 - INFO - root -   Epoch: [50/300][80/283], lr: 0.00000019 	 loss = 0.4869(0.8245)
2023/11/22 20:28:55 - INFO - root -   Epoch: [50/300][100/283], lr: 0.00000019 	 loss = 0.6698(0.7588)
2023/11/22 20:29:16 - INFO - root -   Epoch: [50/300][120/283], lr: 0.00000019 	 loss = 0.5775(0.7032)
2023/11/22 20:29:35 - INFO - root -   Epoch: [50/300][140/283], lr: 0.00000019 	 loss = 0.2991(0.6386)
2023/11/22 20:29:53 - INFO - root -   Epoch: [50/300][160/283], lr: 0.00000019 	 loss = 1.3207(0.6154)
2023/11/22 20:30:05 - INFO - root -   Epoch: [50/300][180/283], lr: 0.00000019 	 loss = 0.2343(0.6113)
2023/11/22 20:30:23 - INFO - root -   Epoch: [50/300][200/283], lr: 0.00000019 	 loss = 0.3345(0.5954)
2023/11/22 20:30:31 - INFO - root -   Epoch: [50/300][220/283], lr: 0.00000019 	 loss = 0.1231(0.5789)
2023/11/22 20:30:41 - INFO - root -   Epoch: [50/300][240/283], lr: 0.00000019 	 loss = 1.0489(0.5773)
2023/11/22 20:31:07 - INFO - root -   Epoch: [50/300][260/283], lr: 0.00000019 	 loss = 1.1921(0.5677)
2023/11/22 20:31:19 - INFO - root -   Epoch: [50/300][280/283], lr: 0.00000019 	 loss = 1.5855(0.5583)
2023/11/22 20:31:20 - INFO - root -   Epoch: [50/300] 	 loss = 0.5553
2023/11/22 20:31:20 - INFO - root -   train_accuracy = 0.8145
2023/11/22 20:31:22 - INFO - root -   Epoch: [51/300][0/283], lr: 0.00000019 	 loss = 0.0430(0.0430)
2023/11/22 20:31:35 - INFO - root -   Epoch: [51/300][20/283], lr: 0.00000019 	 loss = 2.7627(0.7543)
2023/11/22 20:31:52 - INFO - root -   Epoch: [51/300][40/283], lr: 0.00000019 	 loss = 0.2126(0.8451)
2023/11/22 20:32:07 - INFO - root -   Epoch: [51/300][60/283], lr: 0.00000019 	 loss = 0.5165(0.7847)
2023/11/22 20:32:17 - INFO - root -   Epoch: [51/300][80/283], lr: 0.00000019 	 loss = 0.0480(0.7555)
2023/11/22 20:32:29 - INFO - root -   Epoch: [51/300][100/283], lr: 0.00000019 	 loss = 0.3050(0.7060)
2023/11/22 20:32:46 - INFO - root -   Epoch: [51/300][120/283], lr: 0.00000019 	 loss = 0.3280(0.6563)
2023/11/22 20:33:06 - INFO - root -   Epoch: [51/300][140/283], lr: 0.00000019 	 loss = 0.2669(0.6182)
2023/11/22 20:33:22 - INFO - root -   Epoch: [51/300][160/283], lr: 0.00000019 	 loss = 0.4730(0.6036)
2023/11/22 20:33:34 - INFO - root -   Epoch: [51/300][180/283], lr: 0.00000019 	 loss = 0.0634(0.5930)
2023/11/22 20:33:48 - INFO - root -   Epoch: [51/300][200/283], lr: 0.00000019 	 loss = 0.1551(0.5683)
2023/11/22 20:34:01 - INFO - root -   Epoch: [51/300][220/283], lr: 0.00000019 	 loss = 0.2590(0.5509)
2023/11/22 20:34:09 - INFO - root -   Epoch: [51/300][240/283], lr: 0.00000019 	 loss = 0.1471(0.5433)
2023/11/22 20:34:21 - INFO - root -   Epoch: [51/300][260/283], lr: 0.00000019 	 loss = 1.6836(0.5288)
2023/11/22 20:34:34 - INFO - root -   Epoch: [51/300][280/283], lr: 0.00000019 	 loss = 2.2188(0.5211)
2023/11/22 20:34:35 - INFO - root -   Epoch: [51/300] 	 loss = 0.5181
2023/11/22 20:34:35 - INFO - root -   train_accuracy = 0.8110
2023/11/22 20:34:36 - INFO - root -   Epoch: [52/300][0/283], lr: 0.00000019 	 loss = 0.1857(0.1857)
2023/11/22 20:34:58 - INFO - root -   Epoch: [52/300][20/283], lr: 0.00000019 	 loss = 1.1648(0.6635)
2023/11/22 20:35:12 - INFO - root -   Epoch: [52/300][40/283], lr: 0.00000019 	 loss = 0.2643(0.7644)
2023/11/22 20:35:20 - INFO - root -   Epoch: [52/300][60/283], lr: 0.00000019 	 loss = 0.0227(0.7151)
2023/11/22 20:35:41 - INFO - root -   Epoch: [52/300][80/283], lr: 0.00000019 	 loss = 0.0931(0.7530)
2023/11/22 20:36:06 - INFO - root -   Epoch: [52/300][100/283], lr: 0.00000019 	 loss = 0.6981(0.7155)
2023/11/22 20:36:17 - INFO - root -   Epoch: [52/300][120/283], lr: 0.00000019 	 loss = 0.1469(0.6670)
2023/11/22 20:36:39 - INFO - root -   Epoch: [52/300][140/283], lr: 0.00000019 	 loss = 0.1140(0.6176)
2023/11/22 20:37:00 - INFO - root -   Epoch: [52/300][160/283], lr: 0.00000019 	 loss = 0.7033(0.6178)
2023/11/22 20:37:13 - INFO - root -   Epoch: [52/300][180/283], lr: 0.00000019 	 loss = 0.1266(0.6339)
2023/11/22 20:37:26 - INFO - root -   Epoch: [52/300][200/283], lr: 0.00000019 	 loss = 0.3164(0.6199)
2023/11/22 20:37:41 - INFO - root -   Epoch: [52/300][220/283], lr: 0.00000019 	 loss = 0.2046(0.5968)
2023/11/22 20:38:02 - INFO - root -   Epoch: [52/300][240/283], lr: 0.00000019 	 loss = 0.7549(0.5970)
2023/11/22 20:38:13 - INFO - root -   Epoch: [52/300][260/283], lr: 0.00000019 	 loss = 1.1892(0.5839)
2023/11/22 20:38:32 - INFO - root -   Epoch: [52/300][280/283], lr: 0.00000019 	 loss = 1.4984(0.5642)
2023/11/22 20:38:33 - INFO - root -   Epoch: [52/300] 	 loss = 0.5609
2023/11/22 20:38:33 - INFO - root -   train_accuracy = 0.8092
2023/11/22 20:38:35 - INFO - root -   Epoch: [53/300][0/283], lr: 0.00000019 	 loss = 0.2964(0.2964)
2023/11/22 20:38:49 - INFO - root -   Epoch: [53/300][20/283], lr: 0.00000019 	 loss = 1.8308(0.7645)
2023/11/22 20:39:08 - INFO - root -   Epoch: [53/300][40/283], lr: 0.00000019 	 loss = 0.3824(0.7903)
2023/11/22 20:39:26 - INFO - root -   Epoch: [53/300][60/283], lr: 0.00000019 	 loss = 0.1748(0.7229)
2023/11/22 20:39:36 - INFO - root -   Epoch: [53/300][80/283], lr: 0.00000019 	 loss = 0.0839(0.7669)
2023/11/22 20:39:49 - INFO - root -   Epoch: [53/300][100/283], lr: 0.00000019 	 loss = 0.1641(0.7324)
2023/11/22 20:40:13 - INFO - root -   Epoch: [53/300][120/283], lr: 0.00000019 	 loss = 0.4264(0.6777)
2023/11/22 20:40:34 - INFO - root -   Epoch: [53/300][140/283], lr: 0.00000019 	 loss = 0.1945(0.6277)
2023/11/22 20:40:46 - INFO - root -   Epoch: [53/300][160/283], lr: 0.00000019 	 loss = 1.3162(0.6150)
2023/11/22 20:41:01 - INFO - root -   Epoch: [53/300][180/283], lr: 0.00000019 	 loss = 0.2888(0.6143)
2023/11/22 20:41:18 - INFO - root -   Epoch: [53/300][200/283], lr: 0.00000019 	 loss = 0.1673(0.6018)
2023/11/22 20:41:35 - INFO - root -   Epoch: [53/300][220/283], lr: 0.00000019 	 loss = 0.0729(0.5756)
2023/11/22 20:41:55 - INFO - root -   Epoch: [53/300][240/283], lr: 0.00000019 	 loss = 0.0772(0.5719)
2023/11/22 20:42:09 - INFO - root -   Epoch: [53/300][260/283], lr: 0.00000019 	 loss = 1.5918(0.5620)
2023/11/22 20:42:22 - INFO - root -   Epoch: [53/300][280/283], lr: 0.00000019 	 loss = 3.2411(0.5569)
2023/11/22 20:42:23 - INFO - root -   Epoch: [53/300] 	 loss = 0.5536
2023/11/22 20:42:23 - INFO - root -   train_accuracy = 0.8039
2023/11/22 20:42:24 - INFO - root -   Epoch: [54/300][0/283], lr: 0.00000019 	 loss = 0.0711(0.0711)
2023/11/22 20:42:47 - INFO - root -   Epoch: [54/300][20/283], lr: 0.00000019 	 loss = 2.1830(0.7830)
2023/11/22 20:43:04 - INFO - root -   Epoch: [54/300][40/283], lr: 0.00000019 	 loss = 0.4365(0.8416)
2023/11/22 20:43:20 - INFO - root -   Epoch: [54/300][60/283], lr: 0.00000019 	 loss = 0.0945(0.7885)
2023/11/22 20:43:36 - INFO - root -   Epoch: [54/300][80/283], lr: 0.00000019 	 loss = 0.1616(0.7894)
2023/11/22 20:43:44 - INFO - root -   Epoch: [54/300][100/283], lr: 0.00000019 	 loss = 0.3288(0.7419)
2023/11/22 20:44:02 - INFO - root -   Epoch: [54/300][120/283], lr: 0.00000019 	 loss = 0.2046(0.6743)
2023/11/22 20:44:12 - INFO - root -   Epoch: [54/300][140/283], lr: 0.00000019 	 loss = 0.1086(0.6191)
2023/11/22 20:44:25 - INFO - root -   Epoch: [54/300][160/283], lr: 0.00000019 	 loss = 1.0877(0.5976)
2023/11/22 20:44:43 - INFO - root -   Epoch: [54/300][180/283], lr: 0.00000019 	 loss = 0.0291(0.5848)
2023/11/22 20:44:55 - INFO - root -   Epoch: [54/300][200/283], lr: 0.00000019 	 loss = 0.2121(0.5728)
2023/11/22 20:45:11 - INFO - root -   Epoch: [54/300][220/283], lr: 0.00000019 	 loss = 0.2033(0.5699)
2023/11/22 20:45:19 - INFO - root -   Epoch: [54/300][240/283], lr: 0.00000019 	 loss = 1.8355(0.5743)
2023/11/22 20:45:33 - INFO - root -   Epoch: [54/300][260/283], lr: 0.00000019 	 loss = 2.1355(0.5642)
2023/11/22 20:45:41 - INFO - root -   Epoch: [54/300][280/283], lr: 0.00000019 	 loss = 2.0840(0.5515)
2023/11/22 20:45:42 - INFO - root -   Epoch: [54/300] 	 loss = 0.5480
2023/11/22 20:48:35 - INFO - root -   precision = 0.8182
2023/11/22 20:48:35 - INFO - root -   eval_loss = 0.4959
2023/11/22 20:48:35 - INFO - root -   eval_acc = 0.8182
2023/11/22 20:48:36 - INFO - root -   train_accuracy = 0.8057
2023/11/22 20:48:49 - INFO - root -   Epoch: [55/300][0/283], lr: 0.00000020 	 loss = 0.0813(0.0813)
2023/11/22 20:49:03 - INFO - root -   Epoch: [55/300][20/283], lr: 0.00000020 	 loss = 1.2587(0.5469)
2023/11/22 20:49:27 - INFO - root -   Epoch: [55/300][40/283], lr: 0.00000020 	 loss = 0.2545(0.6863)
2023/11/22 20:49:35 - INFO - root -   Epoch: [55/300][60/283], lr: 0.00000020 	 loss = 0.1382(0.6602)
2023/11/22 20:49:50 - INFO - root -   Epoch: [55/300][80/283], lr: 0.00000020 	 loss = 0.1339(0.6773)
2023/11/22 20:49:59 - INFO - root -   Epoch: [55/300][100/283], lr: 0.00000020 	 loss = 0.1898(0.6462)
2023/11/22 20:50:17 - INFO - root -   Epoch: [55/300][120/283], lr: 0.00000020 	 loss = 0.0949(0.6077)
2023/11/22 20:50:36 - INFO - root -   Epoch: [55/300][140/283], lr: 0.00000020 	 loss = 0.2057(0.5669)
2023/11/22 20:50:53 - INFO - root -   Epoch: [55/300][160/283], lr: 0.00000020 	 loss = 0.4989(0.5660)
2023/11/22 20:51:05 - INFO - root -   Epoch: [55/300][180/283], lr: 0.00000020 	 loss = 0.0743(0.5851)
2023/11/22 20:51:31 - INFO - root -   Epoch: [55/300][200/283], lr: 0.00000020 	 loss = 0.2334(0.5694)
2023/11/22 20:51:47 - INFO - root -   Epoch: [55/300][220/283], lr: 0.00000020 	 loss = 0.1186(0.5577)
2023/11/22 20:52:03 - INFO - root -   Epoch: [55/300][240/283], lr: 0.00000020 	 loss = 1.5779(0.5606)
2023/11/22 20:52:18 - INFO - root -   Epoch: [55/300][260/283], lr: 0.00000020 	 loss = 1.2728(0.5499)
2023/11/22 20:52:26 - INFO - root -   Epoch: [55/300][280/283], lr: 0.00000020 	 loss = 3.0081(0.5479)
2023/11/22 20:52:27 - INFO - root -   Epoch: [55/300] 	 loss = 0.5448
2023/11/22 20:52:27 - INFO - root -   train_accuracy = 0.8057
2023/11/22 20:52:34 - INFO - root -   Epoch: [56/300][0/283], lr: 0.00000020 	 loss = 0.1172(0.1172)
2023/11/22 20:52:54 - INFO - root -   Epoch: [56/300][20/283], lr: 0.00000020 	 loss = 1.4812(0.7936)
2023/11/22 20:53:05 - INFO - root -   Epoch: [56/300][40/283], lr: 0.00000020 	 loss = 0.2016(0.7840)
2023/11/22 20:53:18 - INFO - root -   Epoch: [56/300][60/283], lr: 0.00000020 	 loss = 0.0530(0.7409)
2023/11/22 20:53:30 - INFO - root -   Epoch: [56/300][80/283], lr: 0.00000020 	 loss = 0.0252(0.7176)
2023/11/22 20:53:46 - INFO - root -   Epoch: [56/300][100/283], lr: 0.00000020 	 loss = 0.7606(0.6732)
2023/11/22 20:54:03 - INFO - root -   Epoch: [56/300][120/283], lr: 0.00000020 	 loss = 0.3178(0.6345)
2023/11/22 20:54:17 - INFO - root -   Epoch: [56/300][140/283], lr: 0.00000020 	 loss = 0.1312(0.5894)
2023/11/22 20:54:33 - INFO - root -   Epoch: [56/300][160/283], lr: 0.00000020 	 loss = 1.4072(0.5794)
2023/11/22 20:54:46 - INFO - root -   Epoch: [56/300][180/283], lr: 0.00000020 	 loss = 0.4478(0.5982)
2023/11/22 20:55:05 - INFO - root -   Epoch: [56/300][200/283], lr: 0.00000020 	 loss = 0.2061(0.5846)
2023/11/22 20:55:20 - INFO - root -   Epoch: [56/300][220/283], lr: 0.00000020 	 loss = 0.3379(0.5673)
2023/11/22 20:55:37 - INFO - root -   Epoch: [56/300][240/283], lr: 0.00000020 	 loss = 0.6079(0.5744)
2023/11/22 20:55:52 - INFO - root -   Epoch: [56/300][260/283], lr: 0.00000020 	 loss = 1.7604(0.5629)
2023/11/22 20:56:06 - INFO - root -   Epoch: [56/300][280/283], lr: 0.00000020 	 loss = 1.7790(0.5504)
2023/11/22 20:56:07 - INFO - root -   Epoch: [56/300] 	 loss = 0.5491
2023/11/22 20:56:07 - INFO - root -   train_accuracy = 0.7774
2023/11/22 20:56:08 - INFO - root -   Epoch: [57/300][0/283], lr: 0.00000020 	 loss = 0.0880(0.0880)
2023/11/22 20:56:23 - INFO - root -   Epoch: [57/300][20/283], lr: 0.00000020 	 loss = 1.0836(0.7103)
2023/11/22 20:56:40 - INFO - root -   Epoch: [57/300][40/283], lr: 0.00000020 	 loss = 0.1721(0.7259)
2023/11/22 20:56:58 - INFO - root -   Epoch: [57/300][60/283], lr: 0.00000020 	 loss = 0.0857(0.7259)
2023/11/22 20:57:13 - INFO - root -   Epoch: [57/300][80/283], lr: 0.00000020 	 loss = 0.5961(0.7317)
2023/11/22 20:57:26 - INFO - root -   Epoch: [57/300][100/283], lr: 0.00000020 	 loss = 0.3627(0.6738)
2023/11/22 20:57:40 - INFO - root -   Epoch: [57/300][120/283], lr: 0.00000020 	 loss = 0.3407(0.6337)
2023/11/22 20:57:52 - INFO - root -   Epoch: [57/300][140/283], lr: 0.00000020 	 loss = 0.2729(0.5913)
2023/11/22 20:58:07 - INFO - root -   Epoch: [57/300][160/283], lr: 0.00000020 	 loss = 0.5655(0.5651)
2023/11/22 20:58:21 - INFO - root -   Epoch: [57/300][180/283], lr: 0.00000020 	 loss = 0.1161(0.5737)
2023/11/22 20:58:34 - INFO - root -   Epoch: [57/300][200/283], lr: 0.00000020 	 loss = 0.5219(0.5641)
2023/11/22 20:58:49 - INFO - root -   Epoch: [57/300][220/283], lr: 0.00000020 	 loss = 0.2820(0.5427)
2023/11/22 20:59:03 - INFO - root -   Epoch: [57/300][240/283], lr: 0.00000020 	 loss = 1.1858(0.5527)
2023/11/22 20:59:16 - INFO - root -   Epoch: [57/300][260/283], lr: 0.00000020 	 loss = 1.6524(0.5366)
2023/11/22 20:59:27 - INFO - root -   Epoch: [57/300][280/283], lr: 0.00000020 	 loss = 1.6569(0.5193)
2023/11/22 20:59:28 - INFO - root -   Epoch: [57/300] 	 loss = 0.5176
2023/11/22 20:59:28 - INFO - root -   train_accuracy = 0.8216
2023/11/22 20:59:29 - INFO - root -   Epoch: [58/300][0/283], lr: 0.00000020 	 loss = 0.1645(0.1645)
2023/11/22 20:59:43 - INFO - root -   Epoch: [58/300][20/283], lr: 0.00000020 	 loss = 2.6003(0.6692)
2023/11/22 21:00:00 - INFO - root -   Epoch: [58/300][40/283], lr: 0.00000020 	 loss = 0.2462(0.7656)
2023/11/22 21:00:17 - INFO - root -   Epoch: [58/300][60/283], lr: 0.00000020 	 loss = 0.1006(0.7452)
2023/11/22 21:00:28 - INFO - root -   Epoch: [58/300][80/283], lr: 0.00000020 	 loss = 0.1591(0.7858)
2023/11/22 21:00:43 - INFO - root -   Epoch: [58/300][100/283], lr: 0.00000020 	 loss = 0.4618(0.7234)
2023/11/22 21:00:59 - INFO - root -   Epoch: [58/300][120/283], lr: 0.00000020 	 loss = 1.3126(0.6733)
2023/11/22 21:01:16 - INFO - root -   Epoch: [58/300][140/283], lr: 0.00000020 	 loss = 0.0460(0.6306)
2023/11/22 21:01:41 - INFO - root -   Epoch: [58/300][160/283], lr: 0.00000020 	 loss = 1.3957(0.6220)
2023/11/22 21:01:56 - INFO - root -   Epoch: [58/300][180/283], lr: 0.00000020 	 loss = 0.0780(0.6355)
2023/11/22 21:02:14 - INFO - root -   Epoch: [58/300][200/283], lr: 0.00000020 	 loss = 0.3249(0.6191)
2023/11/22 21:02:31 - INFO - root -   Epoch: [58/300][220/283], lr: 0.00000020 	 loss = 0.0380(0.6056)
2023/11/22 21:02:42 - INFO - root -   Epoch: [58/300][240/283], lr: 0.00000020 	 loss = 1.3414(0.6032)
2023/11/22 21:02:55 - INFO - root -   Epoch: [58/300][260/283], lr: 0.00000020 	 loss = 1.2189(0.5847)
2023/11/22 21:03:03 - INFO - root -   Epoch: [58/300][280/283], lr: 0.00000020 	 loss = 1.2304(0.5696)
2023/11/22 21:03:04 - INFO - root -   Epoch: [58/300] 	 loss = 0.5697
2023/11/22 21:03:04 - INFO - root -   train_accuracy = 0.7809
2023/11/22 21:03:13 - INFO - root -   Epoch: [59/300][0/283], lr: 0.00000020 	 loss = 0.1484(0.1484)
2023/11/22 21:03:26 - INFO - root -   Epoch: [59/300][20/283], lr: 0.00000020 	 loss = 0.9223(0.8024)
2023/11/22 21:03:46 - INFO - root -   Epoch: [59/300][40/283], lr: 0.00000020 	 loss = 0.6191(0.8245)
2023/11/22 21:04:02 - INFO - root -   Epoch: [59/300][60/283], lr: 0.00000020 	 loss = 0.4743(0.7629)
2023/11/22 21:04:10 - INFO - root -   Epoch: [59/300][80/283], lr: 0.00000020 	 loss = 0.2733(0.7567)
2023/11/22 21:04:25 - INFO - root -   Epoch: [59/300][100/283], lr: 0.00000020 	 loss = 0.3744(0.7145)
2023/11/22 21:04:34 - INFO - root -   Epoch: [59/300][120/283], lr: 0.00000020 	 loss = 0.1447(0.6661)
2023/11/22 21:04:53 - INFO - root -   Epoch: [59/300][140/283], lr: 0.00000020 	 loss = 0.3035(0.6161)
2023/11/22 21:05:13 - INFO - root -   Epoch: [59/300][160/283], lr: 0.00000020 	 loss = 1.9159(0.6051)
2023/11/22 21:05:35 - INFO - root -   Epoch: [59/300][180/283], lr: 0.00000020 	 loss = 0.0977(0.6005)
2023/11/22 21:05:49 - INFO - root -   Epoch: [59/300][200/283], lr: 0.00000020 	 loss = 0.2324(0.5939)
2023/11/22 21:05:57 - INFO - root -   Epoch: [59/300][220/283], lr: 0.00000020 	 loss = 0.2645(0.5831)
2023/11/22 21:06:18 - INFO - root -   Epoch: [59/300][240/283], lr: 0.00000020 	 loss = 1.2340(0.5807)
2023/11/22 21:06:26 - INFO - root -   Epoch: [59/300][260/283], lr: 0.00000020 	 loss = 1.5931(0.5685)
2023/11/22 21:06:42 - INFO - root -   Epoch: [59/300][280/283], lr: 0.00000020 	 loss = 1.5579(0.5520)
2023/11/22 21:06:43 - INFO - root -   Epoch: [59/300] 	 loss = 0.5487
2023/11/22 21:09:34 - INFO - root -   precision = 0.7972
2023/11/22 21:09:34 - INFO - root -   eval_loss = 0.5320
2023/11/22 21:09:34 - INFO - root -   eval_acc = 0.7972
2023/11/22 21:09:35 - INFO - root -   train_accuracy = 0.8110
2023/11/22 21:09:36 - INFO - root -   Epoch: [60/300][0/283], lr: 0.00000020 	 loss = 0.1350(0.1350)
2023/11/22 21:09:57 - INFO - root -   Epoch: [60/300][20/283], lr: 0.00000020 	 loss = 2.1158(0.8415)
2023/11/22 21:10:15 - INFO - root -   Epoch: [60/300][40/283], lr: 0.00000020 	 loss = 0.6937(0.8042)
2023/11/22 21:10:23 - INFO - root -   Epoch: [60/300][60/283], lr: 0.00000020 	 loss = 0.1630(0.7795)
2023/11/22 21:10:42 - INFO - root -   Epoch: [60/300][80/283], lr: 0.00000020 	 loss = 0.1208(0.7548)
2023/11/22 21:11:00 - INFO - root -   Epoch: [60/300][100/283], lr: 0.00000020 	 loss = 0.3855(0.7165)
2023/11/22 21:11:09 - INFO - root -   Epoch: [60/300][120/283], lr: 0.00000020 	 loss = 0.1516(0.6473)
2023/11/22 21:11:25 - INFO - root -   Epoch: [60/300][140/283], lr: 0.00000020 	 loss = 0.0951(0.5881)
2023/11/22 21:11:33 - INFO - root -   Epoch: [60/300][160/283], lr: 0.00000020 	 loss = 0.4532(0.5779)
2023/11/22 21:11:49 - INFO - root -   Epoch: [60/300][180/283], lr: 0.00000020 	 loss = 0.2869(0.5787)
2023/11/22 21:12:04 - INFO - root -   Epoch: [60/300][200/283], lr: 0.00000020 	 loss = 0.1478(0.5747)
2023/11/22 21:12:25 - INFO - root -   Epoch: [60/300][220/283], lr: 0.00000020 	 loss = 0.0144(0.5494)
2023/11/22 21:12:33 - INFO - root -   Epoch: [60/300][240/283], lr: 0.00000020 	 loss = 0.9451(0.5517)
2023/11/22 21:12:47 - INFO - root -   Epoch: [60/300][260/283], lr: 0.00000020 	 loss = 1.0377(0.5330)
2023/11/22 21:12:55 - INFO - root -   Epoch: [60/300][280/283], lr: 0.00000020 	 loss = 2.0645(0.5239)
2023/11/22 21:12:56 - INFO - root -   Epoch: [60/300] 	 loss = 0.5206
2023/11/22 21:12:56 - INFO - root -   train_accuracy = 0.8127
2023/11/22 21:13:03 - INFO - root -   Epoch: [61/300][0/283], lr: 0.00000021 	 loss = 0.0457(0.0457)
2023/11/22 21:13:11 - INFO - root -   Epoch: [61/300][20/283], lr: 0.00000021 	 loss = 0.9285(0.6485)
2023/11/22 21:13:28 - INFO - root -   Epoch: [61/300][40/283], lr: 0.00000021 	 loss = 0.1338(0.6895)
2023/11/22 21:13:40 - INFO - root -   Epoch: [61/300][60/283], lr: 0.00000021 	 loss = 0.2358(0.7137)
2023/11/22 21:13:53 - INFO - root -   Epoch: [61/300][80/283], lr: 0.00000021 	 loss = 0.3321(0.7324)
2023/11/22 21:14:01 - INFO - root -   Epoch: [61/300][100/283], lr: 0.00000021 	 loss = 0.1350(0.6696)
2023/11/22 21:14:12 - INFO - root -   Epoch: [61/300][120/283], lr: 0.00000021 	 loss = 0.3551(0.6323)
2023/11/22 21:14:35 - INFO - root -   Epoch: [61/300][140/283], lr: 0.00000021 	 loss = 0.2789(0.5785)
2023/11/22 21:14:49 - INFO - root -   Epoch: [61/300][160/283], lr: 0.00000021 	 loss = 1.0989(0.5632)
2023/11/22 21:15:09 - INFO - root -   Epoch: [61/300][180/283], lr: 0.00000021 	 loss = 0.4348(0.5715)
2023/11/22 21:15:20 - INFO - root -   Epoch: [61/300][200/283], lr: 0.00000021 	 loss = 0.1740(0.5658)
2023/11/22 21:15:35 - INFO - root -   Epoch: [61/300][220/283], lr: 0.00000021 	 loss = 0.0809(0.5475)
2023/11/22 21:15:55 - INFO - root -   Epoch: [61/300][240/283], lr: 0.00000021 	 loss = 0.2136(0.5501)
2023/11/22 21:16:03 - INFO - root -   Epoch: [61/300][260/283], lr: 0.00000021 	 loss = 2.6684(0.5528)
2023/11/22 21:16:17 - INFO - root -   Epoch: [61/300][280/283], lr: 0.00000021 	 loss = 2.5785(0.5479)
2023/11/22 21:16:18 - INFO - root -   Epoch: [61/300] 	 loss = 0.5451
2023/11/22 21:16:18 - INFO - root -   train_accuracy = 0.8163
2023/11/22 21:16:28 - INFO - root -   Epoch: [62/300][0/283], lr: 0.00000021 	 loss = 0.3248(0.3248)
2023/11/22 21:16:36 - INFO - root -   Epoch: [62/300][20/283], lr: 0.00000021 	 loss = 0.5972(0.6917)
2023/11/22 21:16:50 - INFO - root -   Epoch: [62/300][40/283], lr: 0.00000021 	 loss = 0.4018(0.7215)
2023/11/22 21:17:01 - INFO - root -   Epoch: [62/300][60/283], lr: 0.00000021 	 loss = 0.0888(0.7060)
2023/11/22 21:17:13 - INFO - root -   Epoch: [62/300][80/283], lr: 0.00000021 	 loss = 0.1915(0.6805)
2023/11/22 21:17:23 - INFO - root -   Epoch: [62/300][100/283], lr: 0.00000021 	 loss = 0.0483(0.6306)
2023/11/22 21:17:39 - INFO - root -   Epoch: [62/300][120/283], lr: 0.00000021 	 loss = 0.0775(0.5900)
2023/11/22 21:17:53 - INFO - root -   Epoch: [62/300][140/283], lr: 0.00000021 	 loss = 0.0961(0.5582)
2023/11/22 21:18:12 - INFO - root -   Epoch: [62/300][160/283], lr: 0.00000021 	 loss = 0.4874(0.5639)
2023/11/22 21:18:28 - INFO - root -   Epoch: [62/300][180/283], lr: 0.00000021 	 loss = 0.2141(0.5495)
2023/11/22 21:18:38 - INFO - root -   Epoch: [62/300][200/283], lr: 0.00000021 	 loss = 0.5674(0.5360)
2023/11/22 21:18:47 - INFO - root -   Epoch: [62/300][220/283], lr: 0.00000021 	 loss = 0.3421(0.5244)
2023/11/22 21:19:06 - INFO - root -   Epoch: [62/300][240/283], lr: 0.00000021 	 loss = 0.9571(0.5257)
2023/11/22 21:19:14 - INFO - root -   Epoch: [62/300][260/283], lr: 0.00000021 	 loss = 1.7920(0.5188)
2023/11/22 21:19:28 - INFO - root -   Epoch: [62/300][280/283], lr: 0.00000021 	 loss = 2.5048(0.5101)
2023/11/22 21:19:29 - INFO - root -   Epoch: [62/300] 	 loss = 0.5079
2023/11/22 21:19:29 - INFO - root -   train_accuracy = 0.8322
2023/11/22 21:19:30 - INFO - root -   Epoch: [63/300][0/283], lr: 0.00000021 	 loss = 0.1072(0.1072)
2023/11/22 21:19:52 - INFO - root -   Epoch: [63/300][20/283], lr: 0.00000021 	 loss = 1.4990(0.9076)
2023/11/22 21:20:04 - INFO - root -   Epoch: [63/300][40/283], lr: 0.00000021 	 loss = 0.2670(0.8449)
2023/11/22 21:20:19 - INFO - root -   Epoch: [63/300][60/283], lr: 0.00000021 	 loss = 0.1363(0.7765)
2023/11/22 21:20:34 - INFO - root -   Epoch: [63/300][80/283], lr: 0.00000021 	 loss = 0.0821(0.7523)
2023/11/22 21:20:52 - INFO - root -   Epoch: [63/300][100/283], lr: 0.00000021 	 loss = 0.1762(0.6976)
2023/11/22 21:21:05 - INFO - root -   Epoch: [63/300][120/283], lr: 0.00000021 	 loss = 0.4689(0.6376)
2023/11/22 21:21:27 - INFO - root -   Epoch: [63/300][140/283], lr: 0.00000021 	 loss = 0.2780(0.5906)
2023/11/22 21:21:46 - INFO - root -   Epoch: [63/300][160/283], lr: 0.00000021 	 loss = 1.5943(0.5809)
2023/11/22 21:22:00 - INFO - root -   Epoch: [63/300][180/283], lr: 0.00000021 	 loss = 0.1218(0.5865)
2023/11/22 21:22:13 - INFO - root -   Epoch: [63/300][200/283], lr: 0.00000021 	 loss = 0.1574(0.5800)
2023/11/22 21:22:33 - INFO - root -   Epoch: [63/300][220/283], lr: 0.00000021 	 loss = 0.3262(0.5569)
2023/11/22 21:22:43 - INFO - root -   Epoch: [63/300][240/283], lr: 0.00000021 	 loss = 1.4354(0.5522)
2023/11/22 21:23:00 - INFO - root -   Epoch: [63/300][260/283], lr: 0.00000021 	 loss = 1.6827(0.5450)
2023/11/22 21:23:16 - INFO - root -   Epoch: [63/300][280/283], lr: 0.00000021 	 loss = 1.4416(0.5255)
2023/11/22 21:23:17 - INFO - root -   Epoch: [63/300] 	 loss = 0.5225
2023/11/22 21:23:17 - INFO - root -   train_accuracy = 0.8233
2023/11/22 21:23:18 - INFO - root -   Epoch: [64/300][0/283], lr: 0.00000021 	 loss = 0.2111(0.2111)
2023/11/22 21:23:47 - INFO - root -   Epoch: [64/300][20/283], lr: 0.00000021 	 loss = 2.1823(0.6971)
2023/11/22 21:24:04 - INFO - root -   Epoch: [64/300][40/283], lr: 0.00000021 	 loss = 0.3457(0.6399)
2023/11/22 21:24:20 - INFO - root -   Epoch: [64/300][60/283], lr: 0.00000021 	 loss = 0.2858(0.6528)
2023/11/22 21:24:39 - INFO - root -   Epoch: [64/300][80/283], lr: 0.00000021 	 loss = 0.1212(0.6924)
2023/11/22 21:24:57 - INFO - root -   Epoch: [64/300][100/283], lr: 0.00000021 	 loss = 0.2465(0.6801)
2023/11/22 21:25:11 - INFO - root -   Epoch: [64/300][120/283], lr: 0.00000021 	 loss = 0.1093(0.6277)
2023/11/22 21:25:29 - INFO - root -   Epoch: [64/300][140/283], lr: 0.00000021 	 loss = 0.1294(0.5740)
2023/11/22 21:25:43 - INFO - root -   Epoch: [64/300][160/283], lr: 0.00000021 	 loss = 2.0953(0.5658)
2023/11/22 21:25:56 - INFO - root -   Epoch: [64/300][180/283], lr: 0.00000021 	 loss = 0.1126(0.5716)
2023/11/22 21:26:15 - INFO - root -   Epoch: [64/300][200/283], lr: 0.00000021 	 loss = 0.1639(0.5570)
2023/11/22 21:26:28 - INFO - root -   Epoch: [64/300][220/283], lr: 0.00000021 	 loss = 0.7321(0.5525)
2023/11/22 21:26:49 - INFO - root -   Epoch: [64/300][240/283], lr: 0.00000021 	 loss = 0.8067(0.5478)
2023/11/22 21:27:07 - INFO - root -   Epoch: [64/300][260/283], lr: 0.00000021 	 loss = 1.8352(0.5441)
2023/11/22 21:27:20 - INFO - root -   Epoch: [64/300][280/283], lr: 0.00000021 	 loss = 3.5940(0.5350)
2023/11/22 21:27:21 - INFO - root -   Epoch: [64/300] 	 loss = 0.5322
2023/11/22 21:30:14 - INFO - root -   precision = 0.8182
2023/11/22 21:30:14 - INFO - root -   eval_loss = 0.5015
2023/11/22 21:30:14 - INFO - root -   eval_acc = 0.8182
2023/11/22 21:30:15 - INFO - root -   train_accuracy = 0.8286
2023/11/22 21:30:16 - INFO - root -   Epoch: [65/300][0/283], lr: 0.00000021 	 loss = 0.1526(0.1526)
2023/11/22 21:30:36 - INFO - root -   Epoch: [65/300][20/283], lr: 0.00000021 	 loss = 2.5049(0.8202)
2023/11/22 21:30:47 - INFO - root -   Epoch: [65/300][40/283], lr: 0.00000021 	 loss = 0.2166(0.8240)
2023/11/22 21:31:01 - INFO - root -   Epoch: [65/300][60/283], lr: 0.00000021 	 loss = 0.2635(0.7991)
2023/11/22 21:31:12 - INFO - root -   Epoch: [65/300][80/283], lr: 0.00000021 	 loss = 0.2777(0.7886)
2023/11/22 21:31:25 - INFO - root -   Epoch: [65/300][100/283], lr: 0.00000021 	 loss = 0.9597(0.7278)
2023/11/22 21:31:43 - INFO - root -   Epoch: [65/300][120/283], lr: 0.00000021 	 loss = 0.2096(0.6736)
2023/11/22 21:32:01 - INFO - root -   Epoch: [65/300][140/283], lr: 0.00000021 	 loss = 0.2022(0.6262)
2023/11/22 21:32:11 - INFO - root -   Epoch: [65/300][160/283], lr: 0.00000021 	 loss = 0.9384(0.5976)
2023/11/22 21:32:26 - INFO - root -   Epoch: [65/300][180/283], lr: 0.00000021 	 loss = 0.3246(0.6039)
2023/11/22 21:32:46 - INFO - root -   Epoch: [65/300][200/283], lr: 0.00000021 	 loss = 0.4075(0.5985)
2023/11/22 21:32:57 - INFO - root -   Epoch: [65/300][220/283], lr: 0.00000021 	 loss = 0.0502(0.5808)
2023/11/22 21:33:21 - INFO - root -   Epoch: [65/300][240/283], lr: 0.00000021 	 loss = 0.2056(0.5779)
2023/11/22 21:33:35 - INFO - root -   Epoch: [65/300][260/283], lr: 0.00000021 	 loss = 1.4913(0.5634)
2023/11/22 21:33:49 - INFO - root -   Epoch: [65/300][280/283], lr: 0.00000021 	 loss = 2.8660(0.5504)
2023/11/22 21:33:50 - INFO - root -   Epoch: [65/300] 	 loss = 0.5470
2023/11/22 21:33:50 - INFO - root -   train_accuracy = 0.8074
2023/11/22 21:34:00 - INFO - root -   Epoch: [66/300][0/283], lr: 0.00000022 	 loss = 0.1062(0.1062)
2023/11/22 21:34:15 - INFO - root -   Epoch: [66/300][20/283], lr: 0.00000022 	 loss = 0.9555(0.5977)
2023/11/22 21:34:29 - INFO - root -   Epoch: [66/300][40/283], lr: 0.00000022 	 loss = 0.1464(0.7121)
2023/11/22 21:34:43 - INFO - root -   Epoch: [66/300][60/283], lr: 0.00000022 	 loss = 0.2952(0.7113)
2023/11/22 21:34:51 - INFO - root -   Epoch: [66/300][80/283], lr: 0.00000022 	 loss = 0.0451(0.7096)
2023/11/22 21:35:06 - INFO - root -   Epoch: [66/300][100/283], lr: 0.00000022 	 loss = 0.0786(0.6621)
2023/11/22 21:35:22 - INFO - root -   Epoch: [66/300][120/283], lr: 0.00000022 	 loss = 0.0457(0.6218)
2023/11/22 21:35:45 - INFO - root -   Epoch: [66/300][140/283], lr: 0.00000022 	 loss = 0.3176(0.5766)
2023/11/22 21:36:00 - INFO - root -   Epoch: [66/300][160/283], lr: 0.00000022 	 loss = 0.8746(0.5625)
2023/11/22 21:36:18 - INFO - root -   Epoch: [66/300][180/283], lr: 0.00000022 	 loss = 0.5228(0.5799)
2023/11/22 21:36:30 - INFO - root -   Epoch: [66/300][200/283], lr: 0.00000022 	 loss = 0.5371(0.5675)
2023/11/22 21:36:45 - INFO - root -   Epoch: [66/300][220/283], lr: 0.00000022 	 loss = 0.2734(0.5617)
2023/11/22 21:37:02 - INFO - root -   Epoch: [66/300][240/283], lr: 0.00000022 	 loss = 0.5329(0.5602)
2023/11/22 21:37:20 - INFO - root -   Epoch: [66/300][260/283], lr: 0.00000022 	 loss = 1.4878(0.5478)
2023/11/22 21:37:33 - INFO - root -   Epoch: [66/300][280/283], lr: 0.00000022 	 loss = 2.4999(0.5451)
2023/11/22 21:37:34 - INFO - root -   Epoch: [66/300] 	 loss = 0.5419
2023/11/22 21:37:34 - INFO - root -   train_accuracy = 0.8021
2023/11/22 21:37:41 - INFO - root -   Epoch: [67/300][0/283], lr: 0.00000022 	 loss = 0.0354(0.0354)
2023/11/22 21:38:00 - INFO - root -   Epoch: [67/300][20/283], lr: 0.00000022 	 loss = 0.8798(0.7831)
2023/11/22 21:38:17 - INFO - root -   Epoch: [67/300][40/283], lr: 0.00000022 	 loss = 0.5997(0.8394)
2023/11/22 21:38:37 - INFO - root -   Epoch: [67/300][60/283], lr: 0.00000022 	 loss = 0.1242(0.7589)
2023/11/22 21:38:54 - INFO - root -   Epoch: [67/300][80/283], lr: 0.00000022 	 loss = 0.1080(0.7628)
2023/11/22 21:39:08 - INFO - root -   Epoch: [67/300][100/283], lr: 0.00000022 	 loss = 0.0803(0.6975)
2023/11/22 21:39:23 - INFO - root -   Epoch: [67/300][120/283], lr: 0.00000022 	 loss = 0.1903(0.6438)
2023/11/22 21:39:36 - INFO - root -   Epoch: [67/300][140/283], lr: 0.00000022 	 loss = 0.0728(0.5937)
2023/11/22 21:39:52 - INFO - root -   Epoch: [67/300][160/283], lr: 0.00000022 	 loss = 0.9911(0.5712)
2023/11/22 21:40:07 - INFO - root -   Epoch: [67/300][180/283], lr: 0.00000022 	 loss = 0.0759(0.5866)
2023/11/22 21:40:19 - INFO - root -   Epoch: [67/300][200/283], lr: 0.00000022 	 loss = 0.1423(0.5708)
2023/11/22 21:40:38 - INFO - root -   Epoch: [67/300][220/283], lr: 0.00000022 	 loss = 0.2988(0.5459)
2023/11/22 21:40:57 - INFO - root -   Epoch: [67/300][240/283], lr: 0.00000022 	 loss = 1.3102(0.5464)
2023/11/22 21:41:05 - INFO - root -   Epoch: [67/300][260/283], lr: 0.00000022 	 loss = 1.5565(0.5425)
2023/11/22 21:41:20 - INFO - root -   Epoch: [67/300][280/283], lr: 0.00000022 	 loss = 2.2664(0.5356)
2023/11/22 21:41:21 - INFO - root -   Epoch: [67/300] 	 loss = 0.5332
2023/11/22 21:41:21 - INFO - root -   train_accuracy = 0.7968
2023/11/22 21:41:28 - INFO - root -   Epoch: [68/300][0/283], lr: 0.00000022 	 loss = 0.2004(0.2004)
2023/11/22 21:41:44 - INFO - root -   Epoch: [68/300][20/283], lr: 0.00000022 	 loss = 1.5402(0.5668)
2023/11/22 21:41:54 - INFO - root -   Epoch: [68/300][40/283], lr: 0.00000022 	 loss = 0.2404(0.6326)
2023/11/22 21:42:13 - INFO - root -   Epoch: [68/300][60/283], lr: 0.00000022 	 loss = 0.1777(0.6707)
2023/11/22 21:42:22 - INFO - root -   Epoch: [68/300][80/283], lr: 0.00000022 	 loss = 0.2363(0.6710)
2023/11/22 21:42:43 - INFO - root -   Epoch: [68/300][100/283], lr: 0.00000022 	 loss = 0.5143(0.6374)
2023/11/22 21:43:01 - INFO - root -   Epoch: [68/300][120/283], lr: 0.00000022 	 loss = 0.2390(0.5984)
2023/11/22 21:43:14 - INFO - root -   Epoch: [68/300][140/283], lr: 0.00000022 	 loss = 0.0757(0.5581)
2023/11/22 21:43:25 - INFO - root -   Epoch: [68/300][160/283], lr: 0.00000022 	 loss = 1.8202(0.5527)
2023/11/22 21:43:43 - INFO - root -   Epoch: [68/300][180/283], lr: 0.00000022 	 loss = 0.0984(0.5535)
2023/11/22 21:44:00 - INFO - root -   Epoch: [68/300][200/283], lr: 0.00000022 	 loss = 0.0880(0.5622)
2023/11/22 21:44:13 - INFO - root -   Epoch: [68/300][220/283], lr: 0.00000022 	 loss = 0.1461(0.5435)
2023/11/22 21:44:30 - INFO - root -   Epoch: [68/300][240/283], lr: 0.00000022 	 loss = 0.5002(0.5458)
2023/11/22 21:44:43 - INFO - root -   Epoch: [68/300][260/283], lr: 0.00000022 	 loss = 1.9288(0.5351)
2023/11/22 21:45:01 - INFO - root -   Epoch: [68/300][280/283], lr: 0.00000022 	 loss = 2.4667(0.5226)
2023/11/22 21:45:01 - INFO - root -   Epoch: [68/300] 	 loss = 0.5197
2023/11/22 21:45:01 - INFO - root -   train_accuracy = 0.8180
2023/11/22 21:45:02 - INFO - root -   Epoch: [69/300][0/283], lr: 0.00000022 	 loss = 0.0783(0.0783)
2023/11/22 21:45:22 - INFO - root -   Epoch: [69/300][20/283], lr: 0.00000022 	 loss = 1.1768(0.6128)
2023/11/22 21:45:35 - INFO - root -   Epoch: [69/300][40/283], lr: 0.00000022 	 loss = 0.4080(0.7936)
2023/11/22 21:45:51 - INFO - root -   Epoch: [69/300][60/283], lr: 0.00000022 	 loss = 0.1599(0.7294)
2023/11/22 21:46:00 - INFO - root -   Epoch: [69/300][80/283], lr: 0.00000022 	 loss = 0.0901(0.6840)
2023/11/22 21:46:18 - INFO - root -   Epoch: [69/300][100/283], lr: 0.00000022 	 loss = 0.0989(0.6510)
2023/11/22 21:46:32 - INFO - root -   Epoch: [69/300][120/283], lr: 0.00000022 	 loss = 0.7005(0.6084)
2023/11/22 21:46:49 - INFO - root -   Epoch: [69/300][140/283], lr: 0.00000022 	 loss = 0.2913(0.5748)
2023/11/22 21:46:58 - INFO - root -   Epoch: [69/300][160/283], lr: 0.00000022 	 loss = 1.3753(0.5581)
2023/11/22 21:47:13 - INFO - root -   Epoch: [69/300][180/283], lr: 0.00000022 	 loss = 0.1625(0.5600)
2023/11/22 21:47:28 - INFO - root -   Epoch: [69/300][200/283], lr: 0.00000022 	 loss = 0.2712(0.5404)
2023/11/22 21:47:46 - INFO - root -   Epoch: [69/300][220/283], lr: 0.00000022 	 loss = 0.1432(0.5329)
2023/11/22 21:47:55 - INFO - root -   Epoch: [69/300][240/283], lr: 0.00000022 	 loss = 0.2408(0.5271)
2023/11/22 21:48:18 - INFO - root -   Epoch: [69/300][260/283], lr: 0.00000022 	 loss = 2.0899(0.5206)
2023/11/22 21:48:27 - INFO - root -   Epoch: [69/300][280/283], lr: 0.00000022 	 loss = 2.5992(0.5081)
2023/11/22 21:48:28 - INFO - root -   Epoch: [69/300] 	 loss = 0.5047
2023/11/22 21:51:21 - INFO - root -   precision = 0.8042
2023/11/22 21:51:21 - INFO - root -   eval_loss = 0.5091
2023/11/22 21:51:21 - INFO - root -   eval_acc = 0.8042
2023/11/22 21:51:22 - INFO - root -   train_accuracy = 0.8322
2023/11/22 21:51:23 - INFO - root -   Epoch: [70/300][0/283], lr: 0.00000022 	 loss = 0.0588(0.0588)
2023/11/22 21:51:41 - INFO - root -   Epoch: [70/300][20/283], lr: 0.00000022 	 loss = 0.7494(0.6587)
2023/11/22 21:52:00 - INFO - root -   Epoch: [70/300][40/283], lr: 0.00000022 	 loss = 0.2020(0.6566)
2023/11/22 21:52:13 - INFO - root -   Epoch: [70/300][60/283], lr: 0.00000022 	 loss = 0.2545(0.6307)
2023/11/22 21:52:27 - INFO - root -   Epoch: [70/300][80/283], lr: 0.00000022 	 loss = 0.1653(0.6322)
2023/11/22 21:52:38 - INFO - root -   Epoch: [70/300][100/283], lr: 0.00000022 	 loss = 0.6763(0.6037)
2023/11/22 21:52:54 - INFO - root -   Epoch: [70/300][120/283], lr: 0.00000022 	 loss = 0.1702(0.5552)
2023/11/22 21:53:08 - INFO - root -   Epoch: [70/300][140/283], lr: 0.00000022 	 loss = 0.3641(0.5133)
2023/11/22 21:53:24 - INFO - root -   Epoch: [70/300][160/283], lr: 0.00000022 	 loss = 1.9086(0.5247)
2023/11/22 21:53:32 - INFO - root -   Epoch: [70/300][180/283], lr: 0.00000022 	 loss = 0.0472(0.5335)
2023/11/22 21:53:49 - INFO - root -   Epoch: [70/300][200/283], lr: 0.00000022 	 loss = 0.2863(0.5219)
2023/11/22 21:54:11 - INFO - root -   Epoch: [70/300][220/283], lr: 0.00000022 	 loss = 0.1448(0.5145)
2023/11/22 21:54:23 - INFO - root -   Epoch: [70/300][240/283], lr: 0.00000022 	 loss = 1.1002(0.5173)
2023/11/22 21:54:36 - INFO - root -   Epoch: [70/300][260/283], lr: 0.00000022 	 loss = 1.5973(0.5088)
2023/11/22 21:54:47 - INFO - root -   Epoch: [70/300][280/283], lr: 0.00000022 	 loss = 3.4593(0.5108)
2023/11/22 21:54:48 - INFO - root -   Epoch: [70/300] 	 loss = 0.5075
2023/11/22 21:54:48 - INFO - root -   train_accuracy = 0.8127
2023/11/22 21:54:56 - INFO - root -   Epoch: [71/300][0/283], lr: 0.00000022 	 loss = 0.3027(0.3027)
2023/11/22 21:55:11 - INFO - root -   Epoch: [71/300][20/283], lr: 0.00000022 	 loss = 1.3181(0.6557)
2023/11/22 21:55:23 - INFO - root -   Epoch: [71/300][40/283], lr: 0.00000022 	 loss = 0.2418(0.6992)
2023/11/22 21:55:39 - INFO - root -   Epoch: [71/300][60/283], lr: 0.00000022 	 loss = 0.0841(0.6799)
2023/11/22 21:55:55 - INFO - root -   Epoch: [71/300][80/283], lr: 0.00000022 	 loss = 0.2330(0.6670)
2023/11/22 21:56:11 - INFO - root -   Epoch: [71/300][100/283], lr: 0.00000022 	 loss = 0.1801(0.6116)
2023/11/22 21:56:29 - INFO - root -   Epoch: [71/300][120/283], lr: 0.00000022 	 loss = 0.2426(0.5870)
2023/11/22 21:56:46 - INFO - root -   Epoch: [71/300][140/283], lr: 0.00000022 	 loss = 0.0359(0.5386)
2023/11/22 21:57:04 - INFO - root -   Epoch: [71/300][160/283], lr: 0.00000022 	 loss = 1.8346(0.5244)
2023/11/22 21:57:12 - INFO - root -   Epoch: [71/300][180/283], lr: 0.00000022 	 loss = 0.1940(0.5262)
2023/11/22 21:57:23 - INFO - root -   Epoch: [71/300][200/283], lr: 0.00000022 	 loss = 0.5501(0.5120)
2023/11/22 21:57:37 - INFO - root -   Epoch: [71/300][220/283], lr: 0.00000022 	 loss = 0.3898(0.4976)
2023/11/22 21:57:56 - INFO - root -   Epoch: [71/300][240/283], lr: 0.00000022 	 loss = 1.9009(0.5048)
2023/11/22 21:58:09 - INFO - root -   Epoch: [71/300][260/283], lr: 0.00000022 	 loss = 1.1917(0.4965)
2023/11/22 21:58:20 - INFO - root -   Epoch: [71/300][280/283], lr: 0.00000022 	 loss = 1.9490(0.4941)
2023/11/22 21:58:21 - INFO - root -   Epoch: [71/300] 	 loss = 0.4923
2023/11/22 21:58:21 - INFO - root -   train_accuracy = 0.8375
2023/11/22 21:58:27 - INFO - root -   Epoch: [72/300][0/283], lr: 0.00000023 	 loss = 0.1323(0.1323)
2023/11/22 21:58:49 - INFO - root -   Epoch: [72/300][20/283], lr: 0.00000023 	 loss = 0.7535(0.7940)
2023/11/22 21:58:57 - INFO - root -   Epoch: [72/300][40/283], lr: 0.00000023 	 loss = 0.2615(0.7956)
2023/11/22 21:59:22 - INFO - root -   Epoch: [72/300][60/283], lr: 0.00000023 	 loss = 0.0192(0.7922)
2023/11/22 21:59:38 - INFO - root -   Epoch: [72/300][80/283], lr: 0.00000023 	 loss = 0.0664(0.7722)
2023/11/22 21:59:53 - INFO - root -   Epoch: [72/300][100/283], lr: 0.00000023 	 loss = 0.2269(0.7223)
2023/11/22 22:00:08 - INFO - root -   Epoch: [72/300][120/283], lr: 0.00000023 	 loss = 0.1399(0.6747)
2023/11/22 22:00:26 - INFO - root -   Epoch: [72/300][140/283], lr: 0.00000023 	 loss = 0.2312(0.6172)
2023/11/22 22:00:49 - INFO - root -   Epoch: [72/300][160/283], lr: 0.00000023 	 loss = 1.7598(0.6035)
2023/11/22 22:01:08 - INFO - root -   Epoch: [72/300][180/283], lr: 0.00000023 	 loss = 0.0863(0.6046)
2023/11/22 22:01:21 - INFO - root -   Epoch: [72/300][200/283], lr: 0.00000023 	 loss = 0.1877(0.5802)
2023/11/22 22:01:31 - INFO - root -   Epoch: [72/300][220/283], lr: 0.00000023 	 loss = 0.0813(0.5613)
2023/11/22 22:01:47 - INFO - root -   Epoch: [72/300][240/283], lr: 0.00000023 	 loss = 1.2996(0.5721)
2023/11/22 22:02:07 - INFO - root -   Epoch: [72/300][260/283], lr: 0.00000023 	 loss = 1.9126(0.5599)
2023/11/22 22:02:15 - INFO - root -   Epoch: [72/300][280/283], lr: 0.00000023 	 loss = 2.7706(0.5492)
2023/11/22 22:02:16 - INFO - root -   Epoch: [72/300] 	 loss = 0.5470
2023/11/22 22:02:16 - INFO - root -   train_accuracy = 0.8233
2023/11/22 22:02:18 - INFO - root -   Epoch: [73/300][0/283], lr: 0.00000023 	 loss = 0.0661(0.0661)
2023/11/22 22:02:37 - INFO - root -   Epoch: [73/300][20/283], lr: 0.00000023 	 loss = 4.0213(0.8823)
2023/11/22 22:02:57 - INFO - root -   Epoch: [73/300][40/283], lr: 0.00000023 	 loss = 0.3648(0.8205)
2023/11/22 22:03:07 - INFO - root -   Epoch: [73/300][60/283], lr: 0.00000023 	 loss = 0.0773(0.7628)
2023/11/22 22:03:17 - INFO - root -   Epoch: [73/300][80/283], lr: 0.00000023 	 loss = 0.1031(0.7327)
2023/11/22 22:03:31 - INFO - root -   Epoch: [73/300][100/283], lr: 0.00000023 	 loss = 0.1342(0.6849)
2023/11/22 22:03:45 - INFO - root -   Epoch: [73/300][120/283], lr: 0.00000023 	 loss = 0.1953(0.6370)
2023/11/22 22:04:00 - INFO - root -   Epoch: [73/300][140/283], lr: 0.00000023 	 loss = 0.1434(0.5801)
2023/11/22 22:04:15 - INFO - root -   Epoch: [73/300][160/283], lr: 0.00000023 	 loss = 1.6832(0.5646)
2023/11/22 22:04:33 - INFO - root -   Epoch: [73/300][180/283], lr: 0.00000023 	 loss = 0.0352(0.5710)
2023/11/22 22:04:50 - INFO - root -   Epoch: [73/300][200/283], lr: 0.00000023 	 loss = 0.0631(0.5619)
2023/11/22 22:05:06 - INFO - root -   Epoch: [73/300][220/283], lr: 0.00000023 	 loss = 0.4185(0.5520)
2023/11/22 22:05:25 - INFO - root -   Epoch: [73/300][240/283], lr: 0.00000023 	 loss = 1.0079(0.5480)
2023/11/22 22:05:36 - INFO - root -   Epoch: [73/300][260/283], lr: 0.00000023 	 loss = 1.5729(0.5312)
2023/11/22 22:05:52 - INFO - root -   Epoch: [73/300][280/283], lr: 0.00000023 	 loss = 2.1572(0.5220)
2023/11/22 22:05:53 - INFO - root -   Epoch: [73/300] 	 loss = 0.5194
2023/11/22 22:05:53 - INFO - root -   train_accuracy = 0.8127
2023/11/22 22:05:54 - INFO - root -   Epoch: [74/300][0/283], lr: 0.00000023 	 loss = 0.0475(0.0475)
2023/11/22 22:06:11 - INFO - root -   Epoch: [74/300][20/283], lr: 0.00000023 	 loss = 1.6241(0.6739)
2023/11/22 22:06:27 - INFO - root -   Epoch: [74/300][40/283], lr: 0.00000023 	 loss = 0.0774(0.7621)
2023/11/22 22:06:42 - INFO - root -   Epoch: [74/300][60/283], lr: 0.00000023 	 loss = 0.1300(0.7125)
2023/11/22 22:06:51 - INFO - root -   Epoch: [74/300][80/283], lr: 0.00000023 	 loss = 0.1273(0.6848)
2023/11/22 22:07:09 - INFO - root -   Epoch: [74/300][100/283], lr: 0.00000023 	 loss = 0.1894(0.6937)
2023/11/22 22:07:36 - INFO - root -   Epoch: [74/300][120/283], lr: 0.00000023 	 loss = 0.1364(0.6761)
2023/11/22 22:07:49 - INFO - root -   Epoch: [74/300][140/283], lr: 0.00000023 	 loss = 0.1078(0.6222)
2023/11/22 22:08:06 - INFO - root -   Epoch: [74/300][160/283], lr: 0.00000023 	 loss = 2.1860(0.6089)
2023/11/22 22:08:22 - INFO - root -   Epoch: [74/300][180/283], lr: 0.00000023 	 loss = 0.0813(0.6030)
2023/11/22 22:08:37 - INFO - root -   Epoch: [74/300][200/283], lr: 0.00000023 	 loss = 0.3760(0.5906)
2023/11/22 22:08:52 - INFO - root -   Epoch: [74/300][220/283], lr: 0.00000023 	 loss = 0.2510(0.5773)
2023/11/22 22:09:07 - INFO - root -   Epoch: [74/300][240/283], lr: 0.00000023 	 loss = 0.3640(0.5766)
2023/11/22 22:09:20 - INFO - root -   Epoch: [74/300][260/283], lr: 0.00000023 	 loss = 0.6451(0.5534)
2023/11/22 22:09:29 - INFO - root -   Epoch: [74/300][280/283], lr: 0.00000023 	 loss = 2.5550(0.5387)
2023/11/22 22:09:30 - INFO - root -   Epoch: [74/300] 	 loss = 0.5353
2023/11/22 22:12:23 - INFO - root -   precision = 0.8182
2023/11/22 22:12:23 - INFO - root -   eval_loss = 0.4943
2023/11/22 22:12:23 - INFO - root -   eval_acc = 0.8182
2023/11/22 22:12:24 - INFO - root -   train_accuracy = 0.8180
2023/11/22 22:12:33 - INFO - root -   Epoch: [75/300][0/283], lr: 0.00000023 	 loss = 0.1573(0.1573)
2023/11/22 22:12:53 - INFO - root -   Epoch: [75/300][20/283], lr: 0.00000023 	 loss = 1.2839(0.6522)
2023/11/22 22:13:08 - INFO - root -   Epoch: [75/300][40/283], lr: 0.00000023 	 loss = 0.2278(0.7770)
2023/11/22 22:13:20 - INFO - root -   Epoch: [75/300][60/283], lr: 0.00000023 	 loss = 0.0153(0.6931)
2023/11/22 22:13:38 - INFO - root -   Epoch: [75/300][80/283], lr: 0.00000023 	 loss = 0.1673(0.6715)
2023/11/22 22:13:49 - INFO - root -   Epoch: [75/300][100/283], lr: 0.00000023 	 loss = 0.2138(0.6196)
2023/11/22 22:14:12 - INFO - root -   Epoch: [75/300][120/283], lr: 0.00000023 	 loss = 0.6155(0.5801)
2023/11/22 22:14:26 - INFO - root -   Epoch: [75/300][140/283], lr: 0.00000023 	 loss = 0.0569(0.5574)
2023/11/22 22:14:38 - INFO - root -   Epoch: [75/300][160/283], lr: 0.00000023 	 loss = 1.4219(0.5571)
2023/11/22 22:14:53 - INFO - root -   Epoch: [75/300][180/283], lr: 0.00000023 	 loss = 0.0867(0.5625)
2023/11/22 22:15:11 - INFO - root -   Epoch: [75/300][200/283], lr: 0.00000023 	 loss = 0.0907(0.5476)
2023/11/22 22:15:22 - INFO - root -   Epoch: [75/300][220/283], lr: 0.00000023 	 loss = 0.0759(0.5372)
2023/11/22 22:15:37 - INFO - root -   Epoch: [75/300][240/283], lr: 0.00000023 	 loss = 1.1831(0.5372)
2023/11/22 22:15:53 - INFO - root -   Epoch: [75/300][260/283], lr: 0.00000023 	 loss = 1.5801(0.5339)
2023/11/22 22:16:01 - INFO - root -   Epoch: [75/300][280/283], lr: 0.00000023 	 loss = 2.6721(0.5258)
2023/11/22 22:16:02 - INFO - root -   Epoch: [75/300] 	 loss = 0.5229
2023/11/22 22:16:02 - INFO - root -   train_accuracy = 0.8198
2023/11/22 22:16:03 - INFO - root -   Epoch: [76/300][0/283], lr: 0.00000023 	 loss = 0.0787(0.0787)
2023/11/22 22:16:22 - INFO - root -   Epoch: [76/300][20/283], lr: 0.00000023 	 loss = 0.8824(0.5977)
2023/11/22 22:16:39 - INFO - root -   Epoch: [76/300][40/283], lr: 0.00000023 	 loss = 0.1845(0.6872)
2023/11/22 22:17:01 - INFO - root -   Epoch: [76/300][60/283], lr: 0.00000023 	 loss = 0.0397(0.6785)
2023/11/22 22:17:21 - INFO - root -   Epoch: [76/300][80/283], lr: 0.00000023 	 loss = 0.0994(0.7129)
2023/11/22 22:17:37 - INFO - root -   Epoch: [76/300][100/283], lr: 0.00000023 	 loss = 0.1926(0.6424)
2023/11/22 22:17:45 - INFO - root -   Epoch: [76/300][120/283], lr: 0.00000023 	 loss = 0.1101(0.6048)
2023/11/22 22:18:05 - INFO - root -   Epoch: [76/300][140/283], lr: 0.00000023 	 loss = 0.0859(0.5640)
2023/11/22 22:18:21 - INFO - root -   Epoch: [76/300][160/283], lr: 0.00000023 	 loss = 2.5015(0.5556)
2023/11/22 22:18:55 - INFO - root -   Epoch: [76/300][180/283], lr: 0.00000023 	 loss = 0.1722(0.5729)
2023/11/22 22:19:04 - INFO - root -   Epoch: [76/300][200/283], lr: 0.00000023 	 loss = 0.2559(0.5518)
2023/11/22 22:19:19 - INFO - root -   Epoch: [76/300][220/283], lr: 0.00000023 	 loss = 0.0705(0.5286)
2023/11/22 22:19:31 - INFO - root -   Epoch: [76/300][240/283], lr: 0.00000023 	 loss = 1.2533(0.5415)
2023/11/22 22:19:54 - INFO - root -   Epoch: [76/300][260/283], lr: 0.00000023 	 loss = 1.5999(0.5336)
2023/11/22 22:20:02 - INFO - root -   Epoch: [76/300][280/283], lr: 0.00000023 	 loss = 1.7438(0.5145)
2023/11/22 22:20:03 - INFO - root -   Epoch: [76/300] 	 loss = 0.5112
2023/11/22 22:20:03 - INFO - root -   train_accuracy = 0.8375
2023/11/22 22:20:18 - INFO - root -   Epoch: [77/300][0/283], lr: 0.00000023 	 loss = 0.1561(0.1561)
2023/11/22 22:20:29 - INFO - root -   Epoch: [77/300][20/283], lr: 0.00000023 	 loss = 1.6259(0.9841)
2023/11/22 22:20:44 - INFO - root -   Epoch: [77/300][40/283], lr: 0.00000023 	 loss = 0.1869(0.8526)
2023/11/22 22:21:00 - INFO - root -   Epoch: [77/300][60/283], lr: 0.00000023 	 loss = 0.0895(0.7323)
2023/11/22 22:21:16 - INFO - root -   Epoch: [77/300][80/283], lr: 0.00000023 	 loss = 0.0832(0.7149)
2023/11/22 22:21:35 - INFO - root -   Epoch: [77/300][100/283], lr: 0.00000023 	 loss = 1.3162(0.6660)
2023/11/22 22:21:56 - INFO - root -   Epoch: [77/300][120/283], lr: 0.00000023 	 loss = 0.2521(0.6160)
2023/11/22 22:22:14 - INFO - root -   Epoch: [77/300][140/283], lr: 0.00000023 	 loss = 0.2960(0.5690)
2023/11/22 22:22:30 - INFO - root -   Epoch: [77/300][160/283], lr: 0.00000023 	 loss = 1.3143(0.5626)
2023/11/22 22:22:51 - INFO - root -   Epoch: [77/300][180/283], lr: 0.00000023 	 loss = 0.2198(0.5680)
2023/11/22 22:23:06 - INFO - root -   Epoch: [77/300][200/283], lr: 0.00000023 	 loss = 0.1208(0.5527)
2023/11/22 22:23:18 - INFO - root -   Epoch: [77/300][220/283], lr: 0.00000023 	 loss = 0.1368(0.5329)
2023/11/22 22:23:35 - INFO - root -   Epoch: [77/300][240/283], lr: 0.00000023 	 loss = 0.9342(0.5467)
2023/11/22 22:23:50 - INFO - root -   Epoch: [77/300][260/283], lr: 0.00000023 	 loss = 0.7270(0.5400)
2023/11/22 22:24:06 - INFO - root -   Epoch: [77/300][280/283], lr: 0.00000023 	 loss = 2.5952(0.5324)
2023/11/22 22:24:07 - INFO - root -   Epoch: [77/300] 	 loss = 0.5293
2023/11/22 22:24:07 - INFO - root -   train_accuracy = 0.8110
2023/11/22 22:24:14 - INFO - root -   Epoch: [78/300][0/283], lr: 0.00000024 	 loss = 0.0489(0.0489)
2023/11/22 22:24:34 - INFO - root -   Epoch: [78/300][20/283], lr: 0.00000024 	 loss = 0.9401(0.7210)
2023/11/22 22:24:47 - INFO - root -   Epoch: [78/300][40/283], lr: 0.00000024 	 loss = 0.3388(0.7406)
2023/11/22 22:25:04 - INFO - root -   Epoch: [78/300][60/283], lr: 0.00000024 	 loss = 0.1554(0.6925)
2023/11/22 22:25:15 - INFO - root -   Epoch: [78/300][80/283], lr: 0.00000024 	 loss = 0.1288(0.7045)
2023/11/22 22:25:26 - INFO - root -   Epoch: [78/300][100/283], lr: 0.00000024 	 loss = 0.4335(0.6536)
2023/11/22 22:25:45 - INFO - root -   Epoch: [78/300][120/283], lr: 0.00000024 	 loss = 0.0755(0.6186)
2023/11/22 22:26:05 - INFO - root -   Epoch: [78/300][140/283], lr: 0.00000024 	 loss = 0.0742(0.5869)
2023/11/22 22:26:25 - INFO - root -   Epoch: [78/300][160/283], lr: 0.00000024 	 loss = 1.1403(0.5963)
2023/11/22 22:26:43 - INFO - root -   Epoch: [78/300][180/283], lr: 0.00000024 	 loss = 0.4661(0.6047)
2023/11/22 22:26:52 - INFO - root -   Epoch: [78/300][200/283], lr: 0.00000024 	 loss = 0.2331(0.5893)
2023/11/22 22:27:06 - INFO - root -   Epoch: [78/300][220/283], lr: 0.00000024 	 loss = 0.2816(0.5612)
2023/11/22 22:27:24 - INFO - root -   Epoch: [78/300][240/283], lr: 0.00000024 	 loss = 2.4296(0.5725)
2023/11/22 22:27:43 - INFO - root -   Epoch: [78/300][260/283], lr: 0.00000024 	 loss = 2.1633(0.5688)
2023/11/22 22:27:57 - INFO - root -   Epoch: [78/300][280/283], lr: 0.00000024 	 loss = 2.9955(0.5640)
2023/11/22 22:27:57 - INFO - root -   Epoch: [78/300] 	 loss = 0.5603
2023/11/22 22:27:57 - INFO - root -   train_accuracy = 0.8127
2023/11/22 22:28:08 - INFO - root -   Epoch: [79/300][0/283], lr: 0.00000024 	 loss = 0.3750(0.3750)
2023/11/22 22:28:18 - INFO - root -   Epoch: [79/300][20/283], lr: 0.00000024 	 loss = 0.7270(0.5910)
2023/11/22 22:28:36 - INFO - root -   Epoch: [79/300][40/283], lr: 0.00000024 	 loss = 0.7497(0.7540)
2023/11/22 22:28:53 - INFO - root -   Epoch: [79/300][60/283], lr: 0.00000024 	 loss = 0.0530(0.6830)
2023/11/22 22:29:08 - INFO - root -   Epoch: [79/300][80/283], lr: 0.00000024 	 loss = 0.1921(0.6986)
2023/11/22 22:29:20 - INFO - root -   Epoch: [79/300][100/283], lr: 0.00000024 	 loss = 0.7441(0.6696)
2023/11/22 22:29:32 - INFO - root -   Epoch: [79/300][120/283], lr: 0.00000024 	 loss = 0.0712(0.6230)
2023/11/22 22:29:46 - INFO - root -   Epoch: [79/300][140/283], lr: 0.00000024 	 loss = 0.2175(0.5755)
2023/11/22 22:29:58 - INFO - root -   Epoch: [79/300][160/283], lr: 0.00000024 	 loss = 0.9534(0.5504)
2023/11/22 22:30:08 - INFO - root -   Epoch: [79/300][180/283], lr: 0.00000024 	 loss = 0.1170(0.5517)
2023/11/22 22:30:23 - INFO - root -   Epoch: [79/300][200/283], lr: 0.00000024 	 loss = 0.0715(0.5355)
2023/11/22 22:30:38 - INFO - root -   Epoch: [79/300][220/283], lr: 0.00000024 	 loss = 0.0363(0.5175)
2023/11/22 22:30:55 - INFO - root -   Epoch: [79/300][240/283], lr: 0.00000024 	 loss = 0.8057(0.5195)
2023/11/22 22:31:11 - INFO - root -   Epoch: [79/300][260/283], lr: 0.00000024 	 loss = 2.2194(0.5139)
2023/11/22 22:31:20 - INFO - root -   Epoch: [79/300][280/283], lr: 0.00000024 	 loss = 3.2412(0.5124)
2023/11/22 22:31:21 - INFO - root -   Epoch: [79/300] 	 loss = 0.5092
2023/11/22 22:34:15 - INFO - root -   precision = 0.8182
2023/11/22 22:34:15 - INFO - root -   eval_loss = 0.4875
2023/11/22 22:34:15 - INFO - root -   eval_acc = 0.8182
2023/11/22 22:34:17 - INFO - root -   train_accuracy = 0.8110
2023/11/22 22:34:18 - INFO - root -   Epoch: [80/300][0/283], lr: 0.00000024 	 loss = 0.0406(0.0406)
2023/11/22 22:34:42 - INFO - root -   Epoch: [80/300][20/283], lr: 0.00000024 	 loss = 0.6570(0.6398)
2023/11/22 22:34:52 - INFO - root -   Epoch: [80/300][40/283], lr: 0.00000024 	 loss = 0.2104(0.6862)
2023/11/22 22:35:04 - INFO - root -   Epoch: [80/300][60/283], lr: 0.00000024 	 loss = 0.0625(0.6587)
2023/11/22 22:35:24 - INFO - root -   Epoch: [80/300][80/283], lr: 0.00000024 	 loss = 0.0447(0.6727)
2023/11/22 22:35:45 - INFO - root -   Epoch: [80/300][100/283], lr: 0.00000024 	 loss = 0.6240(0.6409)
2023/11/22 22:36:04 - INFO - root -   Epoch: [80/300][120/283], lr: 0.00000024 	 loss = 0.2191(0.6029)
2023/11/22 22:36:18 - INFO - root -   Epoch: [80/300][140/283], lr: 0.00000024 	 loss = 0.0596(0.5579)
2023/11/22 22:36:38 - INFO - root -   Epoch: [80/300][160/283], lr: 0.00000024 	 loss = 1.6880(0.5637)
2023/11/22 22:36:50 - INFO - root -   Epoch: [80/300][180/283], lr: 0.00000024 	 loss = 0.0566(0.5782)
2023/11/22 22:37:03 - INFO - root -   Epoch: [80/300][200/283], lr: 0.00000024 	 loss = 0.0551(0.5700)
2023/11/22 22:37:24 - INFO - root -   Epoch: [80/300][220/283], lr: 0.00000024 	 loss = 0.1002(0.5524)
2023/11/22 22:37:38 - INFO - root -   Epoch: [80/300][240/283], lr: 0.00000024 	 loss = 1.3341(0.5503)
2023/11/22 22:38:00 - INFO - root -   Epoch: [80/300][260/283], lr: 0.00000024 	 loss = 1.9918(0.5498)
2023/11/22 22:38:11 - INFO - root -   Epoch: [80/300][280/283], lr: 0.00000024 	 loss = 3.1709(0.5366)
2023/11/22 22:38:12 - INFO - root -   Epoch: [80/300] 	 loss = 0.5341
2023/11/22 22:38:12 - INFO - root -   train_accuracy = 0.8145
2023/11/22 22:38:13 - INFO - root -   Epoch: [81/300][0/283], lr: 0.00000024 	 loss = 0.0672(0.0672)
2023/11/22 22:38:34 - INFO - root -   Epoch: [81/300][20/283], lr: 0.00000024 	 loss = 2.0481(0.6196)
2023/11/22 22:38:53 - INFO - root -   Epoch: [81/300][40/283], lr: 0.00000024 	 loss = 1.0069(0.7145)
2023/11/22 22:39:03 - INFO - root -   Epoch: [81/300][60/283], lr: 0.00000024 	 loss = 0.1620(0.6810)
2023/11/22 22:39:17 - INFO - root -   Epoch: [81/300][80/283], lr: 0.00000024 	 loss = 0.1432(0.6650)
2023/11/22 22:39:29 - INFO - root -   Epoch: [81/300][100/283], lr: 0.00000024 	 loss = 0.1662(0.6253)
2023/11/22 22:39:47 - INFO - root -   Epoch: [81/300][120/283], lr: 0.00000024 	 loss = 0.2155(0.5880)
2023/11/22 22:40:00 - INFO - root -   Epoch: [81/300][140/283], lr: 0.00000024 	 loss = 0.0664(0.5536)
2023/11/22 22:40:25 - INFO - root -   Epoch: [81/300][160/283], lr: 0.00000024 	 loss = 1.1845(0.5511)
2023/11/22 22:40:38 - INFO - root -   Epoch: [81/300][180/283], lr: 0.00000024 	 loss = 0.6944(0.5548)
2023/11/22 22:40:54 - INFO - root -   Epoch: [81/300][200/283], lr: 0.00000024 	 loss = 0.1763(0.5454)
2023/11/22 22:41:02 - INFO - root -   Epoch: [81/300][220/283], lr: 0.00000024 	 loss = 0.0829(0.5432)
2023/11/22 22:41:14 - INFO - root -   Epoch: [81/300][240/283], lr: 0.00000024 	 loss = 1.3937(0.5399)
2023/11/22 22:41:26 - INFO - root -   Epoch: [81/300][260/283], lr: 0.00000024 	 loss = 2.3366(0.5322)
2023/11/22 22:41:38 - INFO - root -   Epoch: [81/300][280/283], lr: 0.00000024 	 loss = 2.5177(0.5236)
2023/11/22 22:41:39 - INFO - root -   Epoch: [81/300] 	 loss = 0.5214
2023/11/22 22:41:39 - INFO - root -   train_accuracy = 0.8216
2023/11/22 22:41:40 - INFO - root -   Epoch: [82/300][0/283], lr: 0.00000024 	 loss = 0.0602(0.0602)
2023/11/22 22:42:05 - INFO - root -   Epoch: [82/300][20/283], lr: 0.00000024 	 loss = 1.5155(0.6633)
2023/11/22 22:42:19 - INFO - root -   Epoch: [82/300][40/283], lr: 0.00000024 	 loss = 0.4137(0.7507)
2023/11/22 22:42:36 - INFO - root -   Epoch: [82/300][60/283], lr: 0.00000024 	 loss = 0.1129(0.6840)
2023/11/22 22:42:49 - INFO - root -   Epoch: [82/300][80/283], lr: 0.00000024 	 loss = 0.2486(0.7074)
2023/11/22 22:43:06 - INFO - root -   Epoch: [82/300][100/283], lr: 0.00000024 	 loss = 0.4703(0.6675)
2023/11/22 22:43:25 - INFO - root -   Epoch: [82/300][120/283], lr: 0.00000024 	 loss = 0.0876(0.6257)
2023/11/22 22:43:37 - INFO - root -   Epoch: [82/300][140/283], lr: 0.00000024 	 loss = 0.0724(0.5835)
2023/11/22 22:43:49 - INFO - root -   Epoch: [82/300][160/283], lr: 0.00000024 	 loss = 1.4075(0.5794)
2023/11/22 22:44:09 - INFO - root -   Epoch: [82/300][180/283], lr: 0.00000024 	 loss = 0.0702(0.5747)
2023/11/22 22:44:26 - INFO - root -   Epoch: [82/300][200/283], lr: 0.00000024 	 loss = 0.3350(0.5613)
2023/11/22 22:44:41 - INFO - root -   Epoch: [82/300][220/283], lr: 0.00000024 	 loss = 0.1371(0.5422)
2023/11/22 22:44:49 - INFO - root -   Epoch: [82/300][240/283], lr: 0.00000024 	 loss = 0.9386(0.5411)
2023/11/22 22:45:04 - INFO - root -   Epoch: [82/300][260/283], lr: 0.00000024 	 loss = 2.0139(0.5322)
2023/11/22 22:45:15 - INFO - root -   Epoch: [82/300][280/283], lr: 0.00000024 	 loss = 1.0464(0.5156)
2023/11/22 22:45:16 - INFO - root -   Epoch: [82/300] 	 loss = 0.5127
2023/11/22 22:45:16 - INFO - root -   train_accuracy = 0.8110
2023/11/22 22:45:28 - INFO - root -   Epoch: [83/300][0/283], lr: 0.00000025 	 loss = 0.1305(0.1305)
2023/11/22 22:45:42 - INFO - root -   Epoch: [83/300][20/283], lr: 0.00000025 	 loss = 1.0650(0.6737)
2023/11/22 22:45:59 - INFO - root -   Epoch: [83/300][40/283], lr: 0.00000025 	 loss = 0.9695(0.7165)
2023/11/22 22:46:08 - INFO - root -   Epoch: [83/300][60/283], lr: 0.00000025 	 loss = 0.0961(0.6724)
2023/11/22 22:46:20 - INFO - root -   Epoch: [83/300][80/283], lr: 0.00000025 	 loss = 0.1083(0.6487)
2023/11/22 22:46:34 - INFO - root -   Epoch: [83/300][100/283], lr: 0.00000025 	 loss = 0.1758(0.6085)
2023/11/22 22:46:42 - INFO - root -   Epoch: [83/300][120/283], lr: 0.00000025 	 loss = 0.2089(0.5831)
2023/11/22 22:47:04 - INFO - root -   Epoch: [83/300][140/283], lr: 0.00000025 	 loss = 0.1647(0.5562)
2023/11/22 22:47:19 - INFO - root -   Epoch: [83/300][160/283], lr: 0.00000025 	 loss = 1.2126(0.5490)
2023/11/22 22:47:40 - INFO - root -   Epoch: [83/300][180/283], lr: 0.00000025 	 loss = 0.2742(0.5513)
2023/11/22 22:47:48 - INFO - root -   Epoch: [83/300][200/283], lr: 0.00000025 	 loss = 0.1768(0.5320)
2023/11/22 22:48:06 - INFO - root -   Epoch: [83/300][220/283], lr: 0.00000025 	 loss = 0.0685(0.5272)
2023/11/22 22:48:21 - INFO - root -   Epoch: [83/300][240/283], lr: 0.00000025 	 loss = 1.0701(0.5316)
2023/11/22 22:48:37 - INFO - root -   Epoch: [83/300][260/283], lr: 0.00000025 	 loss = 1.3098(0.5222)
2023/11/22 22:48:49 - INFO - root -   Epoch: [83/300][280/283], lr: 0.00000025 	 loss = 1.6508(0.5015)
2023/11/22 22:48:49 - INFO - root -   Epoch: [83/300] 	 loss = 0.4986
2023/11/22 22:48:49 - INFO - root -   train_accuracy = 0.8163
2023/11/22 22:48:57 - INFO - root -   Epoch: [84/300][0/283], lr: 0.00000025 	 loss = 0.1523(0.1523)
2023/11/22 22:49:13 - INFO - root -   Epoch: [84/300][20/283], lr: 0.00000025 	 loss = 0.9245(0.7143)
2023/11/22 22:49:31 - INFO - root -   Epoch: [84/300][40/283], lr: 0.00000025 	 loss = 0.2036(0.7721)
2023/11/22 22:49:47 - INFO - root -   Epoch: [84/300][60/283], lr: 0.00000025 	 loss = 0.0587(0.7147)
2023/11/22 22:50:07 - INFO - root -   Epoch: [84/300][80/283], lr: 0.00000025 	 loss = 0.0800(0.7272)
2023/11/22 22:50:22 - INFO - root -   Epoch: [84/300][100/283], lr: 0.00000025 	 loss = 0.2960(0.6917)
2023/11/22 22:50:43 - INFO - root -   Epoch: [84/300][120/283], lr: 0.00000025 	 loss = 0.2639(0.6395)
2023/11/22 22:50:57 - INFO - root -   Epoch: [84/300][140/283], lr: 0.00000025 	 loss = 0.0673(0.5962)
2023/11/22 22:51:09 - INFO - root -   Epoch: [84/300][160/283], lr: 0.00000025 	 loss = 1.2163(0.5725)
2023/11/22 22:51:24 - INFO - root -   Epoch: [84/300][180/283], lr: 0.00000025 	 loss = 0.0949(0.5853)
2023/11/22 22:51:45 - INFO - root -   Epoch: [84/300][200/283], lr: 0.00000025 	 loss = 0.0989(0.5650)
2023/11/22 22:52:00 - INFO - root -   Epoch: [84/300][220/283], lr: 0.00000025 	 loss = 0.2923(0.5593)
2023/11/22 22:52:13 - INFO - root -   Epoch: [84/300][240/283], lr: 0.00000025 	 loss = 1.4680(0.5617)
2023/11/22 22:52:37 - INFO - root -   Epoch: [84/300][260/283], lr: 0.00000025 	 loss = 1.3325(0.5539)
2023/11/22 22:52:50 - INFO - root -   Epoch: [84/300][280/283], lr: 0.00000025 	 loss = 2.4241(0.5433)
2023/11/22 22:52:51 - INFO - root -   Epoch: [84/300] 	 loss = 0.5398
2023/11/22 22:55:47 - INFO - root -   precision = 0.7902
2023/11/22 22:55:47 - INFO - root -   eval_loss = 0.4947
2023/11/22 22:55:47 - INFO - root -   eval_acc = 0.7902
2023/11/22 22:55:48 - INFO - root -   train_accuracy = 0.8127
2023/11/22 22:55:49 - INFO - root -   Epoch: [85/300][0/283], lr: 0.00000025 	 loss = 0.0582(0.0582)
2023/11/22 22:56:09 - INFO - root -   Epoch: [85/300][20/283], lr: 0.00000025 	 loss = 1.3658(0.8101)
2023/11/22 22:56:26 - INFO - root -   Epoch: [85/300][40/283], lr: 0.00000025 	 loss = 0.1868(0.8087)
2023/11/22 22:56:41 - INFO - root -   Epoch: [85/300][60/283], lr: 0.00000025 	 loss = 0.0387(0.8047)
2023/11/22 22:57:02 - INFO - root -   Epoch: [85/300][80/283], lr: 0.00000025 	 loss = 0.0147(0.7564)
2023/11/22 22:57:13 - INFO - root -   Epoch: [85/300][100/283], lr: 0.00000025 	 loss = 1.0246(0.7231)
2023/11/22 22:57:35 - INFO - root -   Epoch: [85/300][120/283], lr: 0.00000025 	 loss = 0.0807(0.6570)
2023/11/22 22:57:47 - INFO - root -   Epoch: [85/300][140/283], lr: 0.00000025 	 loss = 0.1543(0.6113)
2023/11/22 22:58:11 - INFO - root -   Epoch: [85/300][160/283], lr: 0.00000025 	 loss = 1.9997(0.6019)
2023/11/22 22:58:19 - INFO - root -   Epoch: [85/300][180/283], lr: 0.00000025 	 loss = 0.0185(0.5978)
2023/11/22 22:58:28 - INFO - root -   Epoch: [85/300][200/283], lr: 0.00000025 	 loss = 0.1351(0.5712)
2023/11/22 22:58:46 - INFO - root -   Epoch: [85/300][220/283], lr: 0.00000025 	 loss = 0.0877(0.5516)
2023/11/22 22:59:08 - INFO - root -   Epoch: [85/300][240/283], lr: 0.00000025 	 loss = 0.8550(0.5528)
2023/11/22 22:59:26 - INFO - root -   Epoch: [85/300][260/283], lr: 0.00000025 	 loss = 2.3533(0.5489)
2023/11/22 22:59:40 - INFO - root -   Epoch: [85/300][280/283], lr: 0.00000025 	 loss = 2.6575(0.5453)
2023/11/22 22:59:41 - INFO - root -   Epoch: [85/300] 	 loss = 0.5419
2023/11/22 22:59:41 - INFO - root -   train_accuracy = 0.7968
2023/11/22 22:59:55 - INFO - root -   Epoch: [86/300][0/283], lr: 0.00000025 	 loss = 0.2889(0.2889)
2023/11/22 23:00:10 - INFO - root -   Epoch: [86/300][20/283], lr: 0.00000025 	 loss = 3.3798(0.9291)
2023/11/22 23:00:19 - INFO - root -   Epoch: [86/300][40/283], lr: 0.00000025 	 loss = 0.2792(0.8059)
2023/11/22 23:00:36 - INFO - root -   Epoch: [86/300][60/283], lr: 0.00000025 	 loss = 0.2266(0.7057)
2023/11/22 23:00:44 - INFO - root -   Epoch: [86/300][80/283], lr: 0.00000025 	 loss = 0.1359(0.6958)
2023/11/22 23:01:10 - INFO - root -   Epoch: [86/300][100/283], lr: 0.00000025 	 loss = 0.2914(0.6605)
2023/11/22 23:01:23 - INFO - root -   Epoch: [86/300][120/283], lr: 0.00000025 	 loss = 0.0723(0.6152)
2023/11/22 23:01:37 - INFO - root -   Epoch: [86/300][140/283], lr: 0.00000025 	 loss = 0.1154(0.5651)
2023/11/22 23:01:49 - INFO - root -   Epoch: [86/300][160/283], lr: 0.00000025 	 loss = 0.9218(0.5564)
2023/11/22 23:02:05 - INFO - root -   Epoch: [86/300][180/283], lr: 0.00000025 	 loss = 0.1661(0.5576)
2023/11/22 23:02:17 - INFO - root -   Epoch: [86/300][200/283], lr: 0.00000025 	 loss = 0.0801(0.5607)
2023/11/22 23:02:30 - INFO - root -   Epoch: [86/300][220/283], lr: 0.00000025 	 loss = 0.2010(0.5481)
2023/11/22 23:02:51 - INFO - root -   Epoch: [86/300][240/283], lr: 0.00000025 	 loss = 0.9957(0.5423)
2023/11/22 23:03:04 - INFO - root -   Epoch: [86/300][260/283], lr: 0.00000025 	 loss = 0.8882(0.5214)
2023/11/22 23:03:16 - INFO - root -   Epoch: [86/300][280/283], lr: 0.00000025 	 loss = 2.9788(0.5091)
2023/11/22 23:03:17 - INFO - root -   Epoch: [86/300] 	 loss = 0.5061
2023/11/22 23:03:17 - INFO - root -   train_accuracy = 0.8375
2023/11/22 23:03:18 - INFO - root -   Epoch: [87/300][0/283], lr: 0.00000025 	 loss = 0.1137(0.1137)
2023/11/22 23:03:39 - INFO - root -   Epoch: [87/300][20/283], lr: 0.00000025 	 loss = 1.3955(0.7272)
2023/11/22 23:03:48 - INFO - root -   Epoch: [87/300][40/283], lr: 0.00000025 	 loss = 0.5416(0.7652)
2023/11/22 23:04:09 - INFO - root -   Epoch: [87/300][60/283], lr: 0.00000025 	 loss = 0.0344(0.7336)
2023/11/22 23:04:29 - INFO - root -   Epoch: [87/300][80/283], lr: 0.00000025 	 loss = 0.2512(0.7464)
2023/11/22 23:04:37 - INFO - root -   Epoch: [87/300][100/283], lr: 0.00000025 	 loss = 0.4596(0.6999)
2023/11/22 23:04:45 - INFO - root -   Epoch: [87/300][120/283], lr: 0.00000025 	 loss = 0.2590(0.6419)
2023/11/22 23:04:57 - INFO - root -   Epoch: [87/300][140/283], lr: 0.00000025 	 loss = 0.0567(0.6016)
2023/11/22 23:05:18 - INFO - root -   Epoch: [87/300][160/283], lr: 0.00000025 	 loss = 1.3975(0.5900)
2023/11/22 23:05:32 - INFO - root -   Epoch: [87/300][180/283], lr: 0.00000025 	 loss = 0.0675(0.5998)
2023/11/22 23:05:52 - INFO - root -   Epoch: [87/300][200/283], lr: 0.00000025 	 loss = 0.3871(0.5941)
2023/11/22 23:06:03 - INFO - root -   Epoch: [87/300][220/283], lr: 0.00000025 	 loss = 0.1032(0.5785)
2023/11/22 23:06:21 - INFO - root -   Epoch: [87/300][240/283], lr: 0.00000025 	 loss = 0.6801(0.5737)
2023/11/22 23:06:34 - INFO - root -   Epoch: [87/300][260/283], lr: 0.00000025 	 loss = 1.2339(0.5586)
2023/11/22 23:06:45 - INFO - root -   Epoch: [87/300][280/283], lr: 0.00000025 	 loss = 2.7624(0.5444)
2023/11/22 23:06:46 - INFO - root -   Epoch: [87/300] 	 loss = 0.5417
2023/11/22 23:06:46 - INFO - root -   train_accuracy = 0.7968
2023/11/22 23:06:59 - INFO - root -   Epoch: [88/300][0/283], lr: 0.00000025 	 loss = 0.1186(0.1186)
2023/11/22 23:07:09 - INFO - root -   Epoch: [88/300][20/283], lr: 0.00000025 	 loss = 2.5876(0.7497)
2023/11/22 23:07:30 - INFO - root -   Epoch: [88/300][40/283], lr: 0.00000025 	 loss = 0.3105(0.7305)
2023/11/22 23:07:38 - INFO - root -   Epoch: [88/300][60/283], lr: 0.00000025 	 loss = 0.0515(0.6649)
2023/11/22 23:07:54 - INFO - root -   Epoch: [88/300][80/283], lr: 0.00000025 	 loss = 0.0796(0.6799)
2023/11/22 23:08:10 - INFO - root -   Epoch: [88/300][100/283], lr: 0.00000025 	 loss = 0.1760(0.6311)
2023/11/22 23:08:31 - INFO - root -   Epoch: [88/300][120/283], lr: 0.00000025 	 loss = 0.3965(0.5789)
2023/11/22 23:08:42 - INFO - root -   Epoch: [88/300][140/283], lr: 0.00000025 	 loss = 0.0363(0.5349)
2023/11/22 23:09:04 - INFO - root -   Epoch: [88/300][160/283], lr: 0.00000025 	 loss = 1.2166(0.5322)
2023/11/22 23:09:18 - INFO - root -   Epoch: [88/300][180/283], lr: 0.00000025 	 loss = 0.1737(0.5462)
2023/11/22 23:09:29 - INFO - root -   Epoch: [88/300][200/283], lr: 0.00000025 	 loss = 0.0717(0.5362)
2023/11/22 23:09:52 - INFO - root -   Epoch: [88/300][220/283], lr: 0.00000025 	 loss = 0.4583(0.5202)
2023/11/22 23:10:05 - INFO - root -   Epoch: [88/300][240/283], lr: 0.00000025 	 loss = 0.0722(0.5167)
2023/11/22 23:10:23 - INFO - root -   Epoch: [88/300][260/283], lr: 0.00000025 	 loss = 1.3248(0.5087)
2023/11/22 23:10:32 - INFO - root -   Epoch: [88/300][280/283], lr: 0.00000025 	 loss = 1.6674(0.5034)
2023/11/22 23:10:33 - INFO - root -   Epoch: [88/300] 	 loss = 0.5004
2023/11/22 23:10:33 - INFO - root -   train_accuracy = 0.8269
2023/11/22 23:10:47 - INFO - root -   Epoch: [89/300][0/283], lr: 0.00000026 	 loss = 0.7172(0.7172)
2023/11/22 23:10:55 - INFO - root -   Epoch: [89/300][20/283], lr: 0.00000026 	 loss = 0.4494(0.6142)
2023/11/22 23:11:07 - INFO - root -   Epoch: [89/300][40/283], lr: 0.00000026 	 loss = 0.2917(0.7187)
2023/11/22 23:11:23 - INFO - root -   Epoch: [89/300][60/283], lr: 0.00000026 	 loss = 0.1487(0.7016)
2023/11/22 23:11:43 - INFO - root -   Epoch: [89/300][80/283], lr: 0.00000026 	 loss = 0.1117(0.7072)
2023/11/22 23:11:57 - INFO - root -   Epoch: [89/300][100/283], lr: 0.00000026 	 loss = 0.0839(0.6452)
2023/11/22 23:12:21 - INFO - root -   Epoch: [89/300][120/283], lr: 0.00000026 	 loss = 0.8255(0.6050)
2023/11/22 23:12:34 - INFO - root -   Epoch: [89/300][140/283], lr: 0.00000026 	 loss = 0.0469(0.5591)
2023/11/22 23:12:43 - INFO - root -   Epoch: [89/300][160/283], lr: 0.00000026 	 loss = 1.5848(0.5424)
2023/11/22 23:12:53 - INFO - root -   Epoch: [89/300][180/283], lr: 0.00000026 	 loss = 0.1077(0.5670)
2023/11/22 23:13:14 - INFO - root -   Epoch: [89/300][200/283], lr: 0.00000026 	 loss = 0.5095(0.5642)
2023/11/22 23:13:30 - INFO - root -   Epoch: [89/300][220/283], lr: 0.00000026 	 loss = 0.3638(0.5488)
2023/11/22 23:13:52 - INFO - root -   Epoch: [89/300][240/283], lr: 0.00000026 	 loss = 1.6401(0.5562)
2023/11/22 23:14:08 - INFO - root -   Epoch: [89/300][260/283], lr: 0.00000026 	 loss = 2.7474(0.5577)
2023/11/22 23:14:18 - INFO - root -   Epoch: [89/300][280/283], lr: 0.00000026 	 loss = 1.6100(0.5446)
2023/11/22 23:14:19 - INFO - root -   Epoch: [89/300] 	 loss = 0.5411
2023/11/22 23:17:13 - INFO - root -   precision = 0.8182
2023/11/22 23:17:13 - INFO - root -   eval_loss = 0.4826
2023/11/22 23:17:13 - INFO - root -   eval_acc = 0.8182
2023/11/22 23:17:14 - INFO - root -   train_accuracy = 0.8145
2023/11/22 23:17:22 - INFO - root -   Epoch: [90/300][0/283], lr: 0.00000026 	 loss = 0.0424(0.0424)
2023/11/22 23:17:36 - INFO - root -   Epoch: [90/300][20/283], lr: 0.00000026 	 loss = 0.5562(0.5956)
2023/11/22 23:17:45 - INFO - root -   Epoch: [90/300][40/283], lr: 0.00000026 	 loss = 0.2619(0.7525)
2023/11/22 23:18:03 - INFO - root -   Epoch: [90/300][60/283], lr: 0.00000026 	 loss = 0.0193(0.6586)
2023/11/22 23:18:18 - INFO - root -   Epoch: [90/300][80/283], lr: 0.00000026 	 loss = 0.0445(0.6438)
2023/11/22 23:18:31 - INFO - root -   Epoch: [90/300][100/283], lr: 0.00000026 	 loss = 0.0551(0.5974)
2023/11/22 23:18:43 - INFO - root -   Epoch: [90/300][120/283], lr: 0.00000026 	 loss = 0.0614(0.5551)
2023/11/22 23:18:58 - INFO - root -   Epoch: [90/300][140/283], lr: 0.00000026 	 loss = 0.0623(0.5153)
2023/11/22 23:19:19 - INFO - root -   Epoch: [90/300][160/283], lr: 0.00000026 	 loss = 0.2786(0.4979)
2023/11/22 23:19:27 - INFO - root -   Epoch: [90/300][180/283], lr: 0.00000026 	 loss = 0.0920(0.5079)
2023/11/22 23:19:38 - INFO - root -   Epoch: [90/300][200/283], lr: 0.00000026 	 loss = 0.4619(0.4930)
2023/11/22 23:19:57 - INFO - root -   Epoch: [90/300][220/283], lr: 0.00000026 	 loss = 0.2091(0.4960)
2023/11/22 23:20:14 - INFO - root -   Epoch: [90/300][240/283], lr: 0.00000026 	 loss = 1.2990(0.5011)
2023/11/22 23:20:33 - INFO - root -   Epoch: [90/300][260/283], lr: 0.00000026 	 loss = 1.5333(0.4992)
2023/11/22 23:20:44 - INFO - root -   Epoch: [90/300][280/283], lr: 0.00000026 	 loss = 2.4022(0.4930)
2023/11/22 23:20:44 - INFO - root -   Epoch: [90/300] 	 loss = 0.4901
2023/11/22 23:20:44 - INFO - root -   train_accuracy = 0.8198
2023/11/22 23:20:54 - INFO - root -   Epoch: [91/300][0/283], lr: 0.00000026 	 loss = 0.1173(0.1173)
2023/11/22 23:21:14 - INFO - root -   Epoch: [91/300][20/283], lr: 0.00000026 	 loss = 0.7085(0.5706)
2023/11/22 23:21:34 - INFO - root -   Epoch: [91/300][40/283], lr: 0.00000026 	 loss = 0.0703(0.6678)
2023/11/22 23:21:45 - INFO - root -   Epoch: [91/300][60/283], lr: 0.00000026 	 loss = 0.0241(0.6165)
2023/11/22 23:22:04 - INFO - root -   Epoch: [91/300][80/283], lr: 0.00000026 	 loss = 0.0483(0.6251)
2023/11/22 23:22:17 - INFO - root -   Epoch: [91/300][100/283], lr: 0.00000026 	 loss = 0.3869(0.5989)
2023/11/22 23:22:25 - INFO - root -   Epoch: [91/300][120/283], lr: 0.00000026 	 loss = 0.1230(0.5674)
2023/11/22 23:22:42 - INFO - root -   Epoch: [91/300][140/283], lr: 0.00000026 	 loss = 0.0415(0.5360)
2023/11/22 23:22:55 - INFO - root -   Epoch: [91/300][160/283], lr: 0.00000026 	 loss = 1.3607(0.5304)
2023/11/22 23:23:06 - INFO - root -   Epoch: [91/300][180/283], lr: 0.00000026 	 loss = 0.0098(0.5474)
2023/11/22 23:23:18 - INFO - root -   Epoch: [91/300][200/283], lr: 0.00000026 	 loss = 0.1698(0.5393)
2023/11/22 23:23:37 - INFO - root -   Epoch: [91/300][220/283], lr: 0.00000026 	 loss = 0.3625(0.5228)
2023/11/22 23:23:48 - INFO - root -   Epoch: [91/300][240/283], lr: 0.00000026 	 loss = 0.8434(0.5250)
2023/11/22 23:23:56 - INFO - root -   Epoch: [91/300][260/283], lr: 0.00000026 	 loss = 1.2520(0.5156)
2023/11/22 23:24:09 - INFO - root -   Epoch: [91/300][280/283], lr: 0.00000026 	 loss = 2.6348(0.5009)
2023/11/22 23:24:10 - INFO - root -   Epoch: [91/300] 	 loss = 0.4991
2023/11/22 23:24:10 - INFO - root -   train_accuracy = 0.8127
2023/11/22 23:24:11 - INFO - root -   Epoch: [92/300][0/283], lr: 0.00000026 	 loss = 0.0892(0.0892)
2023/11/22 23:24:30 - INFO - root -   Epoch: [92/300][20/283], lr: 0.00000026 	 loss = 0.5749(0.6270)
2023/11/22 23:24:45 - INFO - root -   Epoch: [92/300][40/283], lr: 0.00000026 	 loss = 0.2434(0.7029)
2023/11/22 23:25:05 - INFO - root -   Epoch: [92/300][60/283], lr: 0.00000026 	 loss = 0.0228(0.6736)
2023/11/22 23:25:19 - INFO - root -   Epoch: [92/300][80/283], lr: 0.00000026 	 loss = 0.0562(0.6554)
2023/11/22 23:25:27 - INFO - root -   Epoch: [92/300][100/283], lr: 0.00000026 	 loss = 0.1624(0.6150)
2023/11/22 23:25:49 - INFO - root -   Epoch: [92/300][120/283], lr: 0.00000026 	 loss = 0.2971(0.5731)
2023/11/22 23:26:01 - INFO - root -   Epoch: [92/300][140/283], lr: 0.00000026 	 loss = 0.1871(0.5294)
2023/11/22 23:26:10 - INFO - root -   Epoch: [92/300][160/283], lr: 0.00000026 	 loss = 0.8260(0.5159)
2023/11/22 23:26:21 - INFO - root -   Epoch: [92/300][180/283], lr: 0.00000026 	 loss = 0.0703(0.5187)
2023/11/22 23:26:38 - INFO - root -   Epoch: [92/300][200/283], lr: 0.00000026 	 loss = 0.2656(0.5142)
2023/11/22 23:26:54 - INFO - root -   Epoch: [92/300][220/283], lr: 0.00000026 	 loss = 0.0334(0.4955)
2023/11/22 23:27:06 - INFO - root -   Epoch: [92/300][240/283], lr: 0.00000026 	 loss = 0.1825(0.4916)
2023/11/22 23:27:20 - INFO - root -   Epoch: [92/300][260/283], lr: 0.00000026 	 loss = 1.2039(0.4798)
2023/11/22 23:27:31 - INFO - root -   Epoch: [92/300][280/283], lr: 0.00000026 	 loss = 1.8832(0.4666)
2023/11/22 23:27:32 - INFO - root -   Epoch: [92/300] 	 loss = 0.4639
2023/11/22 23:27:32 - INFO - root -   train_accuracy = 0.8410
2023/11/22 23:27:39 - INFO - root -   Epoch: [93/300][0/283], lr: 0.00000026 	 loss = 0.1119(0.1119)
2023/11/22 23:28:00 - INFO - root -   Epoch: [93/300][20/283], lr: 0.00000026 	 loss = 0.6427(0.5616)
2023/11/22 23:28:15 - INFO - root -   Epoch: [93/300][40/283], lr: 0.00000026 	 loss = 0.1217(0.6815)
2023/11/22 23:28:24 - INFO - root -   Epoch: [93/300][60/283], lr: 0.00000026 	 loss = 0.0552(0.6304)
2023/11/22 23:28:45 - INFO - root -   Epoch: [93/300][80/283], lr: 0.00000026 	 loss = 0.3424(0.6681)
2023/11/22 23:28:59 - INFO - root -   Epoch: [93/300][100/283], lr: 0.00000026 	 loss = 0.1464(0.6409)
2023/11/22 23:29:09 - INFO - root -   Epoch: [93/300][120/283], lr: 0.00000026 	 loss = 0.0825(0.6058)
2023/11/22 23:29:28 - INFO - root -   Epoch: [93/300][140/283], lr: 0.00000026 	 loss = 0.0753(0.5735)
2023/11/22 23:29:46 - INFO - root -   Epoch: [93/300][160/283], lr: 0.00000026 	 loss = 1.6411(0.5620)
2023/11/22 23:29:59 - INFO - root -   Epoch: [93/300][180/283], lr: 0.00000026 	 loss = 0.0366(0.5776)
2023/11/22 23:30:17 - INFO - root -   Epoch: [93/300][200/283], lr: 0.00000026 	 loss = 0.0262(0.5682)
2023/11/22 23:30:36 - INFO - root -   Epoch: [93/300][220/283], lr: 0.00000026 	 loss = 0.1947(0.5491)
2023/11/22 23:30:56 - INFO - root -   Epoch: [93/300][240/283], lr: 0.00000026 	 loss = 1.9020(0.5492)
2023/11/22 23:31:15 - INFO - root -   Epoch: [93/300][260/283], lr: 0.00000026 	 loss = 1.1601(0.5405)
2023/11/22 23:31:28 - INFO - root -   Epoch: [93/300][280/283], lr: 0.00000026 	 loss = 2.8487(0.5249)
2023/11/22 23:31:29 - INFO - root -   Epoch: [93/300] 	 loss = 0.5215
2023/11/22 23:31:29 - INFO - root -   train_accuracy = 0.8233
2023/11/22 23:31:36 - INFO - root -   Epoch: [94/300][0/283], lr: 0.00000026 	 loss = 0.1369(0.1369)
2023/11/22 23:31:48 - INFO - root -   Epoch: [94/300][20/283], lr: 0.00000026 	 loss = 1.3792(0.6459)
2023/11/22 23:31:58 - INFO - root -   Epoch: [94/300][40/283], lr: 0.00000026 	 loss = 0.3046(0.6968)
2023/11/22 23:32:13 - INFO - root -   Epoch: [94/300][60/283], lr: 0.00000026 	 loss = 0.0970(0.6306)
2023/11/22 23:32:27 - INFO - root -   Epoch: [94/300][80/283], lr: 0.00000026 	 loss = 0.0601(0.6355)
2023/11/22 23:32:38 - INFO - root -   Epoch: [94/300][100/283], lr: 0.00000026 	 loss = 0.0768(0.5981)
2023/11/22 23:32:59 - INFO - root -   Epoch: [94/300][120/283], lr: 0.00000026 	 loss = 0.1404(0.5616)
2023/11/22 23:33:09 - INFO - root -   Epoch: [94/300][140/283], lr: 0.00000026 	 loss = 0.0679(0.5243)
2023/11/22 23:33:26 - INFO - root -   Epoch: [94/300][160/283], lr: 0.00000026 	 loss = 1.1878(0.5203)
2023/11/22 23:33:48 - INFO - root -   Epoch: [94/300][180/283], lr: 0.00000026 	 loss = 0.1317(0.5307)
2023/11/22 23:33:58 - INFO - root -   Epoch: [94/300][200/283], lr: 0.00000026 	 loss = 0.1303(0.5208)
2023/11/22 23:34:14 - INFO - root -   Epoch: [94/300][220/283], lr: 0.00000026 	 loss = 0.2426(0.5089)
2023/11/22 23:34:22 - INFO - root -   Epoch: [94/300][240/283], lr: 0.00000026 	 loss = 1.4778(0.5114)
2023/11/22 23:34:37 - INFO - root -   Epoch: [94/300][260/283], lr: 0.00000026 	 loss = 2.1817(0.5049)
2023/11/22 23:34:52 - INFO - root -   Epoch: [94/300][280/283], lr: 0.00000026 	 loss = 2.5151(0.4912)
2023/11/22 23:34:52 - INFO - root -   Epoch: [94/300] 	 loss = 0.4890
2023/11/22 23:37:46 - INFO - root -   precision = 0.8182
2023/11/22 23:37:46 - INFO - root -   eval_loss = 0.5026
2023/11/22 23:37:46 - INFO - root -   eval_acc = 0.8182
2023/11/22 23:37:47 - INFO - root -   train_accuracy = 0.8269
2023/11/22 23:37:48 - INFO - root -   Epoch: [95/300][0/283], lr: 0.00000027 	 loss = 0.1371(0.1371)
2023/11/22 23:38:15 - INFO - root -   Epoch: [95/300][20/283], lr: 0.00000027 	 loss = 0.7419(0.6427)
2023/11/22 23:38:26 - INFO - root -   Epoch: [95/300][40/283], lr: 0.00000027 	 loss = 0.2052(0.6710)
2023/11/22 23:38:44 - INFO - root -   Epoch: [95/300][60/283], lr: 0.00000027 	 loss = 0.3923(0.6168)
2023/11/22 23:38:52 - INFO - root -   Epoch: [95/300][80/283], lr: 0.00000027 	 loss = 0.0292(0.6108)
2023/11/22 23:39:06 - INFO - root -   Epoch: [95/300][100/283], lr: 0.00000027 	 loss = 0.3090(0.5754)
2023/11/22 23:39:24 - INFO - root -   Epoch: [95/300][120/283], lr: 0.00000027 	 loss = 0.1322(0.5340)
2023/11/22 23:39:39 - INFO - root -   Epoch: [95/300][140/283], lr: 0.00000027 	 loss = 0.1997(0.4995)
2023/11/22 23:39:52 - INFO - root -   Epoch: [95/300][160/283], lr: 0.00000027 	 loss = 2.3450(0.4973)
2023/11/22 23:40:05 - INFO - root -   Epoch: [95/300][180/283], lr: 0.00000027 	 loss = 0.3278(0.5134)
2023/11/22 23:40:22 - INFO - root -   Epoch: [95/300][200/283], lr: 0.00000027 	 loss = 0.1331(0.5121)
2023/11/22 23:40:33 - INFO - root -   Epoch: [95/300][220/283], lr: 0.00000027 	 loss = 0.2001(0.5055)
2023/11/22 23:40:50 - INFO - root -   Epoch: [95/300][240/283], lr: 0.00000027 	 loss = 0.1497(0.5028)
2023/11/22 23:41:07 - INFO - root -   Epoch: [95/300][260/283], lr: 0.00000027 	 loss = 2.2191(0.5004)
2023/11/22 23:41:16 - INFO - root -   Epoch: [95/300][280/283], lr: 0.00000027 	 loss = 1.9972(0.4912)
2023/11/22 23:41:17 - INFO - root -   Epoch: [95/300] 	 loss = 0.4884
2023/11/22 23:41:17 - INFO - root -   train_accuracy = 0.8286
2023/11/22 23:41:26 - INFO - root -   Epoch: [96/300][0/283], lr: 0.00000027 	 loss = 0.0614(0.0614)
2023/11/22 23:41:40 - INFO - root -   Epoch: [96/300][20/283], lr: 0.00000027 	 loss = 2.1806(0.6303)
2023/11/22 23:41:48 - INFO - root -   Epoch: [96/300][40/283], lr: 0.00000027 	 loss = 0.0941(0.7057)
2023/11/22 23:42:02 - INFO - root -   Epoch: [96/300][60/283], lr: 0.00000027 	 loss = 0.0239(0.6637)
2023/11/22 23:42:20 - INFO - root -   Epoch: [96/300][80/283], lr: 0.00000027 	 loss = 0.1591(0.6741)
2023/11/22 23:42:32 - INFO - root -   Epoch: [96/300][100/283], lr: 0.00000027 	 loss = 0.1846(0.6372)
2023/11/22 23:42:46 - INFO - root -   Epoch: [96/300][120/283], lr: 0.00000027 	 loss = 0.8341(0.5984)
2023/11/22 23:43:08 - INFO - root -   Epoch: [96/300][140/283], lr: 0.00000027 	 loss = 0.0228(0.5649)
2023/11/22 23:43:16 - INFO - root -   Epoch: [96/300][160/283], lr: 0.00000027 	 loss = 1.1955(0.5425)
2023/11/22 23:43:30 - INFO - root -   Epoch: [96/300][180/283], lr: 0.00000027 	 loss = 0.1223(0.5437)
2023/11/22 23:43:48 - INFO - root -   Epoch: [96/300][200/283], lr: 0.00000027 	 loss = 0.2733(0.5421)
2023/11/22 23:44:04 - INFO - root -   Epoch: [96/300][220/283], lr: 0.00000027 	 loss = 0.3120(0.5276)
2023/11/22 23:44:13 - INFO - root -   Epoch: [96/300][240/283], lr: 0.00000027 	 loss = 1.3391(0.5163)
2023/11/22 23:44:22 - INFO - root -   Epoch: [96/300][260/283], lr: 0.00000027 	 loss = 0.9735(0.5032)
2023/11/22 23:44:32 - INFO - root -   Epoch: [96/300][280/283], lr: 0.00000027 	 loss = 1.8589(0.4844)
2023/11/22 23:44:33 - INFO - root -   Epoch: [96/300] 	 loss = 0.4815
2023/11/22 23:44:33 - INFO - root -   train_accuracy = 0.8233
2023/11/22 23:44:46 - INFO - root -   Epoch: [97/300][0/283], lr: 0.00000027 	 loss = 0.2408(0.2408)
2023/11/22 23:44:54 - INFO - root -   Epoch: [97/300][20/283], lr: 0.00000027 	 loss = 0.6730(0.5817)
2023/11/22 23:45:04 - INFO - root -   Epoch: [97/300][40/283], lr: 0.00000027 	 loss = 0.1296(0.6345)
2023/11/22 23:45:21 - INFO - root -   Epoch: [97/300][60/283], lr: 0.00000027 	 loss = 0.0129(0.6565)
2023/11/22 23:45:36 - INFO - root -   Epoch: [97/300][80/283], lr: 0.00000027 	 loss = 0.0963(0.6235)
2023/11/22 23:45:49 - INFO - root -   Epoch: [97/300][100/283], lr: 0.00000027 	 loss = 0.2572(0.6128)
2023/11/22 23:46:03 - INFO - root -   Epoch: [97/300][120/283], lr: 0.00000027 	 loss = 0.1967(0.5591)
2023/11/22 23:46:13 - INFO - root -   Epoch: [97/300][140/283], lr: 0.00000027 	 loss = 0.0382(0.5172)
2023/11/22 23:46:36 - INFO - root -   Epoch: [97/300][160/283], lr: 0.00000027 	 loss = 1.3567(0.5138)
2023/11/22 23:46:51 - INFO - root -   Epoch: [97/300][180/283], lr: 0.00000027 	 loss = 0.2091(0.5305)
2023/11/22 23:47:04 - INFO - root -   Epoch: [97/300][200/283], lr: 0.00000027 	 loss = 0.0892(0.5179)
2023/11/22 23:47:20 - INFO - root -   Epoch: [97/300][220/283], lr: 0.00000027 	 loss = 0.1126(0.4992)
2023/11/22 23:47:32 - INFO - root -   Epoch: [97/300][240/283], lr: 0.00000027 	 loss = 0.9803(0.5149)
2023/11/22 23:47:44 - INFO - root -   Epoch: [97/300][260/283], lr: 0.00000027 	 loss = 1.2925(0.5021)
2023/11/22 23:47:59 - INFO - root -   Epoch: [97/300][280/283], lr: 0.00000027 	 loss = 1.7545(0.4863)
2023/11/22 23:48:00 - INFO - root -   Epoch: [97/300] 	 loss = 0.4832
2023/11/22 23:48:00 - INFO - root -   train_accuracy = 0.8322
2023/11/22 23:48:07 - INFO - root -   Epoch: [98/300][0/283], lr: 0.00000027 	 loss = 0.0505(0.0505)
2023/11/22 23:48:18 - INFO - root -   Epoch: [98/300][20/283], lr: 0.00000027 	 loss = 0.4129(0.6013)
2023/11/22 23:48:30 - INFO - root -   Epoch: [98/300][40/283], lr: 0.00000027 	 loss = 0.1899(0.6735)
2023/11/22 23:48:42 - INFO - root -   Epoch: [98/300][60/283], lr: 0.00000027 	 loss = 0.0166(0.6714)
2023/11/22 23:48:55 - INFO - root -   Epoch: [98/300][80/283], lr: 0.00000027 	 loss = 0.1009(0.6848)
2023/11/22 23:49:13 - INFO - root -   Epoch: [98/300][100/283], lr: 0.00000027 	 loss = 0.1320(0.6224)
2023/11/22 23:49:27 - INFO - root -   Epoch: [98/300][120/283], lr: 0.00000027 	 loss = 0.0784(0.5773)
2023/11/22 23:49:50 - INFO - root -   Epoch: [98/300][140/283], lr: 0.00000027 	 loss = 0.3043(0.5259)
2023/11/22 23:50:05 - INFO - root -   Epoch: [98/300][160/283], lr: 0.00000027 	 loss = 1.4375(0.5074)
2023/11/22 23:50:19 - INFO - root -   Epoch: [98/300][180/283], lr: 0.00000027 	 loss = 0.0972(0.5156)
2023/11/22 23:50:33 - INFO - root -   Epoch: [98/300][200/283], lr: 0.00000027 	 loss = 0.1199(0.5041)
2023/11/22 23:50:56 - INFO - root -   Epoch: [98/300][220/283], lr: 0.00000027 	 loss = 0.1235(0.4994)
2023/11/22 23:51:15 - INFO - root -   Epoch: [98/300][240/283], lr: 0.00000027 	 loss = 0.8652(0.4995)
2023/11/22 23:51:33 - INFO - root -   Epoch: [98/300][260/283], lr: 0.00000027 	 loss = 1.0730(0.4933)
2023/11/22 23:51:42 - INFO - root -   Epoch: [98/300][280/283], lr: 0.00000027 	 loss = 2.4490(0.4814)
2023/11/22 23:51:43 - INFO - root -   Epoch: [98/300] 	 loss = 0.4782
2023/11/22 23:51:43 - INFO - root -   train_accuracy = 0.8357
2023/11/22 23:51:55 - INFO - root -   Epoch: [99/300][0/283], lr: 0.00000027 	 loss = 0.2455(0.2455)
2023/11/22 23:52:05 - INFO - root -   Epoch: [99/300][20/283], lr: 0.00000027 	 loss = 0.4642(0.7160)
2023/11/22 23:52:20 - INFO - root -   Epoch: [99/300][40/283], lr: 0.00000027 	 loss = 0.5942(0.7250)
2023/11/22 23:52:46 - INFO - root -   Epoch: [99/300][60/283], lr: 0.00000027 	 loss = 0.0532(0.7056)
2023/11/22 23:53:00 - INFO - root -   Epoch: [99/300][80/283], lr: 0.00000027 	 loss = 0.2170(0.6748)
2023/11/22 23:53:09 - INFO - root -   Epoch: [99/300][100/283], lr: 0.00000027 	 loss = 0.6298(0.6325)
2023/11/22 23:53:22 - INFO - root -   Epoch: [99/300][120/283], lr: 0.00000027 	 loss = 0.5611(0.6107)
2023/11/22 23:53:34 - INFO - root -   Epoch: [99/300][140/283], lr: 0.00000027 	 loss = 0.1294(0.5686)
2023/11/22 23:53:43 - INFO - root -   Epoch: [99/300][160/283], lr: 0.00000027 	 loss = 1.7131(0.5618)
2023/11/22 23:54:03 - INFO - root -   Epoch: [99/300][180/283], lr: 0.00000027 	 loss = 0.4731(0.5634)
2023/11/22 23:54:19 - INFO - root -   Epoch: [99/300][200/283], lr: 0.00000027 	 loss = 0.0971(0.5523)
2023/11/22 23:54:31 - INFO - root -   Epoch: [99/300][220/283], lr: 0.00000027 	 loss = 0.1307(0.5410)
2023/11/22 23:54:51 - INFO - root -   Epoch: [99/300][240/283], lr: 0.00000027 	 loss = 1.4279(0.5401)
2023/11/22 23:55:04 - INFO - root -   Epoch: [99/300][260/283], lr: 0.00000027 	 loss = 1.1244(0.5319)
2023/11/22 23:55:18 - INFO - root -   Epoch: [99/300][280/283], lr: 0.00000027 	 loss = 1.7641(0.5176)
2023/11/22 23:55:19 - INFO - root -   Epoch: [99/300] 	 loss = 0.5149
2023/11/22 23:58:13 - INFO - root -   precision = 0.8182
2023/11/22 23:58:13 - INFO - root -   eval_loss = 0.4862
2023/11/22 23:58:13 - INFO - root -   eval_acc = 0.8182
2023/11/22 23:58:14 - INFO - root -   train_accuracy = 0.8163
2023/11/22 23:58:23 - INFO - root -   Epoch: [100/300][0/283], lr: 0.00000027 	 loss = 0.1410(0.1410)
2023/11/22 23:58:35 - INFO - root -   Epoch: [100/300][20/283], lr: 0.00000027 	 loss = 1.1637(0.6473)
2023/11/22 23:58:49 - INFO - root -   Epoch: [100/300][40/283], lr: 0.00000027 	 loss = 0.4278(0.6898)
2023/11/22 23:59:08 - INFO - root -   Epoch: [100/300][60/283], lr: 0.00000027 	 loss = 0.3379(0.6588)
2023/11/22 23:59:19 - INFO - root -   Epoch: [100/300][80/283], lr: 0.00000027 	 loss = 0.1512(0.6419)
2023/11/22 23:59:38 - INFO - root -   Epoch: [100/300][100/283], lr: 0.00000027 	 loss = 0.5895(0.6111)
2023/11/22 23:59:53 - INFO - root -   Epoch: [100/300][120/283], lr: 0.00000027 	 loss = 0.6474(0.5660)
2023/11/23 00:00:09 - INFO - root -   Epoch: [100/300][140/283], lr: 0.00000027 	 loss = 0.0364(0.5283)
2023/11/23 00:00:25 - INFO - root -   Epoch: [100/300][160/283], lr: 0.00000027 	 loss = 1.8437(0.5298)
2023/11/23 00:00:41 - INFO - root -   Epoch: [100/300][180/283], lr: 0.00000027 	 loss = 0.1159(0.5240)
2023/11/23 00:00:58 - INFO - root -   Epoch: [100/300][200/283], lr: 0.00000027 	 loss = 0.1779(0.5056)
2023/11/23 00:01:21 - INFO - root -   Epoch: [100/300][220/283], lr: 0.00000027 	 loss = 0.4445(0.4945)
2023/11/23 00:01:37 - INFO - root -   Epoch: [100/300][240/283], lr: 0.00000027 	 loss = 1.5547(0.4996)
2023/11/23 00:01:45 - INFO - root -   Epoch: [100/300][260/283], lr: 0.00000027 	 loss = 0.6748(0.4860)
2023/11/23 00:02:02 - INFO - root -   Epoch: [100/300][280/283], lr: 0.00000027 	 loss = 3.2083(0.4837)
2023/11/23 00:02:03 - INFO - root -   Epoch: [100/300] 	 loss = 0.4812
2023/11/23 00:02:03 - INFO - root -   train_accuracy = 0.8251
2023/11/23 00:02:10 - INFO - root -   Epoch: [101/300][0/283], lr: 0.00000028 	 loss = 0.1435(0.1435)
2023/11/23 00:02:27 - INFO - root -   Epoch: [101/300][20/283], lr: 0.00000028 	 loss = 2.3680(0.6750)
2023/11/23 00:02:42 - INFO - root -   Epoch: [101/300][40/283], lr: 0.00000028 	 loss = 1.0686(0.7436)
2023/11/23 00:02:56 - INFO - root -   Epoch: [101/300][60/283], lr: 0.00000028 	 loss = 0.0347(0.6560)
2023/11/23 00:03:17 - INFO - root -   Epoch: [101/300][80/283], lr: 0.00000028 	 loss = 0.2143(0.6536)
2023/11/23 00:03:32 - INFO - root -   Epoch: [101/300][100/283], lr: 0.00000028 	 loss = 0.3224(0.6211)
2023/11/23 00:03:44 - INFO - root -   Epoch: [101/300][120/283], lr: 0.00000028 	 loss = 0.0274(0.5747)
2023/11/23 00:03:56 - INFO - root -   Epoch: [101/300][140/283], lr: 0.00000028 	 loss = 0.3042(0.5300)
2023/11/23 00:04:10 - INFO - root -   Epoch: [101/300][160/283], lr: 0.00000028 	 loss = 1.4161(0.5229)
2023/11/23 00:04:28 - INFO - root -   Epoch: [101/300][180/283], lr: 0.00000028 	 loss = 0.1006(0.5323)
2023/11/23 00:04:37 - INFO - root -   Epoch: [101/300][200/283], lr: 0.00000028 	 loss = 0.1847(0.5158)
2023/11/23 00:04:57 - INFO - root -   Epoch: [101/300][220/283], lr: 0.00000028 	 loss = 0.1075(0.4975)
2023/11/23 00:05:11 - INFO - root -   Epoch: [101/300][240/283], lr: 0.00000028 	 loss = 1.5220(0.5002)
2023/11/23 00:05:26 - INFO - root -   Epoch: [101/300][260/283], lr: 0.00000028 	 loss = 1.1518(0.4931)
2023/11/23 00:05:38 - INFO - root -   Epoch: [101/300][280/283], lr: 0.00000028 	 loss = 1.6678(0.4787)
2023/11/23 00:05:39 - INFO - root -   Epoch: [101/300] 	 loss = 0.4762
2023/11/23 00:05:39 - INFO - root -   train_accuracy = 0.8127
2023/11/23 00:05:40 - INFO - root -   Epoch: [102/300][0/283], lr: 0.00000028 	 loss = 0.0453(0.0453)
2023/11/23 00:05:59 - INFO - root -   Epoch: [102/300][20/283], lr: 0.00000028 	 loss = 0.8457(0.7156)
2023/11/23 00:06:10 - INFO - root -   Epoch: [102/300][40/283], lr: 0.00000028 	 loss = 0.1360(0.7654)
2023/11/23 00:06:40 - INFO - root -   Epoch: [102/300][60/283], lr: 0.00000028 	 loss = 0.3251(0.7470)
2023/11/23 00:06:49 - INFO - root -   Epoch: [102/300][80/283], lr: 0.00000028 	 loss = 0.0616(0.6816)
2023/11/23 00:07:02 - INFO - root -   Epoch: [102/300][100/283], lr: 0.00000028 	 loss = 0.1215(0.6224)
2023/11/23 00:07:15 - INFO - root -   Epoch: [102/300][120/283], lr: 0.00000028 	 loss = 0.1700(0.5758)
2023/11/23 00:07:37 - INFO - root -   Epoch: [102/300][140/283], lr: 0.00000028 	 loss = 0.0760(0.5342)
2023/11/23 00:07:56 - INFO - root -   Epoch: [102/300][160/283], lr: 0.00000028 	 loss = 0.5892(0.5245)
2023/11/23 00:08:06 - INFO - root -   Epoch: [102/300][180/283], lr: 0.00000028 	 loss = 0.2460(0.5226)
2023/11/23 00:08:23 - INFO - root -   Epoch: [102/300][200/283], lr: 0.00000028 	 loss = 0.0915(0.5284)
2023/11/23 00:08:31 - INFO - root -   Epoch: [102/300][220/283], lr: 0.00000028 	 loss = 0.0612(0.5177)
2023/11/23 00:08:40 - INFO - root -   Epoch: [102/300][240/283], lr: 0.00000028 	 loss = 0.9435(0.5226)
2023/11/23 00:08:58 - INFO - root -   Epoch: [102/300][260/283], lr: 0.00000028 	 loss = 1.1491(0.5159)
2023/11/23 00:09:06 - INFO - root -   Epoch: [102/300][280/283], lr: 0.00000028 	 loss = 2.6864(0.5067)
2023/11/23 00:09:07 - INFO - root -   Epoch: [102/300] 	 loss = 0.5046
2023/11/23 00:09:07 - INFO - root -   train_accuracy = 0.8180
2023/11/23 00:09:08 - INFO - root -   Epoch: [103/300][0/283], lr: 0.00000028 	 loss = 0.1046(0.1046)
2023/11/23 00:09:27 - INFO - root -   Epoch: [103/300][20/283], lr: 0.00000028 	 loss = 0.2499(0.6412)
2023/11/23 00:09:41 - INFO - root -   Epoch: [103/300][40/283], lr: 0.00000028 	 loss = 0.0881(0.7586)
2023/11/23 00:09:55 - INFO - root -   Epoch: [103/300][60/283], lr: 0.00000028 	 loss = 0.0476(0.6827)
2023/11/23 00:10:09 - INFO - root -   Epoch: [103/300][80/283], lr: 0.00000028 	 loss = 0.2401(0.6485)
2023/11/23 00:10:26 - INFO - root -   Epoch: [103/300][100/283], lr: 0.00000028 	 loss = 0.2433(0.6252)
2023/11/23 00:10:43 - INFO - root -   Epoch: [103/300][120/283], lr: 0.00000028 	 loss = 0.0858(0.5797)
2023/11/23 00:10:55 - INFO - root -   Epoch: [103/300][140/283], lr: 0.00000028 	 loss = 0.1315(0.5350)
2023/11/23 00:11:09 - INFO - root -   Epoch: [103/300][160/283], lr: 0.00000028 	 loss = 0.9670(0.5210)
2023/11/23 00:11:27 - INFO - root -   Epoch: [103/300][180/283], lr: 0.00000028 	 loss = 0.2277(0.5107)
2023/11/23 00:11:42 - INFO - root -   Epoch: [103/300][200/283], lr: 0.00000028 	 loss = 0.0804(0.5084)
2023/11/23 00:11:54 - INFO - root -   Epoch: [103/300][220/283], lr: 0.00000028 	 loss = 0.1089(0.4938)
2023/11/23 00:12:12 - INFO - root -   Epoch: [103/300][240/283], lr: 0.00000028 	 loss = 1.6386(0.5023)
2023/11/23 00:12:26 - INFO - root -   Epoch: [103/300][260/283], lr: 0.00000028 	 loss = 0.6958(0.4909)
2023/11/23 00:12:41 - INFO - root -   Epoch: [103/300][280/283], lr: 0.00000028 	 loss = 3.0205(0.4822)
2023/11/23 00:12:42 - INFO - root -   Epoch: [103/300] 	 loss = 0.4797
2023/11/23 00:12:42 - INFO - root -   train_accuracy = 0.8357
2023/11/23 00:12:43 - INFO - root -   Epoch: [104/300][0/283], lr: 0.00000028 	 loss = 0.0200(0.0200)
2023/11/23 00:13:00 - INFO - root -   Epoch: [104/300][20/283], lr: 0.00000028 	 loss = 2.0340(0.7174)
2023/11/23 00:13:11 - INFO - root -   Epoch: [104/300][40/283], lr: 0.00000028 	 loss = 0.2770(0.7595)
2023/11/23 00:13:20 - INFO - root -   Epoch: [104/300][60/283], lr: 0.00000028 	 loss = 0.0120(0.7068)
2023/11/23 00:13:33 - INFO - root -   Epoch: [104/300][80/283], lr: 0.00000028 	 loss = 0.0545(0.7049)
2023/11/23 00:13:45 - INFO - root -   Epoch: [104/300][100/283], lr: 0.00000028 	 loss = 0.0887(0.6497)
2023/11/23 00:14:00 - INFO - root -   Epoch: [104/300][120/283], lr: 0.00000028 	 loss = 0.1642(0.5918)
2023/11/23 00:14:14 - INFO - root -   Epoch: [104/300][140/283], lr: 0.00000028 	 loss = 0.0189(0.5432)
2023/11/23 00:14:29 - INFO - root -   Epoch: [104/300][160/283], lr: 0.00000028 	 loss = 1.0713(0.5408)
2023/11/23 00:14:41 - INFO - root -   Epoch: [104/300][180/283], lr: 0.00000028 	 loss = 0.0342(0.5268)
2023/11/23 00:14:49 - INFO - root -   Epoch: [104/300][200/283], lr: 0.00000028 	 loss = 0.2931(0.5168)
2023/11/23 00:15:01 - INFO - root -   Epoch: [104/300][220/283], lr: 0.00000028 	 loss = 0.2512(0.4998)
2023/11/23 00:15:11 - INFO - root -   Epoch: [104/300][240/283], lr: 0.00000028 	 loss = 1.2416(0.4968)
2023/11/23 00:15:20 - INFO - root -   Epoch: [104/300][260/283], lr: 0.00000028 	 loss = 1.5133(0.4874)
2023/11/23 00:15:33 - INFO - root -   Epoch: [104/300][280/283], lr: 0.00000028 	 loss = 1.7450(0.4727)
2023/11/23 00:15:34 - INFO - root -   Epoch: [104/300] 	 loss = 0.4697
2023/11/23 00:18:24 - INFO - root -   precision = 0.8112
2023/11/23 00:18:24 - INFO - root -   eval_loss = 0.5057
2023/11/23 00:18:24 - INFO - root -   eval_acc = 0.8112
2023/11/23 00:18:25 - INFO - root -   train_accuracy = 0.8286
2023/11/23 00:18:42 - INFO - root -   Epoch: [105/300][0/283], lr: 0.00000028 	 loss = 0.2069(0.2069)
2023/11/23 00:18:50 - INFO - root -   Epoch: [105/300][20/283], lr: 0.00000028 	 loss = 0.4963(0.5350)
2023/11/23 00:19:19 - INFO - root -   Epoch: [105/300][40/283], lr: 0.00000028 	 loss = 0.5134(0.6695)
2023/11/23 00:19:35 - INFO - root -   Epoch: [105/300][60/283], lr: 0.00000028 	 loss = 0.0677(0.6347)
2023/11/23 00:19:55 - INFO - root -   Epoch: [105/300][80/283], lr: 0.00000028 	 loss = 0.0466(0.6691)
2023/11/23 00:20:13 - INFO - root -   Epoch: [105/300][100/283], lr: 0.00000028 	 loss = 0.0574(0.6418)
2023/11/23 00:20:30 - INFO - root -   Epoch: [105/300][120/283], lr: 0.00000028 	 loss = 0.1021(0.5922)
2023/11/23 00:20:43 - INFO - root -   Epoch: [105/300][140/283], lr: 0.00000028 	 loss = 0.1481(0.5587)
2023/11/23 00:20:58 - INFO - root -   Epoch: [105/300][160/283], lr: 0.00000028 	 loss = 1.3237(0.5506)
2023/11/23 00:21:10 - INFO - root -   Epoch: [105/300][180/283], lr: 0.00000028 	 loss = 0.2652(0.5371)
2023/11/23 00:21:30 - INFO - root -   Epoch: [105/300][200/283], lr: 0.00000028 	 loss = 0.9478(0.5339)
2023/11/23 00:21:47 - INFO - root -   Epoch: [105/300][220/283], lr: 0.00000028 	 loss = 0.0693(0.5169)
2023/11/23 00:21:57 - INFO - root -   Epoch: [105/300][240/283], lr: 0.00000028 	 loss = 1.0638(0.5158)
2023/11/23 00:22:15 - INFO - root -   Epoch: [105/300][260/283], lr: 0.00000028 	 loss = 0.3561(0.5054)
2023/11/23 00:22:30 - INFO - root -   Epoch: [105/300][280/283], lr: 0.00000028 	 loss = 3.1558(0.4966)
2023/11/23 00:22:31 - INFO - root -   Epoch: [105/300] 	 loss = 0.4936
2023/11/23 00:22:31 - INFO - root -   train_accuracy = 0.8233
2023/11/23 00:22:43 - INFO - root -   Epoch: [106/300][0/283], lr: 0.00000029 	 loss = 0.1420(0.1420)
2023/11/23 00:22:56 - INFO - root -   Epoch: [106/300][20/283], lr: 0.00000029 	 loss = 0.9464(0.6999)
2023/11/23 00:23:17 - INFO - root -   Epoch: [106/300][40/283], lr: 0.00000029 	 loss = 1.7054(0.8155)
2023/11/23 00:23:32 - INFO - root -   Epoch: [106/300][60/283], lr: 0.00000029 	 loss = 0.0200(0.7346)
2023/11/23 00:23:45 - INFO - root -   Epoch: [106/300][80/283], lr: 0.00000029 	 loss = 0.2580(0.7020)
2023/11/23 00:23:54 - INFO - root -   Epoch: [106/300][100/283], lr: 0.00000029 	 loss = 0.3427(0.6287)
2023/11/23 00:24:16 - INFO - root -   Epoch: [106/300][120/283], lr: 0.00000029 	 loss = 0.0738(0.6022)
2023/11/23 00:24:26 - INFO - root -   Epoch: [106/300][140/283], lr: 0.00000029 	 loss = 0.0200(0.5476)
2023/11/23 00:24:45 - INFO - root -   Epoch: [106/300][160/283], lr: 0.00000029 	 loss = 1.8738(0.5551)
2023/11/23 00:25:10 - INFO - root -   Epoch: [106/300][180/283], lr: 0.00000029 	 loss = 0.0710(0.5611)
2023/11/23 00:25:18 - INFO - root -   Epoch: [106/300][200/283], lr: 0.00000029 	 loss = 0.0976(0.5460)
2023/11/23 00:25:34 - INFO - root -   Epoch: [106/300][220/283], lr: 0.00000029 	 loss = 0.2807(0.5267)
2023/11/23 00:25:49 - INFO - root -   Epoch: [106/300][240/283], lr: 0.00000029 	 loss = 0.2079(0.5179)
2023/11/23 00:26:04 - INFO - root -   Epoch: [106/300][260/283], lr: 0.00000029 	 loss = 2.5049(0.5094)
2023/11/23 00:26:13 - INFO - root -   Epoch: [106/300][280/283], lr: 0.00000029 	 loss = 1.5945(0.4950)
2023/11/23 00:26:14 - INFO - root -   Epoch: [106/300] 	 loss = 0.4920
2023/11/23 00:26:14 - INFO - root -   train_accuracy = 0.8339
2023/11/23 00:26:15 - INFO - root -   Epoch: [107/300][0/283], lr: 0.00000029 	 loss = 0.0530(0.0530)
2023/11/23 00:26:31 - INFO - root -   Epoch: [107/300][20/283], lr: 0.00000029 	 loss = 1.2403(0.6626)
2023/11/23 00:26:46 - INFO - root -   Epoch: [107/300][40/283], lr: 0.00000029 	 loss = 0.3253(0.7055)
2023/11/23 00:26:58 - INFO - root -   Epoch: [107/300][60/283], lr: 0.00000029 	 loss = 0.2225(0.6402)
2023/11/23 00:27:10 - INFO - root -   Epoch: [107/300][80/283], lr: 0.00000029 	 loss = 0.1808(0.6065)
2023/11/23 00:27:27 - INFO - root -   Epoch: [107/300][100/283], lr: 0.00000029 	 loss = 0.2684(0.5473)
2023/11/23 00:27:35 - INFO - root -   Epoch: [107/300][120/283], lr: 0.00000029 	 loss = 0.1776(0.5148)
2023/11/23 00:27:55 - INFO - root -   Epoch: [107/300][140/283], lr: 0.00000029 	 loss = 0.0372(0.4748)
2023/11/23 00:28:11 - INFO - root -   Epoch: [107/300][160/283], lr: 0.00000029 	 loss = 2.4189(0.4799)
2023/11/23 00:28:31 - INFO - root -   Epoch: [107/300][180/283], lr: 0.00000029 	 loss = 0.0905(0.4873)
2023/11/23 00:28:43 - INFO - root -   Epoch: [107/300][200/283], lr: 0.00000029 	 loss = 0.0941(0.4787)
2023/11/23 00:29:06 - INFO - root -   Epoch: [107/300][220/283], lr: 0.00000029 	 loss = 0.1119(0.4637)
2023/11/23 00:29:18 - INFO - root -   Epoch: [107/300][240/283], lr: 0.00000029 	 loss = 0.1072(0.4701)
2023/11/23 00:29:26 - INFO - root -   Epoch: [107/300][260/283], lr: 0.00000029 	 loss = 0.5584(0.4650)
2023/11/23 00:29:41 - INFO - root -   Epoch: [107/300][280/283], lr: 0.00000029 	 loss = 1.1725(0.4530)
2023/11/23 00:29:42 - INFO - root -   Epoch: [107/300] 	 loss = 0.4506
2023/11/23 00:29:42 - INFO - root -   train_accuracy = 0.8410
2023/11/23 00:29:50 - INFO - root -   Epoch: [108/300][0/283], lr: 0.00000029 	 loss = 0.0463(0.0463)
2023/11/23 00:30:08 - INFO - root -   Epoch: [108/300][20/283], lr: 0.00000029 	 loss = 0.8312(0.5948)
2023/11/23 00:30:18 - INFO - root -   Epoch: [108/300][40/283], lr: 0.00000029 	 loss = 0.6815(0.7208)
2023/11/23 00:30:34 - INFO - root -   Epoch: [108/300][60/283], lr: 0.00000029 	 loss = 0.2905(0.6644)
2023/11/23 00:30:50 - INFO - root -   Epoch: [108/300][80/283], lr: 0.00000029 	 loss = 0.1108(0.6588)
2023/11/23 00:31:05 - INFO - root -   Epoch: [108/300][100/283], lr: 0.00000029 	 loss = 0.1227(0.6352)
2023/11/23 00:31:28 - INFO - root -   Epoch: [108/300][120/283], lr: 0.00000029 	 loss = 0.2408(0.5951)
2023/11/23 00:31:39 - INFO - root -   Epoch: [108/300][140/283], lr: 0.00000029 	 loss = 0.0477(0.5468)
2023/11/23 00:31:48 - INFO - root -   Epoch: [108/300][160/283], lr: 0.00000029 	 loss = 0.5382(0.5231)
2023/11/23 00:32:05 - INFO - root -   Epoch: [108/300][180/283], lr: 0.00000029 	 loss = 0.1441(0.5064)
2023/11/23 00:32:24 - INFO - root -   Epoch: [108/300][200/283], lr: 0.00000029 	 loss = 0.1976(0.4997)
2023/11/23 00:32:43 - INFO - root -   Epoch: [108/300][220/283], lr: 0.00000029 	 loss = 0.0220(0.4837)
2023/11/23 00:32:54 - INFO - root -   Epoch: [108/300][240/283], lr: 0.00000029 	 loss = 1.0101(0.4809)
2023/11/23 00:33:07 - INFO - root -   Epoch: [108/300][260/283], lr: 0.00000029 	 loss = 0.7497(0.4663)
2023/11/23 00:33:19 - INFO - root -   Epoch: [108/300][280/283], lr: 0.00000029 	 loss = 2.3828(0.4521)
2023/11/23 00:33:20 - INFO - root -   Epoch: [108/300] 	 loss = 0.4493
2023/11/23 00:33:20 - INFO - root -   train_accuracy = 0.8198
2023/11/23 00:33:21 - INFO - root -   Epoch: [109/300][0/283], lr: 0.00000029 	 loss = 0.0246(0.0246)
2023/11/23 00:33:43 - INFO - root -   Epoch: [109/300][20/283], lr: 0.00000029 	 loss = 0.3954(0.6192)
2023/11/23 00:33:58 - INFO - root -   Epoch: [109/300][40/283], lr: 0.00000029 	 loss = 0.1561(0.7763)
2023/11/23 00:34:12 - INFO - root -   Epoch: [109/300][60/283], lr: 0.00000029 	 loss = 0.2031(0.6922)
2023/11/23 00:34:34 - INFO - root -   Epoch: [109/300][80/283], lr: 0.00000029 	 loss = 0.1019(0.6340)
2023/11/23 00:34:48 - INFO - root -   Epoch: [109/300][100/283], lr: 0.00000029 	 loss = 0.5369(0.5950)
2023/11/23 00:35:01 - INFO - root -   Epoch: [109/300][120/283], lr: 0.00000029 	 loss = 0.3561(0.5484)
2023/11/23 00:35:21 - INFO - root -   Epoch: [109/300][140/283], lr: 0.00000029 	 loss = 0.1165(0.5018)
2023/11/23 00:35:38 - INFO - root -   Epoch: [109/300][160/283], lr: 0.00000029 	 loss = 0.4091(0.4958)
2023/11/23 00:35:59 - INFO - root -   Epoch: [109/300][180/283], lr: 0.00000029 	 loss = 0.1752(0.5144)
2023/11/23 00:36:07 - INFO - root -   Epoch: [109/300][200/283], lr: 0.00000029 	 loss = 0.1628(0.5049)
2023/11/23 00:36:27 - INFO - root -   Epoch: [109/300][220/283], lr: 0.00000029 	 loss = 0.1740(0.5005)
2023/11/23 00:36:47 - INFO - root -   Epoch: [109/300][240/283], lr: 0.00000029 	 loss = 0.7362(0.5090)
2023/11/23 00:37:06 - INFO - root -   Epoch: [109/300][260/283], lr: 0.00000029 	 loss = 1.6132(0.5001)
2023/11/23 00:37:14 - INFO - root -   Epoch: [109/300][280/283], lr: 0.00000029 	 loss = 3.6825(0.4914)
2023/11/23 00:37:15 - INFO - root -   Epoch: [109/300] 	 loss = 0.4884
2023/11/23 00:40:11 - INFO - root -   precision = 0.8252
2023/11/23 00:40:11 - INFO - root -   eval_loss = 0.4730
2023/11/23 00:40:11 - INFO - root -   eval_acc = 0.8252
2023/11/23 00:40:12 - INFO - root -   train_accuracy = 0.8392
2023/11/23 00:40:14 - INFO - root -   Epoch: [110/300][0/283], lr: 0.00000029 	 loss = 0.0430(0.0430)
2023/11/23 00:40:32 - INFO - root -   Epoch: [110/300][20/283], lr: 0.00000029 	 loss = 0.5491(0.6766)
2023/11/23 00:40:42 - INFO - root -   Epoch: [110/300][40/283], lr: 0.00000029 	 loss = 0.3944(0.7124)
2023/11/23 00:40:56 - INFO - root -   Epoch: [110/300][60/283], lr: 0.00000029 	 loss = 0.0226(0.6432)
2023/11/23 00:41:16 - INFO - root -   Epoch: [110/300][80/283], lr: 0.00000029 	 loss = 0.0959(0.6389)
2023/11/23 00:41:33 - INFO - root -   Epoch: [110/300][100/283], lr: 0.00000029 	 loss = 0.1994(0.5975)
2023/11/23 00:41:46 - INFO - root -   Epoch: [110/300][120/283], lr: 0.00000029 	 loss = 0.1228(0.5717)
2023/11/23 00:41:54 - INFO - root -   Epoch: [110/300][140/283], lr: 0.00000029 	 loss = 0.1380(0.5219)
2023/11/23 00:42:04 - INFO - root -   Epoch: [110/300][160/283], lr: 0.00000029 	 loss = 0.4591(0.5056)
2023/11/23 00:42:19 - INFO - root -   Epoch: [110/300][180/283], lr: 0.00000029 	 loss = 0.2716(0.5088)
2023/11/23 00:42:39 - INFO - root -   Epoch: [110/300][200/283], lr: 0.00000029 	 loss = 0.0944(0.4958)
2023/11/23 00:42:59 - INFO - root -   Epoch: [110/300][220/283], lr: 0.00000029 	 loss = 0.2046(0.4836)
2023/11/23 00:43:19 - INFO - root -   Epoch: [110/300][240/283], lr: 0.00000029 	 loss = 1.1022(0.4878)
2023/11/23 00:43:31 - INFO - root -   Epoch: [110/300][260/283], lr: 0.00000029 	 loss = 0.9881(0.4716)
2023/11/23 00:43:44 - INFO - root -   Epoch: [110/300][280/283], lr: 0.00000029 	 loss = 3.5001(0.4658)
2023/11/23 00:43:45 - INFO - root -   Epoch: [110/300] 	 loss = 0.4633
2023/11/23 00:43:45 - INFO - root -   train_accuracy = 0.8375
2023/11/23 00:43:46 - INFO - root -   Epoch: [111/300][0/283], lr: 0.00000029 	 loss = 0.0902(0.0902)
2023/11/23 00:44:06 - INFO - root -   Epoch: [111/300][20/283], lr: 0.00000029 	 loss = 1.4077(0.5288)
2023/11/23 00:44:23 - INFO - root -   Epoch: [111/300][40/283], lr: 0.00000029 	 loss = 0.3474(0.7141)
2023/11/23 00:44:43 - INFO - root -   Epoch: [111/300][60/283], lr: 0.00000029 	 loss = 0.0059(0.7214)
2023/11/23 00:44:55 - INFO - root -   Epoch: [111/300][80/283], lr: 0.00000029 	 loss = 0.2313(0.7299)
2023/11/23 00:45:08 - INFO - root -   Epoch: [111/300][100/283], lr: 0.00000029 	 loss = 0.4313(0.6697)
2023/11/23 00:45:23 - INFO - root -   Epoch: [111/300][120/283], lr: 0.00000029 	 loss = 0.1611(0.6157)
2023/11/23 00:45:43 - INFO - root -   Epoch: [111/300][140/283], lr: 0.00000029 	 loss = 0.6141(0.5678)
2023/11/23 00:46:03 - INFO - root -   Epoch: [111/300][160/283], lr: 0.00000029 	 loss = 1.3171(0.5644)
2023/11/23 00:46:15 - INFO - root -   Epoch: [111/300][180/283], lr: 0.00000029 	 loss = 0.2398(0.5689)
2023/11/23 00:46:33 - INFO - root -   Epoch: [111/300][200/283], lr: 0.00000029 	 loss = 0.2434(0.5445)
2023/11/23 00:46:46 - INFO - root -   Epoch: [111/300][220/283], lr: 0.00000029 	 loss = 0.0662(0.5239)
2023/11/23 00:47:05 - INFO - root -   Epoch: [111/300][240/283], lr: 0.00000029 	 loss = 1.6695(0.5270)
2023/11/23 00:47:21 - INFO - root -   Epoch: [111/300][260/283], lr: 0.00000029 	 loss = 2.5456(0.5176)
2023/11/23 00:47:35 - INFO - root -   Epoch: [111/300][280/283], lr: 0.00000029 	 loss = 2.4738(0.5114)
2023/11/23 00:47:36 - INFO - root -   Epoch: [111/300] 	 loss = 0.5087
2023/11/23 00:47:36 - INFO - root -   train_accuracy = 0.8251
2023/11/23 00:47:37 - INFO - root -   Epoch: [112/300][0/283], lr: 0.00000030 	 loss = 0.0448(0.0448)
2023/11/23 00:47:54 - INFO - root -   Epoch: [112/300][20/283], lr: 0.00000030 	 loss = 1.5351(0.7082)
2023/11/23 00:48:10 - INFO - root -   Epoch: [112/300][40/283], lr: 0.00000030 	 loss = 0.2182(0.6721)
2023/11/23 00:48:29 - INFO - root -   Epoch: [112/300][60/283], lr: 0.00000030 	 loss = 0.1095(0.6662)
2023/11/23 00:48:43 - INFO - root -   Epoch: [112/300][80/283], lr: 0.00000030 	 loss = 0.1002(0.6823)
2023/11/23 00:48:59 - INFO - root -   Epoch: [112/300][100/283], lr: 0.00000030 	 loss = 0.4073(0.6139)
2023/11/23 00:49:11 - INFO - root -   Epoch: [112/300][120/283], lr: 0.00000030 	 loss = 0.1654(0.5758)
2023/11/23 00:49:26 - INFO - root -   Epoch: [112/300][140/283], lr: 0.00000030 	 loss = 0.2347(0.5378)
2023/11/23 00:49:39 - INFO - root -   Epoch: [112/300][160/283], lr: 0.00000030 	 loss = 0.9444(0.5243)
2023/11/23 00:49:52 - INFO - root -   Epoch: [112/300][180/283], lr: 0.00000030 	 loss = 0.0619(0.5206)
2023/11/23 00:50:03 - INFO - root -   Epoch: [112/300][200/283], lr: 0.00000030 	 loss = 0.1345(0.5055)
2023/11/23 00:50:22 - INFO - root -   Epoch: [112/300][220/283], lr: 0.00000030 	 loss = 0.3323(0.4869)
2023/11/23 00:50:32 - INFO - root -   Epoch: [112/300][240/283], lr: 0.00000030 	 loss = 1.8449(0.4958)
2023/11/23 00:50:48 - INFO - root -   Epoch: [112/300][260/283], lr: 0.00000030 	 loss = 1.6308(0.4900)
2023/11/23 00:50:58 - INFO - root -   Epoch: [112/300][280/283], lr: 0.00000030 	 loss = 3.3986(0.4773)
2023/11/23 00:50:59 - INFO - root -   Epoch: [112/300] 	 loss = 0.4754
2023/11/23 00:50:59 - INFO - root -   train_accuracy = 0.8498
2023/11/23 00:51:06 - INFO - root -   Epoch: [113/300][0/283], lr: 0.00000030 	 loss = 0.0945(0.0945)
2023/11/23 00:51:19 - INFO - root -   Epoch: [113/300][20/283], lr: 0.00000030 	 loss = 0.8218(0.5220)
2023/11/23 00:51:34 - INFO - root -   Epoch: [113/300][40/283], lr: 0.00000030 	 loss = 0.5309(0.6017)
2023/11/23 00:51:53 - INFO - root -   Epoch: [113/300][60/283], lr: 0.00000030 	 loss = 0.0696(0.5897)
2023/11/23 00:52:08 - INFO - root -   Epoch: [113/300][80/283], lr: 0.00000030 	 loss = 0.1109(0.6156)
2023/11/23 00:52:28 - INFO - root -   Epoch: [113/300][100/283], lr: 0.00000030 	 loss = 0.1055(0.6092)
2023/11/23 00:52:43 - INFO - root -   Epoch: [113/300][120/283], lr: 0.00000030 	 loss = 0.1008(0.5543)
2023/11/23 00:52:55 - INFO - root -   Epoch: [113/300][140/283], lr: 0.00000030 	 loss = 0.0587(0.5139)
2023/11/23 00:53:14 - INFO - root -   Epoch: [113/300][160/283], lr: 0.00000030 	 loss = 0.9606(0.5097)
2023/11/23 00:53:28 - INFO - root -   Epoch: [113/300][180/283], lr: 0.00000030 	 loss = 0.4296(0.5242)
2023/11/23 00:53:43 - INFO - root -   Epoch: [113/300][200/283], lr: 0.00000030 	 loss = 0.0972(0.5092)
2023/11/23 00:54:00 - INFO - root -   Epoch: [113/300][220/283], lr: 0.00000030 	 loss = 0.3881(0.4982)
2023/11/23 00:54:09 - INFO - root -   Epoch: [113/300][240/283], lr: 0.00000030 	 loss = 1.4923(0.4893)
2023/11/23 00:54:24 - INFO - root -   Epoch: [113/300][260/283], lr: 0.00000030 	 loss = 1.8124(0.4766)
2023/11/23 00:54:35 - INFO - root -   Epoch: [113/300][280/283], lr: 0.00000030 	 loss = 2.1204(0.4657)
2023/11/23 00:54:36 - INFO - root -   Epoch: [113/300] 	 loss = 0.4633
2023/11/23 00:54:36 - INFO - root -   train_accuracy = 0.8339
2023/11/23 00:54:42 - INFO - root -   Epoch: [114/300][0/283], lr: 0.00000030 	 loss = 0.2821(0.2821)
2023/11/23 00:55:00 - INFO - root -   Epoch: [114/300][20/283], lr: 0.00000030 	 loss = 2.0767(0.6088)
2023/11/23 00:55:09 - INFO - root -   Epoch: [114/300][40/283], lr: 0.00000030 	 loss = 0.0388(0.7314)
2023/11/23 00:55:28 - INFO - root -   Epoch: [114/300][60/283], lr: 0.00000030 	 loss = 0.0380(0.6769)
2023/11/23 00:55:47 - INFO - root -   Epoch: [114/300][80/283], lr: 0.00000030 	 loss = 0.4488(0.6920)
2023/11/23 00:55:59 - INFO - root -   Epoch: [114/300][100/283], lr: 0.00000030 	 loss = 0.9580(0.6725)
2023/11/23 00:56:14 - INFO - root -   Epoch: [114/300][120/283], lr: 0.00000030 	 loss = 0.2750(0.6138)
2023/11/23 00:56:32 - INFO - root -   Epoch: [114/300][140/283], lr: 0.00000030 	 loss = 0.2046(0.5534)
2023/11/23 00:56:46 - INFO - root -   Epoch: [114/300][160/283], lr: 0.00000030 	 loss = 1.3076(0.5464)
2023/11/23 00:57:02 - INFO - root -   Epoch: [114/300][180/283], lr: 0.00000030 	 loss = 0.0333(0.5570)
2023/11/23 00:57:17 - INFO - root -   Epoch: [114/300][200/283], lr: 0.00000030 	 loss = 0.0525(0.5497)
2023/11/23 00:57:31 - INFO - root -   Epoch: [114/300][220/283], lr: 0.00000030 	 loss = 0.2430(0.5329)
2023/11/23 00:57:51 - INFO - root -   Epoch: [114/300][240/283], lr: 0.00000030 	 loss = 0.8970(0.5268)
2023/11/23 00:58:03 - INFO - root -   Epoch: [114/300][260/283], lr: 0.00000030 	 loss = 1.4161(0.5095)
2023/11/23 00:58:13 - INFO - root -   Epoch: [114/300][280/283], lr: 0.00000030 	 loss = 2.3716(0.4950)
2023/11/23 00:58:14 - INFO - root -   Epoch: [114/300] 	 loss = 0.4917
2023/11/23 01:01:08 - INFO - root -   precision = 0.8322
2023/11/23 01:01:08 - INFO - root -   eval_loss = 0.4820
2023/11/23 01:01:08 - INFO - root -   eval_acc = 0.8322
2023/11/23 01:01:09 - INFO - root -   train_accuracy = 0.8357
2023/11/23 01:01:11 - INFO - root -   Epoch: [115/300][0/283], lr: 0.00000030 	 loss = 0.0574(0.0574)
2023/11/23 01:01:32 - INFO - root -   Epoch: [115/300][20/283], lr: 0.00000030 	 loss = 2.1835(0.5898)
2023/11/23 01:01:40 - INFO - root -   Epoch: [115/300][40/283], lr: 0.00000030 	 loss = 0.0222(0.6127)
2023/11/23 01:01:53 - INFO - root -   Epoch: [115/300][60/283], lr: 0.00000030 	 loss = 0.2715(0.5916)
2023/11/23 01:02:07 - INFO - root -   Epoch: [115/300][80/283], lr: 0.00000030 	 loss = 0.1960(0.6054)
2023/11/23 01:02:21 - INFO - root -   Epoch: [115/300][100/283], lr: 0.00000030 	 loss = 0.1602(0.5522)
2023/11/23 01:02:37 - INFO - root -   Epoch: [115/300][120/283], lr: 0.00000030 	 loss = 0.1886(0.5300)
2023/11/23 01:02:54 - INFO - root -   Epoch: [115/300][140/283], lr: 0.00000030 	 loss = 0.0269(0.4947)
2023/11/23 01:03:10 - INFO - root -   Epoch: [115/300][160/283], lr: 0.00000030 	 loss = 1.7857(0.4945)
2023/11/23 01:03:28 - INFO - root -   Epoch: [115/300][180/283], lr: 0.00000030 	 loss = 0.4473(0.5136)
2023/11/23 01:03:39 - INFO - root -   Epoch: [115/300][200/283], lr: 0.00000030 	 loss = 0.2359(0.5134)
2023/11/23 01:03:54 - INFO - root -   Epoch: [115/300][220/283], lr: 0.00000030 	 loss = 0.3176(0.4933)
2023/11/23 01:04:15 - INFO - root -   Epoch: [115/300][240/283], lr: 0.00000030 	 loss = 0.3164(0.4921)
2023/11/23 01:04:27 - INFO - root -   Epoch: [115/300][260/283], lr: 0.00000030 	 loss = 1.1589(0.4835)
2023/11/23 01:04:39 - INFO - root -   Epoch: [115/300][280/283], lr: 0.00000030 	 loss = 3.2025(0.4830)
2023/11/23 01:04:40 - INFO - root -   Epoch: [115/300] 	 loss = 0.4809
2023/11/23 01:04:40 - INFO - root -   train_accuracy = 0.8286
2023/11/23 01:04:41 - INFO - root -   Epoch: [116/300][0/283], lr: 0.00000030 	 loss = 0.0787(0.0787)
2023/11/23 01:05:09 - INFO - root -   Epoch: [116/300][20/283], lr: 0.00000030 	 loss = 0.5348(0.6443)
2023/11/23 01:05:21 - INFO - root -   Epoch: [116/300][40/283], lr: 0.00000030 	 loss = 0.1798(0.6930)
2023/11/23 01:05:35 - INFO - root -   Epoch: [116/300][60/283], lr: 0.00000030 	 loss = 0.0385(0.6068)
2023/11/23 01:05:50 - INFO - root -   Epoch: [116/300][80/283], lr: 0.00000030 	 loss = 0.1942(0.6134)
2023/11/23 01:06:08 - INFO - root -   Epoch: [116/300][100/283], lr: 0.00000030 	 loss = 1.2479(0.5710)
2023/11/23 01:06:18 - INFO - root -   Epoch: [116/300][120/283], lr: 0.00000030 	 loss = 0.2351(0.5375)
2023/11/23 01:06:40 - INFO - root -   Epoch: [116/300][140/283], lr: 0.00000030 	 loss = 0.0544(0.4977)
2023/11/23 01:06:49 - INFO - root -   Epoch: [116/300][160/283], lr: 0.00000030 	 loss = 0.5792(0.5021)
2023/11/23 01:07:00 - INFO - root -   Epoch: [116/300][180/283], lr: 0.00000030 	 loss = 0.0481(0.4962)
2023/11/23 01:07:12 - INFO - root -   Epoch: [116/300][200/283], lr: 0.00000030 	 loss = 0.0601(0.4907)
2023/11/23 01:07:26 - INFO - root -   Epoch: [116/300][220/283], lr: 0.00000030 	 loss = 0.0887(0.4900)
2023/11/23 01:07:44 - INFO - root -   Epoch: [116/300][240/283], lr: 0.00000030 	 loss = 1.3654(0.5013)
2023/11/23 01:07:58 - INFO - root -   Epoch: [116/300][260/283], lr: 0.00000030 	 loss = 1.2652(0.4854)
2023/11/23 01:08:07 - INFO - root -   Epoch: [116/300][280/283], lr: 0.00000030 	 loss = 3.1459(0.4775)
2023/11/23 01:08:08 - INFO - root -   Epoch: [116/300] 	 loss = 0.4749
2023/11/23 01:08:08 - INFO - root -   train_accuracy = 0.8304
2023/11/23 01:08:17 - INFO - root -   Epoch: [117/300][0/283], lr: 0.00000030 	 loss = 0.2678(0.2678)
2023/11/23 01:08:35 - INFO - root -   Epoch: [117/300][20/283], lr: 0.00000030 	 loss = 1.3295(0.5002)
2023/11/23 01:08:49 - INFO - root -   Epoch: [117/300][40/283], lr: 0.00000030 	 loss = 0.1278(0.6296)
2023/11/23 01:08:57 - INFO - root -   Epoch: [117/300][60/283], lr: 0.00000030 	 loss = 0.0257(0.5689)
2023/11/23 01:09:15 - INFO - root -   Epoch: [117/300][80/283], lr: 0.00000030 	 loss = 0.0556(0.5688)
2023/11/23 01:09:32 - INFO - root -   Epoch: [117/300][100/283], lr: 0.00000030 	 loss = 0.1705(0.5389)
2023/11/23 01:09:44 - INFO - root -   Epoch: [117/300][120/283], lr: 0.00000030 	 loss = 0.1086(0.5067)
2023/11/23 01:10:09 - INFO - root -   Epoch: [117/300][140/283], lr: 0.00000030 	 loss = 0.0253(0.4838)
2023/11/23 01:10:17 - INFO - root -   Epoch: [117/300][160/283], lr: 0.00000030 	 loss = 0.4994(0.4935)
2023/11/23 01:10:28 - INFO - root -   Epoch: [117/300][180/283], lr: 0.00000030 	 loss = 0.0599(0.4938)
2023/11/23 01:10:41 - INFO - root -   Epoch: [117/300][200/283], lr: 0.00000030 	 loss = 0.3154(0.4820)
2023/11/23 01:10:56 - INFO - root -   Epoch: [117/300][220/283], lr: 0.00000030 	 loss = 0.1781(0.4675)
2023/11/23 01:11:11 - INFO - root -   Epoch: [117/300][240/283], lr: 0.00000030 	 loss = 0.7686(0.4723)
2023/11/23 01:11:27 - INFO - root -   Epoch: [117/300][260/283], lr: 0.00000030 	 loss = 1.0700(0.4617)
2023/11/23 01:11:46 - INFO - root -   Epoch: [117/300][280/283], lr: 0.00000030 	 loss = 2.6340(0.4576)
2023/11/23 01:11:47 - INFO - root -   Epoch: [117/300] 	 loss = 0.4549
2023/11/23 01:11:47 - INFO - root -   train_accuracy = 0.8410
2023/11/23 01:11:48 - INFO - root -   Epoch: [118/300][0/283], lr: 0.00000031 	 loss = 0.0824(0.0824)
2023/11/23 01:12:09 - INFO - root -   Epoch: [118/300][20/283], lr: 0.00000031 	 loss = 0.1667(0.5983)
2023/11/23 01:12:21 - INFO - root -   Epoch: [118/300][40/283], lr: 0.00000031 	 loss = 0.4037(0.6406)
2023/11/23 01:12:35 - INFO - root -   Epoch: [118/300][60/283], lr: 0.00000031 	 loss = 0.0318(0.6210)
2023/11/23 01:12:56 - INFO - root -   Epoch: [118/300][80/283], lr: 0.00000031 	 loss = 0.1289(0.6328)
2023/11/23 01:13:06 - INFO - root -   Epoch: [118/300][100/283], lr: 0.00000031 	 loss = 0.1213(0.5815)
2023/11/23 01:13:21 - INFO - root -   Epoch: [118/300][120/283], lr: 0.00000031 	 loss = 0.2587(0.5337)
2023/11/23 01:13:31 - INFO - root -   Epoch: [118/300][140/283], lr: 0.00000031 	 loss = 0.3021(0.5060)
2023/11/23 01:13:50 - INFO - root -   Epoch: [118/300][160/283], lr: 0.00000031 	 loss = 1.0116(0.4916)
2023/11/23 01:14:07 - INFO - root -   Epoch: [118/300][180/283], lr: 0.00000031 	 loss = 0.0228(0.5004)
2023/11/23 01:14:19 - INFO - root -   Epoch: [118/300][200/283], lr: 0.00000031 	 loss = 0.0862(0.4915)
2023/11/23 01:14:38 - INFO - root -   Epoch: [118/300][220/283], lr: 0.00000031 	 loss = 0.1334(0.4735)
2023/11/23 01:14:59 - INFO - root -   Epoch: [118/300][240/283], lr: 0.00000031 	 loss = 0.6446(0.4759)
2023/11/23 01:15:19 - INFO - root -   Epoch: [118/300][260/283], lr: 0.00000031 	 loss = 2.0404(0.4689)
2023/11/23 01:15:31 - INFO - root -   Epoch: [118/300][280/283], lr: 0.00000031 	 loss = 2.2490(0.4609)
2023/11/23 01:15:32 - INFO - root -   Epoch: [118/300] 	 loss = 0.4580
2023/11/23 01:15:32 - INFO - root -   train_accuracy = 0.8481
2023/11/23 01:15:40 - INFO - root -   Epoch: [119/300][0/283], lr: 0.00000031 	 loss = 0.1050(0.1050)
2023/11/23 01:15:54 - INFO - root -   Epoch: [119/300][20/283], lr: 0.00000031 	 loss = 1.2186(0.6200)
2023/11/23 01:16:07 - INFO - root -   Epoch: [119/300][40/283], lr: 0.00000031 	 loss = 0.1082(0.6691)
2023/11/23 01:16:25 - INFO - root -   Epoch: [119/300][60/283], lr: 0.00000031 	 loss = 0.0309(0.6146)
2023/11/23 01:16:33 - INFO - root -   Epoch: [119/300][80/283], lr: 0.00000031 	 loss = 0.1668(0.5993)
2023/11/23 01:16:48 - INFO - root -   Epoch: [119/300][100/283], lr: 0.00000031 	 loss = 0.4995(0.5771)
2023/11/23 01:17:04 - INFO - root -   Epoch: [119/300][120/283], lr: 0.00000031 	 loss = 0.0529(0.5297)
2023/11/23 01:17:20 - INFO - root -   Epoch: [119/300][140/283], lr: 0.00000031 	 loss = 0.0337(0.4883)
2023/11/23 01:17:37 - INFO - root -   Epoch: [119/300][160/283], lr: 0.00000031 	 loss = 0.7456(0.4817)
2023/11/23 01:17:57 - INFO - root -   Epoch: [119/300][180/283], lr: 0.00000031 	 loss = 0.1446(0.4920)
2023/11/23 01:18:11 - INFO - root -   Epoch: [119/300][200/283], lr: 0.00000031 	 loss = 0.1068(0.4785)
2023/11/23 01:18:23 - INFO - root -   Epoch: [119/300][220/283], lr: 0.00000031 	 loss = 0.2956(0.4698)
2023/11/23 01:18:34 - INFO - root -   Epoch: [119/300][240/283], lr: 0.00000031 	 loss = 0.3060(0.4636)
2023/11/23 01:18:49 - INFO - root -   Epoch: [119/300][260/283], lr: 0.00000031 	 loss = 1.2276(0.4508)
2023/11/23 01:18:58 - INFO - root -   Epoch: [119/300][280/283], lr: 0.00000031 	 loss = 3.1826(0.4491)
2023/11/23 01:18:58 - INFO - root -   Epoch: [119/300] 	 loss = 0.4466
2023/11/23 01:21:52 - INFO - root -   precision = 0.8112
2023/11/23 01:21:52 - INFO - root -   eval_loss = 0.4760
2023/11/23 01:21:52 - INFO - root -   eval_acc = 0.8112
2023/11/23 01:21:53 - INFO - root -   train_accuracy = 0.8463
2023/11/23 01:21:59 - INFO - root -   Epoch: [120/300][0/283], lr: 0.00000031 	 loss = 0.0837(0.0837)
2023/11/23 01:22:17 - INFO - root -   Epoch: [120/300][20/283], lr: 0.00000031 	 loss = 0.4100(0.5388)
2023/11/23 01:22:31 - INFO - root -   Epoch: [120/300][40/283], lr: 0.00000031 	 loss = 0.1875(0.6279)
2023/11/23 01:22:44 - INFO - root -   Epoch: [120/300][60/283], lr: 0.00000031 	 loss = 0.1070(0.5977)
2023/11/23 01:22:57 - INFO - root -   Epoch: [120/300][80/283], lr: 0.00000031 	 loss = 0.1442(0.5693)
2023/11/23 01:23:14 - INFO - root -   Epoch: [120/300][100/283], lr: 0.00000031 	 loss = 0.2272(0.5491)
2023/11/23 01:23:24 - INFO - root -   Epoch: [120/300][120/283], lr: 0.00000031 	 loss = 0.0446(0.5113)
2023/11/23 01:23:49 - INFO - root -   Epoch: [120/300][140/283], lr: 0.00000031 	 loss = 0.0982(0.4705)
2023/11/23 01:24:00 - INFO - root -   Epoch: [120/300][160/283], lr: 0.00000031 	 loss = 1.9076(0.4760)
2023/11/23 01:24:14 - INFO - root -   Epoch: [120/300][180/283], lr: 0.00000031 	 loss = 0.0488(0.4763)
2023/11/23 01:24:31 - INFO - root -   Epoch: [120/300][200/283], lr: 0.00000031 	 loss = 0.0495(0.4741)
2023/11/23 01:24:47 - INFO - root -   Epoch: [120/300][220/283], lr: 0.00000031 	 loss = 0.1252(0.4647)
2023/11/23 01:24:55 - INFO - root -   Epoch: [120/300][240/283], lr: 0.00000031 	 loss = 1.6902(0.4554)
2023/11/23 01:25:04 - INFO - root -   Epoch: [120/300][260/283], lr: 0.00000031 	 loss = 2.2502(0.4494)
2023/11/23 01:25:15 - INFO - root -   Epoch: [120/300][280/283], lr: 0.00000031 	 loss = 1.9381(0.4522)
2023/11/23 01:25:16 - INFO - root -   Epoch: [120/300] 	 loss = 0.4500
2023/11/23 01:25:16 - INFO - root -   train_accuracy = 0.8551
2023/11/23 01:25:24 - INFO - root -   Epoch: [121/300][0/283], lr: 0.00000031 	 loss = 0.2964(0.2964)
2023/11/23 01:25:41 - INFO - root -   Epoch: [121/300][20/283], lr: 0.00000031 	 loss = 0.3434(0.5272)
2023/11/23 01:25:54 - INFO - root -   Epoch: [121/300][40/283], lr: 0.00000031 	 loss = 0.4661(0.7468)
2023/11/23 01:26:14 - INFO - root -   Epoch: [121/300][60/283], lr: 0.00000031 	 loss = 0.2731(0.7464)
2023/11/23 01:26:36 - INFO - root -   Epoch: [121/300][80/283], lr: 0.00000031 	 loss = 0.1104(0.7554)
2023/11/23 01:26:48 - INFO - root -   Epoch: [121/300][100/283], lr: 0.00000031 	 loss = 0.1320(0.6790)
2023/11/23 01:26:56 - INFO - root -   Epoch: [121/300][120/283], lr: 0.00000031 	 loss = 0.0537(0.6097)
2023/11/23 01:27:07 - INFO - root -   Epoch: [121/300][140/283], lr: 0.00000031 	 loss = 0.0369(0.5723)
2023/11/23 01:27:22 - INFO - root -   Epoch: [121/300][160/283], lr: 0.00000031 	 loss = 0.5913(0.5595)
2023/11/23 01:27:43 - INFO - root -   Epoch: [121/300][180/283], lr: 0.00000031 	 loss = 0.2745(0.5607)
2023/11/23 01:28:02 - INFO - root -   Epoch: [121/300][200/283], lr: 0.00000031 	 loss = 0.0965(0.5488)
2023/11/23 01:28:20 - INFO - root -   Epoch: [121/300][220/283], lr: 0.00000031 	 loss = 0.0881(0.5326)
2023/11/23 01:28:34 - INFO - root -   Epoch: [121/300][240/283], lr: 0.00000031 	 loss = 1.0418(0.5356)
2023/11/23 01:28:43 - INFO - root -   Epoch: [121/300][260/283], lr: 0.00000031 	 loss = 1.2196(0.5163)
2023/11/23 01:28:57 - INFO - root -   Epoch: [121/300][280/283], lr: 0.00000031 	 loss = 3.2372(0.5073)
2023/11/23 01:28:58 - INFO - root -   Epoch: [121/300] 	 loss = 0.5039
2023/11/23 01:28:58 - INFO - root -   train_accuracy = 0.8304
2023/11/23 01:28:59 - INFO - root -   Epoch: [122/300][0/283], lr: 0.00000031 	 loss = 0.0217(0.0217)
2023/11/23 01:29:16 - INFO - root -   Epoch: [122/300][20/283], lr: 0.00000031 	 loss = 0.9710(0.5857)
2023/11/23 01:29:32 - INFO - root -   Epoch: [122/300][40/283], lr: 0.00000031 	 loss = 0.0793(0.7772)
2023/11/23 01:29:50 - INFO - root -   Epoch: [122/300][60/283], lr: 0.00000031 	 loss = 0.0170(0.7126)
2023/11/23 01:30:00 - INFO - root -   Epoch: [122/300][80/283], lr: 0.00000031 	 loss = 0.2998(0.6849)
2023/11/23 01:30:13 - INFO - root -   Epoch: [122/300][100/283], lr: 0.00000031 	 loss = 0.1330(0.6219)
2023/11/23 01:30:31 - INFO - root -   Epoch: [122/300][120/283], lr: 0.00000031 	 loss = 0.1298(0.5772)
2023/11/23 01:30:47 - INFO - root -   Epoch: [122/300][140/283], lr: 0.00000031 	 loss = 0.0355(0.5462)
2023/11/23 01:31:01 - INFO - root -   Epoch: [122/300][160/283], lr: 0.00000031 	 loss = 0.6973(0.5162)
2023/11/23 01:31:14 - INFO - root -   Epoch: [122/300][180/283], lr: 0.00000031 	 loss = 0.1545(0.5134)
2023/11/23 01:31:29 - INFO - root -   Epoch: [122/300][200/283], lr: 0.00000031 	 loss = 0.0951(0.4944)
2023/11/23 01:31:42 - INFO - root -   Epoch: [122/300][220/283], lr: 0.00000031 	 loss = 0.1475(0.4770)
2023/11/23 01:31:57 - INFO - root -   Epoch: [122/300][240/283], lr: 0.00000031 	 loss = 0.9535(0.4761)
2023/11/23 01:32:06 - INFO - root -   Epoch: [122/300][260/283], lr: 0.00000031 	 loss = 0.5696(0.4700)
2023/11/23 01:32:22 - INFO - root -   Epoch: [122/300][280/283], lr: 0.00000031 	 loss = 2.5969(0.4567)
2023/11/23 01:32:23 - INFO - root -   Epoch: [122/300] 	 loss = 0.4543
2023/11/23 01:32:23 - INFO - root -   train_accuracy = 0.8286
2023/11/23 01:32:30 - INFO - root -   Epoch: [123/300][0/283], lr: 0.00000032 	 loss = 0.0347(0.0347)
2023/11/23 01:32:40 - INFO - root -   Epoch: [123/300][20/283], lr: 0.00000032 	 loss = 0.6669(0.6241)
2023/11/23 01:32:56 - INFO - root -   Epoch: [123/300][40/283], lr: 0.00000032 	 loss = 0.1445(0.6342)
2023/11/23 01:33:08 - INFO - root -   Epoch: [123/300][60/283], lr: 0.00000032 	 loss = 0.0289(0.5696)
2023/11/23 01:33:18 - INFO - root -   Epoch: [123/300][80/283], lr: 0.00000032 	 loss = 0.1832(0.5657)
2023/11/23 01:33:37 - INFO - root -   Epoch: [123/300][100/283], lr: 0.00000032 	 loss = 0.1941(0.5467)
2023/11/23 01:33:45 - INFO - root -   Epoch: [123/300][120/283], lr: 0.00000032 	 loss = 0.1042(0.4904)
2023/11/23 01:33:53 - INFO - root -   Epoch: [123/300][140/283], lr: 0.00000032 	 loss = 0.0333(0.4606)
2023/11/23 01:34:14 - INFO - root -   Epoch: [123/300][160/283], lr: 0.00000032 	 loss = 0.7205(0.4564)
2023/11/23 01:34:28 - INFO - root -   Epoch: [123/300][180/283], lr: 0.00000032 	 loss = 0.2396(0.4758)
2023/11/23 01:34:42 - INFO - root -   Epoch: [123/300][200/283], lr: 0.00000032 	 loss = 0.1451(0.4698)
2023/11/23 01:34:56 - INFO - root -   Epoch: [123/300][220/283], lr: 0.00000032 	 loss = 0.1116(0.4686)
2023/11/23 01:35:05 - INFO - root -   Epoch: [123/300][240/283], lr: 0.00000032 	 loss = 0.2868(0.4712)
2023/11/23 01:35:13 - INFO - root -   Epoch: [123/300][260/283], lr: 0.00000032 	 loss = 0.7788(0.4586)
2023/11/23 01:35:26 - INFO - root -   Epoch: [123/300][280/283], lr: 0.00000032 	 loss = 3.1514(0.4524)
2023/11/23 01:35:27 - INFO - root -   Epoch: [123/300] 	 loss = 0.4499
2023/11/23 01:35:27 - INFO - root -   train_accuracy = 0.8410
2023/11/23 01:35:36 - INFO - root -   Epoch: [124/300][0/283], lr: 0.00000032 	 loss = 0.0309(0.0309)
2023/11/23 01:35:47 - INFO - root -   Epoch: [124/300][20/283], lr: 0.00000032 	 loss = 0.3763(0.5916)
2023/11/23 01:36:00 - INFO - root -   Epoch: [124/300][40/283], lr: 0.00000032 	 loss = 0.1297(0.6577)
2023/11/23 01:36:11 - INFO - root -   Epoch: [124/300][60/283], lr: 0.00000032 	 loss = 0.4805(0.5910)
2023/11/23 01:36:26 - INFO - root -   Epoch: [124/300][80/283], lr: 0.00000032 	 loss = 0.2403(0.5889)
2023/11/23 01:36:38 - INFO - root -   Epoch: [124/300][100/283], lr: 0.00000032 	 loss = 0.1100(0.5472)
2023/11/23 01:36:59 - INFO - root -   Epoch: [124/300][120/283], lr: 0.00000032 	 loss = 0.5536(0.5094)
2023/11/23 01:37:11 - INFO - root -   Epoch: [124/300][140/283], lr: 0.00000032 	 loss = 0.0209(0.4790)
2023/11/23 01:37:37 - INFO - root -   Epoch: [124/300][160/283], lr: 0.00000032 	 loss = 0.4163(0.4632)
2023/11/23 01:37:48 - INFO - root -   Epoch: [124/300][180/283], lr: 0.00000032 	 loss = 0.0976(0.4577)
2023/11/23 01:38:11 - INFO - root -   Epoch: [124/300][200/283], lr: 0.00000032 	 loss = 0.0826(0.4629)
2023/11/23 01:38:22 - INFO - root -   Epoch: [124/300][220/283], lr: 0.00000032 	 loss = 0.2266(0.4485)
2023/11/23 01:38:41 - INFO - root -   Epoch: [124/300][240/283], lr: 0.00000032 	 loss = 1.5888(0.4556)
2023/11/23 01:39:02 - INFO - root -   Epoch: [124/300][260/283], lr: 0.00000032 	 loss = 2.2522(0.4484)
2023/11/23 01:39:17 - INFO - root -   Epoch: [124/300][280/283], lr: 0.00000032 	 loss = 2.8750(0.4420)
2023/11/23 01:39:17 - INFO - root -   Epoch: [124/300] 	 loss = 0.4394
2023/11/23 01:42:09 - INFO - root -   precision = 0.8182
2023/11/23 01:42:09 - INFO - root -   eval_loss = 0.4982
2023/11/23 01:42:09 - INFO - root -   eval_acc = 0.8182
2023/11/23 01:42:10 - INFO - root -   train_accuracy = 0.8551
2023/11/23 01:42:20 - INFO - root -   Epoch: [125/300][0/283], lr: 0.00000032 	 loss = 0.0893(0.0893)
2023/11/23 01:42:36 - INFO - root -   Epoch: [125/300][20/283], lr: 0.00000032 	 loss = 0.2777(0.5663)
2023/11/23 01:42:49 - INFO - root -   Epoch: [125/300][40/283], lr: 0.00000032 	 loss = 0.0991(0.6553)
2023/11/23 01:42:57 - INFO - root -   Epoch: [125/300][60/283], lr: 0.00000032 	 loss = 0.0146(0.5702)
2023/11/23 01:43:14 - INFO - root -   Epoch: [125/300][80/283], lr: 0.00000032 	 loss = 0.3718(0.5769)
2023/11/23 01:43:27 - INFO - root -   Epoch: [125/300][100/283], lr: 0.00000032 	 loss = 0.2463(0.5360)
2023/11/23 01:43:40 - INFO - root -   Epoch: [125/300][120/283], lr: 0.00000032 	 loss = 0.3974(0.5154)
2023/11/23 01:43:56 - INFO - root -   Epoch: [125/300][140/283], lr: 0.00000032 	 loss = 0.1321(0.4859)
2023/11/23 01:44:14 - INFO - root -   Epoch: [125/300][160/283], lr: 0.00000032 	 loss = 1.0520(0.4730)
2023/11/23 01:44:22 - INFO - root -   Epoch: [125/300][180/283], lr: 0.00000032 	 loss = 0.1251(0.4831)
2023/11/23 01:44:35 - INFO - root -   Epoch: [125/300][200/283], lr: 0.00000032 	 loss = 0.2765(0.4815)
2023/11/23 01:44:51 - INFO - root -   Epoch: [125/300][220/283], lr: 0.00000032 	 loss = 0.0600(0.4723)
2023/11/23 01:45:04 - INFO - root -   Epoch: [125/300][240/283], lr: 0.00000032 	 loss = 0.6545(0.4706)
2023/11/23 01:45:21 - INFO - root -   Epoch: [125/300][260/283], lr: 0.00000032 	 loss = 1.3360(0.4629)
2023/11/23 01:45:34 - INFO - root -   Epoch: [125/300][280/283], lr: 0.00000032 	 loss = 3.1467(0.4608)
2023/11/23 01:45:35 - INFO - root -   Epoch: [125/300] 	 loss = 0.4580
2023/11/23 01:45:35 - INFO - root -   train_accuracy = 0.8322
2023/11/23 01:45:36 - INFO - root -   Epoch: [126/300][0/283], lr: 0.00000032 	 loss = 0.0342(0.0342)
2023/11/23 01:45:58 - INFO - root -   Epoch: [126/300][20/283], lr: 0.00000032 	 loss = 1.7514(0.5725)
2023/11/23 01:46:14 - INFO - root -   Epoch: [126/300][40/283], lr: 0.00000032 	 loss = 0.1439(0.6575)
2023/11/23 01:46:29 - INFO - root -   Epoch: [126/300][60/283], lr: 0.00000032 	 loss = 0.0354(0.6194)
2023/11/23 01:46:37 - INFO - root -   Epoch: [126/300][80/283], lr: 0.00000032 	 loss = 0.1061(0.6220)
2023/11/23 01:46:48 - INFO - root -   Epoch: [126/300][100/283], lr: 0.00000032 	 loss = 0.0720(0.5817)
2023/11/23 01:46:59 - INFO - root -   Epoch: [126/300][120/283], lr: 0.00000032 	 loss = 0.4360(0.5360)
2023/11/23 01:47:13 - INFO - root -   Epoch: [126/300][140/283], lr: 0.00000032 	 loss = 0.0163(0.4950)
2023/11/23 01:47:22 - INFO - root -   Epoch: [126/300][160/283], lr: 0.00000032 	 loss = 0.3919(0.4750)
2023/11/23 01:47:30 - INFO - root -   Epoch: [126/300][180/283], lr: 0.00000032 	 loss = 0.1252(0.4784)
2023/11/23 01:47:44 - INFO - root -   Epoch: [126/300][200/283], lr: 0.00000032 	 loss = 0.1092(0.4721)
2023/11/23 01:47:55 - INFO - root -   Epoch: [126/300][220/283], lr: 0.00000032 	 loss = 0.1237(0.4608)
2023/11/23 01:48:09 - INFO - root -   Epoch: [126/300][240/283], lr: 0.00000032 	 loss = 1.5258(0.4652)
2023/11/23 01:48:21 - INFO - root -   Epoch: [126/300][260/283], lr: 0.00000032 	 loss = 0.6135(0.4535)
2023/11/23 01:48:38 - INFO - root -   Epoch: [126/300][280/283], lr: 0.00000032 	 loss = 2.8032(0.4473)
2023/11/23 01:48:39 - INFO - root -   Epoch: [126/300] 	 loss = 0.4445
2023/11/23 01:48:39 - INFO - root -   train_accuracy = 0.8392
2023/11/23 01:48:48 - INFO - root -   Epoch: [127/300][0/283], lr: 0.00000032 	 loss = 0.1662(0.1662)
2023/11/23 01:49:04 - INFO - root -   Epoch: [127/300][20/283], lr: 0.00000032 	 loss = 0.7511(0.6918)
2023/11/23 01:49:15 - INFO - root -   Epoch: [127/300][40/283], lr: 0.00000032 	 loss = 0.2677(0.7488)
2023/11/23 01:49:34 - INFO - root -   Epoch: [127/300][60/283], lr: 0.00000032 	 loss = 0.0099(0.6819)
2023/11/23 01:49:50 - INFO - root -   Epoch: [127/300][80/283], lr: 0.00000032 	 loss = 0.0565(0.6687)
2023/11/23 01:50:05 - INFO - root -   Epoch: [127/300][100/283], lr: 0.00000032 	 loss = 0.1631(0.6172)
2023/11/23 01:50:19 - INFO - root -   Epoch: [127/300][120/283], lr: 0.00000032 	 loss = 0.0270(0.5754)
2023/11/23 01:50:38 - INFO - root -   Epoch: [127/300][140/283], lr: 0.00000032 	 loss = 0.0272(0.5238)
2023/11/23 01:50:56 - INFO - root -   Epoch: [127/300][160/283], lr: 0.00000032 	 loss = 1.5573(0.5218)
2023/11/23 01:51:11 - INFO - root -   Epoch: [127/300][180/283], lr: 0.00000032 	 loss = 0.4743(0.5308)
2023/11/23 01:51:24 - INFO - root -   Epoch: [127/300][200/283], lr: 0.00000032 	 loss = 0.2731(0.5101)
2023/11/23 01:51:40 - INFO - root -   Epoch: [127/300][220/283], lr: 0.00000032 	 loss = 0.3174(0.4871)
2023/11/23 01:51:49 - INFO - root -   Epoch: [127/300][240/283], lr: 0.00000032 	 loss = 0.5923(0.4839)
2023/11/23 01:52:11 - INFO - root -   Epoch: [127/300][260/283], lr: 0.00000032 	 loss = 0.6605(0.4804)
2023/11/23 01:52:23 - INFO - root -   Epoch: [127/300][280/283], lr: 0.00000032 	 loss = 3.2405(0.4702)
2023/11/23 01:52:24 - INFO - root -   Epoch: [127/300] 	 loss = 0.4678
2023/11/23 01:52:24 - INFO - root -   train_accuracy = 0.8339
2023/11/23 01:52:25 - INFO - root -   Epoch: [128/300][0/283], lr: 0.00000032 	 loss = 0.0228(0.0228)
2023/11/23 01:52:39 - INFO - root -   Epoch: [128/300][20/283], lr: 0.00000032 	 loss = 1.3530(0.5996)
2023/11/23 01:52:47 - INFO - root -   Epoch: [128/300][40/283], lr: 0.00000032 	 loss = 0.0950(0.6282)
2023/11/23 01:53:01 - INFO - root -   Epoch: [128/300][60/283], lr: 0.00000032 	 loss = 0.1450(0.5789)
2023/11/23 01:53:09 - INFO - root -   Epoch: [128/300][80/283], lr: 0.00000032 	 loss = 0.3195(0.5758)
2023/11/23 01:53:27 - INFO - root -   Epoch: [128/300][100/283], lr: 0.00000032 	 loss = 0.4394(0.5721)
2023/11/23 01:53:43 - INFO - root -   Epoch: [128/300][120/283], lr: 0.00000032 	 loss = 0.0603(0.5482)
2023/11/23 01:53:55 - INFO - root -   Epoch: [128/300][140/283], lr: 0.00000032 	 loss = 0.0346(0.5053)
2023/11/23 01:54:19 - INFO - root -   Epoch: [128/300][160/283], lr: 0.00000032 	 loss = 2.4200(0.5028)
2023/11/23 01:54:30 - INFO - root -   Epoch: [128/300][180/283], lr: 0.00000032 	 loss = 0.0505(0.5103)
2023/11/23 01:54:52 - INFO - root -   Epoch: [128/300][200/283], lr: 0.00000032 	 loss = 0.1018(0.5002)
2023/11/23 01:55:08 - INFO - root -   Epoch: [128/300][220/283], lr: 0.00000032 	 loss = 0.1775(0.4896)
2023/11/23 01:55:25 - INFO - root -   Epoch: [128/300][240/283], lr: 0.00000032 	 loss = 0.7850(0.4988)
2023/11/23 01:55:43 - INFO - root -   Epoch: [128/300][260/283], lr: 0.00000032 	 loss = 1.8249(0.4912)
2023/11/23 01:55:52 - INFO - root -   Epoch: [128/300][280/283], lr: 0.00000032 	 loss = 3.3208(0.4839)
2023/11/23 01:55:53 - INFO - root -   Epoch: [128/300] 	 loss = 0.4809
2023/11/23 01:55:53 - INFO - root -   train_accuracy = 0.8339
2023/11/23 01:56:02 - INFO - root -   Epoch: [129/300][0/283], lr: 0.00000033 	 loss = 0.1812(0.1812)
2023/11/23 01:56:23 - INFO - root -   Epoch: [129/300][20/283], lr: 0.00000033 	 loss = 1.0621(0.6186)
2023/11/23 01:56:35 - INFO - root -   Epoch: [129/300][40/283], lr: 0.00000033 	 loss = 0.3549(0.6893)
2023/11/23 01:56:57 - INFO - root -   Epoch: [129/300][60/283], lr: 0.00000033 	 loss = 0.1447(0.6527)
2023/11/23 01:57:07 - INFO - root -   Epoch: [129/300][80/283], lr: 0.00000033 	 loss = 0.0803(0.6366)
2023/11/23 01:57:27 - INFO - root -   Epoch: [129/300][100/283], lr: 0.00000033 	 loss = 0.2322(0.6016)
2023/11/23 01:57:37 - INFO - root -   Epoch: [129/300][120/283], lr: 0.00000033 	 loss = 0.0591(0.5460)
2023/11/23 01:57:54 - INFO - root -   Epoch: [129/300][140/283], lr: 0.00000033 	 loss = 0.1471(0.4999)
2023/11/23 01:58:10 - INFO - root -   Epoch: [129/300][160/283], lr: 0.00000033 	 loss = 1.4093(0.4879)
2023/11/23 01:58:25 - INFO - root -   Epoch: [129/300][180/283], lr: 0.00000033 	 loss = 0.0661(0.4892)
2023/11/23 01:58:41 - INFO - root -   Epoch: [129/300][200/283], lr: 0.00000033 	 loss = 0.1733(0.4765)
2023/11/23 01:58:56 - INFO - root -   Epoch: [129/300][220/283], lr: 0.00000033 	 loss = 0.0824(0.4771)
2023/11/23 01:59:04 - INFO - root -   Epoch: [129/300][240/283], lr: 0.00000033 	 loss = 1.2292(0.4694)
2023/11/23 01:59:24 - INFO - root -   Epoch: [129/300][260/283], lr: 0.00000033 	 loss = 3.0149(0.4717)
2023/11/23 01:59:38 - INFO - root -   Epoch: [129/300][280/283], lr: 0.00000033 	 loss = 1.7051(0.4572)
2023/11/23 01:59:39 - INFO - root -   Epoch: [129/300] 	 loss = 0.4543
2023/11/23 02:02:30 - INFO - root -   precision = 0.8182
2023/11/23 02:02:30 - INFO - root -   eval_loss = 0.5079
2023/11/23 02:02:30 - INFO - root -   eval_acc = 0.8182
2023/11/23 02:02:31 - INFO - root -   train_accuracy = 0.8445
2023/11/23 02:02:37 - INFO - root -   Epoch: [130/300][0/283], lr: 0.00000033 	 loss = 0.0940(0.0940)
2023/11/23 02:02:50 - INFO - root -   Epoch: [130/300][20/283], lr: 0.00000033 	 loss = 0.2109(0.4866)
2023/11/23 02:03:01 - INFO - root -   Epoch: [130/300][40/283], lr: 0.00000033 	 loss = 0.1976(0.5887)
2023/11/23 02:03:15 - INFO - root -   Epoch: [130/300][60/283], lr: 0.00000033 	 loss = 0.3152(0.6023)
2023/11/23 02:03:29 - INFO - root -   Epoch: [130/300][80/283], lr: 0.00000033 	 loss = 0.0312(0.5974)
2023/11/23 02:03:47 - INFO - root -   Epoch: [130/300][100/283], lr: 0.00000033 	 loss = 0.1459(0.5457)
2023/11/23 02:04:01 - INFO - root -   Epoch: [130/300][120/283], lr: 0.00000033 	 loss = 0.0915(0.5158)
2023/11/23 02:04:11 - INFO - root -   Epoch: [130/300][140/283], lr: 0.00000033 	 loss = 0.0302(0.4771)
2023/11/23 02:04:28 - INFO - root -   Epoch: [130/300][160/283], lr: 0.00000033 	 loss = 0.3819(0.4621)
2023/11/23 02:04:48 - INFO - root -   Epoch: [130/300][180/283], lr: 0.00000033 	 loss = 0.1131(0.4734)
2023/11/23 02:05:00 - INFO - root -   Epoch: [130/300][200/283], lr: 0.00000033 	 loss = 0.1737(0.4675)
2023/11/23 02:05:25 - INFO - root -   Epoch: [130/300][220/283], lr: 0.00000033 	 loss = 0.1150(0.4556)
2023/11/23 02:05:36 - INFO - root -   Epoch: [130/300][240/283], lr: 0.00000033 	 loss = 0.8404(0.4614)
2023/11/23 02:05:53 - INFO - root -   Epoch: [130/300][260/283], lr: 0.00000033 	 loss = 1.9584(0.4515)
2023/11/23 02:06:07 - INFO - root -   Epoch: [130/300][280/283], lr: 0.00000033 	 loss = 3.0590(0.4505)
2023/11/23 02:06:08 - INFO - root -   Epoch: [130/300] 	 loss = 0.4484
2023/11/23 02:06:08 - INFO - root -   train_accuracy = 0.8445
2023/11/23 02:06:16 - INFO - root -   Epoch: [131/300][0/283], lr: 0.00000033 	 loss = 0.0947(0.0947)
2023/11/23 02:06:37 - INFO - root -   Epoch: [131/300][20/283], lr: 0.00000033 	 loss = 2.6050(0.7367)
2023/11/23 02:06:50 - INFO - root -   Epoch: [131/300][40/283], lr: 0.00000033 	 loss = 0.3376(0.6484)
2023/11/23 02:07:00 - INFO - root -   Epoch: [131/300][60/283], lr: 0.00000033 	 loss = 0.1508(0.5899)
2023/11/23 02:07:12 - INFO - root -   Epoch: [131/300][80/283], lr: 0.00000033 	 loss = 1.0402(0.6116)
2023/11/23 02:07:27 - INFO - root -   Epoch: [131/300][100/283], lr: 0.00000033 	 loss = 0.3174(0.5903)
2023/11/23 02:07:41 - INFO - root -   Epoch: [131/300][120/283], lr: 0.00000033 	 loss = 0.0552(0.5448)
2023/11/23 02:07:58 - INFO - root -   Epoch: [131/300][140/283], lr: 0.00000033 	 loss = 0.0192(0.5069)
2023/11/23 02:08:16 - INFO - root -   Epoch: [131/300][160/283], lr: 0.00000033 	 loss = 1.6344(0.4957)
2023/11/23 02:08:29 - INFO - root -   Epoch: [131/300][180/283], lr: 0.00000033 	 loss = 0.0295(0.4983)
2023/11/23 02:08:50 - INFO - root -   Epoch: [131/300][200/283], lr: 0.00000033 	 loss = 0.1189(0.4790)
2023/11/23 02:09:06 - INFO - root -   Epoch: [131/300][220/283], lr: 0.00000033 	 loss = 0.0901(0.4738)
2023/11/23 02:09:20 - INFO - root -   Epoch: [131/300][240/283], lr: 0.00000033 	 loss = 1.0506(0.4682)
2023/11/23 02:09:38 - INFO - root -   Epoch: [131/300][260/283], lr: 0.00000033 	 loss = 1.9356(0.4593)
2023/11/23 02:09:53 - INFO - root -   Epoch: [131/300][280/283], lr: 0.00000033 	 loss = 1.1382(0.4426)
2023/11/23 02:09:54 - INFO - root -   Epoch: [131/300] 	 loss = 0.4398
2023/11/23 02:09:54 - INFO - root -   train_accuracy = 0.8463
2023/11/23 02:09:55 - INFO - root -   Epoch: [132/300][0/283], lr: 0.00000033 	 loss = 0.0943(0.0943)
2023/11/23 02:10:21 - INFO - root -   Epoch: [132/300][20/283], lr: 0.00000033 	 loss = 2.7730(0.7572)
2023/11/23 02:10:29 - INFO - root -   Epoch: [132/300][40/283], lr: 0.00000033 	 loss = 0.0322(0.7044)
2023/11/23 02:10:44 - INFO - root -   Epoch: [132/300][60/283], lr: 0.00000033 	 loss = 0.0125(0.6442)
2023/11/23 02:10:52 - INFO - root -   Epoch: [132/300][80/283], lr: 0.00000033 	 loss = 0.0556(0.6050)
2023/11/23 02:11:09 - INFO - root -   Epoch: [132/300][100/283], lr: 0.00000033 	 loss = 0.5457(0.5647)
2023/11/23 02:11:17 - INFO - root -   Epoch: [132/300][120/283], lr: 0.00000033 	 loss = 0.1516(0.5382)
2023/11/23 02:11:29 - INFO - root -   Epoch: [132/300][140/283], lr: 0.00000033 	 loss = 0.1369(0.4917)
2023/11/23 02:11:54 - INFO - root -   Epoch: [132/300][160/283], lr: 0.00000033 	 loss = 1.7031(0.4987)
2023/11/23 02:12:02 - INFO - root -   Epoch: [132/300][180/283], lr: 0.00000033 	 loss = 0.0534(0.4929)
2023/11/23 02:12:23 - INFO - root -   Epoch: [132/300][200/283], lr: 0.00000033 	 loss = 0.0797(0.4869)
2023/11/23 02:12:39 - INFO - root -   Epoch: [132/300][220/283], lr: 0.00000033 	 loss = 0.1229(0.4677)
2023/11/23 02:12:51 - INFO - root -   Epoch: [132/300][240/283], lr: 0.00000033 	 loss = 1.6101(0.4667)
2023/11/23 02:13:08 - INFO - root -   Epoch: [132/300][260/283], lr: 0.00000033 	 loss = 0.4344(0.4505)
2023/11/23 02:13:20 - INFO - root -   Epoch: [132/300][280/283], lr: 0.00000033 	 loss = 1.9789(0.4362)
2023/11/23 02:13:21 - INFO - root -   Epoch: [132/300] 	 loss = 0.4345
2023/11/23 02:13:21 - INFO - root -   train_accuracy = 0.8481
2023/11/23 02:13:23 - INFO - root -   Epoch: [133/300][0/283], lr: 0.00000033 	 loss = 0.0268(0.0268)
2023/11/23 02:13:44 - INFO - root -   Epoch: [133/300][20/283], lr: 0.00000033 	 loss = 0.6031(0.6170)
2023/11/23 02:14:02 - INFO - root -   Epoch: [133/300][40/283], lr: 0.00000033 	 loss = 0.0568(0.6868)
2023/11/23 02:14:15 - INFO - root -   Epoch: [133/300][60/283], lr: 0.00000033 	 loss = 0.0911(0.6452)
2023/11/23 02:14:28 - INFO - root -   Epoch: [133/300][80/283], lr: 0.00000033 	 loss = 0.1374(0.6200)
2023/11/23 02:14:50 - INFO - root -   Epoch: [133/300][100/283], lr: 0.00000033 	 loss = 0.1126(0.5612)
2023/11/23 02:15:04 - INFO - root -   Epoch: [133/300][120/283], lr: 0.00000033 	 loss = 0.0365(0.5135)
2023/11/23 02:15:20 - INFO - root -   Epoch: [133/300][140/283], lr: 0.00000033 	 loss = 0.1130(0.4833)
2023/11/23 02:15:31 - INFO - root -   Epoch: [133/300][160/283], lr: 0.00000033 	 loss = 0.3976(0.4775)
2023/11/23 02:15:48 - INFO - root -   Epoch: [133/300][180/283], lr: 0.00000033 	 loss = 0.0896(0.4916)
2023/11/23 02:16:05 - INFO - root -   Epoch: [133/300][200/283], lr: 0.00000033 	 loss = 0.5497(0.4874)
2023/11/23 02:16:22 - INFO - root -   Epoch: [133/300][220/283], lr: 0.00000033 	 loss = 0.1599(0.4769)
2023/11/23 02:16:45 - INFO - root -   Epoch: [133/300][240/283], lr: 0.00000033 	 loss = 0.1823(0.4814)
2023/11/23 02:17:00 - INFO - root -   Epoch: [133/300][260/283], lr: 0.00000033 	 loss = 0.6564(0.4670)
2023/11/23 02:17:10 - INFO - root -   Epoch: [133/300][280/283], lr: 0.00000033 	 loss = 3.0168(0.4655)
2023/11/23 02:17:11 - INFO - root -   Epoch: [133/300] 	 loss = 0.4623
2023/11/23 02:17:11 - INFO - root -   train_accuracy = 0.8428
2023/11/23 02:17:12 - INFO - root -   Epoch: [134/300][0/283], lr: 0.00000033 	 loss = 0.0254(0.0254)
2023/11/23 02:17:33 - INFO - root -   Epoch: [134/300][20/283], lr: 0.00000033 	 loss = 0.3149(0.5475)
2023/11/23 02:17:50 - INFO - root -   Epoch: [134/300][40/283], lr: 0.00000033 	 loss = 0.0325(0.6511)
2023/11/23 02:18:12 - INFO - root -   Epoch: [134/300][60/283], lr: 0.00000033 	 loss = 0.1723(0.6364)
2023/11/23 02:18:22 - INFO - root -   Epoch: [134/300][80/283], lr: 0.00000033 	 loss = 0.0882(0.6385)
2023/11/23 02:18:34 - INFO - root -   Epoch: [134/300][100/283], lr: 0.00000033 	 loss = 0.0731(0.5814)
2023/11/23 02:18:42 - INFO - root -   Epoch: [134/300][120/283], lr: 0.00000033 	 loss = 0.0339(0.5299)
2023/11/23 02:18:59 - INFO - root -   Epoch: [134/300][140/283], lr: 0.00000033 	 loss = 0.0567(0.4847)
2023/11/23 02:19:17 - INFO - root -   Epoch: [134/300][160/283], lr: 0.00000033 	 loss = 0.3801(0.4746)
2023/11/23 02:19:32 - INFO - root -   Epoch: [134/300][180/283], lr: 0.00000033 	 loss = 0.0894(0.4767)
2023/11/23 02:19:44 - INFO - root -   Epoch: [134/300][200/283], lr: 0.00000033 	 loss = 0.1476(0.4582)
2023/11/23 02:19:55 - INFO - root -   Epoch: [134/300][220/283], lr: 0.00000033 	 loss = 0.2384(0.4517)
2023/11/23 02:20:15 - INFO - root -   Epoch: [134/300][240/283], lr: 0.00000033 	 loss = 1.3168(0.4491)
2023/11/23 02:20:25 - INFO - root -   Epoch: [134/300][260/283], lr: 0.00000033 	 loss = 1.1373(0.4394)
2023/11/23 02:20:45 - INFO - root -   Epoch: [134/300][280/283], lr: 0.00000033 	 loss = 1.3861(0.4347)
2023/11/23 02:20:46 - INFO - root -   Epoch: [134/300] 	 loss = 0.4319
2023/11/23 02:23:39 - INFO - root -   precision = 0.8322
2023/11/23 02:23:39 - INFO - root -   eval_loss = 0.5059
2023/11/23 02:23:39 - INFO - root -   eval_acc = 0.8322
2023/11/23 02:23:40 - INFO - root -   train_accuracy = 0.8481
2023/11/23 02:23:47 - INFO - root -   Epoch: [135/300][0/283], lr: 0.00000034 	 loss = 0.2564(0.2564)
2023/11/23 02:23:59 - INFO - root -   Epoch: [135/300][20/283], lr: 0.00000034 	 loss = 1.2177(0.5292)
2023/11/23 02:24:15 - INFO - root -   Epoch: [135/300][40/283], lr: 0.00000034 	 loss = 0.4842(0.5998)
2023/11/23 02:24:29 - INFO - root -   Epoch: [135/300][60/283], lr: 0.00000034 	 loss = 0.0763(0.5365)
2023/11/23 02:24:40 - INFO - root -   Epoch: [135/300][80/283], lr: 0.00000034 	 loss = 0.1094(0.5786)
2023/11/23 02:24:53 - INFO - root -   Epoch: [135/300][100/283], lr: 0.00000034 	 loss = 0.1030(0.5519)
2023/11/23 02:25:03 - INFO - root -   Epoch: [135/300][120/283], lr: 0.00000034 	 loss = 0.0752(0.5005)
2023/11/23 02:25:16 - INFO - root -   Epoch: [135/300][140/283], lr: 0.00000034 	 loss = 0.0621(0.4717)
2023/11/23 02:25:32 - INFO - root -   Epoch: [135/300][160/283], lr: 0.00000034 	 loss = 0.8667(0.4681)
2023/11/23 02:25:46 - INFO - root -   Epoch: [135/300][180/283], lr: 0.00000034 	 loss = 0.0311(0.4629)
2023/11/23 02:26:05 - INFO - root -   Epoch: [135/300][200/283], lr: 0.00000034 	 loss = 0.0860(0.4590)
2023/11/23 02:26:23 - INFO - root -   Epoch: [135/300][220/283], lr: 0.00000034 	 loss = 0.3407(0.4482)
2023/11/23 02:26:35 - INFO - root -   Epoch: [135/300][240/283], lr: 0.00000034 	 loss = 0.0912(0.4507)
2023/11/23 02:26:53 - INFO - root -   Epoch: [135/300][260/283], lr: 0.00000034 	 loss = 1.8182(0.4432)
2023/11/23 02:27:03 - INFO - root -   Epoch: [135/300][280/283], lr: 0.00000034 	 loss = 2.5392(0.4306)
2023/11/23 02:27:04 - INFO - root -   Epoch: [135/300] 	 loss = 0.4280
2023/11/23 02:27:04 - INFO - root -   train_accuracy = 0.8534
2023/11/23 02:27:06 - INFO - root -   Epoch: [136/300][0/283], lr: 0.00000034 	 loss = 0.0636(0.0636)
2023/11/23 02:27:34 - INFO - root -   Epoch: [136/300][20/283], lr: 0.00000034 	 loss = 2.3839(0.8023)
2023/11/23 02:27:53 - INFO - root -   Epoch: [136/300][40/283], lr: 0.00000034 	 loss = 0.1502(0.7561)
2023/11/23 02:28:05 - INFO - root -   Epoch: [136/300][60/283], lr: 0.00000034 	 loss = 0.0135(0.7123)
2023/11/23 02:28:18 - INFO - root -   Epoch: [136/300][80/283], lr: 0.00000034 	 loss = 0.0722(0.6793)
2023/11/23 02:28:31 - INFO - root -   Epoch: [136/300][100/283], lr: 0.00000034 	 loss = 0.0247(0.6105)
2023/11/23 02:28:48 - INFO - root -   Epoch: [136/300][120/283], lr: 0.00000034 	 loss = 0.0750(0.5517)
2023/11/23 02:29:03 - INFO - root -   Epoch: [136/300][140/283], lr: 0.00000034 	 loss = 0.0151(0.5259)
2023/11/23 02:29:19 - INFO - root -   Epoch: [136/300][160/283], lr: 0.00000034 	 loss = 1.6172(0.5176)
2023/11/23 02:29:37 - INFO - root -   Epoch: [136/300][180/283], lr: 0.00000034 	 loss = 0.0651(0.5283)
2023/11/23 02:29:47 - INFO - root -   Epoch: [136/300][200/283], lr: 0.00000034 	 loss = 0.0509(0.5236)
2023/11/23 02:30:09 - INFO - root -   Epoch: [136/300][220/283], lr: 0.00000034 	 loss = 0.0883(0.5189)
2023/11/23 02:30:18 - INFO - root -   Epoch: [136/300][240/283], lr: 0.00000034 	 loss = 0.4438(0.5175)
2023/11/23 02:30:35 - INFO - root -   Epoch: [136/300][260/283], lr: 0.00000034 	 loss = 0.6528(0.5097)
2023/11/23 02:30:44 - INFO - root -   Epoch: [136/300][280/283], lr: 0.00000034 	 loss = 3.5030(0.5018)
2023/11/23 02:30:45 - INFO - root -   Epoch: [136/300] 	 loss = 0.4984
2023/11/23 02:30:45 - INFO - root -   train_accuracy = 0.8322
2023/11/23 02:30:53 - INFO - root -   Epoch: [137/300][0/283], lr: 0.00000034 	 loss = 0.1598(0.1598)
2023/11/23 02:31:12 - INFO - root -   Epoch: [137/300][20/283], lr: 0.00000034 	 loss = 1.3109(0.7062)
2023/11/23 02:31:32 - INFO - root -   Epoch: [137/300][40/283], lr: 0.00000034 	 loss = 0.2283(0.7503)
2023/11/23 02:31:50 - INFO - root -   Epoch: [137/300][60/283], lr: 0.00000034 	 loss = 0.0109(0.6742)
2023/11/23 02:32:05 - INFO - root -   Epoch: [137/300][80/283], lr: 0.00000034 	 loss = 0.7895(0.6624)
2023/11/23 02:32:17 - INFO - root -   Epoch: [137/300][100/283], lr: 0.00000034 	 loss = 0.1716(0.6259)
2023/11/23 02:32:35 - INFO - root -   Epoch: [137/300][120/283], lr: 0.00000034 	 loss = 0.2330(0.5687)
2023/11/23 02:32:43 - INFO - root -   Epoch: [137/300][140/283], lr: 0.00000034 	 loss = 0.0122(0.5166)
2023/11/23 02:33:01 - INFO - root -   Epoch: [137/300][160/283], lr: 0.00000034 	 loss = 0.7807(0.5065)
2023/11/23 02:33:20 - INFO - root -   Epoch: [137/300][180/283], lr: 0.00000034 	 loss = 0.0935(0.5014)
2023/11/23 02:33:34 - INFO - root -   Epoch: [137/300][200/283], lr: 0.00000034 	 loss = 0.1781(0.4974)
2023/11/23 02:33:54 - INFO - root -   Epoch: [137/300][220/283], lr: 0.00000034 	 loss = 0.6555(0.4840)
2023/11/23 02:34:07 - INFO - root -   Epoch: [137/300][240/283], lr: 0.00000034 	 loss = 0.9335(0.4803)
2023/11/23 02:34:22 - INFO - root -   Epoch: [137/300][260/283], lr: 0.00000034 	 loss = 0.7784(0.4691)
2023/11/23 02:34:40 - INFO - root -   Epoch: [137/300][280/283], lr: 0.00000034 	 loss = 2.2560(0.4634)
2023/11/23 02:34:41 - INFO - root -   Epoch: [137/300] 	 loss = 0.4604
2023/11/23 02:34:41 - INFO - root -   train_accuracy = 0.8428
2023/11/23 02:34:47 - INFO - root -   Epoch: [138/300][0/283], lr: 0.00000034 	 loss = 0.1162(0.1162)
2023/11/23 02:35:00 - INFO - root -   Epoch: [138/300][20/283], lr: 0.00000034 	 loss = 1.4665(0.5065)
2023/11/23 02:35:20 - INFO - root -   Epoch: [138/300][40/283], lr: 0.00000034 	 loss = 0.0893(0.6037)
2023/11/23 02:35:30 - INFO - root -   Epoch: [138/300][60/283], lr: 0.00000034 	 loss = 0.1811(0.5974)
2023/11/23 02:35:38 - INFO - root -   Epoch: [138/300][80/283], lr: 0.00000034 	 loss = 0.3975(0.5852)
2023/11/23 02:36:02 - INFO - root -   Epoch: [138/300][100/283], lr: 0.00000034 	 loss = 0.2027(0.5547)
2023/11/23 02:36:12 - INFO - root -   Epoch: [138/300][120/283], lr: 0.00000034 	 loss = 0.0350(0.5091)
2023/11/23 02:36:24 - INFO - root -   Epoch: [138/300][140/283], lr: 0.00000034 	 loss = 0.1561(0.4735)
2023/11/23 02:36:37 - INFO - root -   Epoch: [138/300][160/283], lr: 0.00000034 	 loss = 1.4858(0.4576)
2023/11/23 02:36:54 - INFO - root -   Epoch: [138/300][180/283], lr: 0.00000034 	 loss = 0.0525(0.4711)
2023/11/23 02:37:02 - INFO - root -   Epoch: [138/300][200/283], lr: 0.00000034 	 loss = 0.0421(0.4730)
2023/11/23 02:37:20 - INFO - root -   Epoch: [138/300][220/283], lr: 0.00000034 	 loss = 0.1894(0.4599)
2023/11/23 02:37:36 - INFO - root -   Epoch: [138/300][240/283], lr: 0.00000034 	 loss = 0.8663(0.4510)
2023/11/23 02:37:54 - INFO - root -   Epoch: [138/300][260/283], lr: 0.00000034 	 loss = 1.6151(0.4454)
2023/11/23 02:38:11 - INFO - root -   Epoch: [138/300][280/283], lr: 0.00000034 	 loss = 3.2933(0.4389)
2023/11/23 02:38:12 - INFO - root -   Epoch: [138/300] 	 loss = 0.4359
2023/11/23 02:38:12 - INFO - root -   train_accuracy = 0.8481
2023/11/23 02:38:13 - INFO - root -   Epoch: [139/300][0/283], lr: 0.00000034 	 loss = 0.0639(0.0639)
2023/11/23 02:38:30 - INFO - root -   Epoch: [139/300][20/283], lr: 0.00000034 	 loss = 1.6822(0.7365)
2023/11/23 02:38:59 - INFO - root -   Epoch: [139/300][40/283], lr: 0.00000034 	 loss = 0.0860(0.7044)
2023/11/23 02:39:15 - INFO - root -   Epoch: [139/300][60/283], lr: 0.00000034 	 loss = 0.0048(0.6796)
2023/11/23 02:39:23 - INFO - root -   Epoch: [139/300][80/283], lr: 0.00000034 	 loss = 0.3357(0.6360)
2023/11/23 02:39:41 - INFO - root -   Epoch: [139/300][100/283], lr: 0.00000034 	 loss = 0.5937(0.5997)
2023/11/23 02:40:07 - INFO - root -   Epoch: [139/300][120/283], lr: 0.00000034 	 loss = 0.2531(0.5606)
2023/11/23 02:40:22 - INFO - root -   Epoch: [139/300][140/283], lr: 0.00000034 	 loss = 0.0921(0.5191)
2023/11/23 02:40:38 - INFO - root -   Epoch: [139/300][160/283], lr: 0.00000034 	 loss = 0.6692(0.5105)
2023/11/23 02:40:55 - INFO - root -   Epoch: [139/300][180/283], lr: 0.00000034 	 loss = 0.0322(0.5058)
2023/11/23 02:41:12 - INFO - root -   Epoch: [139/300][200/283], lr: 0.00000034 	 loss = 0.2383(0.4963)
2023/11/23 02:41:35 - INFO - root -   Epoch: [139/300][220/283], lr: 0.00000034 	 loss = 0.1207(0.4853)
2023/11/23 02:41:48 - INFO - root -   Epoch: [139/300][240/283], lr: 0.00000034 	 loss = 1.1640(0.4925)
2023/11/23 02:42:02 - INFO - root -   Epoch: [139/300][260/283], lr: 0.00000034 	 loss = 1.4575(0.4821)
2023/11/23 02:42:16 - INFO - root -   Epoch: [139/300][280/283], lr: 0.00000034 	 loss = 2.6575(0.4695)
2023/11/23 02:42:17 - INFO - root -   Epoch: [139/300] 	 loss = 0.4672
2023/11/23 02:45:09 - INFO - root -   precision = 0.8182
2023/11/23 02:45:09 - INFO - root -   eval_loss = 0.4857
2023/11/23 02:45:09 - INFO - root -   eval_acc = 0.8182
2023/11/23 02:45:10 - INFO - root -   train_accuracy = 0.8322
2023/11/23 02:45:18 - INFO - root -   Epoch: [140/300][0/283], lr: 0.00000034 	 loss = 0.1789(0.1789)
2023/11/23 02:45:40 - INFO - root -   Epoch: [140/300][20/283], lr: 0.00000034 	 loss = 0.4575(0.5869)
2023/11/23 02:46:06 - INFO - root -   Epoch: [140/300][40/283], lr: 0.00000034 	 loss = 0.0616(0.7319)
2023/11/23 02:46:25 - INFO - root -   Epoch: [140/300][60/283], lr: 0.00000034 	 loss = 0.0432(0.6749)
2023/11/23 02:46:33 - INFO - root -   Epoch: [140/300][80/283], lr: 0.00000034 	 loss = 0.3453(0.6494)
2023/11/23 02:46:45 - INFO - root -   Epoch: [140/300][100/283], lr: 0.00000034 	 loss = 0.7318(0.5950)
2023/11/23 02:47:06 - INFO - root -   Epoch: [140/300][120/283], lr: 0.00000034 	 loss = 0.6116(0.5374)
2023/11/23 02:47:24 - INFO - root -   Epoch: [140/300][140/283], lr: 0.00000034 	 loss = 0.1348(0.4998)
2023/11/23 02:47:34 - INFO - root -   Epoch: [140/300][160/283], lr: 0.00000034 	 loss = 1.1062(0.4984)
2023/11/23 02:47:43 - INFO - root -   Epoch: [140/300][180/283], lr: 0.00000034 	 loss = 0.0644(0.4844)
2023/11/23 02:47:58 - INFO - root -   Epoch: [140/300][200/283], lr: 0.00000034 	 loss = 0.2125(0.4792)
2023/11/23 02:48:11 - INFO - root -   Epoch: [140/300][220/283], lr: 0.00000034 	 loss = 0.0930(0.4659)
2023/11/23 02:48:23 - INFO - root -   Epoch: [140/300][240/283], lr: 0.00000034 	 loss = 0.0863(0.4666)
2023/11/23 02:48:42 - INFO - root -   Epoch: [140/300][260/283], lr: 0.00000034 	 loss = 2.1415(0.4651)
2023/11/23 02:48:50 - INFO - root -   Epoch: [140/300][280/283], lr: 0.00000034 	 loss = 2.9212(0.4573)
2023/11/23 02:48:51 - INFO - root -   Epoch: [140/300] 	 loss = 0.4544
2023/11/23 02:48:51 - INFO - root -   train_accuracy = 0.8339
2023/11/23 02:48:58 - INFO - root -   Epoch: [141/300][0/283], lr: 0.00000035 	 loss = 0.1937(0.1937)
2023/11/23 02:49:08 - INFO - root -   Epoch: [141/300][20/283], lr: 0.00000035 	 loss = 0.4425(0.4274)
2023/11/23 02:49:29 - INFO - root -   Epoch: [141/300][40/283], lr: 0.00000035 	 loss = 0.0403(0.5636)
2023/11/23 02:49:47 - INFO - root -   Epoch: [141/300][60/283], lr: 0.00000035 	 loss = 0.0170(0.5336)
2023/11/23 02:50:03 - INFO - root -   Epoch: [141/300][80/283], lr: 0.00000035 	 loss = 0.0318(0.5615)
2023/11/23 02:50:15 - INFO - root -   Epoch: [141/300][100/283], lr: 0.00000035 	 loss = 0.3272(0.5270)
2023/11/23 02:50:42 - INFO - root -   Epoch: [141/300][120/283], lr: 0.00000035 	 loss = 0.0569(0.5019)
2023/11/23 02:51:01 - INFO - root -   Epoch: [141/300][140/283], lr: 0.00000035 	 loss = 0.0133(0.4738)
2023/11/23 02:51:18 - INFO - root -   Epoch: [141/300][160/283], lr: 0.00000035 	 loss = 0.6368(0.4700)
2023/11/23 02:51:39 - INFO - root -   Epoch: [141/300][180/283], lr: 0.00000035 	 loss = 0.0545(0.4843)
2023/11/23 02:51:48 - INFO - root -   Epoch: [141/300][200/283], lr: 0.00000035 	 loss = 0.0744(0.4679)
2023/11/23 02:51:56 - INFO - root -   Epoch: [141/300][220/283], lr: 0.00000035 	 loss = 0.0507(0.4501)
2023/11/23 02:52:10 - INFO - root -   Epoch: [141/300][240/283], lr: 0.00000035 	 loss = 1.9448(0.4490)
2023/11/23 02:52:30 - INFO - root -   Epoch: [141/300][260/283], lr: 0.00000035 	 loss = 0.8995(0.4429)
2023/11/23 02:52:39 - INFO - root -   Epoch: [141/300][280/283], lr: 0.00000035 	 loss = 3.1171(0.4322)
2023/11/23 02:52:40 - INFO - root -   Epoch: [141/300] 	 loss = 0.4300
2023/11/23 02:52:40 - INFO - root -   train_accuracy = 0.8587
2023/11/23 02:52:41 - INFO - root -   Epoch: [142/300][0/283], lr: 0.00000035 	 loss = 0.0473(0.0473)
2023/11/23 02:53:00 - INFO - root -   Epoch: [142/300][20/283], lr: 0.00000035 	 loss = 0.6434(0.5712)
2023/11/23 02:53:20 - INFO - root -   Epoch: [142/300][40/283], lr: 0.00000035 	 loss = 0.5726(0.7309)
2023/11/23 02:53:32 - INFO - root -   Epoch: [142/300][60/283], lr: 0.00000035 	 loss = 0.0352(0.6520)
2023/11/23 02:53:50 - INFO - root -   Epoch: [142/300][80/283], lr: 0.00000035 	 loss = 0.0857(0.6597)
2023/11/23 02:54:08 - INFO - root -   Epoch: [142/300][100/283], lr: 0.00000035 	 loss = 0.2234(0.6025)
2023/11/23 02:54:20 - INFO - root -   Epoch: [142/300][120/283], lr: 0.00000035 	 loss = 0.1655(0.5546)
2023/11/23 02:54:34 - INFO - root -   Epoch: [142/300][140/283], lr: 0.00000035 	 loss = 0.1927(0.5033)
2023/11/23 02:54:52 - INFO - root -   Epoch: [142/300][160/283], lr: 0.00000035 	 loss = 1.4590(0.4994)
2023/11/23 02:55:13 - INFO - root -   Epoch: [142/300][180/283], lr: 0.00000035 	 loss = 0.0455(0.5189)
2023/11/23 02:55:22 - INFO - root -   Epoch: [142/300][200/283], lr: 0.00000035 	 loss = 0.0488(0.5087)
2023/11/23 02:55:47 - INFO - root -   Epoch: [142/300][220/283], lr: 0.00000035 	 loss = 0.0701(0.4964)
2023/11/23 02:55:59 - INFO - root -   Epoch: [142/300][240/283], lr: 0.00000035 	 loss = 2.9194(0.4971)
2023/11/23 02:56:12 - INFO - root -   Epoch: [142/300][260/283], lr: 0.00000035 	 loss = 0.5455(0.4806)
2023/11/23 02:56:25 - INFO - root -   Epoch: [142/300][280/283], lr: 0.00000035 	 loss = 1.3054(0.4669)
2023/11/23 02:56:26 - INFO - root -   Epoch: [142/300] 	 loss = 0.4647
2023/11/23 02:56:26 - INFO - root -   train_accuracy = 0.8481
2023/11/23 02:56:27 - INFO - root -   Epoch: [143/300][0/283], lr: 0.00000035 	 loss = 0.0145(0.0145)
2023/11/23 02:56:51 - INFO - root -   Epoch: [143/300][20/283], lr: 0.00000035 	 loss = 1.5302(0.5869)
2023/11/23 02:56:59 - INFO - root -   Epoch: [143/300][40/283], lr: 0.00000035 	 loss = 0.3419(0.6900)
2023/11/23 02:57:25 - INFO - root -   Epoch: [143/300][60/283], lr: 0.00000035 	 loss = 0.0040(0.6249)
2023/11/23 02:57:33 - INFO - root -   Epoch: [143/300][80/283], lr: 0.00000035 	 loss = 0.0403(0.6094)
2023/11/23 02:57:54 - INFO - root -   Epoch: [143/300][100/283], lr: 0.00000035 	 loss = 1.0499(0.5678)
2023/11/23 02:58:07 - INFO - root -   Epoch: [143/300][120/283], lr: 0.00000035 	 loss = 0.4398(0.5394)
2023/11/23 02:58:20 - INFO - root -   Epoch: [143/300][140/283], lr: 0.00000035 	 loss = 0.0284(0.4915)
2023/11/23 02:58:30 - INFO - root -   Epoch: [143/300][160/283], lr: 0.00000035 	 loss = 0.9908(0.4864)
2023/11/23 02:58:44 - INFO - root -   Epoch: [143/300][180/283], lr: 0.00000035 	 loss = 0.0581(0.4881)
2023/11/23 02:58:57 - INFO - root -   Epoch: [143/300][200/283], lr: 0.00000035 	 loss = 0.4001(0.4731)
2023/11/23 02:59:25 - INFO - root -   Epoch: [143/300][220/283], lr: 0.00000035 	 loss = 0.1545(0.4585)
2023/11/23 02:59:33 - INFO - root -   Epoch: [143/300][240/283], lr: 0.00000035 	 loss = 0.4850(0.4571)
2023/11/23 02:59:46 - INFO - root -   Epoch: [143/300][260/283], lr: 0.00000035 	 loss = 2.1770(0.4560)
2023/11/23 02:59:56 - INFO - root -   Epoch: [143/300][280/283], lr: 0.00000035 	 loss = 1.7362(0.4381)
2023/11/23 02:59:57 - INFO - root -   Epoch: [143/300] 	 loss = 0.4353
2023/11/23 02:59:57 - INFO - root -   train_accuracy = 0.8481
2023/11/23 03:00:10 - INFO - root -   Epoch: [144/300][0/283], lr: 0.00000035 	 loss = 0.0951(0.0951)
2023/11/23 03:00:26 - INFO - root -   Epoch: [144/300][20/283], lr: 0.00000035 	 loss = 1.0684(0.6597)
2023/11/23 03:00:41 - INFO - root -   Epoch: [144/300][40/283], lr: 0.00000035 	 loss = 0.3972(0.7509)
2023/11/23 03:00:49 - INFO - root -   Epoch: [144/300][60/283], lr: 0.00000035 	 loss = 0.0146(0.6524)
2023/11/23 03:01:08 - INFO - root -   Epoch: [144/300][80/283], lr: 0.00000035 	 loss = 0.4766(0.6401)
2023/11/23 03:01:22 - INFO - root -   Epoch: [144/300][100/283], lr: 0.00000035 	 loss = 0.1639(0.5811)
2023/11/23 03:01:46 - INFO - root -   Epoch: [144/300][120/283], lr: 0.00000035 	 loss = 0.0628(0.5447)
2023/11/23 03:02:02 - INFO - root -   Epoch: [144/300][140/283], lr: 0.00000035 	 loss = 0.1415(0.5106)
2023/11/23 03:02:10 - INFO - root -   Epoch: [144/300][160/283], lr: 0.00000035 	 loss = 0.6290(0.4917)
2023/11/23 03:02:36 - INFO - root -   Epoch: [144/300][180/283], lr: 0.00000035 	 loss = 0.0959(0.5077)
2023/11/23 03:02:45 - INFO - root -   Epoch: [144/300][200/283], lr: 0.00000035 	 loss = 0.0413(0.5039)
2023/11/23 03:03:02 - INFO - root -   Epoch: [144/300][220/283], lr: 0.00000035 	 loss = 0.0504(0.4848)
2023/11/23 03:03:17 - INFO - root -   Epoch: [144/300][240/283], lr: 0.00000035 	 loss = 0.1245(0.4822)
2023/11/23 03:03:27 - INFO - root -   Epoch: [144/300][260/283], lr: 0.00000035 	 loss = 1.5317(0.4815)
2023/11/23 03:03:38 - INFO - root -   Epoch: [144/300][280/283], lr: 0.00000035 	 loss = 2.6246(0.4715)
2023/11/23 03:03:39 - INFO - root -   Epoch: [144/300] 	 loss = 0.4683
2023/11/23 03:06:32 - INFO - root -   precision = 0.8182
2023/11/23 03:06:32 - INFO - root -   eval_loss = 0.4953
2023/11/23 03:06:32 - INFO - root -   eval_acc = 0.8182
2023/11/23 03:06:33 - INFO - root -   train_accuracy = 0.8375
2023/11/23 03:06:41 - INFO - root -   Epoch: [145/300][0/283], lr: 0.00000035 	 loss = 0.1078(0.1078)
2023/11/23 03:07:00 - INFO - root -   Epoch: [145/300][20/283], lr: 0.00000035 	 loss = 0.3489(0.5586)
2023/11/23 03:07:20 - INFO - root -   Epoch: [145/300][40/283], lr: 0.00000035 	 loss = 0.6997(0.5722)
2023/11/23 03:07:38 - INFO - root -   Epoch: [145/300][60/283], lr: 0.00000035 	 loss = 0.0206(0.5269)
2023/11/23 03:07:53 - INFO - root -   Epoch: [145/300][80/283], lr: 0.00000035 	 loss = 0.1355(0.5583)
2023/11/23 03:08:06 - INFO - root -   Epoch: [145/300][100/283], lr: 0.00000035 	 loss = 0.2080(0.5427)
2023/11/23 03:08:24 - INFO - root -   Epoch: [145/300][120/283], lr: 0.00000035 	 loss = 0.0450(0.5044)
2023/11/23 03:08:37 - INFO - root -   Epoch: [145/300][140/283], lr: 0.00000035 	 loss = 0.0319(0.4702)
2023/11/23 03:08:55 - INFO - root -   Epoch: [145/300][160/283], lr: 0.00000035 	 loss = 0.6398(0.4687)
2023/11/23 03:09:08 - INFO - root -   Epoch: [145/300][180/283], lr: 0.00000035 	 loss = 0.0626(0.4767)
2023/11/23 03:09:22 - INFO - root -   Epoch: [145/300][200/283], lr: 0.00000035 	 loss = 0.2445(0.4676)
2023/11/23 03:09:41 - INFO - root -   Epoch: [145/300][220/283], lr: 0.00000035 	 loss = 0.2460(0.4673)
2023/11/23 03:10:00 - INFO - root -   Epoch: [145/300][240/283], lr: 0.00000035 	 loss = 0.0949(0.4623)
2023/11/23 03:10:16 - INFO - root -   Epoch: [145/300][260/283], lr: 0.00000035 	 loss = 0.8566(0.4504)
2023/11/23 03:10:25 - INFO - root -   Epoch: [145/300][280/283], lr: 0.00000035 	 loss = 2.6357(0.4421)
2023/11/23 03:10:25 - INFO - root -   Epoch: [145/300] 	 loss = 0.4392
2023/11/23 03:10:25 - INFO - root -   train_accuracy = 0.8463
2023/11/23 03:10:33 - INFO - root -   Epoch: [146/300][0/283], lr: 0.00000036 	 loss = 0.1106(0.1106)
2023/11/23 03:10:52 - INFO - root -   Epoch: [146/300][20/283], lr: 0.00000036 	 loss = 1.5587(0.7370)
2023/11/23 03:11:02 - INFO - root -   Epoch: [146/300][40/283], lr: 0.00000036 	 loss = 0.0211(0.7441)
2023/11/23 03:11:18 - INFO - root -   Epoch: [146/300][60/283], lr: 0.00000036 	 loss = 0.0194(0.6810)
2023/11/23 03:11:26 - INFO - root -   Epoch: [146/300][80/283], lr: 0.00000036 	 loss = 0.0546(0.6485)
2023/11/23 03:11:38 - INFO - root -   Epoch: [146/300][100/283], lr: 0.00000036 	 loss = 0.1616(0.6061)
2023/11/23 03:11:56 - INFO - root -   Epoch: [146/300][120/283], lr: 0.00000036 	 loss = 0.3563(0.5743)
2023/11/23 03:12:12 - INFO - root -   Epoch: [146/300][140/283], lr: 0.00000036 	 loss = 0.0256(0.5317)
2023/11/23 03:12:36 - INFO - root -   Epoch: [146/300][160/283], lr: 0.00000036 	 loss = 1.7720(0.5191)
2023/11/23 03:12:51 - INFO - root -   Epoch: [146/300][180/283], lr: 0.00000036 	 loss = 0.2876(0.5266)
2023/11/23 03:13:07 - INFO - root -   Epoch: [146/300][200/283], lr: 0.00000036 	 loss = 0.0416(0.5084)
2023/11/23 03:13:23 - INFO - root -   Epoch: [146/300][220/283], lr: 0.00000036 	 loss = 0.2651(0.4938)
2023/11/23 03:13:45 - INFO - root -   Epoch: [146/300][240/283], lr: 0.00000036 	 loss = 1.4640(0.4813)
2023/11/23 03:13:59 - INFO - root -   Epoch: [146/300][260/283], lr: 0.00000036 	 loss = 1.3778(0.4687)
2023/11/23 03:14:09 - INFO - root -   Epoch: [146/300][280/283], lr: 0.00000036 	 loss = 1.7627(0.4611)
2023/11/23 03:14:10 - INFO - root -   Epoch: [146/300] 	 loss = 0.4579
2023/11/23 03:14:10 - INFO - root -   train_accuracy = 0.8304
2023/11/23 03:14:16 - INFO - root -   Epoch: [147/300][0/283], lr: 0.00000036 	 loss = 0.2781(0.2781)
2023/11/23 03:14:27 - INFO - root -   Epoch: [147/300][20/283], lr: 0.00000036 	 loss = 0.5500(0.4546)
2023/11/23 03:14:37 - INFO - root -   Epoch: [147/300][40/283], lr: 0.00000036 	 loss = 0.1660(0.5797)
2023/11/23 03:14:52 - INFO - root -   Epoch: [147/300][60/283], lr: 0.00000036 	 loss = 0.0437(0.5687)
2023/11/23 03:15:03 - INFO - root -   Epoch: [147/300][80/283], lr: 0.00000036 	 loss = 0.8334(0.6116)
2023/11/23 03:15:16 - INFO - root -   Epoch: [147/300][100/283], lr: 0.00000036 	 loss = 0.1133(0.5549)
2023/11/23 03:15:29 - INFO - root -   Epoch: [147/300][120/283], lr: 0.00000036 	 loss = 0.0581(0.5116)
2023/11/23 03:15:43 - INFO - root -   Epoch: [147/300][140/283], lr: 0.00000036 	 loss = 0.0432(0.4649)
2023/11/23 03:16:01 - INFO - root -   Epoch: [147/300][160/283], lr: 0.00000036 	 loss = 1.1149(0.4742)
2023/11/23 03:16:16 - INFO - root -   Epoch: [147/300][180/283], lr: 0.00000036 	 loss = 0.0529(0.4811)
2023/11/23 03:16:27 - INFO - root -   Epoch: [147/300][200/283], lr: 0.00000036 	 loss = 0.1505(0.4729)
2023/11/23 03:16:44 - INFO - root -   Epoch: [147/300][220/283], lr: 0.00000036 	 loss = 0.0966(0.4640)
2023/11/23 03:16:59 - INFO - root -   Epoch: [147/300][240/283], lr: 0.00000036 	 loss = 0.6424(0.4611)
2023/11/23 03:17:14 - INFO - root -   Epoch: [147/300][260/283], lr: 0.00000036 	 loss = 1.1953(0.4565)
2023/11/23 03:17:26 - INFO - root -   Epoch: [147/300][280/283], lr: 0.00000036 	 loss = 2.6977(0.4484)
2023/11/23 03:17:27 - INFO - root -   Epoch: [147/300] 	 loss = 0.4454
2023/11/23 03:17:27 - INFO - root -   train_accuracy = 0.8375
2023/11/23 03:17:34 - INFO - root -   Epoch: [148/300][0/283], lr: 0.00000036 	 loss = 0.2031(0.2031)
2023/11/23 03:17:47 - INFO - root -   Epoch: [148/300][20/283], lr: 0.00000036 	 loss = 0.3451(0.5126)
2023/11/23 03:17:59 - INFO - root -   Epoch: [148/300][40/283], lr: 0.00000036 	 loss = 0.0923(0.4907)
2023/11/23 03:18:12 - INFO - root -   Epoch: [148/300][60/283], lr: 0.00000036 	 loss = 0.1376(0.5083)
2023/11/23 03:18:38 - INFO - root -   Epoch: [148/300][80/283], lr: 0.00000036 	 loss = 0.0821(0.5525)
2023/11/23 03:18:52 - INFO - root -   Epoch: [148/300][100/283], lr: 0.00000036 	 loss = 0.1603(0.5130)
2023/11/23 03:19:00 - INFO - root -   Epoch: [148/300][120/283], lr: 0.00000036 	 loss = 0.0172(0.4918)
2023/11/23 03:19:20 - INFO - root -   Epoch: [148/300][140/283], lr: 0.00000036 	 loss = 0.2103(0.4612)
2023/11/23 03:19:36 - INFO - root -   Epoch: [148/300][160/283], lr: 0.00000036 	 loss = 0.7239(0.4618)
2023/11/23 03:19:53 - INFO - root -   Epoch: [148/300][180/283], lr: 0.00000036 	 loss = 0.2096(0.4692)
2023/11/23 03:20:11 - INFO - root -   Epoch: [148/300][200/283], lr: 0.00000036 	 loss = 0.4955(0.4694)
2023/11/23 03:20:29 - INFO - root -   Epoch: [148/300][220/283], lr: 0.00000036 	 loss = 0.0619(0.4529)
2023/11/23 03:20:49 - INFO - root -   Epoch: [148/300][240/283], lr: 0.00000036 	 loss = 0.3565(0.4467)
2023/11/23 03:21:10 - INFO - root -   Epoch: [148/300][260/283], lr: 0.00000036 	 loss = 1.2851(0.4365)
2023/11/23 03:21:23 - INFO - root -   Epoch: [148/300][280/283], lr: 0.00000036 	 loss = 1.9025(0.4328)
2023/11/23 03:21:24 - INFO - root -   Epoch: [148/300] 	 loss = 0.4306
2023/11/23 03:21:24 - INFO - root -   train_accuracy = 0.8481
2023/11/23 03:21:29 - INFO - root -   Epoch: [149/300][0/283], lr: 0.00000036 	 loss = 0.1241(0.1241)
2023/11/23 03:21:37 - INFO - root -   Epoch: [149/300][20/283], lr: 0.00000036 	 loss = 0.6499(0.4558)
2023/11/23 03:21:49 - INFO - root -   Epoch: [149/300][40/283], lr: 0.00000036 	 loss = 0.0475(0.5118)
2023/11/23 03:22:04 - INFO - root -   Epoch: [149/300][60/283], lr: 0.00000036 	 loss = 0.0362(0.5166)
2023/11/23 03:22:19 - INFO - root -   Epoch: [149/300][80/283], lr: 0.00000036 	 loss = 0.0822(0.5186)
2023/11/23 03:22:34 - INFO - root -   Epoch: [149/300][100/283], lr: 0.00000036 	 loss = 0.1418(0.4933)
2023/11/23 03:22:42 - INFO - root -   Epoch: [149/300][120/283], lr: 0.00000036 	 loss = 0.3035(0.4618)
2023/11/23 03:23:03 - INFO - root -   Epoch: [149/300][140/283], lr: 0.00000036 	 loss = 0.0187(0.4233)
2023/11/23 03:23:17 - INFO - root -   Epoch: [149/300][160/283], lr: 0.00000036 	 loss = 0.2184(0.4221)
2023/11/23 03:23:40 - INFO - root -   Epoch: [149/300][180/283], lr: 0.00000036 	 loss = 0.2003(0.4605)
2023/11/23 03:23:48 - INFO - root -   Epoch: [149/300][200/283], lr: 0.00000036 	 loss = 0.0535(0.4601)
2023/11/23 03:24:07 - INFO - root -   Epoch: [149/300][220/283], lr: 0.00000036 	 loss = 0.0911(0.4497)
2023/11/23 03:24:17 - INFO - root -   Epoch: [149/300][240/283], lr: 0.00000036 	 loss = 0.7322(0.4445)
2023/11/23 03:24:29 - INFO - root -   Epoch: [149/300][260/283], lr: 0.00000036 	 loss = 1.6289(0.4373)
2023/11/23 03:24:43 - INFO - root -   Epoch: [149/300][280/283], lr: 0.00000036 	 loss = 2.2845(0.4253)
2023/11/23 03:24:44 - INFO - root -   Epoch: [149/300] 	 loss = 0.4232
2023/11/23 03:27:37 - INFO - root -   precision = 0.8182
2023/11/23 03:27:37 - INFO - root -   eval_loss = 0.4836
2023/11/23 03:27:37 - INFO - root -   eval_acc = 0.8182
2023/11/23 03:27:38 - INFO - root -   train_accuracy = 0.8481
2023/11/23 03:27:40 - INFO - root -   Epoch: [150/300][0/283], lr: 0.00000036 	 loss = 0.0469(0.0469)
2023/11/23 03:27:53 - INFO - root -   Epoch: [150/300][20/283], lr: 0.00000036 	 loss = 0.2621(0.4276)
2023/11/23 03:28:04 - INFO - root -   Epoch: [150/300][40/283], lr: 0.00000036 	 loss = 0.3485(0.5433)
2023/11/23 03:28:17 - INFO - root -   Epoch: [150/300][60/283], lr: 0.00000036 	 loss = 0.0123(0.5723)
2023/11/23 03:28:29 - INFO - root -   Epoch: [150/300][80/283], lr: 0.00000036 	 loss = 0.0314(0.5486)
2023/11/23 03:28:42 - INFO - root -   Epoch: [150/300][100/283], lr: 0.00000036 	 loss = 0.1555(0.5170)
2023/11/23 03:29:01 - INFO - root -   Epoch: [150/300][120/283], lr: 0.00000036 	 loss = 0.0383(0.4943)
2023/11/23 03:29:14 - INFO - root -   Epoch: [150/300][140/283], lr: 0.00000036 	 loss = 0.0375(0.4495)
2023/11/23 03:29:33 - INFO - root -   Epoch: [150/300][160/283], lr: 0.00000036 	 loss = 0.4930(0.4403)
2023/11/23 03:29:48 - INFO - root -   Epoch: [150/300][180/283], lr: 0.00000036 	 loss = 0.0487(0.4556)
2023/11/23 03:30:10 - INFO - root -   Epoch: [150/300][200/283], lr: 0.00000036 	 loss = 0.0612(0.4495)
2023/11/23 03:30:23 - INFO - root -   Epoch: [150/300][220/283], lr: 0.00000036 	 loss = 0.1422(0.4352)
2023/11/23 03:30:45 - INFO - root -   Epoch: [150/300][240/283], lr: 0.00000036 	 loss = 0.1786(0.4469)
2023/11/23 03:30:54 - INFO - root -   Epoch: [150/300][260/283], lr: 0.00000036 	 loss = 0.9609(0.4344)
2023/11/23 03:31:04 - INFO - root -   Epoch: [150/300][280/283], lr: 0.00000036 	 loss = 1.6158(0.4216)
2023/11/23 03:31:05 - INFO - root -   Epoch: [150/300] 	 loss = 0.4192
2023/11/23 03:31:05 - INFO - root -   train_accuracy = 0.8569
2023/11/23 03:31:11 - INFO - root -   Epoch: [151/300][0/283], lr: 0.00000036 	 loss = 0.1493(0.1493)
2023/11/23 03:31:30 - INFO - root -   Epoch: [151/300][20/283], lr: 0.00000036 	 loss = 0.2667(0.4363)
2023/11/23 03:31:42 - INFO - root -   Epoch: [151/300][40/283], lr: 0.00000036 	 loss = 0.1660(0.4956)
2023/11/23 03:31:58 - INFO - root -   Epoch: [151/300][60/283], lr: 0.00000036 	 loss = 0.1447(0.4969)
2023/11/23 03:32:18 - INFO - root -   Epoch: [151/300][80/283], lr: 0.00000036 	 loss = 0.1440(0.5293)
2023/11/23 03:32:30 - INFO - root -   Epoch: [151/300][100/283], lr: 0.00000036 	 loss = 0.0890(0.4901)
2023/11/23 03:32:47 - INFO - root -   Epoch: [151/300][120/283], lr: 0.00000036 	 loss = 0.1829(0.4664)
2023/11/23 03:33:03 - INFO - root -   Epoch: [151/300][140/283], lr: 0.00000036 	 loss = 0.0157(0.4311)
2023/11/23 03:33:19 - INFO - root -   Epoch: [151/300][160/283], lr: 0.00000036 	 loss = 0.5738(0.4312)
2023/11/23 03:33:31 - INFO - root -   Epoch: [151/300][180/283], lr: 0.00000036 	 loss = 0.0853(0.4407)
2023/11/23 03:33:45 - INFO - root -   Epoch: [151/300][200/283], lr: 0.00000036 	 loss = 0.0965(0.4279)
2023/11/23 03:34:03 - INFO - root -   Epoch: [151/300][220/283], lr: 0.00000036 	 loss = 0.1117(0.4204)
2023/11/23 03:34:23 - INFO - root -   Epoch: [151/300][240/283], lr: 0.00000036 	 loss = 0.1389(0.4275)
2023/11/23 03:34:32 - INFO - root -   Epoch: [151/300][260/283], lr: 0.00000036 	 loss = 0.5435(0.4165)
2023/11/23 03:34:45 - INFO - root -   Epoch: [151/300][280/283], lr: 0.00000036 	 loss = 2.4808(0.4201)
2023/11/23 03:34:46 - INFO - root -   Epoch: [151/300] 	 loss = 0.4180
2023/11/23 03:34:46 - INFO - root -   train_accuracy = 0.8498
2023/11/23 03:34:48 - INFO - root -   Epoch: [152/300][0/283], lr: 0.00000037 	 loss = 0.0265(0.0265)
2023/11/23 03:35:13 - INFO - root -   Epoch: [152/300][20/283], lr: 0.00000037 	 loss = 0.5735(0.7187)
2023/11/23 03:35:28 - INFO - root -   Epoch: [152/300][40/283], lr: 0.00000037 	 loss = 0.0138(0.6825)
2023/11/23 03:35:44 - INFO - root -   Epoch: [152/300][60/283], lr: 0.00000037 	 loss = 0.0236(0.6113)
2023/11/23 03:35:59 - INFO - root -   Epoch: [152/300][80/283], lr: 0.00000037 	 loss = 0.2773(0.5995)
2023/11/23 03:36:14 - INFO - root -   Epoch: [152/300][100/283], lr: 0.00000037 	 loss = 0.6507(0.5426)
2023/11/23 03:36:30 - INFO - root -   Epoch: [152/300][120/283], lr: 0.00000037 	 loss = 0.1445(0.5170)
2023/11/23 03:36:50 - INFO - root -   Epoch: [152/300][140/283], lr: 0.00000037 	 loss = 0.0186(0.4809)
2023/11/23 03:37:01 - INFO - root -   Epoch: [152/300][160/283], lr: 0.00000037 	 loss = 0.9801(0.4606)
2023/11/23 03:37:19 - INFO - root -   Epoch: [152/300][180/283], lr: 0.00000037 	 loss = 0.0870(0.4606)
2023/11/23 03:37:30 - INFO - root -   Epoch: [152/300][200/283], lr: 0.00000037 	 loss = 0.0698(0.4684)
2023/11/23 03:37:43 - INFO - root -   Epoch: [152/300][220/283], lr: 0.00000037 	 loss = 0.2583(0.4539)
2023/11/23 03:37:55 - INFO - root -   Epoch: [152/300][240/283], lr: 0.00000037 	 loss = 1.0669(0.4435)
2023/11/23 03:38:14 - INFO - root -   Epoch: [152/300][260/283], lr: 0.00000037 	 loss = 2.4282(0.4374)
2023/11/23 03:38:23 - INFO - root -   Epoch: [152/300][280/283], lr: 0.00000037 	 loss = 2.3091(0.4217)
2023/11/23 03:38:24 - INFO - root -   Epoch: [152/300] 	 loss = 0.4197
2023/11/23 03:38:24 - INFO - root -   train_accuracy = 0.8498
2023/11/23 03:38:24 - INFO - root -   Epoch: [153/300][0/283], lr: 0.00000037 	 loss = 0.0902(0.0902)
2023/11/23 03:39:01 - INFO - root -   Epoch: [153/300][20/283], lr: 0.00000037 	 loss = 1.0876(0.5605)
2023/11/23 03:39:15 - INFO - root -   Epoch: [153/300][40/283], lr: 0.00000037 	 loss = 0.0675(0.6271)
2023/11/23 03:39:29 - INFO - root -   Epoch: [153/300][60/283], lr: 0.00000037 	 loss = 0.0332(0.6420)
2023/11/23 03:39:45 - INFO - root -   Epoch: [153/300][80/283], lr: 0.00000037 	 loss = 0.1528(0.6271)
2023/11/23 03:39:58 - INFO - root -   Epoch: [153/300][100/283], lr: 0.00000037 	 loss = 0.4183(0.5649)
2023/11/23 03:40:13 - INFO - root -   Epoch: [153/300][120/283], lr: 0.00000037 	 loss = 0.5831(0.5257)
2023/11/23 03:40:31 - INFO - root -   Epoch: [153/300][140/283], lr: 0.00000037 	 loss = 0.0830(0.4872)
2023/11/23 03:40:44 - INFO - root -   Epoch: [153/300][160/283], lr: 0.00000037 	 loss = 3.3533(0.4868)
2023/11/23 03:40:54 - INFO - root -   Epoch: [153/300][180/283], lr: 0.00000037 	 loss = 0.0885(0.4808)
2023/11/23 03:41:15 - INFO - root -   Epoch: [153/300][200/283], lr: 0.00000037 	 loss = 0.1989(0.4902)
2023/11/23 03:41:33 - INFO - root -   Epoch: [153/300][220/283], lr: 0.00000037 	 loss = 0.5324(0.4801)
2023/11/23 03:41:45 - INFO - root -   Epoch: [153/300][240/283], lr: 0.00000037 	 loss = 0.8929(0.4790)
2023/11/23 03:41:58 - INFO - root -   Epoch: [153/300][260/283], lr: 0.00000037 	 loss = 2.1108(0.4662)
2023/11/23 03:42:07 - INFO - root -   Epoch: [153/300][280/283], lr: 0.00000037 	 loss = 1.6194(0.4503)
2023/11/23 03:42:08 - INFO - root -   Epoch: [153/300] 	 loss = 0.4479
2023/11/23 03:42:08 - INFO - root -   train_accuracy = 0.8463
2023/11/23 03:42:09 - INFO - root -   Epoch: [154/300][0/283], lr: 0.00000037 	 loss = 0.0263(0.0263)
2023/11/23 03:42:30 - INFO - root -   Epoch: [154/300][20/283], lr: 0.00000037 	 loss = 0.9081(0.6463)
2023/11/23 03:42:47 - INFO - root -   Epoch: [154/300][40/283], lr: 0.00000037 	 loss = 0.0342(0.6959)
2023/11/23 03:43:07 - INFO - root -   Epoch: [154/300][60/283], lr: 0.00000037 	 loss = 0.0353(0.6684)
2023/11/23 03:43:17 - INFO - root -   Epoch: [154/300][80/283], lr: 0.00000037 	 loss = 0.1341(0.6203)
2023/11/23 03:43:28 - INFO - root -   Epoch: [154/300][100/283], lr: 0.00000037 	 loss = 0.1248(0.5859)
2023/11/23 03:43:43 - INFO - root -   Epoch: [154/300][120/283], lr: 0.00000037 	 loss = 0.5105(0.5492)
2023/11/23 03:43:51 - INFO - root -   Epoch: [154/300][140/283], lr: 0.00000037 	 loss = 0.0215(0.5075)
2023/11/23 03:44:07 - INFO - root -   Epoch: [154/300][160/283], lr: 0.00000037 	 loss = 1.8994(0.4958)
2023/11/23 03:44:21 - INFO - root -   Epoch: [154/300][180/283], lr: 0.00000037 	 loss = 0.1145(0.4931)
2023/11/23 03:44:30 - INFO - root -   Epoch: [154/300][200/283], lr: 0.00000037 	 loss = 0.1405(0.4777)
2023/11/23 03:44:40 - INFO - root -   Epoch: [154/300][220/283], lr: 0.00000037 	 loss = 0.1199(0.4609)
2023/11/23 03:44:55 - INFO - root -   Epoch: [154/300][240/283], lr: 0.00000037 	 loss = 0.5421(0.4641)
2023/11/23 03:45:11 - INFO - root -   Epoch: [154/300][260/283], lr: 0.00000037 	 loss = 1.9614(0.4586)
2023/11/23 03:45:19 - INFO - root -   Epoch: [154/300][280/283], lr: 0.00000037 	 loss = 2.5889(0.4463)
2023/11/23 03:45:20 - INFO - root -   Epoch: [154/300] 	 loss = 0.4439
2023/11/23 03:48:13 - INFO - root -   precision = 0.8252
2023/11/23 03:48:13 - INFO - root -   eval_loss = 0.4794
2023/11/23 03:48:13 - INFO - root -   eval_acc = 0.8252
2023/11/23 03:48:14 - INFO - root -   train_accuracy = 0.8392
2023/11/23 03:48:15 - INFO - root -   Epoch: [155/300][0/283], lr: 0.00000037 	 loss = 0.0587(0.0587)
2023/11/23 03:48:32 - INFO - root -   Epoch: [155/300][20/283], lr: 0.00000037 	 loss = 2.0430(0.6515)
2023/11/23 03:48:47 - INFO - root -   Epoch: [155/300][40/283], lr: 0.00000037 	 loss = 0.1453(0.5463)
2023/11/23 03:49:04 - INFO - root -   Epoch: [155/300][60/283], lr: 0.00000037 	 loss = 0.1693(0.5711)
2023/11/23 03:49:17 - INFO - root -   Epoch: [155/300][80/283], lr: 0.00000037 	 loss = 0.1546(0.5412)
2023/11/23 03:49:31 - INFO - root -   Epoch: [155/300][100/283], lr: 0.00000037 	 loss = 0.1537(0.5025)
2023/11/23 03:49:55 - INFO - root -   Epoch: [155/300][120/283], lr: 0.00000037 	 loss = 0.1581(0.4676)
2023/11/23 03:50:03 - INFO - root -   Epoch: [155/300][140/283], lr: 0.00000037 	 loss = 0.0360(0.4317)
2023/11/23 03:50:19 - INFO - root -   Epoch: [155/300][160/283], lr: 0.00000037 	 loss = 2.1437(0.4365)
2023/11/23 03:50:34 - INFO - root -   Epoch: [155/300][180/283], lr: 0.00000037 	 loss = 0.0091(0.4447)
2023/11/23 03:50:48 - INFO - root -   Epoch: [155/300][200/283], lr: 0.00000037 	 loss = 0.1143(0.4392)
2023/11/23 03:51:09 - INFO - root -   Epoch: [155/300][220/283], lr: 0.00000037 	 loss = 0.9174(0.4319)
2023/11/23 03:51:19 - INFO - root -   Epoch: [155/300][240/283], lr: 0.00000037 	 loss = 1.0873(0.4412)
2023/11/23 03:51:52 - INFO - root -   Epoch: [155/300][260/283], lr: 0.00000037 	 loss = 2.4603(0.4388)
2023/11/23 03:52:02 - INFO - root -   Epoch: [155/300][280/283], lr: 0.00000037 	 loss = 1.6323(0.4280)
2023/11/23 03:52:04 - INFO - root -   Epoch: [155/300] 	 loss = 0.4257
2023/11/23 03:52:04 - INFO - root -   train_accuracy = 0.8622
2023/11/23 03:52:10 - INFO - root -   Epoch: [156/300][0/283], lr: 0.00000037 	 loss = 0.1250(0.1250)
2023/11/23 03:52:23 - INFO - root -   Epoch: [156/300][20/283], lr: 0.00000037 	 loss = 2.5123(0.4905)
2023/11/23 03:52:38 - INFO - root -   Epoch: [156/300][40/283], lr: 0.00000037 	 loss = 0.0363(0.5807)
2023/11/23 03:52:57 - INFO - root -   Epoch: [156/300][60/283], lr: 0.00000037 	 loss = 0.0063(0.5541)
2023/11/23 03:53:13 - INFO - root -   Epoch: [156/300][80/283], lr: 0.00000037 	 loss = 0.2631(0.5354)
2023/11/23 03:53:29 - INFO - root -   Epoch: [156/300][100/283], lr: 0.00000037 	 loss = 0.4427(0.5016)
2023/11/23 03:53:43 - INFO - root -   Epoch: [156/300][120/283], lr: 0.00000037 	 loss = 0.1818(0.4675)
2023/11/23 03:53:59 - INFO - root -   Epoch: [156/300][140/283], lr: 0.00000037 	 loss = 0.2411(0.4445)
2023/11/23 03:54:12 - INFO - root -   Epoch: [156/300][160/283], lr: 0.00000037 	 loss = 0.5646(0.4230)
2023/11/23 03:54:32 - INFO - root -   Epoch: [156/300][180/283], lr: 0.00000037 	 loss = 0.0846(0.4123)
2023/11/23 03:54:48 - INFO - root -   Epoch: [156/300][200/283], lr: 0.00000037 	 loss = 0.3126(0.4256)
2023/11/23 03:55:03 - INFO - root -   Epoch: [156/300][220/283], lr: 0.00000037 	 loss = 0.2848(0.4190)
2023/11/23 03:55:12 - INFO - root -   Epoch: [156/300][240/283], lr: 0.00000037 	 loss = 0.9556(0.4216)
2023/11/23 03:55:25 - INFO - root -   Epoch: [156/300][260/283], lr: 0.00000037 	 loss = 0.8420(0.4094)
2023/11/23 03:55:35 - INFO - root -   Epoch: [156/300][280/283], lr: 0.00000037 	 loss = 3.0313(0.4029)
2023/11/23 03:55:36 - INFO - root -   Epoch: [156/300] 	 loss = 0.4004
2023/11/23 03:55:36 - INFO - root -   train_accuracy = 0.8498
2023/11/23 03:55:50 - INFO - root -   Epoch: [157/300][0/283], lr: 0.00000037 	 loss = 0.1420(0.1420)
2023/11/23 03:56:02 - INFO - root -   Epoch: [157/300][20/283], lr: 0.00000037 	 loss = 0.6403(0.6420)
2023/11/23 03:56:12 - INFO - root -   Epoch: [157/300][40/283], lr: 0.00000037 	 loss = 0.7434(0.6451)
2023/11/23 03:56:25 - INFO - root -   Epoch: [157/300][60/283], lr: 0.00000037 	 loss = 0.0290(0.6019)
2023/11/23 03:56:44 - INFO - root -   Epoch: [157/300][80/283], lr: 0.00000037 	 loss = 0.2834(0.6075)
2023/11/23 03:56:59 - INFO - root -   Epoch: [157/300][100/283], lr: 0.00000037 	 loss = 0.0612(0.5685)
2023/11/23 03:57:16 - INFO - root -   Epoch: [157/300][120/283], lr: 0.00000037 	 loss = 0.0917(0.5313)
2023/11/23 03:57:35 - INFO - root -   Epoch: [157/300][140/283], lr: 0.00000037 	 loss = 0.0116(0.4799)
2023/11/23 03:57:50 - INFO - root -   Epoch: [157/300][160/283], lr: 0.00000037 	 loss = 1.2425(0.4770)
2023/11/23 03:58:11 - INFO - root -   Epoch: [157/300][180/283], lr: 0.00000037 	 loss = 0.0382(0.4748)
2023/11/23 03:58:24 - INFO - root -   Epoch: [157/300][200/283], lr: 0.00000037 	 loss = 0.1149(0.4615)
2023/11/23 03:58:39 - INFO - root -   Epoch: [157/300][220/283], lr: 0.00000037 	 loss = 0.0947(0.4436)
2023/11/23 03:58:55 - INFO - root -   Epoch: [157/300][240/283], lr: 0.00000037 	 loss = 0.7061(0.4448)
2023/11/23 03:59:15 - INFO - root -   Epoch: [157/300][260/283], lr: 0.00000037 	 loss = 1.0588(0.4375)
2023/11/23 03:59:26 - INFO - root -   Epoch: [157/300][280/283], lr: 0.00000037 	 loss = 2.7360(0.4301)
2023/11/23 03:59:27 - INFO - root -   Epoch: [157/300] 	 loss = 0.4279
2023/11/23 03:59:27 - INFO - root -   train_accuracy = 0.8410
2023/11/23 03:59:36 - INFO - root -   Epoch: [158/300][0/283], lr: 0.00000038 	 loss = 0.2215(0.2215)
2023/11/23 03:59:47 - INFO - root -   Epoch: [158/300][20/283], lr: 0.00000038 	 loss = 0.3565(0.4803)
2023/11/23 04:00:00 - INFO - root -   Epoch: [158/300][40/283], lr: 0.00000038 	 loss = 0.0778(0.5459)
2023/11/23 04:00:09 - INFO - root -   Epoch: [158/300][60/283], lr: 0.00000038 	 loss = 0.0075(0.5322)
2023/11/23 04:00:24 - INFO - root -   Epoch: [158/300][80/283], lr: 0.00000038 	 loss = 0.4187(0.5555)
2023/11/23 04:00:45 - INFO - root -   Epoch: [158/300][100/283], lr: 0.00000038 	 loss = 0.2815(0.5189)
2023/11/23 04:00:55 - INFO - root -   Epoch: [158/300][120/283], lr: 0.00000038 	 loss = 0.1872(0.4774)
2023/11/23 04:01:21 - INFO - root -   Epoch: [158/300][140/283], lr: 0.00000038 	 loss = 0.1424(0.4360)
2023/11/23 04:01:29 - INFO - root -   Epoch: [158/300][160/283], lr: 0.00000038 	 loss = 0.4695(0.4216)
2023/11/23 04:01:47 - INFO - root -   Epoch: [158/300][180/283], lr: 0.00000038 	 loss = 0.3080(0.4492)
2023/11/23 04:01:58 - INFO - root -   Epoch: [158/300][200/283], lr: 0.00000038 	 loss = 0.0474(0.4508)
2023/11/23 04:02:13 - INFO - root -   Epoch: [158/300][220/283], lr: 0.00000038 	 loss = 0.2321(0.4441)
2023/11/23 04:02:31 - INFO - root -   Epoch: [158/300][240/283], lr: 0.00000038 	 loss = 1.0483(0.4557)
2023/11/23 04:02:46 - INFO - root -   Epoch: [158/300][260/283], lr: 0.00000038 	 loss = 2.2518(0.4574)
2023/11/23 04:02:56 - INFO - root -   Epoch: [158/300][280/283], lr: 0.00000038 	 loss = 1.9761(0.4515)
2023/11/23 04:02:57 - INFO - root -   Epoch: [158/300] 	 loss = 0.4489
2023/11/23 04:02:57 - INFO - root -   train_accuracy = 0.8375
2023/11/23 04:03:08 - INFO - root -   Epoch: [159/300][0/283], lr: 0.00000038 	 loss = 0.2211(0.2211)
2023/11/23 04:03:18 - INFO - root -   Epoch: [159/300][20/283], lr: 0.00000038 	 loss = 0.1393(0.6323)
2023/11/23 04:03:35 - INFO - root -   Epoch: [159/300][40/283], lr: 0.00000038 	 loss = 0.3074(0.6822)
2023/11/23 04:03:49 - INFO - root -   Epoch: [159/300][60/283], lr: 0.00000038 	 loss = 0.2659(0.6651)
2023/11/23 04:04:13 - INFO - root -   Epoch: [159/300][80/283], lr: 0.00000038 	 loss = 0.5464(0.7091)
2023/11/23 04:04:27 - INFO - root -   Epoch: [159/300][100/283], lr: 0.00000038 	 loss = 0.2855(0.6348)
2023/11/23 04:04:42 - INFO - root -   Epoch: [159/300][120/283], lr: 0.00000038 	 loss = 0.2441(0.6028)
2023/11/23 04:05:00 - INFO - root -   Epoch: [159/300][140/283], lr: 0.00000038 	 loss = 0.1448(0.5591)
2023/11/23 04:05:17 - INFO - root -   Epoch: [159/300][160/283], lr: 0.00000038 	 loss = 1.2237(0.5368)
2023/11/23 04:05:25 - INFO - root -   Epoch: [159/300][180/283], lr: 0.00000038 	 loss = 0.0395(0.5163)
2023/11/23 04:05:44 - INFO - root -   Epoch: [159/300][200/283], lr: 0.00000038 	 loss = 0.2282(0.5186)
2023/11/23 04:06:01 - INFO - root -   Epoch: [159/300][220/283], lr: 0.00000038 	 loss = 0.3689(0.5079)
2023/11/23 04:06:15 - INFO - root -   Epoch: [159/300][240/283], lr: 0.00000038 	 loss = 0.9678(0.5077)
2023/11/23 04:06:26 - INFO - root -   Epoch: [159/300][260/283], lr: 0.00000038 	 loss = 1.7345(0.4939)
2023/11/23 04:06:41 - INFO - root -   Epoch: [159/300][280/283], lr: 0.00000038 	 loss = 2.6136(0.4829)
2023/11/23 04:06:42 - INFO - root -   Epoch: [159/300] 	 loss = 0.4798
2023/11/23 04:09:37 - INFO - root -   precision = 0.8322
2023/11/23 04:09:37 - INFO - root -   eval_loss = 0.4556
2023/11/23 04:09:37 - INFO - root -   eval_acc = 0.8322
2023/11/23 04:09:38 - INFO - root -   train_accuracy = 0.8145
2023/11/23 04:09:40 - INFO - root -   Epoch: [160/300][0/283], lr: 0.00000038 	 loss = 0.0545(0.0545)
2023/11/23 04:10:01 - INFO - root -   Epoch: [160/300][20/283], lr: 0.00000038 	 loss = 0.6928(0.6662)
2023/11/23 04:10:14 - INFO - root -   Epoch: [160/300][40/283], lr: 0.00000038 	 loss = 0.0543(0.6822)
2023/11/23 04:10:34 - INFO - root -   Epoch: [160/300][60/283], lr: 0.00000038 	 loss = 0.0139(0.6528)
2023/11/23 04:10:48 - INFO - root -   Epoch: [160/300][80/283], lr: 0.00000038 	 loss = 0.1592(0.6494)
2023/11/23 04:11:18 - INFO - root -   Epoch: [160/300][100/283], lr: 0.00000038 	 loss = 0.3824(0.6026)
2023/11/23 04:11:26 - INFO - root -   Epoch: [160/300][120/283], lr: 0.00000038 	 loss = 0.2742(0.5408)
2023/11/23 04:11:44 - INFO - root -   Epoch: [160/300][140/283], lr: 0.00000038 	 loss = 0.0348(0.4969)
2023/11/23 04:12:00 - INFO - root -   Epoch: [160/300][160/283], lr: 0.00000038 	 loss = 0.5357(0.4716)
2023/11/23 04:12:20 - INFO - root -   Epoch: [160/300][180/283], lr: 0.00000038 	 loss = 0.0255(0.4727)
2023/11/23 04:12:30 - INFO - root -   Epoch: [160/300][200/283], lr: 0.00000038 	 loss = 0.1142(0.4658)
2023/11/23 04:12:44 - INFO - root -   Epoch: [160/300][220/283], lr: 0.00000038 	 loss = 0.1575(0.4544)
2023/11/23 04:12:59 - INFO - root -   Epoch: [160/300][240/283], lr: 0.00000038 	 loss = 0.9100(0.4504)
2023/11/23 04:13:12 - INFO - root -   Epoch: [160/300][260/283], lr: 0.00000038 	 loss = 0.5064(0.4398)
2023/11/23 04:13:20 - INFO - root -   Epoch: [160/300][280/283], lr: 0.00000038 	 loss = 2.0338(0.4307)
2023/11/23 04:13:21 - INFO - root -   Epoch: [160/300] 	 loss = 0.4283
2023/11/23 04:13:21 - INFO - root -   train_accuracy = 0.8551
2023/11/23 04:13:28 - INFO - root -   Epoch: [161/300][0/283], lr: 0.00000038 	 loss = 0.0944(0.0944)
2023/11/23 04:13:43 - INFO - root -   Epoch: [161/300][20/283], lr: 0.00000038 	 loss = 0.3276(0.3627)
2023/11/23 04:13:58 - INFO - root -   Epoch: [161/300][40/283], lr: 0.00000038 	 loss = 0.1364(0.5383)
2023/11/23 04:14:15 - INFO - root -   Epoch: [161/300][60/283], lr: 0.00000038 	 loss = 0.0139(0.5084)
2023/11/23 04:14:34 - INFO - root -   Epoch: [161/300][80/283], lr: 0.00000038 	 loss = 0.1666(0.5594)
2023/11/23 04:14:49 - INFO - root -   Epoch: [161/300][100/283], lr: 0.00000038 	 loss = 0.4319(0.5087)
2023/11/23 04:15:07 - INFO - root -   Epoch: [161/300][120/283], lr: 0.00000038 	 loss = 0.2354(0.4888)
2023/11/23 04:15:23 - INFO - root -   Epoch: [161/300][140/283], lr: 0.00000038 	 loss = 0.1909(0.4587)
2023/11/23 04:15:34 - INFO - root -   Epoch: [161/300][160/283], lr: 0.00000038 	 loss = 1.0336(0.4431)
2023/11/23 04:15:50 - INFO - root -   Epoch: [161/300][180/283], lr: 0.00000038 	 loss = 0.0974(0.4387)
2023/11/23 04:16:07 - INFO - root -   Epoch: [161/300][200/283], lr: 0.00000038 	 loss = 0.0615(0.4390)
2023/11/23 04:16:17 - INFO - root -   Epoch: [161/300][220/283], lr: 0.00000038 	 loss = 0.1658(0.4400)
2023/11/23 04:16:33 - INFO - root -   Epoch: [161/300][240/283], lr: 0.00000038 	 loss = 0.5441(0.4394)
2023/11/23 04:16:52 - INFO - root -   Epoch: [161/300][260/283], lr: 0.00000038 	 loss = 1.4844(0.4346)
2023/11/23 04:17:04 - INFO - root -   Epoch: [161/300][280/283], lr: 0.00000038 	 loss = 2.8435(0.4355)
2023/11/23 04:17:05 - INFO - root -   Epoch: [161/300] 	 loss = 0.4329
2023/11/23 04:17:05 - INFO - root -   train_accuracy = 0.8375
2023/11/23 04:17:11 - INFO - root -   Epoch: [162/300][0/283], lr: 0.00000038 	 loss = 0.2521(0.2521)
2023/11/23 04:17:26 - INFO - root -   Epoch: [162/300][20/283], lr: 0.00000038 	 loss = 0.2065(0.5471)
2023/11/23 04:17:42 - INFO - root -   Epoch: [162/300][40/283], lr: 0.00000038 	 loss = 0.0173(0.6050)
2023/11/23 04:17:57 - INFO - root -   Epoch: [162/300][60/283], lr: 0.00000038 	 loss = 0.0125(0.5420)
2023/11/23 04:18:08 - INFO - root -   Epoch: [162/300][80/283], lr: 0.00000038 	 loss = 0.0640(0.5580)
2023/11/23 04:18:29 - INFO - root -   Epoch: [162/300][100/283], lr: 0.00000038 	 loss = 0.4672(0.5194)
2023/11/23 04:18:37 - INFO - root -   Epoch: [162/300][120/283], lr: 0.00000038 	 loss = 0.1373(0.4852)
2023/11/23 04:18:53 - INFO - root -   Epoch: [162/300][140/283], lr: 0.00000038 	 loss = 0.4103(0.4542)
2023/11/23 04:19:11 - INFO - root -   Epoch: [162/300][160/283], lr: 0.00000038 	 loss = 2.2396(0.4522)
2023/11/23 04:19:28 - INFO - root -   Epoch: [162/300][180/283], lr: 0.00000038 	 loss = 0.0590(0.4596)
2023/11/23 04:19:48 - INFO - root -   Epoch: [162/300][200/283], lr: 0.00000038 	 loss = 0.3348(0.4531)
2023/11/23 04:20:03 - INFO - root -   Epoch: [162/300][220/283], lr: 0.00000038 	 loss = 0.2725(0.4428)
2023/11/23 04:20:18 - INFO - root -   Epoch: [162/300][240/283], lr: 0.00000038 	 loss = 0.1067(0.4378)
2023/11/23 04:20:38 - INFO - root -   Epoch: [162/300][260/283], lr: 0.00000038 	 loss = 1.7362(0.4355)
2023/11/23 04:20:50 - INFO - root -   Epoch: [162/300][280/283], lr: 0.00000038 	 loss = 2.7942(0.4270)
2023/11/23 04:20:51 - INFO - root -   Epoch: [162/300] 	 loss = 0.4254
2023/11/23 04:20:51 - INFO - root -   train_accuracy = 0.8534
2023/11/23 04:20:52 - INFO - root -   Epoch: [163/300][0/283], lr: 0.00000039 	 loss = 0.0421(0.0421)
2023/11/23 04:21:13 - INFO - root -   Epoch: [163/300][20/283], lr: 0.00000039 	 loss = 0.1230(0.5322)
2023/11/23 04:21:30 - INFO - root -   Epoch: [163/300][40/283], lr: 0.00000039 	 loss = 0.0890(0.5970)
2023/11/23 04:21:48 - INFO - root -   Epoch: [163/300][60/283], lr: 0.00000039 	 loss = 0.0239(0.5639)
2023/11/23 04:22:07 - INFO - root -   Epoch: [163/300][80/283], lr: 0.00000039 	 loss = 0.5113(0.5772)
2023/11/23 04:22:17 - INFO - root -   Epoch: [163/300][100/283], lr: 0.00000039 	 loss = 0.0357(0.5389)
2023/11/23 04:22:44 - INFO - root -   Epoch: [163/300][120/283], lr: 0.00000039 	 loss = 0.4487(0.5045)
2023/11/23 04:22:57 - INFO - root -   Epoch: [163/300][140/283], lr: 0.00000039 	 loss = 0.2764(0.4583)
2023/11/23 04:23:14 - INFO - root -   Epoch: [163/300][160/283], lr: 0.00000039 	 loss = 0.9204(0.4538)
2023/11/23 04:23:30 - INFO - root -   Epoch: [163/300][180/283], lr: 0.00000039 	 loss = 0.0628(0.4569)
2023/11/23 04:23:39 - INFO - root -   Epoch: [163/300][200/283], lr: 0.00000039 	 loss = 0.0504(0.4578)
2023/11/23 04:23:59 - INFO - root -   Epoch: [163/300][220/283], lr: 0.00000039 	 loss = 0.0402(0.4467)
2023/11/23 04:24:17 - INFO - root -   Epoch: [163/300][240/283], lr: 0.00000039 	 loss = 1.3753(0.4632)
2023/11/23 04:24:32 - INFO - root -   Epoch: [163/300][260/283], lr: 0.00000039 	 loss = 1.0045(0.4563)
2023/11/23 04:24:41 - INFO - root -   Epoch: [163/300][280/283], lr: 0.00000039 	 loss = 2.7755(0.4480)
2023/11/23 04:24:42 - INFO - root -   Epoch: [163/300] 	 loss = 0.4452
2023/11/23 04:24:42 - INFO - root -   train_accuracy = 0.8269
2023/11/23 04:24:50 - INFO - root -   Epoch: [164/300][0/283], lr: 0.00000039 	 loss = 0.0892(0.0892)
2023/11/23 04:25:04 - INFO - root -   Epoch: [164/300][20/283], lr: 0.00000039 	 loss = 0.2346(0.4389)
2023/11/23 04:25:20 - INFO - root -   Epoch: [164/300][40/283], lr: 0.00000039 	 loss = 0.1819(0.5395)
2023/11/23 04:25:36 - INFO - root -   Epoch: [164/300][60/283], lr: 0.00000039 	 loss = 0.0641(0.5345)
2023/11/23 04:25:48 - INFO - root -   Epoch: [164/300][80/283], lr: 0.00000039 	 loss = 0.0347(0.5208)
2023/11/23 04:26:03 - INFO - root -   Epoch: [164/300][100/283], lr: 0.00000039 	 loss = 0.1398(0.4853)
2023/11/23 04:26:11 - INFO - root -   Epoch: [164/300][120/283], lr: 0.00000039 	 loss = 0.0596(0.4411)
2023/11/23 04:26:31 - INFO - root -   Epoch: [164/300][140/283], lr: 0.00000039 	 loss = 0.0684(0.4205)
2023/11/23 04:26:48 - INFO - root -   Epoch: [164/300][160/283], lr: 0.00000039 	 loss = 1.3490(0.4210)
2023/11/23 04:26:58 - INFO - root -   Epoch: [164/300][180/283], lr: 0.00000039 	 loss = 0.1274(0.4178)
2023/11/23 04:27:11 - INFO - root -   Epoch: [164/300][200/283], lr: 0.00000039 	 loss = 0.0377(0.4105)
2023/11/23 04:27:26 - INFO - root -   Epoch: [164/300][220/283], lr: 0.00000039 	 loss = 0.0850(0.4037)
2023/11/23 04:27:38 - INFO - root -   Epoch: [164/300][240/283], lr: 0.00000039 	 loss = 0.2485(0.4106)
2023/11/23 04:27:49 - INFO - root -   Epoch: [164/300][260/283], lr: 0.00000039 	 loss = 1.0785(0.4004)
2023/11/23 04:28:12 - INFO - root -   Epoch: [164/300][280/283], lr: 0.00000039 	 loss = 1.7484(0.3999)
2023/11/23 04:28:13 - INFO - root -   Epoch: [164/300] 	 loss = 0.3974
2023/11/23 04:31:11 - INFO - root -   precision = 0.8112
2023/11/23 04:31:11 - INFO - root -   eval_loss = 0.5192
2023/11/23 04:31:11 - INFO - root -   eval_acc = 0.8112
2023/11/23 04:31:12 - INFO - root -   train_accuracy = 0.8428
2023/11/23 04:31:13 - INFO - root -   Epoch: [165/300][0/283], lr: 0.00000039 	 loss = 0.0414(0.0414)
2023/11/23 04:31:41 - INFO - root -   Epoch: [165/300][20/283], lr: 0.00000039 	 loss = 0.8705(0.5623)
2023/11/23 04:31:54 - INFO - root -   Epoch: [165/300][40/283], lr: 0.00000039 	 loss = 0.0134(0.5729)
2023/11/23 04:32:10 - INFO - root -   Epoch: [165/300][60/283], lr: 0.00000039 	 loss = 0.1076(0.5551)
2023/11/23 04:32:32 - INFO - root -   Epoch: [165/300][80/283], lr: 0.00000039 	 loss = 0.5088(0.5350)
2023/11/23 04:32:40 - INFO - root -   Epoch: [165/300][100/283], lr: 0.00000039 	 loss = 0.1677(0.4854)
2023/11/23 04:32:52 - INFO - root -   Epoch: [165/300][120/283], lr: 0.00000039 	 loss = 0.0435(0.4439)
2023/11/23 04:33:09 - INFO - root -   Epoch: [165/300][140/283], lr: 0.00000039 	 loss = 0.0075(0.4137)
2023/11/23 04:33:28 - INFO - root -   Epoch: [165/300][160/283], lr: 0.00000039 	 loss = 0.2554(0.4128)
2023/11/23 04:33:45 - INFO - root -   Epoch: [165/300][180/283], lr: 0.00000039 	 loss = 0.0336(0.4182)
2023/11/23 04:34:03 - INFO - root -   Epoch: [165/300][200/283], lr: 0.00000039 	 loss = 0.2074(0.4266)
2023/11/23 04:34:19 - INFO - root -   Epoch: [165/300][220/283], lr: 0.00000039 	 loss = 0.2624(0.4153)
2023/11/23 04:34:28 - INFO - root -   Epoch: [165/300][240/283], lr: 0.00000039 	 loss = 1.1020(0.4143)
2023/11/23 04:34:48 - INFO - root -   Epoch: [165/300][260/283], lr: 0.00000039 	 loss = 1.5091(0.4034)
2023/11/23 04:35:04 - INFO - root -   Epoch: [165/300][280/283], lr: 0.00000039 	 loss = 2.5673(0.3983)
2023/11/23 04:35:05 - INFO - root -   Epoch: [165/300] 	 loss = 0.3962
2023/11/23 04:35:05 - INFO - root -   train_accuracy = 0.8551
2023/11/23 04:35:06 - INFO - root -   Epoch: [166/300][0/283], lr: 0.00000039 	 loss = 0.1176(0.1176)
2023/11/23 04:35:23 - INFO - root -   Epoch: [166/300][20/283], lr: 0.00000039 	 loss = 2.0676(0.5233)
2023/11/23 04:35:44 - INFO - root -   Epoch: [166/300][40/283], lr: 0.00000039 	 loss = 0.1406(0.5568)
2023/11/23 04:35:59 - INFO - root -   Epoch: [166/300][60/283], lr: 0.00000039 	 loss = 0.1278(0.5287)
2023/11/23 04:36:17 - INFO - root -   Epoch: [166/300][80/283], lr: 0.00000039 	 loss = 0.0372(0.5499)
2023/11/23 04:36:30 - INFO - root -   Epoch: [166/300][100/283], lr: 0.00000039 	 loss = 0.0762(0.5294)
2023/11/23 04:36:43 - INFO - root -   Epoch: [166/300][120/283], lr: 0.00000039 	 loss = 0.2259(0.4862)
2023/11/23 04:36:55 - INFO - root -   Epoch: [166/300][140/283], lr: 0.00000039 	 loss = 0.0078(0.4622)
2023/11/23 04:37:06 - INFO - root -   Epoch: [166/300][160/283], lr: 0.00000039 	 loss = 0.2181(0.4445)
2023/11/23 04:37:15 - INFO - root -   Epoch: [166/300][180/283], lr: 0.00000039 	 loss = 0.0626(0.4644)
2023/11/23 04:37:33 - INFO - root -   Epoch: [166/300][200/283], lr: 0.00000039 	 loss = 0.2480(0.4646)
2023/11/23 04:37:43 - INFO - root -   Epoch: [166/300][220/283], lr: 0.00000039 	 loss = 0.1907(0.4493)
2023/11/23 04:37:58 - INFO - root -   Epoch: [166/300][240/283], lr: 0.00000039 	 loss = 0.7611(0.4435)
2023/11/23 04:38:15 - INFO - root -   Epoch: [166/300][260/283], lr: 0.00000039 	 loss = 1.1319(0.4373)
2023/11/23 04:38:31 - INFO - root -   Epoch: [166/300][280/283], lr: 0.00000039 	 loss = 1.8581(0.4262)
2023/11/23 04:38:32 - INFO - root -   Epoch: [166/300] 	 loss = 0.4244
2023/11/23 04:38:32 - INFO - root -   train_accuracy = 0.8569
2023/11/23 04:38:33 - INFO - root -   Epoch: [167/300][0/283], lr: 0.00000039 	 loss = 0.0410(0.0410)
2023/11/23 04:38:56 - INFO - root -   Epoch: [167/300][20/283], lr: 0.00000039 	 loss = 0.1662(0.5101)
2023/11/23 04:39:11 - INFO - root -   Epoch: [167/300][40/283], lr: 0.00000039 	 loss = 0.2170(0.5229)
2023/11/23 04:39:24 - INFO - root -   Epoch: [167/300][60/283], lr: 0.00000039 	 loss = 0.0145(0.4903)
2023/11/23 04:39:42 - INFO - root -   Epoch: [167/300][80/283], lr: 0.00000039 	 loss = 0.3656(0.5311)
2023/11/23 04:39:55 - INFO - root -   Epoch: [167/300][100/283], lr: 0.00000039 	 loss = 0.1494(0.5018)
2023/11/23 04:40:08 - INFO - root -   Epoch: [167/300][120/283], lr: 0.00000039 	 loss = 0.2782(0.4537)
2023/11/23 04:40:26 - INFO - root -   Epoch: [167/300][140/283], lr: 0.00000039 	 loss = 0.0113(0.4376)
2023/11/23 04:40:36 - INFO - root -   Epoch: [167/300][160/283], lr: 0.00000039 	 loss = 0.6481(0.4254)
2023/11/23 04:40:59 - INFO - root -   Epoch: [167/300][180/283], lr: 0.00000039 	 loss = 0.1300(0.4341)
2023/11/23 04:41:13 - INFO - root -   Epoch: [167/300][200/283], lr: 0.00000039 	 loss = 0.3781(0.4313)
2023/11/23 04:41:29 - INFO - root -   Epoch: [167/300][220/283], lr: 0.00000039 	 loss = 0.1260(0.4211)
2023/11/23 04:41:47 - INFO - root -   Epoch: [167/300][240/283], lr: 0.00000039 	 loss = 0.4200(0.4163)
2023/11/23 04:42:04 - INFO - root -   Epoch: [167/300][260/283], lr: 0.00000039 	 loss = 2.0113(0.4094)
2023/11/23 04:42:15 - INFO - root -   Epoch: [167/300][280/283], lr: 0.00000039 	 loss = 2.5861(0.4044)
2023/11/23 04:42:16 - INFO - root -   Epoch: [167/300] 	 loss = 0.4018
2023/11/23 04:42:16 - INFO - root -   train_accuracy = 0.8604
2023/11/23 04:42:24 - INFO - root -   Epoch: [168/300][0/283], lr: 0.00000039 	 loss = 0.0101(0.0101)
2023/11/23 04:42:49 - INFO - root -   Epoch: [168/300][20/283], lr: 0.00000039 	 loss = 1.3071(0.6933)
2023/11/23 04:43:03 - INFO - root -   Epoch: [168/300][40/283], lr: 0.00000039 	 loss = 0.0715(0.6530)
2023/11/23 04:43:16 - INFO - root -   Epoch: [168/300][60/283], lr: 0.00000039 	 loss = 0.0084(0.5688)
2023/11/23 04:43:30 - INFO - root -   Epoch: [168/300][80/283], lr: 0.00000039 	 loss = 0.0583(0.5893)
2023/11/23 04:43:48 - INFO - root -   Epoch: [168/300][100/283], lr: 0.00000039 	 loss = 0.0470(0.5443)
2023/11/23 04:44:08 - INFO - root -   Epoch: [168/300][120/283], lr: 0.00000039 	 loss = 0.3406(0.5020)
2023/11/23 04:44:21 - INFO - root -   Epoch: [168/300][140/283], lr: 0.00000039 	 loss = 0.0441(0.4619)
2023/11/23 04:44:36 - INFO - root -   Epoch: [168/300][160/283], lr: 0.00000039 	 loss = 0.1882(0.4525)
2023/11/23 04:44:49 - INFO - root -   Epoch: [168/300][180/283], lr: 0.00000039 	 loss = 0.0941(0.4526)
2023/11/23 04:45:02 - INFO - root -   Epoch: [168/300][200/283], lr: 0.00000039 	 loss = 0.1084(0.4529)
2023/11/23 04:45:15 - INFO - root -   Epoch: [168/300][220/283], lr: 0.00000039 	 loss = 0.1693(0.4388)
2023/11/23 04:45:29 - INFO - root -   Epoch: [168/300][240/283], lr: 0.00000039 	 loss = 0.5177(0.4326)
2023/11/23 04:45:46 - INFO - root -   Epoch: [168/300][260/283], lr: 0.00000039 	 loss = 2.1209(0.4253)
2023/11/23 04:45:59 - INFO - root -   Epoch: [168/300][280/283], lr: 0.00000039 	 loss = 2.9708(0.4156)
2023/11/23 04:46:00 - INFO - root -   Epoch: [168/300] 	 loss = 0.4128
2023/11/23 04:46:00 - INFO - root -   train_accuracy = 0.8498
2023/11/23 04:46:01 - INFO - root -   Epoch: [169/300][0/283], lr: 0.00000040 	 loss = 0.1203(0.1203)
2023/11/23 04:46:17 - INFO - root -   Epoch: [169/300][20/283], lr: 0.00000040 	 loss = 0.4728(0.3582)
2023/11/23 04:46:25 - INFO - root -   Epoch: [169/300][40/283], lr: 0.00000040 	 loss = 0.0234(0.4652)
2023/11/23 04:46:37 - INFO - root -   Epoch: [169/300][60/283], lr: 0.00000040 	 loss = 0.1404(0.4365)
2023/11/23 04:46:54 - INFO - root -   Epoch: [169/300][80/283], lr: 0.00000040 	 loss = 0.2263(0.4878)
2023/11/23 04:47:09 - INFO - root -   Epoch: [169/300][100/283], lr: 0.00000040 	 loss = 0.2084(0.4412)
2023/11/23 04:47:25 - INFO - root -   Epoch: [169/300][120/283], lr: 0.00000040 	 loss = 0.0421(0.4017)
2023/11/23 04:47:39 - INFO - root -   Epoch: [169/300][140/283], lr: 0.00000040 	 loss = 0.0239(0.3788)
2023/11/23 04:48:00 - INFO - root -   Epoch: [169/300][160/283], lr: 0.00000040 	 loss = 2.3344(0.3871)
2023/11/23 04:48:13 - INFO - root -   Epoch: [169/300][180/283], lr: 0.00000040 	 loss = 0.0957(0.4099)
2023/11/23 04:48:29 - INFO - root -   Epoch: [169/300][200/283], lr: 0.00000040 	 loss = 0.0248(0.4047)
2023/11/23 04:48:45 - INFO - root -   Epoch: [169/300][220/283], lr: 0.00000040 	 loss = 0.1153(0.3964)
2023/11/23 04:49:02 - INFO - root -   Epoch: [169/300][240/283], lr: 0.00000040 	 loss = 0.6125(0.3994)
2023/11/23 04:49:18 - INFO - root -   Epoch: [169/300][260/283], lr: 0.00000040 	 loss = 1.1917(0.3989)
2023/11/23 04:49:39 - INFO - root -   Epoch: [169/300][280/283], lr: 0.00000040 	 loss = 3.9714(0.4032)
2023/11/23 04:49:40 - INFO - root -   Epoch: [169/300] 	 loss = 0.4009
2023/11/23 04:52:35 - INFO - root -   precision = 0.8392
2023/11/23 04:52:35 - INFO - root -   eval_loss = 0.4852
2023/11/23 04:52:35 - INFO - root -   eval_acc = 0.8392
2023/11/23 04:52:36 - INFO - root -   train_accuracy = 0.8710
2023/11/23 04:52:37 - INFO - root -   Epoch: [170/300][0/283], lr: 0.00000040 	 loss = 0.0136(0.0136)
2023/11/23 04:53:04 - INFO - root -   Epoch: [170/300][20/283], lr: 0.00000040 	 loss = 0.8627(0.4649)
2023/11/23 04:53:17 - INFO - root -   Epoch: [170/300][40/283], lr: 0.00000040 	 loss = 0.4776(0.5787)
2023/11/23 04:53:31 - INFO - root -   Epoch: [170/300][60/283], lr: 0.00000040 	 loss = 0.3696(0.4926)
2023/11/23 04:53:44 - INFO - root -   Epoch: [170/300][80/283], lr: 0.00000040 	 loss = 0.0470(0.5145)
2023/11/23 04:54:02 - INFO - root -   Epoch: [170/300][100/283], lr: 0.00000040 	 loss = 0.7194(0.5122)
2023/11/23 04:54:16 - INFO - root -   Epoch: [170/300][120/283], lr: 0.00000040 	 loss = 0.0209(0.5069)
2023/11/23 04:54:34 - INFO - root -   Epoch: [170/300][140/283], lr: 0.00000040 	 loss = 0.0048(0.4827)
2023/11/23 04:54:45 - INFO - root -   Epoch: [170/300][160/283], lr: 0.00000040 	 loss = 0.0885(0.4794)
2023/11/23 04:55:00 - INFO - root -   Epoch: [170/300][180/283], lr: 0.00000040 	 loss = 0.0413(0.4866)
2023/11/23 04:55:17 - INFO - root -   Epoch: [170/300][200/283], lr: 0.00000040 	 loss = 0.2261(0.4700)
2023/11/23 04:55:25 - INFO - root -   Epoch: [170/300][220/283], lr: 0.00000040 	 loss = 0.1529(0.4581)
2023/11/23 04:55:41 - INFO - root -   Epoch: [170/300][240/283], lr: 0.00000040 	 loss = 0.1344(0.4466)
2023/11/23 04:55:56 - INFO - root -   Epoch: [170/300][260/283], lr: 0.00000040 	 loss = 0.6721(0.4338)
2023/11/23 04:56:05 - INFO - root -   Epoch: [170/300][280/283], lr: 0.00000040 	 loss = 2.2843(0.4266)
2023/11/23 04:56:06 - INFO - root -   Epoch: [170/300] 	 loss = 0.4241
2023/11/23 04:56:06 - INFO - root -   train_accuracy = 0.8375
2023/11/23 04:56:14 - INFO - root -   Epoch: [171/300][0/283], lr: 0.00000040 	 loss = 0.2738(0.2738)
2023/11/23 04:56:27 - INFO - root -   Epoch: [171/300][20/283], lr: 0.00000040 	 loss = 0.3395(0.4718)
2023/11/23 04:56:41 - INFO - root -   Epoch: [171/300][40/283], lr: 0.00000040 	 loss = 0.1416(0.5854)
2023/11/23 04:56:52 - INFO - root -   Epoch: [171/300][60/283], lr: 0.00000040 	 loss = 0.0189(0.4951)
2023/11/23 04:57:08 - INFO - root -   Epoch: [171/300][80/283], lr: 0.00000040 	 loss = 0.1158(0.5141)
2023/11/23 04:57:32 - INFO - root -   Epoch: [171/300][100/283], lr: 0.00000040 	 loss = 0.6423(0.5084)
2023/11/23 04:57:46 - INFO - root -   Epoch: [171/300][120/283], lr: 0.00000040 	 loss = 0.1150(0.4842)
2023/11/23 04:57:58 - INFO - root -   Epoch: [171/300][140/283], lr: 0.00000040 	 loss = 0.0119(0.4468)
2023/11/23 04:58:14 - INFO - root -   Epoch: [171/300][160/283], lr: 0.00000040 	 loss = 0.4246(0.4255)
2023/11/23 04:58:24 - INFO - root -   Epoch: [171/300][180/283], lr: 0.00000040 	 loss = 0.0619(0.4424)
2023/11/23 04:58:43 - INFO - root -   Epoch: [171/300][200/283], lr: 0.00000040 	 loss = 0.1492(0.4449)
2023/11/23 04:58:54 - INFO - root -   Epoch: [171/300][220/283], lr: 0.00000040 	 loss = 0.2243(0.4454)
2023/11/23 04:59:10 - INFO - root -   Epoch: [171/300][240/283], lr: 0.00000040 	 loss = 0.2428(0.4429)
2023/11/23 04:59:27 - INFO - root -   Epoch: [171/300][260/283], lr: 0.00000040 	 loss = 0.8180(0.4344)
2023/11/23 04:59:39 - INFO - root -   Epoch: [171/300][280/283], lr: 0.00000040 	 loss = 3.8913(0.4286)
2023/11/23 04:59:40 - INFO - root -   Epoch: [171/300] 	 loss = 0.4261
2023/11/23 04:59:40 - INFO - root -   train_accuracy = 0.8551
2023/11/23 04:59:41 - INFO - root -   Epoch: [172/300][0/283], lr: 0.00000040 	 loss = 0.1938(0.1938)
2023/11/23 05:00:01 - INFO - root -   Epoch: [172/300][20/283], lr: 0.00000040 	 loss = 0.0369(0.4382)
2023/11/23 05:00:16 - INFO - root -   Epoch: [172/300][40/283], lr: 0.00000040 	 loss = 0.1514(0.5482)
2023/11/23 05:00:28 - INFO - root -   Epoch: [172/300][60/283], lr: 0.00000040 	 loss = 0.0268(0.5557)
2023/11/23 05:00:42 - INFO - root -   Epoch: [172/300][80/283], lr: 0.00000040 	 loss = 0.3282(0.5266)
2023/11/23 05:00:54 - INFO - root -   Epoch: [172/300][100/283], lr: 0.00000040 	 loss = 0.0804(0.4779)
2023/11/23 05:01:13 - INFO - root -   Epoch: [172/300][120/283], lr: 0.00000040 	 loss = 0.2362(0.4560)
2023/11/23 05:01:31 - INFO - root -   Epoch: [172/300][140/283], lr: 0.00000040 	 loss = 0.0130(0.4234)
2023/11/23 05:01:51 - INFO - root -   Epoch: [172/300][160/283], lr: 0.00000040 	 loss = 0.1441(0.4233)
2023/11/23 05:02:12 - INFO - root -   Epoch: [172/300][180/283], lr: 0.00000040 	 loss = 0.1336(0.4457)
2023/11/23 05:02:25 - INFO - root -   Epoch: [172/300][200/283], lr: 0.00000040 	 loss = 0.0993(0.4436)
2023/11/23 05:02:39 - INFO - root -   Epoch: [172/300][220/283], lr: 0.00000040 	 loss = 0.1292(0.4332)
2023/11/23 05:02:56 - INFO - root -   Epoch: [172/300][240/283], lr: 0.00000040 	 loss = 0.3303(0.4359)
2023/11/23 05:03:13 - INFO - root -   Epoch: [172/300][260/283], lr: 0.00000040 	 loss = 0.8009(0.4372)
2023/11/23 05:03:21 - INFO - root -   Epoch: [172/300][280/283], lr: 0.00000040 	 loss = 3.2649(0.4237)
2023/11/23 05:03:22 - INFO - root -   Epoch: [172/300] 	 loss = 0.4219
2023/11/23 05:03:22 - INFO - root -   train_accuracy = 0.8428
2023/11/23 05:03:36 - INFO - root -   Epoch: [173/300][0/283], lr: 0.00000040 	 loss = 0.1473(0.1473)
2023/11/23 05:03:46 - INFO - root -   Epoch: [173/300][20/283], lr: 0.00000040 	 loss = 1.2429(0.6827)
2023/11/23 05:04:00 - INFO - root -   Epoch: [173/300][40/283], lr: 0.00000040 	 loss = 0.2889(0.6216)
2023/11/23 05:04:22 - INFO - root -   Epoch: [173/300][60/283], lr: 0.00000040 	 loss = 0.3245(0.6025)
2023/11/23 05:04:32 - INFO - root -   Epoch: [173/300][80/283], lr: 0.00000040 	 loss = 0.0840(0.5614)
2023/11/23 05:04:40 - INFO - root -   Epoch: [173/300][100/283], lr: 0.00000040 	 loss = 0.0981(0.4939)
2023/11/23 05:04:59 - INFO - root -   Epoch: [173/300][120/283], lr: 0.00000040 	 loss = 0.0435(0.4664)
2023/11/23 05:05:07 - INFO - root -   Epoch: [173/300][140/283], lr: 0.00000040 	 loss = 0.0253(0.4172)
2023/11/23 05:05:15 - INFO - root -   Epoch: [173/300][160/283], lr: 0.00000040 	 loss = 0.3676(0.4167)
2023/11/23 05:05:31 - INFO - root -   Epoch: [173/300][180/283], lr: 0.00000040 	 loss = 0.0396(0.4144)
2023/11/23 05:05:50 - INFO - root -   Epoch: [173/300][200/283], lr: 0.00000040 	 loss = 0.0999(0.4078)
2023/11/23 05:06:01 - INFO - root -   Epoch: [173/300][220/283], lr: 0.00000040 	 loss = 0.1076(0.3924)
2023/11/23 05:06:16 - INFO - root -   Epoch: [173/300][240/283], lr: 0.00000040 	 loss = 0.1916(0.4047)
2023/11/23 05:06:34 - INFO - root -   Epoch: [173/300][260/283], lr: 0.00000040 	 loss = 0.7816(0.4008)
2023/11/23 05:06:47 - INFO - root -   Epoch: [173/300][280/283], lr: 0.00000040 	 loss = 1.3261(0.3853)
2023/11/23 05:06:48 - INFO - root -   Epoch: [173/300] 	 loss = 0.3831
2023/11/23 05:06:48 - INFO - root -   train_accuracy = 0.8640
2023/11/23 05:06:59 - INFO - root -   Epoch: [174/300][0/283], lr: 0.00000040 	 loss = 0.1651(0.1651)
2023/11/23 05:07:07 - INFO - root -   Epoch: [174/300][20/283], lr: 0.00000040 	 loss = 0.7429(0.5094)
2023/11/23 05:07:15 - INFO - root -   Epoch: [174/300][40/283], lr: 0.00000040 	 loss = 0.0224(0.5528)
2023/11/23 05:07:31 - INFO - root -   Epoch: [174/300][60/283], lr: 0.00000040 	 loss = 0.0073(0.5665)
2023/11/23 05:07:44 - INFO - root -   Epoch: [174/300][80/283], lr: 0.00000040 	 loss = 1.4780(0.5892)
2023/11/23 05:08:05 - INFO - root -   Epoch: [174/300][100/283], lr: 0.00000040 	 loss = 0.1506(0.5332)
2023/11/23 05:08:17 - INFO - root -   Epoch: [174/300][120/283], lr: 0.00000040 	 loss = 0.1360(0.4787)
2023/11/23 05:08:27 - INFO - root -   Epoch: [174/300][140/283], lr: 0.00000040 	 loss = 0.0077(0.4514)
2023/11/23 05:08:48 - INFO - root -   Epoch: [174/300][160/283], lr: 0.00000040 	 loss = 0.8940(0.4562)
2023/11/23 05:09:02 - INFO - root -   Epoch: [174/300][180/283], lr: 0.00000040 	 loss = 0.0231(0.4581)
2023/11/23 05:09:23 - INFO - root -   Epoch: [174/300][200/283], lr: 0.00000040 	 loss = 0.1758(0.4493)
2023/11/23 05:09:38 - INFO - root -   Epoch: [174/300][220/283], lr: 0.00000040 	 loss = 0.1323(0.4367)
2023/11/23 05:09:50 - INFO - root -   Epoch: [174/300][240/283], lr: 0.00000040 	 loss = 0.4937(0.4350)
2023/11/23 05:10:02 - INFO - root -   Epoch: [174/300][260/283], lr: 0.00000040 	 loss = 2.3572(0.4291)
2023/11/23 05:10:22 - INFO - root -   Epoch: [174/300][280/283], lr: 0.00000040 	 loss = 2.6810(0.4325)
2023/11/23 05:10:23 - INFO - root -   Epoch: [174/300] 	 loss = 0.4299
2023/11/23 05:13:18 - INFO - root -   precision = 0.8252
2023/11/23 05:13:18 - INFO - root -   eval_loss = 0.5167
2023/11/23 05:13:18 - INFO - root -   eval_acc = 0.8252
2023/11/23 05:13:19 - INFO - root -   train_accuracy = 0.8463
2023/11/23 05:13:20 - INFO - root -   Epoch: [175/300][0/283], lr: 0.00000041 	 loss = 0.1045(0.1045)
2023/11/23 05:13:40 - INFO - root -   Epoch: [175/300][20/283], lr: 0.00000041 	 loss = 1.1969(0.6800)
2023/11/23 05:14:03 - INFO - root -   Epoch: [175/300][40/283], lr: 0.00000041 	 loss = 0.0552(0.6249)
2023/11/23 05:14:11 - INFO - root -   Epoch: [175/300][60/283], lr: 0.00000041 	 loss = 0.0239(0.5428)
2023/11/23 05:14:30 - INFO - root -   Epoch: [175/300][80/283], lr: 0.00000041 	 loss = 0.2059(0.5137)
2023/11/23 05:14:49 - INFO - root -   Epoch: [175/300][100/283], lr: 0.00000041 	 loss = 0.1431(0.4995)
2023/11/23 05:15:06 - INFO - root -   Epoch: [175/300][120/283], lr: 0.00000041 	 loss = 0.3097(0.4712)
2023/11/23 05:15:16 - INFO - root -   Epoch: [175/300][140/283], lr: 0.00000041 	 loss = 0.0103(0.4441)
2023/11/23 05:15:25 - INFO - root -   Epoch: [175/300][160/283], lr: 0.00000041 	 loss = 0.3819(0.4396)
2023/11/23 05:15:45 - INFO - root -   Epoch: [175/300][180/283], lr: 0.00000041 	 loss = 0.0595(0.4405)
2023/11/23 05:15:59 - INFO - root -   Epoch: [175/300][200/283], lr: 0.00000041 	 loss = 0.2392(0.4373)
2023/11/23 05:16:11 - INFO - root -   Epoch: [175/300][220/283], lr: 0.00000041 	 loss = 0.1937(0.4223)
2023/11/23 05:16:35 - INFO - root -   Epoch: [175/300][240/283], lr: 0.00000041 	 loss = 0.2177(0.4335)
2023/11/23 05:16:50 - INFO - root -   Epoch: [175/300][260/283], lr: 0.00000041 	 loss = 1.7358(0.4391)
2023/11/23 05:16:58 - INFO - root -   Epoch: [175/300][280/283], lr: 0.00000041 	 loss = 2.7730(0.4263)
2023/11/23 05:16:59 - INFO - root -   Epoch: [175/300] 	 loss = 0.4234
2023/11/23 05:16:59 - INFO - root -   train_accuracy = 0.8604
2023/11/23 05:17:01 - INFO - root -   Epoch: [176/300][0/283], lr: 0.00000041 	 loss = 0.0402(0.0402)
2023/11/23 05:17:24 - INFO - root -   Epoch: [176/300][20/283], lr: 0.00000041 	 loss = 2.0835(0.5603)
2023/11/23 05:17:42 - INFO - root -   Epoch: [176/300][40/283], lr: 0.00000041 	 loss = 0.1535(0.6429)
2023/11/23 05:17:54 - INFO - root -   Epoch: [176/300][60/283], lr: 0.00000041 	 loss = 0.0070(0.5895)
2023/11/23 05:18:02 - INFO - root -   Epoch: [176/300][80/283], lr: 0.00000041 	 loss = 0.1616(0.5540)
2023/11/23 05:18:20 - INFO - root -   Epoch: [176/300][100/283], lr: 0.00000041 	 loss = 0.3320(0.4933)
2023/11/23 05:18:35 - INFO - root -   Epoch: [176/300][120/283], lr: 0.00000041 	 loss = 0.0756(0.4519)
2023/11/23 05:18:48 - INFO - root -   Epoch: [176/300][140/283], lr: 0.00000041 	 loss = 0.0374(0.4263)
2023/11/23 05:19:12 - INFO - root -   Epoch: [176/300][160/283], lr: 0.00000041 	 loss = 1.7602(0.4371)
2023/11/23 05:19:28 - INFO - root -   Epoch: [176/300][180/283], lr: 0.00000041 	 loss = 0.2186(0.4416)
2023/11/23 05:19:40 - INFO - root -   Epoch: [176/300][200/283], lr: 0.00000041 	 loss = 0.0179(0.4353)
2023/11/23 05:19:53 - INFO - root -   Epoch: [176/300][220/283], lr: 0.00000041 	 loss = 0.0063(0.4159)
2023/11/23 05:20:07 - INFO - root -   Epoch: [176/300][240/283], lr: 0.00000041 	 loss = 0.4083(0.4147)
2023/11/23 05:20:26 - INFO - root -   Epoch: [176/300][260/283], lr: 0.00000041 	 loss = 1.1050(0.4011)
2023/11/23 05:20:38 - INFO - root -   Epoch: [176/300][280/283], lr: 0.00000041 	 loss = 1.9206(0.3904)
2023/11/23 05:20:38 - INFO - root -   Epoch: [176/300] 	 loss = 0.3884
2023/11/23 05:20:38 - INFO - root -   train_accuracy = 0.8746
2023/11/23 05:20:45 - INFO - root -   Epoch: [177/300][0/283], lr: 0.00000041 	 loss = 0.1153(0.1153)
2023/11/23 05:20:58 - INFO - root -   Epoch: [177/300][20/283], lr: 0.00000041 	 loss = 0.6174(0.6696)
2023/11/23 05:21:11 - INFO - root -   Epoch: [177/300][40/283], lr: 0.00000041 	 loss = 0.0493(0.5977)
2023/11/23 05:21:23 - INFO - root -   Epoch: [177/300][60/283], lr: 0.00000041 	 loss = 0.1288(0.5645)
2023/11/23 05:21:36 - INFO - root -   Epoch: [177/300][80/283], lr: 0.00000041 	 loss = 0.1652(0.5738)
2023/11/23 05:21:45 - INFO - root -   Epoch: [177/300][100/283], lr: 0.00000041 	 loss = 1.6043(0.5596)
2023/11/23 05:22:01 - INFO - root -   Epoch: [177/300][120/283], lr: 0.00000041 	 loss = 0.4079(0.5144)
2023/11/23 05:22:13 - INFO - root -   Epoch: [177/300][140/283], lr: 0.00000041 	 loss = 0.0033(0.4890)
2023/11/23 05:22:21 - INFO - root -   Epoch: [177/300][160/283], lr: 0.00000041 	 loss = 0.4608(0.4793)
2023/11/23 05:22:33 - INFO - root -   Epoch: [177/300][180/283], lr: 0.00000041 	 loss = 0.0344(0.4690)
2023/11/23 05:22:50 - INFO - root -   Epoch: [177/300][200/283], lr: 0.00000041 	 loss = 0.1656(0.4563)
2023/11/23 05:23:04 - INFO - root -   Epoch: [177/300][220/283], lr: 0.00000041 	 loss = 0.0404(0.4437)
2023/11/23 05:23:19 - INFO - root -   Epoch: [177/300][240/283], lr: 0.00000041 	 loss = 0.8042(0.4467)
2023/11/23 05:23:31 - INFO - root -   Epoch: [177/300][260/283], lr: 0.00000041 	 loss = 0.2949(0.4406)
2023/11/23 05:23:49 - INFO - root -   Epoch: [177/300][280/283], lr: 0.00000041 	 loss = 1.9527(0.4375)
2023/11/23 05:23:50 - INFO - root -   Epoch: [177/300] 	 loss = 0.4345
2023/11/23 05:23:50 - INFO - root -   train_accuracy = 0.8551
2023/11/23 05:23:51 - INFO - root -   Epoch: [178/300][0/283], lr: 0.00000041 	 loss = 0.0227(0.0227)
2023/11/23 05:24:05 - INFO - root -   Epoch: [178/300][20/283], lr: 0.00000041 	 loss = 0.1594(0.4022)
2023/11/23 05:24:21 - INFO - root -   Epoch: [178/300][40/283], lr: 0.00000041 	 loss = 0.3257(0.4854)
2023/11/23 05:24:37 - INFO - root -   Epoch: [178/300][60/283], lr: 0.00000041 	 loss = 0.1087(0.4566)
2023/11/23 05:24:54 - INFO - root -   Epoch: [178/300][80/283], lr: 0.00000041 	 loss = 0.0721(0.4490)
2023/11/23 05:25:11 - INFO - root -   Epoch: [178/300][100/283], lr: 0.00000041 	 loss = 0.0850(0.4159)
2023/11/23 05:25:25 - INFO - root -   Epoch: [178/300][120/283], lr: 0.00000041 	 loss = 0.0129(0.3816)
2023/11/23 05:25:42 - INFO - root -   Epoch: [178/300][140/283], lr: 0.00000041 	 loss = 0.0036(0.3649)
2023/11/23 05:25:52 - INFO - root -   Epoch: [178/300][160/283], lr: 0.00000041 	 loss = 1.4629(0.3756)
2023/11/23 05:26:05 - INFO - root -   Epoch: [178/300][180/283], lr: 0.00000041 	 loss = 0.0236(0.3794)
2023/11/23 05:26:17 - INFO - root -   Epoch: [178/300][200/283], lr: 0.00000041 	 loss = 0.0837(0.3784)
2023/11/23 05:26:32 - INFO - root -   Epoch: [178/300][220/283], lr: 0.00000041 	 loss = 0.0350(0.3627)
2023/11/23 05:26:52 - INFO - root -   Epoch: [178/300][240/283], lr: 0.00000041 	 loss = 1.8983(0.3832)
2023/11/23 05:27:07 - INFO - root -   Epoch: [178/300][260/283], lr: 0.00000041 	 loss = 0.6101(0.3742)
2023/11/23 05:27:15 - INFO - root -   Epoch: [178/300][280/283], lr: 0.00000041 	 loss = 2.4158(0.3647)
2023/11/23 05:27:16 - INFO - root -   Epoch: [178/300] 	 loss = 0.3622
2023/11/23 05:27:16 - INFO - root -   train_accuracy = 0.8781
2023/11/23 05:27:25 - INFO - root -   Epoch: [179/300][0/283], lr: 0.00000041 	 loss = 0.1259(0.1259)
2023/11/23 05:27:34 - INFO - root -   Epoch: [179/300][20/283], lr: 0.00000041 	 loss = 0.1856(0.4068)
2023/11/23 05:27:50 - INFO - root -   Epoch: [179/300][40/283], lr: 0.00000041 	 loss = 0.0429(0.4634)
2023/11/23 05:28:07 - INFO - root -   Epoch: [179/300][60/283], lr: 0.00000041 	 loss = 0.0197(0.4801)
2023/11/23 05:28:25 - INFO - root -   Epoch: [179/300][80/283], lr: 0.00000041 	 loss = 0.2813(0.4952)
2023/11/23 05:28:38 - INFO - root -   Epoch: [179/300][100/283], lr: 0.00000041 	 loss = 0.0868(0.4538)
2023/11/23 05:28:47 - INFO - root -   Epoch: [179/300][120/283], lr: 0.00000041 	 loss = 0.1178(0.4317)
2023/11/23 05:28:58 - INFO - root -   Epoch: [179/300][140/283], lr: 0.00000041 	 loss = 0.1706(0.4075)
2023/11/23 05:29:20 - INFO - root -   Epoch: [179/300][160/283], lr: 0.00000041 	 loss = 1.3967(0.4014)
2023/11/23 05:29:31 - INFO - root -   Epoch: [179/300][180/283], lr: 0.00000041 	 loss = 0.1170(0.3967)
2023/11/23 05:29:49 - INFO - root -   Epoch: [179/300][200/283], lr: 0.00000041 	 loss = 0.0198(0.4041)
2023/11/23 05:30:05 - INFO - root -   Epoch: [179/300][220/283], lr: 0.00000041 	 loss = 0.0460(0.3998)
2023/11/23 05:30:27 - INFO - root -   Epoch: [179/300][240/283], lr: 0.00000041 	 loss = 1.3737(0.4138)
2023/11/23 05:30:40 - INFO - root -   Epoch: [179/300][260/283], lr: 0.00000041 	 loss = 0.9812(0.4030)
2023/11/23 05:30:51 - INFO - root -   Epoch: [179/300][280/283], lr: 0.00000041 	 loss = 2.9518(0.3973)
2023/11/23 05:30:52 - INFO - root -   Epoch: [179/300] 	 loss = 0.3962
2023/11/23 05:33:46 - INFO - root -   precision = 0.8392
2023/11/23 05:33:46 - INFO - root -   eval_loss = 0.4774
2023/11/23 05:33:46 - INFO - root -   eval_acc = 0.8392
2023/11/23 05:33:47 - INFO - root -   train_accuracy = 0.8587
2023/11/23 05:33:48 - INFO - root -   Epoch: [180/300][0/283], lr: 0.00000041 	 loss = 0.0244(0.0244)
2023/11/23 05:34:10 - INFO - root -   Epoch: [180/300][20/283], lr: 0.00000041 	 loss = 1.0764(0.4515)
2023/11/23 05:34:27 - INFO - root -   Epoch: [180/300][40/283], lr: 0.00000041 	 loss = 0.0280(0.4956)
2023/11/23 05:34:41 - INFO - root -   Epoch: [180/300][60/283], lr: 0.00000041 	 loss = 0.2900(0.5210)
2023/11/23 05:34:58 - INFO - root -   Epoch: [180/300][80/283], lr: 0.00000041 	 loss = 0.1032(0.5247)
2023/11/23 05:35:06 - INFO - root -   Epoch: [180/300][100/283], lr: 0.00000041 	 loss = 0.0498(0.4759)
2023/11/23 05:35:17 - INFO - root -   Epoch: [180/300][120/283], lr: 0.00000041 	 loss = 0.0802(0.4474)
2023/11/23 05:35:34 - INFO - root -   Epoch: [180/300][140/283], lr: 0.00000041 	 loss = 0.0064(0.4156)
2023/11/23 05:35:45 - INFO - root -   Epoch: [180/300][160/283], lr: 0.00000041 	 loss = 0.4193(0.4014)
2023/11/23 05:36:04 - INFO - root -   Epoch: [180/300][180/283], lr: 0.00000041 	 loss = 0.0207(0.4192)
2023/11/23 05:36:19 - INFO - root -   Epoch: [180/300][200/283], lr: 0.00000041 	 loss = 0.2750(0.4115)
2023/11/23 05:36:40 - INFO - root -   Epoch: [180/300][220/283], lr: 0.00000041 	 loss = 0.1107(0.3959)
2023/11/23 05:36:50 - INFO - root -   Epoch: [180/300][240/283], lr: 0.00000041 	 loss = 2.6912(0.4010)
2023/11/23 05:37:14 - INFO - root -   Epoch: [180/300][260/283], lr: 0.00000041 	 loss = 1.8906(0.3937)
2023/11/23 05:37:28 - INFO - root -   Epoch: [180/300][280/283], lr: 0.00000041 	 loss = 1.8290(0.3912)
2023/11/23 05:37:29 - INFO - root -   Epoch: [180/300] 	 loss = 0.3888
2023/11/23 05:37:29 - INFO - root -   train_accuracy = 0.8587
2023/11/23 05:37:30 - INFO - root -   Epoch: [181/300][0/283], lr: 0.00000042 	 loss = 0.1062(0.1062)
2023/11/23 05:37:56 - INFO - root -   Epoch: [181/300][20/283], lr: 0.00000042 	 loss = 0.2275(0.6480)
2023/11/23 05:38:11 - INFO - root -   Epoch: [181/300][40/283], lr: 0.00000042 	 loss = 0.1302(0.5919)
2023/11/23 05:38:29 - INFO - root -   Epoch: [181/300][60/283], lr: 0.00000042 	 loss = 0.0118(0.5935)
2023/11/23 05:38:43 - INFO - root -   Epoch: [181/300][80/283], lr: 0.00000042 	 loss = 0.0838(0.5842)
2023/11/23 05:39:03 - INFO - root -   Epoch: [181/300][100/283], lr: 0.00000042 	 loss = 0.4424(0.5531)
2023/11/23 05:39:17 - INFO - root -   Epoch: [181/300][120/283], lr: 0.00000042 	 loss = 0.2557(0.5132)
2023/11/23 05:39:27 - INFO - root -   Epoch: [181/300][140/283], lr: 0.00000042 	 loss = 0.0178(0.4580)
2023/11/23 05:39:46 - INFO - root -   Epoch: [181/300][160/283], lr: 0.00000042 	 loss = 1.7450(0.4517)
2023/11/23 05:40:08 - INFO - root -   Epoch: [181/300][180/283], lr: 0.00000042 	 loss = 0.1088(0.4465)
2023/11/23 05:40:18 - INFO - root -   Epoch: [181/300][200/283], lr: 0.00000042 	 loss = 0.0979(0.4581)
2023/11/23 05:40:38 - INFO - root -   Epoch: [181/300][220/283], lr: 0.00000042 	 loss = 0.0539(0.4543)
2023/11/23 05:40:55 - INFO - root -   Epoch: [181/300][240/283], lr: 0.00000042 	 loss = 0.2740(0.4598)
2023/11/23 05:41:16 - INFO - root -   Epoch: [181/300][260/283], lr: 0.00000042 	 loss = 0.5450(0.4442)
2023/11/23 05:41:29 - INFO - root -   Epoch: [181/300][280/283], lr: 0.00000042 	 loss = 2.8799(0.4342)
2023/11/23 05:41:29 - INFO - root -   Epoch: [181/300] 	 loss = 0.4313
2023/11/23 05:41:29 - INFO - root -   train_accuracy = 0.8428
2023/11/23 05:41:45 - INFO - root -   Epoch: [182/300][0/283], lr: 0.00000042 	 loss = 0.0852(0.0852)
2023/11/23 05:41:57 - INFO - root -   Epoch: [182/300][20/283], lr: 0.00000042 	 loss = 0.5776(0.6041)
2023/11/23 05:42:09 - INFO - root -   Epoch: [182/300][40/283], lr: 0.00000042 	 loss = 0.2338(0.5816)
2023/11/23 05:42:23 - INFO - root -   Epoch: [182/300][60/283], lr: 0.00000042 	 loss = 0.0227(0.5551)
2023/11/23 05:42:35 - INFO - root -   Epoch: [182/300][80/283], lr: 0.00000042 	 loss = 0.2635(0.5637)
2023/11/23 05:42:50 - INFO - root -   Epoch: [182/300][100/283], lr: 0.00000042 	 loss = 0.0755(0.5194)
2023/11/23 05:43:03 - INFO - root -   Epoch: [182/300][120/283], lr: 0.00000042 	 loss = 0.3850(0.4853)
2023/11/23 05:43:24 - INFO - root -   Epoch: [182/300][140/283], lr: 0.00000042 	 loss = 0.0085(0.4460)
2023/11/23 05:43:37 - INFO - root -   Epoch: [182/300][160/283], lr: 0.00000042 	 loss = 0.5549(0.4350)
2023/11/23 05:43:51 - INFO - root -   Epoch: [182/300][180/283], lr: 0.00000042 	 loss = 0.1830(0.4453)
2023/11/23 05:44:08 - INFO - root -   Epoch: [182/300][200/283], lr: 0.00000042 	 loss = 0.1072(0.4447)
2023/11/23 05:44:18 - INFO - root -   Epoch: [182/300][220/283], lr: 0.00000042 	 loss = 0.0575(0.4369)
2023/11/23 05:44:35 - INFO - root -   Epoch: [182/300][240/283], lr: 0.00000042 	 loss = 1.0888(0.4397)
2023/11/23 05:44:47 - INFO - root -   Epoch: [182/300][260/283], lr: 0.00000042 	 loss = 0.2425(0.4208)
2023/11/23 05:45:01 - INFO - root -   Epoch: [182/300][280/283], lr: 0.00000042 	 loss = 2.1542(0.4185)
2023/11/23 05:45:02 - INFO - root -   Epoch: [182/300] 	 loss = 0.4161
2023/11/23 05:45:02 - INFO - root -   train_accuracy = 0.8498
2023/11/23 05:45:04 - INFO - root -   Epoch: [183/300][0/283], lr: 0.00000042 	 loss = 0.0269(0.0269)
2023/11/23 05:45:21 - INFO - root -   Epoch: [183/300][20/283], lr: 0.00000042 	 loss = 0.1177(0.6146)
2023/11/23 05:45:36 - INFO - root -   Epoch: [183/300][40/283], lr: 0.00000042 	 loss = 0.0387(0.6401)
2023/11/23 05:45:56 - INFO - root -   Epoch: [183/300][60/283], lr: 0.00000042 	 loss = 0.2229(0.6250)
2023/11/23 05:46:05 - INFO - root -   Epoch: [183/300][80/283], lr: 0.00000042 	 loss = 0.3089(0.5879)
2023/11/23 05:46:16 - INFO - root -   Epoch: [183/300][100/283], lr: 0.00000042 	 loss = 0.3129(0.5431)
2023/11/23 05:46:39 - INFO - root -   Epoch: [183/300][120/283], lr: 0.00000042 	 loss = 0.1375(0.5325)
2023/11/23 05:46:52 - INFO - root -   Epoch: [183/300][140/283], lr: 0.00000042 	 loss = 0.0229(0.5089)
2023/11/23 05:47:09 - INFO - root -   Epoch: [183/300][160/283], lr: 0.00000042 	 loss = 0.4362(0.4871)
2023/11/23 05:47:28 - INFO - root -   Epoch: [183/300][180/283], lr: 0.00000042 	 loss = 0.2382(0.4872)
2023/11/23 05:47:36 - INFO - root -   Epoch: [183/300][200/283], lr: 0.00000042 	 loss = 0.0261(0.4683)
2023/11/23 05:48:00 - INFO - root -   Epoch: [183/300][220/283], lr: 0.00000042 	 loss = 0.2829(0.4547)
2023/11/23 05:48:17 - INFO - root -   Epoch: [183/300][240/283], lr: 0.00000042 	 loss = 0.0980(0.4476)
2023/11/23 05:48:30 - INFO - root -   Epoch: [183/300][260/283], lr: 0.00000042 	 loss = 0.5338(0.4358)
2023/11/23 05:48:48 - INFO - root -   Epoch: [183/300][280/283], lr: 0.00000042 	 loss = 2.7768(0.4339)
2023/11/23 05:48:49 - INFO - root -   Epoch: [183/300] 	 loss = 0.4322
2023/11/23 05:48:49 - INFO - root -   train_accuracy = 0.8392
2023/11/23 05:48:51 - INFO - root -   Epoch: [184/300][0/283], lr: 0.00000042 	 loss = 0.0615(0.0615)
2023/11/23 05:49:10 - INFO - root -   Epoch: [184/300][20/283], lr: 0.00000042 	 loss = 0.4184(0.6184)
2023/11/23 05:49:26 - INFO - root -   Epoch: [184/300][40/283], lr: 0.00000042 	 loss = 0.1509(0.6207)
2023/11/23 05:49:42 - INFO - root -   Epoch: [184/300][60/283], lr: 0.00000042 	 loss = 0.1185(0.5713)
2023/11/23 05:50:02 - INFO - root -   Epoch: [184/300][80/283], lr: 0.00000042 	 loss = 0.0741(0.5611)
2023/11/23 05:50:20 - INFO - root -   Epoch: [184/300][100/283], lr: 0.00000042 	 loss = 0.0501(0.5304)
2023/11/23 05:50:40 - INFO - root -   Epoch: [184/300][120/283], lr: 0.00000042 	 loss = 0.0245(0.4845)
2023/11/23 05:50:48 - INFO - root -   Epoch: [184/300][140/283], lr: 0.00000042 	 loss = 0.0119(0.4483)
2023/11/23 05:50:58 - INFO - root -   Epoch: [184/300][160/283], lr: 0.00000042 	 loss = 0.3929(0.4266)
2023/11/23 05:51:12 - INFO - root -   Epoch: [184/300][180/283], lr: 0.00000042 	 loss = 0.0761(0.4312)
2023/11/23 05:51:37 - INFO - root -   Epoch: [184/300][200/283], lr: 0.00000042 	 loss = 0.5214(0.4439)
2023/11/23 05:51:51 - INFO - root -   Epoch: [184/300][220/283], lr: 0.00000042 	 loss = 0.1089(0.4293)
2023/11/23 05:52:07 - INFO - root -   Epoch: [184/300][240/283], lr: 0.00000042 	 loss = 0.4774(0.4280)
2023/11/23 05:52:19 - INFO - root -   Epoch: [184/300][260/283], lr: 0.00000042 	 loss = 1.9317(0.4298)
2023/11/23 05:52:29 - INFO - root -   Epoch: [184/300][280/283], lr: 0.00000042 	 loss = 1.9841(0.4157)
2023/11/23 05:52:30 - INFO - root -   Epoch: [184/300] 	 loss = 0.4133
2023/11/23 05:55:23 - INFO - root -   precision = 0.8182
2023/11/23 05:55:23 - INFO - root -   eval_loss = 0.4796
2023/11/23 05:55:23 - INFO - root -   eval_acc = 0.8182
2023/11/23 05:55:25 - INFO - root -   train_accuracy = 0.8410
2023/11/23 05:55:33 - INFO - root -   Epoch: [185/300][0/283], lr: 0.00000042 	 loss = 0.1464(0.1464)
2023/11/23 05:55:53 - INFO - root -   Epoch: [185/300][20/283], lr: 0.00000042 	 loss = 0.1848(0.5062)
2023/11/23 05:56:06 - INFO - root -   Epoch: [185/300][40/283], lr: 0.00000042 	 loss = 0.1706(0.6099)
2023/11/23 05:56:24 - INFO - root -   Epoch: [185/300][60/283], lr: 0.00000042 	 loss = 0.0061(0.5819)
2023/11/23 05:56:38 - INFO - root -   Epoch: [185/300][80/283], lr: 0.00000042 	 loss = 0.1221(0.5776)
2023/11/23 05:56:52 - INFO - root -   Epoch: [185/300][100/283], lr: 0.00000042 	 loss = 0.2134(0.5190)
2023/11/23 05:57:02 - INFO - root -   Epoch: [185/300][120/283], lr: 0.00000042 	 loss = 0.0395(0.4613)
2023/11/23 05:57:10 - INFO - root -   Epoch: [185/300][140/283], lr: 0.00000042 	 loss = 0.0124(0.4247)
2023/11/23 05:57:24 - INFO - root -   Epoch: [185/300][160/283], lr: 0.00000042 	 loss = 0.2673(0.4155)
2023/11/23 05:57:41 - INFO - root -   Epoch: [185/300][180/283], lr: 0.00000042 	 loss = 0.0935(0.4246)
2023/11/23 05:57:50 - INFO - root -   Epoch: [185/300][200/283], lr: 0.00000042 	 loss = 0.0297(0.4194)
2023/11/23 05:57:58 - INFO - root -   Epoch: [185/300][220/283], lr: 0.00000042 	 loss = 0.0729(0.4054)
2023/11/23 05:58:21 - INFO - root -   Epoch: [185/300][240/283], lr: 0.00000042 	 loss = 1.4329(0.4230)
2023/11/23 05:58:42 - INFO - root -   Epoch: [185/300][260/283], lr: 0.00000042 	 loss = 1.1554(0.4242)
2023/11/23 05:58:57 - INFO - root -   Epoch: [185/300][280/283], lr: 0.00000042 	 loss = 1.9519(0.4216)
2023/11/23 05:58:58 - INFO - root -   Epoch: [185/300] 	 loss = 0.4192
2023/11/23 05:58:58 - INFO - root -   train_accuracy = 0.8551
2023/11/23 05:58:59 - INFO - root -   Epoch: [186/300][0/283], lr: 0.00000043 	 loss = 0.0664(0.0664)
2023/11/23 05:59:23 - INFO - root -   Epoch: [186/300][20/283], lr: 0.00000043 	 loss = 0.7787(0.5317)
2023/11/23 05:59:38 - INFO - root -   Epoch: [186/300][40/283], lr: 0.00000043 	 loss = 0.6103(0.5991)
2023/11/23 05:59:51 - INFO - root -   Epoch: [186/300][60/283], lr: 0.00000043 	 loss = 0.4035(0.5156)
2023/11/23 06:00:07 - INFO - root -   Epoch: [186/300][80/283], lr: 0.00000043 	 loss = 0.0275(0.5228)
2023/11/23 06:00:32 - INFO - root -   Epoch: [186/300][100/283], lr: 0.00000043 	 loss = 0.1695(0.4867)
2023/11/23 06:00:51 - INFO - root -   Epoch: [186/300][120/283], lr: 0.00000043 	 loss = 0.3737(0.4759)
2023/11/23 06:01:14 - INFO - root -   Epoch: [186/300][140/283], lr: 0.00000043 	 loss = 0.0070(0.4641)
2023/11/23 06:01:28 - INFO - root -   Epoch: [186/300][160/283], lr: 0.00000043 	 loss = 1.4223(0.4598)
2023/11/23 06:01:36 - INFO - root -   Epoch: [186/300][180/283], lr: 0.00000043 	 loss = 0.0603(0.4481)
2023/11/23 06:01:51 - INFO - root -   Epoch: [186/300][200/283], lr: 0.00000043 	 loss = 0.0239(0.4324)
2023/11/23 06:02:06 - INFO - root -   Epoch: [186/300][220/283], lr: 0.00000043 	 loss = 0.2480(0.4229)
2023/11/23 06:02:17 - INFO - root -   Epoch: [186/300][240/283], lr: 0.00000043 	 loss = 0.4399(0.4237)
2023/11/23 06:02:27 - INFO - root -   Epoch: [186/300][260/283], lr: 0.00000043 	 loss = 0.5653(0.4114)
2023/11/23 06:02:47 - INFO - root -   Epoch: [186/300][280/283], lr: 0.00000043 	 loss = 1.4530(0.4056)
2023/11/23 06:02:48 - INFO - root -   Epoch: [186/300] 	 loss = 0.4030
2023/11/23 06:02:48 - INFO - root -   train_accuracy = 0.8445
2023/11/23 06:02:56 - INFO - root -   Epoch: [187/300][0/283], lr: 0.00000043 	 loss = 0.1593(0.1593)
2023/11/23 06:03:12 - INFO - root -   Epoch: [187/300][20/283], lr: 0.00000043 	 loss = 0.2116(0.5083)
2023/11/23 06:03:29 - INFO - root -   Epoch: [187/300][40/283], lr: 0.00000043 	 loss = 0.3006(0.5681)
2023/11/23 06:03:45 - INFO - root -   Epoch: [187/300][60/283], lr: 0.00000043 	 loss = 0.0146(0.5153)
2023/11/23 06:03:54 - INFO - root -   Epoch: [187/300][80/283], lr: 0.00000043 	 loss = 0.1517(0.5093)
2023/11/23 06:04:11 - INFO - root -   Epoch: [187/300][100/283], lr: 0.00000043 	 loss = 0.3134(0.4587)
2023/11/23 06:04:34 - INFO - root -   Epoch: [187/300][120/283], lr: 0.00000043 	 loss = 0.0438(0.4322)
2023/11/23 06:04:47 - INFO - root -   Epoch: [187/300][140/283], lr: 0.00000043 	 loss = 0.0515(0.4109)
2023/11/23 06:04:57 - INFO - root -   Epoch: [187/300][160/283], lr: 0.00000043 	 loss = 1.5991(0.4014)
2023/11/23 06:05:07 - INFO - root -   Epoch: [187/300][180/283], lr: 0.00000043 	 loss = 0.1198(0.3984)
2023/11/23 06:05:24 - INFO - root -   Epoch: [187/300][200/283], lr: 0.00000043 	 loss = 0.1063(0.3925)
2023/11/23 06:05:39 - INFO - root -   Epoch: [187/300][220/283], lr: 0.00000043 	 loss = 0.1357(0.3795)
2023/11/23 06:05:50 - INFO - root -   Epoch: [187/300][240/283], lr: 0.00000043 	 loss = 0.0657(0.3698)
2023/11/23 06:05:58 - INFO - root -   Epoch: [187/300][260/283], lr: 0.00000043 	 loss = 2.5988(0.3694)
2023/11/23 06:06:22 - INFO - root -   Epoch: [187/300][280/283], lr: 0.00000043 	 loss = 2.2349(0.3761)
2023/11/23 06:06:22 - INFO - root -   Epoch: [187/300] 	 loss = 0.3751
2023/11/23 06:06:22 - INFO - root -   train_accuracy = 0.8693
2023/11/23 06:06:23 - INFO - root -   Epoch: [188/300][0/283], lr: 0.00000043 	 loss = 0.0956(0.0956)
2023/11/23 06:06:47 - INFO - root -   Epoch: [188/300][20/283], lr: 0.00000043 	 loss = 1.1295(0.5156)
2023/11/23 06:06:59 - INFO - root -   Epoch: [188/300][40/283], lr: 0.00000043 	 loss = 0.3013(0.5397)
2023/11/23 06:07:11 - INFO - root -   Epoch: [188/300][60/283], lr: 0.00000043 	 loss = 0.0116(0.4709)
2023/11/23 06:07:22 - INFO - root -   Epoch: [188/300][80/283], lr: 0.00000043 	 loss = 0.0471(0.4627)
2023/11/23 06:07:48 - INFO - root -   Epoch: [188/300][100/283], lr: 0.00000043 	 loss = 0.2377(0.4573)
2023/11/23 06:07:59 - INFO - root -   Epoch: [188/300][120/283], lr: 0.00000043 	 loss = 0.3748(0.4385)
2023/11/23 06:08:14 - INFO - root -   Epoch: [188/300][140/283], lr: 0.00000043 	 loss = 0.0296(0.4071)
2023/11/23 06:08:29 - INFO - root -   Epoch: [188/300][160/283], lr: 0.00000043 	 loss = 0.3180(0.4236)
2023/11/23 06:08:39 - INFO - root -   Epoch: [188/300][180/283], lr: 0.00000043 	 loss = 0.3234(0.4238)
2023/11/23 06:08:49 - INFO - root -   Epoch: [188/300][200/283], lr: 0.00000043 	 loss = 0.0253(0.4267)
2023/11/23 06:09:03 - INFO - root -   Epoch: [188/300][220/283], lr: 0.00000043 	 loss = 0.3746(0.4146)
2023/11/23 06:09:24 - INFO - root -   Epoch: [188/300][240/283], lr: 0.00000043 	 loss = 0.1025(0.4136)
2023/11/23 06:09:38 - INFO - root -   Epoch: [188/300][260/283], lr: 0.00000043 	 loss = 0.6077(0.4108)
2023/11/23 06:09:52 - INFO - root -   Epoch: [188/300][280/283], lr: 0.00000043 	 loss = 2.2115(0.4047)
2023/11/23 06:09:53 - INFO - root -   Epoch: [188/300] 	 loss = 0.4029
2023/11/23 06:09:53 - INFO - root -   train_accuracy = 0.8657
2023/11/23 06:09:54 - INFO - root -   Epoch: [189/300][0/283], lr: 0.00000043 	 loss = 0.0183(0.0183)
2023/11/23 06:10:14 - INFO - root -   Epoch: [189/300][20/283], lr: 0.00000043 	 loss = 0.8290(0.3946)
2023/11/23 06:10:33 - INFO - root -   Epoch: [189/300][40/283], lr: 0.00000043 	 loss = 0.7415(0.6386)
2023/11/23 06:10:48 - INFO - root -   Epoch: [189/300][60/283], lr: 0.00000043 	 loss = 0.0777(0.5812)
2023/11/23 06:10:59 - INFO - root -   Epoch: [189/300][80/283], lr: 0.00000043 	 loss = 0.0786(0.5730)
2023/11/23 06:11:14 - INFO - root -   Epoch: [189/300][100/283], lr: 0.00000043 	 loss = 0.0691(0.5323)
2023/11/23 06:11:32 - INFO - root -   Epoch: [189/300][120/283], lr: 0.00000043 	 loss = 0.1569(0.4843)
2023/11/23 06:11:43 - INFO - root -   Epoch: [189/300][140/283], lr: 0.00000043 	 loss = 0.0056(0.4371)
2023/11/23 06:11:59 - INFO - root -   Epoch: [189/300][160/283], lr: 0.00000043 	 loss = 0.7549(0.4333)
2023/11/23 06:12:17 - INFO - root -   Epoch: [189/300][180/283], lr: 0.00000043 	 loss = 0.1842(0.4222)
2023/11/23 06:12:38 - INFO - root -   Epoch: [189/300][200/283], lr: 0.00000043 	 loss = 0.0168(0.4266)
2023/11/23 06:12:52 - INFO - root -   Epoch: [189/300][220/283], lr: 0.00000043 	 loss = 0.0172(0.4213)
2023/11/23 06:13:07 - INFO - root -   Epoch: [189/300][240/283], lr: 0.00000043 	 loss = 0.3503(0.4193)
2023/11/23 06:13:20 - INFO - root -   Epoch: [189/300][260/283], lr: 0.00000043 	 loss = 1.1744(0.4231)
2023/11/23 06:13:36 - INFO - root -   Epoch: [189/300][280/283], lr: 0.00000043 	 loss = 1.9291(0.4180)
2023/11/23 06:13:37 - INFO - root -   Epoch: [189/300] 	 loss = 0.4152
2023/11/23 06:16:32 - INFO - root -   precision = 0.8112
2023/11/23 06:16:32 - INFO - root -   eval_loss = 0.4886
2023/11/23 06:16:32 - INFO - root -   eval_acc = 0.8112
2023/11/23 06:16:33 - INFO - root -   train_accuracy = 0.8640
2023/11/23 06:16:41 - INFO - root -   Epoch: [190/300][0/283], lr: 0.00000043 	 loss = 0.0330(0.0330)
2023/11/23 06:16:56 - INFO - root -   Epoch: [190/300][20/283], lr: 0.00000043 	 loss = 0.1609(0.4802)
2023/11/23 06:17:08 - INFO - root -   Epoch: [190/300][40/283], lr: 0.00000043 	 loss = 0.0109(0.4759)
2023/11/23 06:17:27 - INFO - root -   Epoch: [190/300][60/283], lr: 0.00000043 	 loss = 0.0098(0.4560)
2023/11/23 06:17:40 - INFO - root -   Epoch: [190/300][80/283], lr: 0.00000043 	 loss = 0.2473(0.4774)
2023/11/23 06:17:59 - INFO - root -   Epoch: [190/300][100/283], lr: 0.00000043 	 loss = 0.1934(0.4850)
2023/11/23 06:18:16 - INFO - root -   Epoch: [190/300][120/283], lr: 0.00000043 	 loss = 0.2462(0.4805)
2023/11/23 06:18:34 - INFO - root -   Epoch: [190/300][140/283], lr: 0.00000043 	 loss = 0.0203(0.4381)
2023/11/23 06:18:52 - INFO - root -   Epoch: [190/300][160/283], lr: 0.00000043 	 loss = 0.1745(0.4151)
2023/11/23 06:19:04 - INFO - root -   Epoch: [190/300][180/283], lr: 0.00000043 	 loss = 0.1830(0.4193)
2023/11/23 06:19:17 - INFO - root -   Epoch: [190/300][200/283], lr: 0.00000043 	 loss = 0.0579(0.4156)
2023/11/23 06:19:35 - INFO - root -   Epoch: [190/300][220/283], lr: 0.00000043 	 loss = 0.1891(0.4080)
2023/11/23 06:19:45 - INFO - root -   Epoch: [190/300][240/283], lr: 0.00000043 	 loss = 0.3207(0.4056)
2023/11/23 06:20:00 - INFO - root -   Epoch: [190/300][260/283], lr: 0.00000043 	 loss = 2.0167(0.4015)
2023/11/23 06:20:16 - INFO - root -   Epoch: [190/300][280/283], lr: 0.00000043 	 loss = 2.7555(0.3928)
2023/11/23 06:20:17 - INFO - root -   Epoch: [190/300] 	 loss = 0.3923
2023/11/23 06:20:17 - INFO - root -   train_accuracy = 0.8551
2023/11/23 06:20:26 - INFO - root -   Epoch: [191/300][0/283], lr: 0.00000043 	 loss = 0.1938(0.1938)
2023/11/23 06:20:42 - INFO - root -   Epoch: [191/300][20/283], lr: 0.00000043 	 loss = 1.7198(0.5440)
2023/11/23 06:20:53 - INFO - root -   Epoch: [191/300][40/283], lr: 0.00000043 	 loss = 0.2465(0.4999)
2023/11/23 06:21:09 - INFO - root -   Epoch: [191/300][60/283], lr: 0.00000043 	 loss = 0.0919(0.5306)
2023/11/23 06:21:25 - INFO - root -   Epoch: [191/300][80/283], lr: 0.00000043 	 loss = 0.3981(0.5485)
2023/11/23 06:21:41 - INFO - root -   Epoch: [191/300][100/283], lr: 0.00000043 	 loss = 0.0633(0.4907)
2023/11/23 06:21:57 - INFO - root -   Epoch: [191/300][120/283], lr: 0.00000043 	 loss = 0.0171(0.4435)
2023/11/23 06:22:13 - INFO - root -   Epoch: [191/300][140/283], lr: 0.00000043 	 loss = 0.0120(0.4073)
2023/11/23 06:22:38 - INFO - root -   Epoch: [191/300][160/283], lr: 0.00000043 	 loss = 2.2481(0.4103)
2023/11/23 06:22:49 - INFO - root -   Epoch: [191/300][180/283], lr: 0.00000043 	 loss = 0.0607(0.4122)
2023/11/23 06:23:09 - INFO - root -   Epoch: [191/300][200/283], lr: 0.00000043 	 loss = 0.0525(0.4161)
2023/11/23 06:23:21 - INFO - root -   Epoch: [191/300][220/283], lr: 0.00000043 	 loss = 0.1745(0.3955)
2023/11/23 06:23:34 - INFO - root -   Epoch: [191/300][240/283], lr: 0.00000043 	 loss = 0.1273(0.3793)
2023/11/23 06:23:49 - INFO - root -   Epoch: [191/300][260/283], lr: 0.00000043 	 loss = 1.4938(0.3756)
2023/11/23 06:24:03 - INFO - root -   Epoch: [191/300][280/283], lr: 0.00000043 	 loss = 1.3357(0.3732)
2023/11/23 06:24:04 - INFO - root -   Epoch: [191/300] 	 loss = 0.3713
2023/11/23 06:24:04 - INFO - root -   train_accuracy = 0.8746
2023/11/23 06:24:05 - INFO - root -   Epoch: [192/300][0/283], lr: 0.00000044 	 loss = 0.0539(0.0539)
2023/11/23 06:24:22 - INFO - root -   Epoch: [192/300][20/283], lr: 0.00000044 	 loss = 0.1513(0.3877)
2023/11/23 06:24:40 - INFO - root -   Epoch: [192/300][40/283], lr: 0.00000044 	 loss = 0.0233(0.5334)
2023/11/23 06:24:51 - INFO - root -   Epoch: [192/300][60/283], lr: 0.00000044 	 loss = 0.0077(0.5068)
2023/11/23 06:25:05 - INFO - root -   Epoch: [192/300][80/283], lr: 0.00000044 	 loss = 0.2976(0.5374)
2023/11/23 06:25:23 - INFO - root -   Epoch: [192/300][100/283], lr: 0.00000044 	 loss = 0.0160(0.5214)
2023/11/23 06:25:44 - INFO - root -   Epoch: [192/300][120/283], lr: 0.00000044 	 loss = 0.2209(0.4793)
2023/11/23 06:25:53 - INFO - root -   Epoch: [192/300][140/283], lr: 0.00000044 	 loss = 0.0154(0.4490)
2023/11/23 06:26:12 - INFO - root -   Epoch: [192/300][160/283], lr: 0.00000044 	 loss = 0.4571(0.4240)
2023/11/23 06:26:26 - INFO - root -   Epoch: [192/300][180/283], lr: 0.00000044 	 loss = 0.2002(0.4331)
2023/11/23 06:26:40 - INFO - root -   Epoch: [192/300][200/283], lr: 0.00000044 	 loss = 0.1357(0.4242)
2023/11/23 06:26:59 - INFO - root -   Epoch: [192/300][220/283], lr: 0.00000044 	 loss = 0.1522(0.4115)
2023/11/23 06:27:23 - INFO - root -   Epoch: [192/300][240/283], lr: 0.00000044 	 loss = 0.1324(0.4188)
2023/11/23 06:27:40 - INFO - root -   Epoch: [192/300][260/283], lr: 0.00000044 	 loss = 2.0665(0.4130)
2023/11/23 06:27:55 - INFO - root -   Epoch: [192/300][280/283], lr: 0.00000044 	 loss = 2.3502(0.4036)
2023/11/23 06:27:56 - INFO - root -   Epoch: [192/300] 	 loss = 0.4019
2023/11/23 06:27:56 - INFO - root -   train_accuracy = 0.8481
2023/11/23 06:28:11 - INFO - root -   Epoch: [193/300][0/283], lr: 0.00000044 	 loss = 0.1884(0.1884)
2023/11/23 06:28:26 - INFO - root -   Epoch: [193/300][20/283], lr: 0.00000044 	 loss = 0.1409(0.4690)
2023/11/23 06:28:41 - INFO - root -   Epoch: [193/300][40/283], lr: 0.00000044 	 loss = 0.0322(0.4970)
2023/11/23 06:28:53 - INFO - root -   Epoch: [193/300][60/283], lr: 0.00000044 	 loss = 0.0047(0.4527)
2023/11/23 06:29:12 - INFO - root -   Epoch: [193/300][80/283], lr: 0.00000044 	 loss = 0.5229(0.4689)
2023/11/23 06:29:21 - INFO - root -   Epoch: [193/300][100/283], lr: 0.00000044 	 loss = 0.0522(0.4528)
2023/11/23 06:29:36 - INFO - root -   Epoch: [193/300][120/283], lr: 0.00000044 	 loss = 0.0312(0.4247)
2023/11/23 06:29:53 - INFO - root -   Epoch: [193/300][140/283], lr: 0.00000044 	 loss = 0.3663(0.4049)
2023/11/23 06:30:07 - INFO - root -   Epoch: [193/300][160/283], lr: 0.00000044 	 loss = 1.6391(0.4022)
2023/11/23 06:30:21 - INFO - root -   Epoch: [193/300][180/283], lr: 0.00000044 	 loss = 0.1137(0.4034)
2023/11/23 06:30:35 - INFO - root -   Epoch: [193/300][200/283], lr: 0.00000044 	 loss = 0.0064(0.4010)
2023/11/23 06:30:51 - INFO - root -   Epoch: [193/300][220/283], lr: 0.00000044 	 loss = 0.4741(0.4013)
2023/11/23 06:31:05 - INFO - root -   Epoch: [193/300][240/283], lr: 0.00000044 	 loss = 1.0049(0.4124)
2023/11/23 06:31:35 - INFO - root -   Epoch: [193/300][260/283], lr: 0.00000044 	 loss = 1.6159(0.4079)
2023/11/23 06:31:43 - INFO - root -   Epoch: [193/300][280/283], lr: 0.00000044 	 loss = 2.8204(0.4065)
2023/11/23 06:31:44 - INFO - root -   Epoch: [193/300] 	 loss = 0.4051
2023/11/23 06:31:44 - INFO - root -   train_accuracy = 0.8498
2023/11/23 06:31:53 - INFO - root -   Epoch: [194/300][0/283], lr: 0.00000044 	 loss = 0.0094(0.0094)
2023/11/23 06:32:07 - INFO - root -   Epoch: [194/300][20/283], lr: 0.00000044 	 loss = 2.2026(0.6304)
2023/11/23 06:32:21 - INFO - root -   Epoch: [194/300][40/283], lr: 0.00000044 	 loss = 0.1702(0.5505)
2023/11/23 06:32:29 - INFO - root -   Epoch: [194/300][60/283], lr: 0.00000044 	 loss = 0.0076(0.4745)
2023/11/23 06:32:46 - INFO - root -   Epoch: [194/300][80/283], lr: 0.00000044 	 loss = 0.4035(0.4694)
2023/11/23 06:32:54 - INFO - root -   Epoch: [194/300][100/283], lr: 0.00000044 	 loss = 0.0313(0.4366)
2023/11/23 06:33:14 - INFO - root -   Epoch: [194/300][120/283], lr: 0.00000044 	 loss = 0.0898(0.4147)
2023/11/23 06:33:34 - INFO - root -   Epoch: [194/300][140/283], lr: 0.00000044 	 loss = 0.2196(0.3905)
2023/11/23 06:33:50 - INFO - root -   Epoch: [194/300][160/283], lr: 0.00000044 	 loss = 1.0033(0.3856)
2023/11/23 06:34:09 - INFO - root -   Epoch: [194/300][180/283], lr: 0.00000044 	 loss = 0.5134(0.3945)
2023/11/23 06:34:20 - INFO - root -   Epoch: [194/300][200/283], lr: 0.00000044 	 loss = 0.0153(0.3929)
2023/11/23 06:34:45 - INFO - root -   Epoch: [194/300][220/283], lr: 0.00000044 	 loss = 0.1215(0.3927)
2023/11/23 06:35:02 - INFO - root -   Epoch: [194/300][240/283], lr: 0.00000044 	 loss = 0.1531(0.3950)
2023/11/23 06:35:23 - INFO - root -   Epoch: [194/300][260/283], lr: 0.00000044 	 loss = 0.6190(0.3872)
2023/11/23 06:35:31 - INFO - root -   Epoch: [194/300][280/283], lr: 0.00000044 	 loss = 1.8008(0.3791)
2023/11/23 06:35:31 - INFO - root -   Epoch: [194/300] 	 loss = 0.3765
2023/11/23 06:38:25 - INFO - root -   precision = 0.8182
2023/11/23 06:38:25 - INFO - root -   eval_loss = 0.5312
2023/11/23 06:38:25 - INFO - root -   eval_acc = 0.8182
2023/11/23 06:38:27 - INFO - root -   train_accuracy = 0.8587
2023/11/23 06:38:35 - INFO - root -   Epoch: [195/300][0/283], lr: 0.00000044 	 loss = 0.3859(0.3859)
2023/11/23 06:38:58 - INFO - root -   Epoch: [195/300][20/283], lr: 0.00000044 	 loss = 0.8475(0.5752)
2023/11/23 06:39:06 - INFO - root -   Epoch: [195/300][40/283], lr: 0.00000044 	 loss = 0.0131(0.5475)
2023/11/23 06:39:20 - INFO - root -   Epoch: [195/300][60/283], lr: 0.00000044 	 loss = 0.0086(0.4959)
2023/11/23 06:39:39 - INFO - root -   Epoch: [195/300][80/283], lr: 0.00000044 	 loss = 0.3212(0.5313)
2023/11/23 06:40:00 - INFO - root -   Epoch: [195/300][100/283], lr: 0.00000044 	 loss = 0.0431(0.5118)
2023/11/23 06:40:09 - INFO - root -   Epoch: [195/300][120/283], lr: 0.00000044 	 loss = 0.0454(0.4689)
2023/11/23 06:40:20 - INFO - root -   Epoch: [195/300][140/283], lr: 0.00000044 	 loss = 0.0830(0.4414)
2023/11/23 06:40:32 - INFO - root -   Epoch: [195/300][160/283], lr: 0.00000044 	 loss = 1.3633(0.4316)
2023/11/23 06:40:43 - INFO - root -   Epoch: [195/300][180/283], lr: 0.00000044 	 loss = 0.0536(0.4662)
2023/11/23 06:40:56 - INFO - root -   Epoch: [195/300][200/283], lr: 0.00000044 	 loss = 0.0446(0.4470)
2023/11/23 06:41:08 - INFO - root -   Epoch: [195/300][220/283], lr: 0.00000044 	 loss = 0.2086(0.4287)
2023/11/23 06:41:16 - INFO - root -   Epoch: [195/300][240/283], lr: 0.00000044 	 loss = 0.9828(0.4302)
2023/11/23 06:41:29 - INFO - root -   Epoch: [195/300][260/283], lr: 0.00000044 	 loss = 2.3439(0.4385)
2023/11/23 06:41:40 - INFO - root -   Epoch: [195/300][280/283], lr: 0.00000044 	 loss = 1.9168(0.4272)
2023/11/23 06:41:41 - INFO - root -   Epoch: [195/300] 	 loss = 0.4252
2023/11/23 06:41:41 - INFO - root -   train_accuracy = 0.8498
2023/11/23 06:41:42 - INFO - root -   Epoch: [196/300][0/283], lr: 0.00000044 	 loss = 0.0362(0.0362)
2023/11/23 06:41:57 - INFO - root -   Epoch: [196/300][20/283], lr: 0.00000044 	 loss = 0.0343(0.3452)
2023/11/23 06:42:10 - INFO - root -   Epoch: [196/300][40/283], lr: 0.00000044 	 loss = 0.2056(0.3928)
2023/11/23 06:42:29 - INFO - root -   Epoch: [196/300][60/283], lr: 0.00000044 	 loss = 0.0048(0.4042)
2023/11/23 06:42:41 - INFO - root -   Epoch: [196/300][80/283], lr: 0.00000044 	 loss = 0.0663(0.4150)
2023/11/23 06:42:54 - INFO - root -   Epoch: [196/300][100/283], lr: 0.00000044 	 loss = 0.0873(0.4089)
2023/11/23 06:43:13 - INFO - root -   Epoch: [196/300][120/283], lr: 0.00000044 	 loss = 0.0442(0.3761)
2023/11/23 06:43:24 - INFO - root -   Epoch: [196/300][140/283], lr: 0.00000044 	 loss = 0.0098(0.3531)
2023/11/23 06:43:36 - INFO - root -   Epoch: [196/300][160/283], lr: 0.00000044 	 loss = 0.2051(0.3503)
2023/11/23 06:43:56 - INFO - root -   Epoch: [196/300][180/283], lr: 0.00000044 	 loss = 0.0387(0.3624)
2023/11/23 06:44:07 - INFO - root -   Epoch: [196/300][200/283], lr: 0.00000044 	 loss = 0.0255(0.3622)
2023/11/23 06:44:20 - INFO - root -   Epoch: [196/300][220/283], lr: 0.00000044 	 loss = 0.1572(0.3554)
2023/11/23 06:44:33 - INFO - root -   Epoch: [196/300][240/283], lr: 0.00000044 	 loss = 0.3214(0.3493)
2023/11/23 06:44:49 - INFO - root -   Epoch: [196/300][260/283], lr: 0.00000044 	 loss = 0.5076(0.3437)
2023/11/23 06:45:03 - INFO - root -   Epoch: [196/300][280/283], lr: 0.00000044 	 loss = 4.2542(0.3469)
2023/11/23 06:45:04 - INFO - root -   Epoch: [196/300] 	 loss = 0.3463
2023/11/23 06:45:04 - INFO - root -   train_accuracy = 0.8781
2023/11/23 06:45:05 - INFO - root -   Epoch: [197/300][0/283], lr: 0.00000044 	 loss = 0.0946(0.0946)
2023/11/23 06:45:25 - INFO - root -   Epoch: [197/300][20/283], lr: 0.00000044 	 loss = 1.3046(0.4325)
2023/11/23 06:45:52 - INFO - root -   Epoch: [197/300][40/283], lr: 0.00000044 	 loss = 0.0169(0.5149)
2023/11/23 06:46:06 - INFO - root -   Epoch: [197/300][60/283], lr: 0.00000044 	 loss = 0.0055(0.4761)
2023/11/23 06:46:21 - INFO - root -   Epoch: [197/300][80/283], lr: 0.00000044 	 loss = 0.0238(0.5376)
2023/11/23 06:46:35 - INFO - root -   Epoch: [197/300][100/283], lr: 0.00000044 	 loss = 0.0923(0.4957)
2023/11/23 06:46:46 - INFO - root -   Epoch: [197/300][120/283], lr: 0.00000044 	 loss = 0.0333(0.4336)
2023/11/23 06:46:55 - INFO - root -   Epoch: [197/300][140/283], lr: 0.00000044 	 loss = 0.0144(0.3917)
2023/11/23 06:47:09 - INFO - root -   Epoch: [197/300][160/283], lr: 0.00000044 	 loss = 1.3827(0.4025)
2023/11/23 06:47:28 - INFO - root -   Epoch: [197/300][180/283], lr: 0.00000044 	 loss = 0.2893(0.4230)
2023/11/23 06:47:41 - INFO - root -   Epoch: [197/300][200/283], lr: 0.00000044 	 loss = 0.1401(0.4174)
2023/11/23 06:47:59 - INFO - root -   Epoch: [197/300][220/283], lr: 0.00000044 	 loss = 0.0053(0.4026)
2023/11/23 06:48:07 - INFO - root -   Epoch: [197/300][240/283], lr: 0.00000044 	 loss = 0.4829(0.3992)
2023/11/23 06:48:21 - INFO - root -   Epoch: [197/300][260/283], lr: 0.00000044 	 loss = 1.1637(0.3916)
2023/11/23 06:48:35 - INFO - root -   Epoch: [197/300][280/283], lr: 0.00000044 	 loss = 3.2100(0.3916)
2023/11/23 06:48:36 - INFO - root -   Epoch: [197/300] 	 loss = 0.3893
2023/11/23 06:48:36 - INFO - root -   train_accuracy = 0.8675
2023/11/23 06:48:37 - INFO - root -   Epoch: [198/300][0/283], lr: 0.00000045 	 loss = 0.0548(0.0548)
2023/11/23 06:49:04 - INFO - root -   Epoch: [198/300][20/283], lr: 0.00000045 	 loss = 0.1093(0.4660)
2023/11/23 06:49:17 - INFO - root -   Epoch: [198/300][40/283], lr: 0.00000045 	 loss = 0.6663(0.4922)
2023/11/23 06:49:39 - INFO - root -   Epoch: [198/300][60/283], lr: 0.00000045 	 loss = 0.0060(0.5143)
2023/11/23 06:49:51 - INFO - root -   Epoch: [198/300][80/283], lr: 0.00000045 	 loss = 0.0513(0.5060)
2023/11/23 06:50:07 - INFO - root -   Epoch: [198/300][100/283], lr: 0.00000045 	 loss = 0.1679(0.4810)
2023/11/23 06:50:15 - INFO - root -   Epoch: [198/300][120/283], lr: 0.00000045 	 loss = 0.0564(0.4307)
2023/11/23 06:50:31 - INFO - root -   Epoch: [198/300][140/283], lr: 0.00000045 	 loss = 0.0137(0.3908)
2023/11/23 06:50:46 - INFO - root -   Epoch: [198/300][160/283], lr: 0.00000045 	 loss = 0.4416(0.3791)
2023/11/23 06:50:57 - INFO - root -   Epoch: [198/300][180/283], lr: 0.00000045 	 loss = 0.0724(0.3846)
2023/11/23 06:51:10 - INFO - root -   Epoch: [198/300][200/283], lr: 0.00000045 	 loss = 0.0892(0.3892)
2023/11/23 06:51:24 - INFO - root -   Epoch: [198/300][220/283], lr: 0.00000045 	 loss = 0.0153(0.3830)
2023/11/23 06:51:39 - INFO - root -   Epoch: [198/300][240/283], lr: 0.00000045 	 loss = 0.1407(0.3777)
2023/11/23 06:51:54 - INFO - root -   Epoch: [198/300][260/283], lr: 0.00000045 	 loss = 0.4404(0.3657)
2023/11/23 06:52:13 - INFO - root -   Epoch: [198/300][280/283], lr: 0.00000045 	 loss = 3.4315(0.3632)
2023/11/23 06:52:14 - INFO - root -   Epoch: [198/300] 	 loss = 0.3614
2023/11/23 06:52:14 - INFO - root -   train_accuracy = 0.8728
2023/11/23 06:52:15 - INFO - root -   Epoch: [199/300][0/283], lr: 0.00000045 	 loss = 0.0332(0.0332)
2023/11/23 06:52:33 - INFO - root -   Epoch: [199/300][20/283], lr: 0.00000045 	 loss = 0.9408(0.4967)
2023/11/23 06:52:52 - INFO - root -   Epoch: [199/300][40/283], lr: 0.00000045 	 loss = 0.0104(0.5247)
2023/11/23 06:53:05 - INFO - root -   Epoch: [199/300][60/283], lr: 0.00000045 	 loss = 0.0147(0.4978)
2023/11/23 06:53:17 - INFO - root -   Epoch: [199/300][80/283], lr: 0.00000045 	 loss = 0.2362(0.4988)
2023/11/23 06:53:38 - INFO - root -   Epoch: [199/300][100/283], lr: 0.00000045 	 loss = 0.0510(0.4515)
2023/11/23 06:53:50 - INFO - root -   Epoch: [199/300][120/283], lr: 0.00000045 	 loss = 0.0303(0.4383)
2023/11/23 06:54:05 - INFO - root -   Epoch: [199/300][140/283], lr: 0.00000045 	 loss = 0.0057(0.4150)
2023/11/23 06:54:17 - INFO - root -   Epoch: [199/300][160/283], lr: 0.00000045 	 loss = 0.3928(0.4039)
2023/11/23 06:54:28 - INFO - root -   Epoch: [199/300][180/283], lr: 0.00000045 	 loss = 0.0872(0.3967)
2023/11/23 06:54:47 - INFO - root -   Epoch: [199/300][200/283], lr: 0.00000045 	 loss = 0.0932(0.3843)
2023/11/23 06:55:00 - INFO - root -   Epoch: [199/300][220/283], lr: 0.00000045 	 loss = 0.0059(0.3743)
2023/11/23 06:55:12 - INFO - root -   Epoch: [199/300][240/283], lr: 0.00000045 	 loss = 0.3122(0.3606)
2023/11/23 06:55:25 - INFO - root -   Epoch: [199/300][260/283], lr: 0.00000045 	 loss = 1.3147(0.3570)
2023/11/23 06:55:35 - INFO - root -   Epoch: [199/300][280/283], lr: 0.00000045 	 loss = 1.3956(0.3559)
2023/11/23 06:55:36 - INFO - root -   Epoch: [199/300] 	 loss = 0.3547
2023/11/23 06:58:30 - INFO - root -   precision = 0.8182
2023/11/23 06:58:30 - INFO - root -   eval_loss = 0.5196
2023/11/23 06:58:30 - INFO - root -   eval_acc = 0.8182
2023/11/23 06:58:31 - INFO - root -   train_accuracy = 0.8763
2023/11/23 06:58:32 - INFO - root -   Epoch: [200/300][0/283], lr: 0.00000045 	 loss = 0.1125(0.1125)
2023/11/23 06:58:51 - INFO - root -   Epoch: [200/300][20/283], lr: 0.00000045 	 loss = 0.5113(0.3348)
2023/11/23 06:59:09 - INFO - root -   Epoch: [200/300][40/283], lr: 0.00000045 	 loss = 0.0186(0.4356)
2023/11/23 06:59:25 - INFO - root -   Epoch: [200/300][60/283], lr: 0.00000045 	 loss = 0.0179(0.4619)
2023/11/23 06:59:34 - INFO - root -   Epoch: [200/300][80/283], lr: 0.00000045 	 loss = 0.3514(0.4866)
2023/11/23 06:59:52 - INFO - root -   Epoch: [200/300][100/283], lr: 0.00000045 	 loss = 0.0404(0.4570)
2023/11/23 07:00:14 - INFO - root -   Epoch: [200/300][120/283], lr: 0.00000045 	 loss = 0.2876(0.4490)
2023/11/23 07:00:24 - INFO - root -   Epoch: [200/300][140/283], lr: 0.00000045 	 loss = 0.0132(0.4158)
2023/11/23 07:00:41 - INFO - root -   Epoch: [200/300][160/283], lr: 0.00000045 	 loss = 0.6148(0.4038)
2023/11/23 07:01:00 - INFO - root -   Epoch: [200/300][180/283], lr: 0.00000045 	 loss = 0.0527(0.4001)
2023/11/23 07:01:13 - INFO - root -   Epoch: [200/300][200/283], lr: 0.00000045 	 loss = 0.0271(0.3955)
2023/11/23 07:01:28 - INFO - root -   Epoch: [200/300][220/283], lr: 0.00000045 	 loss = 0.2021(0.3918)
2023/11/23 07:01:41 - INFO - root -   Epoch: [200/300][240/283], lr: 0.00000045 	 loss = 0.3683(0.3829)
2023/11/23 07:01:54 - INFO - root -   Epoch: [200/300][260/283], lr: 0.00000045 	 loss = 1.6512(0.3789)
2023/11/23 07:02:06 - INFO - root -   Epoch: [200/300][280/283], lr: 0.00000045 	 loss = 1.9061(0.3682)
2023/11/23 07:02:07 - INFO - root -   Epoch: [200/300] 	 loss = 0.3657
2023/11/23 07:02:07 - INFO - root -   train_accuracy = 0.8693
2023/11/23 07:02:08 - INFO - root -   Epoch: [201/300][0/283], lr: 0.00000045 	 loss = 0.0256(0.0256)
2023/11/23 07:02:25 - INFO - root -   Epoch: [201/300][20/283], lr: 0.00000045 	 loss = 0.1451(0.5669)
2023/11/23 07:02:34 - INFO - root -   Epoch: [201/300][40/283], lr: 0.00000045 	 loss = 0.0089(0.5051)
2023/11/23 07:02:48 - INFO - root -   Epoch: [201/300][60/283], lr: 0.00000045 	 loss = 0.0097(0.4522)
2023/11/23 07:03:01 - INFO - root -   Epoch: [201/300][80/283], lr: 0.00000045 	 loss = 0.2957(0.4510)
2023/11/23 07:03:24 - INFO - root -   Epoch: [201/300][100/283], lr: 0.00000045 	 loss = 0.0578(0.4344)
2023/11/23 07:03:33 - INFO - root -   Epoch: [201/300][120/283], lr: 0.00000045 	 loss = 0.1598(0.4086)
2023/11/23 07:03:52 - INFO - root -   Epoch: [201/300][140/283], lr: 0.00000045 	 loss = 0.1726(0.3906)
2023/11/23 07:04:01 - INFO - root -   Epoch: [201/300][160/283], lr: 0.00000045 	 loss = 1.7471(0.4027)
2023/11/23 07:04:15 - INFO - root -   Epoch: [201/300][180/283], lr: 0.00000045 	 loss = 0.1659(0.4048)
2023/11/23 07:04:32 - INFO - root -   Epoch: [201/300][200/283], lr: 0.00000045 	 loss = 0.1308(0.4150)
2023/11/23 07:04:52 - INFO - root -   Epoch: [201/300][220/283], lr: 0.00000045 	 loss = 0.1027(0.4151)
2023/11/23 07:05:05 - INFO - root -   Epoch: [201/300][240/283], lr: 0.00000045 	 loss = 0.2909(0.4180)
2023/11/23 07:05:19 - INFO - root -   Epoch: [201/300][260/283], lr: 0.00000045 	 loss = 0.3786(0.4043)
2023/11/23 07:05:31 - INFO - root -   Epoch: [201/300][280/283], lr: 0.00000045 	 loss = 3.5834(0.4080)
2023/11/23 07:05:32 - INFO - root -   Epoch: [201/300] 	 loss = 0.4054
2023/11/23 07:05:32 - INFO - root -   train_accuracy = 0.8551
2023/11/23 07:05:33 - INFO - root -   Epoch: [202/300][0/283], lr: 0.00000045 	 loss = 0.0197(0.0197)
2023/11/23 07:05:54 - INFO - root -   Epoch: [202/300][20/283], lr: 0.00000045 	 loss = 0.8774(0.5559)
2023/11/23 07:06:04 - INFO - root -   Epoch: [202/300][40/283], lr: 0.00000045 	 loss = 0.0585(0.5370)
2023/11/23 07:06:27 - INFO - root -   Epoch: [202/300][60/283], lr: 0.00000045 	 loss = 0.0162(0.4979)
2023/11/23 07:06:42 - INFO - root -   Epoch: [202/300][80/283], lr: 0.00000045 	 loss = 0.2347(0.5277)
2023/11/23 07:07:08 - INFO - root -   Epoch: [202/300][100/283], lr: 0.00000045 	 loss = 0.1227(0.4926)
2023/11/23 07:07:19 - INFO - root -   Epoch: [202/300][120/283], lr: 0.00000045 	 loss = 0.0897(0.4551)
2023/11/23 07:07:38 - INFO - root -   Epoch: [202/300][140/283], lr: 0.00000045 	 loss = 0.1236(0.4181)
2023/11/23 07:07:46 - INFO - root -   Epoch: [202/300][160/283], lr: 0.00000045 	 loss = 0.3068(0.3928)
2023/11/23 07:08:10 - INFO - root -   Epoch: [202/300][180/283], lr: 0.00000045 	 loss = 0.1367(0.4012)
2023/11/23 07:08:23 - INFO - root -   Epoch: [202/300][200/283], lr: 0.00000045 	 loss = 0.0165(0.3985)
2023/11/23 07:08:44 - INFO - root -   Epoch: [202/300][220/283], lr: 0.00000045 	 loss = 0.0592(0.3981)
2023/11/23 07:08:52 - INFO - root -   Epoch: [202/300][240/283], lr: 0.00000045 	 loss = 0.4928(0.3886)
2023/11/23 07:09:09 - INFO - root -   Epoch: [202/300][260/283], lr: 0.00000045 	 loss = 1.6766(0.3805)
2023/11/23 07:09:17 - INFO - root -   Epoch: [202/300][280/283], lr: 0.00000045 	 loss = 2.0207(0.3786)
2023/11/23 07:09:17 - INFO - root -   Epoch: [202/300] 	 loss = 0.3767
2023/11/23 07:09:17 - INFO - root -   train_accuracy = 0.8622
2023/11/23 07:09:19 - INFO - root -   Epoch: [203/300][0/283], lr: 0.00000046 	 loss = 0.1237(0.1237)
2023/11/23 07:09:40 - INFO - root -   Epoch: [203/300][20/283], lr: 0.00000046 	 loss = 0.3241(0.6641)
2023/11/23 07:09:50 - INFO - root -   Epoch: [203/300][40/283], lr: 0.00000046 	 loss = 0.1513(0.5608)
2023/11/23 07:10:15 - INFO - root -   Epoch: [203/300][60/283], lr: 0.00000046 	 loss = 0.1718(0.5597)
2023/11/23 07:10:32 - INFO - root -   Epoch: [203/300][80/283], lr: 0.00000046 	 loss = 0.2260(0.5493)
2023/11/23 07:10:49 - INFO - root -   Epoch: [203/300][100/283], lr: 0.00000046 	 loss = 0.0366(0.4917)
2023/11/23 07:11:10 - INFO - root -   Epoch: [203/300][120/283], lr: 0.00000046 	 loss = 0.0726(0.4371)
2023/11/23 07:11:25 - INFO - root -   Epoch: [203/300][140/283], lr: 0.00000046 	 loss = 0.1498(0.4123)
2023/11/23 07:11:46 - INFO - root -   Epoch: [203/300][160/283], lr: 0.00000046 	 loss = 1.7848(0.4163)
2023/11/23 07:11:54 - INFO - root -   Epoch: [203/300][180/283], lr: 0.00000046 	 loss = 0.0054(0.4075)
2023/11/23 07:12:11 - INFO - root -   Epoch: [203/300][200/283], lr: 0.00000046 	 loss = 0.0077(0.4005)
2023/11/23 07:12:32 - INFO - root -   Epoch: [203/300][220/283], lr: 0.00000046 	 loss = 0.0081(0.3887)
2023/11/23 07:12:42 - INFO - root -   Epoch: [203/300][240/283], lr: 0.00000046 	 loss = 0.0906(0.3879)
2023/11/23 07:13:03 - INFO - root -   Epoch: [203/300][260/283], lr: 0.00000046 	 loss = 2.5090(0.3901)
2023/11/23 07:13:19 - INFO - root -   Epoch: [203/300][280/283], lr: 0.00000046 	 loss = 1.8106(0.3864)
2023/11/23 07:13:20 - INFO - root -   Epoch: [203/300] 	 loss = 0.3839
2023/11/23 07:13:20 - INFO - root -   train_accuracy = 0.8693
2023/11/23 07:13:30 - INFO - root -   Epoch: [204/300][0/283], lr: 0.00000046 	 loss = 0.0529(0.0529)
2023/11/23 07:13:53 - INFO - root -   Epoch: [204/300][20/283], lr: 0.00000046 	 loss = 1.7346(0.4955)
2023/11/23 07:14:08 - INFO - root -   Epoch: [204/300][40/283], lr: 0.00000046 	 loss = 0.0175(0.5489)
2023/11/23 07:14:22 - INFO - root -   Epoch: [204/300][60/283], lr: 0.00000046 	 loss = 0.0074(0.5296)
2023/11/23 07:14:37 - INFO - root -   Epoch: [204/300][80/283], lr: 0.00000046 	 loss = 0.1874(0.5311)
2023/11/23 07:14:54 - INFO - root -   Epoch: [204/300][100/283], lr: 0.00000046 	 loss = 0.0522(0.5018)
2023/11/23 07:15:16 - INFO - root -   Epoch: [204/300][120/283], lr: 0.00000046 	 loss = 0.0267(0.4607)
2023/11/23 07:15:29 - INFO - root -   Epoch: [204/300][140/283], lr: 0.00000046 	 loss = 0.0040(0.4451)
2023/11/23 07:15:44 - INFO - root -   Epoch: [204/300][160/283], lr: 0.00000046 	 loss = 0.7201(0.4417)
2023/11/23 07:16:02 - INFO - root -   Epoch: [204/300][180/283], lr: 0.00000046 	 loss = 0.0788(0.4195)
2023/11/23 07:16:13 - INFO - root -   Epoch: [204/300][200/283], lr: 0.00000046 	 loss = 0.0879(0.4032)
2023/11/23 07:16:30 - INFO - root -   Epoch: [204/300][220/283], lr: 0.00000046 	 loss = 0.0250(0.3944)
2023/11/23 07:16:41 - INFO - root -   Epoch: [204/300][240/283], lr: 0.00000046 	 loss = 0.1598(0.4032)
2023/11/23 07:17:02 - INFO - root -   Epoch: [204/300][260/283], lr: 0.00000046 	 loss = 2.3648(0.3964)
2023/11/23 07:17:10 - INFO - root -   Epoch: [204/300][280/283], lr: 0.00000046 	 loss = 1.3378(0.3897)
2023/11/23 07:17:10 - INFO - root -   Epoch: [204/300] 	 loss = 0.3886
2023/11/23 07:20:05 - INFO - root -   precision = 0.8322
2023/11/23 07:20:05 - INFO - root -   eval_loss = 0.5345
2023/11/23 07:20:05 - INFO - root -   eval_acc = 0.8322
2023/11/23 07:20:06 - INFO - root -   train_accuracy = 0.8516
2023/11/23 07:20:07 - INFO - root -   Epoch: [205/300][0/283], lr: 0.00000046 	 loss = 0.0257(0.0257)
2023/11/23 07:20:26 - INFO - root -   Epoch: [205/300][20/283], lr: 0.00000046 	 loss = 0.3326(0.4985)
2023/11/23 07:20:44 - INFO - root -   Epoch: [205/300][40/283], lr: 0.00000046 	 loss = 0.0144(0.4905)
2023/11/23 07:20:58 - INFO - root -   Epoch: [205/300][60/283], lr: 0.00000046 	 loss = 0.2371(0.4590)
2023/11/23 07:21:08 - INFO - root -   Epoch: [205/300][80/283], lr: 0.00000046 	 loss = 0.3184(0.4898)
2023/11/23 07:21:16 - INFO - root -   Epoch: [205/300][100/283], lr: 0.00000046 	 loss = 0.0376(0.4325)
2023/11/23 07:21:29 - INFO - root -   Epoch: [205/300][120/283], lr: 0.00000046 	 loss = 0.0108(0.3916)
2023/11/23 07:21:41 - INFO - root -   Epoch: [205/300][140/283], lr: 0.00000046 	 loss = 0.0436(0.3684)
2023/11/23 07:21:50 - INFO - root -   Epoch: [205/300][160/283], lr: 0.00000046 	 loss = 0.1532(0.3674)
2023/11/23 07:22:07 - INFO - root -   Epoch: [205/300][180/283], lr: 0.00000046 	 loss = 0.0581(0.3747)
2023/11/23 07:22:25 - INFO - root -   Epoch: [205/300][200/283], lr: 0.00000046 	 loss = 0.0189(0.3691)
2023/11/23 07:22:41 - INFO - root -   Epoch: [205/300][220/283], lr: 0.00000046 	 loss = 0.0832(0.3552)
2023/11/23 07:22:55 - INFO - root -   Epoch: [205/300][240/283], lr: 0.00000046 	 loss = 0.2565(0.3547)
2023/11/23 07:23:09 - INFO - root -   Epoch: [205/300][260/283], lr: 0.00000046 	 loss = 3.7444(0.3655)
2023/11/23 07:23:26 - INFO - root -   Epoch: [205/300][280/283], lr: 0.00000046 	 loss = 2.3935(0.3555)
2023/11/23 07:23:27 - INFO - root -   Epoch: [205/300] 	 loss = 0.3542
2023/11/23 07:23:27 - INFO - root -   train_accuracy = 0.8781
2023/11/23 07:23:29 - INFO - root -   Epoch: [206/300][0/283], lr: 0.00000046 	 loss = 0.0309(0.0309)
2023/11/23 07:23:44 - INFO - root -   Epoch: [206/300][20/283], lr: 0.00000046 	 loss = 0.7523(0.5678)
2023/11/23 07:24:04 - INFO - root -   Epoch: [206/300][40/283], lr: 0.00000046 	 loss = 0.0390(0.5813)
2023/11/23 07:24:19 - INFO - root -   Epoch: [206/300][60/283], lr: 0.00000046 	 loss = 0.0026(0.5404)
2023/11/23 07:24:27 - INFO - root -   Epoch: [206/300][80/283], lr: 0.00000046 	 loss = 0.1147(0.5267)
2023/11/23 07:24:42 - INFO - root -   Epoch: [206/300][100/283], lr: 0.00000046 	 loss = 0.1190(0.4813)
2023/11/23 07:24:54 - INFO - root -   Epoch: [206/300][120/283], lr: 0.00000046 	 loss = 0.5575(0.4389)
2023/11/23 07:25:07 - INFO - root -   Epoch: [206/300][140/283], lr: 0.00000046 	 loss = 0.0097(0.4005)
2023/11/23 07:25:22 - INFO - root -   Epoch: [206/300][160/283], lr: 0.00000046 	 loss = 0.3964(0.3990)
2023/11/23 07:25:31 - INFO - root -   Epoch: [206/300][180/283], lr: 0.00000046 	 loss = 0.0228(0.4057)
2023/11/23 07:25:46 - INFO - root -   Epoch: [206/300][200/283], lr: 0.00000046 	 loss = 0.0075(0.4099)
2023/11/23 07:25:59 - INFO - root -   Epoch: [206/300][220/283], lr: 0.00000046 	 loss = 0.0435(0.3957)
2023/11/23 07:26:12 - INFO - root -   Epoch: [206/300][240/283], lr: 0.00000046 	 loss = 0.3517(0.3942)
2023/11/23 07:26:26 - INFO - root -   Epoch: [206/300][260/283], lr: 0.00000046 	 loss = 1.7602(0.3866)
2023/11/23 07:26:41 - INFO - root -   Epoch: [206/300][280/283], lr: 0.00000046 	 loss = 1.0783(0.3765)
2023/11/23 07:26:42 - INFO - root -   Epoch: [206/300] 	 loss = 0.3741
2023/11/23 07:26:42 - INFO - root -   train_accuracy = 0.8587
2023/11/23 07:26:51 - INFO - root -   Epoch: [207/300][0/283], lr: 0.00000046 	 loss = 0.1920(0.1920)
2023/11/23 07:27:07 - INFO - root -   Epoch: [207/300][20/283], lr: 0.00000046 	 loss = 1.4171(0.4809)
2023/11/23 07:27:15 - INFO - root -   Epoch: [207/300][40/283], lr: 0.00000046 	 loss = 0.0116(0.4873)
2023/11/23 07:27:31 - INFO - root -   Epoch: [207/300][60/283], lr: 0.00000046 	 loss = 0.0091(0.4435)
2023/11/23 07:27:50 - INFO - root -   Epoch: [207/300][80/283], lr: 0.00000046 	 loss = 0.5628(0.4655)
2023/11/23 07:28:00 - INFO - root -   Epoch: [207/300][100/283], lr: 0.00000046 	 loss = 0.0800(0.4597)
2023/11/23 07:28:15 - INFO - root -   Epoch: [207/300][120/283], lr: 0.00000046 	 loss = 0.0180(0.4034)
2023/11/23 07:28:27 - INFO - root -   Epoch: [207/300][140/283], lr: 0.00000046 	 loss = 0.2078(0.3967)
2023/11/23 07:28:41 - INFO - root -   Epoch: [207/300][160/283], lr: 0.00000046 	 loss = 0.8813(0.3934)
2023/11/23 07:28:52 - INFO - root -   Epoch: [207/300][180/283], lr: 0.00000046 	 loss = 0.0316(0.3842)
2023/11/23 07:29:05 - INFO - root -   Epoch: [207/300][200/283], lr: 0.00000046 	 loss = 0.0179(0.3839)
2023/11/23 07:29:24 - INFO - root -   Epoch: [207/300][220/283], lr: 0.00000046 	 loss = 0.1929(0.3724)
2023/11/23 07:29:33 - INFO - root -   Epoch: [207/300][240/283], lr: 0.00000046 	 loss = 1.2762(0.3761)
2023/11/23 07:29:50 - INFO - root -   Epoch: [207/300][260/283], lr: 0.00000046 	 loss = 1.4268(0.3762)
2023/11/23 07:30:01 - INFO - root -   Epoch: [207/300][280/283], lr: 0.00000046 	 loss = 2.1609(0.3678)
2023/11/23 07:30:02 - INFO - root -   Epoch: [207/300] 	 loss = 0.3653
2023/11/23 07:30:02 - INFO - root -   train_accuracy = 0.8746
2023/11/23 07:30:03 - INFO - root -   Epoch: [208/300][0/283], lr: 0.00000046 	 loss = 0.0084(0.0084)
2023/11/23 07:30:21 - INFO - root -   Epoch: [208/300][20/283], lr: 0.00000046 	 loss = 1.8897(0.4793)
2023/11/23 07:30:34 - INFO - root -   Epoch: [208/300][40/283], lr: 0.00000046 	 loss = 0.1423(0.4749)
2023/11/23 07:30:42 - INFO - root -   Epoch: [208/300][60/283], lr: 0.00000046 	 loss = 0.0049(0.4424)
2023/11/23 07:30:58 - INFO - root -   Epoch: [208/300][80/283], lr: 0.00000046 	 loss = 0.3672(0.4751)
2023/11/23 07:31:21 - INFO - root -   Epoch: [208/300][100/283], lr: 0.00000046 	 loss = 0.0455(0.4408)
2023/11/23 07:31:36 - INFO - root -   Epoch: [208/300][120/283], lr: 0.00000046 	 loss = 0.0766(0.4299)
2023/11/23 07:31:49 - INFO - root -   Epoch: [208/300][140/283], lr: 0.00000046 	 loss = 0.0075(0.4381)
2023/11/23 07:32:08 - INFO - root -   Epoch: [208/300][160/283], lr: 0.00000046 	 loss = 0.1105(0.4297)
2023/11/23 07:32:19 - INFO - root -   Epoch: [208/300][180/283], lr: 0.00000046 	 loss = 0.1950(0.4229)
2023/11/23 07:32:33 - INFO - root -   Epoch: [208/300][200/283], lr: 0.00000046 	 loss = 0.4876(0.4183)
2023/11/23 07:32:48 - INFO - root -   Epoch: [208/300][220/283], lr: 0.00000046 	 loss = 0.1050(0.4078)
2023/11/23 07:33:06 - INFO - root -   Epoch: [208/300][240/283], lr: 0.00000046 	 loss = 0.0662(0.4016)
2023/11/23 07:33:17 - INFO - root -   Epoch: [208/300][260/283], lr: 0.00000046 	 loss = 0.2000(0.3881)
2023/11/23 07:33:33 - INFO - root -   Epoch: [208/300][280/283], lr: 0.00000046 	 loss = 2.4734(0.3775)
2023/11/23 07:33:34 - INFO - root -   Epoch: [208/300] 	 loss = 0.3749
2023/11/23 07:33:34 - INFO - root -   train_accuracy = 0.8746
2023/11/23 07:33:44 - INFO - root -   Epoch: [209/300][0/283], lr: 0.00000047 	 loss = 0.1141(0.1141)
2023/11/23 07:33:55 - INFO - root -   Epoch: [209/300][20/283], lr: 0.00000047 	 loss = 0.3209(0.4193)
2023/11/23 07:34:05 - INFO - root -   Epoch: [209/300][40/283], lr: 0.00000047 	 loss = 0.0157(0.4668)
2023/11/23 07:34:16 - INFO - root -   Epoch: [209/300][60/283], lr: 0.00000047 	 loss = 0.2590(0.4493)
2023/11/23 07:34:38 - INFO - root -   Epoch: [209/300][80/283], lr: 0.00000047 	 loss = 0.0241(0.5213)
2023/11/23 07:34:55 - INFO - root -   Epoch: [209/300][100/283], lr: 0.00000047 	 loss = 0.0502(0.4962)
2023/11/23 07:35:07 - INFO - root -   Epoch: [209/300][120/283], lr: 0.00000047 	 loss = 0.0976(0.4397)
2023/11/23 07:35:21 - INFO - root -   Epoch: [209/300][140/283], lr: 0.00000047 	 loss = 0.1369(0.4109)
2023/11/23 07:35:39 - INFO - root -   Epoch: [209/300][160/283], lr: 0.00000047 	 loss = 0.3537(0.4145)
2023/11/23 07:35:54 - INFO - root -   Epoch: [209/300][180/283], lr: 0.00000047 	 loss = 0.0064(0.4114)
2023/11/23 07:36:11 - INFO - root -   Epoch: [209/300][200/283], lr: 0.00000047 	 loss = 0.0802(0.4189)
2023/11/23 07:36:26 - INFO - root -   Epoch: [209/300][220/283], lr: 0.00000047 	 loss = 0.0063(0.4089)
2023/11/23 07:36:37 - INFO - root -   Epoch: [209/300][240/283], lr: 0.00000047 	 loss = 0.0256(0.4000)
2023/11/23 07:36:47 - INFO - root -   Epoch: [209/300][260/283], lr: 0.00000047 	 loss = 1.7421(0.3937)
2023/11/23 07:36:57 - INFO - root -   Epoch: [209/300][280/283], lr: 0.00000047 	 loss = 2.4102(0.3895)
2023/11/23 07:36:58 - INFO - root -   Epoch: [209/300] 	 loss = 0.3869
2023/11/23 07:39:51 - INFO - root -   precision = 0.8252
2023/11/23 07:39:51 - INFO - root -   eval_loss = 0.5083
2023/11/23 07:39:51 - INFO - root -   eval_acc = 0.8252
2023/11/23 07:39:52 - INFO - root -   train_accuracy = 0.8587
2023/11/23 07:39:59 - INFO - root -   Epoch: [210/300][0/283], lr: 0.00000047 	 loss = 0.1940(0.1940)
2023/11/23 07:40:14 - INFO - root -   Epoch: [210/300][20/283], lr: 0.00000047 	 loss = 1.1725(0.4718)
2023/11/23 07:40:36 - INFO - root -   Epoch: [210/300][40/283], lr: 0.00000047 	 loss = 0.0168(0.4831)
2023/11/23 07:40:55 - INFO - root -   Epoch: [210/300][60/283], lr: 0.00000047 	 loss = 0.1127(0.4994)
2023/11/23 07:41:06 - INFO - root -   Epoch: [210/300][80/283], lr: 0.00000047 	 loss = 0.2186(0.4875)
2023/11/23 07:41:23 - INFO - root -   Epoch: [210/300][100/283], lr: 0.00000047 	 loss = 0.0844(0.4813)
2023/11/23 07:41:43 - INFO - root -   Epoch: [210/300][120/283], lr: 0.00000047 	 loss = 0.0090(0.4340)
2023/11/23 07:41:52 - INFO - root -   Epoch: [210/300][140/283], lr: 0.00000047 	 loss = 0.0129(0.4034)
2023/11/23 07:42:10 - INFO - root -   Epoch: [210/300][160/283], lr: 0.00000047 	 loss = 2.1321(0.4037)
2023/11/23 07:42:26 - INFO - root -   Epoch: [210/300][180/283], lr: 0.00000047 	 loss = 0.0258(0.4068)
2023/11/23 07:42:35 - INFO - root -   Epoch: [210/300][200/283], lr: 0.00000047 	 loss = 0.0834(0.3960)
2023/11/23 07:42:53 - INFO - root -   Epoch: [210/300][220/283], lr: 0.00000047 	 loss = 0.1942(0.3921)
2023/11/23 07:43:11 - INFO - root -   Epoch: [210/300][240/283], lr: 0.00000047 	 loss = 2.0254(0.3994)
2023/11/23 07:43:22 - INFO - root -   Epoch: [210/300][260/283], lr: 0.00000047 	 loss = 1.4716(0.3928)
2023/11/23 07:43:36 - INFO - root -   Epoch: [210/300][280/283], lr: 0.00000047 	 loss = 3.2161(0.3900)
2023/11/23 07:43:37 - INFO - root -   Epoch: [210/300] 	 loss = 0.3880
2023/11/23 07:43:37 - INFO - root -   train_accuracy = 0.8657
2023/11/23 07:43:50 - INFO - root -   Epoch: [211/300][0/283], lr: 0.00000047 	 loss = 0.0683(0.0683)
2023/11/23 07:44:05 - INFO - root -   Epoch: [211/300][20/283], lr: 0.00000047 	 loss = 0.4453(0.5892)
2023/11/23 07:44:21 - INFO - root -   Epoch: [211/300][40/283], lr: 0.00000047 	 loss = 0.1510(0.5374)
2023/11/23 07:44:38 - INFO - root -   Epoch: [211/300][60/283], lr: 0.00000047 	 loss = 0.0047(0.4843)
2023/11/23 07:44:50 - INFO - root -   Epoch: [211/300][80/283], lr: 0.00000047 	 loss = 0.1330(0.4803)
2023/11/23 07:45:07 - INFO - root -   Epoch: [211/300][100/283], lr: 0.00000047 	 loss = 0.1530(0.4336)
2023/11/23 07:45:18 - INFO - root -   Epoch: [211/300][120/283], lr: 0.00000047 	 loss = 0.1053(0.3968)
2023/11/23 07:45:28 - INFO - root -   Epoch: [211/300][140/283], lr: 0.00000047 	 loss = 0.0053(0.3736)
2023/11/23 07:45:51 - INFO - root -   Epoch: [211/300][160/283], lr: 0.00000047 	 loss = 1.5989(0.3675)
2023/11/23 07:46:04 - INFO - root -   Epoch: [211/300][180/283], lr: 0.00000047 	 loss = 0.0724(0.3751)
2023/11/23 07:46:17 - INFO - root -   Epoch: [211/300][200/283], lr: 0.00000047 	 loss = 0.0150(0.3642)
2023/11/23 07:46:38 - INFO - root -   Epoch: [211/300][220/283], lr: 0.00000047 	 loss = 0.0146(0.3599)
2023/11/23 07:46:49 - INFO - root -   Epoch: [211/300][240/283], lr: 0.00000047 	 loss = 0.1853(0.3555)
2023/11/23 07:47:00 - INFO - root -   Epoch: [211/300][260/283], lr: 0.00000047 	 loss = 0.9954(0.3422)
2023/11/23 07:47:09 - INFO - root -   Epoch: [211/300][280/283], lr: 0.00000047 	 loss = 0.8620(0.3317)
2023/11/23 07:47:10 - INFO - root -   Epoch: [211/300] 	 loss = 0.3294
2023/11/23 07:47:10 - INFO - root -   train_accuracy = 0.8834
2023/11/23 07:47:16 - INFO - root -   Epoch: [212/300][0/283], lr: 0.00000047 	 loss = 0.1076(0.1076)
2023/11/23 07:47:29 - INFO - root -   Epoch: [212/300][20/283], lr: 0.00000047 	 loss = 0.0745(0.6253)
2023/11/23 07:47:45 - INFO - root -   Epoch: [212/300][40/283], lr: 0.00000047 	 loss = 0.0523(0.5101)
2023/11/23 07:48:00 - INFO - root -   Epoch: [212/300][60/283], lr: 0.00000047 	 loss = 0.1466(0.4519)
2023/11/23 07:48:13 - INFO - root -   Epoch: [212/300][80/283], lr: 0.00000047 	 loss = 0.0235(0.4360)
2023/11/23 07:48:27 - INFO - root -   Epoch: [212/300][100/283], lr: 0.00000047 	 loss = 0.0899(0.4404)
2023/11/23 07:48:42 - INFO - root -   Epoch: [212/300][120/283], lr: 0.00000047 	 loss = 0.1016(0.4277)
2023/11/23 07:48:58 - INFO - root -   Epoch: [212/300][140/283], lr: 0.00000047 	 loss = 0.0464(0.4028)
2023/11/23 07:49:16 - INFO - root -   Epoch: [212/300][160/283], lr: 0.00000047 	 loss = 0.7174(0.3987)
2023/11/23 07:49:36 - INFO - root -   Epoch: [212/300][180/283], lr: 0.00000047 	 loss = 0.2176(0.4098)
2023/11/23 07:49:44 - INFO - root -   Epoch: [212/300][200/283], lr: 0.00000047 	 loss = 0.0177(0.3951)
2023/11/23 07:50:08 - INFO - root -   Epoch: [212/300][220/283], lr: 0.00000047 	 loss = 0.0112(0.3991)
2023/11/23 07:50:17 - INFO - root -   Epoch: [212/300][240/283], lr: 0.00000047 	 loss = 0.1897(0.4000)
2023/11/23 07:50:33 - INFO - root -   Epoch: [212/300][260/283], lr: 0.00000047 	 loss = 1.1476(0.3916)
2023/11/23 07:50:49 - INFO - root -   Epoch: [212/300][280/283], lr: 0.00000047 	 loss = 0.6612(0.3749)
2023/11/23 07:50:50 - INFO - root -   Epoch: [212/300] 	 loss = 0.3725
2023/11/23 07:50:50 - INFO - root -   train_accuracy = 0.8763
2023/11/23 07:50:51 - INFO - root -   Epoch: [213/300][0/283], lr: 0.00000047 	 loss = 0.1397(0.1397)
2023/11/23 07:51:21 - INFO - root -   Epoch: [213/300][20/283], lr: 0.00000047 	 loss = 0.7028(0.6553)
2023/11/23 07:51:36 - INFO - root -   Epoch: [213/300][40/283], lr: 0.00000047 	 loss = 0.3249(0.6130)
2023/11/23 07:51:54 - INFO - root -   Epoch: [213/300][60/283], lr: 0.00000047 	 loss = 0.0564(0.6045)
2023/11/23 07:52:10 - INFO - root -   Epoch: [213/300][80/283], lr: 0.00000047 	 loss = 0.2546(0.5638)
2023/11/23 07:52:20 - INFO - root -   Epoch: [213/300][100/283], lr: 0.00000047 	 loss = 0.1020(0.4980)
2023/11/23 07:52:29 - INFO - root -   Epoch: [213/300][120/283], lr: 0.00000047 	 loss = 0.0247(0.4562)
2023/11/23 07:52:46 - INFO - root -   Epoch: [213/300][140/283], lr: 0.00000047 	 loss = 0.0096(0.4292)
2023/11/23 07:53:02 - INFO - root -   Epoch: [213/300][160/283], lr: 0.00000047 	 loss = 1.3743(0.4155)
2023/11/23 07:53:25 - INFO - root -   Epoch: [213/300][180/283], lr: 0.00000047 	 loss = 0.0620(0.4325)
2023/11/23 07:53:41 - INFO - root -   Epoch: [213/300][200/283], lr: 0.00000047 	 loss = 0.0440(0.4291)
2023/11/23 07:53:57 - INFO - root -   Epoch: [213/300][220/283], lr: 0.00000047 	 loss = 0.0424(0.4123)
2023/11/23 07:54:12 - INFO - root -   Epoch: [213/300][240/283], lr: 0.00000047 	 loss = 2.0947(0.4016)
2023/11/23 07:54:25 - INFO - root -   Epoch: [213/300][260/283], lr: 0.00000047 	 loss = 2.0310(0.3950)
2023/11/23 07:54:33 - INFO - root -   Epoch: [213/300][280/283], lr: 0.00000047 	 loss = 1.3801(0.3843)
2023/11/23 07:54:34 - INFO - root -   Epoch: [213/300] 	 loss = 0.3820
2023/11/23 07:54:34 - INFO - root -   train_accuracy = 0.8675
2023/11/23 07:54:35 - INFO - root -   Epoch: [214/300][0/283], lr: 0.00000047 	 loss = 0.0714(0.0714)
2023/11/23 07:54:58 - INFO - root -   Epoch: [214/300][20/283], lr: 0.00000047 	 loss = 0.5127(0.5732)
2023/11/23 07:55:10 - INFO - root -   Epoch: [214/300][40/283], lr: 0.00000047 	 loss = 0.0246(0.4561)
2023/11/23 07:55:20 - INFO - root -   Epoch: [214/300][60/283], lr: 0.00000047 	 loss = 0.0091(0.4023)
2023/11/23 07:55:36 - INFO - root -   Epoch: [214/300][80/283], lr: 0.00000047 	 loss = 0.0816(0.3993)
2023/11/23 07:55:52 - INFO - root -   Epoch: [214/300][100/283], lr: 0.00000047 	 loss = 0.0499(0.3770)
2023/11/23 07:56:06 - INFO - root -   Epoch: [214/300][120/283], lr: 0.00000047 	 loss = 1.2152(0.3667)
2023/11/23 07:56:33 - INFO - root -   Epoch: [214/300][140/283], lr: 0.00000047 	 loss = 0.0027(0.3470)
2023/11/23 07:56:46 - INFO - root -   Epoch: [214/300][160/283], lr: 0.00000047 	 loss = 1.3392(0.3694)
2023/11/23 07:57:01 - INFO - root -   Epoch: [214/300][180/283], lr: 0.00000047 	 loss = 0.0799(0.3753)
2023/11/23 07:57:17 - INFO - root -   Epoch: [214/300][200/283], lr: 0.00000047 	 loss = 0.1521(0.3684)
2023/11/23 07:57:28 - INFO - root -   Epoch: [214/300][220/283], lr: 0.00000047 	 loss = 0.0089(0.3726)
2023/11/23 07:57:40 - INFO - root -   Epoch: [214/300][240/283], lr: 0.00000047 	 loss = 0.2161(0.3691)
2023/11/23 07:57:55 - INFO - root -   Epoch: [214/300][260/283], lr: 0.00000047 	 loss = 0.1348(0.3601)
2023/11/23 07:58:07 - INFO - root -   Epoch: [214/300][280/283], lr: 0.00000047 	 loss = 1.8555(0.3523)
2023/11/23 07:58:08 - INFO - root -   Epoch: [214/300] 	 loss = 0.3499
2023/11/23 08:01:01 - INFO - root -   precision = 0.8392
2023/11/23 08:01:01 - INFO - root -   eval_loss = 0.5048
2023/11/23 08:01:01 - INFO - root -   eval_acc = 0.8392
2023/11/23 08:01:02 - INFO - root -   train_accuracy = 0.8781
2023/11/23 08:01:03 - INFO - root -   Epoch: [215/300][0/283], lr: 0.00000048 	 loss = 0.1136(0.1136)
2023/11/23 08:01:28 - INFO - root -   Epoch: [215/300][20/283], lr: 0.00000048 	 loss = 0.5859(0.5587)
2023/11/23 08:01:44 - INFO - root -   Epoch: [215/300][40/283], lr: 0.00000048 	 loss = 0.0095(0.5503)
2023/11/23 08:01:52 - INFO - root -   Epoch: [215/300][60/283], lr: 0.00000048 	 loss = 0.0149(0.4596)
2023/11/23 08:02:12 - INFO - root -   Epoch: [215/300][80/283], lr: 0.00000048 	 loss = 0.0254(0.4687)
2023/11/23 08:02:23 - INFO - root -   Epoch: [215/300][100/283], lr: 0.00000048 	 loss = 0.0981(0.4363)
2023/11/23 08:02:40 - INFO - root -   Epoch: [215/300][120/283], lr: 0.00000048 	 loss = 0.5351(0.4140)
2023/11/23 08:02:52 - INFO - root -   Epoch: [215/300][140/283], lr: 0.00000048 	 loss = 0.0332(0.3853)
2023/11/23 08:03:11 - INFO - root -   Epoch: [215/300][160/283], lr: 0.00000048 	 loss = 0.2073(0.3925)
2023/11/23 08:03:23 - INFO - root -   Epoch: [215/300][180/283], lr: 0.00000048 	 loss = 0.1449(0.3854)
2023/11/23 08:03:40 - INFO - root -   Epoch: [215/300][200/283], lr: 0.00000048 	 loss = 0.0893(0.3896)
2023/11/23 08:03:57 - INFO - root -   Epoch: [215/300][220/283], lr: 0.00000048 	 loss = 0.0178(0.3947)
2023/11/23 08:04:16 - INFO - root -   Epoch: [215/300][240/283], lr: 0.00000048 	 loss = 0.1052(0.3904)
2023/11/23 08:04:24 - INFO - root -   Epoch: [215/300][260/283], lr: 0.00000048 	 loss = 0.4311(0.3817)
2023/11/23 08:04:34 - INFO - root -   Epoch: [215/300][280/283], lr: 0.00000048 	 loss = 3.0086(0.3790)
2023/11/23 08:04:35 - INFO - root -   Epoch: [215/300] 	 loss = 0.3773
2023/11/23 08:04:35 - INFO - root -   train_accuracy = 0.8640
2023/11/23 08:04:42 - INFO - root -   Epoch: [216/300][0/283], lr: 0.00000048 	 loss = 0.0551(0.0551)
2023/11/23 08:04:51 - INFO - root -   Epoch: [216/300][20/283], lr: 0.00000048 	 loss = 1.0240(0.4566)
2023/11/23 08:05:08 - INFO - root -   Epoch: [216/300][40/283], lr: 0.00000048 	 loss = 0.0217(0.4320)
2023/11/23 08:05:26 - INFO - root -   Epoch: [216/300][60/283], lr: 0.00000048 	 loss = 0.1867(0.4507)
2023/11/23 08:05:35 - INFO - root -   Epoch: [216/300][80/283], lr: 0.00000048 	 loss = 0.0395(0.4669)
2023/11/23 08:05:53 - INFO - root -   Epoch: [216/300][100/283], lr: 0.00000048 	 loss = 0.0413(0.4495)
2023/11/23 08:06:14 - INFO - root -   Epoch: [216/300][120/283], lr: 0.00000048 	 loss = 0.2295(0.4248)
2023/11/23 08:06:34 - INFO - root -   Epoch: [216/300][140/283], lr: 0.00000048 	 loss = 0.0945(0.4011)
2023/11/23 08:06:43 - INFO - root -   Epoch: [216/300][160/283], lr: 0.00000048 	 loss = 1.5339(0.3943)
2023/11/23 08:06:58 - INFO - root -   Epoch: [216/300][180/283], lr: 0.00000048 	 loss = 0.2031(0.4104)
2023/11/23 08:07:25 - INFO - root -   Epoch: [216/300][200/283], lr: 0.00000048 	 loss = 0.1139(0.4022)
2023/11/23 08:07:37 - INFO - root -   Epoch: [216/300][220/283], lr: 0.00000048 	 loss = 0.0082(0.3963)
2023/11/23 08:07:53 - INFO - root -   Epoch: [216/300][240/283], lr: 0.00000048 	 loss = 1.6871(0.4106)
2023/11/23 08:08:08 - INFO - root -   Epoch: [216/300][260/283], lr: 0.00000048 	 loss = 0.3325(0.4021)
2023/11/23 08:08:18 - INFO - root -   Epoch: [216/300][280/283], lr: 0.00000048 	 loss = 3.5050(0.3994)
2023/11/23 08:08:19 - INFO - root -   Epoch: [216/300] 	 loss = 0.3968
2023/11/23 08:08:19 - INFO - root -   train_accuracy = 0.8640
2023/11/23 08:08:27 - INFO - root -   Epoch: [217/300][0/283], lr: 0.00000048 	 loss = 0.1229(0.1229)
2023/11/23 08:08:40 - INFO - root -   Epoch: [217/300][20/283], lr: 0.00000048 	 loss = 0.1734(0.4259)
2023/11/23 08:08:56 - INFO - root -   Epoch: [217/300][40/283], lr: 0.00000048 	 loss = 0.0195(0.4866)
2023/11/23 08:09:16 - INFO - root -   Epoch: [217/300][60/283], lr: 0.00000048 	 loss = 0.0147(0.5237)
2023/11/23 08:09:24 - INFO - root -   Epoch: [217/300][80/283], lr: 0.00000048 	 loss = 0.0971(0.5026)
2023/11/23 08:09:49 - INFO - root -   Epoch: [217/300][100/283], lr: 0.00000048 	 loss = 0.3324(0.4820)
2023/11/23 08:10:05 - INFO - root -   Epoch: [217/300][120/283], lr: 0.00000048 	 loss = 0.0626(0.4423)
2023/11/23 08:10:20 - INFO - root -   Epoch: [217/300][140/283], lr: 0.00000048 	 loss = 0.0032(0.4125)
2023/11/23 08:10:42 - INFO - root -   Epoch: [217/300][160/283], lr: 0.00000048 	 loss = 0.1460(0.3964)
2023/11/23 08:11:01 - INFO - root -   Epoch: [217/300][180/283], lr: 0.00000048 	 loss = 0.1700(0.3990)
2023/11/23 08:11:11 - INFO - root -   Epoch: [217/300][200/283], lr: 0.00000048 	 loss = 0.1679(0.3885)
2023/11/23 08:11:25 - INFO - root -   Epoch: [217/300][220/283], lr: 0.00000048 	 loss = 0.0153(0.3666)
2023/11/23 08:11:38 - INFO - root -   Epoch: [217/300][240/283], lr: 0.00000048 	 loss = 0.0852(0.3585)
2023/11/23 08:11:54 - INFO - root -   Epoch: [217/300][260/283], lr: 0.00000048 	 loss = 2.3137(0.3545)
2023/11/23 08:12:06 - INFO - root -   Epoch: [217/300][280/283], lr: 0.00000048 	 loss = 2.2698(0.3551)
2023/11/23 08:12:07 - INFO - root -   Epoch: [217/300] 	 loss = 0.3531
2023/11/23 08:12:07 - INFO - root -   train_accuracy = 0.8781
2023/11/23 08:12:16 - INFO - root -   Epoch: [218/300][0/283], lr: 0.00000048 	 loss = 0.1924(0.1924)
2023/11/23 08:12:28 - INFO - root -   Epoch: [218/300][20/283], lr: 0.00000048 	 loss = 0.1078(0.2757)
2023/11/23 08:12:44 - INFO - root -   Epoch: [218/300][40/283], lr: 0.00000048 	 loss = 0.1157(0.4414)
2023/11/23 08:12:57 - INFO - root -   Epoch: [218/300][60/283], lr: 0.00000048 	 loss = 0.0123(0.4359)
2023/11/23 08:13:10 - INFO - root -   Epoch: [218/300][80/283], lr: 0.00000048 	 loss = 0.0321(0.4490)
2023/11/23 08:13:30 - INFO - root -   Epoch: [218/300][100/283], lr: 0.00000048 	 loss = 0.2306(0.4350)
2023/11/23 08:13:45 - INFO - root -   Epoch: [218/300][120/283], lr: 0.00000048 	 loss = 0.0133(0.4002)
2023/11/23 08:14:00 - INFO - root -   Epoch: [218/300][140/283], lr: 0.00000048 	 loss = 0.0127(0.3717)
2023/11/23 08:14:12 - INFO - root -   Epoch: [218/300][160/283], lr: 0.00000048 	 loss = 0.3018(0.3537)
2023/11/23 08:14:28 - INFO - root -   Epoch: [218/300][180/283], lr: 0.00000048 	 loss = 0.0682(0.3811)
2023/11/23 08:14:40 - INFO - root -   Epoch: [218/300][200/283], lr: 0.00000048 	 loss = 0.0292(0.3820)
2023/11/23 08:15:01 - INFO - root -   Epoch: [218/300][220/283], lr: 0.00000048 	 loss = 0.0287(0.3740)
2023/11/23 08:15:11 - INFO - root -   Epoch: [218/300][240/283], lr: 0.00000048 	 loss = 2.2788(0.3793)
2023/11/23 08:15:33 - INFO - root -   Epoch: [218/300][260/283], lr: 0.00000048 	 loss = 1.9858(0.3772)
2023/11/23 08:15:44 - INFO - root -   Epoch: [218/300][280/283], lr: 0.00000048 	 loss = 3.1276(0.3756)
2023/11/23 08:15:45 - INFO - root -   Epoch: [218/300] 	 loss = 0.3740
2023/11/23 08:15:45 - INFO - root -   train_accuracy = 0.8657
2023/11/23 08:15:53 - INFO - root -   Epoch: [219/300][0/283], lr: 0.00000048 	 loss = 0.2465(0.2465)
2023/11/23 08:16:07 - INFO - root -   Epoch: [219/300][20/283], lr: 0.00000048 	 loss = 0.1963(0.3752)
2023/11/23 08:16:31 - INFO - root -   Epoch: [219/300][40/283], lr: 0.00000048 	 loss = 0.4840(0.6469)
2023/11/23 08:16:48 - INFO - root -   Epoch: [219/300][60/283], lr: 0.00000048 	 loss = 0.0082(0.5879)
2023/11/23 08:17:01 - INFO - root -   Epoch: [219/300][80/283], lr: 0.00000048 	 loss = 0.3849(0.6000)
2023/11/23 08:17:11 - INFO - root -   Epoch: [219/300][100/283], lr: 0.00000048 	 loss = 0.0506(0.5340)
2023/11/23 08:17:28 - INFO - root -   Epoch: [219/300][120/283], lr: 0.00000048 	 loss = 0.4061(0.4789)
2023/11/23 08:17:41 - INFO - root -   Epoch: [219/300][140/283], lr: 0.00000048 	 loss = 0.2023(0.4427)
2023/11/23 08:17:54 - INFO - root -   Epoch: [219/300][160/283], lr: 0.00000048 	 loss = 1.5968(0.4308)
2023/11/23 08:18:12 - INFO - root -   Epoch: [219/300][180/283], lr: 0.00000048 	 loss = 0.1041(0.4291)
2023/11/23 08:18:23 - INFO - root -   Epoch: [219/300][200/283], lr: 0.00000048 	 loss = 0.3577(0.4141)
2023/11/23 08:18:39 - INFO - root -   Epoch: [219/300][220/283], lr: 0.00000048 	 loss = 0.3238(0.3933)
2023/11/23 08:18:53 - INFO - root -   Epoch: [219/300][240/283], lr: 0.00000048 	 loss = 0.2758(0.3868)
2023/11/23 08:19:01 - INFO - root -   Epoch: [219/300][260/283], lr: 0.00000048 	 loss = 0.9450(0.3744)
2023/11/23 08:19:18 - INFO - root -   Epoch: [219/300][280/283], lr: 0.00000048 	 loss = 2.8600(0.3695)
2023/11/23 08:19:19 - INFO - root -   Epoch: [219/300] 	 loss = 0.3673
2023/11/23 08:22:13 - INFO - root -   precision = 0.7832
2023/11/23 08:22:13 - INFO - root -   eval_loss = 0.5523
2023/11/23 08:22:13 - INFO - root -   eval_acc = 0.7832
2023/11/23 08:22:14 - INFO - root -   train_accuracy = 0.8569
2023/11/23 08:22:15 - INFO - root -   Epoch: [220/300][0/283], lr: 0.00000048 	 loss = 0.0371(0.0371)
2023/11/23 08:22:38 - INFO - root -   Epoch: [220/300][20/283], lr: 0.00000048 	 loss = 1.8547(0.6482)
2023/11/23 08:22:53 - INFO - root -   Epoch: [220/300][40/283], lr: 0.00000048 	 loss = 0.4809(0.5604)
2023/11/23 08:23:06 - INFO - root -   Epoch: [220/300][60/283], lr: 0.00000048 	 loss = 0.0114(0.5202)
2023/11/23 08:23:22 - INFO - root -   Epoch: [220/300][80/283], lr: 0.00000048 	 loss = 0.1740(0.5568)
2023/11/23 08:23:39 - INFO - root -   Epoch: [220/300][100/283], lr: 0.00000048 	 loss = 0.1837(0.5318)
2023/11/23 08:23:59 - INFO - root -   Epoch: [220/300][120/283], lr: 0.00000048 	 loss = 0.2433(0.5022)
2023/11/23 08:24:17 - INFO - root -   Epoch: [220/300][140/283], lr: 0.00000048 	 loss = 0.0849(0.4598)
2023/11/23 08:24:38 - INFO - root -   Epoch: [220/300][160/283], lr: 0.00000048 	 loss = 0.7442(0.4570)
2023/11/23 08:24:48 - INFO - root -   Epoch: [220/300][180/283], lr: 0.00000048 	 loss = 0.0874(0.4326)
2023/11/23 08:25:10 - INFO - root -   Epoch: [220/300][200/283], lr: 0.00000048 	 loss = 0.0167(0.4312)
2023/11/23 08:25:18 - INFO - root -   Epoch: [220/300][220/283], lr: 0.00000048 	 loss = 0.0290(0.4127)
2023/11/23 08:25:31 - INFO - root -   Epoch: [220/300][240/283], lr: 0.00000048 	 loss = 0.0812(0.4033)
2023/11/23 08:25:46 - INFO - root -   Epoch: [220/300][260/283], lr: 0.00000048 	 loss = 1.8339(0.3973)
2023/11/23 08:25:59 - INFO - root -   Epoch: [220/300][280/283], lr: 0.00000048 	 loss = 1.7707(0.3924)
2023/11/23 08:26:00 - INFO - root -   Epoch: [220/300] 	 loss = 0.3898
2023/11/23 08:26:00 - INFO - root -   train_accuracy = 0.8657
2023/11/23 08:26:01 - INFO - root -   Epoch: [221/300][0/283], lr: 0.00000049 	 loss = 0.0542(0.0542)
2023/11/23 08:26:24 - INFO - root -   Epoch: [221/300][20/283], lr: 0.00000049 	 loss = 1.5443(0.6427)
2023/11/23 08:26:36 - INFO - root -   Epoch: [221/300][40/283], lr: 0.00000049 	 loss = 0.1677(0.6679)
2023/11/23 08:26:50 - INFO - root -   Epoch: [221/300][60/283], lr: 0.00000049 	 loss = 0.0066(0.5850)
2023/11/23 08:27:04 - INFO - root -   Epoch: [221/300][80/283], lr: 0.00000049 	 loss = 0.0411(0.5471)
2023/11/23 08:27:26 - INFO - root -   Epoch: [221/300][100/283], lr: 0.00000049 	 loss = 0.3025(0.5336)
2023/11/23 08:27:40 - INFO - root -   Epoch: [221/300][120/283], lr: 0.00000049 	 loss = 0.3153(0.5000)
2023/11/23 08:27:55 - INFO - root -   Epoch: [221/300][140/283], lr: 0.00000049 	 loss = 0.0094(0.4435)
2023/11/23 08:28:07 - INFO - root -   Epoch: [221/300][160/283], lr: 0.00000049 	 loss = 0.4184(0.4219)
2023/11/23 08:28:27 - INFO - root -   Epoch: [221/300][180/283], lr: 0.00000049 	 loss = 0.0153(0.4328)
2023/11/23 08:28:49 - INFO - root -   Epoch: [221/300][200/283], lr: 0.00000049 	 loss = 0.0140(0.4190)
2023/11/23 08:29:02 - INFO - root -   Epoch: [221/300][220/283], lr: 0.00000049 	 loss = 0.0082(0.4040)
2023/11/23 08:29:25 - INFO - root -   Epoch: [221/300][240/283], lr: 0.00000049 	 loss = 2.8812(0.4033)
2023/11/23 08:29:42 - INFO - root -   Epoch: [221/300][260/283], lr: 0.00000049 	 loss = 0.4114(0.4041)
2023/11/23 08:29:51 - INFO - root -   Epoch: [221/300][280/283], lr: 0.00000049 	 loss = 2.1705(0.3950)
2023/11/23 08:29:52 - INFO - root -   Epoch: [221/300] 	 loss = 0.3928
2023/11/23 08:29:52 - INFO - root -   train_accuracy = 0.8640
2023/11/23 08:30:00 - INFO - root -   Epoch: [222/300][0/283], lr: 0.00000049 	 loss = 0.1017(0.1017)
2023/11/23 08:30:18 - INFO - root -   Epoch: [222/300][20/283], lr: 0.00000049 	 loss = 0.9208(0.4510)
2023/11/23 08:30:29 - INFO - root -   Epoch: [222/300][40/283], lr: 0.00000049 	 loss = 0.0220(0.4137)
2023/11/23 08:30:38 - INFO - root -   Epoch: [222/300][60/283], lr: 0.00000049 	 loss = 0.1023(0.3661)
2023/11/23 08:30:49 - INFO - root -   Epoch: [222/300][80/283], lr: 0.00000049 	 loss = 0.1921(0.3732)
2023/11/23 08:30:57 - INFO - root -   Epoch: [222/300][100/283], lr: 0.00000049 	 loss = 0.0965(0.3424)
2023/11/23 08:31:16 - INFO - root -   Epoch: [222/300][120/283], lr: 0.00000049 	 loss = 0.0143(0.3272)
2023/11/23 08:31:32 - INFO - root -   Epoch: [222/300][140/283], lr: 0.00000049 	 loss = 0.0104(0.3066)
2023/11/23 08:31:44 - INFO - root -   Epoch: [222/300][160/283], lr: 0.00000049 	 loss = 0.6195(0.3113)
2023/11/23 08:31:53 - INFO - root -   Epoch: [222/300][180/283], lr: 0.00000049 	 loss = 0.1421(0.3354)
2023/11/23 08:32:05 - INFO - root -   Epoch: [222/300][200/283], lr: 0.00000049 	 loss = 0.0041(0.3561)
2023/11/23 08:32:20 - INFO - root -   Epoch: [222/300][220/283], lr: 0.00000049 	 loss = 0.0059(0.3617)
2023/11/23 08:32:32 - INFO - root -   Epoch: [222/300][240/283], lr: 0.00000049 	 loss = 0.9770(0.3571)
2023/11/23 08:32:45 - INFO - root -   Epoch: [222/300][260/283], lr: 0.00000049 	 loss = 2.0947(0.3512)
2023/11/23 08:32:53 - INFO - root -   Epoch: [222/300][280/283], lr: 0.00000049 	 loss = 3.1113(0.3528)
2023/11/23 08:32:54 - INFO - root -   Epoch: [222/300] 	 loss = 0.3505
2023/11/23 08:32:54 - INFO - root -   train_accuracy = 0.8763
2023/11/23 08:32:56 - INFO - root -   Epoch: [223/300][0/283], lr: 0.00000049 	 loss = 0.2367(0.2367)
2023/11/23 08:33:09 - INFO - root -   Epoch: [223/300][20/283], lr: 0.00000049 	 loss = 0.1333(0.3213)
2023/11/23 08:33:33 - INFO - root -   Epoch: [223/300][40/283], lr: 0.00000049 	 loss = 0.0114(0.4479)
2023/11/23 08:33:45 - INFO - root -   Epoch: [223/300][60/283], lr: 0.00000049 	 loss = 0.0462(0.4576)
2023/11/23 08:34:03 - INFO - root -   Epoch: [223/300][80/283], lr: 0.00000049 	 loss = 0.1267(0.4411)
2023/11/23 08:34:14 - INFO - root -   Epoch: [223/300][100/283], lr: 0.00000049 	 loss = 0.2441(0.4118)
2023/11/23 08:34:33 - INFO - root -   Epoch: [223/300][120/283], lr: 0.00000049 	 loss = 0.0156(0.3730)
2023/11/23 08:34:46 - INFO - root -   Epoch: [223/300][140/283], lr: 0.00000049 	 loss = 0.2253(0.3532)
2023/11/23 08:35:02 - INFO - root -   Epoch: [223/300][160/283], lr: 0.00000049 	 loss = 1.7670(0.3586)
2023/11/23 08:35:12 - INFO - root -   Epoch: [223/300][180/283], lr: 0.00000049 	 loss = 0.1291(0.3723)
2023/11/23 08:35:35 - INFO - root -   Epoch: [223/300][200/283], lr: 0.00000049 	 loss = 0.0163(0.3690)
2023/11/23 08:35:55 - INFO - root -   Epoch: [223/300][220/283], lr: 0.00000049 	 loss = 0.0133(0.3586)
2023/11/23 08:36:07 - INFO - root -   Epoch: [223/300][240/283], lr: 0.00000049 	 loss = 0.3512(0.3644)
2023/11/23 08:36:21 - INFO - root -   Epoch: [223/300][260/283], lr: 0.00000049 	 loss = 0.2282(0.3489)
2023/11/23 08:36:35 - INFO - root -   Epoch: [223/300][280/283], lr: 0.00000049 	 loss = 2.1680(0.3385)
2023/11/23 08:36:36 - INFO - root -   Epoch: [223/300] 	 loss = 0.3387
2023/11/23 08:36:36 - INFO - root -   train_accuracy = 0.8781
2023/11/23 08:36:38 - INFO - root -   Epoch: [224/300][0/283], lr: 0.00000049 	 loss = 0.0283(0.0283)
2023/11/23 08:36:58 - INFO - root -   Epoch: [224/300][20/283], lr: 0.00000049 	 loss = 0.7791(0.3893)
2023/11/23 08:37:10 - INFO - root -   Epoch: [224/300][40/283], lr: 0.00000049 	 loss = 0.1489(0.5000)
2023/11/23 08:37:35 - INFO - root -   Epoch: [224/300][60/283], lr: 0.00000049 	 loss = 0.2179(0.5239)
2023/11/23 08:37:49 - INFO - root -   Epoch: [224/300][80/283], lr: 0.00000049 	 loss = 0.1913(0.5355)
2023/11/23 08:38:01 - INFO - root -   Epoch: [224/300][100/283], lr: 0.00000049 	 loss = 0.0971(0.5270)
2023/11/23 08:38:19 - INFO - root -   Epoch: [224/300][120/283], lr: 0.00000049 	 loss = 0.1068(0.4864)
2023/11/23 08:38:34 - INFO - root -   Epoch: [224/300][140/283], lr: 0.00000049 	 loss = 0.0676(0.4405)
2023/11/23 08:38:46 - INFO - root -   Epoch: [224/300][160/283], lr: 0.00000049 	 loss = 0.8863(0.4334)
2023/11/23 08:38:57 - INFO - root -   Epoch: [224/300][180/283], lr: 0.00000049 	 loss = 0.2968(0.4368)
2023/11/23 08:39:14 - INFO - root -   Epoch: [224/300][200/283], lr: 0.00000049 	 loss = 0.0212(0.4494)
2023/11/23 08:39:37 - INFO - root -   Epoch: [224/300][220/283], lr: 0.00000049 	 loss = 0.1709(0.4413)
2023/11/23 08:39:59 - INFO - root -   Epoch: [224/300][240/283], lr: 0.00000049 	 loss = 0.1766(0.4409)
2023/11/23 08:40:19 - INFO - root -   Epoch: [224/300][260/283], lr: 0.00000049 	 loss = 0.7171(0.4287)
2023/11/23 08:40:29 - INFO - root -   Epoch: [224/300][280/283], lr: 0.00000049 	 loss = 3.8836(0.4305)
2023/11/23 08:40:30 - INFO - root -   Epoch: [224/300] 	 loss = 0.4282
2023/11/23 08:43:23 - INFO - root -   precision = 0.8462
2023/11/23 08:43:23 - INFO - root -   eval_loss = 0.5501
2023/11/23 08:43:23 - INFO - root -   eval_acc = 0.8462
2023/11/23 08:43:24 - INFO - root -   train_accuracy = 0.8481
2023/11/23 08:43:26 - INFO - root -   Epoch: [225/300][0/283], lr: 0.00000049 	 loss = 0.0727(0.0727)
2023/11/23 08:43:50 - INFO - root -   Epoch: [225/300][20/283], lr: 0.00000049 	 loss = 0.2087(0.4847)
2023/11/23 08:43:59 - INFO - root -   Epoch: [225/300][40/283], lr: 0.00000049 	 loss = 0.0098(0.4166)
2023/11/23 08:44:20 - INFO - root -   Epoch: [225/300][60/283], lr: 0.00000049 	 loss = 0.1275(0.4505)
2023/11/23 08:44:30 - INFO - root -   Epoch: [225/300][80/283], lr: 0.00000049 	 loss = 0.1265(0.4550)
2023/11/23 08:44:48 - INFO - root -   Epoch: [225/300][100/283], lr: 0.00000049 	 loss = 0.1798(0.4070)
2023/11/23 08:45:05 - INFO - root -   Epoch: [225/300][120/283], lr: 0.00000049 	 loss = 0.1332(0.3735)
2023/11/23 08:45:16 - INFO - root -   Epoch: [225/300][140/283], lr: 0.00000049 	 loss = 0.0035(0.3473)
2023/11/23 08:45:30 - INFO - root -   Epoch: [225/300][160/283], lr: 0.00000049 	 loss = 1.7609(0.3598)
2023/11/23 08:45:38 - INFO - root -   Epoch: [225/300][180/283], lr: 0.00000049 	 loss = 0.1636(0.3591)
2023/11/23 08:46:04 - INFO - root -   Epoch: [225/300][200/283], lr: 0.00000049 	 loss = 0.0997(0.3652)
2023/11/23 08:46:20 - INFO - root -   Epoch: [225/300][220/283], lr: 0.00000049 	 loss = 0.0934(0.3612)
2023/11/23 08:46:34 - INFO - root -   Epoch: [225/300][240/283], lr: 0.00000049 	 loss = 0.5806(0.3562)
2023/11/23 08:46:48 - INFO - root -   Epoch: [225/300][260/283], lr: 0.00000049 	 loss = 1.4687(0.3496)
2023/11/23 08:47:00 - INFO - root -   Epoch: [225/300][280/283], lr: 0.00000049 	 loss = 2.2918(0.3481)
2023/11/23 08:47:00 - INFO - root -   Epoch: [225/300] 	 loss = 0.3457
2023/11/23 08:47:00 - INFO - root -   train_accuracy = 0.8763
2023/11/23 08:47:15 - INFO - root -   Epoch: [226/300][0/283], lr: 0.00000050 	 loss = 0.2373(0.2373)
2023/11/23 08:47:23 - INFO - root -   Epoch: [226/300][20/283], lr: 0.00000050 	 loss = 0.3671(0.4218)
2023/11/23 08:47:37 - INFO - root -   Epoch: [226/300][40/283], lr: 0.00000050 	 loss = 0.1169(0.4861)
2023/11/23 08:47:55 - INFO - root -   Epoch: [226/300][60/283], lr: 0.00000050 	 loss = 0.2147(0.4758)
2023/11/23 08:48:03 - INFO - root -   Epoch: [226/300][80/283], lr: 0.00000050 	 loss = 0.5124(0.4448)
2023/11/23 08:48:23 - INFO - root -   Epoch: [226/300][100/283], lr: 0.00000050 	 loss = 0.0678(0.4247)
2023/11/23 08:48:44 - INFO - root -   Epoch: [226/300][120/283], lr: 0.00000050 	 loss = 0.0717(0.3955)
2023/11/23 08:48:54 - INFO - root -   Epoch: [226/300][140/283], lr: 0.00000050 	 loss = 0.0956(0.3739)
2023/11/23 08:49:10 - INFO - root -   Epoch: [226/300][160/283], lr: 0.00000050 	 loss = 0.8189(0.3773)
2023/11/23 08:49:31 - INFO - root -   Epoch: [226/300][180/283], lr: 0.00000050 	 loss = 0.1808(0.3833)
2023/11/23 08:49:52 - INFO - root -   Epoch: [226/300][200/283], lr: 0.00000050 	 loss = 0.0146(0.3817)
2023/11/23 08:50:00 - INFO - root -   Epoch: [226/300][220/283], lr: 0.00000050 	 loss = 0.0202(0.3705)
2023/11/23 08:50:25 - INFO - root -   Epoch: [226/300][240/283], lr: 0.00000050 	 loss = 0.3866(0.3839)
2023/11/23 08:50:42 - INFO - root -   Epoch: [226/300][260/283], lr: 0.00000050 	 loss = 0.5610(0.3820)
2023/11/23 08:50:56 - INFO - root -   Epoch: [226/300][280/283], lr: 0.00000050 	 loss = 2.9154(0.3814)
2023/11/23 08:50:56 - INFO - root -   Epoch: [226/300] 	 loss = 0.3788
2023/11/23 08:50:56 - INFO - root -   train_accuracy = 0.8728
2023/11/23 08:50:58 - INFO - root -   Epoch: [227/300][0/283], lr: 0.00000050 	 loss = 0.0324(0.0324)
2023/11/23 08:51:23 - INFO - root -   Epoch: [227/300][20/283], lr: 0.00000050 	 loss = 1.9548(0.5212)
2023/11/23 08:51:38 - INFO - root -   Epoch: [227/300][40/283], lr: 0.00000050 	 loss = 0.2223(0.4869)
2023/11/23 08:51:53 - INFO - root -   Epoch: [227/300][60/283], lr: 0.00000050 	 loss = 0.1693(0.4352)
2023/11/23 08:52:07 - INFO - root -   Epoch: [227/300][80/283], lr: 0.00000050 	 loss = 0.0833(0.4639)
2023/11/23 08:52:24 - INFO - root -   Epoch: [227/300][100/283], lr: 0.00000050 	 loss = 0.0522(0.4281)
2023/11/23 08:52:46 - INFO - root -   Epoch: [227/300][120/283], lr: 0.00000050 	 loss = 0.0189(0.4252)
2023/11/23 08:52:58 - INFO - root -   Epoch: [227/300][140/283], lr: 0.00000050 	 loss = 0.0244(0.3887)
2023/11/23 08:53:11 - INFO - root -   Epoch: [227/300][160/283], lr: 0.00000050 	 loss = 0.9473(0.3889)
2023/11/23 08:53:28 - INFO - root -   Epoch: [227/300][180/283], lr: 0.00000050 	 loss = 0.1366(0.3941)
2023/11/23 08:53:41 - INFO - root -   Epoch: [227/300][200/283], lr: 0.00000050 	 loss = 0.0087(0.3802)
2023/11/23 08:54:00 - INFO - root -   Epoch: [227/300][220/283], lr: 0.00000050 	 loss = 0.4306(0.3661)
2023/11/23 08:54:10 - INFO - root -   Epoch: [227/300][240/283], lr: 0.00000050 	 loss = 0.0209(0.3588)
2023/11/23 08:54:27 - INFO - root -   Epoch: [227/300][260/283], lr: 0.00000050 	 loss = 1.3433(0.3546)
2023/11/23 08:54:41 - INFO - root -   Epoch: [227/300][280/283], lr: 0.00000050 	 loss = 2.2869(0.3501)
2023/11/23 08:54:42 - INFO - root -   Epoch: [227/300] 	 loss = 0.3480
2023/11/23 08:54:42 - INFO - root -   train_accuracy = 0.8498
2023/11/23 08:54:49 - INFO - root -   Epoch: [228/300][0/283], lr: 0.00000050 	 loss = 0.3147(0.3147)
2023/11/23 08:55:01 - INFO - root -   Epoch: [228/300][20/283], lr: 0.00000050 	 loss = 1.8245(0.5827)
2023/11/23 08:55:23 - INFO - root -   Epoch: [228/300][40/283], lr: 0.00000050 	 loss = 0.0892(0.5713)
2023/11/23 08:55:35 - INFO - root -   Epoch: [228/300][60/283], lr: 0.00000050 	 loss = 0.1603(0.5337)
2023/11/23 08:55:52 - INFO - root -   Epoch: [228/300][80/283], lr: 0.00000050 	 loss = 0.0893(0.5071)
2023/11/23 08:56:07 - INFO - root -   Epoch: [228/300][100/283], lr: 0.00000050 	 loss = 0.3529(0.4723)
2023/11/23 08:56:31 - INFO - root -   Epoch: [228/300][120/283], lr: 0.00000050 	 loss = 0.3869(0.4460)
2023/11/23 08:56:43 - INFO - root -   Epoch: [228/300][140/283], lr: 0.00000050 	 loss = 0.0493(0.4138)
2023/11/23 08:57:09 - INFO - root -   Epoch: [228/300][160/283], lr: 0.00000050 	 loss = 1.1810(0.4243)
2023/11/23 08:57:20 - INFO - root -   Epoch: [228/300][180/283], lr: 0.00000050 	 loss = 0.0434(0.4148)
2023/11/23 08:57:42 - INFO - root -   Epoch: [228/300][200/283], lr: 0.00000050 	 loss = 0.0176(0.4117)
2023/11/23 08:57:56 - INFO - root -   Epoch: [228/300][220/283], lr: 0.00000050 	 loss = 0.0105(0.3943)
2023/11/23 08:58:11 - INFO - root -   Epoch: [228/300][240/283], lr: 0.00000050 	 loss = 0.3051(0.3870)
2023/11/23 08:58:32 - INFO - root -   Epoch: [228/300][260/283], lr: 0.00000050 	 loss = 0.4264(0.3805)
2023/11/23 08:58:42 - INFO - root -   Epoch: [228/300][280/283], lr: 0.00000050 	 loss = 1.9430(0.3756)
2023/11/23 08:58:43 - INFO - root -   Epoch: [228/300] 	 loss = 0.3738
2023/11/23 08:58:43 - INFO - root -   train_accuracy = 0.8728
2023/11/23 08:58:45 - INFO - root -   Epoch: [229/300][0/283], lr: 0.00000050 	 loss = 0.2144(0.2144)
2023/11/23 08:58:59 - INFO - root -   Epoch: [229/300][20/283], lr: 0.00000050 	 loss = 1.9924(0.3877)
2023/11/23 08:59:16 - INFO - root -   Epoch: [229/300][40/283], lr: 0.00000050 	 loss = 0.0466(0.5222)
2023/11/23 08:59:26 - INFO - root -   Epoch: [229/300][60/283], lr: 0.00000050 	 loss = 0.0075(0.5006)
2023/11/23 08:59:43 - INFO - root -   Epoch: [229/300][80/283], lr: 0.00000050 	 loss = 0.1368(0.4848)
2023/11/23 08:59:57 - INFO - root -   Epoch: [229/300][100/283], lr: 0.00000050 	 loss = 0.5039(0.4321)
2023/11/23 09:00:10 - INFO - root -   Epoch: [229/300][120/283], lr: 0.00000050 	 loss = 0.0967(0.4085)
2023/11/23 09:00:29 - INFO - root -   Epoch: [229/300][140/283], lr: 0.00000050 	 loss = 0.1029(0.3720)
2023/11/23 09:00:38 - INFO - root -   Epoch: [229/300][160/283], lr: 0.00000050 	 loss = 0.2734(0.3498)
2023/11/23 09:00:58 - INFO - root -   Epoch: [229/300][180/283], lr: 0.00000050 	 loss = 0.1172(0.3586)
2023/11/23 09:01:12 - INFO - root -   Epoch: [229/300][200/283], lr: 0.00000050 	 loss = 0.0168(0.3516)
2023/11/23 09:01:24 - INFO - root -   Epoch: [229/300][220/283], lr: 0.00000050 	 loss = 0.0098(0.3428)
2023/11/23 09:01:33 - INFO - root -   Epoch: [229/300][240/283], lr: 0.00000050 	 loss = 0.0538(0.3389)
2023/11/23 09:01:46 - INFO - root -   Epoch: [229/300][260/283], lr: 0.00000050 	 loss = 0.8069(0.3304)
2023/11/23 09:02:00 - INFO - root -   Epoch: [229/300][280/283], lr: 0.00000050 	 loss = 1.2713(0.3200)
2023/11/23 09:02:01 - INFO - root -   Epoch: [229/300] 	 loss = 0.3184
2023/11/23 09:04:54 - INFO - root -   precision = 0.7902
2023/11/23 09:04:54 - INFO - root -   eval_loss = 0.7335
2023/11/23 09:04:54 - INFO - root -   eval_acc = 0.7902
2023/11/23 09:04:55 - INFO - root -   train_accuracy = 0.8922
2023/11/23 09:05:02 - INFO - root -   Epoch: [230/300][0/283], lr: 0.00000050 	 loss = 0.0410(0.0410)
2023/11/23 09:05:20 - INFO - root -   Epoch: [230/300][20/283], lr: 0.00000050 	 loss = 0.8797(0.6543)
2023/11/23 09:05:32 - INFO - root -   Epoch: [230/300][40/283], lr: 0.00000050 	 loss = 0.0413(0.5153)
2023/11/23 09:05:48 - INFO - root -   Epoch: [230/300][60/283], lr: 0.00000050 	 loss = 0.3325(0.5309)
2023/11/23 09:06:02 - INFO - root -   Epoch: [230/300][80/283], lr: 0.00000050 	 loss = 0.2064(0.5378)
2023/11/23 09:06:15 - INFO - root -   Epoch: [230/300][100/283], lr: 0.00000050 	 loss = 0.2189(0.5051)
2023/11/23 09:06:23 - INFO - root -   Epoch: [230/300][120/283], lr: 0.00000050 	 loss = 0.0133(0.4454)
2023/11/23 09:06:44 - INFO - root -   Epoch: [230/300][140/283], lr: 0.00000050 	 loss = 0.0056(0.4058)
2023/11/23 09:06:55 - INFO - root -   Epoch: [230/300][160/283], lr: 0.00000050 	 loss = 1.6681(0.4070)
2023/11/23 09:07:12 - INFO - root -   Epoch: [230/300][180/283], lr: 0.00000050 	 loss = 0.2058(0.4172)
2023/11/23 09:07:23 - INFO - root -   Epoch: [230/300][200/283], lr: 0.00000050 	 loss = 0.0485(0.4048)
2023/11/23 09:07:35 - INFO - root -   Epoch: [230/300][220/283], lr: 0.00000050 	 loss = 0.0175(0.3868)
2023/11/23 09:07:49 - INFO - root -   Epoch: [230/300][240/283], lr: 0.00000050 	 loss = 0.1669(0.3851)
2023/11/23 09:08:00 - INFO - root -   Epoch: [230/300][260/283], lr: 0.00000050 	 loss = 0.1450(0.3757)
2023/11/23 09:08:14 - INFO - root -   Epoch: [230/300][280/283], lr: 0.00000050 	 loss = 2.4622(0.3875)
2023/11/23 09:08:15 - INFO - root -   Epoch: [230/300] 	 loss = 0.3849
2023/11/23 09:08:15 - INFO - root -   train_accuracy = 0.8781
2023/11/23 09:08:22 - INFO - root -   Epoch: [231/300][0/283], lr: 0.00000050 	 loss = 0.1104(0.1104)
2023/11/23 09:08:31 - INFO - root -   Epoch: [231/300][20/283], lr: 0.00000050 	 loss = 0.4067(0.4017)
2023/11/23 09:09:03 - INFO - root -   Epoch: [231/300][40/283], lr: 0.00000050 	 loss = 0.2439(0.5456)
2023/11/23 09:09:13 - INFO - root -   Epoch: [231/300][60/283], lr: 0.00000050 	 loss = 0.0453(0.5027)
2023/11/23 09:09:26 - INFO - root -   Epoch: [231/300][80/283], lr: 0.00000050 	 loss = 0.1047(0.4865)
2023/11/23 09:09:44 - INFO - root -   Epoch: [231/300][100/283], lr: 0.00000050 	 loss = 1.2439(0.4562)
2023/11/23 09:10:00 - INFO - root -   Epoch: [231/300][120/283], lr: 0.00000050 	 loss = 0.0170(0.4123)
2023/11/23 09:10:10 - INFO - root -   Epoch: [231/300][140/283], lr: 0.00000050 	 loss = 0.0023(0.3892)
2023/11/23 09:10:25 - INFO - root -   Epoch: [231/300][160/283], lr: 0.00000050 	 loss = 0.5937(0.3848)
2023/11/23 09:10:40 - INFO - root -   Epoch: [231/300][180/283], lr: 0.00000050 	 loss = 0.0622(0.3825)
2023/11/23 09:10:53 - INFO - root -   Epoch: [231/300][200/283], lr: 0.00000050 	 loss = 0.0070(0.3834)
2023/11/23 09:11:16 - INFO - root -   Epoch: [231/300][220/283], lr: 0.00000050 	 loss = 0.2320(0.3690)
2023/11/23 09:11:27 - INFO - root -   Epoch: [231/300][240/283], lr: 0.00000050 	 loss = 0.5828(0.3705)
2023/11/23 09:11:40 - INFO - root -   Epoch: [231/300][260/283], lr: 0.00000050 	 loss = 0.1054(0.3567)
2023/11/23 09:11:51 - INFO - root -   Epoch: [231/300][280/283], lr: 0.00000050 	 loss = 1.3514(0.3409)
2023/11/23 09:11:52 - INFO - root -   Epoch: [231/300] 	 loss = 0.3388
2023/11/23 09:11:52 - INFO - root -   train_accuracy = 0.8693
2023/11/23 09:11:59 - INFO - root -   Epoch: [232/300][0/283], lr: 0.00000051 	 loss = 0.0985(0.0985)
2023/11/23 09:12:16 - INFO - root -   Epoch: [232/300][20/283], lr: 0.00000051 	 loss = 0.0634(0.4270)
2023/11/23 09:12:36 - INFO - root -   Epoch: [232/300][40/283], lr: 0.00000051 	 loss = 0.2082(0.5607)
2023/11/23 09:12:59 - INFO - root -   Epoch: [232/300][60/283], lr: 0.00000051 	 loss = 0.2017(0.4859)
2023/11/23 09:13:15 - INFO - root -   Epoch: [232/300][80/283], lr: 0.00000051 	 loss = 0.0166(0.5301)
2023/11/23 09:13:40 - INFO - root -   Epoch: [232/300][100/283], lr: 0.00000051 	 loss = 0.0818(0.4977)
2023/11/23 09:13:54 - INFO - root -   Epoch: [232/300][120/283], lr: 0.00000051 	 loss = 0.3170(0.4641)
2023/11/23 09:14:11 - INFO - root -   Epoch: [232/300][140/283], lr: 0.00000051 	 loss = 0.0066(0.4268)
2023/11/23 09:14:37 - INFO - root -   Epoch: [232/300][160/283], lr: 0.00000051 	 loss = 0.7459(0.4114)
2023/11/23 09:14:51 - INFO - root -   Epoch: [232/300][180/283], lr: 0.00000051 	 loss = 0.0261(0.4050)
2023/11/23 09:15:06 - INFO - root -   Epoch: [232/300][200/283], lr: 0.00000051 	 loss = 0.2585(0.4045)
2023/11/23 09:15:19 - INFO - root -   Epoch: [232/300][220/283], lr: 0.00000051 	 loss = 0.1171(0.3859)
2023/11/23 09:15:43 - INFO - root -   Epoch: [232/300][240/283], lr: 0.00000051 	 loss = 0.0899(0.3819)
2023/11/23 09:15:57 - INFO - root -   Epoch: [232/300][260/283], lr: 0.00000051 	 loss = 0.1483(0.3752)
2023/11/23 09:16:10 - INFO - root -   Epoch: [232/300][280/283], lr: 0.00000051 	 loss = 2.4196(0.3613)
2023/11/23 09:16:10 - INFO - root -   Epoch: [232/300] 	 loss = 0.3588
2023/11/23 09:16:10 - INFO - root -   train_accuracy = 0.8816
2023/11/23 09:16:12 - INFO - root -   Epoch: [233/300][0/283], lr: 0.00000051 	 loss = 0.1055(0.1055)
2023/11/23 09:16:27 - INFO - root -   Epoch: [233/300][20/283], lr: 0.00000051 	 loss = 1.3768(0.6610)
2023/11/23 09:16:38 - INFO - root -   Epoch: [233/300][40/283], lr: 0.00000051 	 loss = 0.0128(0.5026)
2023/11/23 09:16:56 - INFO - root -   Epoch: [233/300][60/283], lr: 0.00000051 	 loss = 0.0774(0.4529)
2023/11/23 09:17:08 - INFO - root -   Epoch: [233/300][80/283], lr: 0.00000051 	 loss = 0.1491(0.4509)
2023/11/23 09:17:24 - INFO - root -   Epoch: [233/300][100/283], lr: 0.00000051 	 loss = 0.3458(0.4224)
2023/11/23 09:17:40 - INFO - root -   Epoch: [233/300][120/283], lr: 0.00000051 	 loss = 0.0395(0.3813)
2023/11/23 09:18:02 - INFO - root -   Epoch: [233/300][140/283], lr: 0.00000051 	 loss = 0.0695(0.3627)
2023/11/23 09:18:17 - INFO - root -   Epoch: [233/300][160/283], lr: 0.00000051 	 loss = 1.9766(0.3688)
2023/11/23 09:18:29 - INFO - root -   Epoch: [233/300][180/283], lr: 0.00000051 	 loss = 0.4533(0.3734)
2023/11/23 09:18:50 - INFO - root -   Epoch: [233/300][200/283], lr: 0.00000051 	 loss = 0.0109(0.3672)
2023/11/23 09:19:06 - INFO - root -   Epoch: [233/300][220/283], lr: 0.00000051 	 loss = 0.2742(0.3496)
2023/11/23 09:19:20 - INFO - root -   Epoch: [233/300][240/283], lr: 0.00000051 	 loss = 3.2991(0.3630)
2023/11/23 09:19:35 - INFO - root -   Epoch: [233/300][260/283], lr: 0.00000051 	 loss = 0.2786(0.3613)
2023/11/23 09:19:51 - INFO - root -   Epoch: [233/300][280/283], lr: 0.00000051 	 loss = 2.1101(0.3597)
2023/11/23 09:19:52 - INFO - root -   Epoch: [233/300] 	 loss = 0.3578
2023/11/23 09:19:52 - INFO - root -   train_accuracy = 0.8834
2023/11/23 09:19:59 - INFO - root -   Epoch: [234/300][0/283], lr: 0.00000051 	 loss = 0.1683(0.1683)
2023/11/23 09:20:17 - INFO - root -   Epoch: [234/300][20/283], lr: 0.00000051 	 loss = 0.0952(0.4102)
2023/11/23 09:20:25 - INFO - root -   Epoch: [234/300][40/283], lr: 0.00000051 	 loss = 0.1577(0.4173)
2023/11/23 09:20:38 - INFO - root -   Epoch: [234/300][60/283], lr: 0.00000051 	 loss = 0.0105(0.3604)
2023/11/23 09:20:51 - INFO - root -   Epoch: [234/300][80/283], lr: 0.00000051 	 loss = 0.0185(0.4232)
2023/11/23 09:21:08 - INFO - root -   Epoch: [234/300][100/283], lr: 0.00000051 	 loss = 0.0395(0.4182)
2023/11/23 09:21:22 - INFO - root -   Epoch: [234/300][120/283], lr: 0.00000051 	 loss = 0.0071(0.3844)
2023/11/23 09:21:33 - INFO - root -   Epoch: [234/300][140/283], lr: 0.00000051 	 loss = 0.0122(0.3579)
2023/11/23 09:21:53 - INFO - root -   Epoch: [234/300][160/283], lr: 0.00000051 	 loss = 0.0962(0.3600)
2023/11/23 09:22:07 - INFO - root -   Epoch: [234/300][180/283], lr: 0.00000051 	 loss = 0.0062(0.3471)
2023/11/23 09:22:24 - INFO - root -   Epoch: [234/300][200/283], lr: 0.00000051 	 loss = 0.1280(0.3459)
2023/11/23 09:22:39 - INFO - root -   Epoch: [234/300][220/283], lr: 0.00000051 	 loss = 0.1113(0.3466)
2023/11/23 09:22:52 - INFO - root -   Epoch: [234/300][240/283], lr: 0.00000051 	 loss = 0.0887(0.3427)
2023/11/23 09:23:07 - INFO - root -   Epoch: [234/300][260/283], lr: 0.00000051 	 loss = 0.5535(0.3349)
2023/11/23 09:23:21 - INFO - root -   Epoch: [234/300][280/283], lr: 0.00000051 	 loss = 1.9084(0.3343)
2023/11/23 09:23:22 - INFO - root -   Epoch: [234/300] 	 loss = 0.3321
2023/11/23 09:26:15 - INFO - root -   precision = 0.8042
2023/11/23 09:26:15 - INFO - root -   eval_loss = 0.6155
2023/11/23 09:26:15 - INFO - root -   eval_acc = 0.8042
2023/11/23 09:26:16 - INFO - root -   train_accuracy = 0.8834
2023/11/23 09:26:17 - INFO - root -   Epoch: [235/300][0/283], lr: 0.00000051 	 loss = 0.1814(0.1814)
2023/11/23 09:26:43 - INFO - root -   Epoch: [235/300][20/283], lr: 0.00000051 	 loss = 1.2982(0.4054)
2023/11/23 09:26:51 - INFO - root -   Epoch: [235/300][40/283], lr: 0.00000051 	 loss = 0.2963(0.5288)
2023/11/23 09:27:08 - INFO - root -   Epoch: [235/300][60/283], lr: 0.00000051 	 loss = 0.0023(0.4907)
2023/11/23 09:27:26 - INFO - root -   Epoch: [235/300][80/283], lr: 0.00000051 	 loss = 0.2750(0.4957)
2023/11/23 09:27:42 - INFO - root -   Epoch: [235/300][100/283], lr: 0.00000051 	 loss = 0.2936(0.4458)
2023/11/23 09:27:57 - INFO - root -   Epoch: [235/300][120/283], lr: 0.00000051 	 loss = 0.1967(0.4215)
2023/11/23 09:28:17 - INFO - root -   Epoch: [235/300][140/283], lr: 0.00000051 	 loss = 0.5389(0.3994)
2023/11/23 09:28:32 - INFO - root -   Epoch: [235/300][160/283], lr: 0.00000051 	 loss = 0.2156(0.3932)
2023/11/23 09:28:44 - INFO - root -   Epoch: [235/300][180/283], lr: 0.00000051 	 loss = 0.0085(0.4021)
2023/11/23 09:29:03 - INFO - root -   Epoch: [235/300][200/283], lr: 0.00000051 	 loss = 0.1061(0.3937)
2023/11/23 09:29:21 - INFO - root -   Epoch: [235/300][220/283], lr: 0.00000051 	 loss = 0.0933(0.3794)
2023/11/23 09:29:33 - INFO - root -   Epoch: [235/300][240/283], lr: 0.00000051 	 loss = 0.1168(0.3793)
2023/11/23 09:29:47 - INFO - root -   Epoch: [235/300][260/283], lr: 0.00000051 	 loss = 1.6641(0.3759)
2023/11/23 09:29:55 - INFO - root -   Epoch: [235/300][280/283], lr: 0.00000051 	 loss = 1.3346(0.3655)
2023/11/23 09:29:56 - INFO - root -   Epoch: [235/300] 	 loss = 0.3631
2023/11/23 09:29:56 - INFO - root -   train_accuracy = 0.8569
2023/11/23 09:30:04 - INFO - root -   Epoch: [236/300][0/283], lr: 0.00000051 	 loss = 0.1171(0.1171)
2023/11/23 09:30:16 - INFO - root -   Epoch: [236/300][20/283], lr: 0.00000051 	 loss = 0.0778(0.3029)
2023/11/23 09:30:27 - INFO - root -   Epoch: [236/300][40/283], lr: 0.00000051 	 loss = 0.0037(0.3423)
2023/11/23 09:30:47 - INFO - root -   Epoch: [236/300][60/283], lr: 0.00000051 	 loss = 0.1309(0.3745)
2023/11/23 09:31:08 - INFO - root -   Epoch: [236/300][80/283], lr: 0.00000051 	 loss = 0.5139(0.4124)
2023/11/23 09:31:16 - INFO - root -   Epoch: [236/300][100/283], lr: 0.00000051 	 loss = 0.0984(0.3915)
2023/11/23 09:31:42 - INFO - root -   Epoch: [236/300][120/283], lr: 0.00000051 	 loss = 0.1119(0.3668)
2023/11/23 09:32:03 - INFO - root -   Epoch: [236/300][140/283], lr: 0.00000051 	 loss = 0.2083(0.3561)
2023/11/23 09:32:21 - INFO - root -   Epoch: [236/300][160/283], lr: 0.00000051 	 loss = 0.8467(0.3583)
2023/11/23 09:32:35 - INFO - root -   Epoch: [236/300][180/283], lr: 0.00000051 	 loss = 0.1023(0.3789)
2023/11/23 09:32:47 - INFO - root -   Epoch: [236/300][200/283], lr: 0.00000051 	 loss = 0.2348(0.3681)
2023/11/23 09:32:59 - INFO - root -   Epoch: [236/300][220/283], lr: 0.00000051 	 loss = 0.0104(0.3635)
2023/11/23 09:33:18 - INFO - root -   Epoch: [236/300][240/283], lr: 0.00000051 	 loss = 0.5531(0.3620)
2023/11/23 09:33:34 - INFO - root -   Epoch: [236/300][260/283], lr: 0.00000051 	 loss = 1.3340(0.3476)
2023/11/23 09:33:44 - INFO - root -   Epoch: [236/300][280/283], lr: 0.00000051 	 loss = 1.9511(0.3423)
2023/11/23 09:33:45 - INFO - root -   Epoch: [236/300] 	 loss = 0.3400
2023/11/23 09:33:45 - INFO - root -   train_accuracy = 0.8852
2023/11/23 09:33:55 - INFO - root -   Epoch: [237/300][0/283], lr: 0.00000051 	 loss = 0.0568(0.0568)
2023/11/23 09:34:05 - INFO - root -   Epoch: [237/300][20/283], lr: 0.00000051 	 loss = 1.6649(0.4494)
2023/11/23 09:34:20 - INFO - root -   Epoch: [237/300][40/283], lr: 0.00000051 	 loss = 0.0385(0.5067)
2023/11/23 09:34:32 - INFO - root -   Epoch: [237/300][60/283], lr: 0.00000051 	 loss = 0.0295(0.4960)
2023/11/23 09:34:43 - INFO - root -   Epoch: [237/300][80/283], lr: 0.00000051 	 loss = 0.1168(0.4677)
2023/11/23 09:34:58 - INFO - root -   Epoch: [237/300][100/283], lr: 0.00000051 	 loss = 0.1092(0.4136)
2023/11/23 09:35:17 - INFO - root -   Epoch: [237/300][120/283], lr: 0.00000051 	 loss = 0.0268(0.3913)
2023/11/23 09:35:25 - INFO - root -   Epoch: [237/300][140/283], lr: 0.00000051 	 loss = 0.0009(0.3603)
2023/11/23 09:35:42 - INFO - root -   Epoch: [237/300][160/283], lr: 0.00000051 	 loss = 0.5787(0.3765)
2023/11/23 09:35:57 - INFO - root -   Epoch: [237/300][180/283], lr: 0.00000051 	 loss = 0.0978(0.3772)
2023/11/23 09:36:12 - INFO - root -   Epoch: [237/300][200/283], lr: 0.00000051 	 loss = 0.7290(0.3803)
2023/11/23 09:36:20 - INFO - root -   Epoch: [237/300][220/283], lr: 0.00000051 	 loss = 0.3625(0.3652)
2023/11/23 09:36:39 - INFO - root -   Epoch: [237/300][240/283], lr: 0.00000051 	 loss = 0.6613(0.3580)
2023/11/23 09:36:47 - INFO - root -   Epoch: [237/300][260/283], lr: 0.00000051 	 loss = 0.1472(0.3505)
2023/11/23 09:37:04 - INFO - root -   Epoch: [237/300][280/283], lr: 0.00000051 	 loss = 3.8182(0.3511)
2023/11/23 09:37:05 - INFO - root -   Epoch: [237/300] 	 loss = 0.3494
2023/11/23 09:37:05 - INFO - root -   train_accuracy = 0.8816
2023/11/23 09:37:06 - INFO - root -   Epoch: [238/300][0/283], lr: 0.00000052 	 loss = 0.0333(0.0333)
2023/11/23 09:37:24 - INFO - root -   Epoch: [238/300][20/283], lr: 0.00000052 	 loss = 0.1491(0.4393)
2023/11/23 09:37:40 - INFO - root -   Epoch: [238/300][40/283], lr: 0.00000052 	 loss = 0.3762(0.4514)
2023/11/23 09:37:52 - INFO - root -   Epoch: [238/300][60/283], lr: 0.00000052 	 loss = 0.1891(0.4464)
2023/11/23 09:38:05 - INFO - root -   Epoch: [238/300][80/283], lr: 0.00000052 	 loss = 0.0636(0.4697)
2023/11/23 09:38:19 - INFO - root -   Epoch: [238/300][100/283], lr: 0.00000052 	 loss = 0.0515(0.4602)
2023/11/23 09:38:32 - INFO - root -   Epoch: [238/300][120/283], lr: 0.00000052 	 loss = 0.0180(0.4033)
2023/11/23 09:38:42 - INFO - root -   Epoch: [238/300][140/283], lr: 0.00000052 	 loss = 0.0604(0.3705)
2023/11/23 09:38:58 - INFO - root -   Epoch: [238/300][160/283], lr: 0.00000052 	 loss = 0.3319(0.3642)
2023/11/23 09:39:13 - INFO - root -   Epoch: [238/300][180/283], lr: 0.00000052 	 loss = 0.3707(0.3614)
2023/11/23 09:39:29 - INFO - root -   Epoch: [238/300][200/283], lr: 0.00000052 	 loss = 0.1702(0.3583)
2023/11/23 09:39:42 - INFO - root -   Epoch: [238/300][220/283], lr: 0.00000052 	 loss = 0.0100(0.3472)
2023/11/23 09:39:59 - INFO - root -   Epoch: [238/300][240/283], lr: 0.00000052 	 loss = 0.3572(0.3455)
2023/11/23 09:40:09 - INFO - root -   Epoch: [238/300][260/283], lr: 0.00000052 	 loss = 0.1898(0.3397)
2023/11/23 09:40:21 - INFO - root -   Epoch: [238/300][280/283], lr: 0.00000052 	 loss = 2.7504(0.3396)
2023/11/23 09:40:21 - INFO - root -   Epoch: [238/300] 	 loss = 0.3382
2023/11/23 09:40:21 - INFO - root -   train_accuracy = 0.8887
2023/11/23 09:40:33 - INFO - root -   Epoch: [239/300][0/283], lr: 0.00000052 	 loss = 0.1903(0.1903)
2023/11/23 09:40:47 - INFO - root -   Epoch: [239/300][20/283], lr: 0.00000052 	 loss = 1.6395(0.6420)
2023/11/23 09:41:04 - INFO - root -   Epoch: [239/300][40/283], lr: 0.00000052 	 loss = 0.3594(0.6789)
2023/11/23 09:41:16 - INFO - root -   Epoch: [239/300][60/283], lr: 0.00000052 	 loss = 0.0169(0.5521)
2023/11/23 09:41:38 - INFO - root -   Epoch: [239/300][80/283], lr: 0.00000052 	 loss = 0.0538(0.5726)
2023/11/23 09:41:46 - INFO - root -   Epoch: [239/300][100/283], lr: 0.00000052 	 loss = 0.7000(0.4907)
2023/11/23 09:41:59 - INFO - root -   Epoch: [239/300][120/283], lr: 0.00000052 	 loss = 0.0142(0.4417)
2023/11/23 09:42:18 - INFO - root -   Epoch: [239/300][140/283], lr: 0.00000052 	 loss = 0.0064(0.4162)
2023/11/23 09:42:38 - INFO - root -   Epoch: [239/300][160/283], lr: 0.00000052 	 loss = 1.3560(0.3990)
2023/11/23 09:42:58 - INFO - root -   Epoch: [239/300][180/283], lr: 0.00000052 	 loss = 0.2656(0.4376)
2023/11/23 09:43:13 - INFO - root -   Epoch: [239/300][200/283], lr: 0.00000052 	 loss = 0.1573(0.4240)
2023/11/23 09:43:33 - INFO - root -   Epoch: [239/300][220/283], lr: 0.00000052 	 loss = 0.0508(0.4024)
2023/11/23 09:43:46 - INFO - root -   Epoch: [239/300][240/283], lr: 0.00000052 	 loss = 0.1039(0.3988)
2023/11/23 09:44:02 - INFO - root -   Epoch: [239/300][260/283], lr: 0.00000052 	 loss = 0.4373(0.3891)
2023/11/23 09:44:17 - INFO - root -   Epoch: [239/300][280/283], lr: 0.00000052 	 loss = 1.2458(0.3763)
2023/11/23 09:44:18 - INFO - root -   Epoch: [239/300] 	 loss = 0.3744
2023/11/23 09:47:11 - INFO - root -   precision = 0.8112
2023/11/23 09:47:11 - INFO - root -   eval_loss = 0.6878
2023/11/23 09:47:11 - INFO - root -   eval_acc = 0.8112
2023/11/23 09:47:12 - INFO - root -   train_accuracy = 0.8675
2023/11/23 09:47:14 - INFO - root -   Epoch: [240/300][0/283], lr: 0.00000052 	 loss = 0.0323(0.0323)
2023/11/23 09:47:37 - INFO - root -   Epoch: [240/300][20/283], lr: 0.00000052 	 loss = 1.0369(0.3727)
2023/11/23 09:47:49 - INFO - root -   Epoch: [240/300][40/283], lr: 0.00000052 	 loss = 0.0175(0.4314)
2023/11/23 09:48:01 - INFO - root -   Epoch: [240/300][60/283], lr: 0.00000052 	 loss = 0.1606(0.3949)
2023/11/23 09:48:20 - INFO - root -   Epoch: [240/300][80/283], lr: 0.00000052 	 loss = 0.0168(0.4519)
2023/11/23 09:48:37 - INFO - root -   Epoch: [240/300][100/283], lr: 0.00000052 	 loss = 0.5614(0.4248)
2023/11/23 09:48:54 - INFO - root -   Epoch: [240/300][120/283], lr: 0.00000052 	 loss = 0.0303(0.4008)
2023/11/23 09:49:14 - INFO - root -   Epoch: [240/300][140/283], lr: 0.00000052 	 loss = 0.0992(0.3660)
2023/11/23 09:49:22 - INFO - root -   Epoch: [240/300][160/283], lr: 0.00000052 	 loss = 0.1598(0.3712)
2023/11/23 09:49:34 - INFO - root -   Epoch: [240/300][180/283], lr: 0.00000052 	 loss = 0.1031(0.3703)
2023/11/23 09:49:48 - INFO - root -   Epoch: [240/300][200/283], lr: 0.00000052 	 loss = 0.0053(0.3629)
2023/11/23 09:50:18 - INFO - root -   Epoch: [240/300][220/283], lr: 0.00000052 	 loss = 0.1398(0.3740)
2023/11/23 09:50:29 - INFO - root -   Epoch: [240/300][240/283], lr: 0.00000052 	 loss = 2.1541(0.3862)
2023/11/23 09:50:42 - INFO - root -   Epoch: [240/300][260/283], lr: 0.00000052 	 loss = 1.8607(0.3752)
2023/11/23 09:50:59 - INFO - root -   Epoch: [240/300][280/283], lr: 0.00000052 	 loss = 1.4068(0.3745)
2023/11/23 09:51:00 - INFO - root -   Epoch: [240/300] 	 loss = 0.3731
2023/11/23 09:51:00 - INFO - root -   train_accuracy = 0.8746
2023/11/23 09:51:01 - INFO - root -   Epoch: [241/300][0/283], lr: 0.00000052 	 loss = 0.0047(0.0047)
2023/11/23 09:51:20 - INFO - root -   Epoch: [241/300][20/283], lr: 0.00000052 	 loss = 1.3655(0.3539)
2023/11/23 09:51:31 - INFO - root -   Epoch: [241/300][40/283], lr: 0.00000052 	 loss = 0.0749(0.3544)
2023/11/23 09:51:41 - INFO - root -   Epoch: [241/300][60/283], lr: 0.00000052 	 loss = 0.3216(0.3445)
2023/11/23 09:52:02 - INFO - root -   Epoch: [241/300][80/283], lr: 0.00000052 	 loss = 0.0074(0.3795)
2023/11/23 09:52:10 - INFO - root -   Epoch: [241/300][100/283], lr: 0.00000052 	 loss = 0.1823(0.3747)
2023/11/23 09:52:20 - INFO - root -   Epoch: [241/300][120/283], lr: 0.00000052 	 loss = 0.0241(0.3459)
2023/11/23 09:52:40 - INFO - root -   Epoch: [241/300][140/283], lr: 0.00000052 	 loss = 0.0071(0.3334)
2023/11/23 09:52:48 - INFO - root -   Epoch: [241/300][160/283], lr: 0.00000052 	 loss = 1.3789(0.3514)
2023/11/23 09:53:08 - INFO - root -   Epoch: [241/300][180/283], lr: 0.00000052 	 loss = 0.1918(0.3795)
2023/11/23 09:53:26 - INFO - root -   Epoch: [241/300][200/283], lr: 0.00000052 	 loss = 0.0143(0.3823)
2023/11/23 09:53:37 - INFO - root -   Epoch: [241/300][220/283], lr: 0.00000052 	 loss = 0.0149(0.3727)
2023/11/23 09:53:48 - INFO - root -   Epoch: [241/300][240/283], lr: 0.00000052 	 loss = 0.1970(0.3629)
2023/11/23 09:54:05 - INFO - root -   Epoch: [241/300][260/283], lr: 0.00000052 	 loss = 0.5791(0.3507)
2023/11/23 09:54:16 - INFO - root -   Epoch: [241/300][280/283], lr: 0.00000052 	 loss = 2.2947(0.3485)
2023/11/23 09:54:17 - INFO - root -   Epoch: [241/300] 	 loss = 0.3466
2023/11/23 09:54:17 - INFO - root -   train_accuracy = 0.8675
2023/11/23 09:54:18 - INFO - root -   Epoch: [242/300][0/283], lr: 0.00000052 	 loss = 0.0174(0.0174)
2023/11/23 09:54:34 - INFO - root -   Epoch: [242/300][20/283], lr: 0.00000052 	 loss = 0.1676(0.3009)
2023/11/23 09:54:52 - INFO - root -   Epoch: [242/300][40/283], lr: 0.00000052 	 loss = 0.2933(0.4107)
2023/11/23 09:55:06 - INFO - root -   Epoch: [242/300][60/283], lr: 0.00000052 	 loss = 0.0078(0.4207)
2023/11/23 09:55:17 - INFO - root -   Epoch: [242/300][80/283], lr: 0.00000052 	 loss = 0.0107(0.4342)
2023/11/23 09:55:37 - INFO - root -   Epoch: [242/300][100/283], lr: 0.00000052 	 loss = 0.0889(0.4183)
2023/11/23 09:55:51 - INFO - root -   Epoch: [242/300][120/283], lr: 0.00000052 	 loss = 0.2337(0.3751)
2023/11/23 09:56:04 - INFO - root -   Epoch: [242/300][140/283], lr: 0.00000052 	 loss = 0.0184(0.3528)
2023/11/23 09:56:12 - INFO - root -   Epoch: [242/300][160/283], lr: 0.00000052 	 loss = 0.1656(0.3490)
2023/11/23 09:56:27 - INFO - root -   Epoch: [242/300][180/283], lr: 0.00000052 	 loss = 0.0377(0.3486)
2023/11/23 09:56:52 - INFO - root -   Epoch: [242/300][200/283], lr: 0.00000052 	 loss = 0.1434(0.3425)
2023/11/23 09:57:03 - INFO - root -   Epoch: [242/300][220/283], lr: 0.00000052 	 loss = 0.0300(0.3378)
2023/11/23 09:57:15 - INFO - root -   Epoch: [242/300][240/283], lr: 0.00000052 	 loss = 0.1633(0.3336)
2023/11/23 09:57:26 - INFO - root -   Epoch: [242/300][260/283], lr: 0.00000052 	 loss = 0.0675(0.3261)
2023/11/23 09:57:37 - INFO - root -   Epoch: [242/300][280/283], lr: 0.00000052 	 loss = 2.2199(0.3243)
2023/11/23 09:57:38 - INFO - root -   Epoch: [242/300] 	 loss = 0.3231
2023/11/23 09:57:38 - INFO - root -   train_accuracy = 0.8975
2023/11/23 09:57:39 - INFO - root -   Epoch: [243/300][0/283], lr: 0.00000053 	 loss = 0.0281(0.0281)
2023/11/23 09:58:08 - INFO - root -   Epoch: [243/300][20/283], lr: 0.00000053 	 loss = 2.3141(0.5567)
2023/11/23 09:58:26 - INFO - root -   Epoch: [243/300][40/283], lr: 0.00000053 	 loss = 0.0269(0.5411)
2023/11/23 09:58:43 - INFO - root -   Epoch: [243/300][60/283], lr: 0.00000053 	 loss = 0.0063(0.4912)
2023/11/23 09:58:55 - INFO - root -   Epoch: [243/300][80/283], lr: 0.00000053 	 loss = 0.0526(0.4817)
2023/11/23 09:59:12 - INFO - root -   Epoch: [243/300][100/283], lr: 0.00000053 	 loss = 0.0164(0.4332)
2023/11/23 09:59:32 - INFO - root -   Epoch: [243/300][120/283], lr: 0.00000053 	 loss = 0.0067(0.3975)
2023/11/23 09:59:47 - INFO - root -   Epoch: [243/300][140/283], lr: 0.00000053 	 loss = 0.0793(0.3796)
2023/11/23 09:59:59 - INFO - root -   Epoch: [243/300][160/283], lr: 0.00000053 	 loss = 0.8544(0.3658)
2023/11/23 10:00:07 - INFO - root -   Epoch: [243/300][180/283], lr: 0.00000053 	 loss = 0.1731(0.3541)
2023/11/23 10:00:28 - INFO - root -   Epoch: [243/300][200/283], lr: 0.00000053 	 loss = 0.0229(0.3586)
2023/11/23 10:00:45 - INFO - root -   Epoch: [243/300][220/283], lr: 0.00000053 	 loss = 0.1928(0.3523)
2023/11/23 10:00:55 - INFO - root -   Epoch: [243/300][240/283], lr: 0.00000053 	 loss = 2.9090(0.3586)
2023/11/23 10:01:13 - INFO - root -   Epoch: [243/300][260/283], lr: 0.00000053 	 loss = 0.2449(0.3501)
2023/11/23 10:01:22 - INFO - root -   Epoch: [243/300][280/283], lr: 0.00000053 	 loss = 1.4841(0.3473)
2023/11/23 10:01:23 - INFO - root -   Epoch: [243/300] 	 loss = 0.3459
2023/11/23 10:01:23 - INFO - root -   train_accuracy = 0.8640
2023/11/23 10:01:32 - INFO - root -   Epoch: [244/300][0/283], lr: 0.00000053 	 loss = 0.1475(0.1475)
2023/11/23 10:01:48 - INFO - root -   Epoch: [244/300][20/283], lr: 0.00000053 	 loss = 1.1687(0.5845)
2023/11/23 10:02:13 - INFO - root -   Epoch: [244/300][40/283], lr: 0.00000053 	 loss = 0.5344(0.5381)
2023/11/23 10:02:29 - INFO - root -   Epoch: [244/300][60/283], lr: 0.00000053 	 loss = 0.1077(0.5196)
2023/11/23 10:02:41 - INFO - root -   Epoch: [244/300][80/283], lr: 0.00000053 	 loss = 0.0887(0.5485)
2023/11/23 10:02:54 - INFO - root -   Epoch: [244/300][100/283], lr: 0.00000053 	 loss = 0.3113(0.4721)
2023/11/23 10:03:10 - INFO - root -   Epoch: [244/300][120/283], lr: 0.00000053 	 loss = 0.0298(0.4691)
2023/11/23 10:03:27 - INFO - root -   Epoch: [244/300][140/283], lr: 0.00000053 	 loss = 0.0043(0.4270)
2023/11/23 10:03:35 - INFO - root -   Epoch: [244/300][160/283], lr: 0.00000053 	 loss = 0.5035(0.4135)
2023/11/23 10:03:46 - INFO - root -   Epoch: [244/300][180/283], lr: 0.00000053 	 loss = 0.0174(0.4072)
2023/11/23 10:04:01 - INFO - root -   Epoch: [244/300][200/283], lr: 0.00000053 	 loss = 0.1542(0.4137)
2023/11/23 10:04:11 - INFO - root -   Epoch: [244/300][220/283], lr: 0.00000053 	 loss = 0.0140(0.4004)
2023/11/23 10:04:30 - INFO - root -   Epoch: [244/300][240/283], lr: 0.00000053 	 loss = 0.3172(0.4009)
2023/11/23 10:04:47 - INFO - root -   Epoch: [244/300][260/283], lr: 0.00000053 	 loss = 0.3465(0.3952)
2023/11/23 10:04:55 - INFO - root -   Epoch: [244/300][280/283], lr: 0.00000053 	 loss = 2.2853(0.3890)
2023/11/23 10:04:56 - INFO - root -   Epoch: [244/300] 	 loss = 0.3863
2023/11/23 10:07:50 - INFO - root -   precision = 0.8252
2023/11/23 10:07:50 - INFO - root -   eval_loss = 0.5956
2023/11/23 10:07:50 - INFO - root -   eval_acc = 0.8252
2023/11/23 10:07:51 - INFO - root -   train_accuracy = 0.8604
2023/11/23 10:07:52 - INFO - root -   Epoch: [245/300][0/283], lr: 0.00000053 	 loss = 0.0731(0.0731)
2023/11/23 10:08:12 - INFO - root -   Epoch: [245/300][20/283], lr: 0.00000053 	 loss = 0.1691(0.4899)
2023/11/23 10:08:33 - INFO - root -   Epoch: [245/300][40/283], lr: 0.00000053 	 loss = 0.0522(0.5861)
2023/11/23 10:08:43 - INFO - root -   Epoch: [245/300][60/283], lr: 0.00000053 	 loss = 0.1871(0.5215)
2023/11/23 10:09:05 - INFO - root -   Epoch: [245/300][80/283], lr: 0.00000053 	 loss = 0.0331(0.4659)
2023/11/23 10:09:19 - INFO - root -   Epoch: [245/300][100/283], lr: 0.00000053 	 loss = 0.0168(0.4332)
2023/11/23 10:09:33 - INFO - root -   Epoch: [245/300][120/283], lr: 0.00000053 	 loss = 0.0160(0.3933)
2023/11/23 10:09:45 - INFO - root -   Epoch: [245/300][140/283], lr: 0.00000053 	 loss = 0.3924(0.3658)
2023/11/23 10:09:56 - INFO - root -   Epoch: [245/300][160/283], lr: 0.00000053 	 loss = 1.3042(0.3632)
2023/11/23 10:10:09 - INFO - root -   Epoch: [245/300][180/283], lr: 0.00000053 	 loss = 0.0122(0.3608)
2023/11/23 10:10:23 - INFO - root -   Epoch: [245/300][200/283], lr: 0.00000053 	 loss = 0.0038(0.3660)
2023/11/23 10:10:33 - INFO - root -   Epoch: [245/300][220/283], lr: 0.00000053 	 loss = 0.0055(0.3489)
2023/11/23 10:10:51 - INFO - root -   Epoch: [245/300][240/283], lr: 0.00000053 	 loss = 0.3124(0.3463)
2023/11/23 10:11:07 - INFO - root -   Epoch: [245/300][260/283], lr: 0.00000053 	 loss = 0.1865(0.3504)
2023/11/23 10:11:18 - INFO - root -   Epoch: [245/300][280/283], lr: 0.00000053 	 loss = 1.1040(0.3357)
2023/11/23 10:11:19 - INFO - root -   Epoch: [245/300] 	 loss = 0.3338
2023/11/23 10:11:19 - INFO - root -   train_accuracy = 0.8763
2023/11/23 10:11:25 - INFO - root -   Epoch: [246/300][0/283], lr: 0.00000053 	 loss = 0.1482(0.1482)
2023/11/23 10:11:36 - INFO - root -   Epoch: [246/300][20/283], lr: 0.00000053 	 loss = 0.4829(0.4602)
2023/11/23 10:11:49 - INFO - root -   Epoch: [246/300][40/283], lr: 0.00000053 	 loss = 0.8244(0.5212)
2023/11/23 10:11:59 - INFO - root -   Epoch: [246/300][60/283], lr: 0.00000053 	 loss = 0.0028(0.4197)
2023/11/23 10:12:19 - INFO - root -   Epoch: [246/300][80/283], lr: 0.00000053 	 loss = 0.0919(0.4472)
2023/11/23 10:12:35 - INFO - root -   Epoch: [246/300][100/283], lr: 0.00000053 	 loss = 0.0922(0.4338)
2023/11/23 10:12:52 - INFO - root -   Epoch: [246/300][120/283], lr: 0.00000053 	 loss = 0.0063(0.4008)
2023/11/23 10:13:09 - INFO - root -   Epoch: [246/300][140/283], lr: 0.00000053 	 loss = 0.0974(0.3706)
2023/11/23 10:13:24 - INFO - root -   Epoch: [246/300][160/283], lr: 0.00000053 	 loss = 0.6479(0.3718)
2023/11/23 10:13:36 - INFO - root -   Epoch: [246/300][180/283], lr: 0.00000053 	 loss = 0.0670(0.3818)
2023/11/23 10:13:49 - INFO - root -   Epoch: [246/300][200/283], lr: 0.00000053 	 loss = 0.1883(0.3768)
2023/11/23 10:14:05 - INFO - root -   Epoch: [246/300][220/283], lr: 0.00000053 	 loss = 0.2791(0.3719)
2023/11/23 10:14:25 - INFO - root -   Epoch: [246/300][240/283], lr: 0.00000053 	 loss = 0.4211(0.3776)
2023/11/23 10:14:38 - INFO - root -   Epoch: [246/300][260/283], lr: 0.00000053 	 loss = 0.2922(0.3675)
2023/11/23 10:14:48 - INFO - root -   Epoch: [246/300][280/283], lr: 0.00000053 	 loss = 2.5019(0.3636)
2023/11/23 10:14:49 - INFO - root -   Epoch: [246/300] 	 loss = 0.3611
2023/11/23 10:14:49 - INFO - root -   train_accuracy = 0.8640
2023/11/23 10:14:50 - INFO - root -   Epoch: [247/300][0/283], lr: 0.00000053 	 loss = 0.0030(0.0030)
2023/11/23 10:15:11 - INFO - root -   Epoch: [247/300][20/283], lr: 0.00000053 	 loss = 0.1088(0.4666)
2023/11/23 10:15:21 - INFO - root -   Epoch: [247/300][40/283], lr: 0.00000053 	 loss = 0.1707(0.5559)
2023/11/23 10:15:42 - INFO - root -   Epoch: [247/300][60/283], lr: 0.00000053 	 loss = 0.9171(0.5882)
2023/11/23 10:15:59 - INFO - root -   Epoch: [247/300][80/283], lr: 0.00000053 	 loss = 0.0134(0.5089)
2023/11/23 10:16:17 - INFO - root -   Epoch: [247/300][100/283], lr: 0.00000053 	 loss = 0.4727(0.4594)
2023/11/23 10:16:30 - INFO - root -   Epoch: [247/300][120/283], lr: 0.00000053 	 loss = 0.0062(0.4236)
2023/11/23 10:16:46 - INFO - root -   Epoch: [247/300][140/283], lr: 0.00000053 	 loss = 0.0163(0.3956)
2023/11/23 10:17:06 - INFO - root -   Epoch: [247/300][160/283], lr: 0.00000053 	 loss = 1.6212(0.4134)
2023/11/23 10:17:32 - INFO - root -   Epoch: [247/300][180/283], lr: 0.00000053 	 loss = 0.0041(0.4074)
2023/11/23 10:17:45 - INFO - root -   Epoch: [247/300][200/283], lr: 0.00000053 	 loss = 0.2210(0.4190)
2023/11/23 10:18:02 - INFO - root -   Epoch: [247/300][220/283], lr: 0.00000053 	 loss = 0.0067(0.4014)
2023/11/23 10:18:18 - INFO - root -   Epoch: [247/300][240/283], lr: 0.00000053 	 loss = 0.2907(0.4047)
2023/11/23 10:18:27 - INFO - root -   Epoch: [247/300][260/283], lr: 0.00000053 	 loss = 1.9019(0.3958)
2023/11/23 10:18:44 - INFO - root -   Epoch: [247/300][280/283], lr: 0.00000053 	 loss = 1.0376(0.3895)
2023/11/23 10:18:45 - INFO - root -   Epoch: [247/300] 	 loss = 0.3870
2023/11/23 10:18:45 - INFO - root -   train_accuracy = 0.8746
2023/11/23 10:18:46 - INFO - root -   Epoch: [248/300][0/283], lr: 0.00000053 	 loss = 0.0908(0.0908)
2023/11/23 10:19:05 - INFO - root -   Epoch: [248/300][20/283], lr: 0.00000053 	 loss = 0.2604(0.3569)
2023/11/23 10:19:15 - INFO - root -   Epoch: [248/300][40/283], lr: 0.00000053 	 loss = 0.1179(0.3824)
2023/11/23 10:19:37 - INFO - root -   Epoch: [248/300][60/283], lr: 0.00000053 	 loss = 0.3480(0.4057)
2023/11/23 10:19:56 - INFO - root -   Epoch: [248/300][80/283], lr: 0.00000053 	 loss = 0.2618(0.3953)
2023/11/23 10:20:13 - INFO - root -   Epoch: [248/300][100/283], lr: 0.00000053 	 loss = 0.2472(0.4073)
2023/11/23 10:20:28 - INFO - root -   Epoch: [248/300][120/283], lr: 0.00000053 	 loss = 0.3180(0.3941)
2023/11/23 10:20:42 - INFO - root -   Epoch: [248/300][140/283], lr: 0.00000053 	 loss = 0.1947(0.3579)
2023/11/23 10:20:50 - INFO - root -   Epoch: [248/300][160/283], lr: 0.00000053 	 loss = 1.4434(0.3652)
2023/11/23 10:21:07 - INFO - root -   Epoch: [248/300][180/283], lr: 0.00000053 	 loss = 0.2940(0.3661)
2023/11/23 10:21:18 - INFO - root -   Epoch: [248/300][200/283], lr: 0.00000053 	 loss = 0.0029(0.3544)
2023/11/23 10:21:36 - INFO - root -   Epoch: [248/300][220/283], lr: 0.00000053 	 loss = 0.2416(0.3495)
2023/11/23 10:21:50 - INFO - root -   Epoch: [248/300][240/283], lr: 0.00000053 	 loss = 0.1737(0.3469)
2023/11/23 10:22:06 - INFO - root -   Epoch: [248/300][260/283], lr: 0.00000053 	 loss = 0.8380(0.3376)
2023/11/23 10:22:21 - INFO - root -   Epoch: [248/300][280/283], lr: 0.00000053 	 loss = 4.1262(0.3336)
2023/11/23 10:22:22 - INFO - root -   Epoch: [248/300] 	 loss = 0.3314
2023/11/23 10:22:22 - INFO - root -   train_accuracy = 0.8781
2023/11/23 10:22:34 - INFO - root -   Epoch: [249/300][0/283], lr: 0.00000054 	 loss = 0.0902(0.0902)
2023/11/23 10:22:42 - INFO - root -   Epoch: [249/300][20/283], lr: 0.00000054 	 loss = 1.6128(0.6653)
2023/11/23 10:23:08 - INFO - root -   Epoch: [249/300][40/283], lr: 0.00000054 	 loss = 0.1699(0.6236)
2023/11/23 10:23:22 - INFO - root -   Epoch: [249/300][60/283], lr: 0.00000054 	 loss = 0.0050(0.6276)
2023/11/23 10:23:36 - INFO - root -   Epoch: [249/300][80/283], lr: 0.00000054 	 loss = 0.3546(0.6325)
2023/11/23 10:23:56 - INFO - root -   Epoch: [249/300][100/283], lr: 0.00000054 	 loss = 0.5486(0.5834)
2023/11/23 10:24:06 - INFO - root -   Epoch: [249/300][120/283], lr: 0.00000054 	 loss = 0.1368(0.5508)
2023/11/23 10:24:21 - INFO - root -   Epoch: [249/300][140/283], lr: 0.00000054 	 loss = 0.1443(0.4951)
2023/11/23 10:24:39 - INFO - root -   Epoch: [249/300][160/283], lr: 0.00000054 	 loss = 0.5353(0.4852)
2023/11/23 10:24:53 - INFO - root -   Epoch: [249/300][180/283], lr: 0.00000054 	 loss = 0.0330(0.4686)
2023/11/23 10:25:07 - INFO - root -   Epoch: [249/300][200/283], lr: 0.00000054 	 loss = 0.1963(0.4522)
2023/11/23 10:25:27 - INFO - root -   Epoch: [249/300][220/283], lr: 0.00000054 	 loss = 0.0130(0.4321)
2023/11/23 10:25:37 - INFO - root -   Epoch: [249/300][240/283], lr: 0.00000054 	 loss = 0.6380(0.4161)
2023/11/23 10:25:55 - INFO - root -   Epoch: [249/300][260/283], lr: 0.00000054 	 loss = 0.2166(0.3944)
2023/11/23 10:26:07 - INFO - root -   Epoch: [249/300][280/283], lr: 0.00000054 	 loss = 2.5640(0.3894)
2023/11/23 10:26:08 - INFO - root -   Epoch: [249/300] 	 loss = 0.3868
2023/11/23 10:29:01 - INFO - root -   precision = 0.8601
2023/11/23 10:29:01 - INFO - root -   eval_loss = 0.5340
2023/11/23 10:29:01 - INFO - root -   eval_acc = 0.8601
2023/11/23 10:29:02 - INFO - root -   train_accuracy = 0.8604
2023/11/23 10:29:04 - INFO - root -   Epoch: [250/300][0/283], lr: 0.00000054 	 loss = 0.0635(0.0635)
2023/11/23 10:29:25 - INFO - root -   Epoch: [250/300][20/283], lr: 0.00000054 	 loss = 1.1509(0.5687)
2023/11/23 10:29:44 - INFO - root -   Epoch: [250/300][40/283], lr: 0.00000054 	 loss = 0.0291(0.5299)
2023/11/23 10:30:12 - INFO - root -   Epoch: [250/300][60/283], lr: 0.00000054 	 loss = 0.5058(0.5478)
2023/11/23 10:30:26 - INFO - root -   Epoch: [250/300][80/283], lr: 0.00000054 	 loss = 0.4477(0.5601)
2023/11/23 10:30:34 - INFO - root -   Epoch: [250/300][100/283], lr: 0.00000054 	 loss = 0.0161(0.5172)
2023/11/23 10:30:49 - INFO - root -   Epoch: [250/300][120/283], lr: 0.00000054 	 loss = 0.0837(0.4954)
2023/11/23 10:31:12 - INFO - root -   Epoch: [250/300][140/283], lr: 0.00000054 	 loss = 0.0160(0.4592)
2023/11/23 10:31:20 - INFO - root -   Epoch: [250/300][160/283], lr: 0.00000054 	 loss = 0.3315(0.4433)
2023/11/23 10:31:34 - INFO - root -   Epoch: [250/300][180/283], lr: 0.00000054 	 loss = 0.0081(0.4178)
2023/11/23 10:31:46 - INFO - root -   Epoch: [250/300][200/283], lr: 0.00000054 	 loss = 0.0181(0.4117)
2023/11/23 10:32:11 - INFO - root -   Epoch: [250/300][220/283], lr: 0.00000054 	 loss = 0.0171(0.4130)
2023/11/23 10:32:20 - INFO - root -   Epoch: [250/300][240/283], lr: 0.00000054 	 loss = 0.1232(0.4129)
2023/11/23 10:32:38 - INFO - root -   Epoch: [250/300][260/283], lr: 0.00000054 	 loss = 1.4034(0.4110)
2023/11/23 10:32:48 - INFO - root -   Epoch: [250/300][280/283], lr: 0.00000054 	 loss = 0.9776(0.3923)
2023/11/23 10:32:49 - INFO - root -   Epoch: [250/300] 	 loss = 0.3896
2023/11/23 10:32:49 - INFO - root -   train_accuracy = 0.8604
2023/11/23 10:32:57 - INFO - root -   Epoch: [251/300][0/283], lr: 0.00000054 	 loss = 0.1497(0.1497)
2023/11/23 10:33:16 - INFO - root -   Epoch: [251/300][20/283], lr: 0.00000054 	 loss = 0.8934(0.7061)
2023/11/23 10:33:28 - INFO - root -   Epoch: [251/300][40/283], lr: 0.00000054 	 loss = 0.0331(0.6295)
2023/11/23 10:33:45 - INFO - root -   Epoch: [251/300][60/283], lr: 0.00000054 	 loss = 0.0120(0.5395)
2023/11/23 10:34:06 - INFO - root -   Epoch: [251/300][80/283], lr: 0.00000054 	 loss = 0.6717(0.5336)
2023/11/23 10:34:24 - INFO - root -   Epoch: [251/300][100/283], lr: 0.00000054 	 loss = 0.3429(0.4951)
2023/11/23 10:34:32 - INFO - root -   Epoch: [251/300][120/283], lr: 0.00000054 	 loss = 0.1219(0.4363)
2023/11/23 10:34:50 - INFO - root -   Epoch: [251/300][140/283], lr: 0.00000054 	 loss = 0.1982(0.4037)
2023/11/23 10:35:05 - INFO - root -   Epoch: [251/300][160/283], lr: 0.00000054 	 loss = 0.2348(0.3951)
2023/11/23 10:35:26 - INFO - root -   Epoch: [251/300][180/283], lr: 0.00000054 	 loss = 0.0640(0.3959)
2023/11/23 10:35:40 - INFO - root -   Epoch: [251/300][200/283], lr: 0.00000054 	 loss = 0.0412(0.4018)
2023/11/23 10:36:00 - INFO - root -   Epoch: [251/300][220/283], lr: 0.00000054 	 loss = 0.1000(0.3880)
2023/11/23 10:36:17 - INFO - root -   Epoch: [251/300][240/283], lr: 0.00000054 	 loss = 0.0695(0.3912)
2023/11/23 10:36:32 - INFO - root -   Epoch: [251/300][260/283], lr: 0.00000054 	 loss = 0.1307(0.3845)
2023/11/23 10:36:40 - INFO - root -   Epoch: [251/300][280/283], lr: 0.00000054 	 loss = 1.3454(0.3739)
2023/11/23 10:36:41 - INFO - root -   Epoch: [251/300] 	 loss = 0.3714
2023/11/23 10:36:41 - INFO - root -   train_accuracy = 0.8710
2023/11/23 10:36:47 - INFO - root -   Epoch: [252/300][0/283], lr: 0.00000054 	 loss = 0.0678(0.0678)
2023/11/23 10:37:11 - INFO - root -   Epoch: [252/300][20/283], lr: 0.00000054 	 loss = 1.4130(0.5846)
2023/11/23 10:37:23 - INFO - root -   Epoch: [252/300][40/283], lr: 0.00000054 	 loss = 0.1799(0.5061)
2023/11/23 10:37:45 - INFO - root -   Epoch: [252/300][60/283], lr: 0.00000054 	 loss = 0.0079(0.5090)
2023/11/23 10:38:04 - INFO - root -   Epoch: [252/300][80/283], lr: 0.00000054 	 loss = 0.0456(0.5036)
2023/11/23 10:38:16 - INFO - root -   Epoch: [252/300][100/283], lr: 0.00000054 	 loss = 0.0497(0.5003)
2023/11/23 10:38:28 - INFO - root -   Epoch: [252/300][120/283], lr: 0.00000054 	 loss = 0.0203(0.4516)
2023/11/23 10:38:42 - INFO - root -   Epoch: [252/300][140/283], lr: 0.00000054 	 loss = 0.2243(0.4102)
2023/11/23 10:38:51 - INFO - root -   Epoch: [252/300][160/283], lr: 0.00000054 	 loss = 0.9851(0.3953)
2023/11/23 10:39:08 - INFO - root -   Epoch: [252/300][180/283], lr: 0.00000054 	 loss = 0.0278(0.3970)
2023/11/23 10:39:18 - INFO - root -   Epoch: [252/300][200/283], lr: 0.00000054 	 loss = 0.1578(0.3773)
2023/11/23 10:39:47 - INFO - root -   Epoch: [252/300][220/283], lr: 0.00000054 	 loss = 0.2344(0.3785)
2023/11/23 10:40:02 - INFO - root -   Epoch: [252/300][240/283], lr: 0.00000054 	 loss = 0.2989(0.3805)
2023/11/23 10:40:15 - INFO - root -   Epoch: [252/300][260/283], lr: 0.00000054 	 loss = 0.2002(0.3722)
2023/11/23 10:40:26 - INFO - root -   Epoch: [252/300][280/283], lr: 0.00000054 	 loss = 2.3054(0.3608)
2023/11/23 10:40:26 - INFO - root -   Epoch: [252/300] 	 loss = 0.3591
2023/11/23 10:40:26 - INFO - root -   train_accuracy = 0.8710
2023/11/23 10:40:36 - INFO - root -   Epoch: [253/300][0/283], lr: 0.00000054 	 loss = 0.0364(0.0364)
2023/11/23 10:40:48 - INFO - root -   Epoch: [253/300][20/283], lr: 0.00000054 	 loss = 0.8394(0.5052)
2023/11/23 10:41:03 - INFO - root -   Epoch: [253/300][40/283], lr: 0.00000054 	 loss = 0.5081(0.5266)
2023/11/23 10:41:13 - INFO - root -   Epoch: [253/300][60/283], lr: 0.00000054 	 loss = 0.5243(0.4992)
2023/11/23 10:41:26 - INFO - root -   Epoch: [253/300][80/283], lr: 0.00000054 	 loss = 1.4442(0.4792)
2023/11/23 10:41:45 - INFO - root -   Epoch: [253/300][100/283], lr: 0.00000054 	 loss = 0.2351(0.4650)
2023/11/23 10:41:53 - INFO - root -   Epoch: [253/300][120/283], lr: 0.00000054 	 loss = 0.0111(0.4165)
2023/11/23 10:42:11 - INFO - root -   Epoch: [253/300][140/283], lr: 0.00000054 	 loss = 0.0165(0.3791)
2023/11/23 10:42:26 - INFO - root -   Epoch: [253/300][160/283], lr: 0.00000054 	 loss = 0.1280(0.3903)
2023/11/23 10:42:49 - INFO - root -   Epoch: [253/300][180/283], lr: 0.00000054 	 loss = 0.4982(0.3769)
2023/11/23 10:43:00 - INFO - root -   Epoch: [253/300][200/283], lr: 0.00000054 	 loss = 0.0166(0.3732)
2023/11/23 10:43:18 - INFO - root -   Epoch: [253/300][220/283], lr: 0.00000054 	 loss = 0.0634(0.3525)
2023/11/23 10:43:34 - INFO - root -   Epoch: [253/300][240/283], lr: 0.00000054 	 loss = 0.1985(0.3523)
2023/11/23 10:43:51 - INFO - root -   Epoch: [253/300][260/283], lr: 0.00000054 	 loss = 0.2607(0.3405)
2023/11/23 10:44:00 - INFO - root -   Epoch: [253/300][280/283], lr: 0.00000054 	 loss = 0.7847(0.3290)
2023/11/23 10:44:00 - INFO - root -   Epoch: [253/300] 	 loss = 0.3273
2023/11/23 10:44:00 - INFO - root -   train_accuracy = 0.8905
2023/11/23 10:44:11 - INFO - root -   Epoch: [254/300][0/283], lr: 0.00000054 	 loss = 0.1159(0.1159)
2023/11/23 10:44:25 - INFO - root -   Epoch: [254/300][20/283], lr: 0.00000054 	 loss = 0.0717(0.4554)
2023/11/23 10:44:38 - INFO - root -   Epoch: [254/300][40/283], lr: 0.00000054 	 loss = 0.0185(0.3742)
2023/11/23 10:44:54 - INFO - root -   Epoch: [254/300][60/283], lr: 0.00000054 	 loss = 0.0052(0.4130)
2023/11/23 10:45:16 - INFO - root -   Epoch: [254/300][80/283], lr: 0.00000054 	 loss = 0.0318(0.4287)
2023/11/23 10:45:33 - INFO - root -   Epoch: [254/300][100/283], lr: 0.00000054 	 loss = 0.3219(0.4171)
2023/11/23 10:45:46 - INFO - root -   Epoch: [254/300][120/283], lr: 0.00000054 	 loss = 0.3906(0.3870)
2023/11/23 10:46:08 - INFO - root -   Epoch: [254/300][140/283], lr: 0.00000054 	 loss = 0.0139(0.3710)
2023/11/23 10:46:19 - INFO - root -   Epoch: [254/300][160/283], lr: 0.00000054 	 loss = 0.2427(0.3627)
2023/11/23 10:46:31 - INFO - root -   Epoch: [254/300][180/283], lr: 0.00000054 	 loss = 0.0428(0.3535)
2023/11/23 10:46:58 - INFO - root -   Epoch: [254/300][200/283], lr: 0.00000054 	 loss = 0.1005(0.3619)
2023/11/23 10:47:15 - INFO - root -   Epoch: [254/300][220/283], lr: 0.00000054 	 loss = 0.0646(0.3487)
2023/11/23 10:47:37 - INFO - root -   Epoch: [254/300][240/283], lr: 0.00000054 	 loss = 0.1084(0.3515)
2023/11/23 10:47:51 - INFO - root -   Epoch: [254/300][260/283], lr: 0.00000054 	 loss = 1.2221(0.3430)
2023/11/23 10:48:01 - INFO - root -   Epoch: [254/300][280/283], lr: 0.00000054 	 loss = 1.2664(0.3375)
2023/11/23 10:48:02 - INFO - root -   Epoch: [254/300] 	 loss = 0.3356
2023/11/23 10:50:56 - INFO - root -   precision = 0.8112
2023/11/23 10:50:56 - INFO - root -   eval_loss = 0.6068
2023/11/23 10:50:56 - INFO - root -   eval_acc = 0.8112
2023/11/23 10:50:57 - INFO - root -   train_accuracy = 0.8816
2023/11/23 10:50:58 - INFO - root -   Epoch: [255/300][0/283], lr: 0.00000055 	 loss = 0.0313(0.0313)
2023/11/23 10:51:22 - INFO - root -   Epoch: [255/300][20/283], lr: 0.00000055 	 loss = 1.1919(0.3035)
2023/11/23 10:51:38 - INFO - root -   Epoch: [255/300][40/283], lr: 0.00000055 	 loss = 0.0507(0.4274)
2023/11/23 10:51:52 - INFO - root -   Epoch: [255/300][60/283], lr: 0.00000055 	 loss = 0.0047(0.4388)
2023/11/23 10:52:08 - INFO - root -   Epoch: [255/300][80/283], lr: 0.00000055 	 loss = 0.1472(0.4618)
2023/11/23 10:52:16 - INFO - root -   Epoch: [255/300][100/283], lr: 0.00000055 	 loss = 0.2682(0.4244)
2023/11/23 10:52:37 - INFO - root -   Epoch: [255/300][120/283], lr: 0.00000055 	 loss = 0.1419(0.3923)
2023/11/23 10:52:52 - INFO - root -   Epoch: [255/300][140/283], lr: 0.00000055 	 loss = 0.1662(0.3641)
2023/11/23 10:53:04 - INFO - root -   Epoch: [255/300][160/283], lr: 0.00000055 	 loss = 0.1878(0.3564)
2023/11/23 10:53:15 - INFO - root -   Epoch: [255/300][180/283], lr: 0.00000055 	 loss = 0.0110(0.3378)
2023/11/23 10:53:35 - INFO - root -   Epoch: [255/300][200/283], lr: 0.00000055 	 loss = 0.0661(0.3332)
2023/11/23 10:53:57 - INFO - root -   Epoch: [255/300][220/283], lr: 0.00000055 	 loss = 0.0455(0.3337)
2023/11/23 10:54:14 - INFO - root -   Epoch: [255/300][240/283], lr: 0.00000055 	 loss = 2.7148(0.3481)
2023/11/23 10:54:23 - INFO - root -   Epoch: [255/300][260/283], lr: 0.00000055 	 loss = 0.1203(0.3333)
2023/11/23 10:54:35 - INFO - root -   Epoch: [255/300][280/283], lr: 0.00000055 	 loss = 0.9536(0.3278)
2023/11/23 10:54:36 - INFO - root -   Epoch: [255/300] 	 loss = 0.3261
2023/11/23 10:54:36 - INFO - root -   train_accuracy = 0.8834
2023/11/23 10:54:37 - INFO - root -   Epoch: [256/300][0/283], lr: 0.00000055 	 loss = 0.0536(0.0536)
2023/11/23 10:54:57 - INFO - root -   Epoch: [256/300][20/283], lr: 0.00000055 	 loss = 2.3629(0.5528)
2023/11/23 10:55:10 - INFO - root -   Epoch: [256/300][40/283], lr: 0.00000055 	 loss = 0.1252(0.5536)
2023/11/23 10:55:31 - INFO - root -   Epoch: [256/300][60/283], lr: 0.00000055 	 loss = 0.0039(0.5538)
2023/11/23 10:55:43 - INFO - root -   Epoch: [256/300][80/283], lr: 0.00000055 	 loss = 0.0549(0.5240)
2023/11/23 10:55:59 - INFO - root -   Epoch: [256/300][100/283], lr: 0.00000055 	 loss = 0.1279(0.4796)
2023/11/23 10:56:14 - INFO - root -   Epoch: [256/300][120/283], lr: 0.00000055 	 loss = 0.1704(0.4590)
2023/11/23 10:56:36 - INFO - root -   Epoch: [256/300][140/283], lr: 0.00000055 	 loss = 0.0056(0.4267)
2023/11/23 10:56:44 - INFO - root -   Epoch: [256/300][160/283], lr: 0.00000055 	 loss = 0.2513(0.4198)
2023/11/23 10:57:00 - INFO - root -   Epoch: [256/300][180/283], lr: 0.00000055 	 loss = 0.0219(0.4283)
2023/11/23 10:57:16 - INFO - root -   Epoch: [256/300][200/283], lr: 0.00000055 	 loss = 0.5264(0.4158)
2023/11/23 10:57:28 - INFO - root -   Epoch: [256/300][220/283], lr: 0.00000055 	 loss = 0.0272(0.4015)
2023/11/23 10:57:44 - INFO - root -   Epoch: [256/300][240/283], lr: 0.00000055 	 loss = 2.2777(0.3985)
2023/11/23 10:57:57 - INFO - root -   Epoch: [256/300][260/283], lr: 0.00000055 	 loss = 2.7433(0.3963)
2023/11/23 10:58:09 - INFO - root -   Epoch: [256/300][280/283], lr: 0.00000055 	 loss = 2.0370(0.3820)
2023/11/23 10:58:10 - INFO - root -   Epoch: [256/300] 	 loss = 0.3794
2023/11/23 10:58:10 - INFO - root -   train_accuracy = 0.8710
2023/11/23 10:58:11 - INFO - root -   Epoch: [257/300][0/283], lr: 0.00000055 	 loss = 0.0275(0.0275)
2023/11/23 10:58:36 - INFO - root -   Epoch: [257/300][20/283], lr: 0.00000055 	 loss = 0.0680(0.4574)
2023/11/23 10:58:48 - INFO - root -   Epoch: [257/300][40/283], lr: 0.00000055 	 loss = 0.1725(0.5275)
2023/11/23 10:59:08 - INFO - root -   Epoch: [257/300][60/283], lr: 0.00000055 	 loss = 0.7764(0.5172)
2023/11/23 10:59:21 - INFO - root -   Epoch: [257/300][80/283], lr: 0.00000055 	 loss = 0.2173(0.4895)
2023/11/23 10:59:32 - INFO - root -   Epoch: [257/300][100/283], lr: 0.00000055 	 loss = 0.0101(0.4371)
2023/11/23 10:59:48 - INFO - root -   Epoch: [257/300][120/283], lr: 0.00000055 	 loss = 0.0056(0.3947)
2023/11/23 11:00:00 - INFO - root -   Epoch: [257/300][140/283], lr: 0.00000055 	 loss = 0.0030(0.3741)
2023/11/23 11:00:15 - INFO - root -   Epoch: [257/300][160/283], lr: 0.00000055 	 loss = 0.3856(0.3608)
2023/11/23 11:00:34 - INFO - root -   Epoch: [257/300][180/283], lr: 0.00000055 	 loss = 0.0796(0.3686)
2023/11/23 11:00:42 - INFO - root -   Epoch: [257/300][200/283], lr: 0.00000055 	 loss = 0.0514(0.3660)
2023/11/23 11:01:00 - INFO - root -   Epoch: [257/300][220/283], lr: 0.00000055 	 loss = 0.0777(0.3580)
2023/11/23 11:01:16 - INFO - root -   Epoch: [257/300][240/283], lr: 0.00000055 	 loss = 0.7944(0.3510)
2023/11/23 11:01:38 - INFO - root -   Epoch: [257/300][260/283], lr: 0.00000055 	 loss = 0.1322(0.3367)
2023/11/23 11:01:46 - INFO - root -   Epoch: [257/300][280/283], lr: 0.00000055 	 loss = 3.9166(0.3398)
2023/11/23 11:01:47 - INFO - root -   Epoch: [257/300] 	 loss = 0.3383
2023/11/23 11:01:47 - INFO - root -   train_accuracy = 0.8816
2023/11/23 11:01:48 - INFO - root -   Epoch: [258/300][0/283], lr: 0.00000055 	 loss = 0.1615(0.1615)
2023/11/23 11:02:08 - INFO - root -   Epoch: [258/300][20/283], lr: 0.00000055 	 loss = 1.0217(0.4959)
2023/11/23 11:02:22 - INFO - root -   Epoch: [258/300][40/283], lr: 0.00000055 	 loss = 0.0231(0.3770)
2023/11/23 11:02:39 - INFO - root -   Epoch: [258/300][60/283], lr: 0.00000055 	 loss = 0.1604(0.3852)
2023/11/23 11:02:59 - INFO - root -   Epoch: [258/300][80/283], lr: 0.00000055 	 loss = 0.1560(0.3905)
2023/11/23 11:03:17 - INFO - root -   Epoch: [258/300][100/283], lr: 0.00000055 	 loss = 0.2684(0.3759)
2023/11/23 11:03:29 - INFO - root -   Epoch: [258/300][120/283], lr: 0.00000055 	 loss = 0.0087(0.3564)
2023/11/23 11:03:45 - INFO - root -   Epoch: [258/300][140/283], lr: 0.00000055 	 loss = 0.0457(0.3338)
2023/11/23 11:03:57 - INFO - root -   Epoch: [258/300][160/283], lr: 0.00000055 	 loss = 2.7819(0.3481)
2023/11/23 11:04:12 - INFO - root -   Epoch: [258/300][180/283], lr: 0.00000055 	 loss = 0.0772(0.3443)
2023/11/23 11:04:29 - INFO - root -   Epoch: [258/300][200/283], lr: 0.00000055 	 loss = 0.1430(0.3406)
2023/11/23 11:04:45 - INFO - root -   Epoch: [258/300][220/283], lr: 0.00000055 	 loss = 0.0194(0.3348)
2023/11/23 11:05:00 - INFO - root -   Epoch: [258/300][240/283], lr: 0.00000055 	 loss = 0.1017(0.3395)
2023/11/23 11:05:12 - INFO - root -   Epoch: [258/300][260/283], lr: 0.00000055 	 loss = 0.0542(0.3219)
2023/11/23 11:05:31 - INFO - root -   Epoch: [258/300][280/283], lr: 0.00000055 	 loss = 1.6513(0.3197)
2023/11/23 11:05:32 - INFO - root -   Epoch: [258/300] 	 loss = 0.3183
2023/11/23 11:05:32 - INFO - root -   train_accuracy = 0.8975
2023/11/23 11:05:33 - INFO - root -   Epoch: [259/300][0/283], lr: 0.00000055 	 loss = 0.0953(0.0953)
2023/11/23 11:05:54 - INFO - root -   Epoch: [259/300][20/283], lr: 0.00000055 	 loss = 1.1374(0.5558)
2023/11/23 11:06:11 - INFO - root -   Epoch: [259/300][40/283], lr: 0.00000055 	 loss = 0.1263(0.5559)
2023/11/23 11:06:19 - INFO - root -   Epoch: [259/300][60/283], lr: 0.00000055 	 loss = 0.0340(0.4474)
2023/11/23 11:06:35 - INFO - root -   Epoch: [259/300][80/283], lr: 0.00000055 	 loss = 0.4963(0.4586)
2023/11/23 11:06:43 - INFO - root -   Epoch: [259/300][100/283], lr: 0.00000055 	 loss = 0.0918(0.4575)
2023/11/23 11:06:53 - INFO - root -   Epoch: [259/300][120/283], lr: 0.00000055 	 loss = 0.0399(0.4039)
2023/11/23 11:07:04 - INFO - root -   Epoch: [259/300][140/283], lr: 0.00000055 	 loss = 0.2381(0.3749)
2023/11/23 11:07:17 - INFO - root -   Epoch: [259/300][160/283], lr: 0.00000055 	 loss = 0.5784(0.3761)
2023/11/23 11:07:30 - INFO - root -   Epoch: [259/300][180/283], lr: 0.00000055 	 loss = 0.1807(0.3954)
2023/11/23 11:07:39 - INFO - root -   Epoch: [259/300][200/283], lr: 0.00000055 	 loss = 0.3921(0.3870)
2023/11/23 11:08:00 - INFO - root -   Epoch: [259/300][220/283], lr: 0.00000055 	 loss = 0.1695(0.3682)
2023/11/23 11:08:11 - INFO - root -   Epoch: [259/300][240/283], lr: 0.00000055 	 loss = 0.1328(0.3590)
2023/11/23 11:08:29 - INFO - root -   Epoch: [259/300][260/283], lr: 0.00000055 	 loss = 0.2979(0.3556)
2023/11/23 11:08:40 - INFO - root -   Epoch: [259/300][280/283], lr: 0.00000055 	 loss = 1.2831(0.3454)
2023/11/23 11:08:41 - INFO - root -   Epoch: [259/300] 	 loss = 0.3432
2023/11/23 11:11:34 - INFO - root -   precision = 0.8112
2023/11/23 11:11:34 - INFO - root -   eval_loss = 0.5417
2023/11/23 11:11:34 - INFO - root -   eval_acc = 0.8112
2023/11/23 11:11:35 - INFO - root -   train_accuracy = 0.8869
2023/11/23 11:11:36 - INFO - root -   Epoch: [260/300][0/283], lr: 0.00000055 	 loss = 0.0500(0.0500)
2023/11/23 11:12:00 - INFO - root -   Epoch: [260/300][20/283], lr: 0.00000055 	 loss = 1.8594(0.3383)
2023/11/23 11:12:12 - INFO - root -   Epoch: [260/300][40/283], lr: 0.00000055 	 loss = 0.0143(0.3877)
2023/11/23 11:12:27 - INFO - root -   Epoch: [260/300][60/283], lr: 0.00000055 	 loss = 0.0148(0.3716)
2023/11/23 11:12:43 - INFO - root -   Epoch: [260/300][80/283], lr: 0.00000055 	 loss = 0.4422(0.3661)
2023/11/23 11:12:54 - INFO - root -   Epoch: [260/300][100/283], lr: 0.00000055 	 loss = 0.0281(0.3420)
2023/11/23 11:13:10 - INFO - root -   Epoch: [260/300][120/283], lr: 0.00000055 	 loss = 0.2312(0.3342)
2023/11/23 11:13:22 - INFO - root -   Epoch: [260/300][140/283], lr: 0.00000055 	 loss = 0.0183(0.3318)
2023/11/23 11:13:41 - INFO - root -   Epoch: [260/300][160/283], lr: 0.00000055 	 loss = 0.8994(0.3288)
2023/11/23 11:13:51 - INFO - root -   Epoch: [260/300][180/283], lr: 0.00000055 	 loss = 0.0856(0.3601)
2023/11/23 11:14:11 - INFO - root -   Epoch: [260/300][200/283], lr: 0.00000055 	 loss = 0.1783(0.3500)
2023/11/23 11:14:19 - INFO - root -   Epoch: [260/300][220/283], lr: 0.00000055 	 loss = 0.0275(0.3381)
2023/11/23 11:14:28 - INFO - root -   Epoch: [260/300][240/283], lr: 0.00000055 	 loss = 0.1799(0.3324)
2023/11/23 11:14:43 - INFO - root -   Epoch: [260/300][260/283], lr: 0.00000055 	 loss = 2.5469(0.3373)
2023/11/23 11:15:00 - INFO - root -   Epoch: [260/300][280/283], lr: 0.00000055 	 loss = 2.9367(0.3307)
2023/11/23 11:15:00 - INFO - root -   Epoch: [260/300] 	 loss = 0.3287
2023/11/23 11:15:00 - INFO - root -   train_accuracy = 0.9028
2023/11/23 11:15:12 - INFO - root -   Epoch: [261/300][0/283], lr: 0.00000056 	 loss = 0.2673(0.2673)
2023/11/23 11:15:26 - INFO - root -   Epoch: [261/300][20/283], lr: 0.00000056 	 loss = 2.5799(0.3705)
2023/11/23 11:15:46 - INFO - root -   Epoch: [261/300][40/283], lr: 0.00000056 	 loss = 0.0026(0.4674)
2023/11/23 11:15:54 - INFO - root -   Epoch: [261/300][60/283], lr: 0.00000056 	 loss = 0.0019(0.4495)
2023/11/23 11:16:11 - INFO - root -   Epoch: [261/300][80/283], lr: 0.00000056 	 loss = 0.1851(0.4569)
2023/11/23 11:16:30 - INFO - root -   Epoch: [261/300][100/283], lr: 0.00000056 	 loss = 0.1823(0.4135)
2023/11/23 11:16:48 - INFO - root -   Epoch: [261/300][120/283], lr: 0.00000056 	 loss = 0.1869(0.3754)
2023/11/23 11:17:02 - INFO - root -   Epoch: [261/300][140/283], lr: 0.00000056 	 loss = 0.1223(0.3760)
2023/11/23 11:17:15 - INFO - root -   Epoch: [261/300][160/283], lr: 0.00000056 	 loss = 0.0774(0.3769)
2023/11/23 11:17:28 - INFO - root -   Epoch: [261/300][180/283], lr: 0.00000056 	 loss = 0.0068(0.3842)
2023/11/23 11:17:41 - INFO - root -   Epoch: [261/300][200/283], lr: 0.00000056 	 loss = 0.1355(0.3774)
2023/11/23 11:18:02 - INFO - root -   Epoch: [261/300][220/283], lr: 0.00000056 	 loss = 0.1055(0.3628)
2023/11/23 11:18:14 - INFO - root -   Epoch: [261/300][240/283], lr: 0.00000056 	 loss = 0.3037(0.3550)
2023/11/23 11:18:28 - INFO - root -   Epoch: [261/300][260/283], lr: 0.00000056 	 loss = 0.1098(0.3443)
2023/11/23 11:18:43 - INFO - root -   Epoch: [261/300][280/283], lr: 0.00000056 	 loss = 1.7071(0.3417)
2023/11/23 11:18:44 - INFO - root -   Epoch: [261/300] 	 loss = 0.3400
2023/11/23 11:18:44 - INFO - root -   train_accuracy = 0.8834
2023/11/23 11:18:52 - INFO - root -   Epoch: [262/300][0/283], lr: 0.00000056 	 loss = 0.1494(0.1494)
2023/11/23 11:19:08 - INFO - root -   Epoch: [262/300][20/283], lr: 0.00000056 	 loss = 0.9006(0.4974)
2023/11/23 11:19:22 - INFO - root -   Epoch: [262/300][40/283], lr: 0.00000056 	 loss = 0.1350(0.5741)
2023/11/23 11:19:45 - INFO - root -   Epoch: [262/300][60/283], lr: 0.00000056 	 loss = 0.0946(0.5538)
2023/11/23 11:19:58 - INFO - root -   Epoch: [262/300][80/283], lr: 0.00000056 	 loss = 0.0219(0.5644)
2023/11/23 11:20:16 - INFO - root -   Epoch: [262/300][100/283], lr: 0.00000056 	 loss = 0.2209(0.5227)
2023/11/23 11:20:32 - INFO - root -   Epoch: [262/300][120/283], lr: 0.00000056 	 loss = 0.0303(0.4842)
2023/11/23 11:20:49 - INFO - root -   Epoch: [262/300][140/283], lr: 0.00000056 	 loss = 0.2678(0.4523)
2023/11/23 11:21:07 - INFO - root -   Epoch: [262/300][160/283], lr: 0.00000056 	 loss = 0.4660(0.4363)
2023/11/23 11:21:20 - INFO - root -   Epoch: [262/300][180/283], lr: 0.00000056 	 loss = 0.1296(0.4440)
2023/11/23 11:21:28 - INFO - root -   Epoch: [262/300][200/283], lr: 0.00000056 	 loss = 0.0105(0.4274)
2023/11/23 11:21:44 - INFO - root -   Epoch: [262/300][220/283], lr: 0.00000056 	 loss = 0.0187(0.4130)
2023/11/23 11:22:03 - INFO - root -   Epoch: [262/300][240/283], lr: 0.00000056 	 loss = 0.1629(0.4108)
2023/11/23 11:22:12 - INFO - root -   Epoch: [262/300][260/283], lr: 0.00000056 	 loss = 2.2505(0.4017)
2023/11/23 11:22:29 - INFO - root -   Epoch: [262/300][280/283], lr: 0.00000056 	 loss = 3.4245(0.4000)
2023/11/23 11:22:30 - INFO - root -   Epoch: [262/300] 	 loss = 0.3977
2023/11/23 11:22:30 - INFO - root -   train_accuracy = 0.8710
2023/11/23 11:22:38 - INFO - root -   Epoch: [263/300][0/283], lr: 0.00000056 	 loss = 0.1487(0.1487)
2023/11/23 11:22:59 - INFO - root -   Epoch: [263/300][20/283], lr: 0.00000056 	 loss = 1.5754(0.4993)
2023/11/23 11:23:12 - INFO - root -   Epoch: [263/300][40/283], lr: 0.00000056 	 loss = 0.1107(0.4145)
2023/11/23 11:23:32 - INFO - root -   Epoch: [263/300][60/283], lr: 0.00000056 	 loss = 0.0021(0.3508)
2023/11/23 11:23:42 - INFO - root -   Epoch: [263/300][80/283], lr: 0.00000056 	 loss = 0.0241(0.3414)
2023/11/23 11:23:54 - INFO - root -   Epoch: [263/300][100/283], lr: 0.00000056 	 loss = 0.0301(0.3406)
2023/11/23 11:24:12 - INFO - root -   Epoch: [263/300][120/283], lr: 0.00000056 	 loss = 0.0142(0.3175)
2023/11/23 11:24:24 - INFO - root -   Epoch: [263/300][140/283], lr: 0.00000056 	 loss = 0.0136(0.2936)
2023/11/23 11:24:38 - INFO - root -   Epoch: [263/300][160/283], lr: 0.00000056 	 loss = 0.0327(0.2944)
2023/11/23 11:24:50 - INFO - root -   Epoch: [263/300][180/283], lr: 0.00000056 	 loss = 0.1138(0.2903)
2023/11/23 11:25:02 - INFO - root -   Epoch: [263/300][200/283], lr: 0.00000056 	 loss = 0.0090(0.2970)
2023/11/23 11:25:16 - INFO - root -   Epoch: [263/300][220/283], lr: 0.00000056 	 loss = 0.0056(0.2814)
2023/11/23 11:25:34 - INFO - root -   Epoch: [263/300][240/283], lr: 0.00000056 	 loss = 1.5833(0.2988)
2023/11/23 11:25:51 - INFO - root -   Epoch: [263/300][260/283], lr: 0.00000056 	 loss = 0.9163(0.2975)
2023/11/23 11:25:59 - INFO - root -   Epoch: [263/300][280/283], lr: 0.00000056 	 loss = 1.9297(0.2911)
2023/11/23 11:26:00 - INFO - root -   Epoch: [263/300] 	 loss = 0.2896
2023/11/23 11:26:00 - INFO - root -   train_accuracy = 0.8993
2023/11/23 11:26:01 - INFO - root -   Epoch: [264/300][0/283], lr: 0.00000056 	 loss = 0.0161(0.0161)
2023/11/23 11:26:28 - INFO - root -   Epoch: [264/300][20/283], lr: 0.00000056 	 loss = 0.5582(0.5443)
2023/11/23 11:26:42 - INFO - root -   Epoch: [264/300][40/283], lr: 0.00000056 	 loss = 0.0137(0.4540)
2023/11/23 11:26:53 - INFO - root -   Epoch: [264/300][60/283], lr: 0.00000056 	 loss = 0.0885(0.4474)
2023/11/23 11:27:06 - INFO - root -   Epoch: [264/300][80/283], lr: 0.00000056 	 loss = 0.2342(0.4310)
2023/11/23 11:27:21 - INFO - root -   Epoch: [264/300][100/283], lr: 0.00000056 	 loss = 0.1302(0.4374)
2023/11/23 11:27:36 - INFO - root -   Epoch: [264/300][120/283], lr: 0.00000056 	 loss = 0.1108(0.4038)
2023/11/23 11:27:46 - INFO - root -   Epoch: [264/300][140/283], lr: 0.00000056 	 loss = 0.0160(0.3677)
2023/11/23 11:28:02 - INFO - root -   Epoch: [264/300][160/283], lr: 0.00000056 	 loss = 1.7561(0.3760)
2023/11/23 11:28:15 - INFO - root -   Epoch: [264/300][180/283], lr: 0.00000056 	 loss = 0.0110(0.3640)
2023/11/23 11:28:31 - INFO - root -   Epoch: [264/300][200/283], lr: 0.00000056 	 loss = 0.0571(0.3604)
2023/11/23 11:28:39 - INFO - root -   Epoch: [264/300][220/283], lr: 0.00000056 	 loss = 0.0327(0.3441)
2023/11/23 11:29:01 - INFO - root -   Epoch: [264/300][240/283], lr: 0.00000056 	 loss = 0.1637(0.3371)
2023/11/23 11:29:12 - INFO - root -   Epoch: [264/300][260/283], lr: 0.00000056 	 loss = 1.5803(0.3394)
2023/11/23 11:29:23 - INFO - root -   Epoch: [264/300][280/283], lr: 0.00000056 	 loss = 2.9091(0.3481)
2023/11/23 11:29:24 - INFO - root -   Epoch: [264/300] 	 loss = 0.3472
2023/11/23 11:32:18 - INFO - root -   precision = 0.8392
2023/11/23 11:32:18 - INFO - root -   eval_loss = 0.5893
2023/11/23 11:32:18 - INFO - root -   eval_acc = 0.8392
2023/11/23 11:32:19 - INFO - root -   train_accuracy = 0.8799
2023/11/23 11:32:28 - INFO - root -   Epoch: [265/300][0/283], lr: 0.00000056 	 loss = 0.2862(0.2862)
2023/11/23 11:32:43 - INFO - root -   Epoch: [265/300][20/283], lr: 0.00000056 	 loss = 2.0597(0.5467)
2023/11/23 11:32:55 - INFO - root -   Epoch: [265/300][40/283], lr: 0.00000056 	 loss = 0.0291(0.4589)
2023/11/23 11:33:08 - INFO - root -   Epoch: [265/300][60/283], lr: 0.00000056 	 loss = 0.2571(0.3994)
2023/11/23 11:33:28 - INFO - root -   Epoch: [265/300][80/283], lr: 0.00000056 	 loss = 0.0285(0.4618)
2023/11/23 11:33:49 - INFO - root -   Epoch: [265/300][100/283], lr: 0.00000056 	 loss = 0.2603(0.4520)
2023/11/23 11:34:00 - INFO - root -   Epoch: [265/300][120/283], lr: 0.00000056 	 loss = 0.0317(0.4062)
2023/11/23 11:34:08 - INFO - root -   Epoch: [265/300][140/283], lr: 0.00000056 	 loss = 0.0064(0.3654)
2023/11/23 11:34:30 - INFO - root -   Epoch: [265/300][160/283], lr: 0.00000056 	 loss = 0.3492(0.3563)
2023/11/23 11:34:46 - INFO - root -   Epoch: [265/300][180/283], lr: 0.00000056 	 loss = 0.0630(0.3735)
2023/11/23 11:35:04 - INFO - root -   Epoch: [265/300][200/283], lr: 0.00000056 	 loss = 0.0087(0.3685)
2023/11/23 11:35:15 - INFO - root -   Epoch: [265/300][220/283], lr: 0.00000056 	 loss = 0.0065(0.3618)
2023/11/23 11:35:32 - INFO - root -   Epoch: [265/300][240/283], lr: 0.00000056 	 loss = 2.7385(0.3598)
2023/11/23 11:35:53 - INFO - root -   Epoch: [265/300][260/283], lr: 0.00000056 	 loss = 0.5428(0.3618)
2023/11/23 11:36:08 - INFO - root -   Epoch: [265/300][280/283], lr: 0.00000056 	 loss = 1.8937(0.3502)
2023/11/23 11:36:09 - INFO - root -   Epoch: [265/300] 	 loss = 0.3481
2023/11/23 11:36:09 - INFO - root -   train_accuracy = 0.8922
2023/11/23 11:36:17 - INFO - root -   Epoch: [266/300][0/283], lr: 0.00000057 	 loss = 0.2411(0.2411)
2023/11/23 11:36:30 - INFO - root -   Epoch: [266/300][20/283], lr: 0.00000057 	 loss = 2.8224(0.4576)
2023/11/23 11:36:44 - INFO - root -   Epoch: [266/300][40/283], lr: 0.00000057 	 loss = 0.0225(0.4459)
2023/11/23 11:36:59 - INFO - root -   Epoch: [266/300][60/283], lr: 0.00000057 	 loss = 0.0929(0.4285)
2023/11/23 11:37:18 - INFO - root -   Epoch: [266/300][80/283], lr: 0.00000057 	 loss = 0.2325(0.4249)
2023/11/23 11:37:31 - INFO - root -   Epoch: [266/300][100/283], lr: 0.00000057 	 loss = 0.3408(0.3815)
2023/11/23 11:37:41 - INFO - root -   Epoch: [266/300][120/283], lr: 0.00000057 	 loss = 0.1961(0.3382)
2023/11/23 11:37:52 - INFO - root -   Epoch: [266/300][140/283], lr: 0.00000057 	 loss = 0.1587(0.3255)
2023/11/23 11:38:07 - INFO - root -   Epoch: [266/300][160/283], lr: 0.00000057 	 loss = 0.2347(0.3257)
2023/11/23 11:38:29 - INFO - root -   Epoch: [266/300][180/283], lr: 0.00000057 	 loss = 0.0148(0.3433)
2023/11/23 11:38:40 - INFO - root -   Epoch: [266/300][200/283], lr: 0.00000057 	 loss = 0.0074(0.3410)
2023/11/23 11:38:54 - INFO - root -   Epoch: [266/300][220/283], lr: 0.00000057 	 loss = 0.2952(0.3365)
2023/11/23 11:39:07 - INFO - root -   Epoch: [266/300][240/283], lr: 0.00000057 	 loss = 0.1911(0.3375)
2023/11/23 11:39:26 - INFO - root -   Epoch: [266/300][260/283], lr: 0.00000057 	 loss = 0.0272(0.3279)
2023/11/23 11:39:41 - INFO - root -   Epoch: [266/300][280/283], lr: 0.00000057 	 loss = 1.4745(0.3246)
2023/11/23 11:39:42 - INFO - root -   Epoch: [266/300] 	 loss = 0.3227
2023/11/23 11:39:42 - INFO - root -   train_accuracy = 0.8975
2023/11/23 11:39:49 - INFO - root -   Epoch: [267/300][0/283], lr: 0.00000057 	 loss = 0.2001(0.2001)
2023/11/23 11:40:05 - INFO - root -   Epoch: [267/300][20/283], lr: 0.00000057 	 loss = 0.0358(0.5378)
2023/11/23 11:40:17 - INFO - root -   Epoch: [267/300][40/283], lr: 0.00000057 	 loss = 0.0124(0.4662)
2023/11/23 11:40:37 - INFO - root -   Epoch: [267/300][60/283], lr: 0.00000057 	 loss = 0.0637(0.4705)
2023/11/23 11:40:53 - INFO - root -   Epoch: [267/300][80/283], lr: 0.00000057 	 loss = 0.0292(0.4871)
2023/11/23 11:41:09 - INFO - root -   Epoch: [267/300][100/283], lr: 0.00000057 	 loss = 0.0072(0.4375)
2023/11/23 11:41:21 - INFO - root -   Epoch: [267/300][120/283], lr: 0.00000057 	 loss = 0.0149(0.3965)
2023/11/23 11:41:33 - INFO - root -   Epoch: [267/300][140/283], lr: 0.00000057 	 loss = 0.0361(0.3894)
2023/11/23 11:41:51 - INFO - root -   Epoch: [267/300][160/283], lr: 0.00000057 	 loss = 1.0226(0.4029)
2023/11/23 11:42:07 - INFO - root -   Epoch: [267/300][180/283], lr: 0.00000057 	 loss = 0.0114(0.3822)
2023/11/23 11:42:17 - INFO - root -   Epoch: [267/300][200/283], lr: 0.00000057 	 loss = 0.1170(0.3699)
2023/11/23 11:42:34 - INFO - root -   Epoch: [267/300][220/283], lr: 0.00000057 	 loss = 0.2158(0.3604)
2023/11/23 11:42:47 - INFO - root -   Epoch: [267/300][240/283], lr: 0.00000057 	 loss = 0.0430(0.3608)
2023/11/23 11:43:07 - INFO - root -   Epoch: [267/300][260/283], lr: 0.00000057 	 loss = 0.2853(0.3452)
2023/11/23 11:43:16 - INFO - root -   Epoch: [267/300][280/283], lr: 0.00000057 	 loss = 2.9206(0.3379)
2023/11/23 11:43:17 - INFO - root -   Epoch: [267/300] 	 loss = 0.3356
2023/11/23 11:43:17 - INFO - root -   train_accuracy = 0.8905
2023/11/23 11:43:26 - INFO - root -   Epoch: [268/300][0/283], lr: 0.00000057 	 loss = 0.0153(0.0153)
2023/11/23 11:43:38 - INFO - root -   Epoch: [268/300][20/283], lr: 0.00000057 	 loss = 0.0740(0.2752)
2023/11/23 11:43:52 - INFO - root -   Epoch: [268/300][40/283], lr: 0.00000057 	 loss = 0.0337(0.3575)
2023/11/23 11:44:07 - INFO - root -   Epoch: [268/300][60/283], lr: 0.00000057 	 loss = 0.0050(0.4035)
2023/11/23 11:44:28 - INFO - root -   Epoch: [268/300][80/283], lr: 0.00000057 	 loss = 0.1969(0.4398)
2023/11/23 11:44:43 - INFO - root -   Epoch: [268/300][100/283], lr: 0.00000057 	 loss = 0.0296(0.4122)
2023/11/23 11:45:01 - INFO - root -   Epoch: [268/300][120/283], lr: 0.00000057 	 loss = 0.2834(0.3878)
2023/11/23 11:45:16 - INFO - root -   Epoch: [268/300][140/283], lr: 0.00000057 	 loss = 0.0098(0.3674)
2023/11/23 11:45:31 - INFO - root -   Epoch: [268/300][160/283], lr: 0.00000057 	 loss = 2.1224(0.3726)
2023/11/23 11:45:39 - INFO - root -   Epoch: [268/300][180/283], lr: 0.00000057 	 loss = 0.0072(0.3642)
2023/11/23 11:46:07 - INFO - root -   Epoch: [268/300][200/283], lr: 0.00000057 	 loss = 0.0610(0.3637)
2023/11/23 11:46:20 - INFO - root -   Epoch: [268/300][220/283], lr: 0.00000057 	 loss = 0.0690(0.3489)
2023/11/23 11:46:33 - INFO - root -   Epoch: [268/300][240/283], lr: 0.00000057 	 loss = 0.6153(0.3423)
2023/11/23 11:46:46 - INFO - root -   Epoch: [268/300][260/283], lr: 0.00000057 	 loss = 1.7957(0.3509)
2023/11/23 11:47:01 - INFO - root -   Epoch: [268/300][280/283], lr: 0.00000057 	 loss = 2.2949(0.3476)
2023/11/23 11:47:01 - INFO - root -   Epoch: [268/300] 	 loss = 0.3453
2023/11/23 11:47:01 - INFO - root -   train_accuracy = 0.8852
2023/11/23 11:47:17 - INFO - root -   Epoch: [269/300][0/283], lr: 0.00000057 	 loss = 0.1479(0.1479)
2023/11/23 11:47:25 - INFO - root -   Epoch: [269/300][20/283], lr: 0.00000057 	 loss = 0.0140(0.4354)
2023/11/23 11:47:45 - INFO - root -   Epoch: [269/300][40/283], lr: 0.00000057 	 loss = 0.0203(0.3993)
2023/11/23 11:48:03 - INFO - root -   Epoch: [269/300][60/283], lr: 0.00000057 	 loss = 0.0094(0.3994)
2023/11/23 11:48:17 - INFO - root -   Epoch: [269/300][80/283], lr: 0.00000057 	 loss = 0.2372(0.4200)
2023/11/23 11:48:33 - INFO - root -   Epoch: [269/300][100/283], lr: 0.00000057 	 loss = 0.9414(0.4124)
2023/11/23 11:48:49 - INFO - root -   Epoch: [269/300][120/283], lr: 0.00000057 	 loss = 0.0528(0.3762)
2023/11/23 11:49:03 - INFO - root -   Epoch: [269/300][140/283], lr: 0.00000057 	 loss = 0.0249(0.3502)
2023/11/23 11:49:11 - INFO - root -   Epoch: [269/300][160/283], lr: 0.00000057 	 loss = 0.3557(0.3367)
2023/11/23 11:49:27 - INFO - root -   Epoch: [269/300][180/283], lr: 0.00000057 	 loss = 0.2107(0.3359)
2023/11/23 11:49:45 - INFO - root -   Epoch: [269/300][200/283], lr: 0.00000057 	 loss = 0.0716(0.3406)
2023/11/23 11:50:02 - INFO - root -   Epoch: [269/300][220/283], lr: 0.00000057 	 loss = 0.0239(0.3302)
2023/11/23 11:50:14 - INFO - root -   Epoch: [269/300][240/283], lr: 0.00000057 	 loss = 0.0855(0.3401)
2023/11/23 11:50:33 - INFO - root -   Epoch: [269/300][260/283], lr: 0.00000057 	 loss = 0.6239(0.3278)
2023/11/23 11:50:48 - INFO - root -   Epoch: [269/300][280/283], lr: 0.00000057 	 loss = 2.0535(0.3219)
2023/11/23 11:50:49 - INFO - root -   Epoch: [269/300] 	 loss = 0.3201
2023/11/23 11:53:43 - INFO - root -   precision = 0.8252
2023/11/23 11:53:43 - INFO - root -   eval_loss = 0.5858
2023/11/23 11:53:43 - INFO - root -   eval_acc = 0.8252
2023/11/23 11:53:44 - INFO - root -   train_accuracy = 0.8852
2023/11/23 11:53:45 - INFO - root -   Epoch: [270/300][0/283], lr: 0.00000057 	 loss = 0.0261(0.0261)
2023/11/23 11:54:08 - INFO - root -   Epoch: [270/300][20/283], lr: 0.00000057 	 loss = 0.4445(0.5434)
2023/11/23 11:54:24 - INFO - root -   Epoch: [270/300][40/283], lr: 0.00000057 	 loss = 0.5789(0.5261)
2023/11/23 11:54:36 - INFO - root -   Epoch: [270/300][60/283], lr: 0.00000057 	 loss = 0.1043(0.4544)
2023/11/23 11:54:57 - INFO - root -   Epoch: [270/300][80/283], lr: 0.00000057 	 loss = 0.2094(0.4848)
2023/11/23 11:55:17 - INFO - root -   Epoch: [270/300][100/283], lr: 0.00000057 	 loss = 0.2836(0.4419)
2023/11/23 11:55:31 - INFO - root -   Epoch: [270/300][120/283], lr: 0.00000057 	 loss = 0.1793(0.4015)
2023/11/23 11:55:44 - INFO - root -   Epoch: [270/300][140/283], lr: 0.00000057 	 loss = 0.2905(0.3856)
2023/11/23 11:56:06 - INFO - root -   Epoch: [270/300][160/283], lr: 0.00000057 	 loss = 2.2414(0.3740)
2023/11/23 11:56:23 - INFO - root -   Epoch: [270/300][180/283], lr: 0.00000057 	 loss = 0.0870(0.3641)
2023/11/23 11:56:33 - INFO - root -   Epoch: [270/300][200/283], lr: 0.00000057 	 loss = 0.0442(0.3564)
2023/11/23 11:56:44 - INFO - root -   Epoch: [270/300][220/283], lr: 0.00000057 	 loss = 0.0302(0.3479)
2023/11/23 11:57:06 - INFO - root -   Epoch: [270/300][240/283], lr: 0.00000057 	 loss = 0.0505(0.3596)
2023/11/23 11:57:14 - INFO - root -   Epoch: [270/300][260/283], lr: 0.00000057 	 loss = 2.2744(0.3519)
2023/11/23 11:57:29 - INFO - root -   Epoch: [270/300][280/283], lr: 0.00000057 	 loss = 2.4016(0.3455)
2023/11/23 11:57:30 - INFO - root -   Epoch: [270/300] 	 loss = 0.3431
2023/11/23 11:57:30 - INFO - root -   train_accuracy = 0.8763
2023/11/23 11:57:35 - INFO - root -   Epoch: [271/300][0/283], lr: 0.00000057 	 loss = 0.4125(0.4125)
2023/11/23 11:57:57 - INFO - root -   Epoch: [271/300][20/283], lr: 0.00000057 	 loss = 0.1315(0.4748)
2023/11/23 11:58:06 - INFO - root -   Epoch: [271/300][40/283], lr: 0.00000057 	 loss = 0.0071(0.4853)
2023/11/23 11:58:16 - INFO - root -   Epoch: [271/300][60/283], lr: 0.00000057 	 loss = 0.0064(0.4417)
2023/11/23 11:58:31 - INFO - root -   Epoch: [271/300][80/283], lr: 0.00000057 	 loss = 0.0791(0.4551)
2023/11/23 11:58:46 - INFO - root -   Epoch: [271/300][100/283], lr: 0.00000057 	 loss = 0.1598(0.4181)
2023/11/23 11:59:02 - INFO - root -   Epoch: [271/300][120/283], lr: 0.00000057 	 loss = 0.0193(0.3762)
2023/11/23 11:59:18 - INFO - root -   Epoch: [271/300][140/283], lr: 0.00000057 	 loss = 0.0021(0.3629)
2023/11/23 11:59:31 - INFO - root -   Epoch: [271/300][160/283], lr: 0.00000057 	 loss = 0.2719(0.3494)
2023/11/23 11:59:48 - INFO - root -   Epoch: [271/300][180/283], lr: 0.00000057 	 loss = 0.0071(0.3650)
2023/11/23 12:00:01 - INFO - root -   Epoch: [271/300][200/283], lr: 0.00000057 	 loss = 0.1084(0.3586)
2023/11/23 12:00:18 - INFO - root -   Epoch: [271/300][220/283], lr: 0.00000057 	 loss = 0.0131(0.3422)
2023/11/23 12:00:33 - INFO - root -   Epoch: [271/300][240/283], lr: 0.00000057 	 loss = 2.6011(0.3487)
2023/11/23 12:00:44 - INFO - root -   Epoch: [271/300][260/283], lr: 0.00000057 	 loss = 0.4927(0.3390)
2023/11/23 12:00:52 - INFO - root -   Epoch: [271/300][280/283], lr: 0.00000057 	 loss = 2.9576(0.3283)
2023/11/23 12:00:53 - INFO - root -   Epoch: [271/300] 	 loss = 0.3268
2023/11/23 12:00:53 - INFO - root -   train_accuracy = 0.8887
2023/11/23 12:01:03 - INFO - root -   Epoch: [272/300][0/283], lr: 0.00000058 	 loss = 0.2308(0.2308)
2023/11/23 12:01:18 - INFO - root -   Epoch: [272/300][20/283], lr: 0.00000058 	 loss = 0.0923(0.3396)
2023/11/23 12:01:42 - INFO - root -   Epoch: [272/300][40/283], lr: 0.00000058 	 loss = 0.1703(0.4260)
2023/11/23 12:01:51 - INFO - root -   Epoch: [272/300][60/283], lr: 0.00000058 	 loss = 0.0494(0.4254)
2023/11/23 12:02:12 - INFO - root -   Epoch: [272/300][80/283], lr: 0.00000058 	 loss = 0.1683(0.4380)
2023/11/23 12:02:24 - INFO - root -   Epoch: [272/300][100/283], lr: 0.00000058 	 loss = 0.0506(0.3921)
2023/11/23 12:02:41 - INFO - root -   Epoch: [272/300][120/283], lr: 0.00000058 	 loss = 0.0587(0.3531)
2023/11/23 12:03:07 - INFO - root -   Epoch: [272/300][140/283], lr: 0.00000058 	 loss = 0.1489(0.3383)
2023/11/23 12:03:21 - INFO - root -   Epoch: [272/300][160/283], lr: 0.00000058 	 loss = 1.5971(0.3416)
2023/11/23 12:03:33 - INFO - root -   Epoch: [272/300][180/283], lr: 0.00000058 	 loss = 0.0613(0.3346)
2023/11/23 12:03:50 - INFO - root -   Epoch: [272/300][200/283], lr: 0.00000058 	 loss = 0.0118(0.3284)
2023/11/23 12:04:04 - INFO - root -   Epoch: [272/300][220/283], lr: 0.00000058 	 loss = 0.0215(0.3160)
2023/11/23 12:04:21 - INFO - root -   Epoch: [272/300][240/283], lr: 0.00000058 	 loss = 1.1658(0.3251)
2023/11/23 12:04:45 - INFO - root -   Epoch: [272/300][260/283], lr: 0.00000058 	 loss = 1.4899(0.3285)
2023/11/23 12:04:58 - INFO - root -   Epoch: [272/300][280/283], lr: 0.00000058 	 loss = 1.6181(0.3322)
2023/11/23 12:04:59 - INFO - root -   Epoch: [272/300] 	 loss = 0.3303
2023/11/23 12:04:59 - INFO - root -   train_accuracy = 0.8887
2023/11/23 12:05:00 - INFO - root -   Epoch: [273/300][0/283], lr: 0.00000058 	 loss = 0.0643(0.0643)
2023/11/23 12:05:12 - INFO - root -   Epoch: [273/300][20/283], lr: 0.00000058 	 loss = 1.2643(0.3307)
2023/11/23 12:05:30 - INFO - root -   Epoch: [273/300][40/283], lr: 0.00000058 	 loss = 0.2728(0.4293)
2023/11/23 12:05:43 - INFO - root -   Epoch: [273/300][60/283], lr: 0.00000058 	 loss = 0.0153(0.3927)
2023/11/23 12:06:00 - INFO - root -   Epoch: [273/300][80/283], lr: 0.00000058 	 loss = 0.1763(0.4237)
2023/11/23 12:06:21 - INFO - root -   Epoch: [273/300][100/283], lr: 0.00000058 	 loss = 0.1183(0.4075)
2023/11/23 12:06:32 - INFO - root -   Epoch: [273/300][120/283], lr: 0.00000058 	 loss = 0.0280(0.3664)
2023/11/23 12:06:47 - INFO - root -   Epoch: [273/300][140/283], lr: 0.00000058 	 loss = 0.0009(0.3442)
2023/11/23 12:07:01 - INFO - root -   Epoch: [273/300][160/283], lr: 0.00000058 	 loss = 0.2192(0.3378)
2023/11/23 12:07:16 - INFO - root -   Epoch: [273/300][180/283], lr: 0.00000058 	 loss = 0.0089(0.3386)
2023/11/23 12:07:33 - INFO - root -   Epoch: [273/300][200/283], lr: 0.00000058 	 loss = 0.0628(0.3230)
2023/11/23 12:07:54 - INFO - root -   Epoch: [273/300][220/283], lr: 0.00000058 	 loss = 0.2505(0.3235)
2023/11/23 12:08:08 - INFO - root -   Epoch: [273/300][240/283], lr: 0.00000058 	 loss = 0.7314(0.3262)
2023/11/23 12:08:19 - INFO - root -   Epoch: [273/300][260/283], lr: 0.00000058 	 loss = 0.3179(0.3242)
2023/11/23 12:08:31 - INFO - root -   Epoch: [273/300][280/283], lr: 0.00000058 	 loss = 2.1610(0.3175)
2023/11/23 12:08:32 - INFO - root -   Epoch: [273/300] 	 loss = 0.3160
2023/11/23 12:08:32 - INFO - root -   train_accuracy = 0.8869
2023/11/23 12:08:33 - INFO - root -   Epoch: [274/300][0/283], lr: 0.00000058 	 loss = 0.0072(0.0072)
2023/11/23 12:08:53 - INFO - root -   Epoch: [274/300][20/283], lr: 0.00000058 	 loss = 2.1475(0.5105)
2023/11/23 12:09:07 - INFO - root -   Epoch: [274/300][40/283], lr: 0.00000058 	 loss = 0.2042(0.5017)
2023/11/23 12:09:21 - INFO - root -   Epoch: [274/300][60/283], lr: 0.00000058 	 loss = 0.0087(0.4330)
2023/11/23 12:09:29 - INFO - root -   Epoch: [274/300][80/283], lr: 0.00000058 	 loss = 0.2452(0.4144)
2023/11/23 12:09:45 - INFO - root -   Epoch: [274/300][100/283], lr: 0.00000058 	 loss = 0.4561(0.3804)
2023/11/23 12:09:57 - INFO - root -   Epoch: [274/300][120/283], lr: 0.00000058 	 loss = 0.3202(0.3587)
2023/11/23 12:10:05 - INFO - root -   Epoch: [274/300][140/283], lr: 0.00000058 	 loss = 0.0071(0.3340)
2023/11/23 12:10:18 - INFO - root -   Epoch: [274/300][160/283], lr: 0.00000058 	 loss = 0.0806(0.3284)
2023/11/23 12:10:34 - INFO - root -   Epoch: [274/300][180/283], lr: 0.00000058 	 loss = 0.3056(0.3261)
2023/11/23 12:10:47 - INFO - root -   Epoch: [274/300][200/283], lr: 0.00000058 	 loss = 0.0613(0.3379)
2023/11/23 12:11:03 - INFO - root -   Epoch: [274/300][220/283], lr: 0.00000058 	 loss = 0.0722(0.3352)
2023/11/23 12:11:16 - INFO - root -   Epoch: [274/300][240/283], lr: 0.00000058 	 loss = 0.2781(0.3336)
2023/11/23 12:11:35 - INFO - root -   Epoch: [274/300][260/283], lr: 0.00000058 	 loss = 0.1499(0.3216)
2023/11/23 12:11:43 - INFO - root -   Epoch: [274/300][280/283], lr: 0.00000058 	 loss = 1.8799(0.3120)
2023/11/23 12:11:43 - INFO - root -   Epoch: [274/300] 	 loss = 0.3104
2023/11/23 12:14:38 - INFO - root -   precision = 0.8112
2023/11/23 12:14:38 - INFO - root -   eval_loss = 0.6531
2023/11/23 12:14:38 - INFO - root -   eval_acc = 0.8112
2023/11/23 12:14:39 - INFO - root -   train_accuracy = 0.8958
2023/11/23 12:14:40 - INFO - root -   Epoch: [275/300][0/283], lr: 0.00000058 	 loss = 0.0625(0.0625)
2023/11/23 12:14:58 - INFO - root -   Epoch: [275/300][20/283], lr: 0.00000058 	 loss = 0.2776(0.4339)
2023/11/23 12:15:14 - INFO - root -   Epoch: [275/300][40/283], lr: 0.00000058 	 loss = 0.1586(0.4259)
2023/11/23 12:15:34 - INFO - root -   Epoch: [275/300][60/283], lr: 0.00000058 	 loss = 0.3097(0.4372)
2023/11/23 12:15:50 - INFO - root -   Epoch: [275/300][80/283], lr: 0.00000058 	 loss = 0.0707(0.4329)
2023/11/23 12:16:01 - INFO - root -   Epoch: [275/300][100/283], lr: 0.00000058 	 loss = 0.0076(0.4024)
2023/11/23 12:16:19 - INFO - root -   Epoch: [275/300][120/283], lr: 0.00000058 	 loss = 0.0036(0.4105)
2023/11/23 12:16:33 - INFO - root -   Epoch: [275/300][140/283], lr: 0.00000058 	 loss = 0.1142(0.3800)
2023/11/23 12:16:49 - INFO - root -   Epoch: [275/300][160/283], lr: 0.00000058 	 loss = 2.3304(0.3868)
2023/11/23 12:17:05 - INFO - root -   Epoch: [275/300][180/283], lr: 0.00000058 	 loss = 0.0548(0.3797)
2023/11/23 12:17:25 - INFO - root -   Epoch: [275/300][200/283], lr: 0.00000058 	 loss = 0.0981(0.3725)
2023/11/23 12:17:35 - INFO - root -   Epoch: [275/300][220/283], lr: 0.00000058 	 loss = 0.3122(0.3669)
2023/11/23 12:17:49 - INFO - root -   Epoch: [275/300][240/283], lr: 0.00000058 	 loss = 0.0252(0.3620)
2023/11/23 12:18:11 - INFO - root -   Epoch: [275/300][260/283], lr: 0.00000058 	 loss = 1.4434(0.3583)
2023/11/23 12:18:23 - INFO - root -   Epoch: [275/300][280/283], lr: 0.00000058 	 loss = 0.5192(0.3435)
2023/11/23 12:18:24 - INFO - root -   Epoch: [275/300] 	 loss = 0.3427
2023/11/23 12:18:24 - INFO - root -   train_accuracy = 0.8940
2023/11/23 12:18:32 - INFO - root -   Epoch: [276/300][0/283], lr: 0.00000058 	 loss = 0.1217(0.1217)
2023/11/23 12:18:54 - INFO - root -   Epoch: [276/300][20/283], lr: 0.00000058 	 loss = 2.5790(0.6153)
2023/11/23 12:19:07 - INFO - root -   Epoch: [276/300][40/283], lr: 0.00000058 	 loss = 0.1126(0.5710)
2023/11/23 12:19:26 - INFO - root -   Epoch: [276/300][60/283], lr: 0.00000058 	 loss = 0.0061(0.5745)
2023/11/23 12:19:39 - INFO - root -   Epoch: [276/300][80/283], lr: 0.00000058 	 loss = 0.0521(0.5209)
2023/11/23 12:19:59 - INFO - root -   Epoch: [276/300][100/283], lr: 0.00000058 	 loss = 0.0133(0.4879)
2023/11/23 12:20:08 - INFO - root -   Epoch: [276/300][120/283], lr: 0.00000058 	 loss = 0.1502(0.4623)
2023/11/23 12:20:22 - INFO - root -   Epoch: [276/300][140/283], lr: 0.00000058 	 loss = 0.1934(0.4218)
2023/11/23 12:20:44 - INFO - root -   Epoch: [276/300][160/283], lr: 0.00000058 	 loss = 1.9823(0.4186)
2023/11/23 12:21:04 - INFO - root -   Epoch: [276/300][180/283], lr: 0.00000058 	 loss = 0.0490(0.4306)
2023/11/23 12:21:23 - INFO - root -   Epoch: [276/300][200/283], lr: 0.00000058 	 loss = 0.1887(0.4440)
2023/11/23 12:21:47 - INFO - root -   Epoch: [276/300][220/283], lr: 0.00000058 	 loss = 0.1655(0.4249)
2023/11/23 12:21:57 - INFO - root -   Epoch: [276/300][240/283], lr: 0.00000058 	 loss = 0.2360(0.4181)
2023/11/23 12:22:05 - INFO - root -   Epoch: [276/300][260/283], lr: 0.00000058 	 loss = 0.5611(0.3986)
2023/11/23 12:22:15 - INFO - root -   Epoch: [276/300][280/283], lr: 0.00000058 	 loss = 0.7435(0.3793)
2023/11/23 12:22:16 - INFO - root -   Epoch: [276/300] 	 loss = 0.3776
2023/11/23 12:22:16 - INFO - root -   train_accuracy = 0.8622
2023/11/23 12:22:31 - INFO - root -   Epoch: [277/300][0/283], lr: 0.00000058 	 loss = 0.0902(0.0902)
2023/11/23 12:22:43 - INFO - root -   Epoch: [277/300][20/283], lr: 0.00000058 	 loss = 0.3459(0.4589)
2023/11/23 12:23:05 - INFO - root -   Epoch: [277/300][40/283], lr: 0.00000058 	 loss = 0.0138(0.4806)
2023/11/23 12:23:23 - INFO - root -   Epoch: [277/300][60/283], lr: 0.00000058 	 loss = 0.0150(0.4210)
2023/11/23 12:23:39 - INFO - root -   Epoch: [277/300][80/283], lr: 0.00000058 	 loss = 0.0278(0.4082)
2023/11/23 12:23:58 - INFO - root -   Epoch: [277/300][100/283], lr: 0.00000058 	 loss = 0.0560(0.4130)
2023/11/23 12:24:14 - INFO - root -   Epoch: [277/300][120/283], lr: 0.00000058 	 loss = 0.0082(0.3876)
2023/11/23 12:24:28 - INFO - root -   Epoch: [277/300][140/283], lr: 0.00000058 	 loss = 0.0110(0.3582)
2023/11/23 12:24:42 - INFO - root -   Epoch: [277/300][160/283], lr: 0.00000058 	 loss = 0.4703(0.3493)
2023/11/23 12:24:55 - INFO - root -   Epoch: [277/300][180/283], lr: 0.00000058 	 loss = 0.0499(0.3537)
2023/11/23 12:25:08 - INFO - root -   Epoch: [277/300][200/283], lr: 0.00000058 	 loss = 0.0131(0.3408)
2023/11/23 12:25:20 - INFO - root -   Epoch: [277/300][220/283], lr: 0.00000058 	 loss = 0.0068(0.3234)
2023/11/23 12:25:41 - INFO - root -   Epoch: [277/300][240/283], lr: 0.00000058 	 loss = 0.2659(0.3383)
2023/11/23 12:26:01 - INFO - root -   Epoch: [277/300][260/283], lr: 0.00000058 	 loss = 0.9755(0.3302)
2023/11/23 12:26:14 - INFO - root -   Epoch: [277/300][280/283], lr: 0.00000058 	 loss = 2.1259(0.3301)
2023/11/23 12:26:15 - INFO - root -   Epoch: [277/300] 	 loss = 0.3289
2023/11/23 12:26:15 - INFO - root -   train_accuracy = 0.8763
2023/11/23 12:26:16 - INFO - root -   Epoch: [278/300][0/283], lr: 0.00000059 	 loss = 0.0727(0.0727)
2023/11/23 12:26:36 - INFO - root -   Epoch: [278/300][20/283], lr: 0.00000059 	 loss = 0.1558(0.5294)
2023/11/23 12:26:53 - INFO - root -   Epoch: [278/300][40/283], lr: 0.00000059 	 loss = 0.0051(0.5760)
2023/11/23 12:27:08 - INFO - root -   Epoch: [278/300][60/283], lr: 0.00000059 	 loss = 0.0044(0.5034)
2023/11/23 12:27:27 - INFO - root -   Epoch: [278/300][80/283], lr: 0.00000059 	 loss = 0.0970(0.5180)
2023/11/23 12:27:42 - INFO - root -   Epoch: [278/300][100/283], lr: 0.00000059 	 loss = 0.1110(0.4609)
2023/11/23 12:27:58 - INFO - root -   Epoch: [278/300][120/283], lr: 0.00000059 	 loss = 0.1855(0.4252)
2023/11/23 12:28:12 - INFO - root -   Epoch: [278/300][140/283], lr: 0.00000059 	 loss = 0.0070(0.3838)
2023/11/23 12:28:24 - INFO - root -   Epoch: [278/300][160/283], lr: 0.00000059 	 loss = 0.0607(0.3724)
2023/11/23 12:28:38 - INFO - root -   Epoch: [278/300][180/283], lr: 0.00000059 	 loss = 0.2029(0.3705)
2023/11/23 12:28:52 - INFO - root -   Epoch: [278/300][200/283], lr: 0.00000059 	 loss = 0.0134(0.3481)
2023/11/23 12:29:04 - INFO - root -   Epoch: [278/300][220/283], lr: 0.00000059 	 loss = 0.0146(0.3431)
2023/11/23 12:29:20 - INFO - root -   Epoch: [278/300][240/283], lr: 0.00000059 	 loss = 0.5837(0.3385)
2023/11/23 12:29:38 - INFO - root -   Epoch: [278/300][260/283], lr: 0.00000059 	 loss = 1.7327(0.3349)
2023/11/23 12:29:52 - INFO - root -   Epoch: [278/300][280/283], lr: 0.00000059 	 loss = 3.6147(0.3389)
2023/11/23 12:29:53 - INFO - root -   Epoch: [278/300] 	 loss = 0.3370
2023/11/23 12:29:53 - INFO - root -   train_accuracy = 0.8781
2023/11/23 12:30:06 - INFO - root -   Epoch: [279/300][0/283], lr: 0.00000059 	 loss = 0.1546(0.1546)
2023/11/23 12:30:14 - INFO - root -   Epoch: [279/300][20/283], lr: 0.00000059 	 loss = 0.8997(0.4426)
2023/11/23 12:30:39 - INFO - root -   Epoch: [279/300][40/283], lr: 0.00000059 	 loss = 0.1621(0.4254)
2023/11/23 12:30:52 - INFO - root -   Epoch: [279/300][60/283], lr: 0.00000059 	 loss = 0.0155(0.4253)
2023/11/23 12:31:00 - INFO - root -   Epoch: [279/300][80/283], lr: 0.00000059 	 loss = 0.3057(0.4145)
2023/11/23 12:31:17 - INFO - root -   Epoch: [279/300][100/283], lr: 0.00000059 	 loss = 0.0168(0.4085)
2023/11/23 12:31:41 - INFO - root -   Epoch: [279/300][120/283], lr: 0.00000059 	 loss = 0.4275(0.3806)
2023/11/23 12:31:59 - INFO - root -   Epoch: [279/300][140/283], lr: 0.00000059 	 loss = 0.0701(0.3590)
2023/11/23 12:32:18 - INFO - root -   Epoch: [279/300][160/283], lr: 0.00000059 	 loss = 0.0474(0.3493)
2023/11/23 12:32:34 - INFO - root -   Epoch: [279/300][180/283], lr: 0.00000059 	 loss = 0.1770(0.3784)
2023/11/23 12:32:48 - INFO - root -   Epoch: [279/300][200/283], lr: 0.00000059 	 loss = 0.1278(0.3767)
2023/11/23 12:33:00 - INFO - root -   Epoch: [279/300][220/283], lr: 0.00000059 	 loss = 0.0094(0.3580)
2023/11/23 12:33:16 - INFO - root -   Epoch: [279/300][240/283], lr: 0.00000059 	 loss = 0.1518(0.3507)
2023/11/23 12:33:35 - INFO - root -   Epoch: [279/300][260/283], lr: 0.00000059 	 loss = 3.1273(0.3626)
2023/11/23 12:33:49 - INFO - root -   Epoch: [279/300][280/283], lr: 0.00000059 	 loss = 1.3193(0.3544)
2023/11/23 12:33:50 - INFO - root -   Epoch: [279/300] 	 loss = 0.3533
2023/11/23 12:36:43 - INFO - root -   precision = 0.7902
2023/11/23 12:36:43 - INFO - root -   eval_loss = 0.6645
2023/11/23 12:36:43 - INFO - root -   eval_acc = 0.7902
2023/11/23 12:36:44 - INFO - root -   train_accuracy = 0.8604
2023/11/23 12:36:45 - INFO - root -   Epoch: [280/300][0/283], lr: 0.00000059 	 loss = 0.0219(0.0219)
2023/11/23 12:37:09 - INFO - root -   Epoch: [280/300][20/283], lr: 0.00000059 	 loss = 1.7823(0.6216)
2023/11/23 12:37:25 - INFO - root -   Epoch: [280/300][40/283], lr: 0.00000059 	 loss = 0.0772(0.4701)
2023/11/23 12:37:40 - INFO - root -   Epoch: [280/300][60/283], lr: 0.00000059 	 loss = 0.3262(0.4112)
2023/11/23 12:37:57 - INFO - root -   Epoch: [280/300][80/283], lr: 0.00000059 	 loss = 0.5762(0.4707)
2023/11/23 12:38:08 - INFO - root -   Epoch: [280/300][100/283], lr: 0.00000059 	 loss = 0.2942(0.4341)
2023/11/23 12:38:29 - INFO - root -   Epoch: [280/300][120/283], lr: 0.00000059 	 loss = 0.0040(0.4193)
2023/11/23 12:38:45 - INFO - root -   Epoch: [280/300][140/283], lr: 0.00000059 	 loss = 0.0160(0.3912)
2023/11/23 12:38:57 - INFO - root -   Epoch: [280/300][160/283], lr: 0.00000059 	 loss = 0.2823(0.3894)
2023/11/23 12:39:14 - INFO - root -   Epoch: [280/300][180/283], lr: 0.00000059 	 loss = 0.0050(0.4070)
2023/11/23 12:39:35 - INFO - root -   Epoch: [280/300][200/283], lr: 0.00000059 	 loss = 0.2409(0.4272)
2023/11/23 12:39:50 - INFO - root -   Epoch: [280/300][220/283], lr: 0.00000059 	 loss = 0.2501(0.4088)
2023/11/23 12:40:03 - INFO - root -   Epoch: [280/300][240/283], lr: 0.00000059 	 loss = 0.0374(0.3944)
2023/11/23 12:40:27 - INFO - root -   Epoch: [280/300][260/283], lr: 0.00000059 	 loss = 1.7588(0.3917)
2023/11/23 12:40:36 - INFO - root -   Epoch: [280/300][280/283], lr: 0.00000059 	 loss = 2.1468(0.3846)
2023/11/23 12:40:37 - INFO - root -   Epoch: [280/300] 	 loss = 0.3820
2023/11/23 12:40:37 - INFO - root -   train_accuracy = 0.8728
2023/11/23 12:40:38 - INFO - root -   Epoch: [281/300][0/283], lr: 0.00000059 	 loss = 0.0052(0.0052)
2023/11/23 12:41:00 - INFO - root -   Epoch: [281/300][20/283], lr: 0.00000059 	 loss = 0.4219(0.3827)
2023/11/23 12:41:15 - INFO - root -   Epoch: [281/300][40/283], lr: 0.00000059 	 loss = 0.2431(0.4500)
2023/11/23 12:41:33 - INFO - root -   Epoch: [281/300][60/283], lr: 0.00000059 	 loss = 0.2162(0.4773)
2023/11/23 12:41:49 - INFO - root -   Epoch: [281/300][80/283], lr: 0.00000059 	 loss = 0.0337(0.4676)
2023/11/23 12:42:01 - INFO - root -   Epoch: [281/300][100/283], lr: 0.00000059 	 loss = 0.1161(0.4160)
2023/11/23 12:42:18 - INFO - root -   Epoch: [281/300][120/283], lr: 0.00000059 	 loss = 0.0881(0.3789)
2023/11/23 12:42:30 - INFO - root -   Epoch: [281/300][140/283], lr: 0.00000059 	 loss = 0.0033(0.3635)
2023/11/23 12:42:43 - INFO - root -   Epoch: [281/300][160/283], lr: 0.00000059 	 loss = 0.6532(0.3638)
2023/11/23 12:42:57 - INFO - root -   Epoch: [281/300][180/283], lr: 0.00000059 	 loss = 0.1689(0.3654)
2023/11/23 12:43:13 - INFO - root -   Epoch: [281/300][200/283], lr: 0.00000059 	 loss = 0.1268(0.3472)
2023/11/23 12:43:23 - INFO - root -   Epoch: [281/300][220/283], lr: 0.00000059 	 loss = 0.1574(0.3363)
2023/11/23 12:43:36 - INFO - root -   Epoch: [281/300][240/283], lr: 0.00000059 	 loss = 0.1355(0.3332)
2023/11/23 12:43:52 - INFO - root -   Epoch: [281/300][260/283], lr: 0.00000059 	 loss = 0.3204(0.3164)
2023/11/23 12:44:00 - INFO - root -   Epoch: [281/300][280/283], lr: 0.00000059 	 loss = 2.1444(0.3088)
2023/11/23 12:44:01 - INFO - root -   Epoch: [281/300] 	 loss = 0.3073
2023/11/23 12:44:01 - INFO - root -   train_accuracy = 0.9081
2023/11/23 12:44:09 - INFO - root -   Epoch: [282/300][0/283], lr: 0.00000059 	 loss = 0.0108(0.0108)
2023/11/23 12:44:17 - INFO - root -   Epoch: [282/300][20/283], lr: 0.00000059 	 loss = 0.2510(0.2603)
2023/11/23 12:44:38 - INFO - root -   Epoch: [282/300][40/283], lr: 0.00000059 	 loss = 0.5811(0.3770)
2023/11/23 12:44:51 - INFO - root -   Epoch: [282/300][60/283], lr: 0.00000059 	 loss = 0.0165(0.3874)
2023/11/23 12:45:08 - INFO - root -   Epoch: [282/300][80/283], lr: 0.00000059 	 loss = 0.0282(0.4311)
2023/11/23 12:45:25 - INFO - root -   Epoch: [282/300][100/283], lr: 0.00000059 	 loss = 0.6353(0.4096)
2023/11/23 12:45:39 - INFO - root -   Epoch: [282/300][120/283], lr: 0.00000059 	 loss = 0.0154(0.3770)
2023/11/23 12:45:50 - INFO - root -   Epoch: [282/300][140/283], lr: 0.00000059 	 loss = 0.1070(0.3489)
2023/11/23 12:46:06 - INFO - root -   Epoch: [282/300][160/283], lr: 0.00000059 	 loss = 1.8974(0.3476)
2023/11/23 12:46:33 - INFO - root -   Epoch: [282/300][180/283], lr: 0.00000059 	 loss = 0.3289(0.3548)
2023/11/23 12:46:49 - INFO - root -   Epoch: [282/300][200/283], lr: 0.00000059 	 loss = 0.2764(0.3585)
2023/11/23 12:47:04 - INFO - root -   Epoch: [282/300][220/283], lr: 0.00000059 	 loss = 0.1931(0.3545)
2023/11/23 12:47:13 - INFO - root -   Epoch: [282/300][240/283], lr: 0.00000059 	 loss = 1.1712(0.3490)
2023/11/23 12:47:25 - INFO - root -   Epoch: [282/300][260/283], lr: 0.00000059 	 loss = 0.2353(0.3412)
2023/11/23 12:47:35 - INFO - root -   Epoch: [282/300][280/283], lr: 0.00000059 	 loss = 1.7374(0.3362)
2023/11/23 12:47:36 - INFO - root -   Epoch: [282/300] 	 loss = 0.3339
2023/11/23 12:47:36 - INFO - root -   train_accuracy = 0.8869
2023/11/23 12:47:37 - INFO - root -   Epoch: [283/300][0/283], lr: 0.00000060 	 loss = 0.0066(0.0066)
2023/11/23 12:47:58 - INFO - root -   Epoch: [283/300][20/283], lr: 0.00000060 	 loss = 1.3584(0.4532)
2023/11/23 12:48:14 - INFO - root -   Epoch: [283/300][40/283], lr: 0.00000060 	 loss = 0.0090(0.4634)
2023/11/23 12:48:31 - INFO - root -   Epoch: [283/300][60/283], lr: 0.00000060 	 loss = 0.0221(0.4272)
2023/11/23 12:48:42 - INFO - root -   Epoch: [283/300][80/283], lr: 0.00000060 	 loss = 0.0082(0.4189)
2023/11/23 12:48:55 - INFO - root -   Epoch: [283/300][100/283], lr: 0.00000060 	 loss = 0.2915(0.3891)
2023/11/23 12:49:06 - INFO - root -   Epoch: [283/300][120/283], lr: 0.00000060 	 loss = 0.0485(0.3441)
2023/11/23 12:49:22 - INFO - root -   Epoch: [283/300][140/283], lr: 0.00000060 	 loss = 0.0031(0.3404)
2023/11/23 12:49:35 - INFO - root -   Epoch: [283/300][160/283], lr: 0.00000060 	 loss = 0.3521(0.3482)
2023/11/23 12:49:48 - INFO - root -   Epoch: [283/300][180/283], lr: 0.00000060 	 loss = 0.0335(0.3427)
2023/11/23 12:50:05 - INFO - root -   Epoch: [283/300][200/283], lr: 0.00000060 	 loss = 0.1224(0.3285)
2023/11/23 12:50:25 - INFO - root -   Epoch: [283/300][220/283], lr: 0.00000060 	 loss = 0.0031(0.3168)
2023/11/23 12:50:34 - INFO - root -   Epoch: [283/300][240/283], lr: 0.00000060 	 loss = 0.0508(0.3224)
2023/11/23 12:50:54 - INFO - root -   Epoch: [283/300][260/283], lr: 0.00000060 	 loss = 0.1042(0.3174)
2023/11/23 12:51:04 - INFO - root -   Epoch: [283/300][280/283], lr: 0.00000060 	 loss = 1.2067(0.3055)
2023/11/23 12:51:05 - INFO - root -   Epoch: [283/300] 	 loss = 0.3036
2023/11/23 12:51:05 - INFO - root -   train_accuracy = 0.8922
2023/11/23 12:51:06 - INFO - root -   Epoch: [284/300][0/283], lr: 0.00000060 	 loss = 0.0041(0.0041)
2023/11/23 12:51:20 - INFO - root -   Epoch: [284/300][20/283], lr: 0.00000060 	 loss = 0.1179(0.3852)
2023/11/23 12:51:41 - INFO - root -   Epoch: [284/300][40/283], lr: 0.00000060 	 loss = 0.0077(0.4720)
2023/11/23 12:51:55 - INFO - root -   Epoch: [284/300][60/283], lr: 0.00000060 	 loss = 0.0014(0.4166)
2023/11/23 12:52:11 - INFO - root -   Epoch: [284/300][80/283], lr: 0.00000060 	 loss = 1.6748(0.4619)
2023/11/23 12:52:30 - INFO - root -   Epoch: [284/300][100/283], lr: 0.00000060 	 loss = 0.1628(0.4214)
2023/11/23 12:52:47 - INFO - root -   Epoch: [284/300][120/283], lr: 0.00000060 	 loss = 0.0101(0.3694)
2023/11/23 12:52:57 - INFO - root -   Epoch: [284/300][140/283], lr: 0.00000060 	 loss = 0.0917(0.3483)
2023/11/23 12:53:18 - INFO - root -   Epoch: [284/300][160/283], lr: 0.00000060 	 loss = 0.4405(0.3403)
2023/11/23 12:53:30 - INFO - root -   Epoch: [284/300][180/283], lr: 0.00000060 	 loss = 0.0810(0.3375)
2023/11/23 12:53:44 - INFO - root -   Epoch: [284/300][200/283], lr: 0.00000060 	 loss = 0.0924(0.3369)
2023/11/23 12:53:55 - INFO - root -   Epoch: [284/300][220/283], lr: 0.00000060 	 loss = 0.1576(0.3290)
2023/11/23 12:54:10 - INFO - root -   Epoch: [284/300][240/283], lr: 0.00000060 	 loss = 0.0421(0.3282)
2023/11/23 12:54:24 - INFO - root -   Epoch: [284/300][260/283], lr: 0.00000060 	 loss = 0.2135(0.3269)
2023/11/23 12:54:33 - INFO - root -   Epoch: [284/300][280/283], lr: 0.00000060 	 loss = 1.8181(0.3205)
2023/11/23 12:54:34 - INFO - root -   Epoch: [284/300] 	 loss = 0.3201
2023/11/23 12:57:29 - INFO - root -   precision = 0.8322
2023/11/23 12:57:29 - INFO - root -   eval_loss = 0.6464
2023/11/23 12:57:29 - INFO - root -   eval_acc = 0.8322
2023/11/23 12:57:30 - INFO - root -   train_accuracy = 0.8834
2023/11/23 12:57:38 - INFO - root -   Epoch: [285/300][0/283], lr: 0.00000060 	 loss = 0.2394(0.2394)
2023/11/23 12:57:51 - INFO - root -   Epoch: [285/300][20/283], lr: 0.00000060 	 loss = 0.6617(0.4708)
2023/11/23 12:58:05 - INFO - root -   Epoch: [285/300][40/283], lr: 0.00000060 	 loss = 0.0890(0.5382)
2023/11/23 12:58:19 - INFO - root -   Epoch: [285/300][60/283], lr: 0.00000060 	 loss = 0.0108(0.5193)
2023/11/23 12:58:38 - INFO - root -   Epoch: [285/300][80/283], lr: 0.00000060 	 loss = 0.1528(0.5486)
2023/11/23 12:58:51 - INFO - root -   Epoch: [285/300][100/283], lr: 0.00000060 	 loss = 0.5569(0.4853)
2023/11/23 12:59:04 - INFO - root -   Epoch: [285/300][120/283], lr: 0.00000060 	 loss = 0.1119(0.4366)
2023/11/23 12:59:20 - INFO - root -   Epoch: [285/300][140/283], lr: 0.00000060 	 loss = 0.0273(0.4151)
2023/11/23 12:59:33 - INFO - root -   Epoch: [285/300][160/283], lr: 0.00000060 	 loss = 0.0266(0.3871)
2023/11/23 12:59:48 - INFO - root -   Epoch: [285/300][180/283], lr: 0.00000060 	 loss = 0.1443(0.3762)
2023/11/23 13:00:03 - INFO - root -   Epoch: [285/300][200/283], lr: 0.00000060 	 loss = 0.0453(0.3630)
2023/11/23 13:00:17 - INFO - root -   Epoch: [285/300][220/283], lr: 0.00000060 	 loss = 0.2024(0.3604)
2023/11/23 13:00:37 - INFO - root -   Epoch: [285/300][240/283], lr: 0.00000060 	 loss = 0.1506(0.3467)
2023/11/23 13:00:54 - INFO - root -   Epoch: [285/300][260/283], lr: 0.00000060 	 loss = 0.1163(0.3439)
2023/11/23 13:01:04 - INFO - root -   Epoch: [285/300][280/283], lr: 0.00000060 	 loss = 1.6426(0.3293)
2023/11/23 13:01:05 - INFO - root -   Epoch: [285/300] 	 loss = 0.3275
2023/11/23 13:01:05 - INFO - root -   train_accuracy = 0.8922
2023/11/23 13:01:12 - INFO - root -   Epoch: [286/300][0/283], lr: 0.00000060 	 loss = 0.0687(0.0687)
2023/11/23 13:01:26 - INFO - root -   Epoch: [286/300][20/283], lr: 0.00000060 	 loss = 0.2030(0.6214)
2023/11/23 13:01:47 - INFO - root -   Epoch: [286/300][40/283], lr: 0.00000060 	 loss = 0.0172(0.5410)
2023/11/23 13:02:03 - INFO - root -   Epoch: [286/300][60/283], lr: 0.00000060 	 loss = 0.0492(0.4652)
2023/11/23 13:02:13 - INFO - root -   Epoch: [286/300][80/283], lr: 0.00000060 	 loss = 0.0956(0.4412)
2023/11/23 13:02:28 - INFO - root -   Epoch: [286/300][100/283], lr: 0.00000060 	 loss = 0.1744(0.3880)
2023/11/23 13:02:36 - INFO - root -   Epoch: [286/300][120/283], lr: 0.00000060 	 loss = 0.0122(0.3615)
2023/11/23 13:02:57 - INFO - root -   Epoch: [286/300][140/283], lr: 0.00000060 	 loss = 0.3580(0.3537)
2023/11/23 13:03:08 - INFO - root -   Epoch: [286/300][160/283], lr: 0.00000060 	 loss = 1.7646(0.3597)
2023/11/23 13:03:24 - INFO - root -   Epoch: [286/300][180/283], lr: 0.00000060 	 loss = 0.0454(0.3584)
2023/11/23 13:03:38 - INFO - root -   Epoch: [286/300][200/283], lr: 0.00000060 	 loss = 0.0090(0.3579)
2023/11/23 13:03:57 - INFO - root -   Epoch: [286/300][220/283], lr: 0.00000060 	 loss = 0.0150(0.3491)
2023/11/23 13:04:07 - INFO - root -   Epoch: [286/300][240/283], lr: 0.00000060 	 loss = 0.2857(0.3365)
2023/11/23 13:04:23 - INFO - root -   Epoch: [286/300][260/283], lr: 0.00000060 	 loss = 1.9523(0.3370)
2023/11/23 13:04:41 - INFO - root -   Epoch: [286/300][280/283], lr: 0.00000060 	 loss = 0.6962(0.3263)
2023/11/23 13:04:42 - INFO - root -   Epoch: [286/300] 	 loss = 0.3240
2023/11/23 13:04:42 - INFO - root -   train_accuracy = 0.8816
2023/11/23 13:04:43 - INFO - root -   Epoch: [287/300][0/283], lr: 0.00000060 	 loss = 0.0165(0.0165)
2023/11/23 13:05:00 - INFO - root -   Epoch: [287/300][20/283], lr: 0.00000060 	 loss = 0.1676(0.4981)
2023/11/23 13:05:13 - INFO - root -   Epoch: [287/300][40/283], lr: 0.00000060 	 loss = 0.1167(0.4809)
2023/11/23 13:05:32 - INFO - root -   Epoch: [287/300][60/283], lr: 0.00000060 	 loss = 0.3194(0.5015)
2023/11/23 13:05:49 - INFO - root -   Epoch: [287/300][80/283], lr: 0.00000060 	 loss = 0.1366(0.4679)
2023/11/23 13:06:09 - INFO - root -   Epoch: [287/300][100/283], lr: 0.00000060 	 loss = 0.0670(0.4520)
2023/11/23 13:06:21 - INFO - root -   Epoch: [287/300][120/283], lr: 0.00000060 	 loss = 0.0049(0.4219)
2023/11/23 13:06:34 - INFO - root -   Epoch: [287/300][140/283], lr: 0.00000060 	 loss = 0.0934(0.3870)
2023/11/23 13:06:46 - INFO - root -   Epoch: [287/300][160/283], lr: 0.00000060 	 loss = 0.9311(0.3763)
2023/11/23 13:07:01 - INFO - root -   Epoch: [287/300][180/283], lr: 0.00000060 	 loss = 0.0597(0.3731)
2023/11/23 13:07:12 - INFO - root -   Epoch: [287/300][200/283], lr: 0.00000060 	 loss = 0.2999(0.3676)
2023/11/23 13:07:27 - INFO - root -   Epoch: [287/300][220/283], lr: 0.00000060 	 loss = 0.0019(0.3468)
2023/11/23 13:07:43 - INFO - root -   Epoch: [287/300][240/283], lr: 0.00000060 	 loss = 0.0099(0.3366)
2023/11/23 13:07:55 - INFO - root -   Epoch: [287/300][260/283], lr: 0.00000060 	 loss = 0.3205(0.3327)
2023/11/23 13:08:08 - INFO - root -   Epoch: [287/300][280/283], lr: 0.00000060 	 loss = 2.6046(0.3296)
2023/11/23 13:08:09 - INFO - root -   Epoch: [287/300] 	 loss = 0.3285
2023/11/23 13:08:09 - INFO - root -   train_accuracy = 0.8834
2023/11/23 13:08:16 - INFO - root -   Epoch: [288/300][0/283], lr: 0.00000060 	 loss = 0.2485(0.2485)
2023/11/23 13:08:31 - INFO - root -   Epoch: [288/300][20/283], lr: 0.00000060 	 loss = 1.8010(0.5780)
2023/11/23 13:08:43 - INFO - root -   Epoch: [288/300][40/283], lr: 0.00000060 	 loss = 0.0347(0.4573)
2023/11/23 13:08:56 - INFO - root -   Epoch: [288/300][60/283], lr: 0.00000060 	 loss = 0.2381(0.3967)
2023/11/23 13:09:08 - INFO - root -   Epoch: [288/300][80/283], lr: 0.00000060 	 loss = 0.0773(0.4243)
2023/11/23 13:09:23 - INFO - root -   Epoch: [288/300][100/283], lr: 0.00000060 	 loss = 0.0206(0.3717)
2023/11/23 13:09:35 - INFO - root -   Epoch: [288/300][120/283], lr: 0.00000060 	 loss = 0.0657(0.3378)
2023/11/23 13:09:53 - INFO - root -   Epoch: [288/300][140/283], lr: 0.00000060 	 loss = 0.0291(0.3066)
2023/11/23 13:10:06 - INFO - root -   Epoch: [288/300][160/283], lr: 0.00000060 	 loss = 0.0985(0.2879)
2023/11/23 13:10:19 - INFO - root -   Epoch: [288/300][180/283], lr: 0.00000060 	 loss = 0.0263(0.3091)
2023/11/23 13:10:38 - INFO - root -   Epoch: [288/300][200/283], lr: 0.00000060 	 loss = 0.0114(0.3162)
2023/11/23 13:10:50 - INFO - root -   Epoch: [288/300][220/283], lr: 0.00000060 	 loss = 0.0037(0.3016)
2023/11/23 13:11:08 - INFO - root -   Epoch: [288/300][240/283], lr: 0.00000060 	 loss = 0.0576(0.2970)
2023/11/23 13:11:25 - INFO - root -   Epoch: [288/300][260/283], lr: 0.00000060 	 loss = 1.8675(0.2904)
2023/11/23 13:11:43 - INFO - root -   Epoch: [288/300][280/283], lr: 0.00000060 	 loss = 1.8805(0.2873)
2023/11/23 13:11:44 - INFO - root -   Epoch: [288/300] 	 loss = 0.2856
2023/11/23 13:11:44 - INFO - root -   train_accuracy = 0.9099
2023/11/23 13:11:52 - INFO - root -   Epoch: [289/300][0/283], lr: 0.00000061 	 loss = 0.0092(0.0092)
2023/11/23 13:12:05 - INFO - root -   Epoch: [289/300][20/283], lr: 0.00000061 	 loss = 1.3525(0.4498)
2023/11/23 13:12:22 - INFO - root -   Epoch: [289/300][40/283], lr: 0.00000061 	 loss = 0.8671(0.4761)
2023/11/23 13:12:33 - INFO - root -   Epoch: [289/300][60/283], lr: 0.00000061 	 loss = 0.0080(0.4300)
2023/11/23 13:12:46 - INFO - root -   Epoch: [289/300][80/283], lr: 0.00000061 	 loss = 0.3012(0.4225)
2023/11/23 13:13:03 - INFO - root -   Epoch: [289/300][100/283], lr: 0.00000061 	 loss = 0.0029(0.3917)
2023/11/23 13:13:21 - INFO - root -   Epoch: [289/300][120/283], lr: 0.00000061 	 loss = 0.2453(0.3666)
2023/11/23 13:13:40 - INFO - root -   Epoch: [289/300][140/283], lr: 0.00000061 	 loss = 0.0040(0.3439)
2023/11/23 13:13:58 - INFO - root -   Epoch: [289/300][160/283], lr: 0.00000061 	 loss = 0.2996(0.3437)
2023/11/23 13:14:10 - INFO - root -   Epoch: [289/300][180/283], lr: 0.00000061 	 loss = 0.0223(0.3359)
2023/11/23 13:14:32 - INFO - root -   Epoch: [289/300][200/283], lr: 0.00000061 	 loss = 0.0395(0.3427)
2023/11/23 13:14:49 - INFO - root -   Epoch: [289/300][220/283], lr: 0.00000061 	 loss = 0.0099(0.3295)
2023/11/23 13:14:59 - INFO - root -   Epoch: [289/300][240/283], lr: 0.00000061 	 loss = 2.7811(0.3412)
2023/11/23 13:15:19 - INFO - root -   Epoch: [289/300][260/283], lr: 0.00000061 	 loss = 0.0484(0.3318)
2023/11/23 13:15:34 - INFO - root -   Epoch: [289/300][280/283], lr: 0.00000061 	 loss = 0.2368(0.3269)
2023/11/23 13:15:34 - INFO - root -   Epoch: [289/300] 	 loss = 0.3260
2023/11/23 13:18:28 - INFO - root -   precision = 0.7832
2023/11/23 13:18:28 - INFO - root -   eval_loss = 0.6243
2023/11/23 13:18:28 - INFO - root -   eval_acc = 0.7832
2023/11/23 13:18:29 - INFO - root -   train_accuracy = 0.8940
2023/11/23 13:18:31 - INFO - root -   Epoch: [290/300][0/283], lr: 0.00000061 	 loss = 0.0127(0.0127)
2023/11/23 13:18:55 - INFO - root -   Epoch: [290/300][20/283], lr: 0.00000061 	 loss = 0.3052(0.4280)
2023/11/23 13:19:07 - INFO - root -   Epoch: [290/300][40/283], lr: 0.00000061 	 loss = 0.2201(0.4191)
2023/11/23 13:19:23 - INFO - root -   Epoch: [290/300][60/283], lr: 0.00000061 	 loss = 0.0069(0.4769)
2023/11/23 13:19:40 - INFO - root -   Epoch: [290/300][80/283], lr: 0.00000061 	 loss = 0.1571(0.4501)
2023/11/23 13:19:56 - INFO - root -   Epoch: [290/300][100/283], lr: 0.00000061 	 loss = 0.0457(0.4138)
2023/11/23 13:20:13 - INFO - root -   Epoch: [290/300][120/283], lr: 0.00000061 	 loss = 0.0235(0.3762)
2023/11/23 13:20:21 - INFO - root -   Epoch: [290/300][140/283], lr: 0.00000061 	 loss = 0.0019(0.3581)
2023/11/23 13:20:42 - INFO - root -   Epoch: [290/300][160/283], lr: 0.00000061 	 loss = 1.7143(0.3707)
2023/11/23 13:20:51 - INFO - root -   Epoch: [290/300][180/283], lr: 0.00000061 	 loss = 0.0397(0.3721)
2023/11/23 13:21:04 - INFO - root -   Epoch: [290/300][200/283], lr: 0.00000061 	 loss = 0.0177(0.3737)
2023/11/23 13:21:19 - INFO - root -   Epoch: [290/300][220/283], lr: 0.00000061 	 loss = 0.0709(0.3565)
2023/11/23 13:21:39 - INFO - root -   Epoch: [290/300][240/283], lr: 0.00000061 	 loss = 0.4179(0.3553)
2023/11/23 13:21:59 - INFO - root -   Epoch: [290/300][260/283], lr: 0.00000061 	 loss = 0.4592(0.3498)
2023/11/23 13:22:12 - INFO - root -   Epoch: [290/300][280/283], lr: 0.00000061 	 loss = 2.3677(0.3468)
2023/11/23 13:22:13 - INFO - root -   Epoch: [290/300] 	 loss = 0.3446
2023/11/23 13:22:13 - INFO - root -   train_accuracy = 0.8693
2023/11/23 13:22:25 - INFO - root -   Epoch: [291/300][0/283], lr: 0.00000061 	 loss = 0.1437(0.1437)
2023/11/23 13:22:33 - INFO - root -   Epoch: [291/300][20/283], lr: 0.00000061 	 loss = 1.4198(0.5338)
2023/11/23 13:22:49 - INFO - root -   Epoch: [291/300][40/283], lr: 0.00000061 	 loss = 0.0358(0.5328)
2023/11/23 13:22:59 - INFO - root -   Epoch: [291/300][60/283], lr: 0.00000061 	 loss = 0.3542(0.4705)
2023/11/23 13:23:18 - INFO - root -   Epoch: [291/300][80/283], lr: 0.00000061 	 loss = 0.0311(0.5166)
2023/11/23 13:23:38 - INFO - root -   Epoch: [291/300][100/283], lr: 0.00000061 	 loss = 0.1116(0.4810)
2023/11/23 13:23:52 - INFO - root -   Epoch: [291/300][120/283], lr: 0.00000061 	 loss = 0.0182(0.4584)
2023/11/23 13:24:13 - INFO - root -   Epoch: [291/300][140/283], lr: 0.00000061 	 loss = 0.0107(0.4370)
2023/11/23 13:24:29 - INFO - root -   Epoch: [291/300][160/283], lr: 0.00000061 	 loss = 1.2048(0.4246)
2023/11/23 13:24:47 - INFO - root -   Epoch: [291/300][180/283], lr: 0.00000061 	 loss = 0.0369(0.4353)
2023/11/23 13:25:03 - INFO - root -   Epoch: [291/300][200/283], lr: 0.00000061 	 loss = 0.1465(0.4295)
2023/11/23 13:25:17 - INFO - root -   Epoch: [291/300][220/283], lr: 0.00000061 	 loss = 0.3362(0.4046)
2023/11/23 13:25:30 - INFO - root -   Epoch: [291/300][240/283], lr: 0.00000061 	 loss = 0.3245(0.4001)
2023/11/23 13:25:41 - INFO - root -   Epoch: [291/300][260/283], lr: 0.00000061 	 loss = 0.9606(0.3856)
2023/11/23 13:25:55 - INFO - root -   Epoch: [291/300][280/283], lr: 0.00000061 	 loss = 2.4039(0.3824)
2023/11/23 13:25:55 - INFO - root -   Epoch: [291/300] 	 loss = 0.3806
2023/11/23 13:25:55 - INFO - root -   train_accuracy = 0.8622
2023/11/23 13:26:02 - INFO - root -   Epoch: [292/300][0/283], lr: 0.00000061 	 loss = 0.0985(0.0985)
2023/11/23 13:26:16 - INFO - root -   Epoch: [292/300][20/283], lr: 0.00000061 	 loss = 0.3008(0.5957)
2023/11/23 13:26:28 - INFO - root -   Epoch: [292/300][40/283], lr: 0.00000061 	 loss = 0.0119(0.5820)
2023/11/23 13:26:47 - INFO - root -   Epoch: [292/300][60/283], lr: 0.00000061 	 loss = 0.0280(0.5649)
2023/11/23 13:27:00 - INFO - root -   Epoch: [292/300][80/283], lr: 0.00000061 	 loss = 0.2436(0.4990)
2023/11/23 13:27:20 - INFO - root -   Epoch: [292/300][100/283], lr: 0.00000061 	 loss = 0.0499(0.4344)
2023/11/23 13:27:40 - INFO - root -   Epoch: [292/300][120/283], lr: 0.00000061 	 loss = 0.0072(0.3884)
2023/11/23 13:27:56 - INFO - root -   Epoch: [292/300][140/283], lr: 0.00000061 	 loss = 0.0354(0.3717)
2023/11/23 13:28:11 - INFO - root -   Epoch: [292/300][160/283], lr: 0.00000061 	 loss = 0.0164(0.3591)
2023/11/23 13:28:26 - INFO - root -   Epoch: [292/300][180/283], lr: 0.00000061 	 loss = 0.0555(0.3614)
2023/11/23 13:28:42 - INFO - root -   Epoch: [292/300][200/283], lr: 0.00000061 	 loss = 0.0757(0.3659)
2023/11/23 13:28:55 - INFO - root -   Epoch: [292/300][220/283], lr: 0.00000061 	 loss = 0.2634(0.3534)
2023/11/23 13:29:11 - INFO - root -   Epoch: [292/300][240/283], lr: 0.00000061 	 loss = 0.1014(0.3431)
2023/11/23 13:29:27 - INFO - root -   Epoch: [292/300][260/283], lr: 0.00000061 	 loss = 0.2307(0.3275)
2023/11/23 13:29:45 - INFO - root -   Epoch: [292/300][280/283], lr: 0.00000061 	 loss = 0.4357(0.3331)
2023/11/23 13:29:46 - INFO - root -   Epoch: [292/300] 	 loss = 0.3316
2023/11/23 13:29:46 - INFO - root -   train_accuracy = 0.8763
2023/11/23 13:29:54 - INFO - root -   Epoch: [293/300][0/283], lr: 0.00000061 	 loss = 0.0999(0.0999)
2023/11/23 13:30:13 - INFO - root -   Epoch: [293/300][20/283], lr: 0.00000061 	 loss = 0.7630(0.4017)
2023/11/23 13:30:25 - INFO - root -   Epoch: [293/300][40/283], lr: 0.00000061 	 loss = 0.0070(0.4195)
2023/11/23 13:30:46 - INFO - root -   Epoch: [293/300][60/283], lr: 0.00000061 	 loss = 0.0203(0.3933)
2023/11/23 13:31:09 - INFO - root -   Epoch: [293/300][80/283], lr: 0.00000061 	 loss = 0.7018(0.4266)
2023/11/23 13:31:21 - INFO - root -   Epoch: [293/300][100/283], lr: 0.00000061 	 loss = 0.0579(0.4258)
2023/11/23 13:31:35 - INFO - root -   Epoch: [293/300][120/283], lr: 0.00000061 	 loss = 0.1523(0.3905)
2023/11/23 13:31:52 - INFO - root -   Epoch: [293/300][140/283], lr: 0.00000061 	 loss = 0.2018(0.3657)
2023/11/23 13:32:03 - INFO - root -   Epoch: [293/300][160/283], lr: 0.00000061 	 loss = 0.2390(0.3551)
2023/11/23 13:32:19 - INFO - root -   Epoch: [293/300][180/283], lr: 0.00000061 	 loss = 0.1492(0.3514)
2023/11/23 13:32:32 - INFO - root -   Epoch: [293/300][200/283], lr: 0.00000061 	 loss = 0.0967(0.3642)
2023/11/23 13:32:54 - INFO - root -   Epoch: [293/300][220/283], lr: 0.00000061 	 loss = 0.0430(0.3507)
2023/11/23 13:33:05 - INFO - root -   Epoch: [293/300][240/283], lr: 0.00000061 	 loss = 0.0120(0.3536)
2023/11/23 13:33:19 - INFO - root -   Epoch: [293/300][260/283], lr: 0.00000061 	 loss = 0.2129(0.3456)
2023/11/23 13:33:33 - INFO - root -   Epoch: [293/300][280/283], lr: 0.00000061 	 loss = 1.0045(0.3324)
2023/11/23 13:33:34 - INFO - root -   Epoch: [293/300] 	 loss = 0.3310
2023/11/23 13:33:34 - INFO - root -   train_accuracy = 0.8834
2023/11/23 13:33:35 - INFO - root -   Epoch: [294/300][0/283], lr: 0.00000061 	 loss = 0.0515(0.0515)
2023/11/23 13:33:52 - INFO - root -   Epoch: [294/300][20/283], lr: 0.00000061 	 loss = 0.0178(0.4651)
2023/11/23 13:34:05 - INFO - root -   Epoch: [294/300][40/283], lr: 0.00000061 	 loss = 0.1010(0.4159)
2023/11/23 13:34:19 - INFO - root -   Epoch: [294/300][60/283], lr: 0.00000061 	 loss = 0.0022(0.4312)
2023/11/23 13:34:40 - INFO - root -   Epoch: [294/300][80/283], lr: 0.00000061 	 loss = 0.0213(0.4082)
2023/11/23 13:34:55 - INFO - root -   Epoch: [294/300][100/283], lr: 0.00000061 	 loss = 0.2283(0.4099)
2023/11/23 13:35:19 - INFO - root -   Epoch: [294/300][120/283], lr: 0.00000061 	 loss = 0.3689(0.3796)
2023/11/23 13:35:27 - INFO - root -   Epoch: [294/300][140/283], lr: 0.00000061 	 loss = 0.0034(0.3550)
2023/11/23 13:35:36 - INFO - root -   Epoch: [294/300][160/283], lr: 0.00000061 	 loss = 0.0244(0.3365)
2023/11/23 13:35:50 - INFO - root -   Epoch: [294/300][180/283], lr: 0.00000061 	 loss = 0.1466(0.3424)
2023/11/23 13:36:02 - INFO - root -   Epoch: [294/300][200/283], lr: 0.00000061 	 loss = 0.0458(0.3358)
2023/11/23 13:36:20 - INFO - root -   Epoch: [294/300][220/283], lr: 0.00000061 	 loss = 0.1633(0.3274)
2023/11/23 13:36:34 - INFO - root -   Epoch: [294/300][240/283], lr: 0.00000061 	 loss = 0.0296(0.3274)
2023/11/23 13:36:51 - INFO - root -   Epoch: [294/300][260/283], lr: 0.00000061 	 loss = 0.0361(0.3227)
2023/11/23 13:37:03 - INFO - root -   Epoch: [294/300][280/283], lr: 0.00000061 	 loss = 0.1986(0.3050)
2023/11/23 13:37:04 - INFO - root -   Epoch: [294/300] 	 loss = 0.3030
2023/11/23 13:39:56 - INFO - root -   precision = 0.7972
2023/11/23 13:39:56 - INFO - root -   eval_loss = 0.5908
2023/11/23 13:39:56 - INFO - root -   eval_acc = 0.7972
2023/11/23 13:39:57 - INFO - root -   train_accuracy = 0.8905
2023/11/23 13:40:07 - INFO - root -   Epoch: [295/300][0/283], lr: 0.00000062 	 loss = 0.0698(0.0698)
2023/11/23 13:40:23 - INFO - root -   Epoch: [295/300][20/283], lr: 0.00000062 	 loss = 0.1008(0.5257)
2023/11/23 13:40:45 - INFO - root -   Epoch: [295/300][40/283], lr: 0.00000062 	 loss = 0.1918(0.4284)
2023/11/23 13:40:58 - INFO - root -   Epoch: [295/300][60/283], lr: 0.00000062 	 loss = 0.1278(0.3685)
2023/11/23 13:41:17 - INFO - root -   Epoch: [295/300][80/283], lr: 0.00000062 	 loss = 0.2018(0.4206)
2023/11/23 13:41:38 - INFO - root -   Epoch: [295/300][100/283], lr: 0.00000062 	 loss = 0.7806(0.4055)
2023/11/23 13:41:55 - INFO - root -   Epoch: [295/300][120/283], lr: 0.00000062 	 loss = 0.0215(0.3746)
2023/11/23 13:42:10 - INFO - root -   Epoch: [295/300][140/283], lr: 0.00000062 	 loss = 0.1198(0.3485)
2023/11/23 13:42:28 - INFO - root -   Epoch: [295/300][160/283], lr: 0.00000062 	 loss = 0.0421(0.3420)
2023/11/23 13:42:44 - INFO - root -   Epoch: [295/300][180/283], lr: 0.00000062 	 loss = 0.0709(0.3352)
2023/11/23 13:42:58 - INFO - root -   Epoch: [295/300][200/283], lr: 0.00000062 	 loss = 0.0249(0.3223)
2023/11/23 13:43:13 - INFO - root -   Epoch: [295/300][220/283], lr: 0.00000062 	 loss = 0.0043(0.3111)
2023/11/23 13:43:21 - INFO - root -   Epoch: [295/300][240/283], lr: 0.00000062 	 loss = 0.0375(0.3096)
2023/11/23 13:43:44 - INFO - root -   Epoch: [295/300][260/283], lr: 0.00000062 	 loss = 1.1398(0.3032)
2023/11/23 13:43:56 - INFO - root -   Epoch: [295/300][280/283], lr: 0.00000062 	 loss = 1.2305(0.2999)
2023/11/23 13:43:56 - INFO - root -   Epoch: [295/300] 	 loss = 0.2981
2023/11/23 13:43:56 - INFO - root -   train_accuracy = 0.8940
2023/11/23 13:44:03 - INFO - root -   Epoch: [296/300][0/283], lr: 0.00000062 	 loss = 0.0509(0.0509)
2023/11/23 13:44:19 - INFO - root -   Epoch: [296/300][20/283], lr: 0.00000062 	 loss = 0.0307(0.4799)
2023/11/23 13:44:30 - INFO - root -   Epoch: [296/300][40/283], lr: 0.00000062 	 loss = 0.0093(0.4042)
2023/11/23 13:44:52 - INFO - root -   Epoch: [296/300][60/283], lr: 0.00000062 	 loss = 0.1098(0.4070)
2023/11/23 13:45:07 - INFO - root -   Epoch: [296/300][80/283], lr: 0.00000062 	 loss = 0.1435(0.4440)
2023/11/23 13:45:25 - INFO - root -   Epoch: [296/300][100/283], lr: 0.00000062 	 loss = 0.0941(0.4193)
2023/11/23 13:45:43 - INFO - root -   Epoch: [296/300][120/283], lr: 0.00000062 	 loss = 0.0198(0.3806)
2023/11/23 13:45:57 - INFO - root -   Epoch: [296/300][140/283], lr: 0.00000062 	 loss = 0.0053(0.3657)
2023/11/23 13:46:11 - INFO - root -   Epoch: [296/300][160/283], lr: 0.00000062 	 loss = 0.2897(0.3731)
2023/11/23 13:46:21 - INFO - root -   Epoch: [296/300][180/283], lr: 0.00000062 	 loss = 0.0323(0.3614)
2023/11/23 13:46:34 - INFO - root -   Epoch: [296/300][200/283], lr: 0.00000062 	 loss = 0.0756(0.3602)
2023/11/23 13:46:57 - INFO - root -   Epoch: [296/300][220/283], lr: 0.00000062 	 loss = 0.0117(0.3572)
2023/11/23 13:47:09 - INFO - root -   Epoch: [296/300][240/283], lr: 0.00000062 	 loss = 0.4930(0.3545)
2023/11/23 13:47:21 - INFO - root -   Epoch: [296/300][260/283], lr: 0.00000062 	 loss = 0.0340(0.3431)
2023/11/23 13:47:40 - INFO - root -   Epoch: [296/300][280/283], lr: 0.00000062 	 loss = 4.5418(0.3480)
2023/11/23 13:47:41 - INFO - root -   Epoch: [296/300] 	 loss = 0.3459
2023/11/23 13:47:41 - INFO - root -   train_accuracy = 0.8834
2023/11/23 13:47:50 - INFO - root -   Epoch: [297/300][0/283], lr: 0.00000062 	 loss = 0.2063(0.2063)
2023/11/23 13:47:58 - INFO - root -   Epoch: [297/300][20/283], lr: 0.00000062 	 loss = 0.1196(0.3851)
2023/11/23 13:48:15 - INFO - root -   Epoch: [297/300][40/283], lr: 0.00000062 	 loss = 0.0051(0.3972)
2023/11/23 13:48:32 - INFO - root -   Epoch: [297/300][60/283], lr: 0.00000062 	 loss = 0.0152(0.3444)
2023/11/23 13:48:44 - INFO - root -   Epoch: [297/300][80/283], lr: 0.00000062 	 loss = 0.5273(0.3613)
2023/11/23 13:49:11 - INFO - root -   Epoch: [297/300][100/283], lr: 0.00000062 	 loss = 0.0395(0.3352)
2023/11/23 13:49:30 - INFO - root -   Epoch: [297/300][120/283], lr: 0.00000062 	 loss = 0.1713(0.3222)
2023/11/23 13:49:46 - INFO - root -   Epoch: [297/300][140/283], lr: 0.00000062 	 loss = 0.1661(0.3024)
2023/11/23 13:49:57 - INFO - root -   Epoch: [297/300][160/283], lr: 0.00000062 	 loss = 0.0656(0.3135)
2023/11/23 13:50:19 - INFO - root -   Epoch: [297/300][180/283], lr: 0.00000062 	 loss = 0.2549(0.3213)
2023/11/23 13:50:43 - INFO - root -   Epoch: [297/300][200/283], lr: 0.00000062 	 loss = 0.3561(0.3169)
2023/11/23 13:51:09 - INFO - root -   Epoch: [297/300][220/283], lr: 0.00000062 	 loss = 0.0873(0.3179)
2023/11/23 13:51:24 - INFO - root -   Epoch: [297/300][240/283], lr: 0.00000062 	 loss = 0.6096(0.3116)
2023/11/23 13:51:38 - INFO - root -   Epoch: [297/300][260/283], lr: 0.00000062 	 loss = 2.0506(0.3098)
2023/11/23 13:51:46 - INFO - root -   Epoch: [297/300][280/283], lr: 0.00000062 	 loss = 1.4355(0.3074)
2023/11/23 13:51:47 - INFO - root -   Epoch: [297/300] 	 loss = 0.3059
2023/11/23 13:51:47 - INFO - root -   train_accuracy = 0.8922
2023/11/23 13:51:49 - INFO - root -   Epoch: [298/300][0/283], lr: 0.00000062 	 loss = 0.1703(0.1703)
2023/11/23 13:52:14 - INFO - root -   Epoch: [298/300][20/283], lr: 0.00000062 	 loss = 0.5236(0.3899)
2023/11/23 13:52:29 - INFO - root -   Epoch: [298/300][40/283], lr: 0.00000062 	 loss = 0.2009(0.5179)
2023/11/23 13:52:38 - INFO - root -   Epoch: [298/300][60/283], lr: 0.00000062 	 loss = 0.0032(0.5137)
2023/11/23 13:52:51 - INFO - root -   Epoch: [298/300][80/283], lr: 0.00000062 	 loss = 0.0395(0.4677)
2023/11/23 13:53:11 - INFO - root -   Epoch: [298/300][100/283], lr: 0.00000062 	 loss = 0.2396(0.4437)
2023/11/23 13:53:25 - INFO - root -   Epoch: [298/300][120/283], lr: 0.00000062 	 loss = 0.2264(0.4150)
2023/11/23 13:53:42 - INFO - root -   Epoch: [298/300][140/283], lr: 0.00000062 	 loss = 0.2552(0.3774)
2023/11/23 13:53:57 - INFO - root -   Epoch: [298/300][160/283], lr: 0.00000062 	 loss = 1.3836(0.3697)
2023/11/23 13:54:10 - INFO - root -   Epoch: [298/300][180/283], lr: 0.00000062 	 loss = 0.0576(0.3724)
2023/11/23 13:54:24 - INFO - root -   Epoch: [298/300][200/283], lr: 0.00000062 	 loss = 0.0154(0.3760)
2023/11/23 13:54:42 - INFO - root -   Epoch: [298/300][220/283], lr: 0.00000062 	 loss = 0.2717(0.3644)
2023/11/23 13:55:00 - INFO - root -   Epoch: [298/300][240/283], lr: 0.00000062 	 loss = 2.3436(0.3725)
2023/11/23 13:55:19 - INFO - root -   Epoch: [298/300][260/283], lr: 0.00000062 	 loss = 1.0789(0.3699)
2023/11/23 13:55:35 - INFO - root -   Epoch: [298/300][280/283], lr: 0.00000062 	 loss = 1.9049(0.3603)
2023/11/23 13:55:35 - INFO - root -   Epoch: [298/300] 	 loss = 0.3586
2023/11/23 13:55:35 - INFO - root -   train_accuracy = 0.8746
2023/11/23 13:55:37 - INFO - root -   Epoch: [299/300][0/283], lr: 0.00000062 	 loss = 0.0745(0.0745)
2023/11/23 13:55:57 - INFO - root -   Epoch: [299/300][20/283], lr: 0.00000062 	 loss = 1.3032(0.5148)
2023/11/23 13:56:09 - INFO - root -   Epoch: [299/300][40/283], lr: 0.00000062 	 loss = 0.2510(0.3875)
2023/11/23 13:56:21 - INFO - root -   Epoch: [299/300][60/283], lr: 0.00000062 	 loss = 0.0381(0.3690)
2023/11/23 13:56:39 - INFO - root -   Epoch: [299/300][80/283], lr: 0.00000062 	 loss = 0.0239(0.3966)
2023/11/23 13:56:49 - INFO - root -   Epoch: [299/300][100/283], lr: 0.00000062 	 loss = 0.2542(0.3499)
2023/11/23 13:57:03 - INFO - root -   Epoch: [299/300][120/283], lr: 0.00000062 	 loss = 0.0286(0.3144)
2023/11/23 13:57:15 - INFO - root -   Epoch: [299/300][140/283], lr: 0.00000062 	 loss = 0.1357(0.2824)
2023/11/23 13:57:28 - INFO - root -   Epoch: [299/300][160/283], lr: 0.00000062 	 loss = 0.0616(0.2925)
2023/11/23 13:57:37 - INFO - root -   Epoch: [299/300][180/283], lr: 0.00000062 	 loss = 0.0090(0.3058)
2023/11/23 13:57:56 - INFO - root -   Epoch: [299/300][200/283], lr: 0.00000062 	 loss = 0.0162(0.3048)
2023/11/23 13:58:12 - INFO - root -   Epoch: [299/300][220/283], lr: 0.00000062 	 loss = 0.1642(0.3010)
2023/11/23 13:58:29 - INFO - root -   Epoch: [299/300][240/283], lr: 0.00000062 	 loss = 0.1149(0.3030)
2023/11/23 13:58:42 - INFO - root -   Epoch: [299/300][260/283], lr: 0.00000062 	 loss = 0.1336(0.2987)
2023/11/23 13:58:58 - INFO - root -   Epoch: [299/300][280/283], lr: 0.00000062 	 loss = 1.8636(0.2964)
2023/11/23 13:58:59 - INFO - root -   Epoch: [299/300] 	 loss = 0.2949
2023/11/23 14:01:51 - INFO - root -   precision = 0.7552
2023/11/23 14:01:51 - INFO - root -   eval_loss = 0.7460
2023/11/23 14:01:51 - INFO - root -   eval_acc = 0.7552
2023/11/23 14:01:53 - INFO - root -   train_accuracy = 0.9011
