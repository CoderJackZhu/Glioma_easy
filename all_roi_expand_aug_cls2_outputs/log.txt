2023/09/26 19:13:25 - INFO - root -   Num train examples = 693
2023/09/26 19:13:25 - INFO - root -   Num val examples = 174
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/09/26 19:13:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/09/26 19:13:26 - INFO - root -   backend = nccl
2023/09/26 19:13:26 - INFO - root -   batch_size = 2
2023/09/26 19:13:26 - INFO - root -   dropout = 0.5
2023/09/26 19:13:26 - INFO - root -   epochs = 200
2023/09/26 19:13:26 - INFO - root -   eval_freq = 5
2023/09/26 19:13:26 - INFO - root -   focal_loss = False
2023/09/26 19:13:26 - INFO - root -   input_size = 224
2023/09/26 19:13:26 - INFO - root -   is_pretrained = False
2023/09/26 19:13:26 - INFO - root -   label_smooth = False
2023/09/26 19:13:26 - INFO - root -   local_rank = -1
2023/09/26 19:13:26 - INFO - root -   lr = 1e-05
2023/09/26 19:13:26 - INFO - root -   lr_decay_rate = 0.1
2023/09/26 19:13:26 - INFO - root -   lr_steps = [50, 100]
2023/09/26 19:13:26 - INFO - root -   lr_type = cosine
2023/09/26 19:13:26 - INFO - root -   model_depth = 34
2023/09/26 19:13:26 - INFO - root -   model_name = resnet50
2023/09/26 19:13:26 - INFO - root -   momentum = 0.9
2023/09/26 19:13:26 - INFO - root -   num_classes = 2
2023/09/26 19:13:26 - INFO - root -   output = ./all_roi_expand_aug_cls2_outputs
2023/09/26 19:13:26 - INFO - root -   print_freq = 20
2023/09/26 19:13:26 - INFO - root -   resume = 
2023/09/26 19:13:26 - INFO - root -   start_epoch = 0
2023/09/26 19:13:26 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/09/26 19:13:26 - INFO - root -   tune_from = 
2023/09/26 19:13:26 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/09/26 19:13:26 - INFO - root -   warmup_epoch = 20
2023/09/26 19:13:26 - INFO - root -   warmup_multiplier = 100
2023/09/26 19:13:26 - INFO - root -   weight_decay = 0.0005
2023/09/26 19:13:26 - INFO - root -   workers = 8
2023/09/26 19:13:50 - INFO - root -   Epoch: [0/200][0/346], lr: 0.00000010 	 loss = 0.5171(0.5171)
2023/09/26 19:14:31 - INFO - root -   Epoch: [0/200][20/346], lr: 0.00000010 	 loss = 0.8516(0.6260)
2023/09/26 19:15:33 - INFO - root -   Epoch: [0/200][40/346], lr: 0.00000010 	 loss = 0.8660(0.6852)
2023/09/26 19:16:16 - INFO - root -   Epoch: [0/200][60/346], lr: 0.00000010 	 loss = 0.7541(0.7291)
2023/09/26 19:17:16 - INFO - root -   Epoch: [0/200][80/346], lr: 0.00000010 	 loss = 0.9208(0.7425)
2023/09/26 19:17:59 - INFO - root -   Epoch: [0/200][100/346], lr: 0.00000010 	 loss = 0.8540(0.7502)
2023/09/26 19:19:00 - INFO - root -   Epoch: [0/200][120/346], lr: 0.00000010 	 loss = 0.8946(0.7475)
2023/09/26 19:19:43 - INFO - root -   Epoch: [0/200][140/346], lr: 0.00000010 	 loss = 1.0049(0.7437)
2023/09/26 19:20:43 - INFO - root -   Epoch: [0/200][160/346], lr: 0.00000010 	 loss = 0.4076(0.7352)
2023/09/26 19:21:26 - INFO - root -   Epoch: [0/200][180/346], lr: 0.00000010 	 loss = 1.0025(0.7498)
2023/09/26 19:22:26 - INFO - root -   Epoch: [0/200][200/346], lr: 0.00000010 	 loss = 0.4592(0.7577)
2023/09/26 19:23:09 - INFO - root -   Epoch: [0/200][220/346], lr: 0.00000010 	 loss = 0.3073(0.7476)
2023/09/26 19:24:09 - INFO - root -   Epoch: [0/200][240/346], lr: 0.00000010 	 loss = 0.7428(0.7454)
2023/09/26 19:24:52 - INFO - root -   Epoch: [0/200][260/346], lr: 0.00000010 	 loss = 0.3764(0.7453)
2023/09/26 19:25:53 - INFO - root -   Epoch: [0/200][280/346], lr: 0.00000010 	 loss = 0.9265(0.7458)
2023/09/26 19:26:36 - INFO - root -   Epoch: [0/200][300/346], lr: 0.00000010 	 loss = 0.3332(0.7485)
2023/09/26 19:27:36 - INFO - root -   Epoch: [0/200][320/346], lr: 0.00000010 	 loss = 1.1171(0.7538)
2023/09/26 19:28:18 - INFO - root -   Epoch: [0/200][340/346], lr: 0.00000010 	 loss = 0.8793(0.7520)
2023/09/26 19:28:22 - INFO - root -   Epoch: [0/200] 	 loss = 0.7506
2023/09/26 19:28:22 - INFO - root -   train_accuracy = 0.5260
2023/09/26 19:28:44 - INFO - root -   Epoch: [1/200][0/346], lr: 0.00000010 	 loss = 0.8856(0.8856)
2023/09/26 19:29:27 - INFO - root -   Epoch: [1/200][20/346], lr: 0.00000010 	 loss = 1.1655(0.8955)
2023/09/26 19:30:28 - INFO - root -   Epoch: [1/200][40/346], lr: 0.00000010 	 loss = 0.3586(0.8160)
2023/09/26 19:31:12 - INFO - root -   Epoch: [1/200][60/346], lr: 0.00000010 	 loss = 0.3971(0.7924)
2023/09/26 19:32:13 - INFO - root -   Epoch: [1/200][80/346], lr: 0.00000010 	 loss = 0.5887(0.7736)
2023/09/26 19:32:57 - INFO - root -   Epoch: [1/200][100/346], lr: 0.00000010 	 loss = 0.5790(0.7694)
2023/09/26 19:33:58 - INFO - root -   Epoch: [1/200][120/346], lr: 0.00000010 	 loss = 0.6123(0.7818)
2023/09/26 19:34:41 - INFO - root -   Epoch: [1/200][140/346], lr: 0.00000010 	 loss = 1.1345(0.7658)
2023/09/26 19:35:43 - INFO - root -   Epoch: [1/200][160/346], lr: 0.00000010 	 loss = 0.7101(0.7717)
2023/09/26 19:36:26 - INFO - root -   Epoch: [1/200][180/346], lr: 0.00000010 	 loss = 0.7114(0.7754)
2023/09/26 19:37:28 - INFO - root -   Epoch: [1/200][200/346], lr: 0.00000010 	 loss = 0.7044(0.7705)
2023/09/26 19:38:12 - INFO - root -   Epoch: [1/200][220/346], lr: 0.00000010 	 loss = 0.7696(0.7720)
2023/09/26 19:39:12 - INFO - root -   Epoch: [1/200][240/346], lr: 0.00000010 	 loss = 0.4026(0.7706)
2023/09/26 19:39:57 - INFO - root -   Epoch: [1/200][260/346], lr: 0.00000010 	 loss = 0.6059(0.7713)
2023/09/26 19:40:56 - INFO - root -   Epoch: [1/200][280/346], lr: 0.00000010 	 loss = 0.2036(0.7647)
2023/09/26 19:41:41 - INFO - root -   Epoch: [1/200][300/346], lr: 0.00000010 	 loss = 0.6660(0.7720)
2023/09/26 19:42:39 - INFO - root -   Epoch: [1/200][320/346], lr: 0.00000010 	 loss = 0.3063(0.7628)
2023/09/26 19:43:22 - INFO - root -   Epoch: [1/200][340/346], lr: 0.00000010 	 loss = 0.3941(0.7673)
2023/09/26 19:43:26 - INFO - root -   Epoch: [1/200] 	 loss = 0.7700
2023/09/26 19:43:26 - INFO - root -   train_accuracy = 0.5419
2023/09/26 19:43:47 - INFO - root -   Epoch: [2/200][0/346], lr: 0.00000010 	 loss = 0.5865(0.5865)
2023/09/26 19:44:31 - INFO - root -   Epoch: [2/200][20/346], lr: 0.00000010 	 loss = 0.4614(0.7614)
2023/09/26 19:45:32 - INFO - root -   Epoch: [2/200][40/346], lr: 0.00000010 	 loss = 0.8374(0.7468)
2023/09/26 19:46:15 - INFO - root -   Epoch: [2/200][60/346], lr: 0.00000010 	 loss = 1.1678(0.8122)
2023/09/26 19:47:16 - INFO - root -   Epoch: [2/200][80/346], lr: 0.00000010 	 loss = 0.6057(0.7848)
2023/09/26 19:47:59 - INFO - root -   Epoch: [2/200][100/346], lr: 0.00000010 	 loss = 0.6349(0.7729)
2023/09/26 19:49:00 - INFO - root -   Epoch: [2/200][120/346], lr: 0.00000010 	 loss = 0.5837(0.7619)
2023/09/26 19:49:44 - INFO - root -   Epoch: [2/200][140/346], lr: 0.00000010 	 loss = 0.7468(0.7678)
2023/09/26 19:50:45 - INFO - root -   Epoch: [2/200][160/346], lr: 0.00000010 	 loss = 0.8717(0.7756)
2023/09/26 19:51:29 - INFO - root -   Epoch: [2/200][180/346], lr: 0.00000010 	 loss = 0.9523(0.7687)
2023/09/26 19:52:29 - INFO - root -   Epoch: [2/200][200/346], lr: 0.00000010 	 loss = 0.6149(0.7683)
2023/09/26 19:53:14 - INFO - root -   Epoch: [2/200][220/346], lr: 0.00000010 	 loss = 0.4258(0.7554)
2023/09/26 19:54:15 - INFO - root -   Epoch: [2/200][240/346], lr: 0.00000010 	 loss = 0.9726(0.7549)
2023/09/26 19:54:59 - INFO - root -   Epoch: [2/200][260/346], lr: 0.00000010 	 loss = 0.5741(0.7549)
2023/09/26 19:55:59 - INFO - root -   Epoch: [2/200][280/346], lr: 0.00000010 	 loss = 0.4989(0.7646)
2023/09/26 19:56:43 - INFO - root -   Epoch: [2/200][300/346], lr: 0.00000010 	 loss = 0.3523(0.7566)
2023/09/26 19:57:43 - INFO - root -   Epoch: [2/200][320/346], lr: 0.00000010 	 loss = 0.5076(0.7575)
2023/09/26 19:58:23 - INFO - root -   Epoch: [2/200][340/346], lr: 0.00000010 	 loss = 1.0681(0.7561)
2023/09/26 19:58:27 - INFO - root -   Epoch: [2/200] 	 loss = 0.7549
2023/09/26 19:58:27 - INFO - root -   train_accuracy = 0.5246
2023/09/26 19:58:49 - INFO - root -   Epoch: [3/200][0/346], lr: 0.00000010 	 loss = 1.1167(1.1167)
2023/09/26 19:59:32 - INFO - root -   Epoch: [3/200][20/346], lr: 0.00000010 	 loss = 0.5970(0.7481)
2023/09/26 20:00:34 - INFO - root -   Epoch: [3/200][40/346], lr: 0.00000010 	 loss = 0.5505(0.6853)
2023/09/26 20:01:17 - INFO - root -   Epoch: [3/200][60/346], lr: 0.00000010 	 loss = 0.7861(0.6824)
2023/09/26 20:02:18 - INFO - root -   Epoch: [3/200][80/346], lr: 0.00000010 	 loss = 0.7595(0.6814)
2023/09/26 20:03:01 - INFO - root -   Epoch: [3/200][100/346], lr: 0.00000010 	 loss = 1.0192(0.7068)
2023/09/26 20:04:02 - INFO - root -   Epoch: [3/200][120/346], lr: 0.00000010 	 loss = 0.4859(0.7090)
2023/09/26 20:04:45 - INFO - root -   Epoch: [3/200][140/346], lr: 0.00000010 	 loss = 0.4007(0.7092)
2023/09/26 20:05:46 - INFO - root -   Epoch: [3/200][160/346], lr: 0.00000010 	 loss = 0.9001(0.7081)
2023/09/26 20:06:29 - INFO - root -   Epoch: [3/200][180/346], lr: 0.00000010 	 loss = 0.4467(0.7059)
2023/09/26 20:07:29 - INFO - root -   Epoch: [3/200][200/346], lr: 0.00000010 	 loss = 0.7137(0.7015)
2023/09/26 20:08:14 - INFO - root -   Epoch: [3/200][220/346], lr: 0.00000010 	 loss = 0.4233(0.7033)
2023/09/26 20:09:13 - INFO - root -   Epoch: [3/200][240/346], lr: 0.00000010 	 loss = 0.9594(0.7000)
2023/09/26 20:09:57 - INFO - root -   Epoch: [3/200][260/346], lr: 0.00000010 	 loss = 0.6899(0.7054)
2023/09/26 20:10:58 - INFO - root -   Epoch: [3/200][280/346], lr: 0.00000010 	 loss = 0.4742(0.7050)
2023/09/26 20:11:41 - INFO - root -   Epoch: [3/200][300/346], lr: 0.00000010 	 loss = 0.4680(0.7107)
2023/09/26 20:12:42 - INFO - root -   Epoch: [3/200][320/346], lr: 0.00000010 	 loss = 0.9424(0.7133)
2023/09/26 20:13:24 - INFO - root -   Epoch: [3/200][340/346], lr: 0.00000010 	 loss = 1.3875(0.7085)
2023/09/26 20:13:28 - INFO - root -   Epoch: [3/200] 	 loss = 0.7103
2023/09/26 20:13:28 - INFO - root -   train_accuracy = 0.5809
2023/09/26 20:13:50 - INFO - root -   Epoch: [4/200][0/346], lr: 0.00000011 	 loss = 0.6312(0.6312)
2023/09/26 20:14:33 - INFO - root -   Epoch: [4/200][20/346], lr: 0.00000011 	 loss = 0.4443(0.5822)
2023/09/26 20:15:34 - INFO - root -   Epoch: [4/200][40/346], lr: 0.00000011 	 loss = 0.2446(0.5919)
2023/09/26 20:16:17 - INFO - root -   Epoch: [4/200][60/346], lr: 0.00000011 	 loss = 0.7869(0.6274)
2023/09/26 20:17:18 - INFO - root -   Epoch: [4/200][80/346], lr: 0.00000011 	 loss = 0.5684(0.6649)
2023/09/26 20:18:02 - INFO - root -   Epoch: [4/200][100/346], lr: 0.00000011 	 loss = 0.4100(0.6767)
2023/09/26 20:19:03 - INFO - root -   Epoch: [4/200][120/346], lr: 0.00000011 	 loss = 0.4915(0.6921)
2023/09/26 20:19:46 - INFO - root -   Epoch: [4/200][140/346], lr: 0.00000011 	 loss = 0.6845(0.7019)
2023/09/26 20:20:47 - INFO - root -   Epoch: [4/200][160/346], lr: 0.00000011 	 loss = 0.3688(0.6942)
2023/09/26 20:21:30 - INFO - root -   Epoch: [4/200][180/346], lr: 0.00000011 	 loss = 0.4781(0.6909)
2023/09/26 20:22:30 - INFO - root -   Epoch: [4/200][200/346], lr: 0.00000011 	 loss = 0.7405(0.6891)
2023/09/26 20:23:15 - INFO - root -   Epoch: [4/200][220/346], lr: 0.00000011 	 loss = 0.5605(0.6985)
2023/09/26 20:24:14 - INFO - root -   Epoch: [4/200][240/346], lr: 0.00000011 	 loss = 1.0089(0.7118)
2023/09/26 20:24:59 - INFO - root -   Epoch: [4/200][260/346], lr: 0.00000011 	 loss = 0.3248(0.7067)
2023/09/26 20:25:58 - INFO - root -   Epoch: [4/200][280/346], lr: 0.00000011 	 loss = 0.5006(0.7091)
2023/09/26 20:26:43 - INFO - root -   Epoch: [4/200][300/346], lr: 0.00000011 	 loss = 0.4263(0.7098)
2023/09/26 20:27:42 - INFO - root -   Epoch: [4/200][320/346], lr: 0.00000011 	 loss = 0.3016(0.7011)
2023/09/26 20:28:25 - INFO - root -   Epoch: [4/200][340/346], lr: 0.00000011 	 loss = 0.7937(0.7005)
2023/09/26 20:28:29 - INFO - root -   Epoch: [4/200] 	 loss = 0.6991
2023/09/26 20:34:41 - INFO - root -   precision = 0.5575
2023/09/26 20:34:41 - INFO - root -   eval_loss = 0.7094
2023/09/26 20:34:42 - INFO - root -   train_accuracy = 0.5535
2023/09/26 20:35:03 - INFO - root -   Epoch: [5/200][0/346], lr: 0.00000011 	 loss = 0.9722(0.9722)
2023/09/26 20:35:47 - INFO - root -   Epoch: [5/200][20/346], lr: 0.00000011 	 loss = 0.8174(0.7120)
2023/09/26 20:36:49 - INFO - root -   Epoch: [5/200][40/346], lr: 0.00000011 	 loss = 0.7176(0.7348)
2023/09/26 20:37:32 - INFO - root -   Epoch: [5/200][60/346], lr: 0.00000011 	 loss = 1.4157(0.7215)
2023/09/26 20:38:32 - INFO - root -   Epoch: [5/200][80/346], lr: 0.00000011 	 loss = 0.4542(0.7081)
2023/09/26 20:39:16 - INFO - root -   Epoch: [5/200][100/346], lr: 0.00000011 	 loss = 1.0232(0.7222)
2023/09/26 20:40:17 - INFO - root -   Epoch: [5/200][120/346], lr: 0.00000011 	 loss = 0.7811(0.7312)
2023/09/26 20:41:00 - INFO - root -   Epoch: [5/200][140/346], lr: 0.00000011 	 loss = 0.9564(0.7280)
2023/09/26 20:42:01 - INFO - root -   Epoch: [5/200][160/346], lr: 0.00000011 	 loss = 0.6064(0.7426)
2023/09/26 20:42:44 - INFO - root -   Epoch: [5/200][180/346], lr: 0.00000011 	 loss = 0.7976(0.7390)
2023/09/26 20:43:44 - INFO - root -   Epoch: [5/200][200/346], lr: 0.00000011 	 loss = 0.4273(0.7364)
2023/09/26 20:44:28 - INFO - root -   Epoch: [5/200][220/346], lr: 0.00000011 	 loss = 0.9333(0.7371)
2023/09/26 20:45:28 - INFO - root -   Epoch: [5/200][240/346], lr: 0.00000011 	 loss = 0.6273(0.7326)
2023/09/26 20:46:11 - INFO - root -   Epoch: [5/200][260/346], lr: 0.00000011 	 loss = 0.7957(0.7288)
2023/09/26 20:47:11 - INFO - root -   Epoch: [5/200][280/346], lr: 0.00000011 	 loss = 0.5409(0.7385)
2023/09/26 20:47:55 - INFO - root -   Epoch: [5/200][300/346], lr: 0.00000011 	 loss = 0.5665(0.7419)
2023/09/26 20:48:55 - INFO - root -   Epoch: [5/200][320/346], lr: 0.00000011 	 loss = 0.4730(0.7455)
2023/09/26 20:49:36 - INFO - root -   Epoch: [5/200][340/346], lr: 0.00000011 	 loss = 0.8778(0.7442)
2023/09/26 20:49:40 - INFO - root -   Epoch: [5/200] 	 loss = 0.7441
2023/09/26 20:49:40 - INFO - root -   train_accuracy = 0.5275
2023/09/26 20:50:01 - INFO - root -   Epoch: [6/200][0/346], lr: 0.00000011 	 loss = 0.8521(0.8521)
2023/09/26 20:50:45 - INFO - root -   Epoch: [6/200][20/346], lr: 0.00000011 	 loss = 0.9596(0.7207)
2023/09/26 20:51:46 - INFO - root -   Epoch: [6/200][40/346], lr: 0.00000011 	 loss = 0.3273(0.7216)
2023/09/26 20:52:29 - INFO - root -   Epoch: [6/200][60/346], lr: 0.00000011 	 loss = 0.6655(0.7340)
2023/09/26 20:53:30 - INFO - root -   Epoch: [6/200][80/346], lr: 0.00000011 	 loss = 0.5005(0.7378)
2023/09/26 20:54:14 - INFO - root -   Epoch: [6/200][100/346], lr: 0.00000011 	 loss = 0.3335(0.7330)
2023/09/26 20:55:15 - INFO - root -   Epoch: [6/200][120/346], lr: 0.00000011 	 loss = 0.6073(0.7401)
2023/09/26 20:55:58 - INFO - root -   Epoch: [6/200][140/346], lr: 0.00000011 	 loss = 0.7125(0.7377)
2023/09/26 20:56:59 - INFO - root -   Epoch: [6/200][160/346], lr: 0.00000011 	 loss = 0.1892(0.7327)
2023/09/26 20:57:43 - INFO - root -   Epoch: [6/200][180/346], lr: 0.00000011 	 loss = 0.8190(0.7333)
2023/09/26 20:58:43 - INFO - root -   Epoch: [6/200][200/346], lr: 0.00000011 	 loss = 0.2988(0.7264)
2023/09/26 20:59:27 - INFO - root -   Epoch: [6/200][220/346], lr: 0.00000011 	 loss = 0.3334(0.7222)
2023/09/26 21:00:27 - INFO - root -   Epoch: [6/200][240/346], lr: 0.00000011 	 loss = 0.4847(0.7141)
2023/09/26 21:01:11 - INFO - root -   Epoch: [6/200][260/346], lr: 0.00000011 	 loss = 0.5089(0.7145)
2023/09/26 21:02:11 - INFO - root -   Epoch: [6/200][280/346], lr: 0.00000011 	 loss = 0.4790(0.7159)
2023/09/26 21:02:55 - INFO - root -   Epoch: [6/200][300/346], lr: 0.00000011 	 loss = 0.2268(0.7244)
2023/09/26 21:03:55 - INFO - root -   Epoch: [6/200][320/346], lr: 0.00000011 	 loss = 0.2729(0.7179)
2023/09/26 21:04:37 - INFO - root -   Epoch: [6/200][340/346], lr: 0.00000011 	 loss = 0.7224(0.7201)
2023/09/26 21:04:42 - INFO - root -   Epoch: [6/200] 	 loss = 0.7168
2023/09/26 21:04:42 - INFO - root -   train_accuracy = 0.5780
2023/09/26 21:05:03 - INFO - root -   Epoch: [7/200][0/346], lr: 0.00000011 	 loss = 0.5389(0.5389)
2023/09/26 21:05:47 - INFO - root -   Epoch: [7/200][20/346], lr: 0.00000011 	 loss = 0.4363(0.5724)
2023/09/26 21:06:48 - INFO - root -   Epoch: [7/200][40/346], lr: 0.00000011 	 loss = 0.6799(0.6350)
2023/09/26 21:07:31 - INFO - root -   Epoch: [7/200][60/346], lr: 0.00000011 	 loss = 0.8334(0.6550)
2023/09/26 21:08:32 - INFO - root -   Epoch: [7/200][80/346], lr: 0.00000011 	 loss = 0.8179(0.6691)
2023/09/26 21:09:16 - INFO - root -   Epoch: [7/200][100/346], lr: 0.00000011 	 loss = 0.2824(0.6697)
2023/09/26 21:10:17 - INFO - root -   Epoch: [7/200][120/346], lr: 0.00000011 	 loss = 0.8096(0.6792)
2023/09/26 21:11:00 - INFO - root -   Epoch: [7/200][140/346], lr: 0.00000011 	 loss = 0.5692(0.6819)
2023/09/26 21:12:01 - INFO - root -   Epoch: [7/200][160/346], lr: 0.00000011 	 loss = 0.9856(0.6978)
2023/09/26 21:12:44 - INFO - root -   Epoch: [7/200][180/346], lr: 0.00000011 	 loss = 0.7164(0.6987)
2023/09/26 21:13:45 - INFO - root -   Epoch: [7/200][200/346], lr: 0.00000011 	 loss = 0.5177(0.6928)
2023/09/26 21:14:29 - INFO - root -   Epoch: [7/200][220/346], lr: 0.00000011 	 loss = 0.9350(0.7018)
2023/09/26 21:15:29 - INFO - root -   Epoch: [7/200][240/346], lr: 0.00000011 	 loss = 0.5964(0.7040)
2023/09/26 21:16:13 - INFO - root -   Epoch: [7/200][260/346], lr: 0.00000011 	 loss = 0.5421(0.7054)
2023/09/26 21:17:13 - INFO - root -   Epoch: [7/200][280/346], lr: 0.00000011 	 loss = 0.4318(0.6992)
2023/09/26 21:17:57 - INFO - root -   Epoch: [7/200][300/346], lr: 0.00000011 	 loss = 0.4351(0.7008)
2023/09/26 21:18:57 - INFO - root -   Epoch: [7/200][320/346], lr: 0.00000011 	 loss = 0.5350(0.6993)
2023/09/26 21:19:39 - INFO - root -   Epoch: [7/200][340/346], lr: 0.00000011 	 loss = 0.7879(0.7006)
2023/09/26 21:19:43 - INFO - root -   Epoch: [7/200] 	 loss = 0.6961
2023/09/26 21:19:43 - INFO - root -   train_accuracy = 0.5795
2023/09/26 21:20:04 - INFO - root -   Epoch: [8/200][0/346], lr: 0.00000011 	 loss = 0.2863(0.2863)
2023/09/26 21:20:48 - INFO - root -   Epoch: [8/200][20/346], lr: 0.00000011 	 loss = 0.4269(0.5644)
2023/09/26 21:21:49 - INFO - root -   Epoch: [8/200][40/346], lr: 0.00000011 	 loss = 0.6086(0.6570)
2023/09/26 21:22:32 - INFO - root -   Epoch: [8/200][60/346], lr: 0.00000011 	 loss = 0.7023(0.6748)
2023/09/26 21:23:33 - INFO - root -   Epoch: [8/200][80/346], lr: 0.00000011 	 loss = 0.7409(0.6956)
2023/09/26 21:24:17 - INFO - root -   Epoch: [8/200][100/346], lr: 0.00000011 	 loss = 0.6257(0.7104)
2023/09/26 21:25:18 - INFO - root -   Epoch: [8/200][120/346], lr: 0.00000011 	 loss = 0.7187(0.7361)
2023/09/26 21:26:02 - INFO - root -   Epoch: [8/200][140/346], lr: 0.00000011 	 loss = 1.0247(0.7426)
2023/09/26 21:27:02 - INFO - root -   Epoch: [8/200][160/346], lr: 0.00000011 	 loss = 0.3440(0.7392)
2023/09/26 21:27:46 - INFO - root -   Epoch: [8/200][180/346], lr: 0.00000011 	 loss = 0.5827(0.7206)
2023/09/26 21:28:46 - INFO - root -   Epoch: [8/200][200/346], lr: 0.00000011 	 loss = 0.2177(0.7215)
2023/09/26 21:29:30 - INFO - root -   Epoch: [8/200][220/346], lr: 0.00000011 	 loss = 0.7448(0.7113)
2023/09/26 21:30:30 - INFO - root -   Epoch: [8/200][240/346], lr: 0.00000011 	 loss = 0.7405(0.7088)
2023/09/26 21:31:15 - INFO - root -   Epoch: [8/200][260/346], lr: 0.00000011 	 loss = 0.3799(0.7012)
2023/09/26 21:32:14 - INFO - root -   Epoch: [8/200][280/346], lr: 0.00000011 	 loss = 0.8917(0.7049)
2023/09/26 21:32:59 - INFO - root -   Epoch: [8/200][300/346], lr: 0.00000011 	 loss = 0.1244(0.7005)
2023/09/26 21:33:59 - INFO - root -   Epoch: [8/200][320/346], lr: 0.00000011 	 loss = 0.4227(0.7033)
2023/09/26 21:34:41 - INFO - root -   Epoch: [8/200][340/346], lr: 0.00000011 	 loss = 0.8056(0.7005)
2023/09/26 21:34:45 - INFO - root -   Epoch: [8/200] 	 loss = 0.6986
2023/09/26 21:34:45 - INFO - root -   train_accuracy = 0.5838
2023/09/26 21:35:07 - INFO - root -   Epoch: [9/200][0/346], lr: 0.00000011 	 loss = 0.6396(0.6396)
2023/09/26 21:35:50 - INFO - root -   Epoch: [9/200][20/346], lr: 0.00000011 	 loss = 0.3098(0.5887)
2023/09/26 21:36:51 - INFO - root -   Epoch: [9/200][40/346], lr: 0.00000011 	 loss = 0.5906(0.6634)
2023/09/26 21:37:34 - INFO - root -   Epoch: [9/200][60/346], lr: 0.00000011 	 loss = 1.0659(0.6878)
2023/09/26 21:38:35 - INFO - root -   Epoch: [9/200][80/346], lr: 0.00000011 	 loss = 0.7437(0.7070)
2023/09/26 21:39:19 - INFO - root -   Epoch: [9/200][100/346], lr: 0.00000011 	 loss = 0.4155(0.7118)
2023/09/26 21:40:19 - INFO - root -   Epoch: [9/200][120/346], lr: 0.00000011 	 loss = 0.4905(0.7160)
2023/09/26 21:41:03 - INFO - root -   Epoch: [9/200][140/346], lr: 0.00000011 	 loss = 0.6331(0.7184)
2023/09/26 21:42:03 - INFO - root -   Epoch: [9/200][160/346], lr: 0.00000011 	 loss = 0.3289(0.7216)
2023/09/26 21:42:47 - INFO - root -   Epoch: [9/200][180/346], lr: 0.00000011 	 loss = 0.4384(0.7158)
2023/09/26 21:43:47 - INFO - root -   Epoch: [9/200][200/346], lr: 0.00000011 	 loss = 0.1641(0.7026)
2023/09/26 21:44:31 - INFO - root -   Epoch: [9/200][220/346], lr: 0.00000011 	 loss = 0.9151(0.6986)
2023/09/26 21:45:30 - INFO - root -   Epoch: [9/200][240/346], lr: 0.00000011 	 loss = 0.1736(0.7011)
2023/09/26 21:46:15 - INFO - root -   Epoch: [9/200][260/346], lr: 0.00000011 	 loss = 0.2658(0.6994)
2023/09/26 21:47:14 - INFO - root -   Epoch: [9/200][280/346], lr: 0.00000011 	 loss = 0.4969(0.7081)
2023/09/26 21:48:00 - INFO - root -   Epoch: [9/200][300/346], lr: 0.00000011 	 loss = 0.2571(0.7087)
2023/09/26 21:48:58 - INFO - root -   Epoch: [9/200][320/346], lr: 0.00000011 	 loss = 1.0318(0.7082)
2023/09/26 21:49:42 - INFO - root -   Epoch: [9/200][340/346], lr: 0.00000011 	 loss = 0.7386(0.7020)
2023/09/26 21:49:46 - INFO - root -   Epoch: [9/200] 	 loss = 0.7013
2023/09/26 21:55:47 - INFO - root -   precision = 0.5057
2023/09/26 21:55:47 - INFO - root -   eval_loss = 0.7366
2023/09/26 21:55:49 - INFO - root -   train_accuracy = 0.5983
2023/09/26 21:56:10 - INFO - root -   Epoch: [10/200][0/346], lr: 0.00000011 	 loss = 0.2625(0.2625)
2023/09/26 21:56:54 - INFO - root -   Epoch: [10/200][20/346], lr: 0.00000011 	 loss = 0.6491(0.5562)
2023/09/26 21:57:54 - INFO - root -   Epoch: [10/200][40/346], lr: 0.00000011 	 loss = 0.4297(0.6086)
2023/09/26 21:58:38 - INFO - root -   Epoch: [10/200][60/346], lr: 0.00000011 	 loss = 0.5039(0.6249)
2023/09/26 21:59:38 - INFO - root -   Epoch: [10/200][80/346], lr: 0.00000011 	 loss = 0.6122(0.6457)
2023/09/26 22:00:22 - INFO - root -   Epoch: [10/200][100/346], lr: 0.00000011 	 loss = 0.6449(0.6417)
2023/09/26 22:01:22 - INFO - root -   Epoch: [10/200][120/346], lr: 0.00000011 	 loss = 1.2487(0.6777)
2023/09/26 22:02:05 - INFO - root -   Epoch: [10/200][140/346], lr: 0.00000011 	 loss = 0.7708(0.6770)
2023/09/26 22:03:06 - INFO - root -   Epoch: [10/200][160/346], lr: 0.00000011 	 loss = 0.4535(0.6777)
2023/09/26 22:03:49 - INFO - root -   Epoch: [10/200][180/346], lr: 0.00000011 	 loss = 0.5703(0.6762)
2023/09/26 22:04:49 - INFO - root -   Epoch: [10/200][200/346], lr: 0.00000011 	 loss = 0.3333(0.6735)
2023/09/26 22:05:32 - INFO - root -   Epoch: [10/200][220/346], lr: 0.00000011 	 loss = 0.4915(0.6651)
2023/09/26 22:06:33 - INFO - root -   Epoch: [10/200][240/346], lr: 0.00000011 	 loss = 0.1987(0.6715)
2023/09/26 22:07:16 - INFO - root -   Epoch: [10/200][260/346], lr: 0.00000011 	 loss = 0.7956(0.6801)
2023/09/26 22:08:17 - INFO - root -   Epoch: [10/200][280/346], lr: 0.00000011 	 loss = 0.4365(0.6899)
2023/09/26 22:09:00 - INFO - root -   Epoch: [10/200][300/346], lr: 0.00000011 	 loss = 0.0902(0.6898)
2023/09/26 22:10:00 - INFO - root -   Epoch: [10/200][320/346], lr: 0.00000011 	 loss = 0.3162(0.6948)
2023/09/26 22:10:43 - INFO - root -   Epoch: [10/200][340/346], lr: 0.00000011 	 loss = 1.5019(0.6919)
2023/09/26 22:10:47 - INFO - root -   Epoch: [10/200] 	 loss = 0.6884
2023/09/26 22:10:47 - INFO - root -   train_accuracy = 0.5795
2023/09/26 22:11:09 - INFO - root -   Epoch: [11/200][0/346], lr: 0.00000012 	 loss = 0.7569(0.7569)
2023/09/26 22:11:53 - INFO - root -   Epoch: [11/200][20/346], lr: 0.00000012 	 loss = 0.6835(0.6085)
2023/09/26 22:12:54 - INFO - root -   Epoch: [11/200][40/346], lr: 0.00000012 	 loss = 0.7579(0.6912)
2023/09/26 22:13:38 - INFO - root -   Epoch: [11/200][60/346], lr: 0.00000012 	 loss = 0.4819(0.6605)
2023/09/26 22:14:39 - INFO - root -   Epoch: [11/200][80/346], lr: 0.00000012 	 loss = 0.6774(0.6480)
2023/09/26 22:15:22 - INFO - root -   Epoch: [11/200][100/346], lr: 0.00000012 	 loss = 0.9763(0.6586)
2023/09/26 22:16:23 - INFO - root -   Epoch: [11/200][120/346], lr: 0.00000012 	 loss = 0.4353(0.6834)
2023/09/26 22:17:06 - INFO - root -   Epoch: [11/200][140/346], lr: 0.00000012 	 loss = 1.0615(0.6861)
2023/09/26 22:18:07 - INFO - root -   Epoch: [11/200][160/346], lr: 0.00000012 	 loss = 0.3742(0.6845)
2023/09/26 22:18:51 - INFO - root -   Epoch: [11/200][180/346], lr: 0.00000012 	 loss = 0.2211(0.6800)
2023/09/26 22:19:52 - INFO - root -   Epoch: [11/200][200/346], lr: 0.00000012 	 loss = 0.4948(0.6791)
2023/09/26 22:20:35 - INFO - root -   Epoch: [11/200][220/346], lr: 0.00000012 	 loss = 0.5165(0.6713)
2023/09/26 22:21:36 - INFO - root -   Epoch: [11/200][240/346], lr: 0.00000012 	 loss = 0.4168(0.6710)
2023/09/26 22:22:19 - INFO - root -   Epoch: [11/200][260/346], lr: 0.00000012 	 loss = 0.8821(0.6691)
2023/09/26 22:23:20 - INFO - root -   Epoch: [11/200][280/346], lr: 0.00000012 	 loss = 0.2253(0.6698)
2023/09/26 22:24:03 - INFO - root -   Epoch: [11/200][300/346], lr: 0.00000012 	 loss = 0.5453(0.6841)
2023/09/26 22:25:04 - INFO - root -   Epoch: [11/200][320/346], lr: 0.00000012 	 loss = 0.2096(0.6836)
2023/09/26 22:25:46 - INFO - root -   Epoch: [11/200][340/346], lr: 0.00000012 	 loss = 1.0636(0.6824)
2023/09/26 22:25:51 - INFO - root -   Epoch: [11/200] 	 loss = 0.6815
2023/09/26 22:25:51 - INFO - root -   train_accuracy = 0.6026
2023/09/26 22:26:12 - INFO - root -   Epoch: [12/200][0/346], lr: 0.00000012 	 loss = 0.2523(0.2523)
2023/09/26 22:26:56 - INFO - root -   Epoch: [12/200][20/346], lr: 0.00000012 	 loss = 0.9133(0.5592)
2023/09/26 22:27:57 - INFO - root -   Epoch: [12/200][40/346], lr: 0.00000012 	 loss = 0.2494(0.5643)
2023/09/26 22:28:41 - INFO - root -   Epoch: [12/200][60/346], lr: 0.00000012 	 loss = 0.7957(0.5997)
2023/09/26 22:29:42 - INFO - root -   Epoch: [12/200][80/346], lr: 0.00000012 	 loss = 1.1819(0.6260)
2023/09/26 22:30:26 - INFO - root -   Epoch: [12/200][100/346], lr: 0.00000012 	 loss = 0.5482(0.6378)
2023/09/26 22:31:26 - INFO - root -   Epoch: [12/200][120/346], lr: 0.00000012 	 loss = 0.9486(0.6686)
2023/09/26 22:32:10 - INFO - root -   Epoch: [12/200][140/346], lr: 0.00000012 	 loss = 0.7871(0.6706)
2023/09/26 22:33:11 - INFO - root -   Epoch: [12/200][160/346], lr: 0.00000012 	 loss = 0.2282(0.6687)
2023/09/26 22:33:54 - INFO - root -   Epoch: [12/200][180/346], lr: 0.00000012 	 loss = 0.4907(0.6672)
2023/09/26 22:34:55 - INFO - root -   Epoch: [12/200][200/346], lr: 0.00000012 	 loss = 0.8299(0.6690)
2023/09/26 22:35:39 - INFO - root -   Epoch: [12/200][220/346], lr: 0.00000012 	 loss = 0.1112(0.6654)
2023/09/26 22:36:40 - INFO - root -   Epoch: [12/200][240/346], lr: 0.00000012 	 loss = 0.4683(0.6695)
2023/09/26 22:37:23 - INFO - root -   Epoch: [12/200][260/346], lr: 0.00000012 	 loss = 1.0441(0.6721)
2023/09/26 22:38:24 - INFO - root -   Epoch: [12/200][280/346], lr: 0.00000012 	 loss = 0.2065(0.6837)
2023/09/26 22:39:08 - INFO - root -   Epoch: [12/200][300/346], lr: 0.00000012 	 loss = 0.2483(0.6925)
2023/09/26 22:40:09 - INFO - root -   Epoch: [12/200][320/346], lr: 0.00000012 	 loss = 0.3978(0.6922)
2023/09/26 22:40:50 - INFO - root -   Epoch: [12/200][340/346], lr: 0.00000012 	 loss = 1.2514(0.6911)
2023/09/26 22:40:54 - INFO - root -   Epoch: [12/200] 	 loss = 0.6886
2023/09/26 22:40:54 - INFO - root -   train_accuracy = 0.5853
2023/09/26 22:41:16 - INFO - root -   Epoch: [13/200][0/346], lr: 0.00000012 	 loss = 0.5428(0.5428)
2023/09/26 22:42:00 - INFO - root -   Epoch: [13/200][20/346], lr: 0.00000012 	 loss = 1.0751(0.5856)
2023/09/26 22:43:01 - INFO - root -   Epoch: [13/200][40/346], lr: 0.00000012 	 loss = 0.2101(0.6441)
2023/09/26 22:43:44 - INFO - root -   Epoch: [13/200][60/346], lr: 0.00000012 	 loss = 0.3286(0.6260)
2023/09/26 22:44:45 - INFO - root -   Epoch: [13/200][80/346], lr: 0.00000012 	 loss = 0.3595(0.6794)
2023/09/26 22:45:28 - INFO - root -   Epoch: [13/200][100/346], lr: 0.00000012 	 loss = 0.5437(0.6970)
2023/09/26 22:46:28 - INFO - root -   Epoch: [13/200][120/346], lr: 0.00000012 	 loss = 0.6781(0.7046)
2023/09/26 22:47:12 - INFO - root -   Epoch: [13/200][140/346], lr: 0.00000012 	 loss = 0.6656(0.6974)
2023/09/26 22:48:12 - INFO - root -   Epoch: [13/200][160/346], lr: 0.00000012 	 loss = 0.3777(0.7073)
2023/09/26 22:48:56 - INFO - root -   Epoch: [13/200][180/346], lr: 0.00000012 	 loss = 0.8599(0.7007)
2023/09/26 22:49:56 - INFO - root -   Epoch: [13/200][200/346], lr: 0.00000012 	 loss = 0.2444(0.7028)
2023/09/26 22:50:39 - INFO - root -   Epoch: [13/200][220/346], lr: 0.00000012 	 loss = 0.2549(0.7055)
2023/09/26 22:51:40 - INFO - root -   Epoch: [13/200][240/346], lr: 0.00000012 	 loss = 0.2958(0.7058)
2023/09/26 22:52:23 - INFO - root -   Epoch: [13/200][260/346], lr: 0.00000012 	 loss = 0.9321(0.7080)
2023/09/26 22:53:24 - INFO - root -   Epoch: [13/200][280/346], lr: 0.00000012 	 loss = 0.3934(0.7067)
2023/09/26 22:54:07 - INFO - root -   Epoch: [13/200][300/346], lr: 0.00000012 	 loss = 0.3809(0.7085)
2023/09/26 22:55:08 - INFO - root -   Epoch: [13/200][320/346], lr: 0.00000012 	 loss = 0.6112(0.7014)
2023/09/26 22:55:50 - INFO - root -   Epoch: [13/200][340/346], lr: 0.00000012 	 loss = 1.0936(0.6992)
2023/09/26 22:55:54 - INFO - root -   Epoch: [13/200] 	 loss = 0.6997
2023/09/26 22:55:54 - INFO - root -   train_accuracy = 0.5795
2023/09/26 22:56:16 - INFO - root -   Epoch: [14/200][0/346], lr: 0.00000012 	 loss = 0.9047(0.9047)
2023/09/26 22:56:59 - INFO - root -   Epoch: [14/200][20/346], lr: 0.00000012 	 loss = 0.7953(0.5525)
2023/09/26 22:58:00 - INFO - root -   Epoch: [14/200][40/346], lr: 0.00000012 	 loss = 0.6309(0.6023)
2023/09/26 22:58:43 - INFO - root -   Epoch: [14/200][60/346], lr: 0.00000012 	 loss = 0.3224(0.6294)
2023/09/26 22:59:44 - INFO - root -   Epoch: [14/200][80/346], lr: 0.00000012 	 loss = 0.8348(0.6381)
2023/09/26 23:00:28 - INFO - root -   Epoch: [14/200][100/346], lr: 0.00000012 	 loss = 0.7101(0.6594)
2023/09/26 23:01:29 - INFO - root -   Epoch: [14/200][120/346], lr: 0.00000012 	 loss = 0.5478(0.6743)
2023/09/26 23:02:12 - INFO - root -   Epoch: [14/200][140/346], lr: 0.00000012 	 loss = 0.8233(0.6678)
2023/09/26 23:03:13 - INFO - root -   Epoch: [14/200][160/346], lr: 0.00000012 	 loss = 0.2956(0.6712)
2023/09/26 23:03:56 - INFO - root -   Epoch: [14/200][180/346], lr: 0.00000012 	 loss = 0.3847(0.6550)
2023/09/26 23:04:57 - INFO - root -   Epoch: [14/200][200/346], lr: 0.00000012 	 loss = 0.2550(0.6530)
2023/09/26 23:05:40 - INFO - root -   Epoch: [14/200][220/346], lr: 0.00000012 	 loss = 0.2732(0.6556)
2023/09/26 23:06:41 - INFO - root -   Epoch: [14/200][240/346], lr: 0.00000012 	 loss = 0.1809(0.6553)
2023/09/26 23:07:24 - INFO - root -   Epoch: [14/200][260/346], lr: 0.00000012 	 loss = 0.7116(0.6657)
2023/09/26 23:08:25 - INFO - root -   Epoch: [14/200][280/346], lr: 0.00000012 	 loss = 0.3289(0.6732)
2023/09/26 23:09:08 - INFO - root -   Epoch: [14/200][300/346], lr: 0.00000012 	 loss = 0.2765(0.6729)
2023/09/26 23:10:09 - INFO - root -   Epoch: [14/200][320/346], lr: 0.00000012 	 loss = 0.5689(0.6788)
2023/09/26 23:10:50 - INFO - root -   Epoch: [14/200][340/346], lr: 0.00000012 	 loss = 0.7469(0.6777)
2023/09/26 23:10:54 - INFO - root -   Epoch: [14/200] 	 loss = 0.6794
2023/09/26 23:16:55 - INFO - root -   precision = 0.6092
2023/09/26 23:16:55 - INFO - root -   eval_loss = 0.6912
2023/09/26 23:16:56 - INFO - root -   train_accuracy = 0.6084
2023/09/26 23:17:18 - INFO - root -   Epoch: [15/200][0/346], lr: 0.00000012 	 loss = 0.5970(0.5970)
2023/09/26 23:18:02 - INFO - root -   Epoch: [15/200][20/346], lr: 0.00000012 	 loss = 0.3411(0.5967)
2023/09/26 23:19:04 - INFO - root -   Epoch: [15/200][40/346], lr: 0.00000012 	 loss = 0.6399(0.6166)
2023/09/26 23:19:47 - INFO - root -   Epoch: [15/200][60/346], lr: 0.00000012 	 loss = 0.8003(0.5918)
2023/09/26 23:20:48 - INFO - root -   Epoch: [15/200][80/346], lr: 0.00000012 	 loss = 0.9066(0.6290)
2023/09/26 23:21:32 - INFO - root -   Epoch: [15/200][100/346], lr: 0.00000012 	 loss = 0.4926(0.6326)
2023/09/26 23:22:32 - INFO - root -   Epoch: [15/200][120/346], lr: 0.00000012 	 loss = 0.9657(0.6784)
2023/09/26 23:23:16 - INFO - root -   Epoch: [15/200][140/346], lr: 0.00000012 	 loss = 0.7528(0.7108)
2023/09/26 23:24:17 - INFO - root -   Epoch: [15/200][160/346], lr: 0.00000012 	 loss = 0.3048(0.7168)
2023/09/26 23:25:00 - INFO - root -   Epoch: [15/200][180/346], lr: 0.00000012 	 loss = 0.7131(0.7139)
2023/09/26 23:26:01 - INFO - root -   Epoch: [15/200][200/346], lr: 0.00000012 	 loss = 0.1692(0.7063)
2023/09/26 23:26:44 - INFO - root -   Epoch: [15/200][220/346], lr: 0.00000012 	 loss = 0.2581(0.7052)
2023/09/26 23:27:44 - INFO - root -   Epoch: [15/200][240/346], lr: 0.00000012 	 loss = 0.3085(0.6948)
2023/09/26 23:28:27 - INFO - root -   Epoch: [15/200][260/346], lr: 0.00000012 	 loss = 0.5977(0.6868)
2023/09/26 23:29:28 - INFO - root -   Epoch: [15/200][280/346], lr: 0.00000012 	 loss = 0.3279(0.6948)
2023/09/26 23:30:11 - INFO - root -   Epoch: [15/200][300/346], lr: 0.00000012 	 loss = 0.1867(0.6983)
2023/09/26 23:31:11 - INFO - root -   Epoch: [15/200][320/346], lr: 0.00000012 	 loss = 0.6269(0.6963)
2023/09/26 23:31:54 - INFO - root -   Epoch: [15/200][340/346], lr: 0.00000012 	 loss = 0.9312(0.6913)
2023/09/26 23:31:58 - INFO - root -   Epoch: [15/200] 	 loss = 0.6903
2023/09/26 23:31:58 - INFO - root -   train_accuracy = 0.5968
2023/09/26 23:32:20 - INFO - root -   Epoch: [16/200][0/346], lr: 0.00000012 	 loss = 0.2113(0.2113)
2023/09/26 23:33:03 - INFO - root -   Epoch: [16/200][20/346], lr: 0.00000012 	 loss = 1.1721(0.6056)
2023/09/26 23:34:05 - INFO - root -   Epoch: [16/200][40/346], lr: 0.00000012 	 loss = 0.8220(0.6480)
2023/09/26 23:34:48 - INFO - root -   Epoch: [16/200][60/346], lr: 0.00000012 	 loss = 0.7944(0.6468)
2023/09/26 23:35:49 - INFO - root -   Epoch: [16/200][80/346], lr: 0.00000012 	 loss = 0.4770(0.6353)
2023/09/26 23:36:33 - INFO - root -   Epoch: [16/200][100/346], lr: 0.00000012 	 loss = 0.6574(0.6326)
2023/09/26 23:37:34 - INFO - root -   Epoch: [16/200][120/346], lr: 0.00000012 	 loss = 1.1658(0.6740)
2023/09/26 23:38:17 - INFO - root -   Epoch: [16/200][140/346], lr: 0.00000012 	 loss = 0.5163(0.6728)
2023/09/26 23:39:18 - INFO - root -   Epoch: [16/200][160/346], lr: 0.00000012 	 loss = 0.2790(0.6894)
2023/09/26 23:40:01 - INFO - root -   Epoch: [16/200][180/346], lr: 0.00000012 	 loss = 0.5590(0.6958)
2023/09/26 23:41:02 - INFO - root -   Epoch: [16/200][200/346], lr: 0.00000012 	 loss = 0.4449(0.6888)
2023/09/26 23:41:46 - INFO - root -   Epoch: [16/200][220/346], lr: 0.00000012 	 loss = 0.1291(0.6895)
2023/09/26 23:42:46 - INFO - root -   Epoch: [16/200][240/346], lr: 0.00000012 	 loss = 0.2538(0.6833)
2023/09/26 23:43:30 - INFO - root -   Epoch: [16/200][260/346], lr: 0.00000012 	 loss = 0.3960(0.6755)
2023/09/26 23:44:30 - INFO - root -   Epoch: [16/200][280/346], lr: 0.00000012 	 loss = 0.4813(0.6796)
2023/09/26 23:45:14 - INFO - root -   Epoch: [16/200][300/346], lr: 0.00000012 	 loss = 1.0713(0.6816)
2023/09/26 23:46:13 - INFO - root -   Epoch: [16/200][320/346], lr: 0.00000012 	 loss = 0.2349(0.6860)
2023/09/26 23:46:57 - INFO - root -   Epoch: [16/200][340/346], lr: 0.00000012 	 loss = 0.8883(0.6817)
2023/09/26 23:47:01 - INFO - root -   Epoch: [16/200] 	 loss = 0.6848
2023/09/26 23:47:01 - INFO - root -   train_accuracy = 0.6055
2023/09/26 23:47:22 - INFO - root -   Epoch: [17/200][0/346], lr: 0.00000012 	 loss = 0.4047(0.4047)
2023/09/26 23:48:06 - INFO - root -   Epoch: [17/200][20/346], lr: 0.00000012 	 loss = 0.8396(0.6414)
2023/09/26 23:49:07 - INFO - root -   Epoch: [17/200][40/346], lr: 0.00000012 	 loss = 0.9942(0.6372)
2023/09/26 23:49:51 - INFO - root -   Epoch: [17/200][60/346], lr: 0.00000012 	 loss = 0.3739(0.6373)
2023/09/26 23:50:51 - INFO - root -   Epoch: [17/200][80/346], lr: 0.00000012 	 loss = 0.5278(0.6428)
2023/09/26 23:51:35 - INFO - root -   Epoch: [17/200][100/346], lr: 0.00000012 	 loss = 0.8402(0.6592)
2023/09/26 23:52:35 - INFO - root -   Epoch: [17/200][120/346], lr: 0.00000012 	 loss = 0.6192(0.6731)
2023/09/26 23:53:18 - INFO - root -   Epoch: [17/200][140/346], lr: 0.00000012 	 loss = 0.3794(0.6688)
2023/09/26 23:54:19 - INFO - root -   Epoch: [17/200][160/346], lr: 0.00000012 	 loss = 0.1138(0.6758)
2023/09/26 23:55:02 - INFO - root -   Epoch: [17/200][180/346], lr: 0.00000012 	 loss = 0.4765(0.6628)
2023/09/26 23:56:02 - INFO - root -   Epoch: [17/200][200/346], lr: 0.00000012 	 loss = 0.0564(0.6472)
2023/09/26 23:56:46 - INFO - root -   Epoch: [17/200][220/346], lr: 0.00000012 	 loss = 0.2040(0.6523)
2023/09/26 23:57:46 - INFO - root -   Epoch: [17/200][240/346], lr: 0.00000012 	 loss = 0.4651(0.6581)
2023/09/26 23:58:29 - INFO - root -   Epoch: [17/200][260/346], lr: 0.00000012 	 loss = 0.4643(0.6591)
2023/09/26 23:59:30 - INFO - root -   Epoch: [17/200][280/346], lr: 0.00000012 	 loss = 0.0969(0.6688)
2023/09/27 00:00:13 - INFO - root -   Epoch: [17/200][300/346], lr: 0.00000012 	 loss = 0.1892(0.6687)
2023/09/27 00:01:14 - INFO - root -   Epoch: [17/200][320/346], lr: 0.00000012 	 loss = 0.5110(0.6744)
2023/09/27 00:01:56 - INFO - root -   Epoch: [17/200][340/346], lr: 0.00000012 	 loss = 1.3784(0.6767)
2023/09/27 00:02:00 - INFO - root -   Epoch: [17/200] 	 loss = 0.6752
2023/09/27 00:02:00 - INFO - root -   train_accuracy = 0.6069
2023/09/27 00:02:22 - INFO - root -   Epoch: [18/200][0/346], lr: 0.00000013 	 loss = 0.6274(0.6274)
2023/09/27 00:03:05 - INFO - root -   Epoch: [18/200][20/346], lr: 0.00000013 	 loss = 0.7973(0.6435)
2023/09/27 00:04:06 - INFO - root -   Epoch: [18/200][40/346], lr: 0.00000013 	 loss = 0.2666(0.6165)
2023/09/27 00:04:49 - INFO - root -   Epoch: [18/200][60/346], lr: 0.00000013 	 loss = 0.3994(0.5990)
2023/09/27 00:05:50 - INFO - root -   Epoch: [18/200][80/346], lr: 0.00000013 	 loss = 0.5161(0.5933)
2023/09/27 00:06:33 - INFO - root -   Epoch: [18/200][100/346], lr: 0.00000013 	 loss = 0.9089(0.6201)
2023/09/27 00:07:34 - INFO - root -   Epoch: [18/200][120/346], lr: 0.00000013 	 loss = 0.3978(0.6291)
2023/09/27 00:08:17 - INFO - root -   Epoch: [18/200][140/346], lr: 0.00000013 	 loss = 1.0174(0.6265)
2023/09/27 00:09:18 - INFO - root -   Epoch: [18/200][160/346], lr: 0.00000013 	 loss = 0.3837(0.6347)
2023/09/27 00:10:01 - INFO - root -   Epoch: [18/200][180/346], lr: 0.00000013 	 loss = 0.8925(0.6370)
2023/09/27 00:11:01 - INFO - root -   Epoch: [18/200][200/346], lr: 0.00000013 	 loss = 0.3810(0.6484)
2023/09/27 00:11:44 - INFO - root -   Epoch: [18/200][220/346], lr: 0.00000013 	 loss = 0.5924(0.6521)
2023/09/27 00:12:45 - INFO - root -   Epoch: [18/200][240/346], lr: 0.00000013 	 loss = 0.2054(0.6512)
2023/09/27 00:13:28 - INFO - root -   Epoch: [18/200][260/346], lr: 0.00000013 	 loss = 0.4361(0.6498)
2023/09/27 00:14:29 - INFO - root -   Epoch: [18/200][280/346], lr: 0.00000013 	 loss = 0.5545(0.6589)
2023/09/27 00:15:12 - INFO - root -   Epoch: [18/200][300/346], lr: 0.00000013 	 loss = 0.3733(0.6640)
2023/09/27 00:16:12 - INFO - root -   Epoch: [18/200][320/346], lr: 0.00000013 	 loss = 0.6300(0.6610)
2023/09/27 00:16:54 - INFO - root -   Epoch: [18/200][340/346], lr: 0.00000013 	 loss = 1.0689(0.6625)
2023/09/27 00:16:59 - INFO - root -   Epoch: [18/200] 	 loss = 0.6623
2023/09/27 00:16:59 - INFO - root -   train_accuracy = 0.6228
2023/09/27 00:17:21 - INFO - root -   Epoch: [19/200][0/346], lr: 0.00000013 	 loss = 0.6174(0.6174)
2023/09/27 00:18:04 - INFO - root -   Epoch: [19/200][20/346], lr: 0.00000013 	 loss = 0.6414(0.5713)
2023/09/27 00:19:05 - INFO - root -   Epoch: [19/200][40/346], lr: 0.00000013 	 loss = 0.9422(0.6127)
2023/09/27 00:19:49 - INFO - root -   Epoch: [19/200][60/346], lr: 0.00000013 	 loss = 0.4925(0.5960)
2023/09/27 00:20:50 - INFO - root -   Epoch: [19/200][80/346], lr: 0.00000013 	 loss = 0.7194(0.6273)
2023/09/27 00:21:33 - INFO - root -   Epoch: [19/200][100/346], lr: 0.00000013 	 loss = 0.9804(0.6262)
2023/09/27 00:22:34 - INFO - root -   Epoch: [19/200][120/346], lr: 0.00000013 	 loss = 0.4106(0.6546)
2023/09/27 00:23:18 - INFO - root -   Epoch: [19/200][140/346], lr: 0.00000013 	 loss = 0.5824(0.6444)
2023/09/27 00:24:18 - INFO - root -   Epoch: [19/200][160/346], lr: 0.00000013 	 loss = 0.4544(0.6579)
2023/09/27 00:25:01 - INFO - root -   Epoch: [19/200][180/346], lr: 0.00000013 	 loss = 1.0681(0.6566)
2023/09/27 00:26:02 - INFO - root -   Epoch: [19/200][200/346], lr: 0.00000013 	 loss = 0.4245(0.6580)
2023/09/27 00:26:46 - INFO - root -   Epoch: [19/200][220/346], lr: 0.00000013 	 loss = 0.6369(0.6734)
2023/09/27 00:27:47 - INFO - root -   Epoch: [19/200][240/346], lr: 0.00000013 	 loss = 0.5106(0.6754)
2023/09/27 00:28:30 - INFO - root -   Epoch: [19/200][260/346], lr: 0.00000013 	 loss = 0.9704(0.6746)
2023/09/27 00:29:31 - INFO - root -   Epoch: [19/200][280/346], lr: 0.00000013 	 loss = 0.4628(0.6806)
2023/09/27 00:30:14 - INFO - root -   Epoch: [19/200][300/346], lr: 0.00000013 	 loss = 0.3214(0.6782)
2023/09/27 00:31:15 - INFO - root -   Epoch: [19/200][320/346], lr: 0.00000013 	 loss = 0.6661(0.6769)
2023/09/27 00:31:56 - INFO - root -   Epoch: [19/200][340/346], lr: 0.00000013 	 loss = 0.5145(0.6804)
2023/09/27 00:31:59 - INFO - root -   Epoch: [19/200] 	 loss = 0.6797
2023/09/27 00:37:53 - INFO - root -   precision = 0.5690
2023/09/27 00:37:53 - INFO - root -   eval_loss = 0.7591
2023/09/27 00:37:54 - INFO - root -   train_accuracy = 0.6069
2023/09/27 00:38:16 - INFO - root -   Epoch: [20/200][0/346], lr: 0.00000013 	 loss = 0.3227(0.3227)
2023/09/27 00:38:59 - INFO - root -   Epoch: [20/200][20/346], lr: 0.00000013 	 loss = 0.7079(0.5556)
2023/09/27 00:39:59 - INFO - root -   Epoch: [20/200][40/346], lr: 0.00000013 	 loss = 0.6494(0.6198)
2023/09/27 00:40:43 - INFO - root -   Epoch: [20/200][60/346], lr: 0.00000013 	 loss = 0.5035(0.6405)
2023/09/27 00:41:44 - INFO - root -   Epoch: [20/200][80/346], lr: 0.00000013 	 loss = 0.9853(0.6307)
2023/09/27 00:42:27 - INFO - root -   Epoch: [20/200][100/346], lr: 0.00000013 	 loss = 0.6184(0.6327)
2023/09/27 00:43:28 - INFO - root -   Epoch: [20/200][120/346], lr: 0.00000013 	 loss = 0.3400(0.6545)
2023/09/27 00:44:11 - INFO - root -   Epoch: [20/200][140/346], lr: 0.00000013 	 loss = 0.6050(0.6423)
2023/09/27 00:45:11 - INFO - root -   Epoch: [20/200][160/346], lr: 0.00000013 	 loss = 0.2406(0.6393)
2023/09/27 00:45:55 - INFO - root -   Epoch: [20/200][180/346], lr: 0.00000013 	 loss = 0.4238(0.6418)
2023/09/27 00:46:55 - INFO - root -   Epoch: [20/200][200/346], lr: 0.00000013 	 loss = 0.5327(0.6480)
2023/09/27 00:47:38 - INFO - root -   Epoch: [20/200][220/346], lr: 0.00000013 	 loss = 0.2462(0.6472)
2023/09/27 00:48:38 - INFO - root -   Epoch: [20/200][240/346], lr: 0.00000013 	 loss = 0.3302(0.6484)
2023/09/27 00:49:22 - INFO - root -   Epoch: [20/200][260/346], lr: 0.00000013 	 loss = 1.0769(0.6505)
2023/09/27 00:50:21 - INFO - root -   Epoch: [20/200][280/346], lr: 0.00000013 	 loss = 0.9100(0.6606)
2023/09/27 00:51:06 - INFO - root -   Epoch: [20/200][300/346], lr: 0.00000013 	 loss = 0.4562(0.6599)
2023/09/27 00:52:05 - INFO - root -   Epoch: [20/200][320/346], lr: 0.00000013 	 loss = 0.4419(0.6633)
2023/09/27 00:52:48 - INFO - root -   Epoch: [20/200][340/346], lr: 0.00000013 	 loss = 0.7786(0.6588)
2023/09/27 00:52:52 - INFO - root -   Epoch: [20/200] 	 loss = 0.6622
2023/09/27 00:52:52 - INFO - root -   train_accuracy = 0.6055
2023/09/27 00:53:14 - INFO - root -   Epoch: [21/200][0/346], lr: 0.00000013 	 loss = 0.3477(0.3477)
2023/09/27 00:53:57 - INFO - root -   Epoch: [21/200][20/346], lr: 0.00000013 	 loss = 0.8290(0.5748)
2023/09/27 00:54:58 - INFO - root -   Epoch: [21/200][40/346], lr: 0.00000013 	 loss = 0.7637(0.6989)
2023/09/27 00:55:41 - INFO - root -   Epoch: [21/200][60/346], lr: 0.00000013 	 loss = 0.5340(0.6787)
2023/09/27 00:56:42 - INFO - root -   Epoch: [21/200][80/346], lr: 0.00000013 	 loss = 0.6661(0.6577)
2023/09/27 00:57:26 - INFO - root -   Epoch: [21/200][100/346], lr: 0.00000013 	 loss = 0.5611(0.6560)
2023/09/27 00:58:26 - INFO - root -   Epoch: [21/200][120/346], lr: 0.00000013 	 loss = 0.6377(0.6701)
2023/09/27 00:59:10 - INFO - root -   Epoch: [21/200][140/346], lr: 0.00000013 	 loss = 0.8279(0.6598)
2023/09/27 01:00:10 - INFO - root -   Epoch: [21/200][160/346], lr: 0.00000013 	 loss = 0.4022(0.6611)
2023/09/27 01:00:53 - INFO - root -   Epoch: [21/200][180/346], lr: 0.00000013 	 loss = 0.5700(0.6662)
2023/09/27 01:01:54 - INFO - root -   Epoch: [21/200][200/346], lr: 0.00000013 	 loss = 0.6876(0.6794)
2023/09/27 01:02:37 - INFO - root -   Epoch: [21/200][220/346], lr: 0.00000013 	 loss = 0.2907(0.6768)
2023/09/27 01:03:38 - INFO - root -   Epoch: [21/200][240/346], lr: 0.00000013 	 loss = 0.5155(0.6698)
2023/09/27 01:04:21 - INFO - root -   Epoch: [21/200][260/346], lr: 0.00000013 	 loss = 0.9447(0.6696)
2023/09/27 01:05:21 - INFO - root -   Epoch: [21/200][280/346], lr: 0.00000013 	 loss = 0.3235(0.6767)
2023/09/27 01:06:05 - INFO - root -   Epoch: [21/200][300/346], lr: 0.00000013 	 loss = 0.1976(0.6834)
2023/09/27 01:07:05 - INFO - root -   Epoch: [21/200][320/346], lr: 0.00000013 	 loss = 0.6927(0.6819)
2023/09/27 01:07:46 - INFO - root -   Epoch: [21/200][340/346], lr: 0.00000013 	 loss = 0.9392(0.6799)
2023/09/27 01:07:50 - INFO - root -   Epoch: [21/200] 	 loss = 0.6781
2023/09/27 01:07:50 - INFO - root -   train_accuracy = 0.6156
2023/09/27 01:08:12 - INFO - root -   Epoch: [22/200][0/346], lr: 0.00000013 	 loss = 0.2304(0.2304)
2023/09/27 01:08:55 - INFO - root -   Epoch: [22/200][20/346], lr: 0.00000013 	 loss = 0.5264(0.4868)
2023/09/27 01:09:56 - INFO - root -   Epoch: [22/200][40/346], lr: 0.00000013 	 loss = 0.5535(0.5777)
2023/09/27 01:10:39 - INFO - root -   Epoch: [22/200][60/346], lr: 0.00000013 	 loss = 0.9073(0.5917)
2023/09/27 01:11:40 - INFO - root -   Epoch: [22/200][80/346], lr: 0.00000013 	 loss = 0.9738(0.6292)
2023/09/27 01:12:23 - INFO - root -   Epoch: [22/200][100/346], lr: 0.00000013 	 loss = 0.3904(0.6401)
2023/09/27 01:13:24 - INFO - root -   Epoch: [22/200][120/346], lr: 0.00000013 	 loss = 0.4945(0.6654)
2023/09/27 01:14:07 - INFO - root -   Epoch: [22/200][140/346], lr: 0.00000013 	 loss = 0.3841(0.6604)
2023/09/27 01:15:08 - INFO - root -   Epoch: [22/200][160/346], lr: 0.00000013 	 loss = 0.1622(0.6550)
2023/09/27 01:15:51 - INFO - root -   Epoch: [22/200][180/346], lr: 0.00000013 	 loss = 0.5643(0.6537)
2023/09/27 01:16:52 - INFO - root -   Epoch: [22/200][200/346], lr: 0.00000013 	 loss = 0.2171(0.6420)
2023/09/27 01:17:35 - INFO - root -   Epoch: [22/200][220/346], lr: 0.00000013 	 loss = 0.4466(0.6474)
2023/09/27 01:18:36 - INFO - root -   Epoch: [22/200][240/346], lr: 0.00000013 	 loss = 0.1410(0.6459)
2023/09/27 01:19:19 - INFO - root -   Epoch: [22/200][260/346], lr: 0.00000013 	 loss = 0.6642(0.6456)
2023/09/27 01:20:19 - INFO - root -   Epoch: [22/200][280/346], lr: 0.00000013 	 loss = 0.7064(0.6531)
2023/09/27 01:21:02 - INFO - root -   Epoch: [22/200][300/346], lr: 0.00000013 	 loss = 0.3239(0.6581)
2023/09/27 01:22:03 - INFO - root -   Epoch: [22/200][320/346], lr: 0.00000013 	 loss = 0.5717(0.6587)
2023/09/27 01:22:44 - INFO - root -   Epoch: [22/200][340/346], lr: 0.00000013 	 loss = 0.5726(0.6527)
2023/09/27 01:22:48 - INFO - root -   Epoch: [22/200] 	 loss = 0.6504
2023/09/27 01:22:48 - INFO - root -   train_accuracy = 0.6431
2023/09/27 01:23:10 - INFO - root -   Epoch: [23/200][0/346], lr: 0.00000013 	 loss = 0.4254(0.4254)
2023/09/27 01:23:53 - INFO - root -   Epoch: [23/200][20/346], lr: 0.00000013 	 loss = 0.7408(0.5821)
2023/09/27 01:24:53 - INFO - root -   Epoch: [23/200][40/346], lr: 0.00000013 	 loss = 0.6738(0.5998)
2023/09/27 01:25:37 - INFO - root -   Epoch: [23/200][60/346], lr: 0.00000013 	 loss = 0.5882(0.5793)
2023/09/27 01:26:38 - INFO - root -   Epoch: [23/200][80/346], lr: 0.00000013 	 loss = 0.8280(0.6030)
2023/09/27 01:27:21 - INFO - root -   Epoch: [23/200][100/346], lr: 0.00000013 	 loss = 0.9216(0.6283)
2023/09/27 01:28:22 - INFO - root -   Epoch: [23/200][120/346], lr: 0.00000013 	 loss = 0.9062(0.6647)
2023/09/27 01:29:05 - INFO - root -   Epoch: [23/200][140/346], lr: 0.00000013 	 loss = 0.5358(0.6584)
2023/09/27 01:30:06 - INFO - root -   Epoch: [23/200][160/346], lr: 0.00000013 	 loss = 0.1863(0.6573)
2023/09/27 01:30:49 - INFO - root -   Epoch: [23/200][180/346], lr: 0.00000013 	 loss = 0.7295(0.6564)
2023/09/27 01:31:50 - INFO - root -   Epoch: [23/200][200/346], lr: 0.00000013 	 loss = 0.3932(0.6628)
2023/09/27 01:32:33 - INFO - root -   Epoch: [23/200][220/346], lr: 0.00000013 	 loss = 0.1559(0.6587)
2023/09/27 01:33:34 - INFO - root -   Epoch: [23/200][240/346], lr: 0.00000013 	 loss = 0.2589(0.6535)
2023/09/27 01:34:17 - INFO - root -   Epoch: [23/200][260/346], lr: 0.00000013 	 loss = 0.5462(0.6468)
2023/09/27 01:35:18 - INFO - root -   Epoch: [23/200][280/346], lr: 0.00000013 	 loss = 0.2664(0.6547)
2023/09/27 01:36:01 - INFO - root -   Epoch: [23/200][300/346], lr: 0.00000013 	 loss = 0.3023(0.6577)
2023/09/27 01:37:02 - INFO - root -   Epoch: [23/200][320/346], lr: 0.00000013 	 loss = 0.6828(0.6650)
2023/09/27 01:37:44 - INFO - root -   Epoch: [23/200][340/346], lr: 0.00000013 	 loss = 0.7581(0.6602)
2023/09/27 01:37:48 - INFO - root -   Epoch: [23/200] 	 loss = 0.6573
2023/09/27 01:37:48 - INFO - root -   train_accuracy = 0.6257
2023/09/27 01:38:11 - INFO - root -   Epoch: [24/200][0/346], lr: 0.00000013 	 loss = 0.3483(0.3483)
2023/09/27 01:38:55 - INFO - root -   Epoch: [24/200][20/346], lr: 0.00000013 	 loss = 0.9385(0.5529)
2023/09/27 01:39:57 - INFO - root -   Epoch: [24/200][40/346], lr: 0.00000013 	 loss = 0.6586(0.5823)
2023/09/27 01:40:40 - INFO - root -   Epoch: [24/200][60/346], lr: 0.00000013 	 loss = 0.5790(0.5621)
2023/09/27 01:41:42 - INFO - root -   Epoch: [24/200][80/346], lr: 0.00000013 	 loss = 0.8196(0.5783)
2023/09/27 01:42:25 - INFO - root -   Epoch: [24/200][100/346], lr: 0.00000013 	 loss = 0.8090(0.5906)
2023/09/27 01:43:26 - INFO - root -   Epoch: [24/200][120/346], lr: 0.00000013 	 loss = 0.5869(0.6133)
2023/09/27 01:44:10 - INFO - root -   Epoch: [24/200][140/346], lr: 0.00000013 	 loss = 0.6862(0.6191)
2023/09/27 01:45:11 - INFO - root -   Epoch: [24/200][160/346], lr: 0.00000013 	 loss = 0.3391(0.6262)
2023/09/27 01:45:54 - INFO - root -   Epoch: [24/200][180/346], lr: 0.00000013 	 loss = 0.6888(0.6322)
2023/09/27 01:46:55 - INFO - root -   Epoch: [24/200][200/346], lr: 0.00000013 	 loss = 0.3750(0.6245)
2023/09/27 01:47:39 - INFO - root -   Epoch: [24/200][220/346], lr: 0.00000013 	 loss = 0.2670(0.6297)
2023/09/27 01:48:40 - INFO - root -   Epoch: [24/200][240/346], lr: 0.00000013 	 loss = 0.2600(0.6359)
2023/09/27 01:49:23 - INFO - root -   Epoch: [24/200][260/346], lr: 0.00000013 	 loss = 0.5959(0.6326)
2023/09/27 01:50:24 - INFO - root -   Epoch: [24/200][280/346], lr: 0.00000013 	 loss = 0.4810(0.6372)
2023/09/27 01:51:07 - INFO - root -   Epoch: [24/200][300/346], lr: 0.00000013 	 loss = 0.7054(0.6424)
2023/09/27 01:52:08 - INFO - root -   Epoch: [24/200][320/346], lr: 0.00000013 	 loss = 0.3623(0.6394)
2023/09/27 01:52:51 - INFO - root -   Epoch: [24/200][340/346], lr: 0.00000013 	 loss = 0.8538(0.6404)
2023/09/27 01:52:55 - INFO - root -   Epoch: [24/200] 	 loss = 0.6403
2023/09/27 01:58:43 - INFO - root -   precision = 0.6437
2023/09/27 01:58:43 - INFO - root -   eval_loss = 0.6663
2023/09/27 01:58:44 - INFO - root -   train_accuracy = 0.6517
2023/09/27 01:59:05 - INFO - root -   Epoch: [25/200][0/346], lr: 0.00000014 	 loss = 0.4115(0.4115)
2023/09/27 01:59:49 - INFO - root -   Epoch: [25/200][20/346], lr: 0.00000014 	 loss = 0.9682(0.5847)
2023/09/27 02:00:51 - INFO - root -   Epoch: [25/200][40/346], lr: 0.00000014 	 loss = 0.4834(0.6298)
2023/09/27 02:01:35 - INFO - root -   Epoch: [25/200][60/346], lr: 0.00000014 	 loss = 0.3227(0.5885)
2023/09/27 02:02:36 - INFO - root -   Epoch: [25/200][80/346], lr: 0.00000014 	 loss = 1.2356(0.6043)
2023/09/27 02:03:19 - INFO - root -   Epoch: [25/200][100/346], lr: 0.00000014 	 loss = 0.4559(0.6257)
2023/09/27 02:04:20 - INFO - root -   Epoch: [25/200][120/346], lr: 0.00000014 	 loss = 0.3629(0.6491)
2023/09/27 02:05:04 - INFO - root -   Epoch: [25/200][140/346], lr: 0.00000014 	 loss = 0.4804(0.6384)
2023/09/27 02:06:05 - INFO - root -   Epoch: [25/200][160/346], lr: 0.00000014 	 loss = 0.1745(0.6278)
2023/09/27 02:06:48 - INFO - root -   Epoch: [25/200][180/346], lr: 0.00000014 	 loss = 0.3901(0.6365)
2023/09/27 02:07:49 - INFO - root -   Epoch: [25/200][200/346], lr: 0.00000014 	 loss = 0.2794(0.6369)
2023/09/27 02:08:32 - INFO - root -   Epoch: [25/200][220/346], lr: 0.00000014 	 loss = 0.3574(0.6359)
2023/09/27 02:09:33 - INFO - root -   Epoch: [25/200][240/346], lr: 0.00000014 	 loss = 0.5142(0.6351)
2023/09/27 02:10:16 - INFO - root -   Epoch: [25/200][260/346], lr: 0.00000014 	 loss = 0.7329(0.6298)
2023/09/27 02:11:17 - INFO - root -   Epoch: [25/200][280/346], lr: 0.00000014 	 loss = 0.3533(0.6430)
2023/09/27 02:12:00 - INFO - root -   Epoch: [25/200][300/346], lr: 0.00000014 	 loss = 0.3872(0.6465)
2023/09/27 02:13:01 - INFO - root -   Epoch: [25/200][320/346], lr: 0.00000014 	 loss = 0.3688(0.6451)
2023/09/27 02:13:44 - INFO - root -   Epoch: [25/200][340/346], lr: 0.00000014 	 loss = 1.3075(0.6456)
2023/09/27 02:13:48 - INFO - root -   Epoch: [25/200] 	 loss = 0.6435
2023/09/27 02:13:48 - INFO - root -   train_accuracy = 0.6662
2023/09/27 02:14:10 - INFO - root -   Epoch: [26/200][0/346], lr: 0.00000014 	 loss = 0.4974(0.4974)
2023/09/27 02:14:53 - INFO - root -   Epoch: [26/200][20/346], lr: 0.00000014 	 loss = 0.3081(0.6347)
2023/09/27 02:15:54 - INFO - root -   Epoch: [26/200][40/346], lr: 0.00000014 	 loss = 0.5468(0.6581)
2023/09/27 02:16:37 - INFO - root -   Epoch: [26/200][60/346], lr: 0.00000014 	 loss = 0.4757(0.6393)
2023/09/27 02:17:38 - INFO - root -   Epoch: [26/200][80/346], lr: 0.00000014 	 loss = 0.6395(0.6823)
2023/09/27 02:18:21 - INFO - root -   Epoch: [26/200][100/346], lr: 0.00000014 	 loss = 0.5105(0.6991)
2023/09/27 02:19:22 - INFO - root -   Epoch: [26/200][120/346], lr: 0.00000014 	 loss = 0.6562(0.7326)
2023/09/27 02:20:05 - INFO - root -   Epoch: [26/200][140/346], lr: 0.00000014 	 loss = 0.4162(0.7146)
2023/09/27 02:21:06 - INFO - root -   Epoch: [26/200][160/346], lr: 0.00000014 	 loss = 0.2048(0.7091)
2023/09/27 02:21:49 - INFO - root -   Epoch: [26/200][180/346], lr: 0.00000014 	 loss = 0.5920(0.7086)
2023/09/27 02:22:50 - INFO - root -   Epoch: [26/200][200/346], lr: 0.00000014 	 loss = 0.4247(0.7081)
2023/09/27 02:23:33 - INFO - root -   Epoch: [26/200][220/346], lr: 0.00000014 	 loss = 0.5724(0.7117)
2023/09/27 02:24:34 - INFO - root -   Epoch: [26/200][240/346], lr: 0.00000014 	 loss = 0.3289(0.7023)
2023/09/27 02:25:17 - INFO - root -   Epoch: [26/200][260/346], lr: 0.00000014 	 loss = 0.6179(0.6967)
2023/09/27 02:26:17 - INFO - root -   Epoch: [26/200][280/346], lr: 0.00000014 	 loss = 0.9367(0.7060)
2023/09/27 02:27:01 - INFO - root -   Epoch: [26/200][300/346], lr: 0.00000014 	 loss = 0.2460(0.7087)
2023/09/27 02:28:01 - INFO - root -   Epoch: [26/200][320/346], lr: 0.00000014 	 loss = 0.4333(0.7040)
2023/09/27 02:28:43 - INFO - root -   Epoch: [26/200][340/346], lr: 0.00000014 	 loss = 0.9710(0.6975)
2023/09/27 02:28:47 - INFO - root -   Epoch: [26/200] 	 loss = 0.6962
2023/09/27 02:28:47 - INFO - root -   train_accuracy = 0.5925
2023/09/27 02:29:09 - INFO - root -   Epoch: [27/200][0/346], lr: 0.00000014 	 loss = 0.4778(0.4778)
2023/09/27 02:29:53 - INFO - root -   Epoch: [27/200][20/346], lr: 0.00000014 	 loss = 0.5942(0.5255)
2023/09/27 02:30:54 - INFO - root -   Epoch: [27/200][40/346], lr: 0.00000014 	 loss = 0.9466(0.5868)
2023/09/27 02:31:37 - INFO - root -   Epoch: [27/200][60/346], lr: 0.00000014 	 loss = 0.4465(0.5605)
2023/09/27 02:32:38 - INFO - root -   Epoch: [27/200][80/346], lr: 0.00000014 	 loss = 1.0877(0.5823)
2023/09/27 02:33:22 - INFO - root -   Epoch: [27/200][100/346], lr: 0.00000014 	 loss = 0.6943(0.5733)
2023/09/27 02:34:22 - INFO - root -   Epoch: [27/200][120/346], lr: 0.00000014 	 loss = 0.7004(0.6099)
2023/09/27 02:35:06 - INFO - root -   Epoch: [27/200][140/346], lr: 0.00000014 	 loss = 0.6322(0.6118)
2023/09/27 02:36:07 - INFO - root -   Epoch: [27/200][160/346], lr: 0.00000014 	 loss = 0.4483(0.6186)
2023/09/27 02:36:50 - INFO - root -   Epoch: [27/200][180/346], lr: 0.00000014 	 loss = 0.5066(0.6182)
2023/09/27 02:37:51 - INFO - root -   Epoch: [27/200][200/346], lr: 0.00000014 	 loss = 0.3853(0.6181)
2023/09/27 02:38:34 - INFO - root -   Epoch: [27/200][220/346], lr: 0.00000014 	 loss = 0.1561(0.6207)
2023/09/27 02:39:35 - INFO - root -   Epoch: [27/200][240/346], lr: 0.00000014 	 loss = 0.2318(0.6234)
2023/09/27 02:40:18 - INFO - root -   Epoch: [27/200][260/346], lr: 0.00000014 	 loss = 0.6389(0.6192)
2023/09/27 02:41:19 - INFO - root -   Epoch: [27/200][280/346], lr: 0.00000014 	 loss = 0.9156(0.6272)
2023/09/27 02:42:02 - INFO - root -   Epoch: [27/200][300/346], lr: 0.00000014 	 loss = 0.4767(0.6323)
2023/09/27 02:43:03 - INFO - root -   Epoch: [27/200][320/346], lr: 0.00000014 	 loss = 0.6103(0.6307)
2023/09/27 02:43:45 - INFO - root -   Epoch: [27/200][340/346], lr: 0.00000014 	 loss = 0.7873(0.6297)
2023/09/27 02:43:49 - INFO - root -   Epoch: [27/200] 	 loss = 0.6269
2023/09/27 02:43:49 - INFO - root -   train_accuracy = 0.6590
2023/09/27 02:44:11 - INFO - root -   Epoch: [28/200][0/346], lr: 0.00000014 	 loss = 0.2715(0.2715)
2023/09/27 02:44:54 - INFO - root -   Epoch: [28/200][20/346], lr: 0.00000014 	 loss = 0.4222(0.5176)
2023/09/27 02:45:55 - INFO - root -   Epoch: [28/200][40/346], lr: 0.00000014 	 loss = 0.7135(0.5449)
2023/09/27 02:46:38 - INFO - root -   Epoch: [28/200][60/346], lr: 0.00000014 	 loss = 0.6646(0.5327)
2023/09/27 02:47:39 - INFO - root -   Epoch: [28/200][80/346], lr: 0.00000014 	 loss = 0.5463(0.5931)
2023/09/27 02:48:22 - INFO - root -   Epoch: [28/200][100/346], lr: 0.00000014 	 loss = 0.7521(0.6107)
2023/09/27 02:49:23 - INFO - root -   Epoch: [28/200][120/346], lr: 0.00000014 	 loss = 0.6643(0.6266)
2023/09/27 02:50:06 - INFO - root -   Epoch: [28/200][140/346], lr: 0.00000014 	 loss = 0.3385(0.6369)
2023/09/27 02:51:07 - INFO - root -   Epoch: [28/200][160/346], lr: 0.00000014 	 loss = 0.1637(0.6396)
2023/09/27 02:51:50 - INFO - root -   Epoch: [28/200][180/346], lr: 0.00000014 	 loss = 0.7015(0.6352)
2023/09/27 02:52:50 - INFO - root -   Epoch: [28/200][200/346], lr: 0.00000014 	 loss = 0.1316(0.6377)
2023/09/27 02:53:33 - INFO - root -   Epoch: [28/200][220/346], lr: 0.00000014 	 loss = 0.1232(0.6346)
2023/09/27 02:54:34 - INFO - root -   Epoch: [28/200][240/346], lr: 0.00000014 	 loss = 0.3965(0.6378)
2023/09/27 02:55:17 - INFO - root -   Epoch: [28/200][260/346], lr: 0.00000014 	 loss = 0.7153(0.6422)
2023/09/27 02:56:18 - INFO - root -   Epoch: [28/200][280/346], lr: 0.00000014 	 loss = 0.4135(0.6463)
2023/09/27 02:57:01 - INFO - root -   Epoch: [28/200][300/346], lr: 0.00000014 	 loss = 0.3572(0.6450)
2023/09/27 02:58:01 - INFO - root -   Epoch: [28/200][320/346], lr: 0.00000014 	 loss = 0.5667(0.6470)
2023/09/27 02:58:43 - INFO - root -   Epoch: [28/200][340/346], lr: 0.00000014 	 loss = 1.3004(0.6430)
2023/09/27 02:58:47 - INFO - root -   Epoch: [28/200] 	 loss = 0.6413
2023/09/27 02:58:47 - INFO - root -   train_accuracy = 0.6387
2023/09/27 02:59:09 - INFO - root -   Epoch: [29/200][0/346], lr: 0.00000014 	 loss = 0.1362(0.1362)
2023/09/27 02:59:53 - INFO - root -   Epoch: [29/200][20/346], lr: 0.00000014 	 loss = 0.9909(0.5717)
2023/09/27 03:00:55 - INFO - root -   Epoch: [29/200][40/346], lr: 0.00000014 	 loss = 0.5265(0.6000)
2023/09/27 03:01:39 - INFO - root -   Epoch: [29/200][60/346], lr: 0.00000014 	 loss = 0.6477(0.6139)
2023/09/27 03:02:41 - INFO - root -   Epoch: [29/200][80/346], lr: 0.00000014 	 loss = 0.8640(0.6389)
2023/09/27 03:03:24 - INFO - root -   Epoch: [29/200][100/346], lr: 0.00000014 	 loss = 1.1723(0.6350)
2023/09/27 03:04:26 - INFO - root -   Epoch: [29/200][120/346], lr: 0.00000014 	 loss = 0.3510(0.6492)
2023/09/27 03:05:09 - INFO - root -   Epoch: [29/200][140/346], lr: 0.00000014 	 loss = 0.8405(0.6358)
2023/09/27 03:06:11 - INFO - root -   Epoch: [29/200][160/346], lr: 0.00000014 	 loss = 0.1530(0.6379)
2023/09/27 03:06:54 - INFO - root -   Epoch: [29/200][180/346], lr: 0.00000014 	 loss = 0.8913(0.6338)
2023/09/27 03:07:56 - INFO - root -   Epoch: [29/200][200/346], lr: 0.00000014 	 loss = 0.3792(0.6342)
2023/09/27 03:08:39 - INFO - root -   Epoch: [29/200][220/346], lr: 0.00000014 	 loss = 0.3841(0.6352)
2023/09/27 03:09:41 - INFO - root -   Epoch: [29/200][240/346], lr: 0.00000014 	 loss = 0.5871(0.6369)
2023/09/27 03:10:24 - INFO - root -   Epoch: [29/200][260/346], lr: 0.00000014 	 loss = 0.6982(0.6395)
2023/09/27 03:11:26 - INFO - root -   Epoch: [29/200][280/346], lr: 0.00000014 	 loss = 0.5954(0.6429)
2023/09/27 03:12:09 - INFO - root -   Epoch: [29/200][300/346], lr: 0.00000014 	 loss = 0.3496(0.6403)
2023/09/27 03:13:10 - INFO - root -   Epoch: [29/200][320/346], lr: 0.00000014 	 loss = 0.7449(0.6395)
2023/09/27 03:13:52 - INFO - root -   Epoch: [29/200][340/346], lr: 0.00000014 	 loss = 0.9733(0.6351)
2023/09/27 03:13:56 - INFO - root -   Epoch: [29/200] 	 loss = 0.6323
2023/09/27 03:19:53 - INFO - root -   precision = 0.5862
2023/09/27 03:19:53 - INFO - root -   eval_loss = 0.7200
2023/09/27 03:19:54 - INFO - root -   train_accuracy = 0.6590
2023/09/27 03:20:16 - INFO - root -   Epoch: [30/200][0/346], lr: 0.00000014 	 loss = 0.3650(0.3650)
2023/09/27 03:20:59 - INFO - root -   Epoch: [30/200][20/346], lr: 0.00000014 	 loss = 0.9390(0.5467)
2023/09/27 03:22:00 - INFO - root -   Epoch: [30/200][40/346], lr: 0.00000014 	 loss = 0.7775(0.5607)
2023/09/27 03:22:44 - INFO - root -   Epoch: [30/200][60/346], lr: 0.00000014 	 loss = 0.2419(0.5493)
2023/09/27 03:23:45 - INFO - root -   Epoch: [30/200][80/346], lr: 0.00000014 	 loss = 0.6821(0.5766)
2023/09/27 03:24:28 - INFO - root -   Epoch: [30/200][100/346], lr: 0.00000014 	 loss = 0.8891(0.5993)
2023/09/27 03:25:29 - INFO - root -   Epoch: [30/200][120/346], lr: 0.00000014 	 loss = 0.4988(0.6420)
2023/09/27 03:26:13 - INFO - root -   Epoch: [30/200][140/346], lr: 0.00000014 	 loss = 0.5285(0.6274)
2023/09/27 03:27:13 - INFO - root -   Epoch: [30/200][160/346], lr: 0.00000014 	 loss = 0.1434(0.6477)
2023/09/27 03:27:57 - INFO - root -   Epoch: [30/200][180/346], lr: 0.00000014 	 loss = 0.6108(0.6367)
2023/09/27 03:28:57 - INFO - root -   Epoch: [30/200][200/346], lr: 0.00000014 	 loss = 0.2664(0.6406)
2023/09/27 03:29:41 - INFO - root -   Epoch: [30/200][220/346], lr: 0.00000014 	 loss = 0.0799(0.6415)
2023/09/27 03:30:41 - INFO - root -   Epoch: [30/200][240/346], lr: 0.00000014 	 loss = 0.4052(0.6418)
2023/09/27 03:31:25 - INFO - root -   Epoch: [30/200][260/346], lr: 0.00000014 	 loss = 0.5548(0.6391)
2023/09/27 03:32:24 - INFO - root -   Epoch: [30/200][280/346], lr: 0.00000014 	 loss = 0.4038(0.6524)
2023/09/27 03:33:09 - INFO - root -   Epoch: [30/200][300/346], lr: 0.00000014 	 loss = 0.2635(0.6540)
2023/09/27 03:34:08 - INFO - root -   Epoch: [30/200][320/346], lr: 0.00000014 	 loss = 0.2768(0.6516)
2023/09/27 03:34:52 - INFO - root -   Epoch: [30/200][340/346], lr: 0.00000014 	 loss = 0.9413(0.6480)
2023/09/27 03:34:55 - INFO - root -   Epoch: [30/200] 	 loss = 0.6507
2023/09/27 03:34:55 - INFO - root -   train_accuracy = 0.6402
2023/09/27 03:35:17 - INFO - root -   Epoch: [31/200][0/346], lr: 0.00000014 	 loss = 0.2481(0.2481)
2023/09/27 03:36:00 - INFO - root -   Epoch: [31/200][20/346], lr: 0.00000014 	 loss = 0.6314(0.5721)
2023/09/27 03:37:02 - INFO - root -   Epoch: [31/200][40/346], lr: 0.00000014 	 loss = 0.2196(0.5988)
2023/09/27 03:37:45 - INFO - root -   Epoch: [31/200][60/346], lr: 0.00000014 	 loss = 0.1997(0.5713)
2023/09/27 03:38:46 - INFO - root -   Epoch: [31/200][80/346], lr: 0.00000014 	 loss = 0.8370(0.5651)
2023/09/27 03:39:30 - INFO - root -   Epoch: [31/200][100/346], lr: 0.00000014 	 loss = 0.9496(0.5909)
2023/09/27 03:40:31 - INFO - root -   Epoch: [31/200][120/346], lr: 0.00000014 	 loss = 0.6191(0.6288)
2023/09/27 03:41:14 - INFO - root -   Epoch: [31/200][140/346], lr: 0.00000014 	 loss = 0.7984(0.6311)
2023/09/27 03:42:15 - INFO - root -   Epoch: [31/200][160/346], lr: 0.00000014 	 loss = 0.1869(0.6273)
2023/09/27 03:42:58 - INFO - root -   Epoch: [31/200][180/346], lr: 0.00000014 	 loss = 0.6073(0.6286)
2023/09/27 03:43:59 - INFO - root -   Epoch: [31/200][200/346], lr: 0.00000014 	 loss = 0.1821(0.6298)
2023/09/27 03:44:43 - INFO - root -   Epoch: [31/200][220/346], lr: 0.00000014 	 loss = 0.0584(0.6270)
2023/09/27 03:45:44 - INFO - root -   Epoch: [31/200][240/346], lr: 0.00000014 	 loss = 0.2437(0.6269)
2023/09/27 03:46:27 - INFO - root -   Epoch: [31/200][260/346], lr: 0.00000014 	 loss = 0.4554(0.6212)
2023/09/27 03:47:28 - INFO - root -   Epoch: [31/200][280/346], lr: 0.00000014 	 loss = 0.5360(0.6342)
2023/09/27 03:48:12 - INFO - root -   Epoch: [31/200][300/346], lr: 0.00000014 	 loss = 0.2073(0.6343)
2023/09/27 03:49:13 - INFO - root -   Epoch: [31/200][320/346], lr: 0.00000014 	 loss = 0.2869(0.6307)
2023/09/27 03:49:53 - INFO - root -   Epoch: [31/200][340/346], lr: 0.00000014 	 loss = 1.2879(0.6319)
2023/09/27 03:49:57 - INFO - root -   Epoch: [31/200] 	 loss = 0.6329
2023/09/27 03:49:57 - INFO - root -   train_accuracy = 0.6604
2023/09/27 03:50:19 - INFO - root -   Epoch: [32/200][0/346], lr: 0.00000015 	 loss = 0.3706(0.3706)
2023/09/27 03:51:02 - INFO - root -   Epoch: [32/200][20/346], lr: 0.00000015 	 loss = 0.3099(0.4566)
2023/09/27 03:52:04 - INFO - root -   Epoch: [32/200][40/346], lr: 0.00000015 	 loss = 0.3770(0.5713)
2023/09/27 03:52:47 - INFO - root -   Epoch: [32/200][60/346], lr: 0.00000015 	 loss = 0.3857(0.5334)
2023/09/27 03:53:49 - INFO - root -   Epoch: [32/200][80/346], lr: 0.00000015 	 loss = 1.0969(0.5823)
2023/09/27 03:54:32 - INFO - root -   Epoch: [32/200][100/346], lr: 0.00000015 	 loss = 0.4986(0.5895)
2023/09/27 03:55:33 - INFO - root -   Epoch: [32/200][120/346], lr: 0.00000015 	 loss = 0.4265(0.6219)
2023/09/27 03:56:16 - INFO - root -   Epoch: [32/200][140/346], lr: 0.00000015 	 loss = 0.3521(0.6230)
2023/09/27 03:57:17 - INFO - root -   Epoch: [32/200][160/346], lr: 0.00000015 	 loss = 0.1719(0.6383)
2023/09/27 03:58:00 - INFO - root -   Epoch: [32/200][180/346], lr: 0.00000015 	 loss = 0.6061(0.6344)
2023/09/27 03:59:01 - INFO - root -   Epoch: [32/200][200/346], lr: 0.00000015 	 loss = 0.2571(0.6368)
2023/09/27 03:59:44 - INFO - root -   Epoch: [32/200][220/346], lr: 0.00000015 	 loss = 0.0511(0.6409)
2023/09/27 04:00:46 - INFO - root -   Epoch: [32/200][240/346], lr: 0.00000015 	 loss = 0.4568(0.6383)
2023/09/27 04:01:29 - INFO - root -   Epoch: [32/200][260/346], lr: 0.00000015 	 loss = 0.3209(0.6383)
2023/09/27 04:02:30 - INFO - root -   Epoch: [32/200][280/346], lr: 0.00000015 	 loss = 0.2776(0.6435)
2023/09/27 04:03:13 - INFO - root -   Epoch: [32/200][300/346], lr: 0.00000015 	 loss = 0.3167(0.6438)
2023/09/27 04:04:14 - INFO - root -   Epoch: [32/200][320/346], lr: 0.00000015 	 loss = 0.4964(0.6408)
2023/09/27 04:04:55 - INFO - root -   Epoch: [32/200][340/346], lr: 0.00000015 	 loss = 0.7554(0.6377)
2023/09/27 04:04:59 - INFO - root -   Epoch: [32/200] 	 loss = 0.6377
2023/09/27 04:04:59 - INFO - root -   train_accuracy = 0.6488
2023/09/27 04:05:21 - INFO - root -   Epoch: [33/200][0/346], lr: 0.00000015 	 loss = 0.5141(0.5141)
2023/09/27 04:06:05 - INFO - root -   Epoch: [33/200][20/346], lr: 0.00000015 	 loss = 0.5850(0.5385)
2023/09/27 04:07:06 - INFO - root -   Epoch: [33/200][40/346], lr: 0.00000015 	 loss = 0.6944(0.5876)
2023/09/27 04:07:50 - INFO - root -   Epoch: [33/200][60/346], lr: 0.00000015 	 loss = 0.4466(0.5480)
2023/09/27 04:08:51 - INFO - root -   Epoch: [33/200][80/346], lr: 0.00000015 	 loss = 0.8082(0.5806)
2023/09/27 04:09:35 - INFO - root -   Epoch: [33/200][100/346], lr: 0.00000015 	 loss = 1.1893(0.5965)
2023/09/27 04:10:37 - INFO - root -   Epoch: [33/200][120/346], lr: 0.00000015 	 loss = 0.8780(0.6046)
2023/09/27 04:11:20 - INFO - root -   Epoch: [33/200][140/346], lr: 0.00000015 	 loss = 0.4569(0.6064)
2023/09/27 04:12:21 - INFO - root -   Epoch: [33/200][160/346], lr: 0.00000015 	 loss = 0.1172(0.6084)
2023/09/27 04:13:05 - INFO - root -   Epoch: [33/200][180/346], lr: 0.00000015 	 loss = 1.1512(0.6234)
2023/09/27 04:14:06 - INFO - root -   Epoch: [33/200][200/346], lr: 0.00000015 	 loss = 0.3967(0.6268)
2023/09/27 04:14:50 - INFO - root -   Epoch: [33/200][220/346], lr: 0.00000015 	 loss = 0.1315(0.6313)
2023/09/27 04:15:51 - INFO - root -   Epoch: [33/200][240/346], lr: 0.00000015 	 loss = 0.3394(0.6325)
2023/09/27 04:16:35 - INFO - root -   Epoch: [33/200][260/346], lr: 0.00000015 	 loss = 0.4246(0.6293)
2023/09/27 04:17:35 - INFO - root -   Epoch: [33/200][280/346], lr: 0.00000015 	 loss = 0.5810(0.6404)
2023/09/27 04:18:19 - INFO - root -   Epoch: [33/200][300/346], lr: 0.00000015 	 loss = 0.1942(0.6408)
2023/09/27 04:19:19 - INFO - root -   Epoch: [33/200][320/346], lr: 0.00000015 	 loss = 0.6610(0.6421)
2023/09/27 04:20:03 - INFO - root -   Epoch: [33/200][340/346], lr: 0.00000015 	 loss = 0.5018(0.6417)
2023/09/27 04:20:07 - INFO - root -   Epoch: [33/200] 	 loss = 0.6385
2023/09/27 04:20:07 - INFO - root -   train_accuracy = 0.6315
2023/09/27 04:20:29 - INFO - root -   Epoch: [34/200][0/346], lr: 0.00000015 	 loss = 0.4888(0.4888)
2023/09/27 04:21:12 - INFO - root -   Epoch: [34/200][20/346], lr: 0.00000015 	 loss = 0.6642(0.5390)
2023/09/27 04:22:13 - INFO - root -   Epoch: [34/200][40/346], lr: 0.00000015 	 loss = 0.6226(0.5959)
2023/09/27 04:22:56 - INFO - root -   Epoch: [34/200][60/346], lr: 0.00000015 	 loss = 0.5335(0.5710)
2023/09/27 04:23:58 - INFO - root -   Epoch: [34/200][80/346], lr: 0.00000015 	 loss = 0.9121(0.5944)
2023/09/27 04:24:41 - INFO - root -   Epoch: [34/200][100/346], lr: 0.00000015 	 loss = 0.5829(0.6077)
2023/09/27 04:25:42 - INFO - root -   Epoch: [34/200][120/346], lr: 0.00000015 	 loss = 0.4990(0.6202)
2023/09/27 04:26:25 - INFO - root -   Epoch: [34/200][140/346], lr: 0.00000015 	 loss = 0.4159(0.6103)
2023/09/27 04:27:26 - INFO - root -   Epoch: [34/200][160/346], lr: 0.00000015 	 loss = 0.1912(0.6178)
2023/09/27 04:28:10 - INFO - root -   Epoch: [34/200][180/346], lr: 0.00000015 	 loss = 0.4954(0.6239)
2023/09/27 04:29:10 - INFO - root -   Epoch: [34/200][200/346], lr: 0.00000015 	 loss = 0.1714(0.6285)
2023/09/27 04:29:55 - INFO - root -   Epoch: [34/200][220/346], lr: 0.00000015 	 loss = 0.1709(0.6364)
2023/09/27 04:30:54 - INFO - root -   Epoch: [34/200][240/346], lr: 0.00000015 	 loss = 0.3249(0.6354)
2023/09/27 04:31:39 - INFO - root -   Epoch: [34/200][260/346], lr: 0.00000015 	 loss = 0.6983(0.6287)
2023/09/27 04:32:38 - INFO - root -   Epoch: [34/200][280/346], lr: 0.00000015 	 loss = 0.2784(0.6362)
2023/09/27 04:33:23 - INFO - root -   Epoch: [34/200][300/346], lr: 0.00000015 	 loss = 0.4851(0.6415)
2023/09/27 04:34:22 - INFO - root -   Epoch: [34/200][320/346], lr: 0.00000015 	 loss = 0.3963(0.6451)
2023/09/27 04:35:05 - INFO - root -   Epoch: [34/200][340/346], lr: 0.00000015 	 loss = 0.6876(0.6406)
2023/09/27 04:35:09 - INFO - root -   Epoch: [34/200] 	 loss = 0.6385
2023/09/27 04:41:04 - INFO - root -   precision = 0.6092
2023/09/27 04:41:04 - INFO - root -   eval_loss = 0.7107
2023/09/27 04:41:05 - INFO - root -   train_accuracy = 0.6618
2023/09/27 04:41:26 - INFO - root -   Epoch: [35/200][0/346], lr: 0.00000015 	 loss = 0.3443(0.3443)
2023/09/27 04:42:10 - INFO - root -   Epoch: [35/200][20/346], lr: 0.00000015 	 loss = 0.5992(0.4899)
2023/09/27 04:43:12 - INFO - root -   Epoch: [35/200][40/346], lr: 0.00000015 	 loss = 0.6382(0.5376)
2023/09/27 04:43:55 - INFO - root -   Epoch: [35/200][60/346], lr: 0.00000015 	 loss = 0.3569(0.5299)
2023/09/27 04:44:56 - INFO - root -   Epoch: [35/200][80/346], lr: 0.00000015 	 loss = 0.5986(0.5420)
2023/09/27 04:45:40 - INFO - root -   Epoch: [35/200][100/346], lr: 0.00000015 	 loss = 0.6796(0.5621)
2023/09/27 04:46:40 - INFO - root -   Epoch: [35/200][120/346], lr: 0.00000015 	 loss = 0.4905(0.5888)
2023/09/27 04:47:24 - INFO - root -   Epoch: [35/200][140/346], lr: 0.00000015 	 loss = 0.7650(0.5883)
2023/09/27 04:48:25 - INFO - root -   Epoch: [35/200][160/346], lr: 0.00000015 	 loss = 0.1759(0.5978)
2023/09/27 04:49:08 - INFO - root -   Epoch: [35/200][180/346], lr: 0.00000015 	 loss = 0.7646(0.5985)
2023/09/27 04:50:08 - INFO - root -   Epoch: [35/200][200/346], lr: 0.00000015 	 loss = 0.2510(0.5958)
2023/09/27 04:50:51 - INFO - root -   Epoch: [35/200][220/346], lr: 0.00000015 	 loss = 0.1120(0.5994)
2023/09/27 04:51:52 - INFO - root -   Epoch: [35/200][240/346], lr: 0.00000015 	 loss = 0.3015(0.5970)
2023/09/27 04:52:35 - INFO - root -   Epoch: [35/200][260/346], lr: 0.00000015 	 loss = 0.2516(0.5946)
2023/09/27 04:53:35 - INFO - root -   Epoch: [35/200][280/346], lr: 0.00000015 	 loss = 0.8220(0.6132)
2023/09/27 04:54:19 - INFO - root -   Epoch: [35/200][300/346], lr: 0.00000015 	 loss = 0.3468(0.6121)
2023/09/27 04:55:18 - INFO - root -   Epoch: [35/200][320/346], lr: 0.00000015 	 loss = 0.3861(0.6163)
2023/09/27 04:56:01 - INFO - root -   Epoch: [35/200][340/346], lr: 0.00000015 	 loss = 1.0689(0.6193)
2023/09/27 04:56:05 - INFO - root -   Epoch: [35/200] 	 loss = 0.6183
2023/09/27 04:56:05 - INFO - root -   train_accuracy = 0.6792
2023/09/27 04:56:27 - INFO - root -   Epoch: [36/200][0/346], lr: 0.00000015 	 loss = 0.4679(0.4679)
2023/09/27 04:57:10 - INFO - root -   Epoch: [36/200][20/346], lr: 0.00000015 	 loss = 0.2987(0.4767)
2023/09/27 04:58:11 - INFO - root -   Epoch: [36/200][40/346], lr: 0.00000015 	 loss = 0.7675(0.5648)
2023/09/27 04:58:55 - INFO - root -   Epoch: [36/200][60/346], lr: 0.00000015 	 loss = 0.3892(0.5477)
2023/09/27 04:59:56 - INFO - root -   Epoch: [36/200][80/346], lr: 0.00000015 	 loss = 0.8949(0.5849)
2023/09/27 05:00:39 - INFO - root -   Epoch: [36/200][100/346], lr: 0.00000015 	 loss = 0.5105(0.5852)
2023/09/27 05:01:41 - INFO - root -   Epoch: [36/200][120/346], lr: 0.00000015 	 loss = 0.5704(0.6113)
2023/09/27 05:02:24 - INFO - root -   Epoch: [36/200][140/346], lr: 0.00000015 	 loss = 0.7922(0.6124)
2023/09/27 05:03:25 - INFO - root -   Epoch: [36/200][160/346], lr: 0.00000015 	 loss = 0.1960(0.6212)
2023/09/27 05:04:08 - INFO - root -   Epoch: [36/200][180/346], lr: 0.00000015 	 loss = 0.6054(0.6226)
2023/09/27 05:05:09 - INFO - root -   Epoch: [36/200][200/346], lr: 0.00000015 	 loss = 0.1061(0.6174)
2023/09/27 05:05:52 - INFO - root -   Epoch: [36/200][220/346], lr: 0.00000015 	 loss = 0.1817(0.6124)
2023/09/27 05:06:53 - INFO - root -   Epoch: [36/200][240/346], lr: 0.00000015 	 loss = 0.3676(0.6124)
2023/09/27 05:07:37 - INFO - root -   Epoch: [36/200][260/346], lr: 0.00000015 	 loss = 0.9004(0.6084)
2023/09/27 05:08:36 - INFO - root -   Epoch: [36/200][280/346], lr: 0.00000015 	 loss = 0.4244(0.6157)
2023/09/27 05:09:21 - INFO - root -   Epoch: [36/200][300/346], lr: 0.00000015 	 loss = 0.2541(0.6239)
2023/09/27 05:10:20 - INFO - root -   Epoch: [36/200][320/346], lr: 0.00000015 	 loss = 0.1985(0.6196)
2023/09/27 05:11:03 - INFO - root -   Epoch: [36/200][340/346], lr: 0.00000015 	 loss = 1.2806(0.6195)
2023/09/27 05:11:07 - INFO - root -   Epoch: [36/200] 	 loss = 0.6178
2023/09/27 05:11:07 - INFO - root -   train_accuracy = 0.6835
2023/09/27 05:11:29 - INFO - root -   Epoch: [37/200][0/346], lr: 0.00000015 	 loss = 0.3327(0.3327)
2023/09/27 05:12:12 - INFO - root -   Epoch: [37/200][20/346], lr: 0.00000015 	 loss = 0.3025(0.5152)
2023/09/27 05:13:13 - INFO - root -   Epoch: [37/200][40/346], lr: 0.00000015 	 loss = 0.6273(0.5587)
2023/09/27 05:13:57 - INFO - root -   Epoch: [37/200][60/346], lr: 0.00000015 	 loss = 0.6361(0.5308)
2023/09/27 05:14:58 - INFO - root -   Epoch: [37/200][80/346], lr: 0.00000015 	 loss = 0.5155(0.5660)
2023/09/27 05:15:41 - INFO - root -   Epoch: [37/200][100/346], lr: 0.00000015 	 loss = 0.7817(0.5810)
2023/09/27 05:16:42 - INFO - root -   Epoch: [37/200][120/346], lr: 0.00000015 	 loss = 0.7494(0.5969)
2023/09/27 05:17:25 - INFO - root -   Epoch: [37/200][140/346], lr: 0.00000015 	 loss = 0.7694(0.6020)
2023/09/27 05:18:26 - INFO - root -   Epoch: [37/200][160/346], lr: 0.00000015 	 loss = 0.2245(0.6097)
2023/09/27 05:19:09 - INFO - root -   Epoch: [37/200][180/346], lr: 0.00000015 	 loss = 0.4621(0.6051)
2023/09/27 05:20:09 - INFO - root -   Epoch: [37/200][200/346], lr: 0.00000015 	 loss = 0.2410(0.5983)
2023/09/27 05:20:52 - INFO - root -   Epoch: [37/200][220/346], lr: 0.00000015 	 loss = 0.2301(0.6041)
2023/09/27 05:21:53 - INFO - root -   Epoch: [37/200][240/346], lr: 0.00000015 	 loss = 0.5359(0.6040)
2023/09/27 05:22:36 - INFO - root -   Epoch: [37/200][260/346], lr: 0.00000015 	 loss = 0.7000(0.6024)
2023/09/27 05:23:37 - INFO - root -   Epoch: [37/200][280/346], lr: 0.00000015 	 loss = 0.3230(0.6127)
2023/09/27 05:24:20 - INFO - root -   Epoch: [37/200][300/346], lr: 0.00000015 	 loss = 0.5529(0.6179)
2023/09/27 05:25:21 - INFO - root -   Epoch: [37/200][320/346], lr: 0.00000015 	 loss = 0.3031(0.6210)
2023/09/27 05:26:03 - INFO - root -   Epoch: [37/200][340/346], lr: 0.00000015 	 loss = 1.4917(0.6238)
2023/09/27 05:26:07 - INFO - root -   Epoch: [37/200] 	 loss = 0.6228
2023/09/27 05:26:07 - INFO - root -   train_accuracy = 0.6561
2023/09/27 05:26:29 - INFO - root -   Epoch: [38/200][0/346], lr: 0.00000015 	 loss = 0.2367(0.2367)
2023/09/27 05:27:12 - INFO - root -   Epoch: [38/200][20/346], lr: 0.00000015 	 loss = 0.6021(0.5771)
2023/09/27 05:28:13 - INFO - root -   Epoch: [38/200][40/346], lr: 0.00000015 	 loss = 0.5409(0.6438)
2023/09/27 05:28:56 - INFO - root -   Epoch: [38/200][60/346], lr: 0.00000015 	 loss = 0.4065(0.6255)
2023/09/27 05:29:57 - INFO - root -   Epoch: [38/200][80/346], lr: 0.00000015 	 loss = 0.9501(0.6690)
2023/09/27 05:30:41 - INFO - root -   Epoch: [38/200][100/346], lr: 0.00000015 	 loss = 0.2902(0.6678)
2023/09/27 05:31:41 - INFO - root -   Epoch: [38/200][120/346], lr: 0.00000015 	 loss = 0.3850(0.6731)
2023/09/27 05:32:25 - INFO - root -   Epoch: [38/200][140/346], lr: 0.00000015 	 loss = 1.4707(0.6638)
2023/09/27 05:33:25 - INFO - root -   Epoch: [38/200][160/346], lr: 0.00000015 	 loss = 0.2919(0.6562)
2023/09/27 05:34:09 - INFO - root -   Epoch: [38/200][180/346], lr: 0.00000015 	 loss = 0.6258(0.6522)
2023/09/27 05:35:09 - INFO - root -   Epoch: [38/200][200/346], lr: 0.00000015 	 loss = 0.1797(0.6436)
2023/09/27 05:35:53 - INFO - root -   Epoch: [38/200][220/346], lr: 0.00000015 	 loss = 0.2104(0.6418)
2023/09/27 05:36:53 - INFO - root -   Epoch: [38/200][240/346], lr: 0.00000015 	 loss = 0.1992(0.6349)
2023/09/27 05:37:37 - INFO - root -   Epoch: [38/200][260/346], lr: 0.00000015 	 loss = 0.5742(0.6334)
2023/09/27 05:38:36 - INFO - root -   Epoch: [38/200][280/346], lr: 0.00000015 	 loss = 0.7876(0.6449)
2023/09/27 05:39:21 - INFO - root -   Epoch: [38/200][300/346], lr: 0.00000015 	 loss = 0.1137(0.6468)
2023/09/27 05:40:20 - INFO - root -   Epoch: [38/200][320/346], lr: 0.00000015 	 loss = 0.4041(0.6401)
2023/09/27 05:41:03 - INFO - root -   Epoch: [38/200][340/346], lr: 0.00000015 	 loss = 1.0714(0.6345)
2023/09/27 05:41:07 - INFO - root -   Epoch: [38/200] 	 loss = 0.6342
2023/09/27 05:41:07 - INFO - root -   train_accuracy = 0.6431
2023/09/27 05:41:29 - INFO - root -   Epoch: [39/200][0/346], lr: 0.00000016 	 loss = 0.1180(0.1180)
2023/09/27 05:42:12 - INFO - root -   Epoch: [39/200][20/346], lr: 0.00000016 	 loss = 0.6390(0.4686)
2023/09/27 05:43:13 - INFO - root -   Epoch: [39/200][40/346], lr: 0.00000016 	 loss = 0.4646(0.5278)
2023/09/27 05:43:56 - INFO - root -   Epoch: [39/200][60/346], lr: 0.00000016 	 loss = 0.4591(0.5204)
2023/09/27 05:44:57 - INFO - root -   Epoch: [39/200][80/346], lr: 0.00000016 	 loss = 0.3194(0.5347)
2023/09/27 05:45:41 - INFO - root -   Epoch: [39/200][100/346], lr: 0.00000016 	 loss = 0.6872(0.5564)
2023/09/27 05:46:41 - INFO - root -   Epoch: [39/200][120/346], lr: 0.00000016 	 loss = 0.6435(0.6027)
2023/09/27 05:47:25 - INFO - root -   Epoch: [39/200][140/346], lr: 0.00000016 	 loss = 0.4227(0.5859)
2023/09/27 05:48:25 - INFO - root -   Epoch: [39/200][160/346], lr: 0.00000016 	 loss = 0.3544(0.5876)
2023/09/27 05:49:09 - INFO - root -   Epoch: [39/200][180/346], lr: 0.00000016 	 loss = 0.6853(0.5965)
2023/09/27 05:50:09 - INFO - root -   Epoch: [39/200][200/346], lr: 0.00000016 	 loss = 0.1387(0.5979)
2023/09/27 05:50:53 - INFO - root -   Epoch: [39/200][220/346], lr: 0.00000016 	 loss = 0.2499(0.6024)
2023/09/27 05:51:52 - INFO - root -   Epoch: [39/200][240/346], lr: 0.00000016 	 loss = 0.3841(0.6049)
2023/09/27 05:52:36 - INFO - root -   Epoch: [39/200][260/346], lr: 0.00000016 	 loss = 1.0562(0.6071)
2023/09/27 05:53:36 - INFO - root -   Epoch: [39/200][280/346], lr: 0.00000016 	 loss = 0.4200(0.6181)
2023/09/27 05:54:20 - INFO - root -   Epoch: [39/200][300/346], lr: 0.00000016 	 loss = 0.2580(0.6222)
2023/09/27 05:55:20 - INFO - root -   Epoch: [39/200][320/346], lr: 0.00000016 	 loss = 0.2882(0.6222)
2023/09/27 05:56:01 - INFO - root -   Epoch: [39/200][340/346], lr: 0.00000016 	 loss = 1.4747(0.6172)
2023/09/27 05:56:05 - INFO - root -   Epoch: [39/200] 	 loss = 0.6151
2023/09/27 06:01:46 - INFO - root -   precision = 0.6609
2023/09/27 06:01:46 - INFO - root -   eval_loss = 0.6601
2023/09/27 06:01:47 - INFO - root -   train_accuracy = 0.6763
2023/09/27 06:02:09 - INFO - root -   Epoch: [40/200][0/346], lr: 0.00000016 	 loss = 0.3719(0.3719)
2023/09/27 06:02:52 - INFO - root -   Epoch: [40/200][20/346], lr: 0.00000016 	 loss = 0.5375(0.4978)
2023/09/27 06:03:55 - INFO - root -   Epoch: [40/200][40/346], lr: 0.00000016 	 loss = 0.3010(0.5597)
2023/09/27 06:04:38 - INFO - root -   Epoch: [40/200][60/346], lr: 0.00000016 	 loss = 0.1743(0.5098)
2023/09/27 06:05:41 - INFO - root -   Epoch: [40/200][80/346], lr: 0.00000016 	 loss = 1.0401(0.5305)
2023/09/27 06:06:24 - INFO - root -   Epoch: [40/200][100/346], lr: 0.00000016 	 loss = 0.6936(0.5537)
2023/09/27 06:07:25 - INFO - root -   Epoch: [40/200][120/346], lr: 0.00000016 	 loss = 0.6288(0.5995)
2023/09/27 06:08:09 - INFO - root -   Epoch: [40/200][140/346], lr: 0.00000016 	 loss = 1.1726(0.6002)
2023/09/27 06:09:10 - INFO - root -   Epoch: [40/200][160/346], lr: 0.00000016 	 loss = 0.1703(0.6037)
2023/09/27 06:09:53 - INFO - root -   Epoch: [40/200][180/346], lr: 0.00000016 	 loss = 0.4241(0.6069)
2023/09/27 06:10:54 - INFO - root -   Epoch: [40/200][200/346], lr: 0.00000016 	 loss = 0.1443(0.6018)
2023/09/27 06:11:37 - INFO - root -   Epoch: [40/200][220/346], lr: 0.00000016 	 loss = 0.2288(0.6070)
2023/09/27 06:12:38 - INFO - root -   Epoch: [40/200][240/346], lr: 0.00000016 	 loss = 0.0641(0.6050)
2023/09/27 06:13:21 - INFO - root -   Epoch: [40/200][260/346], lr: 0.00000016 	 loss = 0.8232(0.6046)
2023/09/27 06:14:22 - INFO - root -   Epoch: [40/200][280/346], lr: 0.00000016 	 loss = 0.4106(0.6174)
2023/09/27 06:15:05 - INFO - root -   Epoch: [40/200][300/346], lr: 0.00000016 	 loss = 0.2267(0.6190)
2023/09/27 06:16:06 - INFO - root -   Epoch: [40/200][320/346], lr: 0.00000016 	 loss = 0.4717(0.6193)
2023/09/27 06:16:48 - INFO - root -   Epoch: [40/200][340/346], lr: 0.00000016 	 loss = 1.4341(0.6209)
2023/09/27 06:16:52 - INFO - root -   Epoch: [40/200] 	 loss = 0.6186
2023/09/27 06:16:52 - INFO - root -   train_accuracy = 0.6546
2023/09/27 06:17:14 - INFO - root -   Epoch: [41/200][0/346], lr: 0.00000016 	 loss = 0.2707(0.2707)
2023/09/27 06:17:57 - INFO - root -   Epoch: [41/200][20/346], lr: 0.00000016 	 loss = 0.6926(0.5042)
2023/09/27 06:18:59 - INFO - root -   Epoch: [41/200][40/346], lr: 0.00000016 	 loss = 0.8177(0.5478)
2023/09/27 06:19:43 - INFO - root -   Epoch: [41/200][60/346], lr: 0.00000016 	 loss = 0.3130(0.5487)
2023/09/27 06:20:44 - INFO - root -   Epoch: [41/200][80/346], lr: 0.00000016 	 loss = 0.8370(0.5805)
2023/09/27 06:21:28 - INFO - root -   Epoch: [41/200][100/346], lr: 0.00000016 	 loss = 0.7622(0.5905)
2023/09/27 06:22:29 - INFO - root -   Epoch: [41/200][120/346], lr: 0.00000016 	 loss = 0.6621(0.6096)
2023/09/27 06:23:13 - INFO - root -   Epoch: [41/200][140/346], lr: 0.00000016 	 loss = 0.8809(0.6127)
2023/09/27 06:24:13 - INFO - root -   Epoch: [41/200][160/346], lr: 0.00000016 	 loss = 0.2370(0.6075)
2023/09/27 06:24:56 - INFO - root -   Epoch: [41/200][180/346], lr: 0.00000016 	 loss = 0.6831(0.6064)
2023/09/27 06:25:57 - INFO - root -   Epoch: [41/200][200/346], lr: 0.00000016 	 loss = 0.3686(0.6178)
2023/09/27 06:26:41 - INFO - root -   Epoch: [41/200][220/346], lr: 0.00000016 	 loss = 0.0769(0.6166)
2023/09/27 06:27:41 - INFO - root -   Epoch: [41/200][240/346], lr: 0.00000016 	 loss = 0.2592(0.6117)
2023/09/27 06:28:25 - INFO - root -   Epoch: [41/200][260/346], lr: 0.00000016 	 loss = 0.4651(0.6040)
2023/09/27 06:29:25 - INFO - root -   Epoch: [41/200][280/346], lr: 0.00000016 	 loss = 0.4903(0.6204)
2023/09/27 06:30:09 - INFO - root -   Epoch: [41/200][300/346], lr: 0.00000016 	 loss = 0.2544(0.6191)
2023/09/27 06:31:09 - INFO - root -   Epoch: [41/200][320/346], lr: 0.00000016 	 loss = 0.3967(0.6146)
2023/09/27 06:31:52 - INFO - root -   Epoch: [41/200][340/346], lr: 0.00000016 	 loss = 1.5595(0.6107)
2023/09/27 06:31:56 - INFO - root -   Epoch: [41/200] 	 loss = 0.6107
2023/09/27 06:31:56 - INFO - root -   train_accuracy = 0.6821
2023/09/27 06:32:18 - INFO - root -   Epoch: [42/200][0/346], lr: 0.00000016 	 loss = 0.6369(0.6369)
2023/09/27 06:33:01 - INFO - root -   Epoch: [42/200][20/346], lr: 0.00000016 	 loss = 0.4015(0.4569)
2023/09/27 06:34:02 - INFO - root -   Epoch: [42/200][40/346], lr: 0.00000016 	 loss = 0.7274(0.5009)
2023/09/27 06:34:45 - INFO - root -   Epoch: [42/200][60/346], lr: 0.00000016 	 loss = 0.3389(0.4888)
2023/09/27 06:35:46 - INFO - root -   Epoch: [42/200][80/346], lr: 0.00000016 	 loss = 0.8000(0.5325)
2023/09/27 06:36:29 - INFO - root -   Epoch: [42/200][100/346], lr: 0.00000016 	 loss = 1.1327(0.5676)
2023/09/27 06:37:30 - INFO - root -   Epoch: [42/200][120/346], lr: 0.00000016 	 loss = 0.4755(0.5934)
2023/09/27 06:38:14 - INFO - root -   Epoch: [42/200][140/346], lr: 0.00000016 	 loss = 0.7781(0.5771)
2023/09/27 06:39:14 - INFO - root -   Epoch: [42/200][160/346], lr: 0.00000016 	 loss = 0.2400(0.5755)
2023/09/27 06:39:57 - INFO - root -   Epoch: [42/200][180/346], lr: 0.00000016 	 loss = 1.0826(0.5881)
2023/09/27 06:40:58 - INFO - root -   Epoch: [42/200][200/346], lr: 0.00000016 	 loss = 0.4958(0.5980)
2023/09/27 06:41:41 - INFO - root -   Epoch: [42/200][220/346], lr: 0.00000016 	 loss = 0.2074(0.6057)
2023/09/27 06:42:42 - INFO - root -   Epoch: [42/200][240/346], lr: 0.00000016 	 loss = 0.3817(0.6057)
2023/09/27 06:43:25 - INFO - root -   Epoch: [42/200][260/346], lr: 0.00000016 	 loss = 0.4774(0.5988)
2023/09/27 06:44:25 - INFO - root -   Epoch: [42/200][280/346], lr: 0.00000016 	 loss = 0.3013(0.6035)
2023/09/27 06:45:10 - INFO - root -   Epoch: [42/200][300/346], lr: 0.00000016 	 loss = 0.1637(0.6042)
2023/09/27 06:46:09 - INFO - root -   Epoch: [42/200][320/346], lr: 0.00000016 	 loss = 0.3069(0.6043)
2023/09/27 06:46:52 - INFO - root -   Epoch: [42/200][340/346], lr: 0.00000016 	 loss = 0.7999(0.6053)
2023/09/27 06:46:56 - INFO - root -   Epoch: [42/200] 	 loss = 0.6049
2023/09/27 06:46:56 - INFO - root -   train_accuracy = 0.6835
2023/09/27 06:47:18 - INFO - root -   Epoch: [43/200][0/346], lr: 0.00000016 	 loss = 0.3027(0.3027)
2023/09/27 06:48:02 - INFO - root -   Epoch: [43/200][20/346], lr: 0.00000016 	 loss = 0.5924(0.4696)
2023/09/27 06:49:05 - INFO - root -   Epoch: [43/200][40/346], lr: 0.00000016 	 loss = 0.5344(0.5254)
2023/09/27 06:49:49 - INFO - root -   Epoch: [43/200][60/346], lr: 0.00000016 	 loss = 0.4963(0.5229)
2023/09/27 06:50:52 - INFO - root -   Epoch: [43/200][80/346], lr: 0.00000016 	 loss = 0.5148(0.5914)
2023/09/27 06:51:37 - INFO - root -   Epoch: [43/200][100/346], lr: 0.00000016 	 loss = 1.0163(0.6052)
2023/09/27 06:52:39 - INFO - root -   Epoch: [43/200][120/346], lr: 0.00000016 	 loss = 0.7263(0.6341)
2023/09/27 06:53:23 - INFO - root -   Epoch: [43/200][140/346], lr: 0.00000016 	 loss = 0.8243(0.6241)
2023/09/27 06:54:26 - INFO - root -   Epoch: [43/200][160/346], lr: 0.00000016 	 loss = 0.2800(0.6170)
2023/09/27 06:55:10 - INFO - root -   Epoch: [43/200][180/346], lr: 0.00000016 	 loss = 0.7770(0.6229)
2023/09/27 06:56:12 - INFO - root -   Epoch: [43/200][200/346], lr: 0.00000016 	 loss = 0.2118(0.6207)
2023/09/27 06:56:57 - INFO - root -   Epoch: [43/200][220/346], lr: 0.00000016 	 loss = 0.1371(0.6192)
2023/09/27 06:57:59 - INFO - root -   Epoch: [43/200][240/346], lr: 0.00000016 	 loss = 0.1598(0.6117)
2023/09/27 06:58:44 - INFO - root -   Epoch: [43/200][260/346], lr: 0.00000016 	 loss = 0.6656(0.6145)
2023/09/27 06:59:45 - INFO - root -   Epoch: [43/200][280/346], lr: 0.00000016 	 loss = 0.2307(0.6239)
2023/09/27 07:00:30 - INFO - root -   Epoch: [43/200][300/346], lr: 0.00000016 	 loss = 0.4212(0.6224)
2023/09/27 07:01:30 - INFO - root -   Epoch: [43/200][320/346], lr: 0.00000016 	 loss = 0.6566(0.6244)
2023/09/27 07:02:13 - INFO - root -   Epoch: [43/200][340/346], lr: 0.00000016 	 loss = 1.6535(0.6241)
2023/09/27 07:02:17 - INFO - root -   Epoch: [43/200] 	 loss = 0.6237
2023/09/27 07:02:17 - INFO - root -   train_accuracy = 0.6734
2023/09/27 07:02:39 - INFO - root -   Epoch: [44/200][0/346], lr: 0.00000016 	 loss = 0.2749(0.2749)
2023/09/27 07:03:22 - INFO - root -   Epoch: [44/200][20/346], lr: 0.00000016 	 loss = 0.4214(0.4601)
2023/09/27 07:04:23 - INFO - root -   Epoch: [44/200][40/346], lr: 0.00000016 	 loss = 0.3473(0.5247)
2023/09/27 07:05:07 - INFO - root -   Epoch: [44/200][60/346], lr: 0.00000016 	 loss = 0.3324(0.5025)
2023/09/27 07:06:08 - INFO - root -   Epoch: [44/200][80/346], lr: 0.00000016 	 loss = 0.7043(0.5482)
2023/09/27 07:06:51 - INFO - root -   Epoch: [44/200][100/346], lr: 0.00000016 	 loss = 0.6450(0.5573)
2023/09/27 07:07:52 - INFO - root -   Epoch: [44/200][120/346], lr: 0.00000016 	 loss = 0.4774(0.6032)
2023/09/27 07:08:35 - INFO - root -   Epoch: [44/200][140/346], lr: 0.00000016 	 loss = 0.7695(0.6017)
2023/09/27 07:09:36 - INFO - root -   Epoch: [44/200][160/346], lr: 0.00000016 	 loss = 0.1765(0.6039)
2023/09/27 07:10:19 - INFO - root -   Epoch: [44/200][180/346], lr: 0.00000016 	 loss = 0.9793(0.6068)
2023/09/27 07:11:20 - INFO - root -   Epoch: [44/200][200/346], lr: 0.00000016 	 loss = 0.3664(0.6065)
2023/09/27 07:12:03 - INFO - root -   Epoch: [44/200][220/346], lr: 0.00000016 	 loss = 0.3125(0.6155)
2023/09/27 07:13:04 - INFO - root -   Epoch: [44/200][240/346], lr: 0.00000016 	 loss = 0.3602(0.6166)
2023/09/27 07:13:48 - INFO - root -   Epoch: [44/200][260/346], lr: 0.00000016 	 loss = 0.5246(0.6203)
2023/09/27 07:14:49 - INFO - root -   Epoch: [44/200][280/346], lr: 0.00000016 	 loss = 0.3432(0.6329)
2023/09/27 07:15:32 - INFO - root -   Epoch: [44/200][300/346], lr: 0.00000016 	 loss = 0.7349(0.6387)
2023/09/27 07:16:32 - INFO - root -   Epoch: [44/200][320/346], lr: 0.00000016 	 loss = 0.2458(0.6377)
2023/09/27 07:17:15 - INFO - root -   Epoch: [44/200][340/346], lr: 0.00000016 	 loss = 0.5229(0.6320)
2023/09/27 07:17:19 - INFO - root -   Epoch: [44/200] 	 loss = 0.6296
2023/09/27 07:23:19 - INFO - root -   precision = 0.5920
2023/09/27 07:23:19 - INFO - root -   eval_loss = 0.6619
2023/09/27 07:23:20 - INFO - root -   train_accuracy = 0.6792
2023/09/27 07:23:42 - INFO - root -   Epoch: [45/200][0/346], lr: 0.00000016 	 loss = 0.3926(0.3926)
2023/09/27 07:24:26 - INFO - root -   Epoch: [45/200][20/346], lr: 0.00000016 	 loss = 0.3198(0.4937)
2023/09/27 07:25:27 - INFO - root -   Epoch: [45/200][40/346], lr: 0.00000016 	 loss = 0.5503(0.5477)
2023/09/27 07:26:11 - INFO - root -   Epoch: [45/200][60/346], lr: 0.00000016 	 loss = 0.3190(0.5180)
2023/09/27 07:27:12 - INFO - root -   Epoch: [45/200][80/346], lr: 0.00000016 	 loss = 0.7998(0.5567)
2023/09/27 07:27:55 - INFO - root -   Epoch: [45/200][100/346], lr: 0.00000016 	 loss = 1.0560(0.5814)
2023/09/27 07:28:56 - INFO - root -   Epoch: [45/200][120/346], lr: 0.00000016 	 loss = 0.5786(0.6159)
2023/09/27 07:29:40 - INFO - root -   Epoch: [45/200][140/346], lr: 0.00000016 	 loss = 0.5484(0.6148)
2023/09/27 07:30:41 - INFO - root -   Epoch: [45/200][160/346], lr: 0.00000016 	 loss = 0.2559(0.6072)
2023/09/27 07:31:24 - INFO - root -   Epoch: [45/200][180/346], lr: 0.00000016 	 loss = 0.6556(0.6081)
2023/09/27 07:32:25 - INFO - root -   Epoch: [45/200][200/346], lr: 0.00000016 	 loss = 0.3628(0.6015)
2023/09/27 07:33:08 - INFO - root -   Epoch: [45/200][220/346], lr: 0.00000016 	 loss = 0.2008(0.6047)
2023/09/27 07:34:09 - INFO - root -   Epoch: [45/200][240/346], lr: 0.00000016 	 loss = 0.1668(0.6012)
2023/09/27 07:34:52 - INFO - root -   Epoch: [45/200][260/346], lr: 0.00000016 	 loss = 0.4316(0.5994)
2023/09/27 07:35:53 - INFO - root -   Epoch: [45/200][280/346], lr: 0.00000016 	 loss = 0.3263(0.6088)
2023/09/27 07:36:36 - INFO - root -   Epoch: [45/200][300/346], lr: 0.00000016 	 loss = 0.2260(0.6093)
2023/09/27 07:37:37 - INFO - root -   Epoch: [45/200][320/346], lr: 0.00000016 	 loss = 0.3134(0.6121)
2023/09/27 07:38:19 - INFO - root -   Epoch: [45/200][340/346], lr: 0.00000016 	 loss = 1.0398(0.6092)
2023/09/27 07:38:23 - INFO - root -   Epoch: [45/200] 	 loss = 0.6072
2023/09/27 07:38:23 - INFO - root -   train_accuracy = 0.6951
2023/09/27 07:38:45 - INFO - root -   Epoch: [46/200][0/346], lr: 0.00000017 	 loss = 0.1520(0.1520)
2023/09/27 07:39:28 - INFO - root -   Epoch: [46/200][20/346], lr: 0.00000017 	 loss = 0.6359(0.5202)
2023/09/27 07:40:29 - INFO - root -   Epoch: [46/200][40/346], lr: 0.00000017 	 loss = 0.2992(0.5388)
2023/09/27 07:41:13 - INFO - root -   Epoch: [46/200][60/346], lr: 0.00000017 	 loss = 0.5421(0.5179)
2023/09/27 07:42:13 - INFO - root -   Epoch: [46/200][80/346], lr: 0.00000017 	 loss = 0.9684(0.5558)
2023/09/27 07:42:57 - INFO - root -   Epoch: [46/200][100/346], lr: 0.00000017 	 loss = 0.8132(0.5701)
2023/09/27 07:43:57 - INFO - root -   Epoch: [46/200][120/346], lr: 0.00000017 	 loss = 0.5003(0.5915)
2023/09/27 07:44:41 - INFO - root -   Epoch: [46/200][140/346], lr: 0.00000017 	 loss = 0.7408(0.5937)
2023/09/27 07:45:41 - INFO - root -   Epoch: [46/200][160/346], lr: 0.00000017 	 loss = 0.4644(0.6022)
2023/09/27 07:46:24 - INFO - root -   Epoch: [46/200][180/346], lr: 0.00000017 	 loss = 0.6497(0.6016)
2023/09/27 07:47:24 - INFO - root -   Epoch: [46/200][200/346], lr: 0.00000017 	 loss = 0.1774(0.6055)
2023/09/27 07:48:08 - INFO - root -   Epoch: [46/200][220/346], lr: 0.00000017 	 loss = 0.2447(0.6051)
2023/09/27 07:49:08 - INFO - root -   Epoch: [46/200][240/346], lr: 0.00000017 	 loss = 0.0970(0.6041)
2023/09/27 07:49:52 - INFO - root -   Epoch: [46/200][260/346], lr: 0.00000017 	 loss = 0.2407(0.6004)
2023/09/27 07:50:51 - INFO - root -   Epoch: [46/200][280/346], lr: 0.00000017 	 loss = 0.4731(0.6113)
2023/09/27 07:51:35 - INFO - root -   Epoch: [46/200][300/346], lr: 0.00000017 	 loss = 0.1896(0.6116)
2023/09/27 07:52:34 - INFO - root -   Epoch: [46/200][320/346], lr: 0.00000017 	 loss = 0.2485(0.6207)
2023/09/27 07:53:16 - INFO - root -   Epoch: [46/200][340/346], lr: 0.00000017 	 loss = 1.0367(0.6144)
2023/09/27 07:53:20 - INFO - root -   Epoch: [46/200] 	 loss = 0.6142
2023/09/27 07:53:20 - INFO - root -   train_accuracy = 0.6705
2023/09/27 07:53:42 - INFO - root -   Epoch: [47/200][0/346], lr: 0.00000017 	 loss = 0.3575(0.3575)
2023/09/27 07:54:25 - INFO - root -   Epoch: [47/200][20/346], lr: 0.00000017 	 loss = 0.6379(0.4694)
2023/09/27 07:55:26 - INFO - root -   Epoch: [47/200][40/346], lr: 0.00000017 	 loss = 0.2604(0.5459)
2023/09/27 07:56:10 - INFO - root -   Epoch: [47/200][60/346], lr: 0.00000017 	 loss = 0.6149(0.5196)
2023/09/27 07:57:11 - INFO - root -   Epoch: [47/200][80/346], lr: 0.00000017 	 loss = 0.5992(0.5769)
2023/09/27 07:57:54 - INFO - root -   Epoch: [47/200][100/346], lr: 0.00000017 	 loss = 0.5936(0.5887)
2023/09/27 07:58:55 - INFO - root -   Epoch: [47/200][120/346], lr: 0.00000017 	 loss = 0.8156(0.5973)
2023/09/27 07:59:38 - INFO - root -   Epoch: [47/200][140/346], lr: 0.00000017 	 loss = 0.5509(0.5969)
2023/09/27 08:00:39 - INFO - root -   Epoch: [47/200][160/346], lr: 0.00000017 	 loss = 0.1711(0.6057)
2023/09/27 08:01:22 - INFO - root -   Epoch: [47/200][180/346], lr: 0.00000017 	 loss = 0.5549(0.6113)
2023/09/27 08:02:22 - INFO - root -   Epoch: [47/200][200/346], lr: 0.00000017 	 loss = 0.1316(0.6060)
2023/09/27 08:03:06 - INFO - root -   Epoch: [47/200][220/346], lr: 0.00000017 	 loss = 0.1449(0.6076)
2023/09/27 08:04:06 - INFO - root -   Epoch: [47/200][240/346], lr: 0.00000017 	 loss = 0.4262(0.6105)
2023/09/27 08:04:49 - INFO - root -   Epoch: [47/200][260/346], lr: 0.00000017 	 loss = 0.8188(0.6112)
2023/09/27 08:05:50 - INFO - root -   Epoch: [47/200][280/346], lr: 0.00000017 	 loss = 0.2352(0.6198)
2023/09/27 08:06:33 - INFO - root -   Epoch: [47/200][300/346], lr: 0.00000017 	 loss = 0.2713(0.6205)
2023/09/27 08:07:34 - INFO - root -   Epoch: [47/200][320/346], lr: 0.00000017 	 loss = 0.4276(0.6245)
2023/09/27 08:08:16 - INFO - root -   Epoch: [47/200][340/346], lr: 0.00000017 	 loss = 0.9285(0.6208)
2023/09/27 08:08:20 - INFO - root -   Epoch: [47/200] 	 loss = 0.6212
2023/09/27 08:08:20 - INFO - root -   train_accuracy = 0.6850
2023/09/27 08:08:42 - INFO - root -   Epoch: [48/200][0/346], lr: 0.00000017 	 loss = 0.6340(0.6340)
2023/09/27 08:09:25 - INFO - root -   Epoch: [48/200][20/346], lr: 0.00000017 	 loss = 0.4173(0.4843)
2023/09/27 08:10:27 - INFO - root -   Epoch: [48/200][40/346], lr: 0.00000017 	 loss = 0.6690(0.5437)
2023/09/27 08:11:11 - INFO - root -   Epoch: [48/200][60/346], lr: 0.00000017 	 loss = 0.4079(0.5135)
2023/09/27 08:12:12 - INFO - root -   Epoch: [48/200][80/346], lr: 0.00000017 	 loss = 1.0026(0.5724)
2023/09/27 08:12:55 - INFO - root -   Epoch: [48/200][100/346], lr: 0.00000017 	 loss = 0.7747(0.5791)
2023/09/27 08:13:56 - INFO - root -   Epoch: [48/200][120/346], lr: 0.00000017 	 loss = 0.7544(0.6080)
2023/09/27 08:14:40 - INFO - root -   Epoch: [48/200][140/346], lr: 0.00000017 	 loss = 0.5357(0.6002)
2023/09/27 08:15:41 - INFO - root -   Epoch: [48/200][160/346], lr: 0.00000017 	 loss = 0.2145(0.6000)
2023/09/27 08:16:24 - INFO - root -   Epoch: [48/200][180/346], lr: 0.00000017 	 loss = 0.7770(0.6134)
2023/09/27 08:17:26 - INFO - root -   Epoch: [48/200][200/346], lr: 0.00000017 	 loss = 0.5715(0.6096)
2023/09/27 08:18:09 - INFO - root -   Epoch: [48/200][220/346], lr: 0.00000017 	 loss = 0.2653(0.6112)
2023/09/27 08:19:10 - INFO - root -   Epoch: [48/200][240/346], lr: 0.00000017 	 loss = 0.3882(0.6059)
2023/09/27 08:19:54 - INFO - root -   Epoch: [48/200][260/346], lr: 0.00000017 	 loss = 0.5536(0.6010)
2023/09/27 08:20:54 - INFO - root -   Epoch: [48/200][280/346], lr: 0.00000017 	 loss = 0.4409(0.6043)
2023/09/27 08:21:38 - INFO - root -   Epoch: [48/200][300/346], lr: 0.00000017 	 loss = 0.2680(0.6054)
2023/09/27 08:22:39 - INFO - root -   Epoch: [48/200][320/346], lr: 0.00000017 	 loss = 0.1601(0.6054)
2023/09/27 08:23:20 - INFO - root -   Epoch: [48/200][340/346], lr: 0.00000017 	 loss = 1.1341(0.6032)
2023/09/27 08:23:25 - INFO - root -   Epoch: [48/200] 	 loss = 0.6013
2023/09/27 08:23:25 - INFO - root -   train_accuracy = 0.6792
2023/09/27 08:23:47 - INFO - root -   Epoch: [49/200][0/346], lr: 0.00000017 	 loss = 0.2522(0.2522)
2023/09/27 08:24:30 - INFO - root -   Epoch: [49/200][20/346], lr: 0.00000017 	 loss = 0.3751(0.4860)
2023/09/27 08:25:31 - INFO - root -   Epoch: [49/200][40/346], lr: 0.00000017 	 loss = 0.4805(0.5208)
2023/09/27 08:26:14 - INFO - root -   Epoch: [49/200][60/346], lr: 0.00000017 	 loss = 0.3951(0.4842)
2023/09/27 08:27:15 - INFO - root -   Epoch: [49/200][80/346], lr: 0.00000017 	 loss = 0.9466(0.5131)
2023/09/27 08:27:58 - INFO - root -   Epoch: [49/200][100/346], lr: 0.00000017 	 loss = 0.7598(0.5424)
2023/09/27 08:28:59 - INFO - root -   Epoch: [49/200][120/346], lr: 0.00000017 	 loss = 0.6984(0.5679)
2023/09/27 08:29:42 - INFO - root -   Epoch: [49/200][140/346], lr: 0.00000017 	 loss = 0.5742(0.5725)
2023/09/27 08:30:43 - INFO - root -   Epoch: [49/200][160/346], lr: 0.00000017 	 loss = 0.2337(0.5794)
2023/09/27 08:31:26 - INFO - root -   Epoch: [49/200][180/346], lr: 0.00000017 	 loss = 0.5982(0.5881)
2023/09/27 08:32:26 - INFO - root -   Epoch: [49/200][200/346], lr: 0.00000017 	 loss = 0.3309(0.5911)
2023/09/27 08:33:10 - INFO - root -   Epoch: [49/200][220/346], lr: 0.00000017 	 loss = 0.1824(0.5906)
2023/09/27 08:34:10 - INFO - root -   Epoch: [49/200][240/346], lr: 0.00000017 	 loss = 0.2638(0.5942)
2023/09/27 08:34:53 - INFO - root -   Epoch: [49/200][260/346], lr: 0.00000017 	 loss = 0.8025(0.5940)
2023/09/27 08:35:53 - INFO - root -   Epoch: [49/200][280/346], lr: 0.00000017 	 loss = 0.6829(0.6062)
2023/09/27 08:36:37 - INFO - root -   Epoch: [49/200][300/346], lr: 0.00000017 	 loss = 0.3459(0.6049)
2023/09/27 08:37:37 - INFO - root -   Epoch: [49/200][320/346], lr: 0.00000017 	 loss = 0.2775(0.6091)
2023/09/27 08:38:19 - INFO - root -   Epoch: [49/200][340/346], lr: 0.00000017 	 loss = 1.7575(0.6093)
2023/09/27 08:38:23 - INFO - root -   Epoch: [49/200] 	 loss = 0.6093
2023/09/27 08:44:08 - INFO - root -   precision = 0.6207
2023/09/27 08:44:08 - INFO - root -   eval_loss = 0.6972
2023/09/27 08:44:09 - INFO - root -   train_accuracy = 0.6792
2023/09/27 08:44:31 - INFO - root -   Epoch: [50/200][0/346], lr: 0.00000017 	 loss = 0.3161(0.3161)
2023/09/27 08:45:14 - INFO - root -   Epoch: [50/200][20/346], lr: 0.00000017 	 loss = 0.9873(0.5896)
2023/09/27 08:46:18 - INFO - root -   Epoch: [50/200][40/346], lr: 0.00000017 	 loss = 0.3303(0.5824)
2023/09/27 08:47:02 - INFO - root -   Epoch: [50/200][60/346], lr: 0.00000017 	 loss = 0.4341(0.5398)
2023/09/27 08:48:03 - INFO - root -   Epoch: [50/200][80/346], lr: 0.00000017 	 loss = 1.0796(0.5680)
2023/09/27 08:48:46 - INFO - root -   Epoch: [50/200][100/346], lr: 0.00000017 	 loss = 0.5594(0.5856)
2023/09/27 08:49:47 - INFO - root -   Epoch: [50/200][120/346], lr: 0.00000017 	 loss = 0.6868(0.6005)
2023/09/27 08:50:30 - INFO - root -   Epoch: [50/200][140/346], lr: 0.00000017 	 loss = 1.1600(0.5977)
2023/09/27 08:51:31 - INFO - root -   Epoch: [50/200][160/346], lr: 0.00000017 	 loss = 0.1976(0.5995)
2023/09/27 08:52:14 - INFO - root -   Epoch: [50/200][180/346], lr: 0.00000017 	 loss = 0.6015(0.5993)
2023/09/27 08:53:14 - INFO - root -   Epoch: [50/200][200/346], lr: 0.00000017 	 loss = 0.1636(0.5929)
2023/09/27 08:53:58 - INFO - root -   Epoch: [50/200][220/346], lr: 0.00000017 	 loss = 0.1563(0.5940)
2023/09/27 08:54:58 - INFO - root -   Epoch: [50/200][240/346], lr: 0.00000017 	 loss = 0.4265(0.5960)
2023/09/27 08:55:42 - INFO - root -   Epoch: [50/200][260/346], lr: 0.00000017 	 loss = 0.8185(0.5923)
2023/09/27 08:56:42 - INFO - root -   Epoch: [50/200][280/346], lr: 0.00000017 	 loss = 0.5714(0.6066)
2023/09/27 08:57:26 - INFO - root -   Epoch: [50/200][300/346], lr: 0.00000017 	 loss = 0.1622(0.6002)
2023/09/27 08:58:26 - INFO - root -   Epoch: [50/200][320/346], lr: 0.00000017 	 loss = 0.4071(0.5980)
2023/09/27 08:59:08 - INFO - root -   Epoch: [50/200][340/346], lr: 0.00000017 	 loss = 0.9010(0.5951)
2023/09/27 08:59:12 - INFO - root -   Epoch: [50/200] 	 loss = 0.5953
2023/09/27 08:59:12 - INFO - root -   train_accuracy = 0.6879
2023/09/27 08:59:34 - INFO - root -   Epoch: [51/200][0/346], lr: 0.00000017 	 loss = 0.3581(0.3581)
2023/09/27 09:00:18 - INFO - root -   Epoch: [51/200][20/346], lr: 0.00000017 	 loss = 0.4051(0.4926)
2023/09/27 09:01:19 - INFO - root -   Epoch: [51/200][40/346], lr: 0.00000017 	 loss = 0.2666(0.5246)
2023/09/27 09:02:02 - INFO - root -   Epoch: [51/200][60/346], lr: 0.00000017 	 loss = 0.2575(0.5041)
2023/09/27 09:03:03 - INFO - root -   Epoch: [51/200][80/346], lr: 0.00000017 	 loss = 0.6577(0.5442)
2023/09/27 09:03:48 - INFO - root -   Epoch: [51/200][100/346], lr: 0.00000017 	 loss = 0.7277(0.5470)
2023/09/27 09:04:47 - INFO - root -   Epoch: [51/200][120/346], lr: 0.00000017 	 loss = 0.5035(0.5693)
2023/09/27 09:05:33 - INFO - root -   Epoch: [51/200][140/346], lr: 0.00000017 	 loss = 0.5332(0.5639)
2023/09/27 09:06:31 - INFO - root -   Epoch: [51/200][160/346], lr: 0.00000017 	 loss = 0.1378(0.5710)
2023/09/27 09:07:18 - INFO - root -   Epoch: [51/200][180/346], lr: 0.00000017 	 loss = 0.8296(0.5873)
2023/09/27 09:08:15 - INFO - root -   Epoch: [51/200][200/346], lr: 0.00000017 	 loss = 0.1974(0.5867)
2023/09/27 09:09:03 - INFO - root -   Epoch: [51/200][220/346], lr: 0.00000017 	 loss = 0.4570(0.5891)
2023/09/27 09:10:00 - INFO - root -   Epoch: [51/200][240/346], lr: 0.00000017 	 loss = 0.2747(0.5884)
2023/09/27 09:10:47 - INFO - root -   Epoch: [51/200][260/346], lr: 0.00000017 	 loss = 0.4501(0.5902)
2023/09/27 09:11:45 - INFO - root -   Epoch: [51/200][280/346], lr: 0.00000017 	 loss = 0.5031(0.6077)
2023/09/27 09:12:32 - INFO - root -   Epoch: [51/200][300/346], lr: 0.00000017 	 loss = 0.1882(0.6103)
2023/09/27 09:13:29 - INFO - root -   Epoch: [51/200][320/346], lr: 0.00000017 	 loss = 0.4914(0.6145)
2023/09/27 09:14:15 - INFO - root -   Epoch: [51/200][340/346], lr: 0.00000017 	 loss = 1.1236(0.6133)
2023/09/27 09:14:19 - INFO - root -   Epoch: [51/200] 	 loss = 0.6123
2023/09/27 09:14:19 - INFO - root -   train_accuracy = 0.6792
2023/09/27 09:14:40 - INFO - root -   Epoch: [52/200][0/346], lr: 0.00000017 	 loss = 0.1936(0.1936)
2023/09/27 09:15:24 - INFO - root -   Epoch: [52/200][20/346], lr: 0.00000017 	 loss = 0.2867(0.4637)
2023/09/27 09:16:24 - INFO - root -   Epoch: [52/200][40/346], lr: 0.00000017 	 loss = 0.1634(0.5134)
2023/09/27 09:17:08 - INFO - root -   Epoch: [52/200][60/346], lr: 0.00000017 	 loss = 0.6285(0.4768)
2023/09/27 09:18:08 - INFO - root -   Epoch: [52/200][80/346], lr: 0.00000017 	 loss = 0.4984(0.5245)
2023/09/27 09:18:52 - INFO - root -   Epoch: [52/200][100/346], lr: 0.00000017 	 loss = 0.9485(0.5354)
2023/09/27 09:19:52 - INFO - root -   Epoch: [52/200][120/346], lr: 0.00000017 	 loss = 0.3217(0.5678)
2023/09/27 09:20:36 - INFO - root -   Epoch: [52/200][140/346], lr: 0.00000017 	 loss = 0.4202(0.5645)
2023/09/27 09:21:36 - INFO - root -   Epoch: [52/200][160/346], lr: 0.00000017 	 loss = 0.2692(0.5684)
2023/09/27 09:22:20 - INFO - root -   Epoch: [52/200][180/346], lr: 0.00000017 	 loss = 1.0536(0.5752)
2023/09/27 09:23:20 - INFO - root -   Epoch: [52/200][200/346], lr: 0.00000017 	 loss = 0.2668(0.5769)
2023/09/27 09:24:03 - INFO - root -   Epoch: [52/200][220/346], lr: 0.00000017 	 loss = 0.2728(0.5859)
2023/09/27 09:25:04 - INFO - root -   Epoch: [52/200][240/346], lr: 0.00000017 	 loss = 0.3102(0.5836)
2023/09/27 09:25:47 - INFO - root -   Epoch: [52/200][260/346], lr: 0.00000017 	 loss = 0.3587(0.5819)
2023/09/27 09:26:47 - INFO - root -   Epoch: [52/200][280/346], lr: 0.00000017 	 loss = 0.4247(0.5941)
2023/09/27 09:27:31 - INFO - root -   Epoch: [52/200][300/346], lr: 0.00000017 	 loss = 0.5845(0.5979)
2023/09/27 09:28:31 - INFO - root -   Epoch: [52/200][320/346], lr: 0.00000017 	 loss = 0.3918(0.6028)
2023/09/27 09:29:14 - INFO - root -   Epoch: [52/200][340/346], lr: 0.00000017 	 loss = 1.2412(0.6031)
2023/09/27 09:29:17 - INFO - root -   Epoch: [52/200] 	 loss = 0.6048
2023/09/27 09:29:17 - INFO - root -   train_accuracy = 0.7095
2023/09/27 09:29:39 - INFO - root -   Epoch: [53/200][0/346], lr: 0.00000018 	 loss = 0.1866(0.1866)
2023/09/27 09:30:23 - INFO - root -   Epoch: [53/200][20/346], lr: 0.00000018 	 loss = 0.3676(0.4794)
2023/09/27 09:31:24 - INFO - root -   Epoch: [53/200][40/346], lr: 0.00000018 	 loss = 0.2809(0.5584)
2023/09/27 09:32:07 - INFO - root -   Epoch: [53/200][60/346], lr: 0.00000018 	 loss = 0.7043(0.5394)
2023/09/27 09:33:09 - INFO - root -   Epoch: [53/200][80/346], lr: 0.00000018 	 loss = 1.1377(0.6050)
2023/09/27 09:33:52 - INFO - root -   Epoch: [53/200][100/346], lr: 0.00000018 	 loss = 0.8899(0.6032)
2023/09/27 09:34:54 - INFO - root -   Epoch: [53/200][120/346], lr: 0.00000018 	 loss = 0.6674(0.6218)
2023/09/27 09:35:37 - INFO - root -   Epoch: [53/200][140/346], lr: 0.00000018 	 loss = 1.1521(0.6092)
2023/09/27 09:36:38 - INFO - root -   Epoch: [53/200][160/346], lr: 0.00000018 	 loss = 0.1746(0.6132)
2023/09/27 09:37:22 - INFO - root -   Epoch: [53/200][180/346], lr: 0.00000018 	 loss = 0.2813(0.6100)
2023/09/27 09:38:23 - INFO - root -   Epoch: [53/200][200/346], lr: 0.00000018 	 loss = 0.1916(0.6034)
2023/09/27 09:39:06 - INFO - root -   Epoch: [53/200][220/346], lr: 0.00000018 	 loss = 0.1669(0.6080)
2023/09/27 09:40:08 - INFO - root -   Epoch: [53/200][240/346], lr: 0.00000018 	 loss = 0.1330(0.6054)
2023/09/27 09:40:51 - INFO - root -   Epoch: [53/200][260/346], lr: 0.00000018 	 loss = 0.6405(0.6014)
2023/09/27 09:41:52 - INFO - root -   Epoch: [53/200][280/346], lr: 0.00000018 	 loss = 0.2116(0.6072)
2023/09/27 09:42:35 - INFO - root -   Epoch: [53/200][300/346], lr: 0.00000018 	 loss = 0.1316(0.6056)
2023/09/27 09:43:36 - INFO - root -   Epoch: [53/200][320/346], lr: 0.00000018 	 loss = 0.3085(0.6028)
2023/09/27 09:44:18 - INFO - root -   Epoch: [53/200][340/346], lr: 0.00000018 	 loss = 1.1539(0.5995)
2023/09/27 09:44:22 - INFO - root -   Epoch: [53/200] 	 loss = 0.5981
2023/09/27 09:44:22 - INFO - root -   train_accuracy = 0.7038
2023/09/27 09:44:44 - INFO - root -   Epoch: [54/200][0/346], lr: 0.00000018 	 loss = 0.4606(0.4606)
2023/09/27 09:45:27 - INFO - root -   Epoch: [54/200][20/346], lr: 0.00000018 	 loss = 0.2480(0.5063)
2023/09/27 09:46:29 - INFO - root -   Epoch: [54/200][40/346], lr: 0.00000018 	 loss = 0.3513(0.5652)
2023/09/27 09:47:12 - INFO - root -   Epoch: [54/200][60/346], lr: 0.00000018 	 loss = 0.8419(0.5303)
2023/09/27 09:48:14 - INFO - root -   Epoch: [54/200][80/346], lr: 0.00000018 	 loss = 0.8587(0.5472)
2023/09/27 09:48:57 - INFO - root -   Epoch: [54/200][100/346], lr: 0.00000018 	 loss = 0.9937(0.5584)
2023/09/27 09:49:58 - INFO - root -   Epoch: [54/200][120/346], lr: 0.00000018 	 loss = 0.1668(0.5849)
2023/09/27 09:50:42 - INFO - root -   Epoch: [54/200][140/346], lr: 0.00000018 	 loss = 1.2477(0.5800)
2023/09/27 09:51:43 - INFO - root -   Epoch: [54/200][160/346], lr: 0.00000018 	 loss = 0.1412(0.5731)
2023/09/27 09:52:26 - INFO - root -   Epoch: [54/200][180/346], lr: 0.00000018 	 loss = 0.6761(0.5777)
2023/09/27 09:53:27 - INFO - root -   Epoch: [54/200][200/346], lr: 0.00000018 	 loss = 0.1885(0.5702)
2023/09/27 09:54:10 - INFO - root -   Epoch: [54/200][220/346], lr: 0.00000018 	 loss = 0.4505(0.5799)
2023/09/27 09:55:12 - INFO - root -   Epoch: [54/200][240/346], lr: 0.00000018 	 loss = 0.1831(0.5756)
2023/09/27 09:55:55 - INFO - root -   Epoch: [54/200][260/346], lr: 0.00000018 	 loss = 0.3866(0.5777)
2023/09/27 09:56:56 - INFO - root -   Epoch: [54/200][280/346], lr: 0.00000018 	 loss = 0.6562(0.5943)
2023/09/27 09:57:40 - INFO - root -   Epoch: [54/200][300/346], lr: 0.00000018 	 loss = 0.2911(0.5962)
2023/09/27 09:58:41 - INFO - root -   Epoch: [54/200][320/346], lr: 0.00000018 	 loss = 0.3304(0.5947)
2023/09/27 09:59:23 - INFO - root -   Epoch: [54/200][340/346], lr: 0.00000018 	 loss = 1.2785(0.5948)
2023/09/27 09:59:26 - INFO - root -   Epoch: [54/200] 	 loss = 0.5944
2023/09/27 10:05:20 - INFO - root -   precision = 0.5747
2023/09/27 10:05:20 - INFO - root -   eval_loss = 0.7632
2023/09/27 10:05:21 - INFO - root -   train_accuracy = 0.6835
2023/09/27 10:05:43 - INFO - root -   Epoch: [55/200][0/346], lr: 0.00000018 	 loss = 0.1981(0.1981)
2023/09/27 10:06:26 - INFO - root -   Epoch: [55/200][20/346], lr: 0.00000018 	 loss = 0.7195(0.5340)
2023/09/27 10:07:27 - INFO - root -   Epoch: [55/200][40/346], lr: 0.00000018 	 loss = 0.5218(0.5654)
2023/09/27 10:08:11 - INFO - root -   Epoch: [55/200][60/346], lr: 0.00000018 	 loss = 0.4590(0.5392)
2023/09/27 10:09:12 - INFO - root -   Epoch: [55/200][80/346], lr: 0.00000018 	 loss = 0.7830(0.5446)
2023/09/27 10:09:56 - INFO - root -   Epoch: [55/200][100/346], lr: 0.00000018 	 loss = 0.6817(0.5895)
2023/09/27 10:10:56 - INFO - root -   Epoch: [55/200][120/346], lr: 0.00000018 	 loss = 0.7361(0.6182)
2023/09/27 10:11:41 - INFO - root -   Epoch: [55/200][140/346], lr: 0.00000018 	 loss = 0.6603(0.6064)
2023/09/27 10:12:39 - INFO - root -   Epoch: [55/200][160/346], lr: 0.00000018 	 loss = 0.1438(0.6075)
2023/09/27 10:13:27 - INFO - root -   Epoch: [55/200][180/346], lr: 0.00000018 	 loss = 0.4839(0.6038)
2023/09/27 10:14:23 - INFO - root -   Epoch: [55/200][200/346], lr: 0.00000018 	 loss = 0.1585(0.5974)
2023/09/27 10:15:12 - INFO - root -   Epoch: [55/200][220/346], lr: 0.00000018 	 loss = 0.3199(0.5991)
2023/09/27 10:16:08 - INFO - root -   Epoch: [55/200][240/346], lr: 0.00000018 	 loss = 0.2675(0.5965)
2023/09/27 10:16:56 - INFO - root -   Epoch: [55/200][260/346], lr: 0.00000018 	 loss = 0.6231(0.5952)
2023/09/27 10:17:52 - INFO - root -   Epoch: [55/200][280/346], lr: 0.00000018 	 loss = 0.3140(0.6007)
2023/09/27 10:18:40 - INFO - root -   Epoch: [55/200][300/346], lr: 0.00000018 	 loss = 0.2303(0.5957)
2023/09/27 10:19:36 - INFO - root -   Epoch: [55/200][320/346], lr: 0.00000018 	 loss = 0.2645(0.5983)
2023/09/27 10:20:22 - INFO - root -   Epoch: [55/200][340/346], lr: 0.00000018 	 loss = 1.0828(0.5964)
2023/09/27 10:20:26 - INFO - root -   Epoch: [55/200] 	 loss = 0.5954
2023/09/27 10:20:26 - INFO - root -   train_accuracy = 0.6994
2023/09/27 10:20:47 - INFO - root -   Epoch: [56/200][0/346], lr: 0.00000018 	 loss = 0.5526(0.5526)
2023/09/27 10:21:31 - INFO - root -   Epoch: [56/200][20/346], lr: 0.00000018 	 loss = 0.2566(0.4828)
2023/09/27 10:22:33 - INFO - root -   Epoch: [56/200][40/346], lr: 0.00000018 	 loss = 0.1884(0.5225)
2023/09/27 10:23:16 - INFO - root -   Epoch: [56/200][60/346], lr: 0.00000018 	 loss = 0.2144(0.4774)
2023/09/27 10:24:18 - INFO - root -   Epoch: [56/200][80/346], lr: 0.00000018 	 loss = 1.0119(0.5374)
2023/09/27 10:25:02 - INFO - root -   Epoch: [56/200][100/346], lr: 0.00000018 	 loss = 0.8487(0.5469)
2023/09/27 10:26:03 - INFO - root -   Epoch: [56/200][120/346], lr: 0.00000018 	 loss = 0.5369(0.5825)
2023/09/27 10:26:46 - INFO - root -   Epoch: [56/200][140/346], lr: 0.00000018 	 loss = 0.4648(0.5690)
2023/09/27 10:27:48 - INFO - root -   Epoch: [56/200][160/346], lr: 0.00000018 	 loss = 0.2200(0.5796)
2023/09/27 10:28:31 - INFO - root -   Epoch: [56/200][180/346], lr: 0.00000018 	 loss = 0.3773(0.5832)
2023/09/27 10:29:33 - INFO - root -   Epoch: [56/200][200/346], lr: 0.00000018 	 loss = 0.2400(0.5829)
2023/09/27 10:30:16 - INFO - root -   Epoch: [56/200][220/346], lr: 0.00000018 	 loss = 0.1166(0.5888)
2023/09/27 10:31:18 - INFO - root -   Epoch: [56/200][240/346], lr: 0.00000018 	 loss = 0.0601(0.5842)
2023/09/27 10:32:01 - INFO - root -   Epoch: [56/200][260/346], lr: 0.00000018 	 loss = 0.6491(0.5772)
2023/09/27 10:33:02 - INFO - root -   Epoch: [56/200][280/346], lr: 0.00000018 	 loss = 0.2243(0.5875)
2023/09/27 10:33:46 - INFO - root -   Epoch: [56/200][300/346], lr: 0.00000018 	 loss = 0.4303(0.5850)
2023/09/27 10:34:47 - INFO - root -   Epoch: [56/200][320/346], lr: 0.00000018 	 loss = 0.1965(0.5868)
2023/09/27 10:35:29 - INFO - root -   Epoch: [56/200][340/346], lr: 0.00000018 	 loss = 1.0954(0.5812)
2023/09/27 10:35:33 - INFO - root -   Epoch: [56/200] 	 loss = 0.5798
2023/09/27 10:35:33 - INFO - root -   train_accuracy = 0.7197
2023/09/27 10:35:55 - INFO - root -   Epoch: [57/200][0/346], lr: 0.00000018 	 loss = 0.3046(0.3046)
2023/09/27 10:36:38 - INFO - root -   Epoch: [57/200][20/346], lr: 0.00000018 	 loss = 0.6477(0.4943)
2023/09/27 10:37:39 - INFO - root -   Epoch: [57/200][40/346], lr: 0.00000018 	 loss = 0.2819(0.4865)
2023/09/27 10:38:23 - INFO - root -   Epoch: [57/200][60/346], lr: 0.00000018 	 loss = 0.2395(0.4516)
2023/09/27 10:39:24 - INFO - root -   Epoch: [57/200][80/346], lr: 0.00000018 	 loss = 0.8100(0.4720)
2023/09/27 10:40:08 - INFO - root -   Epoch: [57/200][100/346], lr: 0.00000018 	 loss = 0.8647(0.5065)
2023/09/27 10:41:09 - INFO - root -   Epoch: [57/200][120/346], lr: 0.00000018 	 loss = 0.8515(0.5398)
2023/09/27 10:41:52 - INFO - root -   Epoch: [57/200][140/346], lr: 0.00000018 	 loss = 0.8760(0.5414)
2023/09/27 10:42:53 - INFO - root -   Epoch: [57/200][160/346], lr: 0.00000018 	 loss = 0.2153(0.5468)
2023/09/27 10:43:37 - INFO - root -   Epoch: [57/200][180/346], lr: 0.00000018 	 loss = 1.0468(0.5522)
2023/09/27 10:44:38 - INFO - root -   Epoch: [57/200][200/346], lr: 0.00000018 	 loss = 0.1324(0.5531)
2023/09/27 10:45:21 - INFO - root -   Epoch: [57/200][220/346], lr: 0.00000018 	 loss = 0.2260(0.5571)
2023/09/27 10:46:22 - INFO - root -   Epoch: [57/200][240/346], lr: 0.00000018 	 loss = 0.1470(0.5595)
2023/09/27 10:47:06 - INFO - root -   Epoch: [57/200][260/346], lr: 0.00000018 	 loss = 0.3327(0.5540)
2023/09/27 10:48:07 - INFO - root -   Epoch: [57/200][280/346], lr: 0.00000018 	 loss = 0.2367(0.5632)
2023/09/27 10:48:50 - INFO - root -   Epoch: [57/200][300/346], lr: 0.00000018 	 loss = 0.1576(0.5647)
2023/09/27 10:49:51 - INFO - root -   Epoch: [57/200][320/346], lr: 0.00000018 	 loss = 0.4727(0.5729)
2023/09/27 10:50:32 - INFO - root -   Epoch: [57/200][340/346], lr: 0.00000018 	 loss = 1.3029(0.5710)
2023/09/27 10:50:35 - INFO - root -   Epoch: [57/200] 	 loss = 0.5719
2023/09/27 10:50:35 - INFO - root -   train_accuracy = 0.7124
2023/09/27 10:50:57 - INFO - root -   Epoch: [58/200][0/346], lr: 0.00000018 	 loss = 0.2460(0.2460)
2023/09/27 10:51:40 - INFO - root -   Epoch: [58/200][20/346], lr: 0.00000018 	 loss = 0.2643(0.4590)
2023/09/27 10:52:41 - INFO - root -   Epoch: [58/200][40/346], lr: 0.00000018 	 loss = 0.4666(0.5573)
2023/09/27 10:53:25 - INFO - root -   Epoch: [58/200][60/346], lr: 0.00000018 	 loss = 0.2440(0.5253)
2023/09/27 10:54:26 - INFO - root -   Epoch: [58/200][80/346], lr: 0.00000018 	 loss = 1.1080(0.5625)
2023/09/27 10:55:09 - INFO - root -   Epoch: [58/200][100/346], lr: 0.00000018 	 loss = 1.0521(0.5882)
2023/09/27 10:56:10 - INFO - root -   Epoch: [58/200][120/346], lr: 0.00000018 	 loss = 0.4091(0.6181)
2023/09/27 10:56:53 - INFO - root -   Epoch: [58/200][140/346], lr: 0.00000018 	 loss = 0.3618(0.6003)
2023/09/27 10:57:54 - INFO - root -   Epoch: [58/200][160/346], lr: 0.00000018 	 loss = 0.3729(0.6003)
2023/09/27 10:58:38 - INFO - root -   Epoch: [58/200][180/346], lr: 0.00000018 	 loss = 1.2075(0.6045)
2023/09/27 10:59:39 - INFO - root -   Epoch: [58/200][200/346], lr: 0.00000018 	 loss = 0.2093(0.6083)
2023/09/27 11:00:22 - INFO - root -   Epoch: [58/200][220/346], lr: 0.00000018 	 loss = 0.1997(0.6152)
2023/09/27 11:01:23 - INFO - root -   Epoch: [58/200][240/346], lr: 0.00000018 	 loss = 0.1848(0.6096)
2023/09/27 11:02:06 - INFO - root -   Epoch: [58/200][260/346], lr: 0.00000018 	 loss = 0.3668(0.5984)
2023/09/27 11:03:07 - INFO - root -   Epoch: [58/200][280/346], lr: 0.00000018 	 loss = 0.5099(0.6120)
2023/09/27 11:03:50 - INFO - root -   Epoch: [58/200][300/346], lr: 0.00000018 	 loss = 0.3156(0.6084)
2023/09/27 11:04:50 - INFO - root -   Epoch: [58/200][320/346], lr: 0.00000018 	 loss = 0.2646(0.6087)
2023/09/27 11:05:32 - INFO - root -   Epoch: [58/200][340/346], lr: 0.00000018 	 loss = 1.4275(0.6121)
2023/09/27 11:05:36 - INFO - root -   Epoch: [58/200] 	 loss = 0.6113
2023/09/27 11:05:36 - INFO - root -   train_accuracy = 0.6850
2023/09/27 11:05:58 - INFO - root -   Epoch: [59/200][0/346], lr: 0.00000018 	 loss = 0.1669(0.1669)
2023/09/27 11:06:42 - INFO - root -   Epoch: [59/200][20/346], lr: 0.00000018 	 loss = 0.2001(0.4220)
2023/09/27 11:07:43 - INFO - root -   Epoch: [59/200][40/346], lr: 0.00000018 	 loss = 0.6453(0.5365)
2023/09/27 11:08:26 - INFO - root -   Epoch: [59/200][60/346], lr: 0.00000018 	 loss = 0.2970(0.5115)
2023/09/27 11:09:27 - INFO - root -   Epoch: [59/200][80/346], lr: 0.00000018 	 loss = 0.4176(0.5481)
2023/09/27 11:10:11 - INFO - root -   Epoch: [59/200][100/346], lr: 0.00000018 	 loss = 0.8731(0.5661)
2023/09/27 11:11:12 - INFO - root -   Epoch: [59/200][120/346], lr: 0.00000018 	 loss = 0.2967(0.5897)
2023/09/27 11:11:55 - INFO - root -   Epoch: [59/200][140/346], lr: 0.00000018 	 loss = 0.6567(0.5828)
2023/09/27 11:12:56 - INFO - root -   Epoch: [59/200][160/346], lr: 0.00000018 	 loss = 0.0953(0.5843)
2023/09/27 11:13:39 - INFO - root -   Epoch: [59/200][180/346], lr: 0.00000018 	 loss = 0.4403(0.5803)
2023/09/27 11:14:39 - INFO - root -   Epoch: [59/200][200/346], lr: 0.00000018 	 loss = 0.1627(0.5810)
2023/09/27 11:15:23 - INFO - root -   Epoch: [59/200][220/346], lr: 0.00000018 	 loss = 0.1830(0.5941)
2023/09/27 11:16:23 - INFO - root -   Epoch: [59/200][240/346], lr: 0.00000018 	 loss = 0.2457(0.5970)
2023/09/27 11:17:07 - INFO - root -   Epoch: [59/200][260/346], lr: 0.00000018 	 loss = 0.1918(0.5908)
2023/09/27 11:18:07 - INFO - root -   Epoch: [59/200][280/346], lr: 0.00000018 	 loss = 0.3757(0.5987)
2023/09/27 11:18:51 - INFO - root -   Epoch: [59/200][300/346], lr: 0.00000018 	 loss = 0.5429(0.5941)
2023/09/27 11:19:50 - INFO - root -   Epoch: [59/200][320/346], lr: 0.00000018 	 loss = 0.6169(0.5880)
2023/09/27 11:20:33 - INFO - root -   Epoch: [59/200][340/346], lr: 0.00000018 	 loss = 1.2261(0.5847)
2023/09/27 11:20:37 - INFO - root -   Epoch: [59/200] 	 loss = 0.5837
2023/09/27 11:26:31 - INFO - root -   precision = 0.5977
2023/09/27 11:26:31 - INFO - root -   eval_loss = 0.7356
2023/09/27 11:26:32 - INFO - root -   train_accuracy = 0.7009
2023/09/27 11:26:54 - INFO - root -   Epoch: [60/200][0/346], lr: 0.00000019 	 loss = 0.1840(0.1840)
2023/09/27 11:27:37 - INFO - root -   Epoch: [60/200][20/346], lr: 0.00000019 	 loss = 0.6831(0.5174)
2023/09/27 11:28:38 - INFO - root -   Epoch: [60/200][40/346], lr: 0.00000019 	 loss = 0.4131(0.5230)
2023/09/27 11:29:21 - INFO - root -   Epoch: [60/200][60/346], lr: 0.00000019 	 loss = 0.4885(0.4787)
2023/09/27 11:30:22 - INFO - root -   Epoch: [60/200][80/346], lr: 0.00000019 	 loss = 0.7766(0.5188)
2023/09/27 11:31:05 - INFO - root -   Epoch: [60/200][100/346], lr: 0.00000019 	 loss = 0.5501(0.5407)
2023/09/27 11:32:06 - INFO - root -   Epoch: [60/200][120/346], lr: 0.00000019 	 loss = 0.4948(0.5698)
2023/09/27 11:32:49 - INFO - root -   Epoch: [60/200][140/346], lr: 0.00000019 	 loss = 0.6471(0.5623)
2023/09/27 11:33:50 - INFO - root -   Epoch: [60/200][160/346], lr: 0.00000019 	 loss = 0.2120(0.5655)
2023/09/27 11:34:33 - INFO - root -   Epoch: [60/200][180/346], lr: 0.00000019 	 loss = 0.5827(0.5691)
2023/09/27 11:35:34 - INFO - root -   Epoch: [60/200][200/346], lr: 0.00000019 	 loss = 0.0825(0.5598)
2023/09/27 11:36:17 - INFO - root -   Epoch: [60/200][220/346], lr: 0.00000019 	 loss = 0.1681(0.5806)
2023/09/27 11:37:18 - INFO - root -   Epoch: [60/200][240/346], lr: 0.00000019 	 loss = 0.1490(0.5813)
2023/09/27 11:38:01 - INFO - root -   Epoch: [60/200][260/346], lr: 0.00000019 	 loss = 0.5587(0.5772)
2023/09/27 11:39:02 - INFO - root -   Epoch: [60/200][280/346], lr: 0.00000019 	 loss = 0.8311(0.5907)
2023/09/27 11:39:45 - INFO - root -   Epoch: [60/200][300/346], lr: 0.00000019 	 loss = 0.4764(0.5887)
2023/09/27 11:40:45 - INFO - root -   Epoch: [60/200][320/346], lr: 0.00000019 	 loss = 0.4988(0.5921)
2023/09/27 11:41:28 - INFO - root -   Epoch: [60/200][340/346], lr: 0.00000019 	 loss = 1.2423(0.5857)
2023/09/27 11:41:32 - INFO - root -   Epoch: [60/200] 	 loss = 0.5848
2023/09/27 11:41:32 - INFO - root -   train_accuracy = 0.7081
2023/09/27 11:41:54 - INFO - root -   Epoch: [61/200][0/346], lr: 0.00000019 	 loss = 0.2746(0.2746)
2023/09/27 11:42:37 - INFO - root -   Epoch: [61/200][20/346], lr: 0.00000019 	 loss = 0.1850(0.4190)
2023/09/27 11:43:39 - INFO - root -   Epoch: [61/200][40/346], lr: 0.00000019 	 loss = 0.3645(0.4903)
2023/09/27 11:44:22 - INFO - root -   Epoch: [61/200][60/346], lr: 0.00000019 	 loss = 0.2667(0.4612)
2023/09/27 11:45:24 - INFO - root -   Epoch: [61/200][80/346], lr: 0.00000019 	 loss = 1.0603(0.5116)
2023/09/27 11:46:07 - INFO - root -   Epoch: [61/200][100/346], lr: 0.00000019 	 loss = 0.7391(0.5320)
2023/09/27 11:47:08 - INFO - root -   Epoch: [61/200][120/346], lr: 0.00000019 	 loss = 0.6369(0.5635)
2023/09/27 11:47:52 - INFO - root -   Epoch: [61/200][140/346], lr: 0.00000019 	 loss = 0.6654(0.5490)
2023/09/27 11:48:53 - INFO - root -   Epoch: [61/200][160/346], lr: 0.00000019 	 loss = 0.2085(0.5480)
2023/09/27 11:49:36 - INFO - root -   Epoch: [61/200][180/346], lr: 0.00000019 	 loss = 0.7584(0.5520)
2023/09/27 11:50:37 - INFO - root -   Epoch: [61/200][200/346], lr: 0.00000019 	 loss = 0.4569(0.5544)
2023/09/27 11:51:20 - INFO - root -   Epoch: [61/200][220/346], lr: 0.00000019 	 loss = 0.1809(0.5674)
2023/09/27 11:52:21 - INFO - root -   Epoch: [61/200][240/346], lr: 0.00000019 	 loss = 0.0607(0.5591)
2023/09/27 11:53:04 - INFO - root -   Epoch: [61/200][260/346], lr: 0.00000019 	 loss = 0.3075(0.5553)
2023/09/27 11:54:05 - INFO - root -   Epoch: [61/200][280/346], lr: 0.00000019 	 loss = 0.3790(0.5726)
2023/09/27 11:54:48 - INFO - root -   Epoch: [61/200][300/346], lr: 0.00000019 	 loss = 0.2565(0.5704)
2023/09/27 11:55:49 - INFO - root -   Epoch: [61/200][320/346], lr: 0.00000019 	 loss = 0.8212(0.5794)
2023/09/27 11:56:31 - INFO - root -   Epoch: [61/200][340/346], lr: 0.00000019 	 loss = 1.2173(0.5760)
2023/09/27 11:56:35 - INFO - root -   Epoch: [61/200] 	 loss = 0.5738
2023/09/27 11:56:35 - INFO - root -   train_accuracy = 0.7197
2023/09/27 11:56:57 - INFO - root -   Epoch: [62/200][0/346], lr: 0.00000019 	 loss = 0.3225(0.3225)
2023/09/27 11:57:41 - INFO - root -   Epoch: [62/200][20/346], lr: 0.00000019 	 loss = 0.4779(0.4410)
2023/09/27 11:58:42 - INFO - root -   Epoch: [62/200][40/346], lr: 0.00000019 	 loss = 0.5281(0.5032)
2023/09/27 11:59:25 - INFO - root -   Epoch: [62/200][60/346], lr: 0.00000019 	 loss = 0.4681(0.4867)
2023/09/27 12:00:26 - INFO - root -   Epoch: [62/200][80/346], lr: 0.00000019 	 loss = 0.4244(0.5200)
2023/09/27 12:01:10 - INFO - root -   Epoch: [62/200][100/346], lr: 0.00000019 	 loss = 1.0476(0.5531)
2023/09/27 12:02:10 - INFO - root -   Epoch: [62/200][120/346], lr: 0.00000019 	 loss = 0.8666(0.5807)
2023/09/27 12:02:54 - INFO - root -   Epoch: [62/200][140/346], lr: 0.00000019 	 loss = 0.5845(0.5681)
2023/09/27 12:03:54 - INFO - root -   Epoch: [62/200][160/346], lr: 0.00000019 	 loss = 0.1187(0.5654)
2023/09/27 12:04:38 - INFO - root -   Epoch: [62/200][180/346], lr: 0.00000019 	 loss = 0.6333(0.5760)
2023/09/27 12:05:38 - INFO - root -   Epoch: [62/200][200/346], lr: 0.00000019 	 loss = 0.0765(0.5768)
2023/09/27 12:06:22 - INFO - root -   Epoch: [62/200][220/346], lr: 0.00000019 	 loss = 0.1895(0.5871)
2023/09/27 12:07:22 - INFO - root -   Epoch: [62/200][240/346], lr: 0.00000019 	 loss = 0.1313(0.5944)
2023/09/27 12:08:05 - INFO - root -   Epoch: [62/200][260/346], lr: 0.00000019 	 loss = 0.5826(0.5907)
2023/09/27 12:09:06 - INFO - root -   Epoch: [62/200][280/346], lr: 0.00000019 	 loss = 0.2086(0.6027)
2023/09/27 12:09:49 - INFO - root -   Epoch: [62/200][300/346], lr: 0.00000019 	 loss = 0.3778(0.6036)
2023/09/27 12:10:50 - INFO - root -   Epoch: [62/200][320/346], lr: 0.00000019 	 loss = 0.4487(0.6046)
2023/09/27 12:11:31 - INFO - root -   Epoch: [62/200][340/346], lr: 0.00000019 	 loss = 1.3812(0.6012)
2023/09/27 12:11:35 - INFO - root -   Epoch: [62/200] 	 loss = 0.6014
2023/09/27 12:11:35 - INFO - root -   train_accuracy = 0.6879
2023/09/27 12:11:57 - INFO - root -   Epoch: [63/200][0/346], lr: 0.00000019 	 loss = 0.2059(0.2059)
2023/09/27 12:12:41 - INFO - root -   Epoch: [63/200][20/346], lr: 0.00000019 	 loss = 0.2299(0.4369)
2023/09/27 12:13:42 - INFO - root -   Epoch: [63/200][40/346], lr: 0.00000019 	 loss = 0.2689(0.4900)
2023/09/27 12:14:25 - INFO - root -   Epoch: [63/200][60/346], lr: 0.00000019 	 loss = 0.1965(0.4617)
2023/09/27 12:15:26 - INFO - root -   Epoch: [63/200][80/346], lr: 0.00000019 	 loss = 1.3062(0.4956)
2023/09/27 12:16:09 - INFO - root -   Epoch: [63/200][100/346], lr: 0.00000019 	 loss = 1.0610(0.5229)
2023/09/27 12:17:10 - INFO - root -   Epoch: [63/200][120/346], lr: 0.00000019 	 loss = 0.2776(0.5594)
2023/09/27 12:17:54 - INFO - root -   Epoch: [63/200][140/346], lr: 0.00000019 	 loss = 1.1637(0.5659)
2023/09/27 12:18:54 - INFO - root -   Epoch: [63/200][160/346], lr: 0.00000019 	 loss = 0.0559(0.5639)
2023/09/27 12:19:38 - INFO - root -   Epoch: [63/200][180/346], lr: 0.00000019 	 loss = 0.4181(0.5749)
2023/09/27 12:20:38 - INFO - root -   Epoch: [63/200][200/346], lr: 0.00000019 	 loss = 0.2164(0.5662)
2023/09/27 12:21:22 - INFO - root -   Epoch: [63/200][220/346], lr: 0.00000019 	 loss = 0.1360(0.5769)
2023/09/27 12:22:22 - INFO - root -   Epoch: [63/200][240/346], lr: 0.00000019 	 loss = 0.2435(0.5757)
2023/09/27 12:23:05 - INFO - root -   Epoch: [63/200][260/346], lr: 0.00000019 	 loss = 0.4978(0.5746)
2023/09/27 12:24:06 - INFO - root -   Epoch: [63/200][280/346], lr: 0.00000019 	 loss = 0.4796(0.5851)
2023/09/27 12:24:49 - INFO - root -   Epoch: [63/200][300/346], lr: 0.00000019 	 loss = 0.3505(0.5859)
2023/09/27 12:25:49 - INFO - root -   Epoch: [63/200][320/346], lr: 0.00000019 	 loss = 0.2856(0.5857)
2023/09/27 12:26:32 - INFO - root -   Epoch: [63/200][340/346], lr: 0.00000019 	 loss = 0.7098(0.5828)
2023/09/27 12:26:36 - INFO - root -   Epoch: [63/200] 	 loss = 0.5824
2023/09/27 12:26:36 - INFO - root -   train_accuracy = 0.7168
2023/09/27 12:26:58 - INFO - root -   Epoch: [64/200][0/346], lr: 0.00000019 	 loss = 0.2951(0.2951)
2023/09/27 12:27:41 - INFO - root -   Epoch: [64/200][20/346], lr: 0.00000019 	 loss = 0.2199(0.4559)
2023/09/27 12:28:42 - INFO - root -   Epoch: [64/200][40/346], lr: 0.00000019 	 loss = 0.1646(0.5078)
2023/09/27 12:29:26 - INFO - root -   Epoch: [64/200][60/346], lr: 0.00000019 	 loss = 0.5752(0.4819)
2023/09/27 12:30:27 - INFO - root -   Epoch: [64/200][80/346], lr: 0.00000019 	 loss = 0.7647(0.5099)
2023/09/27 12:31:10 - INFO - root -   Epoch: [64/200][100/346], lr: 0.00000019 	 loss = 0.7408(0.5119)
2023/09/27 12:32:11 - INFO - root -   Epoch: [64/200][120/346], lr: 0.00000019 	 loss = 0.5541(0.5433)
2023/09/27 12:32:54 - INFO - root -   Epoch: [64/200][140/346], lr: 0.00000019 	 loss = 0.6476(0.5384)
2023/09/27 12:33:55 - INFO - root -   Epoch: [64/200][160/346], lr: 0.00000019 	 loss = 0.1130(0.5478)
2023/09/27 12:34:39 - INFO - root -   Epoch: [64/200][180/346], lr: 0.00000019 	 loss = 0.5680(0.5505)
2023/09/27 12:35:39 - INFO - root -   Epoch: [64/200][200/346], lr: 0.00000019 	 loss = 0.2594(0.5460)
2023/09/27 12:36:23 - INFO - root -   Epoch: [64/200][220/346], lr: 0.00000019 	 loss = 0.3196(0.5475)
2023/09/27 12:37:23 - INFO - root -   Epoch: [64/200][240/346], lr: 0.00000019 	 loss = 0.0832(0.5475)
2023/09/27 12:38:07 - INFO - root -   Epoch: [64/200][260/346], lr: 0.00000019 	 loss = 0.6367(0.5493)
2023/09/27 12:39:07 - INFO - root -   Epoch: [64/200][280/346], lr: 0.00000019 	 loss = 0.4491(0.5705)
2023/09/27 12:39:51 - INFO - root -   Epoch: [64/200][300/346], lr: 0.00000019 	 loss = 0.3946(0.5663)
2023/09/27 12:40:51 - INFO - root -   Epoch: [64/200][320/346], lr: 0.00000019 	 loss = 0.1396(0.5678)
2023/09/27 12:41:33 - INFO - root -   Epoch: [64/200][340/346], lr: 0.00000019 	 loss = 1.3725(0.5647)
2023/09/27 12:41:38 - INFO - root -   Epoch: [64/200] 	 loss = 0.5629
2023/09/27 12:47:34 - INFO - root -   precision = 0.6207
2023/09/27 12:47:34 - INFO - root -   eval_loss = 0.7125
2023/09/27 12:47:35 - INFO - root -   train_accuracy = 0.7095
2023/09/27 12:47:57 - INFO - root -   Epoch: [65/200][0/346], lr: 0.00000019 	 loss = 0.2432(0.2432)
2023/09/27 12:48:41 - INFO - root -   Epoch: [65/200][20/346], lr: 0.00000019 	 loss = 0.3088(0.4128)
2023/09/27 12:49:43 - INFO - root -   Epoch: [65/200][40/346], lr: 0.00000019 	 loss = 0.2341(0.4874)
2023/09/27 12:50:26 - INFO - root -   Epoch: [65/200][60/346], lr: 0.00000019 	 loss = 0.5069(0.4618)
2023/09/27 12:51:28 - INFO - root -   Epoch: [65/200][80/346], lr: 0.00000019 	 loss = 0.9201(0.4934)
2023/09/27 12:52:12 - INFO - root -   Epoch: [65/200][100/346], lr: 0.00000019 	 loss = 0.8212(0.5262)
2023/09/27 12:53:13 - INFO - root -   Epoch: [65/200][120/346], lr: 0.00000019 	 loss = 0.5576(0.5481)
2023/09/27 12:53:57 - INFO - root -   Epoch: [65/200][140/346], lr: 0.00000019 	 loss = 0.7163(0.5451)
2023/09/27 12:54:58 - INFO - root -   Epoch: [65/200][160/346], lr: 0.00000019 	 loss = 0.2613(0.5494)
2023/09/27 12:55:42 - INFO - root -   Epoch: [65/200][180/346], lr: 0.00000019 	 loss = 0.6497(0.5538)
2023/09/27 12:56:43 - INFO - root -   Epoch: [65/200][200/346], lr: 0.00000019 	 loss = 0.1461(0.5485)
2023/09/27 12:57:27 - INFO - root -   Epoch: [65/200][220/346], lr: 0.00000019 	 loss = 0.2083(0.5612)
2023/09/27 12:58:28 - INFO - root -   Epoch: [65/200][240/346], lr: 0.00000019 	 loss = 0.4084(0.5543)
2023/09/27 12:59:12 - INFO - root -   Epoch: [65/200][260/346], lr: 0.00000019 	 loss = 0.8633(0.5505)
2023/09/27 13:00:13 - INFO - root -   Epoch: [65/200][280/346], lr: 0.00000019 	 loss = 0.5826(0.5651)
2023/09/27 13:00:57 - INFO - root -   Epoch: [65/200][300/346], lr: 0.00000019 	 loss = 0.2711(0.5621)
2023/09/27 13:01:58 - INFO - root -   Epoch: [65/200][320/346], lr: 0.00000019 	 loss = 0.1489(0.5606)
2023/09/27 13:02:40 - INFO - root -   Epoch: [65/200][340/346], lr: 0.00000019 	 loss = 1.2012(0.5589)
2023/09/27 13:02:44 - INFO - root -   Epoch: [65/200] 	 loss = 0.5587
2023/09/27 13:02:44 - INFO - root -   train_accuracy = 0.7413
2023/09/27 13:03:05 - INFO - root -   Epoch: [66/200][0/346], lr: 0.00000019 	 loss = 0.7011(0.7011)
2023/09/27 13:03:49 - INFO - root -   Epoch: [66/200][20/346], lr: 0.00000019 	 loss = 0.6662(0.4386)
2023/09/27 13:04:50 - INFO - root -   Epoch: [66/200][40/346], lr: 0.00000019 	 loss = 0.3117(0.4840)
2023/09/27 13:05:33 - INFO - root -   Epoch: [66/200][60/346], lr: 0.00000019 	 loss = 0.5648(0.4539)
2023/09/27 13:06:35 - INFO - root -   Epoch: [66/200][80/346], lr: 0.00000019 	 loss = 0.7242(0.5042)
2023/09/27 13:07:18 - INFO - root -   Epoch: [66/200][100/346], lr: 0.00000019 	 loss = 1.1075(0.5286)
2023/09/27 13:08:19 - INFO - root -   Epoch: [66/200][120/346], lr: 0.00000019 	 loss = 0.6678(0.5560)
2023/09/27 13:09:03 - INFO - root -   Epoch: [66/200][140/346], lr: 0.00000019 	 loss = 0.9849(0.5463)
2023/09/27 13:10:04 - INFO - root -   Epoch: [66/200][160/346], lr: 0.00000019 	 loss = 0.2740(0.5569)
2023/09/27 13:10:48 - INFO - root -   Epoch: [66/200][180/346], lr: 0.00000019 	 loss = 0.8423(0.5690)
2023/09/27 13:11:48 - INFO - root -   Epoch: [66/200][200/346], lr: 0.00000019 	 loss = 0.1594(0.5597)
2023/09/27 13:12:32 - INFO - root -   Epoch: [66/200][220/346], lr: 0.00000019 	 loss = 0.1566(0.5600)
2023/09/27 13:13:32 - INFO - root -   Epoch: [66/200][240/346], lr: 0.00000019 	 loss = 0.1317(0.5599)
2023/09/27 13:14:17 - INFO - root -   Epoch: [66/200][260/346], lr: 0.00000019 	 loss = 0.9304(0.5605)
2023/09/27 13:15:17 - INFO - root -   Epoch: [66/200][280/346], lr: 0.00000019 	 loss = 0.1993(0.5718)
2023/09/27 13:16:02 - INFO - root -   Epoch: [66/200][300/346], lr: 0.00000019 	 loss = 0.3636(0.5716)
2023/09/27 13:17:01 - INFO - root -   Epoch: [66/200][320/346], lr: 0.00000019 	 loss = 0.3513(0.5712)
2023/09/27 13:17:44 - INFO - root -   Epoch: [66/200][340/346], lr: 0.00000019 	 loss = 1.3423(0.5719)
2023/09/27 13:17:48 - INFO - root -   Epoch: [66/200] 	 loss = 0.5715
2023/09/27 13:17:48 - INFO - root -   train_accuracy = 0.7168
2023/09/27 13:18:10 - INFO - root -   Epoch: [67/200][0/346], lr: 0.00000020 	 loss = 0.1529(0.1529)
2023/09/27 13:18:53 - INFO - root -   Epoch: [67/200][20/346], lr: 0.00000020 	 loss = 0.1893(0.3857)
2023/09/27 13:19:55 - INFO - root -   Epoch: [67/200][40/346], lr: 0.00000020 	 loss = 0.1805(0.4915)
2023/09/27 13:20:38 - INFO - root -   Epoch: [67/200][60/346], lr: 0.00000020 	 loss = 0.1820(0.4420)
2023/09/27 13:21:41 - INFO - root -   Epoch: [67/200][80/346], lr: 0.00000020 	 loss = 0.7352(0.5094)
2023/09/27 13:22:25 - INFO - root -   Epoch: [67/200][100/346], lr: 0.00000020 	 loss = 0.5994(0.5412)
2023/09/27 13:23:26 - INFO - root -   Epoch: [67/200][120/346], lr: 0.00000020 	 loss = 0.6766(0.5628)
2023/09/27 13:24:10 - INFO - root -   Epoch: [67/200][140/346], lr: 0.00000020 	 loss = 1.2043(0.5685)
2023/09/27 13:25:15 - INFO - root -   Epoch: [67/200][160/346], lr: 0.00000020 	 loss = 0.3280(0.5782)
2023/09/27 13:25:58 - INFO - root -   Epoch: [67/200][180/346], lr: 0.00000020 	 loss = 0.4871(0.5747)
2023/09/27 13:27:00 - INFO - root -   Epoch: [67/200][200/346], lr: 0.00000020 	 loss = 0.2537(0.5663)
2023/09/27 13:27:43 - INFO - root -   Epoch: [67/200][220/346], lr: 0.00000020 	 loss = 0.1034(0.5758)
2023/09/27 13:28:45 - INFO - root -   Epoch: [67/200][240/346], lr: 0.00000020 	 loss = 0.1935(0.5721)
2023/09/27 13:29:28 - INFO - root -   Epoch: [67/200][260/346], lr: 0.00000020 	 loss = 0.6412(0.5643)
2023/09/27 13:30:29 - INFO - root -   Epoch: [67/200][280/346], lr: 0.00000020 	 loss = 0.2056(0.5763)
2023/09/27 13:31:13 - INFO - root -   Epoch: [67/200][300/346], lr: 0.00000020 	 loss = 0.1808(0.5764)
2023/09/27 13:32:14 - INFO - root -   Epoch: [67/200][320/346], lr: 0.00000020 	 loss = 0.1875(0.5760)
2023/09/27 13:32:57 - INFO - root -   Epoch: [67/200][340/346], lr: 0.00000020 	 loss = 1.4661(0.5745)
2023/09/27 13:33:01 - INFO - root -   Epoch: [67/200] 	 loss = 0.5719
2023/09/27 13:33:01 - INFO - root -   train_accuracy = 0.6994
2023/09/27 13:33:23 - INFO - root -   Epoch: [68/200][0/346], lr: 0.00000020 	 loss = 0.2446(0.2446)
2023/09/27 13:34:07 - INFO - root -   Epoch: [68/200][20/346], lr: 0.00000020 	 loss = 0.1419(0.4606)
2023/09/27 13:35:08 - INFO - root -   Epoch: [68/200][40/346], lr: 0.00000020 	 loss = 0.2905(0.4972)
2023/09/27 13:35:51 - INFO - root -   Epoch: [68/200][60/346], lr: 0.00000020 	 loss = 0.5286(0.4609)
2023/09/27 13:36:53 - INFO - root -   Epoch: [68/200][80/346], lr: 0.00000020 	 loss = 0.6844(0.5203)
2023/09/27 13:37:36 - INFO - root -   Epoch: [68/200][100/346], lr: 0.00000020 	 loss = 0.6140(0.5318)
2023/09/27 13:38:37 - INFO - root -   Epoch: [68/200][120/346], lr: 0.00000020 	 loss = 0.7484(0.5586)
2023/09/27 13:39:20 - INFO - root -   Epoch: [68/200][140/346], lr: 0.00000020 	 loss = 1.0909(0.5486)
2023/09/27 13:40:21 - INFO - root -   Epoch: [68/200][160/346], lr: 0.00000020 	 loss = 0.2177(0.5521)
2023/09/27 13:41:05 - INFO - root -   Epoch: [68/200][180/346], lr: 0.00000020 	 loss = 0.7999(0.5624)
2023/09/27 13:42:06 - INFO - root -   Epoch: [68/200][200/346], lr: 0.00000020 	 loss = 0.2083(0.5535)
2023/09/27 13:42:49 - INFO - root -   Epoch: [68/200][220/346], lr: 0.00000020 	 loss = 0.1059(0.5636)
2023/09/27 13:43:50 - INFO - root -   Epoch: [68/200][240/346], lr: 0.00000020 	 loss = 0.2956(0.5650)
2023/09/27 13:44:34 - INFO - root -   Epoch: [68/200][260/346], lr: 0.00000020 	 loss = 0.2994(0.5609)
2023/09/27 13:45:34 - INFO - root -   Epoch: [68/200][280/346], lr: 0.00000020 	 loss = 0.5336(0.5701)
2023/09/27 13:46:18 - INFO - root -   Epoch: [68/200][300/346], lr: 0.00000020 	 loss = 0.1912(0.5654)
2023/09/27 13:47:18 - INFO - root -   Epoch: [68/200][320/346], lr: 0.00000020 	 loss = 0.3458(0.5639)
2023/09/27 13:48:01 - INFO - root -   Epoch: [68/200][340/346], lr: 0.00000020 	 loss = 1.4804(0.5633)
2023/09/27 13:48:05 - INFO - root -   Epoch: [68/200] 	 loss = 0.5612
2023/09/27 13:48:05 - INFO - root -   train_accuracy = 0.7327
2023/09/27 13:48:27 - INFO - root -   Epoch: [69/200][0/346], lr: 0.00000020 	 loss = 0.2409(0.2409)
2023/09/27 13:49:10 - INFO - root -   Epoch: [69/200][20/346], lr: 0.00000020 	 loss = 0.3958(0.4951)
2023/09/27 13:50:12 - INFO - root -   Epoch: [69/200][40/346], lr: 0.00000020 	 loss = 0.2666(0.5252)
2023/09/27 13:50:56 - INFO - root -   Epoch: [69/200][60/346], lr: 0.00000020 	 loss = 0.2063(0.4847)
2023/09/27 13:51:57 - INFO - root -   Epoch: [69/200][80/346], lr: 0.00000020 	 loss = 0.9192(0.5116)
2023/09/27 13:52:41 - INFO - root -   Epoch: [69/200][100/346], lr: 0.00000020 	 loss = 0.4050(0.5175)
2023/09/27 13:53:42 - INFO - root -   Epoch: [69/200][120/346], lr: 0.00000020 	 loss = 0.8457(0.5619)
2023/09/27 13:54:25 - INFO - root -   Epoch: [69/200][140/346], lr: 0.00000020 	 loss = 0.7433(0.5634)
2023/09/27 13:55:26 - INFO - root -   Epoch: [69/200][160/346], lr: 0.00000020 	 loss = 0.0874(0.5679)
2023/09/27 13:56:09 - INFO - root -   Epoch: [69/200][180/346], lr: 0.00000020 	 loss = 0.5360(0.5718)
2023/09/27 13:57:10 - INFO - root -   Epoch: [69/200][200/346], lr: 0.00000020 	 loss = 0.2536(0.5741)
2023/09/27 13:57:54 - INFO - root -   Epoch: [69/200][220/346], lr: 0.00000020 	 loss = 0.3245(0.5841)
2023/09/27 13:58:55 - INFO - root -   Epoch: [69/200][240/346], lr: 0.00000020 	 loss = 0.3456(0.5863)
2023/09/27 13:59:38 - INFO - root -   Epoch: [69/200][260/346], lr: 0.00000020 	 loss = 0.1782(0.5767)
2023/09/27 14:00:39 - INFO - root -   Epoch: [69/200][280/346], lr: 0.00000020 	 loss = 0.2978(0.5863)
2023/09/27 14:01:23 - INFO - root -   Epoch: [69/200][300/346], lr: 0.00000020 	 loss = 0.3941(0.5888)
2023/09/27 14:02:23 - INFO - root -   Epoch: [69/200][320/346], lr: 0.00000020 	 loss = 0.3300(0.5872)
2023/09/27 14:03:06 - INFO - root -   Epoch: [69/200][340/346], lr: 0.00000020 	 loss = 1.1401(0.5858)
2023/09/27 14:03:10 - INFO - root -   Epoch: [69/200] 	 loss = 0.5850
