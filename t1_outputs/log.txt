2023/09/14 16:50:37 - INFO - root -   Num train examples = 693
2023/09/14 16:50:37 - INFO - root -   Num val examples = 174
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:37 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/09/14 16:50:38 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/09/14 16:50:38 - INFO - root -   backend = nccl
2023/09/14 16:50:38 - INFO - root -   batch_size = 2
2023/09/14 16:50:38 - INFO - root -   dropout = 0.5
2023/09/14 16:50:38 - INFO - root -   epochs = 500
2023/09/14 16:50:38 - INFO - root -   eval_freq = 5
2023/09/14 16:50:38 - INFO - root -   focal_loss = False
2023/09/14 16:50:38 - INFO - root -   input_size = 224
2023/09/14 16:50:38 - INFO - root -   is_pretrained = False
2023/09/14 16:50:38 - INFO - root -   label_smooth = False
2023/09/14 16:50:38 - INFO - root -   local_rank = -1
2023/09/14 16:50:38 - INFO - root -   lr = 0.0001
2023/09/14 16:50:38 - INFO - root -   lr_decay_rate = 0.1
2023/09/14 16:50:38 - INFO - root -   lr_steps = [50, 100]
2023/09/14 16:50:38 - INFO - root -   lr_type = cosine
2023/09/14 16:50:38 - INFO - root -   model_depth = 34
2023/09/14 16:50:38 - INFO - root -   model_name = resnet50
2023/09/14 16:50:38 - INFO - root -   momentum = 0.9
2023/09/14 16:50:38 - INFO - root -   num_classes = 4
2023/09/14 16:50:38 - INFO - root -   output = ./t1_outputs
2023/09/14 16:50:38 - INFO - root -   print_freq = 20
2023/09/14 16:50:38 - INFO - root -   resume = 
2023/09/14 16:50:38 - INFO - root -   start_epoch = 0
2023/09/14 16:50:38 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/09/14 16:50:38 - INFO - root -   tune_from = 
2023/09/14 16:50:38 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/09/14 16:50:38 - INFO - root -   warmup_epoch = 20
2023/09/14 16:50:38 - INFO - root -   warmup_multiplier = 100
2023/09/14 16:50:38 - INFO - root -   weight_decay = 0.0005
2023/09/14 16:50:38 - INFO - root -   workers = 8
2023/09/14 16:50:48 - INFO - root -   Epoch: [0/500][0/346], lr: 0.00000100 	 loss = 2.2072(2.2072)
2023/09/14 16:51:04 - INFO - root -   Epoch: [0/500][20/346], lr: 0.00000100 	 loss = 1.7742(1.4143)
2023/09/14 16:51:25 - INFO - root -   Epoch: [0/500][40/346], lr: 0.00000100 	 loss = 1.6568(1.3818)
2023/09/14 16:51:43 - INFO - root -   Epoch: [0/500][60/346], lr: 0.00000100 	 loss = 0.7314(1.3261)
2023/09/14 16:52:03 - INFO - root -   Epoch: [0/500][80/346], lr: 0.00000100 	 loss = 0.8853(1.3591)
2023/09/14 16:52:19 - INFO - root -   Epoch: [0/500][100/346], lr: 0.00000100 	 loss = 0.9016(1.3653)
2023/09/14 16:52:39 - INFO - root -   Epoch: [0/500][120/346], lr: 0.00000100 	 loss = 1.2836(1.3692)
2023/09/14 16:52:56 - INFO - root -   Epoch: [0/500][140/346], lr: 0.00000100 	 loss = 1.7774(1.3515)
2023/09/14 16:53:17 - INFO - root -   Epoch: [0/500][160/346], lr: 0.00000100 	 loss = 0.4920(1.3270)
2023/09/14 16:53:34 - INFO - root -   Epoch: [0/500][180/346], lr: 0.00000100 	 loss = 1.1154(1.3079)
2023/09/14 16:53:57 - INFO - root -   Epoch: [0/500][200/346], lr: 0.00000100 	 loss = 0.7905(1.2898)
2023/09/14 16:54:12 - INFO - root -   Epoch: [0/500][220/346], lr: 0.00000100 	 loss = 1.6295(1.2994)
2023/09/14 16:54:30 - INFO - root -   Epoch: [0/500][240/346], lr: 0.00000100 	 loss = 0.8778(1.2933)
2023/09/14 16:54:48 - INFO - root -   Epoch: [0/500][260/346], lr: 0.00000100 	 loss = 0.5084(1.2815)
2023/09/14 16:55:06 - INFO - root -   Epoch: [0/500][280/346], lr: 0.00000100 	 loss = 1.6416(1.2731)
2023/09/14 16:55:28 - INFO - root -   Epoch: [0/500][300/346], lr: 0.00000100 	 loss = 2.0356(1.2768)
2023/09/14 16:55:47 - INFO - root -   Epoch: [0/500][320/346], lr: 0.00000100 	 loss = 0.9305(1.2789)
2023/09/14 16:56:05 - INFO - root -   Epoch: [0/500][340/346], lr: 0.00000100 	 loss = 2.7343(1.2739)
2023/09/14 16:56:07 - INFO - root -   Epoch: [0/500] 	 loss = 1.2676
2023/09/14 16:56:07 - INFO - root -   train_accuracy = 0.4292
2023/09/14 16:56:16 - INFO - root -   Epoch: [1/500][0/346], lr: 0.00000101 	 loss = 1.8592(1.8592)
2023/09/14 16:56:32 - INFO - root -   Epoch: [1/500][20/346], lr: 0.00000101 	 loss = 2.0590(1.3642)
2023/09/14 16:56:53 - INFO - root -   Epoch: [1/500][40/346], lr: 0.00000101 	 loss = 0.5305(1.3139)
2023/09/14 16:57:10 - INFO - root -   Epoch: [1/500][60/346], lr: 0.00000101 	 loss = 0.7571(1.3410)
2023/09/14 16:57:31 - INFO - root -   Epoch: [1/500][80/346], lr: 0.00000101 	 loss = 1.0330(1.3344)
2023/09/14 16:57:47 - INFO - root -   Epoch: [1/500][100/346], lr: 0.00000101 	 loss = 1.8060(1.3502)
2023/09/14 16:58:08 - INFO - root -   Epoch: [1/500][120/346], lr: 0.00000101 	 loss = 0.8783(1.3523)
2023/09/14 16:58:25 - INFO - root -   Epoch: [1/500][140/346], lr: 0.00000101 	 loss = 2.2573(1.3351)
2023/09/14 16:58:46 - INFO - root -   Epoch: [1/500][160/346], lr: 0.00000101 	 loss = 0.2070(1.2900)
2023/09/14 16:59:03 - INFO - root -   Epoch: [1/500][180/346], lr: 0.00000101 	 loss = 1.6079(1.2892)
2023/09/14 16:59:23 - INFO - root -   Epoch: [1/500][200/346], lr: 0.00000101 	 loss = 0.9671(1.2812)
2023/09/14 16:59:40 - INFO - root -   Epoch: [1/500][220/346], lr: 0.00000101 	 loss = 2.6781(1.2836)
2023/09/14 17:00:01 - INFO - root -   Epoch: [1/500][240/346], lr: 0.00000101 	 loss = 0.8592(1.2791)
2023/09/14 17:00:17 - INFO - root -   Epoch: [1/500][260/346], lr: 0.00000101 	 loss = 0.9685(1.2799)
2023/09/14 17:00:38 - INFO - root -   Epoch: [1/500][280/346], lr: 0.00000101 	 loss = 1.8681(1.2638)
2023/09/14 17:00:55 - INFO - root -   Epoch: [1/500][300/346], lr: 0.00000101 	 loss = 2.0320(1.2656)
2023/09/14 17:01:16 - INFO - root -   Epoch: [1/500][320/346], lr: 0.00000101 	 loss = 0.4250(1.2700)
2023/09/14 17:01:32 - INFO - root -   Epoch: [1/500][340/346], lr: 0.00000101 	 loss = 1.4419(1.2670)
2023/09/14 17:01:34 - INFO - root -   Epoch: [1/500] 	 loss = 1.2618
2023/09/14 17:01:34 - INFO - root -   train_accuracy = 0.4812
2023/09/14 17:01:43 - INFO - root -   Epoch: [2/500][0/346], lr: 0.00000103 	 loss = 2.0236(2.0236)
2023/09/14 17:01:59 - INFO - root -   Epoch: [2/500][20/346], lr: 0.00000103 	 loss = 2.9211(1.2469)
2023/09/14 17:02:20 - INFO - root -   Epoch: [2/500][40/346], lr: 0.00000103 	 loss = 0.8737(1.1883)
2023/09/14 17:02:37 - INFO - root -   Epoch: [2/500][60/346], lr: 0.00000103 	 loss = 1.0023(1.2180)
2023/09/14 17:02:58 - INFO - root -   Epoch: [2/500][80/346], lr: 0.00000103 	 loss = 1.1534(1.2733)
2023/09/14 17:03:14 - INFO - root -   Epoch: [2/500][100/346], lr: 0.00000103 	 loss = 1.1074(1.2882)
2023/09/14 17:03:36 - INFO - root -   Epoch: [2/500][120/346], lr: 0.00000103 	 loss = 1.1787(1.2814)
2023/09/14 17:03:52 - INFO - root -   Epoch: [2/500][140/346], lr: 0.00000103 	 loss = 0.6644(1.2811)
2023/09/14 17:04:13 - INFO - root -   Epoch: [2/500][160/346], lr: 0.00000103 	 loss = 0.5454(1.2451)
2023/09/14 17:04:30 - INFO - root -   Epoch: [2/500][180/346], lr: 0.00000103 	 loss = 1.3037(1.2309)
2023/09/14 17:04:51 - INFO - root -   Epoch: [2/500][200/346], lr: 0.00000103 	 loss = 0.5997(1.2341)
2023/09/14 17:05:07 - INFO - root -   Epoch: [2/500][220/346], lr: 0.00000103 	 loss = 2.3377(1.2463)
2023/09/14 17:05:28 - INFO - root -   Epoch: [2/500][240/346], lr: 0.00000103 	 loss = 1.2887(1.2484)
2023/09/14 17:05:45 - INFO - root -   Epoch: [2/500][260/346], lr: 0.00000103 	 loss = 0.5287(1.2536)
2023/09/14 17:06:06 - INFO - root -   Epoch: [2/500][280/346], lr: 0.00000103 	 loss = 2.2096(1.2434)
2023/09/14 17:06:23 - INFO - root -   Epoch: [2/500][300/346], lr: 0.00000103 	 loss = 1.5515(1.2345)
2023/09/14 17:06:44 - INFO - root -   Epoch: [2/500][320/346], lr: 0.00000103 	 loss = 0.3779(1.2332)
2023/09/14 17:07:00 - INFO - root -   Epoch: [2/500][340/346], lr: 0.00000103 	 loss = 2.4834(1.2344)
2023/09/14 17:07:02 - INFO - root -   Epoch: [2/500] 	 loss = 1.2327
2023/09/14 17:07:02 - INFO - root -   train_accuracy = 0.4899
2023/09/14 17:07:10 - INFO - root -   Epoch: [3/500][0/346], lr: 0.00000104 	 loss = 2.0781(2.0781)
2023/09/14 17:07:27 - INFO - root -   Epoch: [3/500][20/346], lr: 0.00000104 	 loss = 2.4326(1.2579)
2023/09/14 17:07:48 - INFO - root -   Epoch: [3/500][40/346], lr: 0.00000104 	 loss = 1.0212(1.2269)
2023/09/14 17:08:05 - INFO - root -   Epoch: [3/500][60/346], lr: 0.00000104 	 loss = 1.0733(1.2458)
2023/09/14 17:08:26 - INFO - root -   Epoch: [3/500][80/346], lr: 0.00000104 	 loss = 0.8674(1.2811)
2023/09/14 17:08:42 - INFO - root -   Epoch: [3/500][100/346], lr: 0.00000104 	 loss = 1.0837(1.3001)
2023/09/14 17:09:03 - INFO - root -   Epoch: [3/500][120/346], lr: 0.00000104 	 loss = 0.9477(1.2993)
2023/09/14 17:09:20 - INFO - root -   Epoch: [3/500][140/346], lr: 0.00000104 	 loss = 1.5422(1.3102)
2023/09/14 17:09:41 - INFO - root -   Epoch: [3/500][160/346], lr: 0.00000104 	 loss = 0.8156(1.2761)
2023/09/14 17:09:57 - INFO - root -   Epoch: [3/500][180/346], lr: 0.00000104 	 loss = 1.1740(1.2599)
2023/09/14 17:10:18 - INFO - root -   Epoch: [3/500][200/346], lr: 0.00000104 	 loss = 1.0886(1.2390)
2023/09/14 17:10:34 - INFO - root -   Epoch: [3/500][220/346], lr: 0.00000104 	 loss = 1.8079(1.2475)
2023/09/14 17:10:55 - INFO - root -   Epoch: [3/500][240/346], lr: 0.00000104 	 loss = 1.1626(1.2504)
2023/09/14 17:11:11 - INFO - root -   Epoch: [3/500][260/346], lr: 0.00000104 	 loss = 0.7905(1.2532)
2023/09/14 17:11:32 - INFO - root -   Epoch: [3/500][280/346], lr: 0.00000104 	 loss = 1.5931(1.2462)
2023/09/14 17:11:49 - INFO - root -   Epoch: [3/500][300/346], lr: 0.00000104 	 loss = 1.9030(1.2454)
2023/09/14 17:12:09 - INFO - root -   Epoch: [3/500][320/346], lr: 0.00000104 	 loss = 0.3320(1.2572)
2023/09/14 17:12:26 - INFO - root -   Epoch: [3/500][340/346], lr: 0.00000104 	 loss = 1.6004(1.2554)
2023/09/14 17:12:28 - INFO - root -   Epoch: [3/500] 	 loss = 1.2518
2023/09/14 17:12:28 - INFO - root -   train_accuracy = 0.4913
2023/09/14 17:12:36 - INFO - root -   Epoch: [4/500][0/346], lr: 0.00000106 	 loss = 2.4166(2.4166)
2023/09/14 17:12:53 - INFO - root -   Epoch: [4/500][20/346], lr: 0.00000106 	 loss = 1.9892(1.3694)
2023/09/14 17:13:14 - INFO - root -   Epoch: [4/500][40/346], lr: 0.00000106 	 loss = 0.9980(1.2450)
2023/09/14 17:13:31 - INFO - root -   Epoch: [4/500][60/346], lr: 0.00000106 	 loss = 1.4843(1.2427)
2023/09/14 17:13:52 - INFO - root -   Epoch: [4/500][80/346], lr: 0.00000106 	 loss = 1.3272(1.2724)
2023/09/14 17:14:09 - INFO - root -   Epoch: [4/500][100/346], lr: 0.00000106 	 loss = 0.6234(1.2704)
2023/09/14 17:14:30 - INFO - root -   Epoch: [4/500][120/346], lr: 0.00000106 	 loss = 1.1729(1.2660)
2023/09/14 17:14:46 - INFO - root -   Epoch: [4/500][140/346], lr: 0.00000106 	 loss = 2.0057(1.2592)
2023/09/14 17:15:07 - INFO - root -   Epoch: [4/500][160/346], lr: 0.00000106 	 loss = 0.3552(1.2362)
2023/09/14 17:15:24 - INFO - root -   Epoch: [4/500][180/346], lr: 0.00000106 	 loss = 1.0165(1.2216)
2023/09/14 17:15:45 - INFO - root -   Epoch: [4/500][200/346], lr: 0.00000106 	 loss = 0.7075(1.2262)
2023/09/14 17:16:02 - INFO - root -   Epoch: [4/500][220/346], lr: 0.00000106 	 loss = 1.9040(1.2264)
2023/09/14 17:16:23 - INFO - root -   Epoch: [4/500][240/346], lr: 0.00000106 	 loss = 0.9903(1.2352)
2023/09/14 17:16:39 - INFO - root -   Epoch: [4/500][260/346], lr: 0.00000106 	 loss = 0.4470(1.2335)
2023/09/14 17:17:00 - INFO - root -   Epoch: [4/500][280/346], lr: 0.00000106 	 loss = 1.5616(1.2203)
2023/09/14 17:17:17 - INFO - root -   Epoch: [4/500][300/346], lr: 0.00000106 	 loss = 1.8155(1.2218)
2023/09/14 17:17:38 - INFO - root -   Epoch: [4/500][320/346], lr: 0.00000106 	 loss = 0.8724(1.2265)
2023/09/14 17:17:54 - INFO - root -   Epoch: [4/500][340/346], lr: 0.00000106 	 loss = 1.5433(1.2284)
2023/09/14 17:17:56 - INFO - root -   Epoch: [4/500] 	 loss = 1.2289
2023/09/14 17:19:19 - INFO - root -   precision = 0.5345
2023/09/14 17:19:19 - INFO - root -   eval_loss = 1.1732
2023/09/14 17:19:20 - INFO - root -   train_accuracy = 0.5087
2023/09/14 17:19:28 - INFO - root -   Epoch: [5/500][0/346], lr: 0.00000107 	 loss = 2.4247(2.4247)
2023/09/14 17:19:45 - INFO - root -   Epoch: [5/500][20/346], lr: 0.00000107 	 loss = 1.2113(1.2765)
2023/09/14 17:20:06 - INFO - root -   Epoch: [5/500][40/346], lr: 0.00000107 	 loss = 0.6443(1.2421)
2023/09/14 17:20:22 - INFO - root -   Epoch: [5/500][60/346], lr: 0.00000107 	 loss = 0.7302(1.2431)
2023/09/14 17:20:43 - INFO - root -   Epoch: [5/500][80/346], lr: 0.00000107 	 loss = 1.0928(1.2568)
2023/09/14 17:21:00 - INFO - root -   Epoch: [5/500][100/346], lr: 0.00000107 	 loss = 0.8116(1.2635)
2023/09/14 17:21:21 - INFO - root -   Epoch: [5/500][120/346], lr: 0.00000107 	 loss = 1.0172(1.2760)
2023/09/14 17:21:37 - INFO - root -   Epoch: [5/500][140/346], lr: 0.00000107 	 loss = 2.3412(1.2723)
2023/09/14 17:21:58 - INFO - root -   Epoch: [5/500][160/346], lr: 0.00000107 	 loss = 0.4565(1.2556)
2023/09/14 17:22:15 - INFO - root -   Epoch: [5/500][180/346], lr: 0.00000107 	 loss = 0.8002(1.2362)
2023/09/14 17:22:36 - INFO - root -   Epoch: [5/500][200/346], lr: 0.00000107 	 loss = 0.6213(1.2247)
2023/09/14 17:22:52 - INFO - root -   Epoch: [5/500][220/346], lr: 0.00000107 	 loss = 2.2230(1.2311)
2023/09/14 17:23:13 - INFO - root -   Epoch: [5/500][240/346], lr: 0.00000107 	 loss = 0.6491(1.2331)
2023/09/14 17:23:30 - INFO - root -   Epoch: [5/500][260/346], lr: 0.00000107 	 loss = 0.8764(1.2332)
2023/09/14 17:23:50 - INFO - root -   Epoch: [5/500][280/346], lr: 0.00000107 	 loss = 2.1326(1.2239)
2023/09/14 17:24:07 - INFO - root -   Epoch: [5/500][300/346], lr: 0.00000107 	 loss = 2.0881(1.2240)
2023/09/14 17:24:28 - INFO - root -   Epoch: [5/500][320/346], lr: 0.00000107 	 loss = 0.6005(1.2307)
2023/09/14 17:24:44 - INFO - root -   Epoch: [5/500][340/346], lr: 0.00000107 	 loss = 2.0885(1.2312)
2023/09/14 17:24:46 - INFO - root -   Epoch: [5/500] 	 loss = 1.2297
2023/09/14 17:24:46 - INFO - root -   train_accuracy = 0.4986
2023/09/14 17:24:55 - INFO - root -   Epoch: [6/500][0/346], lr: 0.00000109 	 loss = 1.5303(1.5303)
2023/09/14 17:25:11 - INFO - root -   Epoch: [6/500][20/346], lr: 0.00000109 	 loss = 2.5180(1.3063)
2023/09/14 17:25:32 - INFO - root -   Epoch: [6/500][40/346], lr: 0.00000109 	 loss = 1.5897(1.2504)
2023/09/14 17:25:48 - INFO - root -   Epoch: [6/500][60/346], lr: 0.00000109 	 loss = 0.8772(1.2673)
2023/09/14 17:26:09 - INFO - root -   Epoch: [6/500][80/346], lr: 0.00000109 	 loss = 0.9506(1.3032)
2023/09/14 17:26:26 - INFO - root -   Epoch: [6/500][100/346], lr: 0.00000109 	 loss = 0.8464(1.3045)
2023/09/14 17:26:47 - INFO - root -   Epoch: [6/500][120/346], lr: 0.00000109 	 loss = 0.8149(1.3039)
2023/09/14 17:27:03 - INFO - root -   Epoch: [6/500][140/346], lr: 0.00000109 	 loss = 1.2010(1.2854)
2023/09/14 17:27:24 - INFO - root -   Epoch: [6/500][160/346], lr: 0.00000109 	 loss = 0.4072(1.2609)
2023/09/14 17:27:41 - INFO - root -   Epoch: [6/500][180/346], lr: 0.00000109 	 loss = 1.3037(1.2602)
2023/09/14 17:28:02 - INFO - root -   Epoch: [6/500][200/346], lr: 0.00000109 	 loss = 0.2550(1.2424)
2023/09/14 17:28:19 - INFO - root -   Epoch: [6/500][220/346], lr: 0.00000109 	 loss = 1.8393(1.2410)
2023/09/14 17:28:40 - INFO - root -   Epoch: [6/500][240/346], lr: 0.00000109 	 loss = 1.2264(1.2436)
2023/09/14 17:28:57 - INFO - root -   Epoch: [6/500][260/346], lr: 0.00000109 	 loss = 0.7747(1.2405)
2023/09/14 17:29:18 - INFO - root -   Epoch: [6/500][280/346], lr: 0.00000109 	 loss = 2.6442(1.2382)
2023/09/14 17:29:34 - INFO - root -   Epoch: [6/500][300/346], lr: 0.00000109 	 loss = 2.4383(1.2462)
2023/09/14 17:29:55 - INFO - root -   Epoch: [6/500][320/346], lr: 0.00000109 	 loss = 0.3565(1.2454)
2023/09/14 17:30:11 - INFO - root -   Epoch: [6/500][340/346], lr: 0.00000109 	 loss = 1.7711(1.2439)
2023/09/14 17:30:13 - INFO - root -   Epoch: [6/500] 	 loss = 1.2400
2023/09/14 17:30:13 - INFO - root -   train_accuracy = 0.4798
2023/09/14 17:30:22 - INFO - root -   Epoch: [7/500][0/346], lr: 0.00000110 	 loss = 1.4070(1.4070)
2023/09/14 17:30:38 - INFO - root -   Epoch: [7/500][20/346], lr: 0.00000110 	 loss = 2.4102(1.2966)
2023/09/14 17:30:59 - INFO - root -   Epoch: [7/500][40/346], lr: 0.00000110 	 loss = 0.8961(1.2895)
2023/09/14 17:31:16 - INFO - root -   Epoch: [7/500][60/346], lr: 0.00000110 	 loss = 1.0386(1.2606)
2023/09/14 17:31:37 - INFO - root -   Epoch: [7/500][80/346], lr: 0.00000110 	 loss = 0.9796(1.2549)
2023/09/14 17:31:53 - INFO - root -   Epoch: [7/500][100/346], lr: 0.00000110 	 loss = 0.6703(1.2631)
2023/09/14 17:32:14 - INFO - root -   Epoch: [7/500][120/346], lr: 0.00000110 	 loss = 1.2081(1.2821)
2023/09/14 17:32:31 - INFO - root -   Epoch: [7/500][140/346], lr: 0.00000110 	 loss = 1.7593(1.2757)
2023/09/14 17:32:52 - INFO - root -   Epoch: [7/500][160/346], lr: 0.00000110 	 loss = 0.5091(1.2507)
2023/09/14 17:33:08 - INFO - root -   Epoch: [7/500][180/346], lr: 0.00000110 	 loss = 1.4055(1.2442)
2023/09/14 17:33:29 - INFO - root -   Epoch: [7/500][200/346], lr: 0.00000110 	 loss = 0.5363(1.2409)
2023/09/14 17:33:46 - INFO - root -   Epoch: [7/500][220/346], lr: 0.00000110 	 loss = 2.0045(1.2376)
2023/09/14 17:34:07 - INFO - root -   Epoch: [7/500][240/346], lr: 0.00000110 	 loss = 0.7095(1.2407)
2023/09/14 17:34:23 - INFO - root -   Epoch: [7/500][260/346], lr: 0.00000110 	 loss = 0.5654(1.2373)
2023/09/14 17:34:44 - INFO - root -   Epoch: [7/500][280/346], lr: 0.00000110 	 loss = 2.0828(1.2233)
2023/09/14 17:35:01 - INFO - root -   Epoch: [7/500][300/346], lr: 0.00000110 	 loss = 2.6505(1.2188)
2023/09/14 17:35:22 - INFO - root -   Epoch: [7/500][320/346], lr: 0.00000110 	 loss = 0.4612(1.2263)
2023/09/14 17:35:39 - INFO - root -   Epoch: [7/500][340/346], lr: 0.00000110 	 loss = 2.7584(1.2335)
2023/09/14 17:35:41 - INFO - root -   Epoch: [7/500] 	 loss = 1.2302
2023/09/14 17:35:41 - INFO - root -   train_accuracy = 0.5072
2023/09/14 17:35:49 - INFO - root -   Epoch: [8/500][0/346], lr: 0.00000111 	 loss = 2.1330(2.1330)
2023/09/14 17:36:05 - INFO - root -   Epoch: [8/500][20/346], lr: 0.00000111 	 loss = 1.9594(1.3408)
2023/09/14 17:36:26 - INFO - root -   Epoch: [8/500][40/346], lr: 0.00000111 	 loss = 1.0124(1.2817)
2023/09/14 17:36:43 - INFO - root -   Epoch: [8/500][60/346], lr: 0.00000111 	 loss = 1.1410(1.2751)
2023/09/14 17:37:04 - INFO - root -   Epoch: [8/500][80/346], lr: 0.00000111 	 loss = 1.3299(1.2879)
2023/09/14 17:37:21 - INFO - root -   Epoch: [8/500][100/346], lr: 0.00000111 	 loss = 0.6517(1.2868)
2023/09/14 17:37:41 - INFO - root -   Epoch: [8/500][120/346], lr: 0.00000111 	 loss = 0.8846(1.2856)
2023/09/14 17:37:58 - INFO - root -   Epoch: [8/500][140/346], lr: 0.00000111 	 loss = 2.9317(1.2964)
2023/09/14 17:38:19 - INFO - root -   Epoch: [8/500][160/346], lr: 0.00000111 	 loss = 0.5148(1.2701)
2023/09/14 17:38:35 - INFO - root -   Epoch: [8/500][180/346], lr: 0.00000111 	 loss = 1.0005(1.2528)
2023/09/14 17:38:56 - INFO - root -   Epoch: [8/500][200/346], lr: 0.00000111 	 loss = 0.7713(1.2480)
2023/09/14 17:39:13 - INFO - root -   Epoch: [8/500][220/346], lr: 0.00000111 	 loss = 1.5313(1.2424)
2023/09/14 17:39:34 - INFO - root -   Epoch: [8/500][240/346], lr: 0.00000111 	 loss = 0.7671(1.2376)
2023/09/14 17:39:50 - INFO - root -   Epoch: [8/500][260/346], lr: 0.00000111 	 loss = 0.6478(1.2359)
2023/09/14 17:40:11 - INFO - root -   Epoch: [8/500][280/346], lr: 0.00000111 	 loss = 1.6855(1.2324)
2023/09/14 17:40:28 - INFO - root -   Epoch: [8/500][300/346], lr: 0.00000111 	 loss = 3.1174(1.2380)
2023/09/14 17:40:49 - INFO - root -   Epoch: [8/500][320/346], lr: 0.00000111 	 loss = 0.4741(1.2421)
2023/09/14 17:41:05 - INFO - root -   Epoch: [8/500][340/346], lr: 0.00000111 	 loss = 2.0718(1.2416)
2023/09/14 17:41:07 - INFO - root -   Epoch: [8/500] 	 loss = 1.2401
2023/09/14 17:41:07 - INFO - root -   train_accuracy = 0.4899
2023/09/14 17:41:16 - INFO - root -   Epoch: [9/500][0/346], lr: 0.00000113 	 loss = 2.0737(2.0737)
2023/09/14 17:41:32 - INFO - root -   Epoch: [9/500][20/346], lr: 0.00000113 	 loss = 2.1514(1.3444)
2023/09/14 17:41:53 - INFO - root -   Epoch: [9/500][40/346], lr: 0.00000113 	 loss = 1.0834(1.2501)
2023/09/14 17:42:09 - INFO - root -   Epoch: [9/500][60/346], lr: 0.00000113 	 loss = 0.8295(1.2423)
2023/09/14 17:42:30 - INFO - root -   Epoch: [9/500][80/346], lr: 0.00000113 	 loss = 1.0792(1.2615)
2023/09/14 17:42:47 - INFO - root -   Epoch: [9/500][100/346], lr: 0.00000113 	 loss = 0.6088(1.2660)
2023/09/14 17:43:07 - INFO - root -   Epoch: [9/500][120/346], lr: 0.00000113 	 loss = 0.8969(1.2946)
2023/09/14 17:43:24 - INFO - root -   Epoch: [9/500][140/346], lr: 0.00000113 	 loss = 2.1360(1.2906)
2023/09/14 17:43:44 - INFO - root -   Epoch: [9/500][160/346], lr: 0.00000113 	 loss = 0.7132(1.2632)
2023/09/14 17:44:01 - INFO - root -   Epoch: [9/500][180/346], lr: 0.00000113 	 loss = 1.2314(1.2555)
2023/09/14 17:44:22 - INFO - root -   Epoch: [9/500][200/346], lr: 0.00000113 	 loss = 0.4271(1.2402)
2023/09/14 17:44:38 - INFO - root -   Epoch: [9/500][220/346], lr: 0.00000113 	 loss = 1.1921(1.2389)
2023/09/14 17:44:59 - INFO - root -   Epoch: [9/500][240/346], lr: 0.00000113 	 loss = 0.8961(1.2470)
2023/09/14 17:45:15 - INFO - root -   Epoch: [9/500][260/346], lr: 0.00000113 	 loss = 1.2175(1.2476)
2023/09/14 17:45:36 - INFO - root -   Epoch: [9/500][280/346], lr: 0.00000113 	 loss = 2.0363(1.2387)
2023/09/14 17:45:52 - INFO - root -   Epoch: [9/500][300/346], lr: 0.00000113 	 loss = 2.1656(1.2360)
2023/09/14 17:46:13 - INFO - root -   Epoch: [9/500][320/346], lr: 0.00000113 	 loss = 0.4379(1.2323)
2023/09/14 17:46:29 - INFO - root -   Epoch: [9/500][340/346], lr: 0.00000113 	 loss = 2.2905(1.2285)
2023/09/14 17:46:31 - INFO - root -   Epoch: [9/500] 	 loss = 1.2280
2023/09/14 17:47:53 - INFO - root -   precision = 0.5345
2023/09/14 17:47:53 - INFO - root -   eval_loss = 1.1559
2023/09/14 17:47:54 - INFO - root -   train_accuracy = 0.4913
2023/09/14 17:48:02 - INFO - root -   Epoch: [10/500][0/346], lr: 0.00000114 	 loss = 1.7781(1.7781)
2023/09/14 17:48:19 - INFO - root -   Epoch: [10/500][20/346], lr: 0.00000114 	 loss = 2.2513(1.2585)
2023/09/14 17:48:39 - INFO - root -   Epoch: [10/500][40/346], lr: 0.00000114 	 loss = 1.3110(1.2522)
2023/09/14 17:48:56 - INFO - root -   Epoch: [10/500][60/346], lr: 0.00000114 	 loss = 0.7563(1.2188)
2023/09/14 17:49:16 - INFO - root -   Epoch: [10/500][80/346], lr: 0.00000114 	 loss = 1.1472(1.2403)
2023/09/14 17:49:32 - INFO - root -   Epoch: [10/500][100/346], lr: 0.00000114 	 loss = 1.2133(1.2635)
2023/09/14 17:49:52 - INFO - root -   Epoch: [10/500][120/346], lr: 0.00000114 	 loss = 1.0908(1.2823)
2023/09/14 17:50:09 - INFO - root -   Epoch: [10/500][140/346], lr: 0.00000114 	 loss = 2.2677(1.2753)
2023/09/14 17:50:29 - INFO - root -   Epoch: [10/500][160/346], lr: 0.00000114 	 loss = 0.5212(1.2603)
2023/09/14 17:50:46 - INFO - root -   Epoch: [10/500][180/346], lr: 0.00000114 	 loss = 1.1193(1.2521)
2023/09/14 17:51:06 - INFO - root -   Epoch: [10/500][200/346], lr: 0.00000114 	 loss = 0.2872(1.2342)
2023/09/14 17:51:23 - INFO - root -   Epoch: [10/500][220/346], lr: 0.00000114 	 loss = 1.7329(1.2358)
2023/09/14 17:51:42 - INFO - root -   Epoch: [10/500][240/346], lr: 0.00000114 	 loss = 0.6701(1.2433)
2023/09/14 17:52:00 - INFO - root -   Epoch: [10/500][260/346], lr: 0.00000114 	 loss = 0.8915(1.2410)
2023/09/14 17:52:20 - INFO - root -   Epoch: [10/500][280/346], lr: 0.00000114 	 loss = 1.6995(1.2264)
2023/09/14 17:52:37 - INFO - root -   Epoch: [10/500][300/346], lr: 0.00000114 	 loss = 2.6552(1.2261)
2023/09/14 17:52:57 - INFO - root -   Epoch: [10/500][320/346], lr: 0.00000114 	 loss = 0.3562(1.2275)
2023/09/14 17:53:14 - INFO - root -   Epoch: [10/500][340/346], lr: 0.00000114 	 loss = 2.8196(1.2286)
2023/09/14 17:53:16 - INFO - root -   Epoch: [10/500] 	 loss = 1.2262
2023/09/14 17:53:16 - INFO - root -   train_accuracy = 0.4783
2023/09/14 17:53:24 - INFO - root -   Epoch: [11/500][0/346], lr: 0.00000116 	 loss = 1.5357(1.5357)
2023/09/14 17:53:40 - INFO - root -   Epoch: [11/500][20/346], lr: 0.00000116 	 loss = 2.1031(1.2137)
2023/09/14 17:54:01 - INFO - root -   Epoch: [11/500][40/346], lr: 0.00000116 	 loss = 1.2057(1.2080)
2023/09/14 17:54:18 - INFO - root -   Epoch: [11/500][60/346], lr: 0.00000116 	 loss = 1.1567(1.2259)
2023/09/14 17:54:39 - INFO - root -   Epoch: [11/500][80/346], lr: 0.00000116 	 loss = 1.1528(1.2854)
2023/09/14 17:54:56 - INFO - root -   Epoch: [11/500][100/346], lr: 0.00000116 	 loss = 0.7100(1.2636)
2023/09/14 17:55:17 - INFO - root -   Epoch: [11/500][120/346], lr: 0.00000116 	 loss = 0.9570(1.2745)
2023/09/14 17:55:33 - INFO - root -   Epoch: [11/500][140/346], lr: 0.00000116 	 loss = 1.8157(1.2565)
2023/09/14 17:55:54 - INFO - root -   Epoch: [11/500][160/346], lr: 0.00000116 	 loss = 0.9804(1.2563)
2023/09/14 17:56:11 - INFO - root -   Epoch: [11/500][180/346], lr: 0.00000116 	 loss = 1.3137(1.2480)
2023/09/14 17:56:31 - INFO - root -   Epoch: [11/500][200/346], lr: 0.00000116 	 loss = 0.4966(1.2411)
2023/09/14 17:56:48 - INFO - root -   Epoch: [11/500][220/346], lr: 0.00000116 	 loss = 1.3161(1.2438)
2023/09/14 17:57:08 - INFO - root -   Epoch: [11/500][240/346], lr: 0.00000116 	 loss = 1.1663(1.2545)
2023/09/14 17:57:26 - INFO - root -   Epoch: [11/500][260/346], lr: 0.00000116 	 loss = 0.6113(1.2518)
2023/09/14 17:57:45 - INFO - root -   Epoch: [11/500][280/346], lr: 0.00000116 	 loss = 2.2400(1.2391)
2023/09/14 17:58:03 - INFO - root -   Epoch: [11/500][300/346], lr: 0.00000116 	 loss = 2.2571(1.2378)
2023/09/14 17:58:21 - INFO - root -   Epoch: [11/500][320/346], lr: 0.00000116 	 loss = 0.5537(1.2388)
2023/09/14 17:58:40 - INFO - root -   Epoch: [11/500][340/346], lr: 0.00000116 	 loss = 1.7175(1.2355)
2023/09/14 17:58:42 - INFO - root -   Epoch: [11/500] 	 loss = 1.2318
2023/09/14 17:58:42 - INFO - root -   train_accuracy = 0.4957
2023/09/14 17:58:50 - INFO - root -   Epoch: [12/500][0/346], lr: 0.00000117 	 loss = 1.7068(1.7068)
2023/09/14 17:59:07 - INFO - root -   Epoch: [12/500][20/346], lr: 0.00000117 	 loss = 1.4696(1.2260)
2023/09/14 17:59:28 - INFO - root -   Epoch: [12/500][40/346], lr: 0.00000117 	 loss = 1.4252(1.2340)
2023/09/14 17:59:44 - INFO - root -   Epoch: [12/500][60/346], lr: 0.00000117 	 loss = 0.8764(1.2506)
2023/09/14 18:00:05 - INFO - root -   Epoch: [12/500][80/346], lr: 0.00000117 	 loss = 1.3500(1.2491)
2023/09/14 18:00:22 - INFO - root -   Epoch: [12/500][100/346], lr: 0.00000117 	 loss = 0.9753(1.2452)
2023/09/14 18:00:43 - INFO - root -   Epoch: [12/500][120/346], lr: 0.00000117 	 loss = 0.8569(1.2562)
2023/09/14 18:01:00 - INFO - root -   Epoch: [12/500][140/346], lr: 0.00000117 	 loss = 2.9636(1.2678)
2023/09/14 18:01:21 - INFO - root -   Epoch: [12/500][160/346], lr: 0.00000117 	 loss = 0.3869(1.2355)
2023/09/14 18:01:37 - INFO - root -   Epoch: [12/500][180/346], lr: 0.00000117 	 loss = 1.2618(1.2322)
2023/09/14 18:01:58 - INFO - root -   Epoch: [12/500][200/346], lr: 0.00000117 	 loss = 0.1794(1.2166)
2023/09/14 18:02:15 - INFO - root -   Epoch: [12/500][220/346], lr: 0.00000117 	 loss = 1.6315(1.2231)
2023/09/14 18:02:36 - INFO - root -   Epoch: [12/500][240/346], lr: 0.00000117 	 loss = 1.0665(1.2225)
2023/09/14 18:02:52 - INFO - root -   Epoch: [12/500][260/346], lr: 0.00000117 	 loss = 1.0600(1.2266)
2023/09/14 18:03:13 - INFO - root -   Epoch: [12/500][280/346], lr: 0.00000117 	 loss = 1.5803(1.2133)
2023/09/14 18:03:30 - INFO - root -   Epoch: [12/500][300/346], lr: 0.00000117 	 loss = 2.3774(1.2114)
2023/09/14 18:03:51 - INFO - root -   Epoch: [12/500][320/346], lr: 0.00000117 	 loss = 0.5346(1.2189)
2023/09/14 18:04:08 - INFO - root -   Epoch: [12/500][340/346], lr: 0.00000117 	 loss = 2.0654(1.2212)
2023/09/14 18:04:10 - INFO - root -   Epoch: [12/500] 	 loss = 1.2178
2023/09/14 18:04:10 - INFO - root -   train_accuracy = 0.4942
2023/09/14 18:04:18 - INFO - root -   Epoch: [13/500][0/346], lr: 0.00000119 	 loss = 1.9138(1.9138)
2023/09/14 18:04:35 - INFO - root -   Epoch: [13/500][20/346], lr: 0.00000119 	 loss = 1.7177(1.2647)
2023/09/14 18:04:56 - INFO - root -   Epoch: [13/500][40/346], lr: 0.00000119 	 loss = 0.9719(1.2078)
2023/09/14 18:05:12 - INFO - root -   Epoch: [13/500][60/346], lr: 0.00000119 	 loss = 0.6906(1.2352)
2023/09/14 18:05:33 - INFO - root -   Epoch: [13/500][80/346], lr: 0.00000119 	 loss = 0.9175(1.2948)
2023/09/14 18:05:50 - INFO - root -   Epoch: [13/500][100/346], lr: 0.00000119 	 loss = 0.7456(1.3161)
2023/09/14 18:06:11 - INFO - root -   Epoch: [13/500][120/346], lr: 0.00000119 	 loss = 0.9043(1.2968)
2023/09/14 18:06:28 - INFO - root -   Epoch: [13/500][140/346], lr: 0.00000119 	 loss = 2.0457(1.2814)
2023/09/14 18:06:49 - INFO - root -   Epoch: [13/500][160/346], lr: 0.00000119 	 loss = 0.4198(1.2561)
2023/09/14 18:07:06 - INFO - root -   Epoch: [13/500][180/346], lr: 0.00000119 	 loss = 1.5479(1.2524)
2023/09/14 18:07:27 - INFO - root -   Epoch: [13/500][200/346], lr: 0.00000119 	 loss = 0.4354(1.2327)
2023/09/14 18:07:43 - INFO - root -   Epoch: [13/500][220/346], lr: 0.00000119 	 loss = 1.3925(1.2248)
2023/09/14 18:08:04 - INFO - root -   Epoch: [13/500][240/346], lr: 0.00000119 	 loss = 0.9113(1.2296)
2023/09/14 18:08:21 - INFO - root -   Epoch: [13/500][260/346], lr: 0.00000119 	 loss = 0.4819(1.2280)
2023/09/14 18:08:42 - INFO - root -   Epoch: [13/500][280/346], lr: 0.00000119 	 loss = 1.8236(1.2170)
2023/09/14 18:08:58 - INFO - root -   Epoch: [13/500][300/346], lr: 0.00000119 	 loss = 1.9151(1.2187)
2023/09/14 18:09:20 - INFO - root -   Epoch: [13/500][320/346], lr: 0.00000119 	 loss = 0.4685(1.2182)
2023/09/14 18:09:36 - INFO - root -   Epoch: [13/500][340/346], lr: 0.00000119 	 loss = 2.0582(1.2191)
2023/09/14 18:09:38 - INFO - root -   Epoch: [13/500] 	 loss = 1.2180
2023/09/14 18:09:38 - INFO - root -   train_accuracy = 0.5058
2023/09/14 18:09:47 - INFO - root -   Epoch: [14/500][0/346], lr: 0.00000120 	 loss = 1.7931(1.7931)
2023/09/14 18:10:03 - INFO - root -   Epoch: [14/500][20/346], lr: 0.00000120 	 loss = 2.2961(1.1942)
2023/09/14 18:10:24 - INFO - root -   Epoch: [14/500][40/346], lr: 0.00000120 	 loss = 1.2525(1.2243)
2023/09/14 18:10:41 - INFO - root -   Epoch: [14/500][60/346], lr: 0.00000120 	 loss = 0.8878(1.2006)
2023/09/14 18:11:02 - INFO - root -   Epoch: [14/500][80/346], lr: 0.00000120 	 loss = 1.2491(1.2298)
2023/09/14 18:11:18 - INFO - root -   Epoch: [14/500][100/346], lr: 0.00000120 	 loss = 0.7278(1.2482)
2023/09/14 18:11:39 - INFO - root -   Epoch: [14/500][120/346], lr: 0.00000120 	 loss = 1.1864(1.2550)
2023/09/14 18:11:56 - INFO - root -   Epoch: [14/500][140/346], lr: 0.00000120 	 loss = 1.7625(1.2486)
2023/09/14 18:12:17 - INFO - root -   Epoch: [14/500][160/346], lr: 0.00000120 	 loss = 0.5476(1.2266)
2023/09/14 18:12:34 - INFO - root -   Epoch: [14/500][180/346], lr: 0.00000120 	 loss = 1.1635(1.2106)
2023/09/14 18:12:55 - INFO - root -   Epoch: [14/500][200/346], lr: 0.00000120 	 loss = 0.5585(1.1940)
2023/09/14 18:13:12 - INFO - root -   Epoch: [14/500][220/346], lr: 0.00000120 	 loss = 1.9677(1.1958)
2023/09/14 18:13:33 - INFO - root -   Epoch: [14/500][240/346], lr: 0.00000120 	 loss = 0.6519(1.2023)
2023/09/14 18:13:49 - INFO - root -   Epoch: [14/500][260/346], lr: 0.00000120 	 loss = 1.2280(1.2025)
2023/09/14 18:14:10 - INFO - root -   Epoch: [14/500][280/346], lr: 0.00000120 	 loss = 2.2556(1.1948)
2023/09/14 18:14:27 - INFO - root -   Epoch: [14/500][300/346], lr: 0.00000120 	 loss = 1.8318(1.1946)
2023/09/14 18:14:48 - INFO - root -   Epoch: [14/500][320/346], lr: 0.00000120 	 loss = 0.6491(1.2035)
2023/09/14 18:15:05 - INFO - root -   Epoch: [14/500][340/346], lr: 0.00000120 	 loss = 1.7524(1.2135)
2023/09/14 18:15:07 - INFO - root -   Epoch: [14/500] 	 loss = 1.2108
2023/09/14 18:16:30 - INFO - root -   precision = 0.5345
2023/09/14 18:16:30 - INFO - root -   eval_loss = 1.1639
2023/09/14 18:16:31 - INFO - root -   train_accuracy = 0.5000
2023/09/14 18:16:39 - INFO - root -   Epoch: [15/500][0/346], lr: 0.00000121 	 loss = 2.1497(2.1497)
2023/09/14 18:16:55 - INFO - root -   Epoch: [15/500][20/346], lr: 0.00000121 	 loss = 2.2048(1.2644)
2023/09/14 18:17:17 - INFO - root -   Epoch: [15/500][40/346], lr: 0.00000121 	 loss = 1.0604(1.1852)
2023/09/14 18:17:33 - INFO - root -   Epoch: [15/500][60/346], lr: 0.00000121 	 loss = 0.7077(1.2141)
2023/09/14 18:17:54 - INFO - root -   Epoch: [15/500][80/346], lr: 0.00000121 	 loss = 1.0319(1.2283)
2023/09/14 18:18:11 - INFO - root -   Epoch: [15/500][100/346], lr: 0.00000121 	 loss = 0.7836(1.2385)
2023/09/14 18:18:32 - INFO - root -   Epoch: [15/500][120/346], lr: 0.00000121 	 loss = 1.1668(1.2505)
2023/09/14 18:18:48 - INFO - root -   Epoch: [15/500][140/346], lr: 0.00000121 	 loss = 1.5032(1.2363)
2023/09/14 18:19:09 - INFO - root -   Epoch: [15/500][160/346], lr: 0.00000121 	 loss = 0.3210(1.2237)
2023/09/14 18:19:26 - INFO - root -   Epoch: [15/500][180/346], lr: 0.00000121 	 loss = 0.6612(1.2038)
2023/09/14 18:19:47 - INFO - root -   Epoch: [15/500][200/346], lr: 0.00000121 	 loss = 0.5292(1.1925)
2023/09/14 18:20:04 - INFO - root -   Epoch: [15/500][220/346], lr: 0.00000121 	 loss = 1.8433(1.1998)
2023/09/14 18:20:25 - INFO - root -   Epoch: [15/500][240/346], lr: 0.00000121 	 loss = 0.7264(1.2097)
2023/09/14 18:20:41 - INFO - root -   Epoch: [15/500][260/346], lr: 0.00000121 	 loss = 0.5629(1.2117)
2023/09/14 18:21:02 - INFO - root -   Epoch: [15/500][280/346], lr: 0.00000121 	 loss = 1.6982(1.2039)
2023/09/14 18:21:19 - INFO - root -   Epoch: [15/500][300/346], lr: 0.00000121 	 loss = 2.1895(1.1993)
2023/09/14 18:21:40 - INFO - root -   Epoch: [15/500][320/346], lr: 0.00000121 	 loss = 0.3600(1.2092)
2023/09/14 18:21:56 - INFO - root -   Epoch: [15/500][340/346], lr: 0.00000121 	 loss = 1.9472(1.2111)
2023/09/14 18:21:58 - INFO - root -   Epoch: [15/500] 	 loss = 1.2090
2023/09/14 18:21:58 - INFO - root -   train_accuracy = 0.4942
2023/09/14 18:22:06 - INFO - root -   Epoch: [16/500][0/346], lr: 0.00000123 	 loss = 1.4790(1.4790)
2023/09/14 18:22:23 - INFO - root -   Epoch: [16/500][20/346], lr: 0.00000123 	 loss = 1.9047(1.2024)
2023/09/14 18:22:44 - INFO - root -   Epoch: [16/500][40/346], lr: 0.00000123 	 loss = 0.9705(1.1825)
2023/09/14 18:23:00 - INFO - root -   Epoch: [16/500][60/346], lr: 0.00000123 	 loss = 1.0147(1.1814)
2023/09/14 18:23:21 - INFO - root -   Epoch: [16/500][80/346], lr: 0.00000123 	 loss = 0.9815(1.2147)
2023/09/14 18:23:38 - INFO - root -   Epoch: [16/500][100/346], lr: 0.00000123 	 loss = 0.6564(1.2180)
2023/09/14 18:23:58 - INFO - root -   Epoch: [16/500][120/346], lr: 0.00000123 	 loss = 1.1196(1.2365)
2023/09/14 18:24:15 - INFO - root -   Epoch: [16/500][140/346], lr: 0.00000123 	 loss = 1.6056(1.2109)
2023/09/14 18:24:36 - INFO - root -   Epoch: [16/500][160/346], lr: 0.00000123 	 loss = 0.4193(1.1971)
2023/09/14 18:24:53 - INFO - root -   Epoch: [16/500][180/346], lr: 0.00000123 	 loss = 1.1860(1.1937)
2023/09/14 18:25:13 - INFO - root -   Epoch: [16/500][200/346], lr: 0.00000123 	 loss = 0.4872(1.1765)
2023/09/14 18:25:30 - INFO - root -   Epoch: [16/500][220/346], lr: 0.00000123 	 loss = 1.5699(1.1750)
2023/09/14 18:25:51 - INFO - root -   Epoch: [16/500][240/346], lr: 0.00000123 	 loss = 0.7693(1.1722)
2023/09/14 18:26:08 - INFO - root -   Epoch: [16/500][260/346], lr: 0.00000123 	 loss = 0.8056(1.1831)
2023/09/14 18:26:29 - INFO - root -   Epoch: [16/500][280/346], lr: 0.00000123 	 loss = 2.0050(1.1749)
2023/09/14 18:26:45 - INFO - root -   Epoch: [16/500][300/346], lr: 0.00000123 	 loss = 2.0674(1.1752)
2023/09/14 18:27:06 - INFO - root -   Epoch: [16/500][320/346], lr: 0.00000123 	 loss = 0.4786(1.1811)
2023/09/14 18:27:23 - INFO - root -   Epoch: [16/500][340/346], lr: 0.00000123 	 loss = 2.4770(1.1895)
2023/09/14 18:27:25 - INFO - root -   Epoch: [16/500] 	 loss = 1.1860
2023/09/14 18:27:25 - INFO - root -   train_accuracy = 0.5145
2023/09/14 18:27:33 - INFO - root -   Epoch: [17/500][0/346], lr: 0.00000124 	 loss = 1.7024(1.7024)
2023/09/14 18:27:50 - INFO - root -   Epoch: [17/500][20/346], lr: 0.00000124 	 loss = 2.2396(1.2491)
2023/09/14 18:28:11 - INFO - root -   Epoch: [17/500][40/346], lr: 0.00000124 	 loss = 1.1388(1.2173)
2023/09/14 18:28:27 - INFO - root -   Epoch: [17/500][60/346], lr: 0.00000124 	 loss = 0.8823(1.2293)
2023/09/14 18:28:48 - INFO - root -   Epoch: [17/500][80/346], lr: 0.00000124 	 loss = 0.8741(1.2489)
2023/09/14 18:29:05 - INFO - root -   Epoch: [17/500][100/346], lr: 0.00000124 	 loss = 0.9153(1.2700)
2023/09/14 18:29:26 - INFO - root -   Epoch: [17/500][120/346], lr: 0.00000124 	 loss = 1.0535(1.2900)
2023/09/14 18:29:42 - INFO - root -   Epoch: [17/500][140/346], lr: 0.00000124 	 loss = 2.4507(1.2744)
2023/09/14 18:30:03 - INFO - root -   Epoch: [17/500][160/346], lr: 0.00000124 	 loss = 0.3136(1.2422)
2023/09/14 18:30:19 - INFO - root -   Epoch: [17/500][180/346], lr: 0.00000124 	 loss = 1.0221(1.2315)
2023/09/14 18:30:40 - INFO - root -   Epoch: [17/500][200/346], lr: 0.00000124 	 loss = 0.4245(1.2265)
2023/09/14 18:30:57 - INFO - root -   Epoch: [17/500][220/346], lr: 0.00000124 	 loss = 1.6422(1.2312)
2023/09/14 18:31:18 - INFO - root -   Epoch: [17/500][240/346], lr: 0.00000124 	 loss = 1.0104(1.2305)
2023/09/14 18:31:34 - INFO - root -   Epoch: [17/500][260/346], lr: 0.00000124 	 loss = 0.5304(1.2317)
2023/09/14 18:31:55 - INFO - root -   Epoch: [17/500][280/346], lr: 0.00000124 	 loss = 1.6352(1.2210)
2023/09/14 18:32:12 - INFO - root -   Epoch: [17/500][300/346], lr: 0.00000124 	 loss = 2.1645(1.2178)
2023/09/14 18:32:33 - INFO - root -   Epoch: [17/500][320/346], lr: 0.00000124 	 loss = 0.7401(1.2201)
2023/09/14 18:32:49 - INFO - root -   Epoch: [17/500][340/346], lr: 0.00000124 	 loss = 2.3014(1.2244)
2023/09/14 18:32:51 - INFO - root -   Epoch: [17/500] 	 loss = 1.2222
2023/09/14 18:32:51 - INFO - root -   train_accuracy = 0.5014
2023/09/14 18:32:59 - INFO - root -   Epoch: [18/500][0/346], lr: 0.00000126 	 loss = 1.7757(1.7757)
2023/09/14 18:33:16 - INFO - root -   Epoch: [18/500][20/346], lr: 0.00000126 	 loss = 2.3412(1.1883)
2023/09/14 18:33:37 - INFO - root -   Epoch: [18/500][40/346], lr: 0.00000126 	 loss = 0.9273(1.2333)
2023/09/14 18:33:53 - INFO - root -   Epoch: [18/500][60/346], lr: 0.00000126 	 loss = 0.5410(1.2365)
2023/09/14 18:34:14 - INFO - root -   Epoch: [18/500][80/346], lr: 0.00000126 	 loss = 1.0377(1.2650)
2023/09/14 18:34:30 - INFO - root -   Epoch: [18/500][100/346], lr: 0.00000126 	 loss = 0.7769(1.2570)
2023/09/14 18:34:51 - INFO - root -   Epoch: [18/500][120/346], lr: 0.00000126 	 loss = 1.2811(1.2724)
2023/09/14 18:35:07 - INFO - root -   Epoch: [18/500][140/346], lr: 0.00000126 	 loss = 2.1203(1.2690)
2023/09/14 18:35:28 - INFO - root -   Epoch: [18/500][160/346], lr: 0.00000126 	 loss = 0.4076(1.2440)
2023/09/14 18:35:45 - INFO - root -   Epoch: [18/500][180/346], lr: 0.00000126 	 loss = 1.2674(1.2365)
2023/09/14 18:36:05 - INFO - root -   Epoch: [18/500][200/346], lr: 0.00000126 	 loss = 0.3736(1.2224)
2023/09/14 18:36:22 - INFO - root -   Epoch: [18/500][220/346], lr: 0.00000126 	 loss = 1.6397(1.2210)
2023/09/14 18:36:42 - INFO - root -   Epoch: [18/500][240/346], lr: 0.00000126 	 loss = 1.0353(1.2278)
2023/09/14 18:36:59 - INFO - root -   Epoch: [18/500][260/346], lr: 0.00000126 	 loss = 0.6721(1.2236)
2023/09/14 18:37:19 - INFO - root -   Epoch: [18/500][280/346], lr: 0.00000126 	 loss = 1.8360(1.2195)
2023/09/14 18:37:36 - INFO - root -   Epoch: [18/500][300/346], lr: 0.00000126 	 loss = 1.6302(1.2131)
2023/09/14 18:37:57 - INFO - root -   Epoch: [18/500][320/346], lr: 0.00000126 	 loss = 0.3513(1.2195)
2023/09/14 18:38:12 - INFO - root -   Epoch: [18/500][340/346], lr: 0.00000126 	 loss = 1.7946(1.2192)
2023/09/14 18:38:14 - INFO - root -   Epoch: [18/500] 	 loss = 1.2176
2023/09/14 18:38:14 - INFO - root -   train_accuracy = 0.4884
2023/09/14 18:38:22 - INFO - root -   Epoch: [19/500][0/346], lr: 0.00000127 	 loss = 1.2927(1.2927)
2023/09/14 18:38:39 - INFO - root -   Epoch: [19/500][20/346], lr: 0.00000127 	 loss = 1.8893(1.1114)
2023/09/14 18:39:00 - INFO - root -   Epoch: [19/500][40/346], lr: 0.00000127 	 loss = 1.4400(1.1327)
2023/09/14 18:39:16 - INFO - root -   Epoch: [19/500][60/346], lr: 0.00000127 	 loss = 0.4491(1.1462)
2023/09/14 18:39:37 - INFO - root -   Epoch: [19/500][80/346], lr: 0.00000127 	 loss = 1.1878(1.1811)
2023/09/14 18:39:54 - INFO - root -   Epoch: [19/500][100/346], lr: 0.00000127 	 loss = 0.7569(1.2074)
2023/09/14 18:40:15 - INFO - root -   Epoch: [19/500][120/346], lr: 0.00000127 	 loss = 1.1996(1.2296)
2023/09/14 18:40:31 - INFO - root -   Epoch: [19/500][140/346], lr: 0.00000127 	 loss = 1.6864(1.2233)
2023/09/14 18:40:52 - INFO - root -   Epoch: [19/500][160/346], lr: 0.00000127 	 loss = 0.6092(1.2009)
2023/09/14 18:41:09 - INFO - root -   Epoch: [19/500][180/346], lr: 0.00000127 	 loss = 1.0390(1.1946)
2023/09/14 18:41:29 - INFO - root -   Epoch: [19/500][200/346], lr: 0.00000127 	 loss = 0.3365(1.1859)
2023/09/14 18:41:46 - INFO - root -   Epoch: [19/500][220/346], lr: 0.00000127 	 loss = 1.6715(1.1897)
2023/09/14 18:42:07 - INFO - root -   Epoch: [19/500][240/346], lr: 0.00000127 	 loss = 0.7060(1.1819)
2023/09/14 18:42:23 - INFO - root -   Epoch: [19/500][260/346], lr: 0.00000127 	 loss = 1.0425(1.1809)
2023/09/14 18:42:44 - INFO - root -   Epoch: [19/500][280/346], lr: 0.00000127 	 loss = 1.9751(1.1775)
2023/09/14 18:43:01 - INFO - root -   Epoch: [19/500][300/346], lr: 0.00000127 	 loss = 2.4786(1.1747)
2023/09/14 18:43:22 - INFO - root -   Epoch: [19/500][320/346], lr: 0.00000127 	 loss = 0.5977(1.1814)
2023/09/14 18:43:38 - INFO - root -   Epoch: [19/500][340/346], lr: 0.00000127 	 loss = 1.6765(1.1826)
2023/09/14 18:43:40 - INFO - root -   Epoch: [19/500] 	 loss = 1.1831
2023/09/14 18:45:03 - INFO - root -   precision = 0.5345
2023/09/14 18:45:03 - INFO - root -   eval_loss = 1.1596
2023/09/14 18:45:04 - INFO - root -   train_accuracy = 0.5043
2023/09/14 18:45:12 - INFO - root -   Epoch: [20/500][0/346], lr: 0.00000129 	 loss = 1.8308(1.8308)
2023/09/14 18:45:29 - INFO - root -   Epoch: [20/500][20/346], lr: 0.00000129 	 loss = 1.8478(1.2249)
2023/09/14 18:45:49 - INFO - root -   Epoch: [20/500][40/346], lr: 0.00000129 	 loss = 1.3643(1.2031)
2023/09/14 18:46:06 - INFO - root -   Epoch: [20/500][60/346], lr: 0.00000129 	 loss = 0.8499(1.1938)
2023/09/14 18:46:27 - INFO - root -   Epoch: [20/500][80/346], lr: 0.00000129 	 loss = 1.0915(1.2289)
2023/09/14 18:46:43 - INFO - root -   Epoch: [20/500][100/346], lr: 0.00000129 	 loss = 0.9290(1.2341)
2023/09/14 18:47:04 - INFO - root -   Epoch: [20/500][120/346], lr: 0.00000129 	 loss = 0.7990(1.2546)
2023/09/14 18:47:20 - INFO - root -   Epoch: [20/500][140/346], lr: 0.00000129 	 loss = 2.4474(1.2535)
2023/09/14 18:47:41 - INFO - root -   Epoch: [20/500][160/346], lr: 0.00000129 	 loss = 0.6046(1.2255)
2023/09/14 18:47:58 - INFO - root -   Epoch: [20/500][180/346], lr: 0.00000129 	 loss = 0.9797(1.2067)
2023/09/14 18:48:19 - INFO - root -   Epoch: [20/500][200/346], lr: 0.00000129 	 loss = 0.3450(1.1838)
2023/09/14 18:48:36 - INFO - root -   Epoch: [20/500][220/346], lr: 0.00000129 	 loss = 2.5255(1.1911)
2023/09/14 18:48:56 - INFO - root -   Epoch: [20/500][240/346], lr: 0.00000129 	 loss = 0.8699(1.1888)
2023/09/14 18:49:13 - INFO - root -   Epoch: [20/500][260/346], lr: 0.00000129 	 loss = 0.8252(1.1933)
2023/09/14 18:49:34 - INFO - root -   Epoch: [20/500][280/346], lr: 0.00000129 	 loss = 2.0465(1.1913)
2023/09/14 18:49:50 - INFO - root -   Epoch: [20/500][300/346], lr: 0.00000129 	 loss = 1.9692(1.1893)
2023/09/14 18:50:11 - INFO - root -   Epoch: [20/500][320/346], lr: 0.00000129 	 loss = 0.8119(1.1952)
2023/09/14 18:50:28 - INFO - root -   Epoch: [20/500][340/346], lr: 0.00000129 	 loss = 1.9325(1.1955)
2023/09/14 18:50:30 - INFO - root -   Epoch: [20/500] 	 loss = 1.1946
2023/09/14 18:50:30 - INFO - root -   train_accuracy = 0.5072
2023/09/14 18:50:38 - INFO - root -   Epoch: [21/500][0/346], lr: 0.00000130 	 loss = 1.5890(1.5890)
2023/09/14 18:50:55 - INFO - root -   Epoch: [21/500][20/346], lr: 0.00000130 	 loss = 1.7898(1.1820)
2023/09/14 18:51:15 - INFO - root -   Epoch: [21/500][40/346], lr: 0.00000130 	 loss = 1.0528(1.2060)
2023/09/14 18:51:32 - INFO - root -   Epoch: [21/500][60/346], lr: 0.00000130 	 loss = 0.7499(1.2089)
2023/09/14 18:51:53 - INFO - root -   Epoch: [21/500][80/346], lr: 0.00000130 	 loss = 1.2807(1.2150)
2023/09/14 18:52:09 - INFO - root -   Epoch: [21/500][100/346], lr: 0.00000130 	 loss = 0.9259(1.2440)
2023/09/14 18:52:30 - INFO - root -   Epoch: [21/500][120/346], lr: 0.00000130 	 loss = 0.8519(1.2610)
2023/09/14 18:52:47 - INFO - root -   Epoch: [21/500][140/346], lr: 0.00000130 	 loss = 1.7464(1.2450)
2023/09/14 18:53:08 - INFO - root -   Epoch: [21/500][160/346], lr: 0.00000130 	 loss = 0.4804(1.2248)
2023/09/14 18:53:24 - INFO - root -   Epoch: [21/500][180/346], lr: 0.00000130 	 loss = 1.0174(1.2115)
2023/09/14 18:53:45 - INFO - root -   Epoch: [21/500][200/346], lr: 0.00000130 	 loss = 0.5547(1.1996)
2023/09/14 18:54:02 - INFO - root -   Epoch: [21/500][220/346], lr: 0.00000130 	 loss = 1.6811(1.1982)
2023/09/14 18:54:23 - INFO - root -   Epoch: [21/500][240/346], lr: 0.00000130 	 loss = 0.9504(1.2065)
2023/09/14 18:54:40 - INFO - root -   Epoch: [21/500][260/346], lr: 0.00000130 	 loss = 0.6979(1.2094)
2023/09/14 18:55:00 - INFO - root -   Epoch: [21/500][280/346], lr: 0.00000130 	 loss = 1.8588(1.2015)
2023/09/14 18:55:17 - INFO - root -   Epoch: [21/500][300/346], lr: 0.00000130 	 loss = 2.8520(1.2055)
2023/09/14 18:55:38 - INFO - root -   Epoch: [21/500][320/346], lr: 0.00000130 	 loss = 0.5943(1.2014)
2023/09/14 18:55:55 - INFO - root -   Epoch: [21/500][340/346], lr: 0.00000130 	 loss = 1.5611(1.2008)
2023/09/14 18:55:57 - INFO - root -   Epoch: [21/500] 	 loss = 1.1977
2023/09/14 18:55:57 - INFO - root -   train_accuracy = 0.4957
2023/09/14 18:56:05 - INFO - root -   Epoch: [22/500][0/346], lr: 0.00000131 	 loss = 1.6624(1.6624)
2023/09/14 18:56:21 - INFO - root -   Epoch: [22/500][20/346], lr: 0.00000131 	 loss = 2.3558(1.1619)
2023/09/14 18:56:42 - INFO - root -   Epoch: [22/500][40/346], lr: 0.00000131 	 loss = 1.0371(1.1719)
2023/09/14 18:56:58 - INFO - root -   Epoch: [22/500][60/346], lr: 0.00000131 	 loss = 0.5809(1.1731)
2023/09/14 18:57:18 - INFO - root -   Epoch: [22/500][80/346], lr: 0.00000131 	 loss = 1.0745(1.2142)
2023/09/14 18:57:35 - INFO - root -   Epoch: [22/500][100/346], lr: 0.00000131 	 loss = 0.8346(1.2341)
2023/09/14 18:57:56 - INFO - root -   Epoch: [22/500][120/346], lr: 0.00000131 	 loss = 1.1437(1.2507)
2023/09/14 18:58:12 - INFO - root -   Epoch: [22/500][140/346], lr: 0.00000131 	 loss = 1.6199(1.2479)
2023/09/14 18:58:33 - INFO - root -   Epoch: [22/500][160/346], lr: 0.00000131 	 loss = 0.3893(1.2282)
2023/09/14 18:58:50 - INFO - root -   Epoch: [22/500][180/346], lr: 0.00000131 	 loss = 1.0715(1.2198)
2023/09/14 18:59:11 - INFO - root -   Epoch: [22/500][200/346], lr: 0.00000131 	 loss = 0.5479(1.2073)
2023/09/14 18:59:27 - INFO - root -   Epoch: [22/500][220/346], lr: 0.00000131 	 loss = 1.5796(1.2157)
2023/09/14 18:59:48 - INFO - root -   Epoch: [22/500][240/346], lr: 0.00000131 	 loss = 0.8779(1.2179)
2023/09/14 19:00:05 - INFO - root -   Epoch: [22/500][260/346], lr: 0.00000131 	 loss = 0.7268(1.2207)
2023/09/14 19:00:25 - INFO - root -   Epoch: [22/500][280/346], lr: 0.00000131 	 loss = 1.9062(1.2113)
2023/09/14 19:00:42 - INFO - root -   Epoch: [22/500][300/346], lr: 0.00000131 	 loss = 1.5451(1.2123)
2023/09/14 19:01:03 - INFO - root -   Epoch: [22/500][320/346], lr: 0.00000131 	 loss = 0.5367(1.2149)
2023/09/14 19:01:19 - INFO - root -   Epoch: [22/500][340/346], lr: 0.00000131 	 loss = 1.1496(1.2140)
2023/09/14 19:01:21 - INFO - root -   Epoch: [22/500] 	 loss = 1.2126
2023/09/14 19:01:21 - INFO - root -   train_accuracy = 0.5130
2023/09/14 19:01:29 - INFO - root -   Epoch: [23/500][0/346], lr: 0.00000133 	 loss = 2.0706(2.0706)
2023/09/14 19:01:46 - INFO - root -   Epoch: [23/500][20/346], lr: 0.00000133 	 loss = 2.0116(1.2031)
2023/09/14 19:02:07 - INFO - root -   Epoch: [23/500][40/346], lr: 0.00000133 	 loss = 1.2647(1.2106)
2023/09/14 19:02:23 - INFO - root -   Epoch: [23/500][60/346], lr: 0.00000133 	 loss = 0.8969(1.2020)
2023/09/14 19:02:44 - INFO - root -   Epoch: [23/500][80/346], lr: 0.00000133 	 loss = 0.9925(1.2386)
2023/09/14 19:03:00 - INFO - root -   Epoch: [23/500][100/346], lr: 0.00000133 	 loss = 0.6811(1.2454)
2023/09/14 19:03:21 - INFO - root -   Epoch: [23/500][120/346], lr: 0.00000133 	 loss = 1.1881(1.2522)
2023/09/14 19:03:38 - INFO - root -   Epoch: [23/500][140/346], lr: 0.00000133 	 loss = 1.8334(1.2446)
2023/09/14 19:03:59 - INFO - root -   Epoch: [23/500][160/346], lr: 0.00000133 	 loss = 0.7130(1.2230)
2023/09/14 19:04:15 - INFO - root -   Epoch: [23/500][180/346], lr: 0.00000133 	 loss = 0.9551(1.2143)
2023/09/14 19:04:36 - INFO - root -   Epoch: [23/500][200/346], lr: 0.00000133 	 loss = 0.4976(1.2002)
2023/09/14 19:04:53 - INFO - root -   Epoch: [23/500][220/346], lr: 0.00000133 	 loss = 1.9662(1.2064)
2023/09/14 19:05:14 - INFO - root -   Epoch: [23/500][240/346], lr: 0.00000133 	 loss = 0.8458(1.2066)
2023/09/14 19:05:30 - INFO - root -   Epoch: [23/500][260/346], lr: 0.00000133 	 loss = 0.7376(1.2064)
2023/09/14 19:05:51 - INFO - root -   Epoch: [23/500][280/346], lr: 0.00000133 	 loss = 1.9806(1.1965)
2023/09/14 19:06:08 - INFO - root -   Epoch: [23/500][300/346], lr: 0.00000133 	 loss = 1.8305(1.1935)
2023/09/14 19:06:28 - INFO - root -   Epoch: [23/500][320/346], lr: 0.00000133 	 loss = 0.5877(1.1966)
2023/09/14 19:06:45 - INFO - root -   Epoch: [23/500][340/346], lr: 0.00000133 	 loss = 2.0022(1.1962)
2023/09/14 19:06:47 - INFO - root -   Epoch: [23/500] 	 loss = 1.1925
2023/09/14 19:06:47 - INFO - root -   train_accuracy = 0.5101
2023/09/14 19:06:55 - INFO - root -   Epoch: [24/500][0/346], lr: 0.00000134 	 loss = 2.4250(2.4250)
2023/09/14 19:07:12 - INFO - root -   Epoch: [24/500][20/346], lr: 0.00000134 	 loss = 1.9577(1.3181)
2023/09/14 19:07:32 - INFO - root -   Epoch: [24/500][40/346], lr: 0.00000134 	 loss = 1.2735(1.2553)
2023/09/14 19:07:49 - INFO - root -   Epoch: [24/500][60/346], lr: 0.00000134 	 loss = 1.0967(1.2360)
2023/09/14 19:08:10 - INFO - root -   Epoch: [24/500][80/346], lr: 0.00000134 	 loss = 0.9432(1.2626)
2023/09/14 19:08:26 - INFO - root -   Epoch: [24/500][100/346], lr: 0.00000134 	 loss = 0.6635(1.2726)
2023/09/14 19:08:47 - INFO - root -   Epoch: [24/500][120/346], lr: 0.00000134 	 loss = 0.9261(1.2696)
2023/09/14 19:09:04 - INFO - root -   Epoch: [24/500][140/346], lr: 0.00000134 	 loss = 2.1385(1.2789)
2023/09/14 19:09:25 - INFO - root -   Epoch: [24/500][160/346], lr: 0.00000134 	 loss = 0.5264(1.2450)
2023/09/14 19:09:41 - INFO - root -   Epoch: [24/500][180/346], lr: 0.00000134 	 loss = 1.2069(1.2396)
2023/09/14 19:10:02 - INFO - root -   Epoch: [24/500][200/346], lr: 0.00000134 	 loss = 0.3984(1.2200)
2023/09/14 19:10:19 - INFO - root -   Epoch: [24/500][220/346], lr: 0.00000134 	 loss = 1.8124(1.2190)
2023/09/14 19:10:39 - INFO - root -   Epoch: [24/500][240/346], lr: 0.00000134 	 loss = 0.8817(1.2210)
2023/09/14 19:10:56 - INFO - root -   Epoch: [24/500][260/346], lr: 0.00000134 	 loss = 0.5908(1.2207)
2023/09/14 19:11:17 - INFO - root -   Epoch: [24/500][280/346], lr: 0.00000134 	 loss = 1.9323(1.2060)
2023/09/14 19:11:33 - INFO - root -   Epoch: [24/500][300/346], lr: 0.00000134 	 loss = 1.9801(1.2014)
2023/09/14 19:11:54 - INFO - root -   Epoch: [24/500][320/346], lr: 0.00000134 	 loss = 0.4652(1.2032)
2023/09/14 19:12:11 - INFO - root -   Epoch: [24/500][340/346], lr: 0.00000134 	 loss = 1.7756(1.2037)
2023/09/14 19:12:13 - INFO - root -   Epoch: [24/500] 	 loss = 1.1994
2023/09/14 19:13:35 - INFO - root -   precision = 0.5345
2023/09/14 19:13:35 - INFO - root -   eval_loss = 1.1590
2023/09/14 19:13:36 - INFO - root -   train_accuracy = 0.5087
2023/09/14 19:13:44 - INFO - root -   Epoch: [25/500][0/346], lr: 0.00000136 	 loss = 1.8741(1.8741)
2023/09/14 19:14:01 - INFO - root -   Epoch: [25/500][20/346], lr: 0.00000136 	 loss = 1.8000(1.2285)
2023/09/14 19:14:22 - INFO - root -   Epoch: [25/500][40/346], lr: 0.00000136 	 loss = 1.0451(1.2101)
2023/09/14 19:14:38 - INFO - root -   Epoch: [25/500][60/346], lr: 0.00000136 	 loss = 0.9006(1.1939)
2023/09/14 19:14:59 - INFO - root -   Epoch: [25/500][80/346], lr: 0.00000136 	 loss = 0.9853(1.2208)
2023/09/14 19:15:15 - INFO - root -   Epoch: [25/500][100/346], lr: 0.00000136 	 loss = 1.0416(1.2263)
2023/09/14 19:15:36 - INFO - root -   Epoch: [25/500][120/346], lr: 0.00000136 	 loss = 1.2707(1.2514)
2023/09/14 19:15:53 - INFO - root -   Epoch: [25/500][140/346], lr: 0.00000136 	 loss = 1.5043(1.2376)
2023/09/14 19:16:14 - INFO - root -   Epoch: [25/500][160/346], lr: 0.00000136 	 loss = 0.4386(1.2225)
2023/09/14 19:16:30 - INFO - root -   Epoch: [25/500][180/346], lr: 0.00000136 	 loss = 0.8630(1.2115)
2023/09/14 19:16:51 - INFO - root -   Epoch: [25/500][200/346], lr: 0.00000136 	 loss = 0.3222(1.1952)
2023/09/14 19:17:08 - INFO - root -   Epoch: [25/500][220/346], lr: 0.00000136 	 loss = 1.5004(1.1893)
2023/09/14 19:17:29 - INFO - root -   Epoch: [25/500][240/346], lr: 0.00000136 	 loss = 1.1029(1.1917)
2023/09/14 19:17:45 - INFO - root -   Epoch: [25/500][260/346], lr: 0.00000136 	 loss = 0.5851(1.1954)
2023/09/14 19:18:06 - INFO - root -   Epoch: [25/500][280/346], lr: 0.00000136 	 loss = 1.4846(1.1817)
2023/09/14 19:18:23 - INFO - root -   Epoch: [25/500][300/346], lr: 0.00000136 	 loss = 1.8617(1.1817)
2023/09/14 19:18:44 - INFO - root -   Epoch: [25/500][320/346], lr: 0.00000136 	 loss = 0.7242(1.1884)
2023/09/14 19:19:00 - INFO - root -   Epoch: [25/500][340/346], lr: 0.00000136 	 loss = 1.8378(1.1890)
2023/09/14 19:19:02 - INFO - root -   Epoch: [25/500] 	 loss = 1.1870
2023/09/14 19:19:02 - INFO - root -   train_accuracy = 0.5130
2023/09/14 19:19:10 - INFO - root -   Epoch: [26/500][0/346], lr: 0.00000137 	 loss = 2.1668(2.1668)
2023/09/14 19:19:27 - INFO - root -   Epoch: [26/500][20/346], lr: 0.00000137 	 loss = 2.1576(1.2446)
2023/09/14 19:19:48 - INFO - root -   Epoch: [26/500][40/346], lr: 0.00000137 	 loss = 0.7997(1.2223)
2023/09/14 19:20:04 - INFO - root -   Epoch: [26/500][60/346], lr: 0.00000137 	 loss = 0.5865(1.2457)
2023/09/14 19:20:25 - INFO - root -   Epoch: [26/500][80/346], lr: 0.00000137 	 loss = 0.7193(1.2601)
2023/09/14 19:20:42 - INFO - root -   Epoch: [26/500][100/346], lr: 0.00000137 	 loss = 0.7208(1.2641)
2023/09/14 19:21:03 - INFO - root -   Epoch: [26/500][120/346], lr: 0.00000137 	 loss = 0.8266(1.2654)
2023/09/14 19:21:19 - INFO - root -   Epoch: [26/500][140/346], lr: 0.00000137 	 loss = 1.8103(1.2641)
2023/09/14 19:21:40 - INFO - root -   Epoch: [26/500][160/346], lr: 0.00000137 	 loss = 0.4705(1.2394)
2023/09/14 19:21:57 - INFO - root -   Epoch: [26/500][180/346], lr: 0.00000137 	 loss = 1.3026(1.2369)
2023/09/14 19:22:18 - INFO - root -   Epoch: [26/500][200/346], lr: 0.00000137 	 loss = 0.5221(1.2247)
2023/09/14 19:22:34 - INFO - root -   Epoch: [26/500][220/346], lr: 0.00000137 	 loss = 1.6877(1.2279)
2023/09/14 19:22:55 - INFO - root -   Epoch: [26/500][240/346], lr: 0.00000137 	 loss = 1.0166(1.2310)
2023/09/14 19:23:12 - INFO - root -   Epoch: [26/500][260/346], lr: 0.00000137 	 loss = 0.7380(1.2275)
2023/09/14 19:23:32 - INFO - root -   Epoch: [26/500][280/346], lr: 0.00000137 	 loss = 1.8116(1.2158)
2023/09/14 19:23:49 - INFO - root -   Epoch: [26/500][300/346], lr: 0.00000137 	 loss = 2.0474(1.2118)
2023/09/14 19:24:10 - INFO - root -   Epoch: [26/500][320/346], lr: 0.00000137 	 loss = 0.7297(1.2170)
2023/09/14 19:24:26 - INFO - root -   Epoch: [26/500][340/346], lr: 0.00000137 	 loss = 1.9908(1.2146)
2023/09/14 19:24:28 - INFO - root -   Epoch: [26/500] 	 loss = 1.2125
2023/09/14 19:24:28 - INFO - root -   train_accuracy = 0.4928
2023/09/14 19:24:37 - INFO - root -   Epoch: [27/500][0/346], lr: 0.00000139 	 loss = 1.5649(1.5649)
2023/09/14 19:24:53 - INFO - root -   Epoch: [27/500][20/346], lr: 0.00000139 	 loss = 2.6282(1.2423)
2023/09/14 19:25:14 - INFO - root -   Epoch: [27/500][40/346], lr: 0.00000139 	 loss = 1.2486(1.2370)
2023/09/14 19:25:31 - INFO - root -   Epoch: [27/500][60/346], lr: 0.00000139 	 loss = 0.9275(1.2385)
2023/09/14 19:25:52 - INFO - root -   Epoch: [27/500][80/346], lr: 0.00000139 	 loss = 0.9984(1.2713)
2023/09/14 19:26:08 - INFO - root -   Epoch: [27/500][100/346], lr: 0.00000139 	 loss = 0.9346(1.2728)
2023/09/14 19:26:29 - INFO - root -   Epoch: [27/500][120/346], lr: 0.00000139 	 loss = 1.0341(1.2622)
2023/09/14 19:26:45 - INFO - root -   Epoch: [27/500][140/346], lr: 0.00000139 	 loss = 1.7319(1.2448)
2023/09/14 19:27:06 - INFO - root -   Epoch: [27/500][160/346], lr: 0.00000139 	 loss = 0.5323(1.2178)
2023/09/14 19:27:23 - INFO - root -   Epoch: [27/500][180/346], lr: 0.00000139 	 loss = 1.1498(1.2132)
2023/09/14 19:27:44 - INFO - root -   Epoch: [27/500][200/346], lr: 0.00000139 	 loss = 0.2656(1.2002)
2023/09/14 19:28:00 - INFO - root -   Epoch: [27/500][220/346], lr: 0.00000139 	 loss = 2.1095(1.1994)
2023/09/14 19:28:22 - INFO - root -   Epoch: [27/500][240/346], lr: 0.00000139 	 loss = 0.8938(1.2034)
2023/09/14 19:28:38 - INFO - root -   Epoch: [27/500][260/346], lr: 0.00000139 	 loss = 0.6606(1.2031)
2023/09/14 19:28:59 - INFO - root -   Epoch: [27/500][280/346], lr: 0.00000139 	 loss = 1.6171(1.1914)
2023/09/14 19:29:16 - INFO - root -   Epoch: [27/500][300/346], lr: 0.00000139 	 loss = 2.2377(1.1978)
2023/09/14 19:29:37 - INFO - root -   Epoch: [27/500][320/346], lr: 0.00000139 	 loss = 0.6664(1.1993)
2023/09/14 19:29:53 - INFO - root -   Epoch: [27/500][340/346], lr: 0.00000139 	 loss = 1.9222(1.2022)
2023/09/14 19:29:55 - INFO - root -   Epoch: [27/500] 	 loss = 1.2010
2023/09/14 19:29:55 - INFO - root -   train_accuracy = 0.5058
2023/09/14 19:30:03 - INFO - root -   Epoch: [28/500][0/346], lr: 0.00000140 	 loss = 1.3509(1.3509)
2023/09/14 19:30:20 - INFO - root -   Epoch: [28/500][20/346], lr: 0.00000140 	 loss = 1.7948(1.1818)
2023/09/14 19:30:41 - INFO - root -   Epoch: [28/500][40/346], lr: 0.00000140 	 loss = 1.3752(1.2065)
2023/09/14 19:30:58 - INFO - root -   Epoch: [28/500][60/346], lr: 0.00000140 	 loss = 0.8334(1.2017)
2023/09/14 19:31:19 - INFO - root -   Epoch: [28/500][80/346], lr: 0.00000140 	 loss = 1.0503(1.2191)
2023/09/14 19:31:35 - INFO - root -   Epoch: [28/500][100/346], lr: 0.00000140 	 loss = 0.8203(1.2333)
2023/09/14 19:31:56 - INFO - root -   Epoch: [28/500][120/346], lr: 0.00000140 	 loss = 1.1430(1.2437)
2023/09/14 19:32:13 - INFO - root -   Epoch: [28/500][140/346], lr: 0.00000140 	 loss = 1.4987(1.2394)
2023/09/14 19:32:34 - INFO - root -   Epoch: [28/500][160/346], lr: 0.00000140 	 loss = 0.4008(1.2267)
2023/09/14 19:32:50 - INFO - root -   Epoch: [28/500][180/346], lr: 0.00000140 	 loss = 1.1312(1.2166)
2023/09/14 19:33:11 - INFO - root -   Epoch: [28/500][200/346], lr: 0.00000140 	 loss = 0.5154(1.2010)
2023/09/14 19:33:28 - INFO - root -   Epoch: [28/500][220/346], lr: 0.00000140 	 loss = 1.8344(1.2063)
2023/09/14 19:33:49 - INFO - root -   Epoch: [28/500][240/346], lr: 0.00000140 	 loss = 1.2440(1.2103)
2023/09/14 19:34:05 - INFO - root -   Epoch: [28/500][260/346], lr: 0.00000140 	 loss = 0.5154(1.2130)
2023/09/14 19:34:26 - INFO - root -   Epoch: [28/500][280/346], lr: 0.00000140 	 loss = 1.5100(1.1998)
2023/09/14 19:34:43 - INFO - root -   Epoch: [28/500][300/346], lr: 0.00000140 	 loss = 2.0358(1.1983)
2023/09/14 19:35:04 - INFO - root -   Epoch: [28/500][320/346], lr: 0.00000140 	 loss = 0.5919(1.2016)
2023/09/14 19:35:21 - INFO - root -   Epoch: [28/500][340/346], lr: 0.00000140 	 loss = 1.8872(1.1943)
2023/09/14 19:35:23 - INFO - root -   Epoch: [28/500] 	 loss = 1.1923
2023/09/14 19:35:23 - INFO - root -   train_accuracy = 0.5202
2023/09/14 19:35:31 - INFO - root -   Epoch: [29/500][0/346], lr: 0.00000141 	 loss = 1.4483(1.4483)
2023/09/14 19:35:48 - INFO - root -   Epoch: [29/500][20/346], lr: 0.00000141 	 loss = 2.0951(1.1884)
2023/09/14 19:36:09 - INFO - root -   Epoch: [29/500][40/346], lr: 0.00000141 	 loss = 0.9033(1.1889)
2023/09/14 19:36:25 - INFO - root -   Epoch: [29/500][60/346], lr: 0.00000141 	 loss = 0.7250(1.1875)
2023/09/14 19:36:46 - INFO - root -   Epoch: [29/500][80/346], lr: 0.00000141 	 loss = 0.6153(1.2131)
2023/09/14 19:37:03 - INFO - root -   Epoch: [29/500][100/346], lr: 0.00000141 	 loss = 0.9016(1.2339)
2023/09/14 19:37:24 - INFO - root -   Epoch: [29/500][120/346], lr: 0.00000141 	 loss = 0.9137(1.2484)
2023/09/14 19:37:41 - INFO - root -   Epoch: [29/500][140/346], lr: 0.00000141 	 loss = 1.5780(1.2254)
2023/09/14 19:38:02 - INFO - root -   Epoch: [29/500][160/346], lr: 0.00000141 	 loss = 0.5188(1.2046)
2023/09/14 19:38:19 - INFO - root -   Epoch: [29/500][180/346], lr: 0.00000141 	 loss = 1.5102(1.1979)
2023/09/14 19:38:39 - INFO - root -   Epoch: [29/500][200/346], lr: 0.00000141 	 loss = 0.5319(1.1888)
2023/09/14 19:38:56 - INFO - root -   Epoch: [29/500][220/346], lr: 0.00000141 	 loss = 1.6746(1.1943)
2023/09/14 19:39:17 - INFO - root -   Epoch: [29/500][240/346], lr: 0.00000141 	 loss = 0.9198(1.1918)
2023/09/14 19:39:33 - INFO - root -   Epoch: [29/500][260/346], lr: 0.00000141 	 loss = 0.9007(1.1894)
2023/09/14 19:39:54 - INFO - root -   Epoch: [29/500][280/346], lr: 0.00000141 	 loss = 1.2988(1.1788)
2023/09/14 19:40:11 - INFO - root -   Epoch: [29/500][300/346], lr: 0.00000141 	 loss = 1.8571(1.1798)
2023/09/14 19:40:32 - INFO - root -   Epoch: [29/500][320/346], lr: 0.00000141 	 loss = 0.4031(1.1881)
2023/09/14 19:40:49 - INFO - root -   Epoch: [29/500][340/346], lr: 0.00000141 	 loss = 1.9955(1.1832)
2023/09/14 19:40:51 - INFO - root -   Epoch: [29/500] 	 loss = 1.1805
2023/09/14 19:42:13 - INFO - root -   precision = 0.5345
2023/09/14 19:42:13 - INFO - root -   eval_loss = 1.1538
2023/09/14 19:42:14 - INFO - root -   train_accuracy = 0.5275
2023/09/14 19:42:23 - INFO - root -   Epoch: [30/500][0/346], lr: 0.00000143 	 loss = 2.1782(2.1782)
2023/09/14 19:42:39 - INFO - root -   Epoch: [30/500][20/346], lr: 0.00000143 	 loss = 1.9559(1.2331)
2023/09/14 19:43:00 - INFO - root -   Epoch: [30/500][40/346], lr: 0.00000143 	 loss = 0.9520(1.1757)
2023/09/14 19:43:17 - INFO - root -   Epoch: [30/500][60/346], lr: 0.00000143 	 loss = 0.7285(1.1907)
2023/09/14 19:43:38 - INFO - root -   Epoch: [30/500][80/346], lr: 0.00000143 	 loss = 1.1388(1.2111)
2023/09/14 19:43:55 - INFO - root -   Epoch: [30/500][100/346], lr: 0.00000143 	 loss = 0.9522(1.2282)
2023/09/14 19:44:15 - INFO - root -   Epoch: [30/500][120/346], lr: 0.00000143 	 loss = 1.1989(1.2386)
2023/09/14 19:44:32 - INFO - root -   Epoch: [30/500][140/346], lr: 0.00000143 	 loss = 1.6324(1.2210)
2023/09/14 19:44:53 - INFO - root -   Epoch: [30/500][160/346], lr: 0.00000143 	 loss = 0.4591(1.2024)
2023/09/14 19:45:10 - INFO - root -   Epoch: [30/500][180/346], lr: 0.00000143 	 loss = 0.9496(1.1966)
2023/09/14 19:45:31 - INFO - root -   Epoch: [30/500][200/346], lr: 0.00000143 	 loss = 0.2881(1.1854)
2023/09/14 19:45:47 - INFO - root -   Epoch: [30/500][220/346], lr: 0.00000143 	 loss = 1.5827(1.1901)
2023/09/14 19:46:08 - INFO - root -   Epoch: [30/500][240/346], lr: 0.00000143 	 loss = 0.8656(1.1977)
2023/09/14 19:46:25 - INFO - root -   Epoch: [30/500][260/346], lr: 0.00000143 	 loss = 0.5958(1.1959)
2023/09/14 19:46:46 - INFO - root -   Epoch: [30/500][280/346], lr: 0.00000143 	 loss = 1.6465(1.1862)
2023/09/14 19:47:02 - INFO - root -   Epoch: [30/500][300/346], lr: 0.00000143 	 loss = 2.0618(1.1935)
2023/09/14 19:47:23 - INFO - root -   Epoch: [30/500][320/346], lr: 0.00000143 	 loss = 0.5553(1.1955)
2023/09/14 19:47:39 - INFO - root -   Epoch: [30/500][340/346], lr: 0.00000143 	 loss = 1.5829(1.1945)
2023/09/14 19:47:42 - INFO - root -   Epoch: [30/500] 	 loss = 1.1906
2023/09/14 19:47:42 - INFO - root -   train_accuracy = 0.5087
2023/09/14 19:47:50 - INFO - root -   Epoch: [31/500][0/346], lr: 0.00000144 	 loss = 1.7350(1.7350)
2023/09/14 19:48:06 - INFO - root -   Epoch: [31/500][20/346], lr: 0.00000144 	 loss = 1.9831(1.1550)
2023/09/14 19:48:27 - INFO - root -   Epoch: [31/500][40/346], lr: 0.00000144 	 loss = 1.1785(1.1512)
2023/09/14 19:48:44 - INFO - root -   Epoch: [31/500][60/346], lr: 0.00000144 	 loss = 0.8078(1.1493)
2023/09/14 19:49:05 - INFO - root -   Epoch: [31/500][80/346], lr: 0.00000144 	 loss = 1.0691(1.1854)
2023/09/14 19:49:21 - INFO - root -   Epoch: [31/500][100/346], lr: 0.00000144 	 loss = 0.7806(1.1853)
2023/09/14 19:49:43 - INFO - root -   Epoch: [31/500][120/346], lr: 0.00000144 	 loss = 0.9077(1.2008)
2023/09/14 19:49:59 - INFO - root -   Epoch: [31/500][140/346], lr: 0.00000144 	 loss = 1.6935(1.2028)
2023/09/14 19:50:20 - INFO - root -   Epoch: [31/500][160/346], lr: 0.00000144 	 loss = 0.6393(1.1830)
2023/09/14 19:50:37 - INFO - root -   Epoch: [31/500][180/346], lr: 0.00000144 	 loss = 1.4293(1.1854)
2023/09/14 19:50:58 - INFO - root -   Epoch: [31/500][200/346], lr: 0.00000144 	 loss = 0.6297(1.1742)
2023/09/14 19:51:14 - INFO - root -   Epoch: [31/500][220/346], lr: 0.00000144 	 loss = 1.4955(1.1783)
2023/09/14 19:51:35 - INFO - root -   Epoch: [31/500][240/346], lr: 0.00000144 	 loss = 0.8158(1.1763)
2023/09/14 19:51:52 - INFO - root -   Epoch: [31/500][260/346], lr: 0.00000144 	 loss = 1.1036(1.1792)
2023/09/14 19:52:13 - INFO - root -   Epoch: [31/500][280/346], lr: 0.00000144 	 loss = 2.6591(1.1780)
2023/09/14 19:52:30 - INFO - root -   Epoch: [31/500][300/346], lr: 0.00000144 	 loss = 2.8025(1.1812)
2023/09/14 19:52:51 - INFO - root -   Epoch: [31/500][320/346], lr: 0.00000144 	 loss = 0.4313(1.1838)
2023/09/14 19:53:07 - INFO - root -   Epoch: [31/500][340/346], lr: 0.00000144 	 loss = 1.7096(1.1836)
2023/09/14 19:53:09 - INFO - root -   Epoch: [31/500] 	 loss = 1.1809
2023/09/14 19:53:09 - INFO - root -   train_accuracy = 0.5072
2023/09/14 19:53:18 - INFO - root -   Epoch: [32/500][0/346], lr: 0.00000146 	 loss = 1.9225(1.9225)
2023/09/14 19:53:34 - INFO - root -   Epoch: [32/500][20/346], lr: 0.00000146 	 loss = 2.0317(1.2450)
2023/09/14 19:53:55 - INFO - root -   Epoch: [32/500][40/346], lr: 0.00000146 	 loss = 1.6111(1.2068)
2023/09/14 19:54:12 - INFO - root -   Epoch: [32/500][60/346], lr: 0.00000146 	 loss = 0.8214(1.2059)
2023/09/14 19:54:32 - INFO - root -   Epoch: [32/500][80/346], lr: 0.00000146 	 loss = 0.9556(1.2429)
2023/09/14 19:54:49 - INFO - root -   Epoch: [32/500][100/346], lr: 0.00000146 	 loss = 0.8752(1.2478)
2023/09/14 19:55:10 - INFO - root -   Epoch: [32/500][120/346], lr: 0.00000146 	 loss = 0.6699(1.2494)
2023/09/14 19:55:26 - INFO - root -   Epoch: [32/500][140/346], lr: 0.00000146 	 loss = 1.8687(1.2455)
2023/09/14 19:55:47 - INFO - root -   Epoch: [32/500][160/346], lr: 0.00000146 	 loss = 0.5304(1.2211)
2023/09/14 19:56:04 - INFO - root -   Epoch: [32/500][180/346], lr: 0.00000146 	 loss = 1.1180(1.2144)
2023/09/14 19:56:25 - INFO - root -   Epoch: [32/500][200/346], lr: 0.00000146 	 loss = 0.4894(1.2022)
2023/09/14 19:56:41 - INFO - root -   Epoch: [32/500][220/346], lr: 0.00000146 	 loss = 1.4421(1.1994)
2023/09/14 19:57:02 - INFO - root -   Epoch: [32/500][240/346], lr: 0.00000146 	 loss = 0.9919(1.2025)
2023/09/14 19:57:19 - INFO - root -   Epoch: [32/500][260/346], lr: 0.00000146 	 loss = 0.6010(1.2028)
2023/09/14 19:57:39 - INFO - root -   Epoch: [32/500][280/346], lr: 0.00000146 	 loss = 1.4366(1.1972)
2023/09/14 19:57:56 - INFO - root -   Epoch: [32/500][300/346], lr: 0.00000146 	 loss = 1.8782(1.1973)
2023/09/14 19:58:17 - INFO - root -   Epoch: [32/500][320/346], lr: 0.00000146 	 loss = 0.4729(1.2036)
2023/09/14 19:58:33 - INFO - root -   Epoch: [32/500][340/346], lr: 0.00000146 	 loss = 2.1835(1.2089)
2023/09/14 19:58:35 - INFO - root -   Epoch: [32/500] 	 loss = 1.2060
2023/09/14 19:58:35 - INFO - root -   train_accuracy = 0.5043
2023/09/14 19:58:43 - INFO - root -   Epoch: [33/500][0/346], lr: 0.00000147 	 loss = 1.9302(1.9302)
2023/09/14 19:58:59 - INFO - root -   Epoch: [33/500][20/346], lr: 0.00000147 	 loss = 1.6250(1.2075)
2023/09/14 19:59:20 - INFO - root -   Epoch: [33/500][40/346], lr: 0.00000147 	 loss = 1.2749(1.2331)
2023/09/14 19:59:37 - INFO - root -   Epoch: [33/500][60/346], lr: 0.00000147 	 loss = 0.8692(1.2070)
2023/09/14 19:59:58 - INFO - root -   Epoch: [33/500][80/346], lr: 0.00000147 	 loss = 1.1083(1.2327)
2023/09/14 20:00:15 - INFO - root -   Epoch: [33/500][100/346], lr: 0.00000147 	 loss = 0.9803(1.2392)
2023/09/14 20:00:35 - INFO - root -   Epoch: [33/500][120/346], lr: 0.00000147 	 loss = 0.9364(1.2482)
2023/09/14 20:00:52 - INFO - root -   Epoch: [33/500][140/346], lr: 0.00000147 	 loss = 2.0224(1.2508)
2023/09/14 20:01:13 - INFO - root -   Epoch: [33/500][160/346], lr: 0.00000147 	 loss = 0.5136(1.2333)
2023/09/14 20:01:29 - INFO - root -   Epoch: [33/500][180/346], lr: 0.00000147 	 loss = 1.0545(1.2296)
2023/09/14 20:01:50 - INFO - root -   Epoch: [33/500][200/346], lr: 0.00000147 	 loss = 0.4862(1.2147)
2023/09/14 20:02:07 - INFO - root -   Epoch: [33/500][220/346], lr: 0.00000147 	 loss = 1.3802(1.2143)
2023/09/14 20:02:28 - INFO - root -   Epoch: [33/500][240/346], lr: 0.00000147 	 loss = 0.7424(1.2080)
2023/09/14 20:02:44 - INFO - root -   Epoch: [33/500][260/346], lr: 0.00000147 	 loss = 0.6574(1.2097)
2023/09/14 20:03:05 - INFO - root -   Epoch: [33/500][280/346], lr: 0.00000147 	 loss = 1.6393(1.1986)
2023/09/14 20:03:22 - INFO - root -   Epoch: [33/500][300/346], lr: 0.00000147 	 loss = 2.0962(1.1974)
2023/09/14 20:03:43 - INFO - root -   Epoch: [33/500][320/346], lr: 0.00000147 	 loss = 0.7119(1.2016)
2023/09/14 20:03:59 - INFO - root -   Epoch: [33/500][340/346], lr: 0.00000147 	 loss = 1.4783(1.2012)
2023/09/14 20:04:01 - INFO - root -   Epoch: [33/500] 	 loss = 1.1987
2023/09/14 20:04:01 - INFO - root -   train_accuracy = 0.5145
2023/09/14 20:04:10 - INFO - root -   Epoch: [34/500][0/346], lr: 0.00000149 	 loss = 2.0832(2.0832)
2023/09/14 20:04:26 - INFO - root -   Epoch: [34/500][20/346], lr: 0.00000149 	 loss = 2.1930(1.2528)
2023/09/14 20:04:47 - INFO - root -   Epoch: [34/500][40/346], lr: 0.00000149 	 loss = 1.3881(1.2329)
2023/09/14 20:05:04 - INFO - root -   Epoch: [34/500][60/346], lr: 0.00000149 	 loss = 0.6616(1.2047)
2023/09/14 20:05:25 - INFO - root -   Epoch: [34/500][80/346], lr: 0.00000149 	 loss = 0.9625(1.2492)
2023/09/14 20:05:41 - INFO - root -   Epoch: [34/500][100/346], lr: 0.00000149 	 loss = 0.7869(1.2437)
2023/09/14 20:06:02 - INFO - root -   Epoch: [34/500][120/346], lr: 0.00000149 	 loss = 0.8572(1.2529)
2023/09/14 20:06:19 - INFO - root -   Epoch: [34/500][140/346], lr: 0.00000149 	 loss = 1.7202(1.2424)
2023/09/14 20:06:40 - INFO - root -   Epoch: [34/500][160/346], lr: 0.00000149 	 loss = 0.8247(1.2275)
2023/09/14 20:06:57 - INFO - root -   Epoch: [34/500][180/346], lr: 0.00000149 	 loss = 1.1758(1.2069)
2023/09/14 20:07:18 - INFO - root -   Epoch: [34/500][200/346], lr: 0.00000149 	 loss = 0.7016(1.1933)
2023/09/14 20:07:34 - INFO - root -   Epoch: [34/500][220/346], lr: 0.00000149 	 loss = 1.5257(1.1927)
2023/09/14 20:07:55 - INFO - root -   Epoch: [34/500][240/346], lr: 0.00000149 	 loss = 0.9659(1.1997)
2023/09/14 20:08:12 - INFO - root -   Epoch: [34/500][260/346], lr: 0.00000149 	 loss = 0.7831(1.1953)
2023/09/14 20:08:33 - INFO - root -   Epoch: [34/500][280/346], lr: 0.00000149 	 loss = 1.7634(1.1841)
2023/09/14 20:08:49 - INFO - root -   Epoch: [34/500][300/346], lr: 0.00000149 	 loss = 2.0275(1.1880)
2023/09/14 20:09:10 - INFO - root -   Epoch: [34/500][320/346], lr: 0.00000149 	 loss = 0.5707(1.1872)
2023/09/14 20:09:27 - INFO - root -   Epoch: [34/500][340/346], lr: 0.00000149 	 loss = 1.6051(1.1905)
2023/09/14 20:09:29 - INFO - root -   Epoch: [34/500] 	 loss = 1.1894
2023/09/14 20:10:51 - INFO - root -   precision = 0.5345
2023/09/14 20:10:51 - INFO - root -   eval_loss = 1.1527
2023/09/14 20:10:52 - INFO - root -   train_accuracy = 0.5116
2023/09/14 20:11:00 - INFO - root -   Epoch: [35/500][0/346], lr: 0.00000150 	 loss = 1.9562(1.9562)
2023/09/14 20:11:17 - INFO - root -   Epoch: [35/500][20/346], lr: 0.00000150 	 loss = 1.9379(1.3090)
2023/09/14 20:11:38 - INFO - root -   Epoch: [35/500][40/346], lr: 0.00000150 	 loss = 1.4709(1.2650)
2023/09/14 20:11:55 - INFO - root -   Epoch: [35/500][60/346], lr: 0.00000150 	 loss = 1.0150(1.2295)
2023/09/14 20:12:15 - INFO - root -   Epoch: [35/500][80/346], lr: 0.00000150 	 loss = 1.2975(1.2441)
2023/09/14 20:12:31 - INFO - root -   Epoch: [35/500][100/346], lr: 0.00000150 	 loss = 0.6559(1.2404)
2023/09/14 20:12:52 - INFO - root -   Epoch: [35/500][120/346], lr: 0.00000150 	 loss = 0.6969(1.2399)
2023/09/14 20:13:08 - INFO - root -   Epoch: [35/500][140/346], lr: 0.00000150 	 loss = 1.9445(1.2250)
2023/09/14 20:13:29 - INFO - root -   Epoch: [35/500][160/346], lr: 0.00000150 	 loss = 0.5373(1.1984)
2023/09/14 20:13:46 - INFO - root -   Epoch: [35/500][180/346], lr: 0.00000150 	 loss = 0.8301(1.1871)
2023/09/14 20:14:07 - INFO - root -   Epoch: [35/500][200/346], lr: 0.00000150 	 loss = 0.5393(1.1724)
2023/09/14 20:14:23 - INFO - root -   Epoch: [35/500][220/346], lr: 0.00000150 	 loss = 1.5087(1.1813)
2023/09/14 20:14:44 - INFO - root -   Epoch: [35/500][240/346], lr: 0.00000150 	 loss = 0.8225(1.1860)
2023/09/14 20:15:00 - INFO - root -   Epoch: [35/500][260/346], lr: 0.00000150 	 loss = 0.9391(1.1881)
2023/09/14 20:15:21 - INFO - root -   Epoch: [35/500][280/346], lr: 0.00000150 	 loss = 2.0941(1.1815)
2023/09/14 20:15:38 - INFO - root -   Epoch: [35/500][300/346], lr: 0.00000150 	 loss = 2.2517(1.1785)
2023/09/14 20:15:58 - INFO - root -   Epoch: [35/500][320/346], lr: 0.00000150 	 loss = 0.5855(1.1853)
2023/09/14 20:16:14 - INFO - root -   Epoch: [35/500][340/346], lr: 0.00000150 	 loss = 2.2261(1.1846)
2023/09/14 20:16:16 - INFO - root -   Epoch: [35/500] 	 loss = 1.1811
2023/09/14 20:16:16 - INFO - root -   train_accuracy = 0.5072
2023/09/14 20:16:24 - INFO - root -   Epoch: [36/500][0/346], lr: 0.00000152 	 loss = 1.8105(1.8105)
2023/09/14 20:16:41 - INFO - root -   Epoch: [36/500][20/346], lr: 0.00000152 	 loss = 1.8507(1.2296)
2023/09/14 20:17:02 - INFO - root -   Epoch: [36/500][40/346], lr: 0.00000152 	 loss = 1.5843(1.2185)
2023/09/14 20:17:18 - INFO - root -   Epoch: [36/500][60/346], lr: 0.00000152 	 loss = 0.6112(1.1979)
2023/09/14 20:17:39 - INFO - root -   Epoch: [36/500][80/346], lr: 0.00000152 	 loss = 1.1306(1.2271)
2023/09/14 20:17:55 - INFO - root -   Epoch: [36/500][100/346], lr: 0.00000152 	 loss = 0.6719(1.2306)
2023/09/14 20:18:16 - INFO - root -   Epoch: [36/500][120/346], lr: 0.00000152 	 loss = 0.9456(1.2386)
2023/09/14 20:18:33 - INFO - root -   Epoch: [36/500][140/346], lr: 0.00000152 	 loss = 1.6193(1.2241)
2023/09/14 20:18:53 - INFO - root -   Epoch: [36/500][160/346], lr: 0.00000152 	 loss = 0.5549(1.1995)
2023/09/14 20:19:10 - INFO - root -   Epoch: [36/500][180/346], lr: 0.00000152 	 loss = 1.1138(1.1914)
2023/09/14 20:19:31 - INFO - root -   Epoch: [36/500][200/346], lr: 0.00000152 	 loss = 0.4711(1.1789)
2023/09/14 20:19:47 - INFO - root -   Epoch: [36/500][220/346], lr: 0.00000152 	 loss = 1.4277(1.1803)
2023/09/14 20:20:08 - INFO - root -   Epoch: [36/500][240/346], lr: 0.00000152 	 loss = 0.7252(1.1782)
2023/09/14 20:20:24 - INFO - root -   Epoch: [36/500][260/346], lr: 0.00000152 	 loss = 0.7037(1.1764)
2023/09/14 20:20:45 - INFO - root -   Epoch: [36/500][280/346], lr: 0.00000152 	 loss = 1.3458(1.1662)
2023/09/14 20:21:02 - INFO - root -   Epoch: [36/500][300/346], lr: 0.00000152 	 loss = 1.8485(1.1615)
2023/09/14 20:21:23 - INFO - root -   Epoch: [36/500][320/346], lr: 0.00000152 	 loss = 0.5426(1.1714)
2023/09/14 20:21:39 - INFO - root -   Epoch: [36/500][340/346], lr: 0.00000152 	 loss = 1.6669(1.1723)
2023/09/14 20:21:41 - INFO - root -   Epoch: [36/500] 	 loss = 1.1690
2023/09/14 20:21:41 - INFO - root -   train_accuracy = 0.5260
2023/09/14 20:21:49 - INFO - root -   Epoch: [37/500][0/346], lr: 0.00000153 	 loss = 1.4221(1.4221)
2023/09/14 20:22:05 - INFO - root -   Epoch: [37/500][20/346], lr: 0.00000153 	 loss = 1.8149(1.2523)
2023/09/14 20:22:26 - INFO - root -   Epoch: [37/500][40/346], lr: 0.00000153 	 loss = 1.2676(1.1912)
2023/09/14 20:22:43 - INFO - root -   Epoch: [37/500][60/346], lr: 0.00000153 	 loss = 0.7307(1.1827)
2023/09/14 20:23:03 - INFO - root -   Epoch: [37/500][80/346], lr: 0.00000153 	 loss = 1.1949(1.2319)
2023/09/14 20:23:20 - INFO - root -   Epoch: [37/500][100/346], lr: 0.00000153 	 loss = 0.7668(1.2394)
2023/09/14 20:23:40 - INFO - root -   Epoch: [37/500][120/346], lr: 0.00000153 	 loss = 1.2237(1.2591)
2023/09/14 20:23:57 - INFO - root -   Epoch: [37/500][140/346], lr: 0.00000153 	 loss = 1.3001(1.2331)
2023/09/14 20:24:16 - INFO - root -   Epoch: [37/500][160/346], lr: 0.00000153 	 loss = 0.6368(1.2138)
2023/09/14 20:24:34 - INFO - root -   Epoch: [37/500][180/346], lr: 0.00000153 	 loss = 1.0330(1.2006)
2023/09/14 20:24:53 - INFO - root -   Epoch: [37/500][200/346], lr: 0.00000153 	 loss = 0.5238(1.1855)
2023/09/14 20:25:11 - INFO - root -   Epoch: [37/500][220/346], lr: 0.00000153 	 loss = 1.2672(1.1903)
2023/09/14 20:25:29 - INFO - root -   Epoch: [37/500][240/346], lr: 0.00000153 	 loss = 0.8357(1.1931)
2023/09/14 20:25:49 - INFO - root -   Epoch: [37/500][260/346], lr: 0.00000153 	 loss = 0.8818(1.1969)
2023/09/14 20:26:06 - INFO - root -   Epoch: [37/500][280/346], lr: 0.00000153 	 loss = 1.6226(1.1849)
2023/09/14 20:26:26 - INFO - root -   Epoch: [37/500][300/346], lr: 0.00000153 	 loss = 1.7055(1.1855)
2023/09/14 20:26:44 - INFO - root -   Epoch: [37/500][320/346], lr: 0.00000153 	 loss = 0.6054(1.1860)
2023/09/14 20:27:03 - INFO - root -   Epoch: [37/500][340/346], lr: 0.00000153 	 loss = 2.1924(1.1886)
2023/09/14 20:27:05 - INFO - root -   Epoch: [37/500] 	 loss = 1.1878
2023/09/14 20:27:05 - INFO - root -   train_accuracy = 0.4957
2023/09/14 20:27:13 - INFO - root -   Epoch: [38/500][0/346], lr: 0.00000154 	 loss = 1.7426(1.7426)
2023/09/14 20:27:30 - INFO - root -   Epoch: [38/500][20/346], lr: 0.00000154 	 loss = 2.2137(1.2249)
2023/09/14 20:27:51 - INFO - root -   Epoch: [38/500][40/346], lr: 0.00000154 	 loss = 1.1389(1.2074)
2023/09/14 20:28:08 - INFO - root -   Epoch: [38/500][60/346], lr: 0.00000154 	 loss = 0.7330(1.2021)
2023/09/14 20:28:28 - INFO - root -   Epoch: [38/500][80/346], lr: 0.00000154 	 loss = 1.0736(1.2389)
2023/09/14 20:28:45 - INFO - root -   Epoch: [38/500][100/346], lr: 0.00000154 	 loss = 1.1312(1.2458)
2023/09/14 20:29:06 - INFO - root -   Epoch: [38/500][120/346], lr: 0.00000154 	 loss = 0.9840(1.2539)
2023/09/14 20:29:23 - INFO - root -   Epoch: [38/500][140/346], lr: 0.00000154 	 loss = 1.8761(1.2466)
2023/09/14 20:29:44 - INFO - root -   Epoch: [38/500][160/346], lr: 0.00000154 	 loss = 0.6936(1.2206)
2023/09/14 20:30:00 - INFO - root -   Epoch: [38/500][180/346], lr: 0.00000154 	 loss = 0.9261(1.2062)
2023/09/14 20:30:21 - INFO - root -   Epoch: [38/500][200/346], lr: 0.00000154 	 loss = 0.3691(1.1921)
2023/09/14 20:30:38 - INFO - root -   Epoch: [38/500][220/346], lr: 0.00000154 	 loss = 1.3630(1.1923)
2023/09/14 20:30:59 - INFO - root -   Epoch: [38/500][240/346], lr: 0.00000154 	 loss = 1.1221(1.1966)
2023/09/14 20:31:15 - INFO - root -   Epoch: [38/500][260/346], lr: 0.00000154 	 loss = 0.7784(1.1998)
2023/09/14 20:31:36 - INFO - root -   Epoch: [38/500][280/346], lr: 0.00000154 	 loss = 2.0375(1.1919)
2023/09/14 20:31:53 - INFO - root -   Epoch: [38/500][300/346], lr: 0.00000154 	 loss = 1.8783(1.1949)
2023/09/14 20:32:14 - INFO - root -   Epoch: [38/500][320/346], lr: 0.00000154 	 loss = 0.4161(1.1974)
2023/09/14 20:32:30 - INFO - root -   Epoch: [38/500][340/346], lr: 0.00000154 	 loss = 2.4726(1.1987)
2023/09/14 20:32:33 - INFO - root -   Epoch: [38/500] 	 loss = 1.1953
2023/09/14 20:32:33 - INFO - root -   train_accuracy = 0.5246
2023/09/14 20:32:41 - INFO - root -   Epoch: [39/500][0/346], lr: 0.00000156 	 loss = 1.5133(1.5133)
2023/09/14 20:32:57 - INFO - root -   Epoch: [39/500][20/346], lr: 0.00000156 	 loss = 1.8844(1.2814)
2023/09/14 20:33:18 - INFO - root -   Epoch: [39/500][40/346], lr: 0.00000156 	 loss = 1.2660(1.2227)
2023/09/14 20:33:35 - INFO - root -   Epoch: [39/500][60/346], lr: 0.00000156 	 loss = 0.7773(1.2013)
2023/09/14 20:33:55 - INFO - root -   Epoch: [39/500][80/346], lr: 0.00000156 	 loss = 0.9550(1.2359)
2023/09/14 20:34:12 - INFO - root -   Epoch: [39/500][100/346], lr: 0.00000156 	 loss = 0.9530(1.2457)
2023/09/14 20:34:33 - INFO - root -   Epoch: [39/500][120/346], lr: 0.00000156 	 loss = 1.3917(1.2500)
2023/09/14 20:34:49 - INFO - root -   Epoch: [39/500][140/346], lr: 0.00000156 	 loss = 1.6043(1.2376)
2023/09/14 20:35:10 - INFO - root -   Epoch: [39/500][160/346], lr: 0.00000156 	 loss = 0.6412(1.2156)
2023/09/14 20:35:27 - INFO - root -   Epoch: [39/500][180/346], lr: 0.00000156 	 loss = 1.1937(1.2058)
2023/09/14 20:35:48 - INFO - root -   Epoch: [39/500][200/346], lr: 0.00000156 	 loss = 0.4972(1.1950)
2023/09/14 20:36:05 - INFO - root -   Epoch: [39/500][220/346], lr: 0.00000156 	 loss = 1.5644(1.1962)
2023/09/14 20:36:26 - INFO - root -   Epoch: [39/500][240/346], lr: 0.00000156 	 loss = 0.8680(1.1959)
2023/09/14 20:36:42 - INFO - root -   Epoch: [39/500][260/346], lr: 0.00000156 	 loss = 0.9660(1.2002)
2023/09/14 20:37:03 - INFO - root -   Epoch: [39/500][280/346], lr: 0.00000156 	 loss = 2.0160(1.1922)
2023/09/14 20:37:20 - INFO - root -   Epoch: [39/500][300/346], lr: 0.00000156 	 loss = 2.0062(1.1907)
2023/09/14 20:37:41 - INFO - root -   Epoch: [39/500][320/346], lr: 0.00000156 	 loss = 0.5498(1.1924)
2023/09/14 20:37:58 - INFO - root -   Epoch: [39/500][340/346], lr: 0.00000156 	 loss = 1.5321(1.1913)
2023/09/14 20:38:00 - INFO - root -   Epoch: [39/500] 	 loss = 1.1871
2023/09/14 20:39:22 - INFO - root -   precision = 0.5345
2023/09/14 20:39:22 - INFO - root -   eval_loss = 1.1568
2023/09/14 20:39:23 - INFO - root -   train_accuracy = 0.5188
2023/09/14 20:39:31 - INFO - root -   Epoch: [40/500][0/346], lr: 0.00000157 	 loss = 1.6647(1.6647)
2023/09/14 20:39:48 - INFO - root -   Epoch: [40/500][20/346], lr: 0.00000157 	 loss = 1.7528(1.1408)
2023/09/14 20:40:09 - INFO - root -   Epoch: [40/500][40/346], lr: 0.00000157 	 loss = 1.1874(1.1630)
2023/09/14 20:40:25 - INFO - root -   Epoch: [40/500][60/346], lr: 0.00000157 	 loss = 0.7420(1.1627)
2023/09/14 20:40:46 - INFO - root -   Epoch: [40/500][80/346], lr: 0.00000157 	 loss = 1.0636(1.2096)
2023/09/14 20:41:02 - INFO - root -   Epoch: [40/500][100/346], lr: 0.00000157 	 loss = 0.7106(1.2224)
2023/09/14 20:41:23 - INFO - root -   Epoch: [40/500][120/346], lr: 0.00000157 	 loss = 1.3489(1.2133)
2023/09/14 20:41:40 - INFO - root -   Epoch: [40/500][140/346], lr: 0.00000157 	 loss = 1.6331(1.2056)
2023/09/14 20:42:01 - INFO - root -   Epoch: [40/500][160/346], lr: 0.00000157 	 loss = 0.6995(1.1886)
2023/09/14 20:42:17 - INFO - root -   Epoch: [40/500][180/346], lr: 0.00000157 	 loss = 1.3177(1.1818)
2023/09/14 20:42:38 - INFO - root -   Epoch: [40/500][200/346], lr: 0.00000157 	 loss = 0.5672(1.1726)
2023/09/14 20:42:55 - INFO - root -   Epoch: [40/500][220/346], lr: 0.00000157 	 loss = 1.8457(1.1776)
2023/09/14 20:43:15 - INFO - root -   Epoch: [40/500][240/346], lr: 0.00000157 	 loss = 0.8693(1.1826)
2023/09/14 20:43:32 - INFO - root -   Epoch: [40/500][260/346], lr: 0.00000157 	 loss = 0.8535(1.1846)
2023/09/14 20:43:53 - INFO - root -   Epoch: [40/500][280/346], lr: 0.00000157 	 loss = 1.2522(1.1736)
2023/09/14 20:44:10 - INFO - root -   Epoch: [40/500][300/346], lr: 0.00000157 	 loss = 1.9315(1.1721)
2023/09/14 20:44:31 - INFO - root -   Epoch: [40/500][320/346], lr: 0.00000157 	 loss = 0.3951(1.1750)
2023/09/14 20:44:47 - INFO - root -   Epoch: [40/500][340/346], lr: 0.00000157 	 loss = 1.6984(1.1761)
2023/09/14 20:44:49 - INFO - root -   Epoch: [40/500] 	 loss = 1.1744
2023/09/14 20:44:49 - INFO - root -   train_accuracy = 0.5159
2023/09/14 20:44:57 - INFO - root -   Epoch: [41/500][0/346], lr: 0.00000159 	 loss = 1.6396(1.6396)
2023/09/14 20:45:14 - INFO - root -   Epoch: [41/500][20/346], lr: 0.00000159 	 loss = 1.9829(1.2244)
2023/09/14 20:45:35 - INFO - root -   Epoch: [41/500][40/346], lr: 0.00000159 	 loss = 1.5794(1.2014)
2023/09/14 20:45:52 - INFO - root -   Epoch: [41/500][60/346], lr: 0.00000159 	 loss = 0.6227(1.1978)
2023/09/14 20:46:12 - INFO - root -   Epoch: [41/500][80/346], lr: 0.00000159 	 loss = 1.0536(1.2288)
2023/09/14 20:46:29 - INFO - root -   Epoch: [41/500][100/346], lr: 0.00000159 	 loss = 0.7248(1.2321)
2023/09/14 20:46:50 - INFO - root -   Epoch: [41/500][120/346], lr: 0.00000159 	 loss = 0.9505(1.2432)
2023/09/14 20:47:07 - INFO - root -   Epoch: [41/500][140/346], lr: 0.00000159 	 loss = 1.8693(1.2423)
2023/09/14 20:47:28 - INFO - root -   Epoch: [41/500][160/346], lr: 0.00000159 	 loss = 0.6373(1.2281)
2023/09/14 20:47:44 - INFO - root -   Epoch: [41/500][180/346], lr: 0.00000159 	 loss = 0.9678(1.2190)
2023/09/14 20:48:05 - INFO - root -   Epoch: [41/500][200/346], lr: 0.00000159 	 loss = 0.4610(1.2051)
2023/09/14 20:48:22 - INFO - root -   Epoch: [41/500][220/346], lr: 0.00000159 	 loss = 1.2738(1.2051)
2023/09/14 20:48:43 - INFO - root -   Epoch: [41/500][240/346], lr: 0.00000159 	 loss = 0.9113(1.2018)
2023/09/14 20:49:00 - INFO - root -   Epoch: [41/500][260/346], lr: 0.00000159 	 loss = 0.8441(1.2069)
2023/09/14 20:49:21 - INFO - root -   Epoch: [41/500][280/346], lr: 0.00000159 	 loss = 1.9601(1.1935)
2023/09/14 20:49:37 - INFO - root -   Epoch: [41/500][300/346], lr: 0.00000159 	 loss = 2.0890(1.1907)
2023/09/14 20:49:58 - INFO - root -   Epoch: [41/500][320/346], lr: 0.00000159 	 loss = 0.5877(1.1946)
2023/09/14 20:50:15 - INFO - root -   Epoch: [41/500][340/346], lr: 0.00000159 	 loss = 2.0139(1.1917)
2023/09/14 20:50:17 - INFO - root -   Epoch: [41/500] 	 loss = 1.1896
2023/09/14 20:50:17 - INFO - root -   train_accuracy = 0.5275
2023/09/14 20:50:25 - INFO - root -   Epoch: [42/500][0/346], lr: 0.00000160 	 loss = 1.8111(1.8111)
2023/09/14 20:50:42 - INFO - root -   Epoch: [42/500][20/346], lr: 0.00000160 	 loss = 2.2833(1.2377)
2023/09/14 20:51:03 - INFO - root -   Epoch: [42/500][40/346], lr: 0.00000160 	 loss = 1.2773(1.2033)
2023/09/14 20:51:19 - INFO - root -   Epoch: [42/500][60/346], lr: 0.00000160 	 loss = 0.7716(1.1866)
2023/09/14 20:51:40 - INFO - root -   Epoch: [42/500][80/346], lr: 0.00000160 	 loss = 1.0707(1.2133)
2023/09/14 20:51:57 - INFO - root -   Epoch: [42/500][100/346], lr: 0.00000160 	 loss = 0.6586(1.2190)
2023/09/14 20:52:18 - INFO - root -   Epoch: [42/500][120/346], lr: 0.00000160 	 loss = 0.9838(1.2272)
2023/09/14 20:52:34 - INFO - root -   Epoch: [42/500][140/346], lr: 0.00000160 	 loss = 1.9508(1.2133)
2023/09/14 20:52:55 - INFO - root -   Epoch: [42/500][160/346], lr: 0.00000160 	 loss = 0.5585(1.1968)
2023/09/14 20:53:12 - INFO - root -   Epoch: [42/500][180/346], lr: 0.00000160 	 loss = 0.9301(1.1897)
2023/09/14 20:53:33 - INFO - root -   Epoch: [42/500][200/346], lr: 0.00000160 	 loss = 0.6438(1.1794)
2023/09/14 20:53:49 - INFO - root -   Epoch: [42/500][220/346], lr: 0.00000160 	 loss = 1.5426(1.1832)
2023/09/14 20:54:10 - INFO - root -   Epoch: [42/500][240/346], lr: 0.00000160 	 loss = 0.9880(1.1888)
2023/09/14 20:54:26 - INFO - root -   Epoch: [42/500][260/346], lr: 0.00000160 	 loss = 0.5776(1.1911)
2023/09/14 20:54:47 - INFO - root -   Epoch: [42/500][280/346], lr: 0.00000160 	 loss = 2.2382(1.1892)
2023/09/14 20:55:04 - INFO - root -   Epoch: [42/500][300/346], lr: 0.00000160 	 loss = 2.2450(1.1888)
2023/09/14 20:55:24 - INFO - root -   Epoch: [42/500][320/346], lr: 0.00000160 	 loss = 0.8095(1.1890)
2023/09/14 20:55:40 - INFO - root -   Epoch: [42/500][340/346], lr: 0.00000160 	 loss = 1.8960(1.1911)
2023/09/14 20:55:43 - INFO - root -   Epoch: [42/500] 	 loss = 1.1899
2023/09/14 20:55:43 - INFO - root -   train_accuracy = 0.5159
2023/09/14 20:55:51 - INFO - root -   Epoch: [43/500][0/346], lr: 0.00000162 	 loss = 2.0036(2.0036)
2023/09/14 20:56:07 - INFO - root -   Epoch: [43/500][20/346], lr: 0.00000162 	 loss = 1.9256(1.1855)
2023/09/14 20:56:28 - INFO - root -   Epoch: [43/500][40/346], lr: 0.00000162 	 loss = 1.2525(1.1715)
2023/09/14 20:56:45 - INFO - root -   Epoch: [43/500][60/346], lr: 0.00000162 	 loss = 0.7014(1.1657)
2023/09/14 20:57:05 - INFO - root -   Epoch: [43/500][80/346], lr: 0.00000162 	 loss = 1.0688(1.2083)
2023/09/14 20:57:22 - INFO - root -   Epoch: [43/500][100/346], lr: 0.00000162 	 loss = 0.6404(1.2095)
2023/09/14 20:57:42 - INFO - root -   Epoch: [43/500][120/346], lr: 0.00000162 	 loss = 0.9575(1.2214)
2023/09/14 20:57:59 - INFO - root -   Epoch: [43/500][140/346], lr: 0.00000162 	 loss = 2.0204(1.2211)
2023/09/14 20:58:19 - INFO - root -   Epoch: [43/500][160/346], lr: 0.00000162 	 loss = 0.5275(1.2085)
2023/09/14 20:58:36 - INFO - root -   Epoch: [43/500][180/346], lr: 0.00000162 	 loss = 1.1602(1.2098)
2023/09/14 20:58:55 - INFO - root -   Epoch: [43/500][200/346], lr: 0.00000162 	 loss = 0.4895(1.1999)
2023/09/14 20:59:13 - INFO - root -   Epoch: [43/500][220/346], lr: 0.00000162 	 loss = 1.5935(1.2076)
2023/09/14 20:59:32 - INFO - root -   Epoch: [43/500][240/346], lr: 0.00000162 	 loss = 0.9927(1.2095)
2023/09/14 20:59:50 - INFO - root -   Epoch: [43/500][260/346], lr: 0.00000162 	 loss = 0.9111(1.2109)
2023/09/14 21:00:09 - INFO - root -   Epoch: [43/500][280/346], lr: 0.00000162 	 loss = 1.7271(1.2012)
2023/09/14 21:00:28 - INFO - root -   Epoch: [43/500][300/346], lr: 0.00000162 	 loss = 2.6641(1.1962)
2023/09/14 21:00:46 - INFO - root -   Epoch: [43/500][320/346], lr: 0.00000162 	 loss = 0.4692(1.1977)
2023/09/14 21:01:05 - INFO - root -   Epoch: [43/500][340/346], lr: 0.00000162 	 loss = 1.9379(1.1986)
2023/09/14 21:01:07 - INFO - root -   Epoch: [43/500] 	 loss = 1.1958
2023/09/14 21:01:07 - INFO - root -   train_accuracy = 0.5072
2023/09/14 21:01:15 - INFO - root -   Epoch: [44/500][0/346], lr: 0.00000163 	 loss = 1.5822(1.5822)
2023/09/14 21:01:32 - INFO - root -   Epoch: [44/500][20/346], lr: 0.00000163 	 loss = 2.1933(1.2409)
2023/09/14 21:01:53 - INFO - root -   Epoch: [44/500][40/346], lr: 0.00000163 	 loss = 1.1813(1.1982)
2023/09/14 21:02:09 - INFO - root -   Epoch: [44/500][60/346], lr: 0.00000163 	 loss = 0.7596(1.1958)
2023/09/14 21:02:30 - INFO - root -   Epoch: [44/500][80/346], lr: 0.00000163 	 loss = 1.2102(1.2147)
2023/09/14 21:02:47 - INFO - root -   Epoch: [44/500][100/346], lr: 0.00000163 	 loss = 0.8487(1.2204)
2023/09/14 21:03:08 - INFO - root -   Epoch: [44/500][120/346], lr: 0.00000163 	 loss = 1.0122(1.2360)
2023/09/14 21:03:24 - INFO - root -   Epoch: [44/500][140/346], lr: 0.00000163 	 loss = 1.6495(1.2222)
2023/09/14 21:03:45 - INFO - root -   Epoch: [44/500][160/346], lr: 0.00000163 	 loss = 0.5902(1.1953)
2023/09/14 21:04:01 - INFO - root -   Epoch: [44/500][180/346], lr: 0.00000163 	 loss = 1.1446(1.1858)
2023/09/14 21:04:22 - INFO - root -   Epoch: [44/500][200/346], lr: 0.00000163 	 loss = 0.5325(1.1700)
2023/09/14 21:04:39 - INFO - root -   Epoch: [44/500][220/346], lr: 0.00000163 	 loss = 1.5272(1.1760)
2023/09/14 21:04:59 - INFO - root -   Epoch: [44/500][240/346], lr: 0.00000163 	 loss = 1.2461(1.1773)
2023/09/14 21:05:16 - INFO - root -   Epoch: [44/500][260/346], lr: 0.00000163 	 loss = 0.7317(1.1776)
2023/09/14 21:05:36 - INFO - root -   Epoch: [44/500][280/346], lr: 0.00000163 	 loss = 1.7082(1.1677)
2023/09/14 21:05:53 - INFO - root -   Epoch: [44/500][300/346], lr: 0.00000163 	 loss = 2.0139(1.1628)
2023/09/14 21:06:13 - INFO - root -   Epoch: [44/500][320/346], lr: 0.00000163 	 loss = 0.4756(1.1652)
2023/09/14 21:06:29 - INFO - root -   Epoch: [44/500][340/346], lr: 0.00000163 	 loss = 2.4057(1.1678)
2023/09/14 21:06:31 - INFO - root -   Epoch: [44/500] 	 loss = 1.1661
2023/09/14 21:07:54 - INFO - root -   precision = 0.5345
2023/09/14 21:07:54 - INFO - root -   eval_loss = 1.1506
2023/09/14 21:07:55 - INFO - root -   train_accuracy = 0.5260
2023/09/14 21:08:03 - INFO - root -   Epoch: [45/500][0/346], lr: 0.00000164 	 loss = 1.7976(1.7976)
2023/09/14 21:08:20 - INFO - root -   Epoch: [45/500][20/346], lr: 0.00000164 	 loss = 2.1787(1.2216)
2023/09/14 21:08:40 - INFO - root -   Epoch: [45/500][40/346], lr: 0.00000164 	 loss = 1.1763(1.2062)
2023/09/14 21:08:57 - INFO - root -   Epoch: [45/500][60/346], lr: 0.00000164 	 loss = 0.8142(1.2060)
2023/09/14 21:09:18 - INFO - root -   Epoch: [45/500][80/346], lr: 0.00000164 	 loss = 0.9057(1.2168)
2023/09/14 21:09:34 - INFO - root -   Epoch: [45/500][100/346], lr: 0.00000164 	 loss = 0.6379(1.2279)
2023/09/14 21:09:55 - INFO - root -   Epoch: [45/500][120/346], lr: 0.00000164 	 loss = 1.1379(1.2307)
2023/09/14 21:10:12 - INFO - root -   Epoch: [45/500][140/346], lr: 0.00000164 	 loss = 1.7512(1.2207)
2023/09/14 21:10:33 - INFO - root -   Epoch: [45/500][160/346], lr: 0.00000164 	 loss = 0.3836(1.1936)
2023/09/14 21:10:49 - INFO - root -   Epoch: [45/500][180/346], lr: 0.00000164 	 loss = 1.3912(1.1892)
2023/09/14 21:11:10 - INFO - root -   Epoch: [45/500][200/346], lr: 0.00000164 	 loss = 0.4243(1.1784)
2023/09/14 21:11:27 - INFO - root -   Epoch: [45/500][220/346], lr: 0.00000164 	 loss = 1.6316(1.1802)
2023/09/14 21:11:48 - INFO - root -   Epoch: [45/500][240/346], lr: 0.00000164 	 loss = 0.7634(1.1783)
2023/09/14 21:12:04 - INFO - root -   Epoch: [45/500][260/346], lr: 0.00000164 	 loss = 0.8365(1.1789)
2023/09/14 21:12:25 - INFO - root -   Epoch: [45/500][280/346], lr: 0.00000164 	 loss = 1.5624(1.1671)
2023/09/14 21:12:41 - INFO - root -   Epoch: [45/500][300/346], lr: 0.00000164 	 loss = 2.2427(1.1661)
2023/09/14 21:13:02 - INFO - root -   Epoch: [45/500][320/346], lr: 0.00000164 	 loss = 0.5204(1.1709)
2023/09/14 21:13:18 - INFO - root -   Epoch: [45/500][340/346], lr: 0.00000164 	 loss = 1.8062(1.1738)
2023/09/14 21:13:20 - INFO - root -   Epoch: [45/500] 	 loss = 1.1706
2023/09/14 21:13:20 - INFO - root -   train_accuracy = 0.5246
2023/09/14 21:13:29 - INFO - root -   Epoch: [46/500][0/346], lr: 0.00000166 	 loss = 1.5698(1.5698)
2023/09/14 21:13:45 - INFO - root -   Epoch: [46/500][20/346], lr: 0.00000166 	 loss = 2.2088(1.2127)
2023/09/14 21:14:06 - INFO - root -   Epoch: [46/500][40/346], lr: 0.00000166 	 loss = 1.0822(1.1853)
2023/09/14 21:14:23 - INFO - root -   Epoch: [46/500][60/346], lr: 0.00000166 	 loss = 0.6951(1.1898)
2023/09/14 21:14:44 - INFO - root -   Epoch: [46/500][80/346], lr: 0.00000166 	 loss = 1.1091(1.2266)
2023/09/14 21:15:01 - INFO - root -   Epoch: [46/500][100/346], lr: 0.00000166 	 loss = 0.4951(1.2251)
2023/09/14 21:15:22 - INFO - root -   Epoch: [46/500][120/346], lr: 0.00000166 	 loss = 0.9330(1.2337)
2023/09/14 21:15:38 - INFO - root -   Epoch: [46/500][140/346], lr: 0.00000166 	 loss = 2.4188(1.2263)
2023/09/14 21:15:59 - INFO - root -   Epoch: [46/500][160/346], lr: 0.00000166 	 loss = 0.6605(1.2077)
2023/09/14 21:16:16 - INFO - root -   Epoch: [46/500][180/346], lr: 0.00000166 	 loss = 1.2298(1.1985)
2023/09/14 21:16:37 - INFO - root -   Epoch: [46/500][200/346], lr: 0.00000166 	 loss = 0.5585(1.1881)
2023/09/14 21:16:53 - INFO - root -   Epoch: [46/500][220/346], lr: 0.00000166 	 loss = 1.7000(1.1926)
2023/09/14 21:17:14 - INFO - root -   Epoch: [46/500][240/346], lr: 0.00000166 	 loss = 1.0279(1.1928)
2023/09/14 21:17:31 - INFO - root -   Epoch: [46/500][260/346], lr: 0.00000166 	 loss = 0.8082(1.1954)
2023/09/14 21:17:52 - INFO - root -   Epoch: [46/500][280/346], lr: 0.00000166 	 loss = 1.5677(1.1843)
2023/09/14 21:18:09 - INFO - root -   Epoch: [46/500][300/346], lr: 0.00000166 	 loss = 1.7587(1.1826)
2023/09/14 21:18:30 - INFO - root -   Epoch: [46/500][320/346], lr: 0.00000166 	 loss = 0.6369(1.1840)
2023/09/14 21:18:47 - INFO - root -   Epoch: [46/500][340/346], lr: 0.00000166 	 loss = 1.7583(1.1887)
2023/09/14 21:18:49 - INFO - root -   Epoch: [46/500] 	 loss = 1.1856
2023/09/14 21:18:49 - INFO - root -   train_accuracy = 0.5058
2023/09/14 21:18:57 - INFO - root -   Epoch: [47/500][0/346], lr: 0.00000167 	 loss = 1.7677(1.7677)
2023/09/14 21:19:14 - INFO - root -   Epoch: [47/500][20/346], lr: 0.00000167 	 loss = 2.0992(1.1888)
2023/09/14 21:19:34 - INFO - root -   Epoch: [47/500][40/346], lr: 0.00000167 	 loss = 1.1774(1.1685)
2023/09/14 21:19:51 - INFO - root -   Epoch: [47/500][60/346], lr: 0.00000167 	 loss = 0.7586(1.1767)
2023/09/14 21:20:12 - INFO - root -   Epoch: [47/500][80/346], lr: 0.00000167 	 loss = 1.0581(1.2137)
2023/09/14 21:20:29 - INFO - root -   Epoch: [47/500][100/346], lr: 0.00000167 	 loss = 0.6089(1.2194)
2023/09/14 21:20:49 - INFO - root -   Epoch: [47/500][120/346], lr: 0.00000167 	 loss = 0.9149(1.2302)
2023/09/14 21:21:06 - INFO - root -   Epoch: [47/500][140/346], lr: 0.00000167 	 loss = 1.9298(1.2219)
2023/09/14 21:21:27 - INFO - root -   Epoch: [47/500][160/346], lr: 0.00000167 	 loss = 0.6339(1.1958)
2023/09/14 21:21:44 - INFO - root -   Epoch: [47/500][180/346], lr: 0.00000167 	 loss = 1.1750(1.1857)
2023/09/14 21:22:05 - INFO - root -   Epoch: [47/500][200/346], lr: 0.00000167 	 loss = 0.5841(1.1733)
2023/09/14 21:22:21 - INFO - root -   Epoch: [47/500][220/346], lr: 0.00000167 	 loss = 1.2522(1.1753)
2023/09/14 21:22:42 - INFO - root -   Epoch: [47/500][240/346], lr: 0.00000167 	 loss = 1.0017(1.1820)
2023/09/14 21:22:59 - INFO - root -   Epoch: [47/500][260/346], lr: 0.00000167 	 loss = 0.6271(1.1815)
2023/09/14 21:23:20 - INFO - root -   Epoch: [47/500][280/346], lr: 0.00000167 	 loss = 1.8841(1.1736)
2023/09/14 21:23:36 - INFO - root -   Epoch: [47/500][300/346], lr: 0.00000167 	 loss = 1.8457(1.1770)
2023/09/14 21:23:57 - INFO - root -   Epoch: [47/500][320/346], lr: 0.00000167 	 loss = 0.5617(1.1827)
2023/09/14 21:24:13 - INFO - root -   Epoch: [47/500][340/346], lr: 0.00000167 	 loss = 1.9745(1.1823)
2023/09/14 21:24:15 - INFO - root -   Epoch: [47/500] 	 loss = 1.1796
2023/09/14 21:24:15 - INFO - root -   train_accuracy = 0.5116
2023/09/14 21:24:23 - INFO - root -   Epoch: [48/500][0/346], lr: 0.00000169 	 loss = 1.6072(1.6072)
2023/09/14 21:24:40 - INFO - root -   Epoch: [48/500][20/346], lr: 0.00000169 	 loss = 1.8938(1.1699)
2023/09/14 21:25:00 - INFO - root -   Epoch: [48/500][40/346], lr: 0.00000169 	 loss = 1.1275(1.1835)
2023/09/14 21:25:17 - INFO - root -   Epoch: [48/500][60/346], lr: 0.00000169 	 loss = 0.6595(1.1790)
2023/09/14 21:25:38 - INFO - root -   Epoch: [48/500][80/346], lr: 0.00000169 	 loss = 0.9887(1.2061)
2023/09/14 21:25:54 - INFO - root -   Epoch: [48/500][100/346], lr: 0.00000169 	 loss = 0.6780(1.2067)
2023/09/14 21:26:14 - INFO - root -   Epoch: [48/500][120/346], lr: 0.00000169 	 loss = 1.5356(1.2198)
2023/09/14 21:26:31 - INFO - root -   Epoch: [48/500][140/346], lr: 0.00000169 	 loss = 1.7470(1.2185)
2023/09/14 21:26:51 - INFO - root -   Epoch: [48/500][160/346], lr: 0.00000169 	 loss = 0.5571(1.1975)
2023/09/14 21:27:09 - INFO - root -   Epoch: [48/500][180/346], lr: 0.00000169 	 loss = 1.2491(1.1893)
2023/09/14 21:27:27 - INFO - root -   Epoch: [48/500][200/346], lr: 0.00000169 	 loss = 0.5605(1.1814)
2023/09/14 21:27:46 - INFO - root -   Epoch: [48/500][220/346], lr: 0.00000169 	 loss = 1.5028(1.1784)
2023/09/14 21:28:04 - INFO - root -   Epoch: [48/500][240/346], lr: 0.00000169 	 loss = 0.9902(1.1836)
2023/09/14 21:28:23 - INFO - root -   Epoch: [48/500][260/346], lr: 0.00000169 	 loss = 0.7847(1.1855)
2023/09/14 21:28:41 - INFO - root -   Epoch: [48/500][280/346], lr: 0.00000169 	 loss = 1.6683(1.1766)
2023/09/14 21:29:01 - INFO - root -   Epoch: [48/500][300/346], lr: 0.00000169 	 loss = 2.0755(1.1842)
2023/09/14 21:29:18 - INFO - root -   Epoch: [48/500][320/346], lr: 0.00000169 	 loss = 0.6378(1.1875)
2023/09/14 21:29:38 - INFO - root -   Epoch: [48/500][340/346], lr: 0.00000169 	 loss = 1.8253(1.1907)
2023/09/14 21:29:40 - INFO - root -   Epoch: [48/500] 	 loss = 1.1917
2023/09/14 21:29:40 - INFO - root -   train_accuracy = 0.5130
2023/09/14 21:29:48 - INFO - root -   Epoch: [49/500][0/346], lr: 0.00000170 	 loss = 1.5979(1.5979)
2023/09/14 21:30:05 - INFO - root -   Epoch: [49/500][20/346], lr: 0.00000170 	 loss = 1.8227(1.2167)
2023/09/14 21:30:26 - INFO - root -   Epoch: [49/500][40/346], lr: 0.00000170 	 loss = 1.2489(1.2069)
2023/09/14 21:30:42 - INFO - root -   Epoch: [49/500][60/346], lr: 0.00000170 	 loss = 0.7495(1.2203)
2023/09/14 21:31:03 - INFO - root -   Epoch: [49/500][80/346], lr: 0.00000170 	 loss = 0.9929(1.2336)
2023/09/14 21:31:20 - INFO - root -   Epoch: [49/500][100/346], lr: 0.00000170 	 loss = 0.6511(1.2429)
2023/09/14 21:31:41 - INFO - root -   Epoch: [49/500][120/346], lr: 0.00000170 	 loss = 0.9631(1.2375)
2023/09/14 21:31:57 - INFO - root -   Epoch: [49/500][140/346], lr: 0.00000170 	 loss = 1.6110(1.2354)
2023/09/14 21:32:18 - INFO - root -   Epoch: [49/500][160/346], lr: 0.00000170 	 loss = 0.6411(1.2163)
2023/09/14 21:32:35 - INFO - root -   Epoch: [49/500][180/346], lr: 0.00000170 	 loss = 0.9178(1.2055)
2023/09/14 21:32:56 - INFO - root -   Epoch: [49/500][200/346], lr: 0.00000170 	 loss = 0.4279(1.1894)
2023/09/14 21:33:12 - INFO - root -   Epoch: [49/500][220/346], lr: 0.00000170 	 loss = 1.5750(1.1908)
2023/09/14 21:33:34 - INFO - root -   Epoch: [49/500][240/346], lr: 0.00000170 	 loss = 0.8205(1.1933)
2023/09/14 21:33:50 - INFO - root -   Epoch: [49/500][260/346], lr: 0.00000170 	 loss = 0.8146(1.1928)
2023/09/14 21:34:11 - INFO - root -   Epoch: [49/500][280/346], lr: 0.00000170 	 loss = 1.9348(1.1833)
2023/09/14 21:34:28 - INFO - root -   Epoch: [49/500][300/346], lr: 0.00000170 	 loss = 1.4859(1.1779)
2023/09/14 21:34:49 - INFO - root -   Epoch: [49/500][320/346], lr: 0.00000170 	 loss = 0.6741(1.1791)
2023/09/14 21:35:05 - INFO - root -   Epoch: [49/500][340/346], lr: 0.00000170 	 loss = 1.5216(1.1805)
2023/09/14 21:35:07 - INFO - root -   Epoch: [49/500] 	 loss = 1.1785
2023/09/14 21:36:31 - INFO - root -   precision = 0.5345
2023/09/14 21:36:31 - INFO - root -   eval_loss = 1.1485
2023/09/14 21:36:32 - INFO - root -   train_accuracy = 0.5072
2023/09/14 21:36:40 - INFO - root -   Epoch: [50/500][0/346], lr: 0.00000172 	 loss = 1.3942(1.3942)
2023/09/14 21:36:57 - INFO - root -   Epoch: [50/500][20/346], lr: 0.00000172 	 loss = 2.1081(1.2127)
2023/09/14 21:37:18 - INFO - root -   Epoch: [50/500][40/346], lr: 0.00000172 	 loss = 1.4190(1.2427)
2023/09/14 21:37:34 - INFO - root -   Epoch: [50/500][60/346], lr: 0.00000172 	 loss = 0.6208(1.2155)
2023/09/14 21:37:55 - INFO - root -   Epoch: [50/500][80/346], lr: 0.00000172 	 loss = 1.1541(1.2422)
2023/09/14 21:38:12 - INFO - root -   Epoch: [50/500][100/346], lr: 0.00000172 	 loss = 0.7001(1.2458)
2023/09/14 21:38:33 - INFO - root -   Epoch: [50/500][120/346], lr: 0.00000172 	 loss = 1.0420(1.2489)
2023/09/14 21:38:49 - INFO - root -   Epoch: [50/500][140/346], lr: 0.00000172 	 loss = 1.7849(1.2475)
2023/09/14 21:39:10 - INFO - root -   Epoch: [50/500][160/346], lr: 0.00000172 	 loss = 0.4915(1.2231)
2023/09/14 21:39:27 - INFO - root -   Epoch: [50/500][180/346], lr: 0.00000172 	 loss = 0.9758(1.2094)
2023/09/14 21:39:47 - INFO - root -   Epoch: [50/500][200/346], lr: 0.00000172 	 loss = 0.5606(1.1937)
2023/09/14 21:40:04 - INFO - root -   Epoch: [50/500][220/346], lr: 0.00000172 	 loss = 1.7226(1.1927)
2023/09/14 21:40:25 - INFO - root -   Epoch: [50/500][240/346], lr: 0.00000172 	 loss = 0.9022(1.1903)
2023/09/14 21:40:41 - INFO - root -   Epoch: [50/500][260/346], lr: 0.00000172 	 loss = 0.7252(1.1872)
2023/09/14 21:41:02 - INFO - root -   Epoch: [50/500][280/346], lr: 0.00000172 	 loss = 1.7482(1.1768)
2023/09/14 21:41:19 - INFO - root -   Epoch: [50/500][300/346], lr: 0.00000172 	 loss = 1.8884(1.1745)
2023/09/14 21:41:39 - INFO - root -   Epoch: [50/500][320/346], lr: 0.00000172 	 loss = 0.7009(1.1774)
2023/09/14 21:41:56 - INFO - root -   Epoch: [50/500][340/346], lr: 0.00000172 	 loss = 1.6842(1.1768)
2023/09/14 21:41:58 - INFO - root -   Epoch: [50/500] 	 loss = 1.1760
2023/09/14 21:41:58 - INFO - root -   train_accuracy = 0.5173
2023/09/14 21:42:06 - INFO - root -   Epoch: [51/500][0/346], lr: 0.00000173 	 loss = 1.5521(1.5521)
2023/09/14 21:42:23 - INFO - root -   Epoch: [51/500][20/346], lr: 0.00000173 	 loss = 1.6790(1.1868)
2023/09/14 21:42:44 - INFO - root -   Epoch: [51/500][40/346], lr: 0.00000173 	 loss = 0.8482(1.1967)
2023/09/14 21:43:00 - INFO - root -   Epoch: [51/500][60/346], lr: 0.00000173 	 loss = 0.5738(1.1806)
2023/09/14 21:43:21 - INFO - root -   Epoch: [51/500][80/346], lr: 0.00000173 	 loss = 0.9009(1.2055)
2023/09/14 21:43:37 - INFO - root -   Epoch: [51/500][100/346], lr: 0.00000173 	 loss = 0.9179(1.2136)
2023/09/14 21:43:58 - INFO - root -   Epoch: [51/500][120/346], lr: 0.00000173 	 loss = 1.1296(1.2245)
2023/09/14 21:44:15 - INFO - root -   Epoch: [51/500][140/346], lr: 0.00000173 	 loss = 1.6882(1.2270)
2023/09/14 21:44:36 - INFO - root -   Epoch: [51/500][160/346], lr: 0.00000173 	 loss = 0.4444(1.2056)
2023/09/14 21:44:52 - INFO - root -   Epoch: [51/500][180/346], lr: 0.00000173 	 loss = 0.9490(1.1911)
2023/09/14 21:45:13 - INFO - root -   Epoch: [51/500][200/346], lr: 0.00000173 	 loss = 0.4533(1.1797)
2023/09/14 21:45:30 - INFO - root -   Epoch: [51/500][220/346], lr: 0.00000173 	 loss = 1.5077(1.1823)
2023/09/14 21:45:51 - INFO - root -   Epoch: [51/500][240/346], lr: 0.00000173 	 loss = 0.9491(1.1862)
2023/09/14 21:46:07 - INFO - root -   Epoch: [51/500][260/346], lr: 0.00000173 	 loss = 0.5988(1.1825)
2023/09/14 21:46:28 - INFO - root -   Epoch: [51/500][280/346], lr: 0.00000173 	 loss = 1.5519(1.1755)
2023/09/14 21:46:45 - INFO - root -   Epoch: [51/500][300/346], lr: 0.00000173 	 loss = 1.9577(1.1744)
2023/09/14 21:47:06 - INFO - root -   Epoch: [51/500][320/346], lr: 0.00000173 	 loss = 0.6405(1.1794)
2023/09/14 21:47:22 - INFO - root -   Epoch: [51/500][340/346], lr: 0.00000173 	 loss = 1.6034(1.1780)
2023/09/14 21:47:24 - INFO - root -   Epoch: [51/500] 	 loss = 1.1750
2023/09/14 21:47:24 - INFO - root -   train_accuracy = 0.5173
2023/09/14 21:47:32 - INFO - root -   Epoch: [52/500][0/346], lr: 0.00000174 	 loss = 1.4287(1.4287)
2023/09/14 21:47:49 - INFO - root -   Epoch: [52/500][20/346], lr: 0.00000174 	 loss = 2.1683(1.2022)
2023/09/14 21:48:10 - INFO - root -   Epoch: [52/500][40/346], lr: 0.00000174 	 loss = 1.3654(1.1835)
2023/09/14 21:48:27 - INFO - root -   Epoch: [52/500][60/346], lr: 0.00000174 	 loss = 0.7495(1.1689)
2023/09/14 21:48:47 - INFO - root -   Epoch: [52/500][80/346], lr: 0.00000174 	 loss = 0.9741(1.2018)
2023/09/14 21:49:04 - INFO - root -   Epoch: [52/500][100/346], lr: 0.00000174 	 loss = 0.6705(1.2129)
2023/09/14 21:49:25 - INFO - root -   Epoch: [52/500][120/346], lr: 0.00000174 	 loss = 1.0137(1.2338)
2023/09/14 21:49:42 - INFO - root -   Epoch: [52/500][140/346], lr: 0.00000174 	 loss = 2.0147(1.2204)
2023/09/14 21:50:02 - INFO - root -   Epoch: [52/500][160/346], lr: 0.00000174 	 loss = 0.5776(1.2004)
2023/09/14 21:50:19 - INFO - root -   Epoch: [52/500][180/346], lr: 0.00000174 	 loss = 1.2900(1.1984)
2023/09/14 21:50:40 - INFO - root -   Epoch: [52/500][200/346], lr: 0.00000174 	 loss = 0.4804(1.1874)
2023/09/14 21:50:56 - INFO - root -   Epoch: [52/500][220/346], lr: 0.00000174 	 loss = 1.3553(1.1906)
2023/09/14 21:51:17 - INFO - root -   Epoch: [52/500][240/346], lr: 0.00000174 	 loss = 0.9109(1.1882)
2023/09/14 21:51:34 - INFO - root -   Epoch: [52/500][260/346], lr: 0.00000174 	 loss = 0.7816(1.1844)
2023/09/14 21:51:55 - INFO - root -   Epoch: [52/500][280/346], lr: 0.00000174 	 loss = 1.6658(1.1745)
2023/09/14 21:52:12 - INFO - root -   Epoch: [52/500][300/346], lr: 0.00000174 	 loss = 1.7069(1.1758)
2023/09/14 21:52:33 - INFO - root -   Epoch: [52/500][320/346], lr: 0.00000174 	 loss = 0.5981(1.1794)
2023/09/14 21:52:49 - INFO - root -   Epoch: [52/500][340/346], lr: 0.00000174 	 loss = 2.0053(1.1779)
2023/09/14 21:52:51 - INFO - root -   Epoch: [52/500] 	 loss = 1.1767
2023/09/14 21:52:51 - INFO - root -   train_accuracy = 0.5390
2023/09/14 21:52:59 - INFO - root -   Epoch: [53/500][0/346], lr: 0.00000176 	 loss = 1.7063(1.7063)
2023/09/14 21:53:16 - INFO - root -   Epoch: [53/500][20/346], lr: 0.00000176 	 loss = 2.0866(1.1658)
2023/09/14 21:53:37 - INFO - root -   Epoch: [53/500][40/346], lr: 0.00000176 	 loss = 1.2734(1.1715)
2023/09/14 21:53:53 - INFO - root -   Epoch: [53/500][60/346], lr: 0.00000176 	 loss = 0.7093(1.1622)
2023/09/14 21:54:14 - INFO - root -   Epoch: [53/500][80/346], lr: 0.00000176 	 loss = 0.8003(1.1964)
2023/09/14 21:54:31 - INFO - root -   Epoch: [53/500][100/346], lr: 0.00000176 	 loss = 0.7171(1.2190)
2023/09/14 21:54:52 - INFO - root -   Epoch: [53/500][120/346], lr: 0.00000176 	 loss = 1.3807(1.2250)
2023/09/14 21:55:08 - INFO - root -   Epoch: [53/500][140/346], lr: 0.00000176 	 loss = 2.2346(1.2220)
2023/09/14 21:55:29 - INFO - root -   Epoch: [53/500][160/346], lr: 0.00000176 	 loss = 0.4400(1.2039)
2023/09/14 21:55:46 - INFO - root -   Epoch: [53/500][180/346], lr: 0.00000176 	 loss = 0.8283(1.1948)
2023/09/14 21:56:07 - INFO - root -   Epoch: [53/500][200/346], lr: 0.00000176 	 loss = 0.4872(1.1789)
2023/09/14 21:56:23 - INFO - root -   Epoch: [53/500][220/346], lr: 0.00000176 	 loss = 1.7965(1.1787)
2023/09/14 21:56:44 - INFO - root -   Epoch: [53/500][240/346], lr: 0.00000176 	 loss = 0.6356(1.1788)
2023/09/14 21:57:01 - INFO - root -   Epoch: [53/500][260/346], lr: 0.00000176 	 loss = 0.7684(1.1790)
2023/09/14 21:57:22 - INFO - root -   Epoch: [53/500][280/346], lr: 0.00000176 	 loss = 1.8173(1.1682)
2023/09/14 21:57:38 - INFO - root -   Epoch: [53/500][300/346], lr: 0.00000176 	 loss = 2.1164(1.1682)
2023/09/14 21:57:59 - INFO - root -   Epoch: [53/500][320/346], lr: 0.00000176 	 loss = 0.6238(1.1728)
2023/09/14 21:58:16 - INFO - root -   Epoch: [53/500][340/346], lr: 0.00000176 	 loss = 1.9181(1.1717)
2023/09/14 21:58:18 - INFO - root -   Epoch: [53/500] 	 loss = 1.1706
2023/09/14 21:58:18 - INFO - root -   train_accuracy = 0.5303
2023/09/14 21:58:26 - INFO - root -   Epoch: [54/500][0/346], lr: 0.00000177 	 loss = 1.7477(1.7477)
2023/09/14 21:58:43 - INFO - root -   Epoch: [54/500][20/346], lr: 0.00000177 	 loss = 1.5454(1.1642)
2023/09/14 21:59:04 - INFO - root -   Epoch: [54/500][40/346], lr: 0.00000177 	 loss = 1.4526(1.2307)
2023/09/14 21:59:20 - INFO - root -   Epoch: [54/500][60/346], lr: 0.00000177 	 loss = 0.6900(1.2090)
2023/09/14 21:59:41 - INFO - root -   Epoch: [54/500][80/346], lr: 0.00000177 	 loss = 1.0141(1.2266)
2023/09/14 21:59:58 - INFO - root -   Epoch: [54/500][100/346], lr: 0.00000177 	 loss = 0.6504(1.2280)
2023/09/14 22:00:19 - INFO - root -   Epoch: [54/500][120/346], lr: 0.00000177 	 loss = 1.0747(1.2342)
2023/09/14 22:00:35 - INFO - root -   Epoch: [54/500][140/346], lr: 0.00000177 	 loss = 2.0425(1.2211)
2023/09/14 22:00:56 - INFO - root -   Epoch: [54/500][160/346], lr: 0.00000177 	 loss = 0.4600(1.1990)
2023/09/14 22:01:13 - INFO - root -   Epoch: [54/500][180/346], lr: 0.00000177 	 loss = 1.1219(1.1913)
2023/09/14 22:01:34 - INFO - root -   Epoch: [54/500][200/346], lr: 0.00000177 	 loss = 0.4597(1.1806)
2023/09/14 22:01:50 - INFO - root -   Epoch: [54/500][220/346], lr: 0.00000177 	 loss = 1.2521(1.1776)
2023/09/14 22:02:11 - INFO - root -   Epoch: [54/500][240/346], lr: 0.00000177 	 loss = 0.8391(1.1766)
2023/09/14 22:02:28 - INFO - root -   Epoch: [54/500][260/346], lr: 0.00000177 	 loss = 0.7757(1.1760)
2023/09/14 22:02:49 - INFO - root -   Epoch: [54/500][280/346], lr: 0.00000177 	 loss = 1.7239(1.1706)
2023/09/14 22:03:05 - INFO - root -   Epoch: [54/500][300/346], lr: 0.00000177 	 loss = 1.6681(1.1719)
2023/09/14 22:03:26 - INFO - root -   Epoch: [54/500][320/346], lr: 0.00000177 	 loss = 0.5605(1.1764)
2023/09/14 22:03:43 - INFO - root -   Epoch: [54/500][340/346], lr: 0.00000177 	 loss = 2.1578(1.1794)
2023/09/14 22:03:45 - INFO - root -   Epoch: [54/500] 	 loss = 1.1761
2023/09/14 22:05:07 - INFO - root -   precision = 0.5345
2023/09/14 22:05:07 - INFO - root -   eval_loss = 1.1512
2023/09/14 22:05:08 - INFO - root -   train_accuracy = 0.5188
2023/09/14 22:05:17 - INFO - root -   Epoch: [55/500][0/346], lr: 0.00000179 	 loss = 1.9181(1.9181)
2023/09/14 22:05:33 - INFO - root -   Epoch: [55/500][20/346], lr: 0.00000179 	 loss = 1.8659(1.1566)
2023/09/14 22:05:54 - INFO - root -   Epoch: [55/500][40/346], lr: 0.00000179 	 loss = 1.4276(1.1993)
2023/09/14 22:06:10 - INFO - root -   Epoch: [55/500][60/346], lr: 0.00000179 	 loss = 1.0746(1.1914)
2023/09/14 22:06:31 - INFO - root -   Epoch: [55/500][80/346], lr: 0.00000179 	 loss = 1.0672(1.2098)
2023/09/14 22:06:48 - INFO - root -   Epoch: [55/500][100/346], lr: 0.00000179 	 loss = 0.7546(1.2060)
2023/09/14 22:07:09 - INFO - root -   Epoch: [55/500][120/346], lr: 0.00000179 	 loss = 0.8387(1.2160)
2023/09/14 22:07:25 - INFO - root -   Epoch: [55/500][140/346], lr: 0.00000179 	 loss = 1.7230(1.2068)
2023/09/14 22:07:46 - INFO - root -   Epoch: [55/500][160/346], lr: 0.00000179 	 loss = 0.3263(1.1872)
2023/09/14 22:08:03 - INFO - root -   Epoch: [55/500][180/346], lr: 0.00000179 	 loss = 1.3811(1.1817)
2023/09/14 22:08:23 - INFO - root -   Epoch: [55/500][200/346], lr: 0.00000179 	 loss = 0.4551(1.1706)
2023/09/14 22:08:40 - INFO - root -   Epoch: [55/500][220/346], lr: 0.00000179 	 loss = 1.5428(1.1798)
2023/09/14 22:09:01 - INFO - root -   Epoch: [55/500][240/346], lr: 0.00000179 	 loss = 0.6673(1.1834)
2023/09/14 22:09:17 - INFO - root -   Epoch: [55/500][260/346], lr: 0.00000179 	 loss = 0.6014(1.1815)
2023/09/14 22:09:38 - INFO - root -   Epoch: [55/500][280/346], lr: 0.00000179 	 loss = 1.7793(1.1743)
2023/09/14 22:09:55 - INFO - root -   Epoch: [55/500][300/346], lr: 0.00000179 	 loss = 2.0220(1.1753)
2023/09/14 22:10:15 - INFO - root -   Epoch: [55/500][320/346], lr: 0.00000179 	 loss = 0.5724(1.1770)
2023/09/14 22:10:32 - INFO - root -   Epoch: [55/500][340/346], lr: 0.00000179 	 loss = 1.9600(1.1801)
2023/09/14 22:10:34 - INFO - root -   Epoch: [55/500] 	 loss = 1.1772
2023/09/14 22:10:34 - INFO - root -   train_accuracy = 0.5246
2023/09/14 22:10:42 - INFO - root -   Epoch: [56/500][0/346], lr: 0.00000180 	 loss = 1.6536(1.6536)
2023/09/14 22:10:59 - INFO - root -   Epoch: [56/500][20/346], lr: 0.00000180 	 loss = 2.3114(1.2499)
2023/09/14 22:11:19 - INFO - root -   Epoch: [56/500][40/346], lr: 0.00000180 	 loss = 1.2208(1.1930)
2023/09/14 22:11:36 - INFO - root -   Epoch: [56/500][60/346], lr: 0.00000180 	 loss = 0.7778(1.1777)
2023/09/14 22:11:56 - INFO - root -   Epoch: [56/500][80/346], lr: 0.00000180 	 loss = 1.0526(1.2177)
2023/09/14 22:12:13 - INFO - root -   Epoch: [56/500][100/346], lr: 0.00000180 	 loss = 0.9023(1.2286)
2023/09/14 22:12:34 - INFO - root -   Epoch: [56/500][120/346], lr: 0.00000180 	 loss = 0.9907(1.2307)
2023/09/14 22:12:50 - INFO - root -   Epoch: [56/500][140/346], lr: 0.00000180 	 loss = 2.0007(1.2206)
2023/09/14 22:13:11 - INFO - root -   Epoch: [56/500][160/346], lr: 0.00000180 	 loss = 0.5026(1.2000)
2023/09/14 22:13:27 - INFO - root -   Epoch: [56/500][180/346], lr: 0.00000180 	 loss = 1.3083(1.1880)
2023/09/14 22:13:48 - INFO - root -   Epoch: [56/500][200/346], lr: 0.00000180 	 loss = 0.6503(1.1734)
2023/09/14 22:14:05 - INFO - root -   Epoch: [56/500][220/346], lr: 0.00000180 	 loss = 1.9109(1.1749)
2023/09/14 22:14:26 - INFO - root -   Epoch: [56/500][240/346], lr: 0.00000180 	 loss = 0.8615(1.1766)
2023/09/14 22:14:42 - INFO - root -   Epoch: [56/500][260/346], lr: 0.00000180 	 loss = 0.6251(1.1750)
2023/09/14 22:15:03 - INFO - root -   Epoch: [56/500][280/346], lr: 0.00000180 	 loss = 1.5686(1.1694)
2023/09/14 22:15:20 - INFO - root -   Epoch: [56/500][300/346], lr: 0.00000180 	 loss = 1.8502(1.1698)
2023/09/14 22:15:41 - INFO - root -   Epoch: [56/500][320/346], lr: 0.00000180 	 loss = 0.4368(1.1736)
2023/09/14 22:15:57 - INFO - root -   Epoch: [56/500][340/346], lr: 0.00000180 	 loss = 1.9296(1.1719)
2023/09/14 22:15:59 - INFO - root -   Epoch: [56/500] 	 loss = 1.1700
2023/09/14 22:15:59 - INFO - root -   train_accuracy = 0.5145
2023/09/14 22:16:08 - INFO - root -   Epoch: [57/500][0/346], lr: 0.00000182 	 loss = 1.5697(1.5697)
2023/09/14 22:16:24 - INFO - root -   Epoch: [57/500][20/346], lr: 0.00000182 	 loss = 1.6785(1.1034)
2023/09/14 22:16:45 - INFO - root -   Epoch: [57/500][40/346], lr: 0.00000182 	 loss = 1.1828(1.1137)
2023/09/14 22:17:01 - INFO - root -   Epoch: [57/500][60/346], lr: 0.00000182 	 loss = 0.6894(1.1375)
2023/09/14 22:17:22 - INFO - root -   Epoch: [57/500][80/346], lr: 0.00000182 	 loss = 1.1308(1.1698)
2023/09/14 22:17:39 - INFO - root -   Epoch: [57/500][100/346], lr: 0.00000182 	 loss = 0.6393(1.1870)
2023/09/14 22:17:59 - INFO - root -   Epoch: [57/500][120/346], lr: 0.00000182 	 loss = 1.1265(1.2098)
2023/09/14 22:18:16 - INFO - root -   Epoch: [57/500][140/346], lr: 0.00000182 	 loss = 1.9202(1.2049)
2023/09/14 22:18:37 - INFO - root -   Epoch: [57/500][160/346], lr: 0.00000182 	 loss = 0.5856(1.1833)
2023/09/14 22:18:53 - INFO - root -   Epoch: [57/500][180/346], lr: 0.00000182 	 loss = 1.4120(1.1806)
2023/09/14 22:19:14 - INFO - root -   Epoch: [57/500][200/346], lr: 0.00000182 	 loss = 0.6183(1.1650)
2023/09/14 22:19:31 - INFO - root -   Epoch: [57/500][220/346], lr: 0.00000182 	 loss = 1.4912(1.1625)
2023/09/14 22:19:52 - INFO - root -   Epoch: [57/500][240/346], lr: 0.00000182 	 loss = 0.8756(1.1621)
2023/09/14 22:20:08 - INFO - root -   Epoch: [57/500][260/346], lr: 0.00000182 	 loss = 0.7750(1.1652)
2023/09/14 22:20:29 - INFO - root -   Epoch: [57/500][280/346], lr: 0.00000182 	 loss = 1.6976(1.1596)
2023/09/14 22:20:45 - INFO - root -   Epoch: [57/500][300/346], lr: 0.00000182 	 loss = 2.1442(1.1636)
2023/09/14 22:21:06 - INFO - root -   Epoch: [57/500][320/346], lr: 0.00000182 	 loss = 0.5346(1.1676)
2023/09/14 22:21:22 - INFO - root -   Epoch: [57/500][340/346], lr: 0.00000182 	 loss = 1.9352(1.1697)
2023/09/14 22:21:24 - INFO - root -   Epoch: [57/500] 	 loss = 1.1687
2023/09/14 22:21:24 - INFO - root -   train_accuracy = 0.5217
2023/09/14 22:21:33 - INFO - root -   Epoch: [58/500][0/346], lr: 0.00000183 	 loss = 1.3928(1.3928)
2023/09/14 22:21:49 - INFO - root -   Epoch: [58/500][20/346], lr: 0.00000183 	 loss = 1.8378(1.1885)
2023/09/14 22:22:10 - INFO - root -   Epoch: [58/500][40/346], lr: 0.00000183 	 loss = 1.1546(1.1822)
2023/09/14 22:22:27 - INFO - root -   Epoch: [58/500][60/346], lr: 0.00000183 	 loss = 0.7577(1.1619)
2023/09/14 22:22:47 - INFO - root -   Epoch: [58/500][80/346], lr: 0.00000183 	 loss = 1.2713(1.1801)
2023/09/14 22:23:04 - INFO - root -   Epoch: [58/500][100/346], lr: 0.00000183 	 loss = 0.9175(1.2024)
2023/09/14 22:23:25 - INFO - root -   Epoch: [58/500][120/346], lr: 0.00000183 	 loss = 1.1151(1.2170)
2023/09/14 22:23:41 - INFO - root -   Epoch: [58/500][140/346], lr: 0.00000183 	 loss = 2.4186(1.2167)
2023/09/14 22:24:02 - INFO - root -   Epoch: [58/500][160/346], lr: 0.00000183 	 loss = 0.5267(1.1979)
2023/09/14 22:24:19 - INFO - root -   Epoch: [58/500][180/346], lr: 0.00000183 	 loss = 1.0535(1.1826)
2023/09/14 22:24:40 - INFO - root -   Epoch: [58/500][200/346], lr: 0.00000183 	 loss = 0.6015(1.1691)
2023/09/14 22:24:56 - INFO - root -   Epoch: [58/500][220/346], lr: 0.00000183 	 loss = 1.4053(1.1718)
2023/09/14 22:25:17 - INFO - root -   Epoch: [58/500][240/346], lr: 0.00000183 	 loss = 0.7644(1.1700)
2023/09/14 22:25:34 - INFO - root -   Epoch: [58/500][260/346], lr: 0.00000183 	 loss = 0.7060(1.1688)
2023/09/14 22:25:55 - INFO - root -   Epoch: [58/500][280/346], lr: 0.00000183 	 loss = 1.5062(1.1572)
2023/09/14 22:26:11 - INFO - root -   Epoch: [58/500][300/346], lr: 0.00000183 	 loss = 1.9897(1.1519)
2023/09/14 22:26:32 - INFO - root -   Epoch: [58/500][320/346], lr: 0.00000183 	 loss = 0.7097(1.1593)
2023/09/14 22:26:48 - INFO - root -   Epoch: [58/500][340/346], lr: 0.00000183 	 loss = 1.8283(1.1620)
2023/09/14 22:26:51 - INFO - root -   Epoch: [58/500] 	 loss = 1.1610
2023/09/14 22:26:51 - INFO - root -   train_accuracy = 0.5275
2023/09/14 22:26:59 - INFO - root -   Epoch: [59/500][0/346], lr: 0.00000184 	 loss = 1.5515(1.5515)
2023/09/14 22:27:15 - INFO - root -   Epoch: [59/500][20/346], lr: 0.00000184 	 loss = 1.9782(1.2059)
2023/09/14 22:27:36 - INFO - root -   Epoch: [59/500][40/346], lr: 0.00000184 	 loss = 1.3494(1.1934)
2023/09/14 22:27:52 - INFO - root -   Epoch: [59/500][60/346], lr: 0.00000184 	 loss = 0.8644(1.2069)
2023/09/14 22:28:13 - INFO - root -   Epoch: [59/500][80/346], lr: 0.00000184 	 loss = 1.1423(1.2234)
2023/09/14 22:28:30 - INFO - root -   Epoch: [59/500][100/346], lr: 0.00000184 	 loss = 0.6060(1.2239)
2023/09/14 22:28:51 - INFO - root -   Epoch: [59/500][120/346], lr: 0.00000184 	 loss = 0.9942(1.2300)
2023/09/14 22:29:07 - INFO - root -   Epoch: [59/500][140/346], lr: 0.00000184 	 loss = 1.9253(1.2234)
2023/09/14 22:29:28 - INFO - root -   Epoch: [59/500][160/346], lr: 0.00000184 	 loss = 0.6280(1.2006)
2023/09/14 22:29:44 - INFO - root -   Epoch: [59/500][180/346], lr: 0.00000184 	 loss = 1.2716(1.1987)
2023/09/14 22:30:05 - INFO - root -   Epoch: [59/500][200/346], lr: 0.00000184 	 loss = 0.5291(1.1872)
2023/09/14 22:30:22 - INFO - root -   Epoch: [59/500][220/346], lr: 0.00000184 	 loss = 1.6015(1.1925)
2023/09/14 22:30:42 - INFO - root -   Epoch: [59/500][240/346], lr: 0.00000184 	 loss = 0.6962(1.1908)
2023/09/14 22:30:59 - INFO - root -   Epoch: [59/500][260/346], lr: 0.00000184 	 loss = 0.6331(1.1878)
2023/09/14 22:31:20 - INFO - root -   Epoch: [59/500][280/346], lr: 0.00000184 	 loss = 1.5762(1.1771)
2023/09/14 22:31:36 - INFO - root -   Epoch: [59/500][300/346], lr: 0.00000184 	 loss = 1.9042(1.1757)
2023/09/14 22:31:57 - INFO - root -   Epoch: [59/500][320/346], lr: 0.00000184 	 loss = 0.5430(1.1757)
2023/09/14 22:32:13 - INFO - root -   Epoch: [59/500][340/346], lr: 0.00000184 	 loss = 1.5935(1.1749)
2023/09/14 22:32:15 - INFO - root -   Epoch: [59/500] 	 loss = 1.1730
2023/09/14 22:33:37 - INFO - root -   precision = 0.5345
2023/09/14 22:33:37 - INFO - root -   eval_loss = 1.1478
2023/09/14 22:33:38 - INFO - root -   train_accuracy = 0.5231
2023/09/14 22:33:46 - INFO - root -   Epoch: [60/500][0/346], lr: 0.00000186 	 loss = 1.7076(1.7076)
2023/09/14 22:34:03 - INFO - root -   Epoch: [60/500][20/346], lr: 0.00000186 	 loss = 2.3334(1.2394)
2023/09/14 22:34:24 - INFO - root -   Epoch: [60/500][40/346], lr: 0.00000186 	 loss = 0.9523(1.2035)
2023/09/14 22:34:40 - INFO - root -   Epoch: [60/500][60/346], lr: 0.00000186 	 loss = 0.5673(1.2131)
2023/09/14 22:35:01 - INFO - root -   Epoch: [60/500][80/346], lr: 0.00000186 	 loss = 1.0785(1.2203)
2023/09/14 22:35:17 - INFO - root -   Epoch: [60/500][100/346], lr: 0.00000186 	 loss = 0.8088(1.2260)
2023/09/14 22:35:38 - INFO - root -   Epoch: [60/500][120/346], lr: 0.00000186 	 loss = 0.9795(1.2396)
2023/09/14 22:35:55 - INFO - root -   Epoch: [60/500][140/346], lr: 0.00000186 	 loss = 1.9585(1.2303)
2023/09/14 22:36:16 - INFO - root -   Epoch: [60/500][160/346], lr: 0.00000186 	 loss = 0.6320(1.2142)
2023/09/14 22:36:32 - INFO - root -   Epoch: [60/500][180/346], lr: 0.00000186 	 loss = 0.9231(1.2066)
2023/09/14 22:36:53 - INFO - root -   Epoch: [60/500][200/346], lr: 0.00000186 	 loss = 0.4272(1.1954)
2023/09/14 22:37:09 - INFO - root -   Epoch: [60/500][220/346], lr: 0.00000186 	 loss = 1.7682(1.1954)
2023/09/14 22:37:30 - INFO - root -   Epoch: [60/500][240/346], lr: 0.00000186 	 loss = 0.8749(1.1973)
2023/09/14 22:37:47 - INFO - root -   Epoch: [60/500][260/346], lr: 0.00000186 	 loss = 0.6749(1.1948)
2023/09/14 22:38:08 - INFO - root -   Epoch: [60/500][280/346], lr: 0.00000186 	 loss = 1.6866(1.1827)
2023/09/14 22:38:24 - INFO - root -   Epoch: [60/500][300/346], lr: 0.00000186 	 loss = 1.7313(1.1837)
2023/09/14 22:38:45 - INFO - root -   Epoch: [60/500][320/346], lr: 0.00000186 	 loss = 0.5453(1.1864)
2023/09/14 22:39:01 - INFO - root -   Epoch: [60/500][340/346], lr: 0.00000186 	 loss = 1.9348(1.1829)
2023/09/14 22:39:04 - INFO - root -   Epoch: [60/500] 	 loss = 1.1795
2023/09/14 22:39:04 - INFO - root -   train_accuracy = 0.5188
2023/09/14 22:39:12 - INFO - root -   Epoch: [61/500][0/346], lr: 0.00000187 	 loss = 1.7011(1.7011)
2023/09/14 22:39:28 - INFO - root -   Epoch: [61/500][20/346], lr: 0.00000187 	 loss = 1.6193(1.1567)
2023/09/14 22:39:49 - INFO - root -   Epoch: [61/500][40/346], lr: 0.00000187 	 loss = 1.3485(1.1808)
2023/09/14 22:40:06 - INFO - root -   Epoch: [61/500][60/346], lr: 0.00000187 	 loss = 0.7330(1.1600)
2023/09/14 22:40:27 - INFO - root -   Epoch: [61/500][80/346], lr: 0.00000187 	 loss = 1.0050(1.1933)
2023/09/14 22:40:43 - INFO - root -   Epoch: [61/500][100/346], lr: 0.00000187 	 loss = 0.8010(1.1952)
2023/09/14 22:41:04 - INFO - root -   Epoch: [61/500][120/346], lr: 0.00000187 	 loss = 0.9571(1.2042)
2023/09/14 22:41:21 - INFO - root -   Epoch: [61/500][140/346], lr: 0.00000187 	 loss = 1.5345(1.1964)
2023/09/14 22:41:42 - INFO - root -   Epoch: [61/500][160/346], lr: 0.00000187 	 loss = 0.5178(1.1770)
2023/09/14 22:41:58 - INFO - root -   Epoch: [61/500][180/346], lr: 0.00000187 	 loss = 1.0075(1.1714)
2023/09/14 22:42:19 - INFO - root -   Epoch: [61/500][200/346], lr: 0.00000187 	 loss = 0.5004(1.1596)
2023/09/14 22:42:36 - INFO - root -   Epoch: [61/500][220/346], lr: 0.00000187 	 loss = 1.4012(1.1620)
2023/09/14 22:42:57 - INFO - root -   Epoch: [61/500][240/346], lr: 0.00000187 	 loss = 0.9596(1.1623)
2023/09/14 22:43:14 - INFO - root -   Epoch: [61/500][260/346], lr: 0.00000187 	 loss = 0.6411(1.1625)
2023/09/14 22:43:35 - INFO - root -   Epoch: [61/500][280/346], lr: 0.00000187 	 loss = 1.6772(1.1531)
2023/09/14 22:43:51 - INFO - root -   Epoch: [61/500][300/346], lr: 0.00000187 	 loss = 1.6734(1.1540)
2023/09/14 22:44:12 - INFO - root -   Epoch: [61/500][320/346], lr: 0.00000187 	 loss = 0.5975(1.1559)
2023/09/14 22:44:29 - INFO - root -   Epoch: [61/500][340/346], lr: 0.00000187 	 loss = 1.9932(1.1549)
2023/09/14 22:44:31 - INFO - root -   Epoch: [61/500] 	 loss = 1.1541
2023/09/14 22:44:31 - INFO - root -   train_accuracy = 0.5318
2023/09/14 22:44:39 - INFO - root -   Epoch: [62/500][0/346], lr: 0.00000189 	 loss = 1.4755(1.4755)
2023/09/14 22:44:55 - INFO - root -   Epoch: [62/500][20/346], lr: 0.00000189 	 loss = 2.2675(1.1609)
2023/09/14 22:45:16 - INFO - root -   Epoch: [62/500][40/346], lr: 0.00000189 	 loss = 1.2731(1.1677)
2023/09/14 22:45:33 - INFO - root -   Epoch: [62/500][60/346], lr: 0.00000189 	 loss = 0.7531(1.1560)
2023/09/14 22:45:53 - INFO - root -   Epoch: [62/500][80/346], lr: 0.00000189 	 loss = 0.8996(1.1877)
2023/09/14 22:46:10 - INFO - root -   Epoch: [62/500][100/346], lr: 0.00000189 	 loss = 0.6921(1.2074)
2023/09/14 22:46:31 - INFO - root -   Epoch: [62/500][120/346], lr: 0.00000189 	 loss = 1.1165(1.2233)
2023/09/14 22:46:47 - INFO - root -   Epoch: [62/500][140/346], lr: 0.00000189 	 loss = 2.0398(1.2191)
2023/09/14 22:47:08 - INFO - root -   Epoch: [62/500][160/346], lr: 0.00000189 	 loss = 0.5032(1.1957)
2023/09/14 22:47:25 - INFO - root -   Epoch: [62/500][180/346], lr: 0.00000189 	 loss = 1.1003(1.1805)
2023/09/14 22:47:46 - INFO - root -   Epoch: [62/500][200/346], lr: 0.00000189 	 loss = 0.4506(1.1700)
2023/09/14 22:48:03 - INFO - root -   Epoch: [62/500][220/346], lr: 0.00000189 	 loss = 1.6888(1.1734)
2023/09/14 22:48:24 - INFO - root -   Epoch: [62/500][240/346], lr: 0.00000189 	 loss = 0.8457(1.1747)
2023/09/14 22:48:40 - INFO - root -   Epoch: [62/500][260/346], lr: 0.00000189 	 loss = 0.5698(1.1651)
2023/09/14 22:49:01 - INFO - root -   Epoch: [62/500][280/346], lr: 0.00000189 	 loss = 1.7313(1.1565)
2023/09/14 22:49:18 - INFO - root -   Epoch: [62/500][300/346], lr: 0.00000189 	 loss = 1.9904(1.1561)
2023/09/14 22:49:38 - INFO - root -   Epoch: [62/500][320/346], lr: 0.00000189 	 loss = 0.5049(1.1596)
2023/09/14 22:49:55 - INFO - root -   Epoch: [62/500][340/346], lr: 0.00000189 	 loss = 1.7270(1.1611)
2023/09/14 22:49:57 - INFO - root -   Epoch: [62/500] 	 loss = 1.1597
2023/09/14 22:49:57 - INFO - root -   train_accuracy = 0.5246
2023/09/14 22:50:05 - INFO - root -   Epoch: [63/500][0/346], lr: 0.00000190 	 loss = 1.6255(1.6255)
2023/09/14 22:50:22 - INFO - root -   Epoch: [63/500][20/346], lr: 0.00000190 	 loss = 2.0201(1.1987)
2023/09/14 22:50:43 - INFO - root -   Epoch: [63/500][40/346], lr: 0.00000190 	 loss = 1.5436(1.1819)
2023/09/14 22:50:59 - INFO - root -   Epoch: [63/500][60/346], lr: 0.00000190 	 loss = 0.8859(1.1887)
2023/09/14 22:51:20 - INFO - root -   Epoch: [63/500][80/346], lr: 0.00000190 	 loss = 0.9572(1.1891)
2023/09/14 22:51:37 - INFO - root -   Epoch: [63/500][100/346], lr: 0.00000190 	 loss = 0.7649(1.1945)
2023/09/14 22:51:58 - INFO - root -   Epoch: [63/500][120/346], lr: 0.00000190 	 loss = 1.0612(1.2208)
2023/09/14 22:52:15 - INFO - root -   Epoch: [63/500][140/346], lr: 0.00000190 	 loss = 1.8642(1.2148)
2023/09/14 22:52:36 - INFO - root -   Epoch: [63/500][160/346], lr: 0.00000190 	 loss = 0.6130(1.1896)
2023/09/14 22:52:52 - INFO - root -   Epoch: [63/500][180/346], lr: 0.00000190 	 loss = 1.2402(1.1829)
2023/09/14 22:53:13 - INFO - root -   Epoch: [63/500][200/346], lr: 0.00000190 	 loss = 0.5608(1.1705)
2023/09/14 22:53:30 - INFO - root -   Epoch: [63/500][220/346], lr: 0.00000190 	 loss = 1.7882(1.1756)
2023/09/14 22:53:51 - INFO - root -   Epoch: [63/500][240/346], lr: 0.00000190 	 loss = 0.8434(1.1713)
2023/09/14 22:54:07 - INFO - root -   Epoch: [63/500][260/346], lr: 0.00000190 	 loss = 0.6958(1.1750)
2023/09/14 22:54:28 - INFO - root -   Epoch: [63/500][280/346], lr: 0.00000190 	 loss = 1.3869(1.1660)
2023/09/14 22:54:45 - INFO - root -   Epoch: [63/500][300/346], lr: 0.00000190 	 loss = 1.9871(1.1678)
2023/09/14 22:55:06 - INFO - root -   Epoch: [63/500][320/346], lr: 0.00000190 	 loss = 0.5898(1.1727)
2023/09/14 22:55:22 - INFO - root -   Epoch: [63/500][340/346], lr: 0.00000190 	 loss = 1.8416(1.1727)
2023/09/14 22:55:24 - INFO - root -   Epoch: [63/500] 	 loss = 1.1704
2023/09/14 22:55:24 - INFO - root -   train_accuracy = 0.5188
2023/09/14 22:55:33 - INFO - root -   Epoch: [64/500][0/346], lr: 0.00000192 	 loss = 1.8422(1.8422)
2023/09/14 22:55:49 - INFO - root -   Epoch: [64/500][20/346], lr: 0.00000192 	 loss = 1.9113(1.2005)
2023/09/14 22:56:10 - INFO - root -   Epoch: [64/500][40/346], lr: 0.00000192 	 loss = 1.3528(1.1734)
2023/09/14 22:56:27 - INFO - root -   Epoch: [64/500][60/346], lr: 0.00000192 	 loss = 0.9014(1.1854)
2023/09/14 22:56:48 - INFO - root -   Epoch: [64/500][80/346], lr: 0.00000192 	 loss = 1.1889(1.2134)
2023/09/14 22:57:04 - INFO - root -   Epoch: [64/500][100/346], lr: 0.00000192 	 loss = 0.7184(1.2145)
2023/09/14 22:57:25 - INFO - root -   Epoch: [64/500][120/346], lr: 0.00000192 	 loss = 0.8990(1.2204)
2023/09/14 22:57:42 - INFO - root -   Epoch: [64/500][140/346], lr: 0.00000192 	 loss = 1.7961(1.2198)
2023/09/14 22:58:03 - INFO - root -   Epoch: [64/500][160/346], lr: 0.00000192 	 loss = 0.5501(1.1963)
2023/09/14 22:58:20 - INFO - root -   Epoch: [64/500][180/346], lr: 0.00000192 	 loss = 1.2258(1.1908)
2023/09/14 22:58:41 - INFO - root -   Epoch: [64/500][200/346], lr: 0.00000192 	 loss = 0.6643(1.1813)
2023/09/14 22:58:57 - INFO - root -   Epoch: [64/500][220/346], lr: 0.00000192 	 loss = 1.4939(1.1828)
2023/09/14 22:59:18 - INFO - root -   Epoch: [64/500][240/346], lr: 0.00000192 	 loss = 0.9179(1.1799)
2023/09/14 22:59:35 - INFO - root -   Epoch: [64/500][260/346], lr: 0.00000192 	 loss = 0.4842(1.1799)
2023/09/14 22:59:56 - INFO - root -   Epoch: [64/500][280/346], lr: 0.00000192 	 loss = 1.3856(1.1723)
2023/09/14 23:00:12 - INFO - root -   Epoch: [64/500][300/346], lr: 0.00000192 	 loss = 2.1216(1.1684)
2023/09/14 23:00:33 - INFO - root -   Epoch: [64/500][320/346], lr: 0.00000192 	 loss = 0.5229(1.1740)
2023/09/14 23:00:50 - INFO - root -   Epoch: [64/500][340/346], lr: 0.00000192 	 loss = 1.9389(1.1742)
2023/09/14 23:00:52 - INFO - root -   Epoch: [64/500] 	 loss = 1.1729
2023/09/14 23:02:14 - INFO - root -   precision = 0.5345
2023/09/14 23:02:14 - INFO - root -   eval_loss = 1.1459
2023/09/14 23:02:15 - INFO - root -   train_accuracy = 0.5231
2023/09/14 23:02:24 - INFO - root -   Epoch: [65/500][0/346], lr: 0.00000193 	 loss = 1.9633(1.9633)
2023/09/14 23:02:40 - INFO - root -   Epoch: [65/500][20/346], lr: 0.00000193 	 loss = 1.8588(1.1781)
2023/09/14 23:03:01 - INFO - root -   Epoch: [65/500][40/346], lr: 0.00000193 	 loss = 1.4309(1.1722)
2023/09/14 23:03:18 - INFO - root -   Epoch: [65/500][60/346], lr: 0.00000193 	 loss = 0.8484(1.1643)
2023/09/14 23:03:38 - INFO - root -   Epoch: [65/500][80/346], lr: 0.00000193 	 loss = 1.0226(1.1939)
2023/09/14 23:03:55 - INFO - root -   Epoch: [65/500][100/346], lr: 0.00000193 	 loss = 0.7204(1.1898)
2023/09/14 23:04:16 - INFO - root -   Epoch: [65/500][120/346], lr: 0.00000193 	 loss = 1.0489(1.2051)
2023/09/14 23:04:33 - INFO - root -   Epoch: [65/500][140/346], lr: 0.00000193 	 loss = 1.7715(1.1984)
2023/09/14 23:04:54 - INFO - root -   Epoch: [65/500][160/346], lr: 0.00000193 	 loss = 0.5431(1.1787)
2023/09/14 23:05:10 - INFO - root -   Epoch: [65/500][180/346], lr: 0.00000193 	 loss = 1.0893(1.1721)
2023/09/14 23:05:31 - INFO - root -   Epoch: [65/500][200/346], lr: 0.00000193 	 loss = 0.4361(1.1650)
2023/09/14 23:05:48 - INFO - root -   Epoch: [65/500][220/346], lr: 0.00000193 	 loss = 1.5793(1.1673)
2023/09/14 23:06:09 - INFO - root -   Epoch: [65/500][240/346], lr: 0.00000193 	 loss = 1.0338(1.1674)
2023/09/14 23:06:25 - INFO - root -   Epoch: [65/500][260/346], lr: 0.00000193 	 loss = 0.7565(1.1668)
2023/09/14 23:06:46 - INFO - root -   Epoch: [65/500][280/346], lr: 0.00000193 	 loss = 1.3700(1.1572)
2023/09/14 23:07:03 - INFO - root -   Epoch: [65/500][300/346], lr: 0.00000193 	 loss = 2.0706(1.1582)
2023/09/14 23:07:24 - INFO - root -   Epoch: [65/500][320/346], lr: 0.00000193 	 loss = 0.4534(1.1611)
2023/09/14 23:07:40 - INFO - root -   Epoch: [65/500][340/346], lr: 0.00000193 	 loss = 1.8825(1.1655)
2023/09/14 23:07:42 - INFO - root -   Epoch: [65/500] 	 loss = 1.1619
2023/09/14 23:07:42 - INFO - root -   train_accuracy = 0.5202
2023/09/14 23:07:51 - INFO - root -   Epoch: [66/500][0/346], lr: 0.00000194 	 loss = 1.6600(1.6600)
2023/09/14 23:08:07 - INFO - root -   Epoch: [66/500][20/346], lr: 0.00000194 	 loss = 1.9663(1.1473)
2023/09/14 23:08:28 - INFO - root -   Epoch: [66/500][40/346], lr: 0.00000194 	 loss = 1.1929(1.1842)
2023/09/14 23:08:45 - INFO - root -   Epoch: [66/500][60/346], lr: 0.00000194 	 loss = 0.5959(1.1612)
2023/09/14 23:09:06 - INFO - root -   Epoch: [66/500][80/346], lr: 0.00000194 	 loss = 1.0554(1.1995)
2023/09/14 23:09:23 - INFO - root -   Epoch: [66/500][100/346], lr: 0.00000194 	 loss = 0.7482(1.2172)
2023/09/14 23:09:44 - INFO - root -   Epoch: [66/500][120/346], lr: 0.00000194 	 loss = 1.0023(1.2133)
2023/09/14 23:10:00 - INFO - root -   Epoch: [66/500][140/346], lr: 0.00000194 	 loss = 1.6912(1.2224)
2023/09/14 23:10:21 - INFO - root -   Epoch: [66/500][160/346], lr: 0.00000194 	 loss = 0.5285(1.1990)
2023/09/14 23:10:38 - INFO - root -   Epoch: [66/500][180/346], lr: 0.00000194 	 loss = 0.9774(1.1911)
2023/09/14 23:10:59 - INFO - root -   Epoch: [66/500][200/346], lr: 0.00000194 	 loss = 0.5112(1.1796)
2023/09/14 23:11:15 - INFO - root -   Epoch: [66/500][220/346], lr: 0.00000194 	 loss = 1.2832(1.1817)
2023/09/14 23:11:36 - INFO - root -   Epoch: [66/500][240/346], lr: 0.00000194 	 loss = 1.0089(1.1804)
2023/09/14 23:11:53 - INFO - root -   Epoch: [66/500][260/346], lr: 0.00000194 	 loss = 0.7709(1.1823)
2023/09/14 23:12:14 - INFO - root -   Epoch: [66/500][280/346], lr: 0.00000194 	 loss = 1.5440(1.1733)
2023/09/14 23:12:30 - INFO - root -   Epoch: [66/500][300/346], lr: 0.00000194 	 loss = 1.4722(1.1682)
2023/09/14 23:12:51 - INFO - root -   Epoch: [66/500][320/346], lr: 0.00000194 	 loss = 0.4566(1.1742)
2023/09/14 23:13:08 - INFO - root -   Epoch: [66/500][340/346], lr: 0.00000194 	 loss = 2.1171(1.1774)
2023/09/14 23:13:10 - INFO - root -   Epoch: [66/500] 	 loss = 1.1755
2023/09/14 23:13:10 - INFO - root -   train_accuracy = 0.5202
2023/09/14 23:13:18 - INFO - root -   Epoch: [67/500][0/346], lr: 0.00000196 	 loss = 1.4457(1.4457)
2023/09/14 23:13:35 - INFO - root -   Epoch: [67/500][20/346], lr: 0.00000196 	 loss = 1.9640(1.1716)
2023/09/14 23:13:56 - INFO - root -   Epoch: [67/500][40/346], lr: 0.00000196 	 loss = 1.4338(1.1559)
2023/09/14 23:14:12 - INFO - root -   Epoch: [67/500][60/346], lr: 0.00000196 	 loss = 0.5872(1.1674)
2023/09/14 23:14:33 - INFO - root -   Epoch: [67/500][80/346], lr: 0.00000196 	 loss = 0.7707(1.2097)
2023/09/14 23:14:50 - INFO - root -   Epoch: [67/500][100/346], lr: 0.00000196 	 loss = 0.7456(1.2139)
2023/09/14 23:15:11 - INFO - root -   Epoch: [67/500][120/346], lr: 0.00000196 	 loss = 1.0224(1.2227)
2023/09/14 23:15:28 - INFO - root -   Epoch: [67/500][140/346], lr: 0.00000196 	 loss = 1.8682(1.2120)
2023/09/14 23:15:48 - INFO - root -   Epoch: [67/500][160/346], lr: 0.00000196 	 loss = 0.5241(1.1904)
2023/09/14 23:16:05 - INFO - root -   Epoch: [67/500][180/346], lr: 0.00000196 	 loss = 1.1940(1.1819)
2023/09/14 23:16:26 - INFO - root -   Epoch: [67/500][200/346], lr: 0.00000196 	 loss = 0.5406(1.1697)
2023/09/14 23:16:42 - INFO - root -   Epoch: [67/500][220/346], lr: 0.00000196 	 loss = 1.6205(1.1721)
2023/09/14 23:17:03 - INFO - root -   Epoch: [67/500][240/346], lr: 0.00000196 	 loss = 0.8094(1.1700)
2023/09/14 23:17:20 - INFO - root -   Epoch: [67/500][260/346], lr: 0.00000196 	 loss = 0.6917(1.1644)
2023/09/14 23:17:41 - INFO - root -   Epoch: [67/500][280/346], lr: 0.00000196 	 loss = 1.6391(1.1551)
2023/09/14 23:17:58 - INFO - root -   Epoch: [67/500][300/346], lr: 0.00000196 	 loss = 2.2568(1.1551)
2023/09/14 23:18:19 - INFO - root -   Epoch: [67/500][320/346], lr: 0.00000196 	 loss = 0.5837(1.1604)
2023/09/14 23:18:35 - INFO - root -   Epoch: [67/500][340/346], lr: 0.00000196 	 loss = 1.6933(1.1615)
2023/09/14 23:18:37 - INFO - root -   Epoch: [67/500] 	 loss = 1.1601
2023/09/14 23:18:37 - INFO - root -   train_accuracy = 0.5231
2023/09/14 23:18:45 - INFO - root -   Epoch: [68/500][0/346], lr: 0.00000197 	 loss = 1.3409(1.3409)
2023/09/14 23:19:02 - INFO - root -   Epoch: [68/500][20/346], lr: 0.00000197 	 loss = 1.8397(1.1081)
2023/09/14 23:19:23 - INFO - root -   Epoch: [68/500][40/346], lr: 0.00000197 	 loss = 1.2367(1.1426)
2023/09/14 23:19:39 - INFO - root -   Epoch: [68/500][60/346], lr: 0.00000197 	 loss = 0.8914(1.1388)
2023/09/14 23:20:00 - INFO - root -   Epoch: [68/500][80/346], lr: 0.00000197 	 loss = 0.8990(1.1734)
2023/09/14 23:20:17 - INFO - root -   Epoch: [68/500][100/346], lr: 0.00000197 	 loss = 0.7001(1.1957)
2023/09/14 23:20:38 - INFO - root -   Epoch: [68/500][120/346], lr: 0.00000197 	 loss = 0.7616(1.2148)
2023/09/14 23:20:55 - INFO - root -   Epoch: [68/500][140/346], lr: 0.00000197 	 loss = 1.8938(1.2146)
2023/09/14 23:21:15 - INFO - root -   Epoch: [68/500][160/346], lr: 0.00000197 	 loss = 0.7204(1.1906)
2023/09/14 23:21:32 - INFO - root -   Epoch: [68/500][180/346], lr: 0.00000197 	 loss = 1.2607(1.1816)
2023/09/14 23:21:53 - INFO - root -   Epoch: [68/500][200/346], lr: 0.00000197 	 loss = 0.4644(1.1705)
2023/09/14 23:22:09 - INFO - root -   Epoch: [68/500][220/346], lr: 0.00000197 	 loss = 1.5954(1.1705)
2023/09/14 23:22:30 - INFO - root -   Epoch: [68/500][240/346], lr: 0.00000197 	 loss = 0.9530(1.1712)
2023/09/14 23:22:47 - INFO - root -   Epoch: [68/500][260/346], lr: 0.00000197 	 loss = 0.9169(1.1738)
2023/09/14 23:23:08 - INFO - root -   Epoch: [68/500][280/346], lr: 0.00000197 	 loss = 1.6521(1.1652)
2023/09/14 23:23:24 - INFO - root -   Epoch: [68/500][300/346], lr: 0.00000197 	 loss = 2.1644(1.1650)
2023/09/14 23:23:45 - INFO - root -   Epoch: [68/500][320/346], lr: 0.00000197 	 loss = 0.5601(1.1687)
2023/09/14 23:24:01 - INFO - root -   Epoch: [68/500][340/346], lr: 0.00000197 	 loss = 2.0479(1.1695)
2023/09/14 23:24:03 - INFO - root -   Epoch: [68/500] 	 loss = 1.1671
2023/09/14 23:24:03 - INFO - root -   train_accuracy = 0.5318
2023/09/14 23:24:11 - INFO - root -   Epoch: [69/500][0/346], lr: 0.00000199 	 loss = 1.6267(1.6267)
2023/09/14 23:24:28 - INFO - root -   Epoch: [69/500][20/346], lr: 0.00000199 	 loss = 2.2383(1.2189)
2023/09/14 23:24:49 - INFO - root -   Epoch: [69/500][40/346], lr: 0.00000199 	 loss = 1.6039(1.2088)
2023/09/14 23:25:06 - INFO - root -   Epoch: [69/500][60/346], lr: 0.00000199 	 loss = 0.7060(1.1975)
2023/09/14 23:25:26 - INFO - root -   Epoch: [69/500][80/346], lr: 0.00000199 	 loss = 1.0071(1.2202)
2023/09/14 23:25:43 - INFO - root -   Epoch: [69/500][100/346], lr: 0.00000199 	 loss = 0.6636(1.2247)
2023/09/14 23:26:04 - INFO - root -   Epoch: [69/500][120/346], lr: 0.00000199 	 loss = 0.7966(1.2267)
2023/09/14 23:26:21 - INFO - root -   Epoch: [69/500][140/346], lr: 0.00000199 	 loss = 1.5451(1.2122)
2023/09/14 23:26:41 - INFO - root -   Epoch: [69/500][160/346], lr: 0.00000199 	 loss = 0.6580(1.1929)
2023/09/14 23:26:58 - INFO - root -   Epoch: [69/500][180/346], lr: 0.00000199 	 loss = 0.9015(1.1901)
2023/09/14 23:27:19 - INFO - root -   Epoch: [69/500][200/346], lr: 0.00000199 	 loss = 0.5523(1.1782)
2023/09/14 23:27:35 - INFO - root -   Epoch: [69/500][220/346], lr: 0.00000199 	 loss = 1.5784(1.1763)
2023/09/14 23:27:56 - INFO - root -   Epoch: [69/500][240/346], lr: 0.00000199 	 loss = 0.7379(1.1773)
2023/09/14 23:28:13 - INFO - root -   Epoch: [69/500][260/346], lr: 0.00000199 	 loss = 0.4932(1.1762)
2023/09/14 23:28:34 - INFO - root -   Epoch: [69/500][280/346], lr: 0.00000199 	 loss = 1.4715(1.1669)
2023/09/14 23:28:50 - INFO - root -   Epoch: [69/500][300/346], lr: 0.00000199 	 loss = 1.8537(1.1622)
2023/09/14 23:29:11 - INFO - root -   Epoch: [69/500][320/346], lr: 0.00000199 	 loss = 0.5994(1.1657)
2023/09/14 23:29:28 - INFO - root -   Epoch: [69/500][340/346], lr: 0.00000199 	 loss = 1.6158(1.1656)
2023/09/14 23:29:30 - INFO - root -   Epoch: [69/500] 	 loss = 1.1630
2023/09/14 23:30:52 - INFO - root -   precision = 0.5345
2023/09/14 23:30:52 - INFO - root -   eval_loss = 1.1430
2023/09/14 23:30:53 - INFO - root -   train_accuracy = 0.5188
2023/09/14 23:31:02 - INFO - root -   Epoch: [70/500][0/346], lr: 0.00000200 	 loss = 2.0600(2.0600)
2023/09/14 23:31:18 - INFO - root -   Epoch: [70/500][20/346], lr: 0.00000200 	 loss = 2.0621(1.2069)
2023/09/14 23:31:39 - INFO - root -   Epoch: [70/500][40/346], lr: 0.00000200 	 loss = 1.4787(1.2049)
2023/09/14 23:31:56 - INFO - root -   Epoch: [70/500][60/346], lr: 0.00000200 	 loss = 0.7408(1.1803)
2023/09/14 23:32:16 - INFO - root -   Epoch: [70/500][80/346], lr: 0.00000200 	 loss = 0.8514(1.1993)
2023/09/14 23:32:33 - INFO - root -   Epoch: [70/500][100/346], lr: 0.00000200 	 loss = 0.7829(1.2148)
2023/09/14 23:32:54 - INFO - root -   Epoch: [70/500][120/346], lr: 0.00000200 	 loss = 1.1654(1.2134)
2023/09/14 23:33:10 - INFO - root -   Epoch: [70/500][140/346], lr: 0.00000200 	 loss = 1.4165(1.2048)
2023/09/14 23:33:31 - INFO - root -   Epoch: [70/500][160/346], lr: 0.00000200 	 loss = 0.5334(1.1830)
2023/09/14 23:33:48 - INFO - root -   Epoch: [70/500][180/346], lr: 0.00000200 	 loss = 1.3040(1.1720)
2023/09/14 23:34:09 - INFO - root -   Epoch: [70/500][200/346], lr: 0.00000200 	 loss = 0.5048(1.1515)
2023/09/14 23:34:26 - INFO - root -   Epoch: [70/500][220/346], lr: 0.00000200 	 loss = 2.2030(1.1578)
2023/09/14 23:34:46 - INFO - root -   Epoch: [70/500][240/346], lr: 0.00000200 	 loss = 0.7700(1.1592)
2023/09/14 23:35:03 - INFO - root -   Epoch: [70/500][260/346], lr: 0.00000200 	 loss = 0.8642(1.1599)
2023/09/14 23:35:24 - INFO - root -   Epoch: [70/500][280/346], lr: 0.00000200 	 loss = 1.6329(1.1497)
2023/09/14 23:35:40 - INFO - root -   Epoch: [70/500][300/346], lr: 0.00000200 	 loss = 2.1541(1.1494)
2023/09/14 23:36:01 - INFO - root -   Epoch: [70/500][320/346], lr: 0.00000200 	 loss = 0.6646(1.1556)
2023/09/14 23:36:18 - INFO - root -   Epoch: [70/500][340/346], lr: 0.00000200 	 loss = 1.9956(1.1581)
2023/09/14 23:36:20 - INFO - root -   Epoch: [70/500] 	 loss = 1.1546
2023/09/14 23:36:20 - INFO - root -   train_accuracy = 0.5361
2023/09/14 23:36:28 - INFO - root -   Epoch: [71/500][0/346], lr: 0.00000202 	 loss = 1.5765(1.5765)
2023/09/14 23:36:45 - INFO - root -   Epoch: [71/500][20/346], lr: 0.00000202 	 loss = 2.1616(1.1835)
2023/09/14 23:37:06 - INFO - root -   Epoch: [71/500][40/346], lr: 0.00000202 	 loss = 1.2536(1.1860)
2023/09/14 23:37:22 - INFO - root -   Epoch: [71/500][60/346], lr: 0.00000202 	 loss = 0.6058(1.1700)
2023/09/14 23:37:43 - INFO - root -   Epoch: [71/500][80/346], lr: 0.00000202 	 loss = 0.9371(1.2052)
2023/09/14 23:37:59 - INFO - root -   Epoch: [71/500][100/346], lr: 0.00000202 	 loss = 0.8440(1.2026)
2023/09/14 23:38:21 - INFO - root -   Epoch: [71/500][120/346], lr: 0.00000202 	 loss = 1.0650(1.2120)
2023/09/14 23:38:37 - INFO - root -   Epoch: [71/500][140/346], lr: 0.00000202 	 loss = 2.0295(1.2081)
2023/09/14 23:38:58 - INFO - root -   Epoch: [71/500][160/346], lr: 0.00000202 	 loss = 0.5077(1.1893)
2023/09/14 23:39:15 - INFO - root -   Epoch: [71/500][180/346], lr: 0.00000202 	 loss = 1.2558(1.1836)
2023/09/14 23:39:36 - INFO - root -   Epoch: [71/500][200/346], lr: 0.00000202 	 loss = 0.4950(1.1675)
2023/09/14 23:39:52 - INFO - root -   Epoch: [71/500][220/346], lr: 0.00000202 	 loss = 1.4825(1.1709)
2023/09/14 23:40:13 - INFO - root -   Epoch: [71/500][240/346], lr: 0.00000202 	 loss = 0.7908(1.1715)
2023/09/14 23:40:29 - INFO - root -   Epoch: [71/500][260/346], lr: 0.00000202 	 loss = 0.8689(1.1693)
2023/09/14 23:40:50 - INFO - root -   Epoch: [71/500][280/346], lr: 0.00000202 	 loss = 1.4718(1.1597)
2023/09/14 23:41:07 - INFO - root -   Epoch: [71/500][300/346], lr: 0.00000202 	 loss = 2.0348(1.1584)
2023/09/14 23:41:27 - INFO - root -   Epoch: [71/500][320/346], lr: 0.00000202 	 loss = 0.5375(1.1604)
2023/09/14 23:41:44 - INFO - root -   Epoch: [71/500][340/346], lr: 0.00000202 	 loss = 2.2130(1.1629)
2023/09/14 23:41:46 - INFO - root -   Epoch: [71/500] 	 loss = 1.1601
2023/09/14 23:41:46 - INFO - root -   train_accuracy = 0.5145
2023/09/14 23:41:54 - INFO - root -   Epoch: [72/500][0/346], lr: 0.00000203 	 loss = 1.7295(1.7295)
2023/09/14 23:42:11 - INFO - root -   Epoch: [72/500][20/346], lr: 0.00000203 	 loss = 1.8394(1.1290)
2023/09/14 23:42:32 - INFO - root -   Epoch: [72/500][40/346], lr: 0.00000203 	 loss = 1.2588(1.1511)
2023/09/14 23:42:48 - INFO - root -   Epoch: [72/500][60/346], lr: 0.00000203 	 loss = 0.7910(1.1629)
2023/09/14 23:43:09 - INFO - root -   Epoch: [72/500][80/346], lr: 0.00000203 	 loss = 0.9465(1.1864)
2023/09/14 23:43:26 - INFO - root -   Epoch: [72/500][100/346], lr: 0.00000203 	 loss = 0.6804(1.1835)
2023/09/14 23:43:47 - INFO - root -   Epoch: [72/500][120/346], lr: 0.00000203 	 loss = 0.9278(1.1929)
2023/09/14 23:44:03 - INFO - root -   Epoch: [72/500][140/346], lr: 0.00000203 	 loss = 1.8220(1.1924)
2023/09/14 23:44:24 - INFO - root -   Epoch: [72/500][160/346], lr: 0.00000203 	 loss = 0.7548(1.1732)
2023/09/14 23:44:41 - INFO - root -   Epoch: [72/500][180/346], lr: 0.00000203 	 loss = 1.1274(1.1685)
2023/09/14 23:45:02 - INFO - root -   Epoch: [72/500][200/346], lr: 0.00000203 	 loss = 0.4557(1.1542)
2023/09/14 23:45:18 - INFO - root -   Epoch: [72/500][220/346], lr: 0.00000203 	 loss = 1.7139(1.1605)
2023/09/14 23:45:39 - INFO - root -   Epoch: [72/500][240/346], lr: 0.00000203 	 loss = 0.8643(1.1645)
2023/09/14 23:45:56 - INFO - root -   Epoch: [72/500][260/346], lr: 0.00000203 	 loss = 0.7357(1.1621)
2023/09/14 23:46:17 - INFO - root -   Epoch: [72/500][280/346], lr: 0.00000203 	 loss = 1.8165(1.1535)
2023/09/14 23:46:33 - INFO - root -   Epoch: [72/500][300/346], lr: 0.00000203 	 loss = 1.9800(1.1523)
2023/09/14 23:46:55 - INFO - root -   Epoch: [72/500][320/346], lr: 0.00000203 	 loss = 0.4183(1.1575)
2023/09/14 23:47:11 - INFO - root -   Epoch: [72/500][340/346], lr: 0.00000203 	 loss = 1.9256(1.1628)
2023/09/14 23:47:13 - INFO - root -   Epoch: [72/500] 	 loss = 1.1606
2023/09/14 23:47:13 - INFO - root -   train_accuracy = 0.5332
2023/09/14 23:47:22 - INFO - root -   Epoch: [73/500][0/346], lr: 0.00000204 	 loss = 1.5211(1.5211)
2023/09/14 23:47:38 - INFO - root -   Epoch: [73/500][20/346], lr: 0.00000204 	 loss = 1.5927(1.1362)
2023/09/14 23:47:59 - INFO - root -   Epoch: [73/500][40/346], lr: 0.00000204 	 loss = 1.1934(1.1494)
2023/09/14 23:48:16 - INFO - root -   Epoch: [73/500][60/346], lr: 0.00000204 	 loss = 0.7301(1.1820)
2023/09/14 23:48:37 - INFO - root -   Epoch: [73/500][80/346], lr: 0.00000204 	 loss = 1.0767(1.2206)
2023/09/14 23:48:54 - INFO - root -   Epoch: [73/500][100/346], lr: 0.00000204 	 loss = 1.0023(1.2301)
2023/09/14 23:49:15 - INFO - root -   Epoch: [73/500][120/346], lr: 0.00000204 	 loss = 0.9352(1.2371)
2023/09/14 23:49:32 - INFO - root -   Epoch: [73/500][140/346], lr: 0.00000204 	 loss = 1.6114(1.2173)
2023/09/14 23:49:53 - INFO - root -   Epoch: [73/500][160/346], lr: 0.00000204 	 loss = 0.4266(1.1937)
2023/09/14 23:50:09 - INFO - root -   Epoch: [73/500][180/346], lr: 0.00000204 	 loss = 1.3280(1.1922)
2023/09/14 23:50:30 - INFO - root -   Epoch: [73/500][200/346], lr: 0.00000204 	 loss = 0.5289(1.1781)
2023/09/14 23:50:47 - INFO - root -   Epoch: [73/500][220/346], lr: 0.00000204 	 loss = 1.5149(1.1790)
2023/09/14 23:51:08 - INFO - root -   Epoch: [73/500][240/346], lr: 0.00000204 	 loss = 0.8471(1.1779)
2023/09/14 23:51:25 - INFO - root -   Epoch: [73/500][260/346], lr: 0.00000204 	 loss = 0.7903(1.1749)
2023/09/14 23:51:46 - INFO - root -   Epoch: [73/500][280/346], lr: 0.00000204 	 loss = 1.6676(1.1655)
2023/09/14 23:52:02 - INFO - root -   Epoch: [73/500][300/346], lr: 0.00000204 	 loss = 2.0819(1.1677)
2023/09/14 23:52:23 - INFO - root -   Epoch: [73/500][320/346], lr: 0.00000204 	 loss = 0.6797(1.1727)
2023/09/14 23:52:40 - INFO - root -   Epoch: [73/500][340/346], lr: 0.00000204 	 loss = 2.0453(1.1735)
2023/09/14 23:52:42 - INFO - root -   Epoch: [73/500] 	 loss = 1.1716
2023/09/14 23:52:42 - INFO - root -   train_accuracy = 0.5318
2023/09/14 23:52:50 - INFO - root -   Epoch: [74/500][0/346], lr: 0.00000206 	 loss = 1.6852(1.6852)
2023/09/14 23:53:07 - INFO - root -   Epoch: [74/500][20/346], lr: 0.00000206 	 loss = 2.1202(1.1721)
2023/09/14 23:53:28 - INFO - root -   Epoch: [74/500][40/346], lr: 0.00000206 	 loss = 1.3110(1.1823)
2023/09/14 23:53:44 - INFO - root -   Epoch: [74/500][60/346], lr: 0.00000206 	 loss = 0.8609(1.1665)
2023/09/14 23:54:05 - INFO - root -   Epoch: [74/500][80/346], lr: 0.00000206 	 loss = 1.0610(1.2051)
2023/09/14 23:54:22 - INFO - root -   Epoch: [74/500][100/346], lr: 0.00000206 	 loss = 0.7684(1.2162)
2023/09/14 23:54:43 - INFO - root -   Epoch: [74/500][120/346], lr: 0.00000206 	 loss = 1.0117(1.2316)
2023/09/14 23:54:59 - INFO - root -   Epoch: [74/500][140/346], lr: 0.00000206 	 loss = 1.9533(1.2221)
2023/09/14 23:55:20 - INFO - root -   Epoch: [74/500][160/346], lr: 0.00000206 	 loss = 0.4628(1.1983)
2023/09/14 23:55:37 - INFO - root -   Epoch: [74/500][180/346], lr: 0.00000206 	 loss = 1.3089(1.1871)
2023/09/14 23:55:58 - INFO - root -   Epoch: [74/500][200/346], lr: 0.00000206 	 loss = 0.5284(1.1703)
2023/09/14 23:56:15 - INFO - root -   Epoch: [74/500][220/346], lr: 0.00000206 	 loss = 1.3594(1.1693)
2023/09/14 23:56:36 - INFO - root -   Epoch: [74/500][240/346], lr: 0.00000206 	 loss = 0.6862(1.1665)
2023/09/14 23:56:52 - INFO - root -   Epoch: [74/500][260/346], lr: 0.00000206 	 loss = 0.9211(1.1616)
2023/09/14 23:57:13 - INFO - root -   Epoch: [74/500][280/346], lr: 0.00000206 	 loss = 1.6145(1.1519)
2023/09/14 23:57:30 - INFO - root -   Epoch: [74/500][300/346], lr: 0.00000206 	 loss = 2.1514(1.1541)
2023/09/14 23:57:51 - INFO - root -   Epoch: [74/500][320/346], lr: 0.00000206 	 loss = 0.5417(1.1597)
2023/09/14 23:58:07 - INFO - root -   Epoch: [74/500][340/346], lr: 0.00000206 	 loss = 1.8142(1.1615)
2023/09/14 23:58:09 - INFO - root -   Epoch: [74/500] 	 loss = 1.1598
2023/09/14 23:59:32 - INFO - root -   precision = 0.5345
2023/09/14 23:59:32 - INFO - root -   eval_loss = 1.1315
2023/09/14 23:59:33 - INFO - root -   train_accuracy = 0.5246
2023/09/14 23:59:41 - INFO - root -   Epoch: [75/500][0/346], lr: 0.00000207 	 loss = 1.4289(1.4289)
2023/09/14 23:59:58 - INFO - root -   Epoch: [75/500][20/346], lr: 0.00000207 	 loss = 2.0769(1.0803)
2023/09/15 00:00:18 - INFO - root -   Epoch: [75/500][40/346], lr: 0.00000207 	 loss = 1.3106(1.1107)
2023/09/15 00:00:35 - INFO - root -   Epoch: [75/500][60/346], lr: 0.00000207 	 loss = 0.5175(1.1159)
2023/09/15 00:00:55 - INFO - root -   Epoch: [75/500][80/346], lr: 0.00000207 	 loss = 1.1277(1.1403)
2023/09/15 00:01:12 - INFO - root -   Epoch: [75/500][100/346], lr: 0.00000207 	 loss = 0.7845(1.1558)
2023/09/15 00:01:33 - INFO - root -   Epoch: [75/500][120/346], lr: 0.00000207 	 loss = 1.1563(1.1711)
2023/09/15 00:01:49 - INFO - root -   Epoch: [75/500][140/346], lr: 0.00000207 	 loss = 1.8765(1.1665)
2023/09/15 00:02:10 - INFO - root -   Epoch: [75/500][160/346], lr: 0.00000207 	 loss = 0.5102(1.1466)
2023/09/15 00:02:26 - INFO - root -   Epoch: [75/500][180/346], lr: 0.00000207 	 loss = 1.3472(1.1493)
2023/09/15 00:02:47 - INFO - root -   Epoch: [75/500][200/346], lr: 0.00000207 	 loss = 0.4874(1.1334)
2023/09/15 00:03:03 - INFO - root -   Epoch: [75/500][220/346], lr: 0.00000207 	 loss = 1.2545(1.1357)
2023/09/15 00:03:24 - INFO - root -   Epoch: [75/500][240/346], lr: 0.00000207 	 loss = 1.0672(1.1347)
2023/09/15 00:03:40 - INFO - root -   Epoch: [75/500][260/346], lr: 0.00000207 	 loss = 0.6842(1.1315)
2023/09/15 00:04:01 - INFO - root -   Epoch: [75/500][280/346], lr: 0.00000207 	 loss = 2.3126(1.1246)
2023/09/15 00:04:18 - INFO - root -   Epoch: [75/500][300/346], lr: 0.00000207 	 loss = 1.9528(1.1224)
2023/09/15 00:04:38 - INFO - root -   Epoch: [75/500][320/346], lr: 0.00000207 	 loss = 0.6812(1.1296)
2023/09/15 00:04:53 - INFO - root -   Epoch: [75/500][340/346], lr: 0.00000207 	 loss = 1.8005(1.1319)
2023/09/15 00:04:55 - INFO - root -   Epoch: [75/500] 	 loss = 1.1290
2023/09/15 00:04:55 - INFO - root -   train_accuracy = 0.5390
2023/09/15 00:05:04 - INFO - root -   Epoch: [76/500][0/346], lr: 0.00000209 	 loss = 1.4113(1.4113)
2023/09/15 00:05:20 - INFO - root -   Epoch: [76/500][20/346], lr: 0.00000209 	 loss = 1.9190(1.1385)
2023/09/15 00:05:41 - INFO - root -   Epoch: [76/500][40/346], lr: 0.00000209 	 loss = 1.2612(1.1822)
2023/09/15 00:05:58 - INFO - root -   Epoch: [76/500][60/346], lr: 0.00000209 	 loss = 0.6955(1.1765)
2023/09/15 00:06:18 - INFO - root -   Epoch: [76/500][80/346], lr: 0.00000209 	 loss = 1.0286(1.2009)
2023/09/15 00:06:35 - INFO - root -   Epoch: [76/500][100/346], lr: 0.00000209 	 loss = 0.6166(1.2037)
2023/09/15 00:06:56 - INFO - root -   Epoch: [76/500][120/346], lr: 0.00000209 	 loss = 1.0429(1.2088)
2023/09/15 00:07:13 - INFO - root -   Epoch: [76/500][140/346], lr: 0.00000209 	 loss = 1.7219(1.2029)
2023/09/15 00:07:33 - INFO - root -   Epoch: [76/500][160/346], lr: 0.00000209 	 loss = 0.4758(1.1823)
2023/09/15 00:07:50 - INFO - root -   Epoch: [76/500][180/346], lr: 0.00000209 	 loss = 1.0679(1.1735)
2023/09/15 00:08:11 - INFO - root -   Epoch: [76/500][200/346], lr: 0.00000209 	 loss = 0.5391(1.1537)
2023/09/15 00:08:27 - INFO - root -   Epoch: [76/500][220/346], lr: 0.00000209 	 loss = 1.7088(1.1581)
2023/09/15 00:08:48 - INFO - root -   Epoch: [76/500][240/346], lr: 0.00000209 	 loss = 0.9993(1.1574)
2023/09/15 00:09:05 - INFO - root -   Epoch: [76/500][260/346], lr: 0.00000209 	 loss = 0.6088(1.1601)
2023/09/15 00:09:26 - INFO - root -   Epoch: [76/500][280/346], lr: 0.00000209 	 loss = 1.9381(1.1499)
2023/09/15 00:09:42 - INFO - root -   Epoch: [76/500][300/346], lr: 0.00000209 	 loss = 1.9447(1.1476)
2023/09/15 00:10:03 - INFO - root -   Epoch: [76/500][320/346], lr: 0.00000209 	 loss = 0.5395(1.1504)
2023/09/15 00:10:19 - INFO - root -   Epoch: [76/500][340/346], lr: 0.00000209 	 loss = 1.9120(1.1498)
2023/09/15 00:10:21 - INFO - root -   Epoch: [76/500] 	 loss = 1.1473
2023/09/15 00:10:21 - INFO - root -   train_accuracy = 0.5217
2023/09/15 00:10:30 - INFO - root -   Epoch: [77/500][0/346], lr: 0.00000210 	 loss = 1.4625(1.4625)
2023/09/15 00:10:46 - INFO - root -   Epoch: [77/500][20/346], lr: 0.00000210 	 loss = 1.8764(1.1475)
2023/09/15 00:11:08 - INFO - root -   Epoch: [77/500][40/346], lr: 0.00000210 	 loss = 1.1364(1.1478)
2023/09/15 00:11:24 - INFO - root -   Epoch: [77/500][60/346], lr: 0.00000210 	 loss = 0.8840(1.1746)
2023/09/15 00:11:45 - INFO - root -   Epoch: [77/500][80/346], lr: 0.00000210 	 loss = 0.9360(1.1969)
2023/09/15 00:12:01 - INFO - root -   Epoch: [77/500][100/346], lr: 0.00000210 	 loss = 0.6768(1.2006)
2023/09/15 00:12:22 - INFO - root -   Epoch: [77/500][120/346], lr: 0.00000210 	 loss = 1.1166(1.2127)
2023/09/15 00:12:39 - INFO - root -   Epoch: [77/500][140/346], lr: 0.00000210 	 loss = 1.7930(1.2069)
2023/09/15 00:13:00 - INFO - root -   Epoch: [77/500][160/346], lr: 0.00000210 	 loss = 0.4817(1.1866)
2023/09/15 00:13:16 - INFO - root -   Epoch: [77/500][180/346], lr: 0.00000210 	 loss = 1.0180(1.1760)
2023/09/15 00:13:37 - INFO - root -   Epoch: [77/500][200/346], lr: 0.00000210 	 loss = 0.4428(1.1692)
2023/09/15 00:13:54 - INFO - root -   Epoch: [77/500][220/346], lr: 0.00000210 	 loss = 1.5301(1.1671)
2023/09/15 00:14:15 - INFO - root -   Epoch: [77/500][240/346], lr: 0.00000210 	 loss = 1.0291(1.1637)
2023/09/15 00:14:31 - INFO - root -   Epoch: [77/500][260/346], lr: 0.00000210 	 loss = 0.7853(1.1655)
2023/09/15 00:14:52 - INFO - root -   Epoch: [77/500][280/346], lr: 0.00000210 	 loss = 1.9336(1.1592)
2023/09/15 00:15:09 - INFO - root -   Epoch: [77/500][300/346], lr: 0.00000210 	 loss = 1.9267(1.1583)
2023/09/15 00:15:30 - INFO - root -   Epoch: [77/500][320/346], lr: 0.00000210 	 loss = 0.5105(1.1634)
2023/09/15 00:15:47 - INFO - root -   Epoch: [77/500][340/346], lr: 0.00000210 	 loss = 1.7788(1.1628)
2023/09/15 00:15:49 - INFO - root -   Epoch: [77/500] 	 loss = 1.1601
2023/09/15 00:15:49 - INFO - root -   train_accuracy = 0.5303
2023/09/15 00:15:57 - INFO - root -   Epoch: [78/500][0/346], lr: 0.00000212 	 loss = 1.8329(1.8329)
2023/09/15 00:16:13 - INFO - root -   Epoch: [78/500][20/346], lr: 0.00000212 	 loss = 2.1244(1.1901)
2023/09/15 00:16:34 - INFO - root -   Epoch: [78/500][40/346], lr: 0.00000212 	 loss = 1.3802(1.1588)
2023/09/15 00:16:51 - INFO - root -   Epoch: [78/500][60/346], lr: 0.00000212 	 loss = 0.7712(1.1620)
2023/09/15 00:17:12 - INFO - root -   Epoch: [78/500][80/346], lr: 0.00000212 	 loss = 1.1705(1.1935)
2023/09/15 00:17:28 - INFO - root -   Epoch: [78/500][100/346], lr: 0.00000212 	 loss = 0.7139(1.1890)
2023/09/15 00:17:49 - INFO - root -   Epoch: [78/500][120/346], lr: 0.00000212 	 loss = 1.0355(1.2006)
2023/09/15 00:18:06 - INFO - root -   Epoch: [78/500][140/346], lr: 0.00000212 	 loss = 1.7800(1.1907)
2023/09/15 00:18:27 - INFO - root -   Epoch: [78/500][160/346], lr: 0.00000212 	 loss = 0.5662(1.1612)
2023/09/15 00:18:43 - INFO - root -   Epoch: [78/500][180/346], lr: 0.00000212 	 loss = 1.0230(1.1537)
2023/09/15 00:19:04 - INFO - root -   Epoch: [78/500][200/346], lr: 0.00000212 	 loss = 0.5705(1.1435)
2023/09/15 00:19:21 - INFO - root -   Epoch: [78/500][220/346], lr: 0.00000212 	 loss = 1.7056(1.1489)
2023/09/15 00:19:42 - INFO - root -   Epoch: [78/500][240/346], lr: 0.00000212 	 loss = 0.9602(1.1520)
2023/09/15 00:19:58 - INFO - root -   Epoch: [78/500][260/346], lr: 0.00000212 	 loss = 0.7526(1.1510)
2023/09/15 00:20:19 - INFO - root -   Epoch: [78/500][280/346], lr: 0.00000212 	 loss = 1.5545(1.1452)
2023/09/15 00:20:36 - INFO - root -   Epoch: [78/500][300/346], lr: 0.00000212 	 loss = 1.5551(1.1443)
2023/09/15 00:20:57 - INFO - root -   Epoch: [78/500][320/346], lr: 0.00000212 	 loss = 0.6151(1.1488)
2023/09/15 00:21:13 - INFO - root -   Epoch: [78/500][340/346], lr: 0.00000212 	 loss = 1.6016(1.1483)
2023/09/15 00:21:15 - INFO - root -   Epoch: [78/500] 	 loss = 1.1469
2023/09/15 00:21:15 - INFO - root -   train_accuracy = 0.5303
2023/09/15 00:21:23 - INFO - root -   Epoch: [79/500][0/346], lr: 0.00000213 	 loss = 1.6063(1.6063)
2023/09/15 00:21:40 - INFO - root -   Epoch: [79/500][20/346], lr: 0.00000213 	 loss = 1.6371(1.1293)
2023/09/15 00:22:01 - INFO - root -   Epoch: [79/500][40/346], lr: 0.00000213 	 loss = 1.0309(1.1164)
2023/09/15 00:22:18 - INFO - root -   Epoch: [79/500][60/346], lr: 0.00000213 	 loss = 0.5880(1.1208)
2023/09/15 00:22:38 - INFO - root -   Epoch: [79/500][80/346], lr: 0.00000213 	 loss = 0.9869(1.1569)
2023/09/15 00:22:55 - INFO - root -   Epoch: [79/500][100/346], lr: 0.00000213 	 loss = 0.7779(1.1536)
2023/09/15 00:23:16 - INFO - root -   Epoch: [79/500][120/346], lr: 0.00000213 	 loss = 1.0230(1.1736)
2023/09/15 00:23:33 - INFO - root -   Epoch: [79/500][140/346], lr: 0.00000213 	 loss = 2.2146(1.1743)
2023/09/15 00:23:54 - INFO - root -   Epoch: [79/500][160/346], lr: 0.00000213 	 loss = 0.4403(1.1490)
2023/09/15 00:24:10 - INFO - root -   Epoch: [79/500][180/346], lr: 0.00000213 	 loss = 1.1936(1.1482)
2023/09/15 00:24:31 - INFO - root -   Epoch: [79/500][200/346], lr: 0.00000213 	 loss = 0.3947(1.1328)
2023/09/15 00:24:48 - INFO - root -   Epoch: [79/500][220/346], lr: 0.00000213 	 loss = 1.6811(1.1388)
2023/09/15 00:25:09 - INFO - root -   Epoch: [79/500][240/346], lr: 0.00000213 	 loss = 0.8055(1.1408)
2023/09/15 00:25:26 - INFO - root -   Epoch: [79/500][260/346], lr: 0.00000213 	 loss = 0.9103(1.1397)
2023/09/15 00:25:46 - INFO - root -   Epoch: [79/500][280/346], lr: 0.00000213 	 loss = 1.3824(1.1350)
2023/09/15 00:26:03 - INFO - root -   Epoch: [79/500][300/346], lr: 0.00000213 	 loss = 1.8972(1.1359)
2023/09/15 00:26:24 - INFO - root -   Epoch: [79/500][320/346], lr: 0.00000213 	 loss = 0.5233(1.1416)
2023/09/15 00:26:40 - INFO - root -   Epoch: [79/500][340/346], lr: 0.00000213 	 loss = 1.5341(1.1420)
2023/09/15 00:26:42 - INFO - root -   Epoch: [79/500] 	 loss = 1.1405
2023/09/15 00:28:05 - INFO - root -   precision = 0.5345
2023/09/15 00:28:05 - INFO - root -   eval_loss = 1.1245
2023/09/15 00:28:06 - INFO - root -   train_accuracy = 0.5246
2023/09/15 00:28:14 - INFO - root -   Epoch: [80/500][0/346], lr: 0.00000214 	 loss = 1.6345(1.6345)
2023/09/15 00:28:31 - INFO - root -   Epoch: [80/500][20/346], lr: 0.00000214 	 loss = 1.9448(1.1369)
2023/09/15 00:28:52 - INFO - root -   Epoch: [80/500][40/346], lr: 0.00000214 	 loss = 1.0699(1.1021)
2023/09/15 00:29:09 - INFO - root -   Epoch: [80/500][60/346], lr: 0.00000214 	 loss = 0.7598(1.1114)
2023/09/15 00:29:30 - INFO - root -   Epoch: [80/500][80/346], lr: 0.00000214 	 loss = 0.7529(1.1548)
2023/09/15 00:29:47 - INFO - root -   Epoch: [80/500][100/346], lr: 0.00000214 	 loss = 0.6770(1.1678)
2023/09/15 00:30:08 - INFO - root -   Epoch: [80/500][120/346], lr: 0.00000214 	 loss = 1.0105(1.1753)
2023/09/15 00:30:24 - INFO - root -   Epoch: [80/500][140/346], lr: 0.00000214 	 loss = 2.5150(1.1735)
2023/09/15 00:30:46 - INFO - root -   Epoch: [80/500][160/346], lr: 0.00000214 	 loss = 0.6289(1.1587)
2023/09/15 00:31:02 - INFO - root -   Epoch: [80/500][180/346], lr: 0.00000214 	 loss = 1.1695(1.1560)
2023/09/15 00:31:23 - INFO - root -   Epoch: [80/500][200/346], lr: 0.00000214 	 loss = 0.4237(1.1434)
2023/09/15 00:31:40 - INFO - root -   Epoch: [80/500][220/346], lr: 0.00000214 	 loss = 1.6371(1.1463)
2023/09/15 00:32:01 - INFO - root -   Epoch: [80/500][240/346], lr: 0.00000214 	 loss = 0.8928(1.1482)
2023/09/15 00:32:17 - INFO - root -   Epoch: [80/500][260/346], lr: 0.00000214 	 loss = 0.6895(1.1474)
2023/09/15 00:32:38 - INFO - root -   Epoch: [80/500][280/346], lr: 0.00000214 	 loss = 1.8364(1.1429)
2023/09/15 00:32:55 - INFO - root -   Epoch: [80/500][300/346], lr: 0.00000214 	 loss = 1.7483(1.1396)
2023/09/15 00:33:16 - INFO - root -   Epoch: [80/500][320/346], lr: 0.00000214 	 loss = 0.4930(1.1447)
2023/09/15 00:33:32 - INFO - root -   Epoch: [80/500][340/346], lr: 0.00000214 	 loss = 2.0846(1.1447)
2023/09/15 00:33:34 - INFO - root -   Epoch: [80/500] 	 loss = 1.1411
2023/09/15 00:33:34 - INFO - root -   train_accuracy = 0.5332
2023/09/15 00:33:42 - INFO - root -   Epoch: [81/500][0/346], lr: 0.00000216 	 loss = 1.6243(1.6243)
2023/09/15 00:33:59 - INFO - root -   Epoch: [81/500][20/346], lr: 0.00000216 	 loss = 1.6693(1.1195)
2023/09/15 00:34:20 - INFO - root -   Epoch: [81/500][40/346], lr: 0.00000216 	 loss = 1.3308(1.1371)
2023/09/15 00:34:36 - INFO - root -   Epoch: [81/500][60/346], lr: 0.00000216 	 loss = 0.5702(1.1282)
2023/09/15 00:34:57 - INFO - root -   Epoch: [81/500][80/346], lr: 0.00000216 	 loss = 0.9442(1.1704)
2023/09/15 00:35:13 - INFO - root -   Epoch: [81/500][100/346], lr: 0.00000216 	 loss = 0.7035(1.1848)
2023/09/15 00:35:34 - INFO - root -   Epoch: [81/500][120/346], lr: 0.00000216 	 loss = 0.9330(1.1978)
2023/09/15 00:35:51 - INFO - root -   Epoch: [81/500][140/346], lr: 0.00000216 	 loss = 1.9950(1.1911)
2023/09/15 00:36:12 - INFO - root -   Epoch: [81/500][160/346], lr: 0.00000216 	 loss = 0.4936(1.1615)
2023/09/15 00:36:28 - INFO - root -   Epoch: [81/500][180/346], lr: 0.00000216 	 loss = 1.0841(1.1546)
2023/09/15 00:36:49 - INFO - root -   Epoch: [81/500][200/346], lr: 0.00000216 	 loss = 0.4137(1.1431)
2023/09/15 00:37:06 - INFO - root -   Epoch: [81/500][220/346], lr: 0.00000216 	 loss = 1.7994(1.1442)
2023/09/15 00:37:27 - INFO - root -   Epoch: [81/500][240/346], lr: 0.00000216 	 loss = 0.6310(1.1470)
2023/09/15 00:37:44 - INFO - root -   Epoch: [81/500][260/346], lr: 0.00000216 	 loss = 0.7293(1.1417)
2023/09/15 00:38:04 - INFO - root -   Epoch: [81/500][280/346], lr: 0.00000216 	 loss = 1.8495(1.1360)
2023/09/15 00:38:21 - INFO - root -   Epoch: [81/500][300/346], lr: 0.00000216 	 loss = 1.8909(1.1349)
2023/09/15 00:38:42 - INFO - root -   Epoch: [81/500][320/346], lr: 0.00000216 	 loss = 0.4774(1.1392)
2023/09/15 00:38:58 - INFO - root -   Epoch: [81/500][340/346], lr: 0.00000216 	 loss = 1.6295(1.1383)
2023/09/15 00:39:00 - INFO - root -   Epoch: [81/500] 	 loss = 1.1374
2023/09/15 00:39:00 - INFO - root -   train_accuracy = 0.5390
2023/09/15 00:39:08 - INFO - root -   Epoch: [82/500][0/346], lr: 0.00000217 	 loss = 1.4885(1.4885)
2023/09/15 00:39:25 - INFO - root -   Epoch: [82/500][20/346], lr: 0.00000217 	 loss = 1.7700(1.1719)
2023/09/15 00:39:46 - INFO - root -   Epoch: [82/500][40/346], lr: 0.00000217 	 loss = 1.0780(1.1855)
2023/09/15 00:40:03 - INFO - root -   Epoch: [82/500][60/346], lr: 0.00000217 	 loss = 0.7661(1.1524)
2023/09/15 00:40:23 - INFO - root -   Epoch: [82/500][80/346], lr: 0.00000217 	 loss = 0.9253(1.1850)
2023/09/15 00:40:40 - INFO - root -   Epoch: [82/500][100/346], lr: 0.00000217 	 loss = 0.7516(1.1832)
2023/09/15 00:41:01 - INFO - root -   Epoch: [82/500][120/346], lr: 0.00000217 	 loss = 1.3648(1.1921)
2023/09/15 00:41:18 - INFO - root -   Epoch: [82/500][140/346], lr: 0.00000217 	 loss = 2.2100(1.1886)
2023/09/15 00:41:38 - INFO - root -   Epoch: [82/500][160/346], lr: 0.00000217 	 loss = 0.6025(1.1641)
2023/09/15 00:41:55 - INFO - root -   Epoch: [82/500][180/346], lr: 0.00000217 	 loss = 0.9875(1.1636)
2023/09/15 00:42:16 - INFO - root -   Epoch: [82/500][200/346], lr: 0.00000217 	 loss = 0.5358(1.1476)
2023/09/15 00:42:32 - INFO - root -   Epoch: [82/500][220/346], lr: 0.00000217 	 loss = 1.7426(1.1482)
2023/09/15 00:42:53 - INFO - root -   Epoch: [82/500][240/346], lr: 0.00000217 	 loss = 0.6606(1.1487)
2023/09/15 00:43:10 - INFO - root -   Epoch: [82/500][260/346], lr: 0.00000217 	 loss = 0.5798(1.1437)
2023/09/15 00:43:31 - INFO - root -   Epoch: [82/500][280/346], lr: 0.00000217 	 loss = 1.8370(1.1373)
2023/09/15 00:43:48 - INFO - root -   Epoch: [82/500][300/346], lr: 0.00000217 	 loss = 1.9920(1.1369)
2023/09/15 00:44:08 - INFO - root -   Epoch: [82/500][320/346], lr: 0.00000217 	 loss = 0.4928(1.1424)
2023/09/15 00:44:25 - INFO - root -   Epoch: [82/500][340/346], lr: 0.00000217 	 loss = 1.6961(1.1454)
2023/09/15 00:44:27 - INFO - root -   Epoch: [82/500] 	 loss = 1.1432
2023/09/15 00:44:27 - INFO - root -   train_accuracy = 0.5462
2023/09/15 00:44:35 - INFO - root -   Epoch: [83/500][0/346], lr: 0.00000219 	 loss = 1.5724(1.5724)
2023/09/15 00:44:52 - INFO - root -   Epoch: [83/500][20/346], lr: 0.00000219 	 loss = 1.4711(1.0764)
2023/09/15 00:45:13 - INFO - root -   Epoch: [83/500][40/346], lr: 0.00000219 	 loss = 1.0341(1.0742)
2023/09/15 00:45:29 - INFO - root -   Epoch: [83/500][60/346], lr: 0.00000219 	 loss = 0.6659(1.0924)
2023/09/15 00:45:50 - INFO - root -   Epoch: [83/500][80/346], lr: 0.00000219 	 loss = 0.7720(1.1280)
2023/09/15 00:46:07 - INFO - root -   Epoch: [83/500][100/346], lr: 0.00000219 	 loss = 0.6526(1.1307)
2023/09/15 00:46:28 - INFO - root -   Epoch: [83/500][120/346], lr: 0.00000219 	 loss = 1.1658(1.1616)
2023/09/15 00:46:45 - INFO - root -   Epoch: [83/500][140/346], lr: 0.00000219 	 loss = 1.9878(1.1686)
2023/09/15 00:47:06 - INFO - root -   Epoch: [83/500][160/346], lr: 0.00000219 	 loss = 0.6726(1.1489)
2023/09/15 00:47:22 - INFO - root -   Epoch: [83/500][180/346], lr: 0.00000219 	 loss = 1.0719(1.1448)
2023/09/15 00:47:43 - INFO - root -   Epoch: [83/500][200/346], lr: 0.00000219 	 loss = 0.4323(1.1324)
2023/09/15 00:48:00 - INFO - root -   Epoch: [83/500][220/346], lr: 0.00000219 	 loss = 1.9777(1.1324)
2023/09/15 00:48:21 - INFO - root -   Epoch: [83/500][240/346], lr: 0.00000219 	 loss = 0.8212(1.1326)
2023/09/15 00:48:38 - INFO - root -   Epoch: [83/500][260/346], lr: 0.00000219 	 loss = 0.7584(1.1326)
2023/09/15 00:48:59 - INFO - root -   Epoch: [83/500][280/346], lr: 0.00000219 	 loss = 1.6616(1.1307)
2023/09/15 00:49:15 - INFO - root -   Epoch: [83/500][300/346], lr: 0.00000219 	 loss = 1.9704(1.1254)
2023/09/15 00:49:36 - INFO - root -   Epoch: [83/500][320/346], lr: 0.00000219 	 loss = 0.5613(1.1294)
2023/09/15 00:49:53 - INFO - root -   Epoch: [83/500][340/346], lr: 0.00000219 	 loss = 2.2144(1.1326)
2023/09/15 00:49:55 - INFO - root -   Epoch: [83/500] 	 loss = 1.1304
2023/09/15 00:49:55 - INFO - root -   train_accuracy = 0.5303
2023/09/15 00:50:03 - INFO - root -   Epoch: [84/500][0/346], lr: 0.00000220 	 loss = 1.7140(1.7140)
2023/09/15 00:50:20 - INFO - root -   Epoch: [84/500][20/346], lr: 0.00000220 	 loss = 1.7397(1.1533)
2023/09/15 00:50:41 - INFO - root -   Epoch: [84/500][40/346], lr: 0.00000220 	 loss = 1.0912(1.1064)
2023/09/15 00:50:57 - INFO - root -   Epoch: [84/500][60/346], lr: 0.00000220 	 loss = 0.6849(1.1100)
2023/09/15 00:51:18 - INFO - root -   Epoch: [84/500][80/346], lr: 0.00000220 	 loss = 1.0762(1.1520)
2023/09/15 00:51:35 - INFO - root -   Epoch: [84/500][100/346], lr: 0.00000220 	 loss = 0.6430(1.1593)
2023/09/15 00:51:56 - INFO - root -   Epoch: [84/500][120/346], lr: 0.00000220 	 loss = 1.1325(1.1728)
2023/09/15 00:52:12 - INFO - root -   Epoch: [84/500][140/346], lr: 0.00000220 	 loss = 1.8269(1.1646)
2023/09/15 00:52:33 - INFO - root -   Epoch: [84/500][160/346], lr: 0.00000220 	 loss = 0.6061(1.1440)
2023/09/15 00:52:49 - INFO - root -   Epoch: [84/500][180/346], lr: 0.00000220 	 loss = 1.3632(1.1436)
2023/09/15 00:53:10 - INFO - root -   Epoch: [84/500][200/346], lr: 0.00000220 	 loss = 0.4488(1.1360)
2023/09/15 00:53:27 - INFO - root -   Epoch: [84/500][220/346], lr: 0.00000220 	 loss = 1.7561(1.1374)
2023/09/15 00:53:48 - INFO - root -   Epoch: [84/500][240/346], lr: 0.00000220 	 loss = 0.5658(1.1320)
2023/09/15 00:54:04 - INFO - root -   Epoch: [84/500][260/346], lr: 0.00000220 	 loss = 0.6637(1.1302)
2023/09/15 00:54:25 - INFO - root -   Epoch: [84/500][280/346], lr: 0.00000220 	 loss = 1.7182(1.1236)
2023/09/15 00:54:42 - INFO - root -   Epoch: [84/500][300/346], lr: 0.00000220 	 loss = 2.0429(1.1212)
2023/09/15 00:55:03 - INFO - root -   Epoch: [84/500][320/346], lr: 0.00000220 	 loss = 0.5501(1.1261)
2023/09/15 00:55:19 - INFO - root -   Epoch: [84/500][340/346], lr: 0.00000220 	 loss = 1.8615(1.1281)
2023/09/15 00:55:22 - INFO - root -   Epoch: [84/500] 	 loss = 1.1259
2023/09/15 00:56:44 - INFO - root -   precision = 0.5345
2023/09/15 00:56:44 - INFO - root -   eval_loss = 1.1145
2023/09/15 00:56:45 - INFO - root -   train_accuracy = 0.5448
2023/09/15 00:56:53 - INFO - root -   Epoch: [85/500][0/346], lr: 0.00000222 	 loss = 1.6997(1.6997)
2023/09/15 00:57:10 - INFO - root -   Epoch: [85/500][20/346], lr: 0.00000222 	 loss = 1.9670(1.1476)
2023/09/15 00:57:31 - INFO - root -   Epoch: [85/500][40/346], lr: 0.00000222 	 loss = 1.2646(1.1303)
2023/09/15 00:57:47 - INFO - root -   Epoch: [85/500][60/346], lr: 0.00000222 	 loss = 0.7028(1.1248)
2023/09/15 00:58:08 - INFO - root -   Epoch: [85/500][80/346], lr: 0.00000222 	 loss = 1.0617(1.1679)
2023/09/15 00:58:25 - INFO - root -   Epoch: [85/500][100/346], lr: 0.00000222 	 loss = 0.7261(1.1650)
2023/09/15 00:58:46 - INFO - root -   Epoch: [85/500][120/346], lr: 0.00000222 	 loss = 1.0463(1.1851)
2023/09/15 00:59:02 - INFO - root -   Epoch: [85/500][140/346], lr: 0.00000222 	 loss = 2.1668(1.1888)
2023/09/15 00:59:23 - INFO - root -   Epoch: [85/500][160/346], lr: 0.00000222 	 loss = 0.5586(1.1646)
2023/09/15 00:59:40 - INFO - root -   Epoch: [85/500][180/346], lr: 0.00000222 	 loss = 1.0273(1.1623)
2023/09/15 01:00:01 - INFO - root -   Epoch: [85/500][200/346], lr: 0.00000222 	 loss = 0.3714(1.1401)
2023/09/15 01:00:17 - INFO - root -   Epoch: [85/500][220/346], lr: 0.00000222 	 loss = 1.8534(1.1443)
2023/09/15 01:00:38 - INFO - root -   Epoch: [85/500][240/346], lr: 0.00000222 	 loss = 0.5135(1.1408)
2023/09/15 01:00:55 - INFO - root -   Epoch: [85/500][260/346], lr: 0.00000222 	 loss = 0.7961(1.1370)
2023/09/15 01:01:16 - INFO - root -   Epoch: [85/500][280/346], lr: 0.00000222 	 loss = 1.5937(1.1275)
2023/09/15 01:01:33 - INFO - root -   Epoch: [85/500][300/346], lr: 0.00000222 	 loss = 1.9170(1.1265)
2023/09/15 01:01:53 - INFO - root -   Epoch: [85/500][320/346], lr: 0.00000222 	 loss = 0.5060(1.1308)
2023/09/15 01:02:10 - INFO - root -   Epoch: [85/500][340/346], lr: 0.00000222 	 loss = 1.8216(1.1340)
2023/09/15 01:02:12 - INFO - root -   Epoch: [85/500] 	 loss = 1.1322
2023/09/15 01:02:12 - INFO - root -   train_accuracy = 0.5520
2023/09/15 01:02:20 - INFO - root -   Epoch: [86/500][0/346], lr: 0.00000223 	 loss = 1.9390(1.9390)
2023/09/15 01:02:37 - INFO - root -   Epoch: [86/500][20/346], lr: 0.00000223 	 loss = 1.9470(1.1420)
2023/09/15 01:02:58 - INFO - root -   Epoch: [86/500][40/346], lr: 0.00000223 	 loss = 1.1205(1.1090)
2023/09/15 01:03:14 - INFO - root -   Epoch: [86/500][60/346], lr: 0.00000223 	 loss = 0.8145(1.1311)
2023/09/15 01:03:35 - INFO - root -   Epoch: [86/500][80/346], lr: 0.00000223 	 loss = 0.9666(1.1784)
2023/09/15 01:03:52 - INFO - root -   Epoch: [86/500][100/346], lr: 0.00000223 	 loss = 0.4679(1.1717)
2023/09/15 01:04:13 - INFO - root -   Epoch: [86/500][120/346], lr: 0.00000223 	 loss = 0.9530(1.1832)
2023/09/15 01:04:29 - INFO - root -   Epoch: [86/500][140/346], lr: 0.00000223 	 loss = 2.2223(1.1805)
2023/09/15 01:04:50 - INFO - root -   Epoch: [86/500][160/346], lr: 0.00000223 	 loss = 0.6508(1.1519)
2023/09/15 01:05:07 - INFO - root -   Epoch: [86/500][180/346], lr: 0.00000223 	 loss = 1.2677(1.1473)
2023/09/15 01:05:28 - INFO - root -   Epoch: [86/500][200/346], lr: 0.00000223 	 loss = 0.5106(1.1313)
2023/09/15 01:05:44 - INFO - root -   Epoch: [86/500][220/346], lr: 0.00000223 	 loss = 2.2967(1.1399)
2023/09/15 01:06:05 - INFO - root -   Epoch: [86/500][240/346], lr: 0.00000223 	 loss = 0.7845(1.1370)
2023/09/15 01:06:22 - INFO - root -   Epoch: [86/500][260/346], lr: 0.00000223 	 loss = 0.7403(1.1305)
2023/09/15 01:06:43 - INFO - root -   Epoch: [86/500][280/346], lr: 0.00000223 	 loss = 1.3253(1.1259)
2023/09/15 01:06:59 - INFO - root -   Epoch: [86/500][300/346], lr: 0.00000223 	 loss = 1.6800(1.1260)
2023/09/15 01:07:20 - INFO - root -   Epoch: [86/500][320/346], lr: 0.00000223 	 loss = 0.3885(1.1313)
2023/09/15 01:07:37 - INFO - root -   Epoch: [86/500][340/346], lr: 0.00000223 	 loss = 2.2990(1.1384)
2023/09/15 01:07:39 - INFO - root -   Epoch: [86/500] 	 loss = 1.1360
2023/09/15 01:07:39 - INFO - root -   train_accuracy = 0.5231
2023/09/15 01:07:47 - INFO - root -   Epoch: [87/500][0/346], lr: 0.00000224 	 loss = 1.5494(1.5494)
2023/09/15 01:08:04 - INFO - root -   Epoch: [87/500][20/346], lr: 0.00000224 	 loss = 1.7041(1.1475)
2023/09/15 01:08:24 - INFO - root -   Epoch: [87/500][40/346], lr: 0.00000224 	 loss = 1.4183(1.1569)
2023/09/15 01:08:41 - INFO - root -   Epoch: [87/500][60/346], lr: 0.00000224 	 loss = 0.7084(1.1409)
2023/09/15 01:09:02 - INFO - root -   Epoch: [87/500][80/346], lr: 0.00000224 	 loss = 1.0927(1.1825)
2023/09/15 01:09:19 - INFO - root -   Epoch: [87/500][100/346], lr: 0.00000224 	 loss = 0.5146(1.1733)
2023/09/15 01:09:40 - INFO - root -   Epoch: [87/500][120/346], lr: 0.00000224 	 loss = 0.9477(1.1794)
2023/09/15 01:09:56 - INFO - root -   Epoch: [87/500][140/346], lr: 0.00000224 	 loss = 2.1139(1.1824)
2023/09/15 01:10:17 - INFO - root -   Epoch: [87/500][160/346], lr: 0.00000224 	 loss = 0.4977(1.1503)
2023/09/15 01:10:34 - INFO - root -   Epoch: [87/500][180/346], lr: 0.00000224 	 loss = 1.3068(1.1457)
2023/09/15 01:10:55 - INFO - root -   Epoch: [87/500][200/346], lr: 0.00000224 	 loss = 0.5505(1.1281)
2023/09/15 01:11:11 - INFO - root -   Epoch: [87/500][220/346], lr: 0.00000224 	 loss = 1.9079(1.1275)
2023/09/15 01:11:32 - INFO - root -   Epoch: [87/500][240/346], lr: 0.00000224 	 loss = 0.5707(1.1287)
2023/09/15 01:11:49 - INFO - root -   Epoch: [87/500][260/346], lr: 0.00000224 	 loss = 0.8605(1.1233)
2023/09/15 01:12:09 - INFO - root -   Epoch: [87/500][280/346], lr: 0.00000224 	 loss = 1.3225(1.1177)
2023/09/15 01:12:26 - INFO - root -   Epoch: [87/500][300/346], lr: 0.00000224 	 loss = 1.8568(1.1135)
2023/09/15 01:12:47 - INFO - root -   Epoch: [87/500][320/346], lr: 0.00000224 	 loss = 0.6704(1.1175)
2023/09/15 01:13:04 - INFO - root -   Epoch: [87/500][340/346], lr: 0.00000224 	 loss = 1.7676(1.1191)
2023/09/15 01:13:06 - INFO - root -   Epoch: [87/500] 	 loss = 1.1169
2023/09/15 01:13:06 - INFO - root -   train_accuracy = 0.5318
2023/09/15 01:13:14 - INFO - root -   Epoch: [88/500][0/346], lr: 0.00000226 	 loss = 1.5161(1.5161)
2023/09/15 01:13:30 - INFO - root -   Epoch: [88/500][20/346], lr: 0.00000226 	 loss = 1.8071(1.0995)
2023/09/15 01:13:52 - INFO - root -   Epoch: [88/500][40/346], lr: 0.00000226 	 loss = 0.9307(1.0986)
2023/09/15 01:14:08 - INFO - root -   Epoch: [88/500][60/346], lr: 0.00000226 	 loss = 0.6716(1.1117)
2023/09/15 01:14:29 - INFO - root -   Epoch: [88/500][80/346], lr: 0.00000226 	 loss = 0.9547(1.1600)
2023/09/15 01:14:45 - INFO - root -   Epoch: [88/500][100/346], lr: 0.00000226 	 loss = 0.6441(1.1601)
2023/09/15 01:15:06 - INFO - root -   Epoch: [88/500][120/346], lr: 0.00000226 	 loss = 1.2020(1.1729)
2023/09/15 01:15:23 - INFO - root -   Epoch: [88/500][140/346], lr: 0.00000226 	 loss = 1.8647(1.1743)
2023/09/15 01:15:44 - INFO - root -   Epoch: [88/500][160/346], lr: 0.00000226 	 loss = 0.4796(1.1466)
2023/09/15 01:16:00 - INFO - root -   Epoch: [88/500][180/346], lr: 0.00000226 	 loss = 0.9543(1.1365)
2023/09/15 01:16:21 - INFO - root -   Epoch: [88/500][200/346], lr: 0.00000226 	 loss = 0.3638(1.1191)
2023/09/15 01:16:38 - INFO - root -   Epoch: [88/500][220/346], lr: 0.00000226 	 loss = 2.3345(1.1278)
2023/09/15 01:16:59 - INFO - root -   Epoch: [88/500][240/346], lr: 0.00000226 	 loss = 0.7571(1.1289)
2023/09/15 01:17:16 - INFO - root -   Epoch: [88/500][260/346], lr: 0.00000226 	 loss = 0.9780(1.1232)
2023/09/15 01:17:36 - INFO - root -   Epoch: [88/500][280/346], lr: 0.00000226 	 loss = 1.4794(1.1187)
2023/09/15 01:17:53 - INFO - root -   Epoch: [88/500][300/346], lr: 0.00000226 	 loss = 1.6882(1.1170)
2023/09/15 01:18:14 - INFO - root -   Epoch: [88/500][320/346], lr: 0.00000226 	 loss = 0.4132(1.1225)
2023/09/15 01:18:31 - INFO - root -   Epoch: [88/500][340/346], lr: 0.00000226 	 loss = 2.3220(1.1247)
2023/09/15 01:18:33 - INFO - root -   Epoch: [88/500] 	 loss = 1.1212
2023/09/15 01:18:33 - INFO - root -   train_accuracy = 0.5477
2023/09/15 01:18:41 - INFO - root -   Epoch: [89/500][0/346], lr: 0.00000227 	 loss = 1.4510(1.4510)
2023/09/15 01:18:57 - INFO - root -   Epoch: [89/500][20/346], lr: 0.00000227 	 loss = 1.5190(1.1137)
2023/09/15 01:19:18 - INFO - root -   Epoch: [89/500][40/346], lr: 0.00000227 	 loss = 0.9202(1.0819)
2023/09/15 01:19:35 - INFO - root -   Epoch: [89/500][60/346], lr: 0.00000227 	 loss = 0.5566(1.1046)
2023/09/15 01:19:56 - INFO - root -   Epoch: [89/500][80/346], lr: 0.00000227 	 loss = 1.0162(1.1538)
2023/09/15 01:20:12 - INFO - root -   Epoch: [89/500][100/346], lr: 0.00000227 	 loss = 0.7235(1.1440)
2023/09/15 01:20:33 - INFO - root -   Epoch: [89/500][120/346], lr: 0.00000227 	 loss = 1.0276(1.1589)
2023/09/15 01:20:50 - INFO - root -   Epoch: [89/500][140/346], lr: 0.00000227 	 loss = 2.1354(1.1678)
2023/09/15 01:21:11 - INFO - root -   Epoch: [89/500][160/346], lr: 0.00000227 	 loss = 0.5729(1.1496)
2023/09/15 01:21:27 - INFO - root -   Epoch: [89/500][180/346], lr: 0.00000227 	 loss = 1.0916(1.1415)
2023/09/15 01:21:48 - INFO - root -   Epoch: [89/500][200/346], lr: 0.00000227 	 loss = 0.2992(1.1191)
2023/09/15 01:22:04 - INFO - root -   Epoch: [89/500][220/346], lr: 0.00000227 	 loss = 1.7211(1.1251)
2023/09/15 01:22:25 - INFO - root -   Epoch: [89/500][240/346], lr: 0.00000227 	 loss = 0.6435(1.1228)
2023/09/15 01:22:42 - INFO - root -   Epoch: [89/500][260/346], lr: 0.00000227 	 loss = 0.9308(1.1201)
2023/09/15 01:23:02 - INFO - root -   Epoch: [89/500][280/346], lr: 0.00000227 	 loss = 1.5091(1.1144)
2023/09/15 01:23:19 - INFO - root -   Epoch: [89/500][300/346], lr: 0.00000227 	 loss = 1.8375(1.1149)
2023/09/15 01:23:40 - INFO - root -   Epoch: [89/500][320/346], lr: 0.00000227 	 loss = 0.5344(1.1196)
2023/09/15 01:23:56 - INFO - root -   Epoch: [89/500][340/346], lr: 0.00000227 	 loss = 2.0902(1.1207)
2023/09/15 01:23:58 - INFO - root -   Epoch: [89/500] 	 loss = 1.1178
2023/09/15 01:25:21 - INFO - root -   precision = 0.5402
2023/09/15 01:25:21 - INFO - root -   eval_loss = 1.1012
2023/09/15 01:25:22 - INFO - root -   train_accuracy = 0.5303
2023/09/15 01:25:30 - INFO - root -   Epoch: [90/500][0/346], lr: 0.00000229 	 loss = 2.0507(2.0507)
2023/09/15 01:25:46 - INFO - root -   Epoch: [90/500][20/346], lr: 0.00000229 	 loss = 1.7981(1.0615)
2023/09/15 01:26:07 - INFO - root -   Epoch: [90/500][40/346], lr: 0.00000229 	 loss = 0.9536(1.0627)
2023/09/15 01:26:23 - INFO - root -   Epoch: [90/500][60/346], lr: 0.00000229 	 loss = 0.5839(1.0748)
2023/09/15 01:26:43 - INFO - root -   Epoch: [90/500][80/346], lr: 0.00000229 	 loss = 0.7964(1.1329)
2023/09/15 01:27:01 - INFO - root -   Epoch: [90/500][100/346], lr: 0.00000229 	 loss = 0.6480(1.1410)
2023/09/15 01:27:20 - INFO - root -   Epoch: [90/500][120/346], lr: 0.00000229 	 loss = 1.1092(1.1531)
2023/09/15 01:27:37 - INFO - root -   Epoch: [90/500][140/346], lr: 0.00000229 	 loss = 2.2763(1.1596)
2023/09/15 01:27:56 - INFO - root -   Epoch: [90/500][160/346], lr: 0.00000229 	 loss = 0.5056(1.1341)
2023/09/15 01:28:13 - INFO - root -   Epoch: [90/500][180/346], lr: 0.00000229 	 loss = 0.9855(1.1309)
2023/09/15 01:28:33 - INFO - root -   Epoch: [90/500][200/346], lr: 0.00000229 	 loss = 0.3972(1.1153)
2023/09/15 01:28:49 - INFO - root -   Epoch: [90/500][220/346], lr: 0.00000229 	 loss = 1.7702(1.1239)
2023/09/15 01:29:09 - INFO - root -   Epoch: [90/500][240/346], lr: 0.00000229 	 loss = 0.5944(1.1241)
2023/09/15 01:29:25 - INFO - root -   Epoch: [90/500][260/346], lr: 0.00000229 	 loss = 0.9850(1.1174)
2023/09/15 01:29:45 - INFO - root -   Epoch: [90/500][280/346], lr: 0.00000229 	 loss = 1.3163(1.1122)
2023/09/15 01:30:01 - INFO - root -   Epoch: [90/500][300/346], lr: 0.00000229 	 loss = 1.8081(1.1120)
2023/09/15 01:30:22 - INFO - root -   Epoch: [90/500][320/346], lr: 0.00000229 	 loss = 0.4319(1.1135)
2023/09/15 01:30:38 - INFO - root -   Epoch: [90/500][340/346], lr: 0.00000229 	 loss = 1.9307(1.1151)
2023/09/15 01:30:40 - INFO - root -   Epoch: [90/500] 	 loss = 1.1106
2023/09/15 01:30:40 - INFO - root -   train_accuracy = 0.5405
2023/09/15 01:30:48 - INFO - root -   Epoch: [91/500][0/346], lr: 0.00000230 	 loss = 1.6771(1.6771)
2023/09/15 01:31:05 - INFO - root -   Epoch: [91/500][20/346], lr: 0.00000230 	 loss = 1.7192(1.1175)
2023/09/15 01:31:25 - INFO - root -   Epoch: [91/500][40/346], lr: 0.00000230 	 loss = 0.8694(1.0859)
2023/09/15 01:31:42 - INFO - root -   Epoch: [91/500][60/346], lr: 0.00000230 	 loss = 0.4407(1.1016)
2023/09/15 01:32:03 - INFO - root -   Epoch: [91/500][80/346], lr: 0.00000230 	 loss = 0.9061(1.1629)
2023/09/15 01:32:20 - INFO - root -   Epoch: [91/500][100/346], lr: 0.00000230 	 loss = 0.6290(1.1550)
2023/09/15 01:32:41 - INFO - root -   Epoch: [91/500][120/346], lr: 0.00000230 	 loss = 1.1743(1.1603)
2023/09/15 01:32:57 - INFO - root -   Epoch: [91/500][140/346], lr: 0.00000230 	 loss = 1.8466(1.1645)
2023/09/15 01:33:18 - INFO - root -   Epoch: [91/500][160/346], lr: 0.00000230 	 loss = 0.6238(1.1400)
2023/09/15 01:33:35 - INFO - root -   Epoch: [91/500][180/346], lr: 0.00000230 	 loss = 0.9648(1.1366)
2023/09/15 01:33:56 - INFO - root -   Epoch: [91/500][200/346], lr: 0.00000230 	 loss = 0.3701(1.1222)
2023/09/15 01:34:12 - INFO - root -   Epoch: [91/500][220/346], lr: 0.00000230 	 loss = 1.6620(1.1267)
2023/09/15 01:34:33 - INFO - root -   Epoch: [91/500][240/346], lr: 0.00000230 	 loss = 0.5022(1.1246)
2023/09/15 01:34:50 - INFO - root -   Epoch: [91/500][260/346], lr: 0.00000230 	 loss = 0.8915(1.1193)
2023/09/15 01:35:11 - INFO - root -   Epoch: [91/500][280/346], lr: 0.00000230 	 loss = 1.6212(1.1132)
2023/09/15 01:35:27 - INFO - root -   Epoch: [91/500][300/346], lr: 0.00000230 	 loss = 1.7818(1.1141)
2023/09/15 01:35:48 - INFO - root -   Epoch: [91/500][320/346], lr: 0.00000230 	 loss = 0.3350(1.1193)
2023/09/15 01:36:04 - INFO - root -   Epoch: [91/500][340/346], lr: 0.00000230 	 loss = 2.0530(1.1210)
2023/09/15 01:36:06 - INFO - root -   Epoch: [91/500] 	 loss = 1.1173
2023/09/15 01:36:06 - INFO - root -   train_accuracy = 0.5376
2023/09/15 01:36:15 - INFO - root -   Epoch: [92/500][0/346], lr: 0.00000232 	 loss = 1.6690(1.6690)
2023/09/15 01:36:31 - INFO - root -   Epoch: [92/500][20/346], lr: 0.00000232 	 loss = 1.6914(1.1199)
2023/09/15 01:36:52 - INFO - root -   Epoch: [92/500][40/346], lr: 0.00000232 	 loss = 1.0166(1.0881)
2023/09/15 01:37:09 - INFO - root -   Epoch: [92/500][60/346], lr: 0.00000232 	 loss = 0.5170(1.0899)
2023/09/15 01:37:29 - INFO - root -   Epoch: [92/500][80/346], lr: 0.00000232 	 loss = 1.0672(1.1463)
2023/09/15 01:37:46 - INFO - root -   Epoch: [92/500][100/346], lr: 0.00000232 	 loss = 0.6069(1.1470)
2023/09/15 01:38:07 - INFO - root -   Epoch: [92/500][120/346], lr: 0.00000232 	 loss = 1.4068(1.1589)
2023/09/15 01:38:23 - INFO - root -   Epoch: [92/500][140/346], lr: 0.00000232 	 loss = 2.1161(1.1618)
2023/09/15 01:38:44 - INFO - root -   Epoch: [92/500][160/346], lr: 0.00000232 	 loss = 0.6707(1.1351)
2023/09/15 01:39:00 - INFO - root -   Epoch: [92/500][180/346], lr: 0.00000232 	 loss = 1.0207(1.1348)
2023/09/15 01:39:21 - INFO - root -   Epoch: [92/500][200/346], lr: 0.00000232 	 loss = 0.3979(1.1179)
2023/09/15 01:39:38 - INFO - root -   Epoch: [92/500][220/346], lr: 0.00000232 	 loss = 2.0718(1.1225)
2023/09/15 01:39:59 - INFO - root -   Epoch: [92/500][240/346], lr: 0.00000232 	 loss = 0.5185(1.1274)
2023/09/15 01:40:15 - INFO - root -   Epoch: [92/500][260/346], lr: 0.00000232 	 loss = 0.6535(1.1177)
2023/09/15 01:40:36 - INFO - root -   Epoch: [92/500][280/346], lr: 0.00000232 	 loss = 1.6500(1.1147)
2023/09/15 01:40:52 - INFO - root -   Epoch: [92/500][300/346], lr: 0.00000232 	 loss = 1.6392(1.1149)
2023/09/15 01:41:13 - INFO - root -   Epoch: [92/500][320/346], lr: 0.00000232 	 loss = 0.4953(1.1227)
2023/09/15 01:41:30 - INFO - root -   Epoch: [92/500][340/346], lr: 0.00000232 	 loss = 2.1918(1.1254)
2023/09/15 01:41:32 - INFO - root -   Epoch: [92/500] 	 loss = 1.1235
2023/09/15 01:41:32 - INFO - root -   train_accuracy = 0.5578
2023/09/15 01:41:41 - INFO - root -   Epoch: [93/500][0/346], lr: 0.00000233 	 loss = 1.6958(1.6958)
2023/09/15 01:41:57 - INFO - root -   Epoch: [93/500][20/346], lr: 0.00000233 	 loss = 1.7032(1.0941)
2023/09/15 01:42:18 - INFO - root -   Epoch: [93/500][40/346], lr: 0.00000233 	 loss = 0.8907(1.0759)
2023/09/15 01:42:34 - INFO - root -   Epoch: [93/500][60/346], lr: 0.00000233 	 loss = 0.5232(1.0870)
2023/09/15 01:42:55 - INFO - root -   Epoch: [93/500][80/346], lr: 0.00000233 	 loss = 1.1588(1.1489)
2023/09/15 01:43:12 - INFO - root -   Epoch: [93/500][100/346], lr: 0.00000233 	 loss = 0.6314(1.1522)
2023/09/15 01:43:33 - INFO - root -   Epoch: [93/500][120/346], lr: 0.00000233 	 loss = 1.3073(1.1661)
2023/09/15 01:43:49 - INFO - root -   Epoch: [93/500][140/346], lr: 0.00000233 	 loss = 1.9201(1.1610)
2023/09/15 01:44:10 - INFO - root -   Epoch: [93/500][160/346], lr: 0.00000233 	 loss = 0.7088(1.1294)
2023/09/15 01:44:27 - INFO - root -   Epoch: [93/500][180/346], lr: 0.00000233 	 loss = 1.0318(1.1259)
2023/09/15 01:44:47 - INFO - root -   Epoch: [93/500][200/346], lr: 0.00000233 	 loss = 0.3813(1.1068)
2023/09/15 01:45:04 - INFO - root -   Epoch: [93/500][220/346], lr: 0.00000233 	 loss = 1.8643(1.1175)
2023/09/15 01:45:25 - INFO - root -   Epoch: [93/500][240/346], lr: 0.00000233 	 loss = 0.6919(1.1162)
2023/09/15 01:45:42 - INFO - root -   Epoch: [93/500][260/346], lr: 0.00000233 	 loss = 0.7205(1.1054)
2023/09/15 01:46:02 - INFO - root -   Epoch: [93/500][280/346], lr: 0.00000233 	 loss = 1.3294(1.1018)
2023/09/15 01:46:19 - INFO - root -   Epoch: [93/500][300/346], lr: 0.00000233 	 loss = 1.8137(1.0997)
2023/09/15 01:46:40 - INFO - root -   Epoch: [93/500][320/346], lr: 0.00000233 	 loss = 0.4713(1.1030)
2023/09/15 01:46:56 - INFO - root -   Epoch: [93/500][340/346], lr: 0.00000233 	 loss = 1.7249(1.1053)
2023/09/15 01:46:59 - INFO - root -   Epoch: [93/500] 	 loss = 1.1013
2023/09/15 01:46:59 - INFO - root -   train_accuracy = 0.5491
2023/09/15 01:47:07 - INFO - root -   Epoch: [94/500][0/346], lr: 0.00000234 	 loss = 1.8190(1.8190)
2023/09/15 01:47:23 - INFO - root -   Epoch: [94/500][20/346], lr: 0.00000234 	 loss = 1.5723(1.0533)
2023/09/15 01:47:44 - INFO - root -   Epoch: [94/500][40/346], lr: 0.00000234 	 loss = 0.9259(1.0354)
2023/09/15 01:48:01 - INFO - root -   Epoch: [94/500][60/346], lr: 0.00000234 	 loss = 0.5453(1.0730)
2023/09/15 01:48:22 - INFO - root -   Epoch: [94/500][80/346], lr: 0.00000234 	 loss = 1.0722(1.1345)
2023/09/15 01:48:38 - INFO - root -   Epoch: [94/500][100/346], lr: 0.00000234 	 loss = 0.5359(1.1361)
2023/09/15 01:48:59 - INFO - root -   Epoch: [94/500][120/346], lr: 0.00000234 	 loss = 0.9966(1.1515)
2023/09/15 01:49:15 - INFO - root -   Epoch: [94/500][140/346], lr: 0.00000234 	 loss = 2.1793(1.1516)
2023/09/15 01:49:36 - INFO - root -   Epoch: [94/500][160/346], lr: 0.00000234 	 loss = 0.6639(1.1278)
2023/09/15 01:49:53 - INFO - root -   Epoch: [94/500][180/346], lr: 0.00000234 	 loss = 1.1481(1.1232)
2023/09/15 01:50:13 - INFO - root -   Epoch: [94/500][200/346], lr: 0.00000234 	 loss = 0.4146(1.0985)
2023/09/15 01:50:30 - INFO - root -   Epoch: [94/500][220/346], lr: 0.00000234 	 loss = 1.6018(1.1012)
2023/09/15 01:50:51 - INFO - root -   Epoch: [94/500][240/346], lr: 0.00000234 	 loss = 0.5271(1.1027)
2023/09/15 01:51:08 - INFO - root -   Epoch: [94/500][260/346], lr: 0.00000234 	 loss = 0.6496(1.0945)
2023/09/15 01:51:29 - INFO - root -   Epoch: [94/500][280/346], lr: 0.00000234 	 loss = 1.6385(1.0940)
2023/09/15 01:51:45 - INFO - root -   Epoch: [94/500][300/346], lr: 0.00000234 	 loss = 2.0055(1.0986)
2023/09/15 01:52:06 - INFO - root -   Epoch: [94/500][320/346], lr: 0.00000234 	 loss = 0.5412(1.1035)
2023/09/15 01:52:23 - INFO - root -   Epoch: [94/500][340/346], lr: 0.00000234 	 loss = 1.8029(1.1048)
2023/09/15 01:52:25 - INFO - root -   Epoch: [94/500] 	 loss = 1.1033
2023/09/15 01:53:47 - INFO - root -   precision = 0.5345
2023/09/15 01:53:47 - INFO - root -   eval_loss = 1.0944
2023/09/15 01:53:48 - INFO - root -   train_accuracy = 0.5564
2023/09/15 01:53:56 - INFO - root -   Epoch: [95/500][0/346], lr: 0.00000236 	 loss = 1.9183(1.9183)
2023/09/15 01:54:13 - INFO - root -   Epoch: [95/500][20/346], lr: 0.00000236 	 loss = 1.5259(1.1180)
2023/09/15 01:54:34 - INFO - root -   Epoch: [95/500][40/346], lr: 0.00000236 	 loss = 0.9131(1.0901)
2023/09/15 01:54:50 - INFO - root -   Epoch: [95/500][60/346], lr: 0.00000236 	 loss = 0.6213(1.0928)
2023/09/15 01:55:11 - INFO - root -   Epoch: [95/500][80/346], lr: 0.00000236 	 loss = 0.8601(1.1296)
2023/09/15 01:55:27 - INFO - root -   Epoch: [95/500][100/346], lr: 0.00000236 	 loss = 0.5211(1.1261)
2023/09/15 01:55:48 - INFO - root -   Epoch: [95/500][120/346], lr: 0.00000236 	 loss = 1.0730(1.1465)
2023/09/15 01:56:05 - INFO - root -   Epoch: [95/500][140/346], lr: 0.00000236 	 loss = 2.2793(1.1449)
2023/09/15 01:56:25 - INFO - root -   Epoch: [95/500][160/346], lr: 0.00000236 	 loss = 0.6104(1.1184)
2023/09/15 01:56:42 - INFO - root -   Epoch: [95/500][180/346], lr: 0.00000236 	 loss = 0.8416(1.1128)
2023/09/15 01:57:03 - INFO - root -   Epoch: [95/500][200/346], lr: 0.00000236 	 loss = 0.3582(1.0920)
2023/09/15 01:57:20 - INFO - root -   Epoch: [95/500][220/346], lr: 0.00000236 	 loss = 2.0384(1.0980)
2023/09/15 01:57:41 - INFO - root -   Epoch: [95/500][240/346], lr: 0.00000236 	 loss = 0.4601(1.1006)
2023/09/15 01:57:57 - INFO - root -   Epoch: [95/500][260/346], lr: 0.00000236 	 loss = 0.5937(1.0948)
2023/09/15 01:58:18 - INFO - root -   Epoch: [95/500][280/346], lr: 0.00000236 	 loss = 1.6799(1.0934)
2023/09/15 01:58:35 - INFO - root -   Epoch: [95/500][300/346], lr: 0.00000236 	 loss = 2.0792(1.0962)
2023/09/15 01:58:55 - INFO - root -   Epoch: [95/500][320/346], lr: 0.00000236 	 loss = 0.3516(1.1007)
2023/09/15 01:59:12 - INFO - root -   Epoch: [95/500][340/346], lr: 0.00000236 	 loss = 2.0837(1.1035)
2023/09/15 01:59:14 - INFO - root -   Epoch: [95/500] 	 loss = 1.1005
2023/09/15 01:59:14 - INFO - root -   train_accuracy = 0.5564
2023/09/15 01:59:22 - INFO - root -   Epoch: [96/500][0/346], lr: 0.00000237 	 loss = 1.8029(1.8029)
2023/09/15 01:59:39 - INFO - root -   Epoch: [96/500][20/346], lr: 0.00000237 	 loss = 1.4009(1.1131)
2023/09/15 02:00:00 - INFO - root -   Epoch: [96/500][40/346], lr: 0.00000237 	 loss = 0.8565(1.0841)
2023/09/15 02:00:16 - INFO - root -   Epoch: [96/500][60/346], lr: 0.00000237 	 loss = 0.5740(1.0678)
2023/09/15 02:00:37 - INFO - root -   Epoch: [96/500][80/346], lr: 0.00000237 	 loss = 1.0266(1.1269)
2023/09/15 02:00:54 - INFO - root -   Epoch: [96/500][100/346], lr: 0.00000237 	 loss = 0.5842(1.1329)
2023/09/15 02:01:15 - INFO - root -   Epoch: [96/500][120/346], lr: 0.00000237 	 loss = 1.0488(1.1416)
2023/09/15 02:01:31 - INFO - root -   Epoch: [96/500][140/346], lr: 0.00000237 	 loss = 2.3739(1.1411)
2023/09/15 02:01:52 - INFO - root -   Epoch: [96/500][160/346], lr: 0.00000237 	 loss = 0.6487(1.1169)
2023/09/15 02:02:09 - INFO - root -   Epoch: [96/500][180/346], lr: 0.00000237 	 loss = 0.9481(1.1109)
2023/09/15 02:02:30 - INFO - root -   Epoch: [96/500][200/346], lr: 0.00000237 	 loss = 0.3389(1.0923)
2023/09/15 02:02:46 - INFO - root -   Epoch: [96/500][220/346], lr: 0.00000237 	 loss = 1.8862(1.0998)
2023/09/15 02:03:07 - INFO - root -   Epoch: [96/500][240/346], lr: 0.00000237 	 loss = 0.6187(1.1029)
2023/09/15 02:03:24 - INFO - root -   Epoch: [96/500][260/346], lr: 0.00000237 	 loss = 0.8113(1.0957)
2023/09/15 02:03:45 - INFO - root -   Epoch: [96/500][280/346], lr: 0.00000237 	 loss = 1.2723(1.0917)
2023/09/15 02:04:01 - INFO - root -   Epoch: [96/500][300/346], lr: 0.00000237 	 loss = 1.6321(1.0914)
2023/09/15 02:04:22 - INFO - root -   Epoch: [96/500][320/346], lr: 0.00000237 	 loss = 0.3764(1.0988)
2023/09/15 02:04:39 - INFO - root -   Epoch: [96/500][340/346], lr: 0.00000237 	 loss = 2.1964(1.1015)
2023/09/15 02:04:41 - INFO - root -   Epoch: [96/500] 	 loss = 1.0986
2023/09/15 02:04:41 - INFO - root -   train_accuracy = 0.5549
2023/09/15 02:04:49 - INFO - root -   Epoch: [97/500][0/346], lr: 0.00000239 	 loss = 1.5448(1.5448)
2023/09/15 02:05:05 - INFO - root -   Epoch: [97/500][20/346], lr: 0.00000239 	 loss = 1.7430(1.0359)
2023/09/15 02:05:26 - INFO - root -   Epoch: [97/500][40/346], lr: 0.00000239 	 loss = 0.8185(1.0382)
2023/09/15 02:05:43 - INFO - root -   Epoch: [97/500][60/346], lr: 0.00000239 	 loss = 0.5067(1.0489)
2023/09/15 02:06:03 - INFO - root -   Epoch: [97/500][80/346], lr: 0.00000239 	 loss = 1.0929(1.1315)
2023/09/15 02:06:20 - INFO - root -   Epoch: [97/500][100/346], lr: 0.00000239 	 loss = 0.6059(1.1384)
2023/09/15 02:06:41 - INFO - root -   Epoch: [97/500][120/346], lr: 0.00000239 	 loss = 1.3615(1.1648)
2023/09/15 02:06:57 - INFO - root -   Epoch: [97/500][140/346], lr: 0.00000239 	 loss = 2.5828(1.1672)
2023/09/15 02:07:18 - INFO - root -   Epoch: [97/500][160/346], lr: 0.00000239 	 loss = 0.9862(1.1414)
2023/09/15 02:07:35 - INFO - root -   Epoch: [97/500][180/346], lr: 0.00000239 	 loss = 1.0516(1.1319)
2023/09/15 02:07:55 - INFO - root -   Epoch: [97/500][200/346], lr: 0.00000239 	 loss = 0.3069(1.1150)
2023/09/15 02:08:12 - INFO - root -   Epoch: [97/500][220/346], lr: 0.00000239 	 loss = 1.9802(1.1167)
2023/09/15 02:08:33 - INFO - root -   Epoch: [97/500][240/346], lr: 0.00000239 	 loss = 0.4812(1.1164)
2023/09/15 02:08:49 - INFO - root -   Epoch: [97/500][260/346], lr: 0.00000239 	 loss = 0.6689(1.1071)
2023/09/15 02:09:10 - INFO - root -   Epoch: [97/500][280/346], lr: 0.00000239 	 loss = 1.3455(1.1013)
2023/09/15 02:09:27 - INFO - root -   Epoch: [97/500][300/346], lr: 0.00000239 	 loss = 1.3443(1.1005)
2023/09/15 02:09:48 - INFO - root -   Epoch: [97/500][320/346], lr: 0.00000239 	 loss = 0.4294(1.1022)
2023/09/15 02:10:04 - INFO - root -   Epoch: [97/500][340/346], lr: 0.00000239 	 loss = 2.1722(1.1030)
2023/09/15 02:10:06 - INFO - root -   Epoch: [97/500] 	 loss = 1.1009
2023/09/15 02:10:06 - INFO - root -   train_accuracy = 0.5578
2023/09/15 02:10:14 - INFO - root -   Epoch: [98/500][0/346], lr: 0.00000240 	 loss = 1.6323(1.6323)
2023/09/15 02:10:31 - INFO - root -   Epoch: [98/500][20/346], lr: 0.00000240 	 loss = 1.4763(1.0966)
2023/09/15 02:10:51 - INFO - root -   Epoch: [98/500][40/346], lr: 0.00000240 	 loss = 1.1008(1.0738)
2023/09/15 02:11:08 - INFO - root -   Epoch: [98/500][60/346], lr: 0.00000240 	 loss = 0.5166(1.0940)
2023/09/15 02:11:28 - INFO - root -   Epoch: [98/500][80/346], lr: 0.00000240 	 loss = 1.3026(1.1666)
2023/09/15 02:11:45 - INFO - root -   Epoch: [98/500][100/346], lr: 0.00000240 	 loss = 0.4700(1.1627)
2023/09/15 02:12:06 - INFO - root -   Epoch: [98/500][120/346], lr: 0.00000240 	 loss = 1.0044(1.1648)
2023/09/15 02:12:22 - INFO - root -   Epoch: [98/500][140/346], lr: 0.00000240 	 loss = 1.8308(1.1543)
2023/09/15 02:12:43 - INFO - root -   Epoch: [98/500][160/346], lr: 0.00000240 	 loss = 0.7705(1.1283)
2023/09/15 02:12:59 - INFO - root -   Epoch: [98/500][180/346], lr: 0.00000240 	 loss = 1.2467(1.1249)
2023/09/15 02:13:20 - INFO - root -   Epoch: [98/500][200/346], lr: 0.00000240 	 loss = 0.2774(1.1047)
2023/09/15 02:13:36 - INFO - root -   Epoch: [98/500][220/346], lr: 0.00000240 	 loss = 1.9467(1.1090)
2023/09/15 02:13:57 - INFO - root -   Epoch: [98/500][240/346], lr: 0.00000240 	 loss = 0.5159(1.1068)
2023/09/15 02:14:13 - INFO - root -   Epoch: [98/500][260/346], lr: 0.00000240 	 loss = 0.9793(1.0987)
2023/09/15 02:14:33 - INFO - root -   Epoch: [98/500][280/346], lr: 0.00000240 	 loss = 1.3315(1.0934)
2023/09/15 02:14:50 - INFO - root -   Epoch: [98/500][300/346], lr: 0.00000240 	 loss = 1.7176(1.0952)
2023/09/15 02:15:10 - INFO - root -   Epoch: [98/500][320/346], lr: 0.00000240 	 loss = 0.2427(1.0983)
2023/09/15 02:15:26 - INFO - root -   Epoch: [98/500][340/346], lr: 0.00000240 	 loss = 1.9401(1.0997)
2023/09/15 02:15:28 - INFO - root -   Epoch: [98/500] 	 loss = 1.0966
2023/09/15 02:15:28 - INFO - root -   train_accuracy = 0.5520
2023/09/15 02:15:36 - INFO - root -   Epoch: [99/500][0/346], lr: 0.00000242 	 loss = 1.7609(1.7609)
2023/09/15 02:15:52 - INFO - root -   Epoch: [99/500][20/346], lr: 0.00000242 	 loss = 1.8025(1.0892)
2023/09/15 02:16:13 - INFO - root -   Epoch: [99/500][40/346], lr: 0.00000242 	 loss = 0.8896(1.0605)
2023/09/15 02:16:30 - INFO - root -   Epoch: [99/500][60/346], lr: 0.00000242 	 loss = 0.5802(1.0712)
2023/09/15 02:16:50 - INFO - root -   Epoch: [99/500][80/346], lr: 0.00000242 	 loss = 0.8715(1.1304)
2023/09/15 02:17:07 - INFO - root -   Epoch: [99/500][100/346], lr: 0.00000242 	 loss = 0.5698(1.1338)
2023/09/15 02:17:28 - INFO - root -   Epoch: [99/500][120/346], lr: 0.00000242 	 loss = 1.0715(1.1502)
2023/09/15 02:17:44 - INFO - root -   Epoch: [99/500][140/346], lr: 0.00000242 	 loss = 1.9672(1.1475)
2023/09/15 02:18:05 - INFO - root -   Epoch: [99/500][160/346], lr: 0.00000242 	 loss = 0.5115(1.1127)
2023/09/15 02:18:22 - INFO - root -   Epoch: [99/500][180/346], lr: 0.00000242 	 loss = 0.9889(1.1110)
2023/09/15 02:18:43 - INFO - root -   Epoch: [99/500][200/346], lr: 0.00000242 	 loss = 0.2692(1.0909)
2023/09/15 02:18:59 - INFO - root -   Epoch: [99/500][220/346], lr: 0.00000242 	 loss = 1.8726(1.0944)
2023/09/15 02:19:20 - INFO - root -   Epoch: [99/500][240/346], lr: 0.00000242 	 loss = 0.5049(1.0968)
2023/09/15 02:19:36 - INFO - root -   Epoch: [99/500][260/346], lr: 0.00000242 	 loss = 0.6370(1.0881)
2023/09/15 02:19:57 - INFO - root -   Epoch: [99/500][280/346], lr: 0.00000242 	 loss = 1.2442(1.0832)
2023/09/15 02:20:14 - INFO - root -   Epoch: [99/500][300/346], lr: 0.00000242 	 loss = 1.6046(1.0855)
2023/09/15 02:20:34 - INFO - root -   Epoch: [99/500][320/346], lr: 0.00000242 	 loss = 0.5617(1.0905)
2023/09/15 02:20:51 - INFO - root -   Epoch: [99/500][340/346], lr: 0.00000242 	 loss = 1.7001(1.0930)
2023/09/15 02:20:53 - INFO - root -   Epoch: [99/500] 	 loss = 1.0908
2023/09/15 02:22:15 - INFO - root -   precision = 0.5230
2023/09/15 02:22:15 - INFO - root -   eval_loss = 1.0890
2023/09/15 02:22:16 - INFO - root -   train_accuracy = 0.5708
2023/09/15 02:22:24 - INFO - root -   Epoch: [100/500][0/346], lr: 0.00000243 	 loss = 1.7097(1.7097)
2023/09/15 02:22:41 - INFO - root -   Epoch: [100/500][20/346], lr: 0.00000243 	 loss = 1.6268(1.0776)
2023/09/15 02:23:01 - INFO - root -   Epoch: [100/500][40/346], lr: 0.00000243 	 loss = 0.8617(1.0683)
2023/09/15 02:23:18 - INFO - root -   Epoch: [100/500][60/346], lr: 0.00000243 	 loss = 0.6601(1.0847)
2023/09/15 02:23:39 - INFO - root -   Epoch: [100/500][80/346], lr: 0.00000243 	 loss = 1.1013(1.1377)
2023/09/15 02:23:55 - INFO - root -   Epoch: [100/500][100/346], lr: 0.00000243 	 loss = 0.5886(1.1399)
2023/09/15 02:24:16 - INFO - root -   Epoch: [100/500][120/346], lr: 0.00000243 	 loss = 1.3219(1.1513)
2023/09/15 02:24:32 - INFO - root -   Epoch: [100/500][140/346], lr: 0.00000243 	 loss = 2.0039(1.1530)
2023/09/15 02:24:53 - INFO - root -   Epoch: [100/500][160/346], lr: 0.00000243 	 loss = 0.6775(1.1278)
2023/09/15 02:25:10 - INFO - root -   Epoch: [100/500][180/346], lr: 0.00000243 	 loss = 1.0054(1.1182)
2023/09/15 02:25:31 - INFO - root -   Epoch: [100/500][200/346], lr: 0.00000243 	 loss = 0.2154(1.0944)
2023/09/15 02:25:47 - INFO - root -   Epoch: [100/500][220/346], lr: 0.00000243 	 loss = 2.0721(1.0980)
2023/09/15 02:26:08 - INFO - root -   Epoch: [100/500][240/346], lr: 0.00000243 	 loss = 0.5359(1.0970)
2023/09/15 02:26:24 - INFO - root -   Epoch: [100/500][260/346], lr: 0.00000243 	 loss = 0.8606(1.0895)
2023/09/15 02:26:45 - INFO - root -   Epoch: [100/500][280/346], lr: 0.00000243 	 loss = 1.1788(1.0803)
2023/09/15 02:27:02 - INFO - root -   Epoch: [100/500][300/346], lr: 0.00000243 	 loss = 1.3434(1.0823)
2023/09/15 02:27:22 - INFO - root -   Epoch: [100/500][320/346], lr: 0.00000243 	 loss = 0.3653(1.0806)
2023/09/15 02:27:39 - INFO - root -   Epoch: [100/500][340/346], lr: 0.00000243 	 loss = 2.0346(1.0859)
2023/09/15 02:27:41 - INFO - root -   Epoch: [100/500] 	 loss = 1.0838
2023/09/15 02:27:41 - INFO - root -   train_accuracy = 0.5564
2023/09/15 02:27:49 - INFO - root -   Epoch: [101/500][0/346], lr: 0.00000244 	 loss = 1.8307(1.8307)
2023/09/15 02:28:06 - INFO - root -   Epoch: [101/500][20/346], lr: 0.00000244 	 loss = 1.5001(1.1090)
2023/09/15 02:28:26 - INFO - root -   Epoch: [101/500][40/346], lr: 0.00000244 	 loss = 0.9053(1.0558)
2023/09/15 02:28:43 - INFO - root -   Epoch: [101/500][60/346], lr: 0.00000244 	 loss = 0.7618(1.0720)
2023/09/15 02:29:04 - INFO - root -   Epoch: [101/500][80/346], lr: 0.00000244 	 loss = 1.1425(1.1229)
2023/09/15 02:29:21 - INFO - root -   Epoch: [101/500][100/346], lr: 0.00000244 	 loss = 0.7337(1.1113)
2023/09/15 02:29:41 - INFO - root -   Epoch: [101/500][120/346], lr: 0.00000244 	 loss = 1.2309(1.1259)
2023/09/15 02:29:58 - INFO - root -   Epoch: [101/500][140/346], lr: 0.00000244 	 loss = 2.0523(1.1226)
2023/09/15 02:30:18 - INFO - root -   Epoch: [101/500][160/346], lr: 0.00000244 	 loss = 0.7199(1.1008)
2023/09/15 02:30:35 - INFO - root -   Epoch: [101/500][180/346], lr: 0.00000244 	 loss = 0.9921(1.0937)
2023/09/15 02:30:56 - INFO - root -   Epoch: [101/500][200/346], lr: 0.00000244 	 loss = 0.2711(1.0687)
2023/09/15 02:31:13 - INFO - root -   Epoch: [101/500][220/346], lr: 0.00000244 	 loss = 1.5526(1.0751)
2023/09/15 02:31:33 - INFO - root -   Epoch: [101/500][240/346], lr: 0.00000244 	 loss = 0.5414(1.0771)
2023/09/15 02:31:50 - INFO - root -   Epoch: [101/500][260/346], lr: 0.00000244 	 loss = 0.8497(1.0708)
2023/09/15 02:32:11 - INFO - root -   Epoch: [101/500][280/346], lr: 0.00000244 	 loss = 1.1900(1.0672)
2023/09/15 02:32:27 - INFO - root -   Epoch: [101/500][300/346], lr: 0.00000244 	 loss = 1.5201(1.0681)
2023/09/15 02:32:48 - INFO - root -   Epoch: [101/500][320/346], lr: 0.00000244 	 loss = 0.3600(1.0686)
2023/09/15 02:33:04 - INFO - root -   Epoch: [101/500][340/346], lr: 0.00000244 	 loss = 1.6439(1.0683)
2023/09/15 02:33:06 - INFO - root -   Epoch: [101/500] 	 loss = 1.0667
2023/09/15 02:33:06 - INFO - root -   train_accuracy = 0.5592
2023/09/15 02:33:14 - INFO - root -   Epoch: [102/500][0/346], lr: 0.00000246 	 loss = 1.7769(1.7769)
2023/09/15 02:33:31 - INFO - root -   Epoch: [102/500][20/346], lr: 0.00000246 	 loss = 1.5867(1.0607)
2023/09/15 02:33:51 - INFO - root -   Epoch: [102/500][40/346], lr: 0.00000246 	 loss = 1.0704(1.0222)
2023/09/15 02:34:08 - INFO - root -   Epoch: [102/500][60/346], lr: 0.00000246 	 loss = 0.5480(1.0402)
2023/09/15 02:34:29 - INFO - root -   Epoch: [102/500][80/346], lr: 0.00000246 	 loss = 1.0732(1.1105)
2023/09/15 02:34:46 - INFO - root -   Epoch: [102/500][100/346], lr: 0.00000246 	 loss = 0.4922(1.1178)
2023/09/15 02:35:06 - INFO - root -   Epoch: [102/500][120/346], lr: 0.00000246 	 loss = 1.0344(1.1271)
2023/09/15 02:35:23 - INFO - root -   Epoch: [102/500][140/346], lr: 0.00000246 	 loss = 2.2336(1.1332)
2023/09/15 02:35:44 - INFO - root -   Epoch: [102/500][160/346], lr: 0.00000246 	 loss = 0.8378(1.1034)
2023/09/15 02:36:00 - INFO - root -   Epoch: [102/500][180/346], lr: 0.00000246 	 loss = 0.8622(1.0975)
2023/09/15 02:36:21 - INFO - root -   Epoch: [102/500][200/346], lr: 0.00000246 	 loss = 0.2052(1.0744)
2023/09/15 02:36:38 - INFO - root -   Epoch: [102/500][220/346], lr: 0.00000246 	 loss = 2.1062(1.0799)
2023/09/15 02:36:59 - INFO - root -   Epoch: [102/500][240/346], lr: 0.00000246 	 loss = 0.5449(1.0817)
2023/09/15 02:37:15 - INFO - root -   Epoch: [102/500][260/346], lr: 0.00000246 	 loss = 1.0199(1.0757)
2023/09/15 02:37:36 - INFO - root -   Epoch: [102/500][280/346], lr: 0.00000246 	 loss = 1.2882(1.0690)
2023/09/15 02:37:53 - INFO - root -   Epoch: [102/500][300/346], lr: 0.00000246 	 loss = 1.5735(1.0693)
2023/09/15 02:38:14 - INFO - root -   Epoch: [102/500][320/346], lr: 0.00000246 	 loss = 0.3366(1.0735)
2023/09/15 02:38:30 - INFO - root -   Epoch: [102/500][340/346], lr: 0.00000246 	 loss = 2.0307(1.0755)
2023/09/15 02:38:32 - INFO - root -   Epoch: [102/500] 	 loss = 1.0718
2023/09/15 02:38:32 - INFO - root -   train_accuracy = 0.5506
2023/09/15 02:38:41 - INFO - root -   Epoch: [103/500][0/346], lr: 0.00000247 	 loss = 1.9555(1.9555)
2023/09/15 02:38:57 - INFO - root -   Epoch: [103/500][20/346], lr: 0.00000247 	 loss = 1.1804(0.9907)
2023/09/15 02:39:18 - INFO - root -   Epoch: [103/500][40/346], lr: 0.00000247 	 loss = 1.0044(1.0228)
2023/09/15 02:39:34 - INFO - root -   Epoch: [103/500][60/346], lr: 0.00000247 	 loss = 0.6546(1.0433)
2023/09/15 02:39:55 - INFO - root -   Epoch: [103/500][80/346], lr: 0.00000247 	 loss = 1.1722(1.1054)
2023/09/15 02:40:12 - INFO - root -   Epoch: [103/500][100/346], lr: 0.00000247 	 loss = 0.4353(1.1038)
2023/09/15 02:40:32 - INFO - root -   Epoch: [103/500][120/346], lr: 0.00000247 	 loss = 1.1166(1.1280)
2023/09/15 02:40:49 - INFO - root -   Epoch: [103/500][140/346], lr: 0.00000247 	 loss = 2.0779(1.1245)
2023/09/15 02:41:10 - INFO - root -   Epoch: [103/500][160/346], lr: 0.00000247 	 loss = 0.8554(1.0986)
2023/09/15 02:41:26 - INFO - root -   Epoch: [103/500][180/346], lr: 0.00000247 	 loss = 1.0192(1.0936)
2023/09/15 02:41:47 - INFO - root -   Epoch: [103/500][200/346], lr: 0.00000247 	 loss = 0.2767(1.0690)
2023/09/15 02:42:03 - INFO - root -   Epoch: [103/500][220/346], lr: 0.00000247 	 loss = 1.7085(1.0705)
2023/09/15 02:42:24 - INFO - root -   Epoch: [103/500][240/346], lr: 0.00000247 	 loss = 0.4804(1.0694)
2023/09/15 02:42:41 - INFO - root -   Epoch: [103/500][260/346], lr: 0.00000247 	 loss = 0.8277(1.0636)
2023/09/15 02:43:02 - INFO - root -   Epoch: [103/500][280/346], lr: 0.00000247 	 loss = 1.6750(1.0603)
2023/09/15 02:43:18 - INFO - root -   Epoch: [103/500][300/346], lr: 0.00000247 	 loss = 1.6740(1.0650)
2023/09/15 02:43:39 - INFO - root -   Epoch: [103/500][320/346], lr: 0.00000247 	 loss = 0.3554(1.0697)
2023/09/15 02:43:56 - INFO - root -   Epoch: [103/500][340/346], lr: 0.00000247 	 loss = 1.7502(1.0726)
2023/09/15 02:43:58 - INFO - root -   Epoch: [103/500] 	 loss = 1.0702
2023/09/15 02:43:58 - INFO - root -   train_accuracy = 0.5824
2023/09/15 02:44:06 - INFO - root -   Epoch: [104/500][0/346], lr: 0.00000249 	 loss = 1.8048(1.8048)
2023/09/15 02:44:22 - INFO - root -   Epoch: [104/500][20/346], lr: 0.00000249 	 loss = 1.5424(1.0618)
2023/09/15 02:44:43 - INFO - root -   Epoch: [104/500][40/346], lr: 0.00000249 	 loss = 0.7518(1.0186)
2023/09/15 02:45:00 - INFO - root -   Epoch: [104/500][60/346], lr: 0.00000249 	 loss = 0.5374(1.0339)
2023/09/15 02:45:21 - INFO - root -   Epoch: [104/500][80/346], lr: 0.00000249 	 loss = 1.1026(1.0943)
2023/09/15 02:45:38 - INFO - root -   Epoch: [104/500][100/346], lr: 0.00000249 	 loss = 0.5915(1.0937)
2023/09/15 02:45:58 - INFO - root -   Epoch: [104/500][120/346], lr: 0.00000249 	 loss = 1.4840(1.1131)
2023/09/15 02:46:15 - INFO - root -   Epoch: [104/500][140/346], lr: 0.00000249 	 loss = 2.3547(1.1119)
2023/09/15 02:46:36 - INFO - root -   Epoch: [104/500][160/346], lr: 0.00000249 	 loss = 0.7518(1.0879)
2023/09/15 02:46:53 - INFO - root -   Epoch: [104/500][180/346], lr: 0.00000249 	 loss = 1.0146(1.0822)
2023/09/15 02:47:13 - INFO - root -   Epoch: [104/500][200/346], lr: 0.00000249 	 loss = 0.2892(1.0627)
2023/09/15 02:47:30 - INFO - root -   Epoch: [104/500][220/346], lr: 0.00000249 	 loss = 1.8637(1.0654)
2023/09/15 02:47:51 - INFO - root -   Epoch: [104/500][240/346], lr: 0.00000249 	 loss = 0.5270(1.0685)
2023/09/15 02:48:08 - INFO - root -   Epoch: [104/500][260/346], lr: 0.00000249 	 loss = 0.7830(1.0607)
2023/09/15 02:48:28 - INFO - root -   Epoch: [104/500][280/346], lr: 0.00000249 	 loss = 1.2407(1.0537)
2023/09/15 02:48:45 - INFO - root -   Epoch: [104/500][300/346], lr: 0.00000249 	 loss = 1.6198(1.0566)
2023/09/15 02:49:06 - INFO - root -   Epoch: [104/500][320/346], lr: 0.00000249 	 loss = 0.3289(1.0628)
2023/09/15 02:49:22 - INFO - root -   Epoch: [104/500][340/346], lr: 0.00000249 	 loss = 2.0316(1.0652)
2023/09/15 02:49:24 - INFO - root -   Epoch: [104/500] 	 loss = 1.0614
2023/09/15 02:50:47 - INFO - root -   precision = 0.5345
2023/09/15 02:50:47 - INFO - root -   eval_loss = 1.0819
2023/09/15 02:50:48 - INFO - root -   train_accuracy = 0.5766
2023/09/15 02:50:56 - INFO - root -   Epoch: [105/500][0/346], lr: 0.00000250 	 loss = 1.9842(1.9842)
2023/09/15 02:51:12 - INFO - root -   Epoch: [105/500][20/346], lr: 0.00000250 	 loss = 1.3168(1.0295)
2023/09/15 02:51:33 - INFO - root -   Epoch: [105/500][40/346], lr: 0.00000250 	 loss = 1.0349(0.9936)
2023/09/15 02:51:50 - INFO - root -   Epoch: [105/500][60/346], lr: 0.00000250 	 loss = 0.7563(1.0200)
2023/09/15 02:52:09 - INFO - root -   Epoch: [105/500][80/346], lr: 0.00000250 	 loss = 1.1250(1.0692)
2023/09/15 02:52:28 - INFO - root -   Epoch: [105/500][100/346], lr: 0.00000250 	 loss = 0.5146(1.0860)
2023/09/15 02:52:46 - INFO - root -   Epoch: [105/500][120/346], lr: 0.00000250 	 loss = 1.0239(1.1084)
2023/09/15 02:53:05 - INFO - root -   Epoch: [105/500][140/346], lr: 0.00000250 	 loss = 2.2252(1.1115)
2023/09/15 02:53:23 - INFO - root -   Epoch: [105/500][160/346], lr: 0.00000250 	 loss = 0.7000(1.0808)
2023/09/15 02:53:42 - INFO - root -   Epoch: [105/500][180/346], lr: 0.00000250 	 loss = 0.9088(1.0764)
2023/09/15 02:54:00 - INFO - root -   Epoch: [105/500][200/346], lr: 0.00000250 	 loss = 0.2497(1.0584)
2023/09/15 02:54:18 - INFO - root -   Epoch: [105/500][220/346], lr: 0.00000250 	 loss = 2.2136(1.0627)
2023/09/15 02:54:37 - INFO - root -   Epoch: [105/500][240/346], lr: 0.00000250 	 loss = 0.5843(1.0622)
2023/09/15 02:54:54 - INFO - root -   Epoch: [105/500][260/346], lr: 0.00000250 	 loss = 0.7476(1.0563)
2023/09/15 02:55:14 - INFO - root -   Epoch: [105/500][280/346], lr: 0.00000250 	 loss = 1.0921(1.0477)
2023/09/15 02:55:31 - INFO - root -   Epoch: [105/500][300/346], lr: 0.00000250 	 loss = 1.6062(1.0510)
2023/09/15 02:55:51 - INFO - root -   Epoch: [105/500][320/346], lr: 0.00000250 	 loss = 0.3247(1.0504)
2023/09/15 02:56:07 - INFO - root -   Epoch: [105/500][340/346], lr: 0.00000250 	 loss = 2.0307(1.0578)
2023/09/15 02:56:09 - INFO - root -   Epoch: [105/500] 	 loss = 1.0550
2023/09/15 02:56:09 - INFO - root -   train_accuracy = 0.5809
2023/09/15 02:56:17 - INFO - root -   Epoch: [106/500][0/346], lr: 0.00000252 	 loss = 1.5117(1.5117)
2023/09/15 02:56:34 - INFO - root -   Epoch: [106/500][20/346], lr: 0.00000252 	 loss = 1.4395(0.9775)
2023/09/15 02:56:55 - INFO - root -   Epoch: [106/500][40/346], lr: 0.00000252 	 loss = 0.8477(1.0070)
2023/09/15 02:57:11 - INFO - root -   Epoch: [106/500][60/346], lr: 0.00000252 	 loss = 0.5543(1.0276)
2023/09/15 02:57:32 - INFO - root -   Epoch: [106/500][80/346], lr: 0.00000252 	 loss = 1.1716(1.1069)
2023/09/15 02:57:48 - INFO - root -   Epoch: [106/500][100/346], lr: 0.00000252 	 loss = 0.4824(1.1014)
2023/09/15 02:58:09 - INFO - root -   Epoch: [106/500][120/346], lr: 0.00000252 	 loss = 1.2446(1.1213)
2023/09/15 02:58:26 - INFO - root -   Epoch: [106/500][140/346], lr: 0.00000252 	 loss = 2.2458(1.1159)
2023/09/15 02:58:46 - INFO - root -   Epoch: [106/500][160/346], lr: 0.00000252 	 loss = 0.8139(1.0818)
2023/09/15 02:59:03 - INFO - root -   Epoch: [106/500][180/346], lr: 0.00000252 	 loss = 1.0128(1.0693)
2023/09/15 02:59:24 - INFO - root -   Epoch: [106/500][200/346], lr: 0.00000252 	 loss = 0.1428(1.0445)
2023/09/15 02:59:40 - INFO - root -   Epoch: [106/500][220/346], lr: 0.00000252 	 loss = 2.0663(1.0478)
2023/09/15 03:00:01 - INFO - root -   Epoch: [106/500][240/346], lr: 0.00000252 	 loss = 0.7246(1.0481)
2023/09/15 03:00:17 - INFO - root -   Epoch: [106/500][260/346], lr: 0.00000252 	 loss = 0.8990(1.0413)
2023/09/15 03:00:38 - INFO - root -   Epoch: [106/500][280/346], lr: 0.00000252 	 loss = 1.1388(1.0340)
2023/09/15 03:00:54 - INFO - root -   Epoch: [106/500][300/346], lr: 0.00000252 	 loss = 1.4485(1.0361)
2023/09/15 03:01:15 - INFO - root -   Epoch: [106/500][320/346], lr: 0.00000252 	 loss = 0.3103(1.0386)
2023/09/15 03:01:31 - INFO - root -   Epoch: [106/500][340/346], lr: 0.00000252 	 loss = 1.6279(1.0403)
2023/09/15 03:01:33 - INFO - root -   Epoch: [106/500] 	 loss = 1.0371
2023/09/15 03:01:33 - INFO - root -   train_accuracy = 0.5867
2023/09/15 03:01:42 - INFO - root -   Epoch: [107/500][0/346], lr: 0.00000253 	 loss = 1.7955(1.7955)
2023/09/15 03:01:59 - INFO - root -   Epoch: [107/500][20/346], lr: 0.00000253 	 loss = 1.4997(1.0650)
2023/09/15 03:02:20 - INFO - root -   Epoch: [107/500][40/346], lr: 0.00000253 	 loss = 0.8639(1.0503)
2023/09/15 03:02:36 - INFO - root -   Epoch: [107/500][60/346], lr: 0.00000253 	 loss = 0.9285(1.0533)
2023/09/15 03:02:57 - INFO - root -   Epoch: [107/500][80/346], lr: 0.00000253 	 loss = 1.0739(1.1137)
2023/09/15 03:03:14 - INFO - root -   Epoch: [107/500][100/346], lr: 0.00000253 	 loss = 0.3731(1.1089)
2023/09/15 03:03:34 - INFO - root -   Epoch: [107/500][120/346], lr: 0.00000253 	 loss = 1.0483(1.1236)
2023/09/15 03:03:51 - INFO - root -   Epoch: [107/500][140/346], lr: 0.00000253 	 loss = 2.0102(1.1168)
2023/09/15 03:04:12 - INFO - root -   Epoch: [107/500][160/346], lr: 0.00000253 	 loss = 0.8332(1.0857)
2023/09/15 03:04:29 - INFO - root -   Epoch: [107/500][180/346], lr: 0.00000253 	 loss = 1.1368(1.0805)
2023/09/15 03:04:50 - INFO - root -   Epoch: [107/500][200/346], lr: 0.00000253 	 loss = 0.2220(1.0527)
2023/09/15 03:05:06 - INFO - root -   Epoch: [107/500][220/346], lr: 0.00000253 	 loss = 2.0047(1.0583)
2023/09/15 03:05:27 - INFO - root -   Epoch: [107/500][240/346], lr: 0.00000253 	 loss = 0.5351(1.0563)
2023/09/15 03:05:44 - INFO - root -   Epoch: [107/500][260/346], lr: 0.00000253 	 loss = 0.8442(1.0493)
2023/09/15 03:06:05 - INFO - root -   Epoch: [107/500][280/346], lr: 0.00000253 	 loss = 1.3356(1.0425)
2023/09/15 03:06:21 - INFO - root -   Epoch: [107/500][300/346], lr: 0.00000253 	 loss = 1.4456(1.0479)
2023/09/15 03:06:42 - INFO - root -   Epoch: [107/500][320/346], lr: 0.00000253 	 loss = 0.3045(1.0483)
2023/09/15 03:06:59 - INFO - root -   Epoch: [107/500][340/346], lr: 0.00000253 	 loss = 2.2091(1.0540)
2023/09/15 03:07:01 - INFO - root -   Epoch: [107/500] 	 loss = 1.0513
2023/09/15 03:07:01 - INFO - root -   train_accuracy = 0.5882
2023/09/15 03:07:09 - INFO - root -   Epoch: [108/500][0/346], lr: 0.00000255 	 loss = 2.0581(2.0581)
2023/09/15 03:07:25 - INFO - root -   Epoch: [108/500][20/346], lr: 0.00000255 	 loss = 1.4166(0.9751)
2023/09/15 03:07:46 - INFO - root -   Epoch: [108/500][40/346], lr: 0.00000255 	 loss = 0.9565(0.9955)
2023/09/15 03:08:03 - INFO - root -   Epoch: [108/500][60/346], lr: 0.00000255 	 loss = 0.9246(1.0185)
2023/09/15 03:08:24 - INFO - root -   Epoch: [108/500][80/346], lr: 0.00000255 	 loss = 0.9642(1.0871)
2023/09/15 03:08:40 - INFO - root -   Epoch: [108/500][100/346], lr: 0.00000255 	 loss = 0.6248(1.0924)
2023/09/15 03:09:01 - INFO - root -   Epoch: [108/500][120/346], lr: 0.00000255 	 loss = 1.0351(1.1011)
2023/09/15 03:09:17 - INFO - root -   Epoch: [108/500][140/346], lr: 0.00000255 	 loss = 2.1589(1.1040)
2023/09/15 03:09:38 - INFO - root -   Epoch: [108/500][160/346], lr: 0.00000255 	 loss = 0.9553(1.0811)
2023/09/15 03:09:55 - INFO - root -   Epoch: [108/500][180/346], lr: 0.00000255 	 loss = 0.8902(1.0703)
2023/09/15 03:10:16 - INFO - root -   Epoch: [108/500][200/346], lr: 0.00000255 	 loss = 0.1974(1.0475)
2023/09/15 03:10:32 - INFO - root -   Epoch: [108/500][220/346], lr: 0.00000255 	 loss = 2.0416(1.0448)
2023/09/15 03:10:53 - INFO - root -   Epoch: [108/500][240/346], lr: 0.00000255 	 loss = 0.4518(1.0452)
2023/09/15 03:11:10 - INFO - root -   Epoch: [108/500][260/346], lr: 0.00000255 	 loss = 0.7645(1.0375)
2023/09/15 03:11:30 - INFO - root -   Epoch: [108/500][280/346], lr: 0.00000255 	 loss = 1.1515(1.0291)
2023/09/15 03:11:47 - INFO - root -   Epoch: [108/500][300/346], lr: 0.00000255 	 loss = 1.1589(1.0340)
2023/09/15 03:12:07 - INFO - root -   Epoch: [108/500][320/346], lr: 0.00000255 	 loss = 0.2501(1.0317)
2023/09/15 03:12:24 - INFO - root -   Epoch: [108/500][340/346], lr: 0.00000255 	 loss = 1.7935(1.0349)
2023/09/15 03:12:26 - INFO - root -   Epoch: [108/500] 	 loss = 1.0330
2023/09/15 03:12:26 - INFO - root -   train_accuracy = 0.5968
2023/09/15 03:12:34 - INFO - root -   Epoch: [109/500][0/346], lr: 0.00000256 	 loss = 2.0043(2.0043)
2023/09/15 03:12:50 - INFO - root -   Epoch: [109/500][20/346], lr: 0.00000256 	 loss = 1.1855(0.9869)
2023/09/15 03:13:11 - INFO - root -   Epoch: [109/500][40/346], lr: 0.00000256 	 loss = 1.0688(0.9836)
2023/09/15 03:13:27 - INFO - root -   Epoch: [109/500][60/346], lr: 0.00000256 	 loss = 0.6028(1.0077)
2023/09/15 03:13:48 - INFO - root -   Epoch: [109/500][80/346], lr: 0.00000256 	 loss = 1.0611(1.0738)
2023/09/15 03:14:04 - INFO - root -   Epoch: [109/500][100/346], lr: 0.00000256 	 loss = 0.5685(1.0880)
2023/09/15 03:14:25 - INFO - root -   Epoch: [109/500][120/346], lr: 0.00000256 	 loss = 1.1733(1.1107)
2023/09/15 03:14:41 - INFO - root -   Epoch: [109/500][140/346], lr: 0.00000256 	 loss = 2.2924(1.1081)
2023/09/15 03:15:01 - INFO - root -   Epoch: [109/500][160/346], lr: 0.00000256 	 loss = 0.6613(1.0822)
2023/09/15 03:15:18 - INFO - root -   Epoch: [109/500][180/346], lr: 0.00000256 	 loss = 1.0697(1.0743)
2023/09/15 03:15:38 - INFO - root -   Epoch: [109/500][200/346], lr: 0.00000256 	 loss = 0.1277(1.0497)
2023/09/15 03:15:55 - INFO - root -   Epoch: [109/500][220/346], lr: 0.00000256 	 loss = 1.6177(1.0505)
2023/09/15 03:16:14 - INFO - root -   Epoch: [109/500][240/346], lr: 0.00000256 	 loss = 0.5190(1.0469)
2023/09/15 03:16:31 - INFO - root -   Epoch: [109/500][260/346], lr: 0.00000256 	 loss = 0.9224(1.0431)
2023/09/15 03:16:50 - INFO - root -   Epoch: [109/500][280/346], lr: 0.00000256 	 loss = 1.2443(1.0346)
2023/09/15 03:17:08 - INFO - root -   Epoch: [109/500][300/346], lr: 0.00000256 	 loss = 1.5589(1.0415)
2023/09/15 03:17:27 - INFO - root -   Epoch: [109/500][320/346], lr: 0.00000256 	 loss = 0.3167(1.0448)
2023/09/15 03:17:45 - INFO - root -   Epoch: [109/500][340/346], lr: 0.00000256 	 loss = 2.0606(1.0456)
2023/09/15 03:17:47 - INFO - root -   Epoch: [109/500] 	 loss = 1.0417
2023/09/15 03:19:09 - INFO - root -   precision = 0.5575
2023/09/15 03:19:09 - INFO - root -   eval_loss = 1.0832
2023/09/15 03:19:10 - INFO - root -   train_accuracy = 0.5954
2023/09/15 03:19:19 - INFO - root -   Epoch: [110/500][0/346], lr: 0.00000257 	 loss = 2.1059(2.1059)
2023/09/15 03:19:35 - INFO - root -   Epoch: [110/500][20/346], lr: 0.00000257 	 loss = 1.2809(0.9787)
2023/09/15 03:19:56 - INFO - root -   Epoch: [110/500][40/346], lr: 0.00000257 	 loss = 1.1224(0.9944)
2023/09/15 03:20:13 - INFO - root -   Epoch: [110/500][60/346], lr: 0.00000257 	 loss = 0.6763(1.0256)
2023/09/15 03:20:34 - INFO - root -   Epoch: [110/500][80/346], lr: 0.00000257 	 loss = 1.1683(1.0733)
2023/09/15 03:20:50 - INFO - root -   Epoch: [110/500][100/346], lr: 0.00000257 	 loss = 0.6629(1.0786)
2023/09/15 03:21:11 - INFO - root -   Epoch: [110/500][120/346], lr: 0.00000257 	 loss = 1.1212(1.0992)
2023/09/15 03:21:28 - INFO - root -   Epoch: [110/500][140/346], lr: 0.00000257 	 loss = 2.2473(1.1054)
2023/09/15 03:21:49 - INFO - root -   Epoch: [110/500][160/346], lr: 0.00000257 	 loss = 0.6301(1.0786)
2023/09/15 03:22:05 - INFO - root -   Epoch: [110/500][180/346], lr: 0.00000257 	 loss = 1.0833(1.0677)
2023/09/15 03:22:26 - INFO - root -   Epoch: [110/500][200/346], lr: 0.00000257 	 loss = 0.1629(1.0422)
2023/09/15 03:22:43 - INFO - root -   Epoch: [110/500][220/346], lr: 0.00000257 	 loss = 1.9485(1.0400)
2023/09/15 03:23:04 - INFO - root -   Epoch: [110/500][240/346], lr: 0.00000257 	 loss = 0.5105(1.0424)
2023/09/15 03:23:20 - INFO - root -   Epoch: [110/500][260/346], lr: 0.00000257 	 loss = 0.8969(1.0389)
2023/09/15 03:23:41 - INFO - root -   Epoch: [110/500][280/346], lr: 0.00000257 	 loss = 1.2177(1.0350)
2023/09/15 03:23:57 - INFO - root -   Epoch: [110/500][300/346], lr: 0.00000257 	 loss = 1.9750(1.0422)
2023/09/15 03:24:18 - INFO - root -   Epoch: [110/500][320/346], lr: 0.00000257 	 loss = 0.3129(1.0432)
2023/09/15 03:24:34 - INFO - root -   Epoch: [110/500][340/346], lr: 0.00000257 	 loss = 2.0235(1.0439)
2023/09/15 03:24:36 - INFO - root -   Epoch: [110/500] 	 loss = 1.0399
2023/09/15 03:24:36 - INFO - root -   train_accuracy = 0.5954
2023/09/15 03:24:44 - INFO - root -   Epoch: [111/500][0/346], lr: 0.00000259 	 loss = 1.7659(1.7659)
2023/09/15 03:25:01 - INFO - root -   Epoch: [111/500][20/346], lr: 0.00000259 	 loss = 1.3928(0.9880)
2023/09/15 03:25:22 - INFO - root -   Epoch: [111/500][40/346], lr: 0.00000259 	 loss = 0.7334(0.9759)
2023/09/15 03:25:38 - INFO - root -   Epoch: [111/500][60/346], lr: 0.00000259 	 loss = 0.7219(1.0169)
2023/09/15 03:25:59 - INFO - root -   Epoch: [111/500][80/346], lr: 0.00000259 	 loss = 1.2501(1.0842)
2023/09/15 03:26:16 - INFO - root -   Epoch: [111/500][100/346], lr: 0.00000259 	 loss = 0.4178(1.0771)
2023/09/15 03:26:37 - INFO - root -   Epoch: [111/500][120/346], lr: 0.00000259 	 loss = 1.1346(1.0920)
2023/09/15 03:26:53 - INFO - root -   Epoch: [111/500][140/346], lr: 0.00000259 	 loss = 2.0698(1.0953)
2023/09/15 03:27:15 - INFO - root -   Epoch: [111/500][160/346], lr: 0.00000259 	 loss = 0.9135(1.0660)
2023/09/15 03:27:31 - INFO - root -   Epoch: [111/500][180/346], lr: 0.00000259 	 loss = 0.9300(1.0512)
2023/09/15 03:27:52 - INFO - root -   Epoch: [111/500][200/346], lr: 0.00000259 	 loss = 0.1078(1.0256)
2023/09/15 03:28:09 - INFO - root -   Epoch: [111/500][220/346], lr: 0.00000259 	 loss = 1.8962(1.0238)
2023/09/15 03:28:30 - INFO - root -   Epoch: [111/500][240/346], lr: 0.00000259 	 loss = 0.3990(1.0247)
2023/09/15 03:28:47 - INFO - root -   Epoch: [111/500][260/346], lr: 0.00000259 	 loss = 0.9275(1.0175)
2023/09/15 03:29:08 - INFO - root -   Epoch: [111/500][280/346], lr: 0.00000259 	 loss = 1.0684(1.0117)
2023/09/15 03:29:24 - INFO - root -   Epoch: [111/500][300/346], lr: 0.00000259 	 loss = 1.2655(1.0181)
2023/09/15 03:29:45 - INFO - root -   Epoch: [111/500][320/346], lr: 0.00000259 	 loss = 0.1132(1.0176)
2023/09/15 03:30:01 - INFO - root -   Epoch: [111/500][340/346], lr: 0.00000259 	 loss = 2.0555(1.0197)
2023/09/15 03:30:03 - INFO - root -   Epoch: [111/500] 	 loss = 1.0162
2023/09/15 03:30:03 - INFO - root -   train_accuracy = 0.6098
2023/09/15 03:30:12 - INFO - root -   Epoch: [112/500][0/346], lr: 0.00000260 	 loss = 1.8494(1.8494)
2023/09/15 03:30:28 - INFO - root -   Epoch: [112/500][20/346], lr: 0.00000260 	 loss = 1.3078(1.0202)
2023/09/15 03:30:49 - INFO - root -   Epoch: [112/500][40/346], lr: 0.00000260 	 loss = 0.8080(0.9858)
2023/09/15 03:31:06 - INFO - root -   Epoch: [112/500][60/346], lr: 0.00000260 	 loss = 0.7125(1.0162)
2023/09/15 03:31:26 - INFO - root -   Epoch: [112/500][80/346], lr: 0.00000260 	 loss = 1.0956(1.0761)
2023/09/15 03:31:43 - INFO - root -   Epoch: [112/500][100/346], lr: 0.00000260 	 loss = 0.5150(1.0813)
2023/09/15 03:32:04 - INFO - root -   Epoch: [112/500][120/346], lr: 0.00000260 	 loss = 1.1803(1.1025)
2023/09/15 03:32:20 - INFO - root -   Epoch: [112/500][140/346], lr: 0.00000260 	 loss = 2.1836(1.0968)
2023/09/15 03:32:41 - INFO - root -   Epoch: [112/500][160/346], lr: 0.00000260 	 loss = 0.9777(1.0723)
2023/09/15 03:32:58 - INFO - root -   Epoch: [112/500][180/346], lr: 0.00000260 	 loss = 1.2284(1.0619)
2023/09/15 03:33:19 - INFO - root -   Epoch: [112/500][200/346], lr: 0.00000260 	 loss = 0.2452(1.0412)
2023/09/15 03:33:35 - INFO - root -   Epoch: [112/500][220/346], lr: 0.00000260 	 loss = 2.2731(1.0419)
2023/09/15 03:33:56 - INFO - root -   Epoch: [112/500][240/346], lr: 0.00000260 	 loss = 0.5762(1.0440)
2023/09/15 03:34:13 - INFO - root -   Epoch: [112/500][260/346], lr: 0.00000260 	 loss = 1.0654(1.0379)
2023/09/15 03:34:34 - INFO - root -   Epoch: [112/500][280/346], lr: 0.00000260 	 loss = 1.2175(1.0300)
2023/09/15 03:34:51 - INFO - root -   Epoch: [112/500][300/346], lr: 0.00000260 	 loss = 1.1259(1.0370)
2023/09/15 03:35:12 - INFO - root -   Epoch: [112/500][320/346], lr: 0.00000260 	 loss = 0.3370(1.0378)
2023/09/15 03:35:28 - INFO - root -   Epoch: [112/500][340/346], lr: 0.00000260 	 loss = 1.7767(1.0342)
2023/09/15 03:35:30 - INFO - root -   Epoch: [112/500] 	 loss = 1.0306
2023/09/15 03:35:30 - INFO - root -   train_accuracy = 0.6084
2023/09/15 03:35:39 - INFO - root -   Epoch: [113/500][0/346], lr: 0.00000262 	 loss = 2.1502(2.1502)
2023/09/15 03:35:55 - INFO - root -   Epoch: [113/500][20/346], lr: 0.00000262 	 loss = 1.3796(1.0733)
2023/09/15 03:36:16 - INFO - root -   Epoch: [113/500][40/346], lr: 0.00000262 	 loss = 1.0516(1.0240)
2023/09/15 03:36:33 - INFO - root -   Epoch: [113/500][60/346], lr: 0.00000262 	 loss = 0.8716(1.0312)
2023/09/15 03:36:53 - INFO - root -   Epoch: [113/500][80/346], lr: 0.00000262 	 loss = 1.2967(1.0779)
2023/09/15 03:37:10 - INFO - root -   Epoch: [113/500][100/346], lr: 0.00000262 	 loss = 0.4586(1.0740)
2023/09/15 03:37:31 - INFO - root -   Epoch: [113/500][120/346], lr: 0.00000262 	 loss = 1.3206(1.0941)
2023/09/15 03:37:47 - INFO - root -   Epoch: [113/500][140/346], lr: 0.00000262 	 loss = 2.2632(1.0951)
2023/09/15 03:38:08 - INFO - root -   Epoch: [113/500][160/346], lr: 0.00000262 	 loss = 0.8017(1.0627)
2023/09/15 03:38:25 - INFO - root -   Epoch: [113/500][180/346], lr: 0.00000262 	 loss = 1.0497(1.0534)
2023/09/15 03:38:45 - INFO - root -   Epoch: [113/500][200/346], lr: 0.00000262 	 loss = 0.1085(1.0339)
2023/09/15 03:39:02 - INFO - root -   Epoch: [113/500][220/346], lr: 0.00000262 	 loss = 1.8272(1.0299)
2023/09/15 03:39:23 - INFO - root -   Epoch: [113/500][240/346], lr: 0.00000262 	 loss = 0.4680(1.0295)
2023/09/15 03:39:39 - INFO - root -   Epoch: [113/500][260/346], lr: 0.00000262 	 loss = 0.9202(1.0239)
2023/09/15 03:40:00 - INFO - root -   Epoch: [113/500][280/346], lr: 0.00000262 	 loss = 1.0853(1.0175)
2023/09/15 03:40:17 - INFO - root -   Epoch: [113/500][300/346], lr: 0.00000262 	 loss = 1.7868(1.0235)
2023/09/15 03:40:38 - INFO - root -   Epoch: [113/500][320/346], lr: 0.00000262 	 loss = 0.2120(1.0254)
2023/09/15 03:40:54 - INFO - root -   Epoch: [113/500][340/346], lr: 0.00000262 	 loss = 2.0397(1.0289)
2023/09/15 03:40:56 - INFO - root -   Epoch: [113/500] 	 loss = 1.0255
2023/09/15 03:40:56 - INFO - root -   train_accuracy = 0.5954
2023/09/15 03:41:05 - INFO - root -   Epoch: [114/500][0/346], lr: 0.00000263 	 loss = 1.9659(1.9659)
2023/09/15 03:41:21 - INFO - root -   Epoch: [114/500][20/346], lr: 0.00000263 	 loss = 1.1976(0.9714)
2023/09/15 03:41:42 - INFO - root -   Epoch: [114/500][40/346], lr: 0.00000263 	 loss = 0.9808(0.9554)
2023/09/15 03:41:59 - INFO - root -   Epoch: [114/500][60/346], lr: 0.00000263 	 loss = 0.8090(0.9768)
2023/09/15 03:42:20 - INFO - root -   Epoch: [114/500][80/346], lr: 0.00000263 	 loss = 1.3071(1.0482)
2023/09/15 03:42:36 - INFO - root -   Epoch: [114/500][100/346], lr: 0.00000263 	 loss = 0.5173(1.0446)
2023/09/15 03:42:57 - INFO - root -   Epoch: [114/500][120/346], lr: 0.00000263 	 loss = 1.1655(1.0562)
2023/09/15 03:43:14 - INFO - root -   Epoch: [114/500][140/346], lr: 0.00000263 	 loss = 2.1299(1.0635)
2023/09/15 03:43:35 - INFO - root -   Epoch: [114/500][160/346], lr: 0.00000263 	 loss = 0.9746(1.0362)
2023/09/15 03:43:51 - INFO - root -   Epoch: [114/500][180/346], lr: 0.00000263 	 loss = 0.9486(1.0257)
2023/09/15 03:44:12 - INFO - root -   Epoch: [114/500][200/346], lr: 0.00000263 	 loss = 0.1662(1.0041)
2023/09/15 03:44:29 - INFO - root -   Epoch: [114/500][220/346], lr: 0.00000263 	 loss = 1.9236(1.0059)
2023/09/15 03:44:49 - INFO - root -   Epoch: [114/500][240/346], lr: 0.00000263 	 loss = 0.4391(1.0090)
2023/09/15 03:45:06 - INFO - root -   Epoch: [114/500][260/346], lr: 0.00000263 	 loss = 0.9426(1.0035)
2023/09/15 03:45:27 - INFO - root -   Epoch: [114/500][280/346], lr: 0.00000263 	 loss = 0.8245(0.9950)
2023/09/15 03:45:43 - INFO - root -   Epoch: [114/500][300/346], lr: 0.00000263 	 loss = 1.4508(0.9996)
2023/09/15 03:46:04 - INFO - root -   Epoch: [114/500][320/346], lr: 0.00000263 	 loss = 0.2640(0.9986)
2023/09/15 03:46:21 - INFO - root -   Epoch: [114/500][340/346], lr: 0.00000263 	 loss = 2.1545(1.0021)
2023/09/15 03:46:23 - INFO - root -   Epoch: [114/500] 	 loss = 0.9995
2023/09/15 03:47:45 - INFO - root -   precision = 0.5632
2023/09/15 03:47:45 - INFO - root -   eval_loss = 1.0950
2023/09/15 03:47:46 - INFO - root -   train_accuracy = 0.6069
2023/09/15 03:47:54 - INFO - root -   Epoch: [115/500][0/346], lr: 0.00000265 	 loss = 1.9979(1.9979)
2023/09/15 03:48:11 - INFO - root -   Epoch: [115/500][20/346], lr: 0.00000265 	 loss = 1.4797(1.0055)
2023/09/15 03:48:31 - INFO - root -   Epoch: [115/500][40/346], lr: 0.00000265 	 loss = 0.9247(0.9940)
2023/09/15 03:48:48 - INFO - root -   Epoch: [115/500][60/346], lr: 0.00000265 	 loss = 0.7787(1.0070)
2023/09/15 03:49:09 - INFO - root -   Epoch: [115/500][80/346], lr: 0.00000265 	 loss = 1.4430(1.0516)
2023/09/15 03:49:25 - INFO - root -   Epoch: [115/500][100/346], lr: 0.00000265 	 loss = 0.3332(1.0453)
2023/09/15 03:49:46 - INFO - root -   Epoch: [115/500][120/346], lr: 0.00000265 	 loss = 1.0939(1.0629)
2023/09/15 03:50:03 - INFO - root -   Epoch: [115/500][140/346], lr: 0.00000265 	 loss = 1.8316(1.0579)
2023/09/15 03:50:24 - INFO - root -   Epoch: [115/500][160/346], lr: 0.00000265 	 loss = 0.8508(1.0369)
2023/09/15 03:50:40 - INFO - root -   Epoch: [115/500][180/346], lr: 0.00000265 	 loss = 0.8119(1.0185)
2023/09/15 03:51:01 - INFO - root -   Epoch: [115/500][200/346], lr: 0.00000265 	 loss = 0.1200(0.9996)
2023/09/15 03:51:18 - INFO - root -   Epoch: [115/500][220/346], lr: 0.00000265 	 loss = 1.6499(0.9990)
2023/09/15 03:51:39 - INFO - root -   Epoch: [115/500][240/346], lr: 0.00000265 	 loss = 0.3886(0.9974)
2023/09/15 03:51:56 - INFO - root -   Epoch: [115/500][260/346], lr: 0.00000265 	 loss = 0.8543(0.9901)
2023/09/15 03:52:16 - INFO - root -   Epoch: [115/500][280/346], lr: 0.00000265 	 loss = 1.2162(0.9830)
2023/09/15 03:52:33 - INFO - root -   Epoch: [115/500][300/346], lr: 0.00000265 	 loss = 1.4580(0.9875)
2023/09/15 03:52:54 - INFO - root -   Epoch: [115/500][320/346], lr: 0.00000265 	 loss = 0.3352(0.9877)
2023/09/15 03:53:10 - INFO - root -   Epoch: [115/500][340/346], lr: 0.00000265 	 loss = 1.9071(0.9885)
2023/09/15 03:53:12 - INFO - root -   Epoch: [115/500] 	 loss = 0.9851
2023/09/15 03:53:12 - INFO - root -   train_accuracy = 0.6228
2023/09/15 03:53:20 - INFO - root -   Epoch: [116/500][0/346], lr: 0.00000266 	 loss = 2.3055(2.3055)
2023/09/15 03:53:37 - INFO - root -   Epoch: [116/500][20/346], lr: 0.00000266 	 loss = 1.3373(1.0550)
2023/09/15 03:53:58 - INFO - root -   Epoch: [116/500][40/346], lr: 0.00000266 	 loss = 0.8636(0.9894)
2023/09/15 03:54:14 - INFO - root -   Epoch: [116/500][60/346], lr: 0.00000266 	 loss = 0.6213(0.9772)
2023/09/15 03:54:35 - INFO - root -   Epoch: [116/500][80/346], lr: 0.00000266 	 loss = 0.9712(1.0310)
2023/09/15 03:54:52 - INFO - root -   Epoch: [116/500][100/346], lr: 0.00000266 	 loss = 0.7140(1.0340)
2023/09/15 03:55:13 - INFO - root -   Epoch: [116/500][120/346], lr: 0.00000266 	 loss = 1.1794(1.0681)
2023/09/15 03:55:30 - INFO - root -   Epoch: [116/500][140/346], lr: 0.00000266 	 loss = 2.2704(1.0689)
2023/09/15 03:55:50 - INFO - root -   Epoch: [116/500][160/346], lr: 0.00000266 	 loss = 1.1042(1.0411)
2023/09/15 03:56:07 - INFO - root -   Epoch: [116/500][180/346], lr: 0.00000266 	 loss = 1.4296(1.0315)
2023/09/15 03:56:28 - INFO - root -   Epoch: [116/500][200/346], lr: 0.00000266 	 loss = 0.1224(1.0078)
2023/09/15 03:56:45 - INFO - root -   Epoch: [116/500][220/346], lr: 0.00000266 	 loss = 1.8645(1.0067)
2023/09/15 03:57:06 - INFO - root -   Epoch: [116/500][240/346], lr: 0.00000266 	 loss = 0.3553(1.0053)
2023/09/15 03:57:22 - INFO - root -   Epoch: [116/500][260/346], lr: 0.00000266 	 loss = 1.0404(0.9972)
2023/09/15 03:57:43 - INFO - root -   Epoch: [116/500][280/346], lr: 0.00000266 	 loss = 1.0726(0.9878)
2023/09/15 03:58:00 - INFO - root -   Epoch: [116/500][300/346], lr: 0.00000266 	 loss = 1.3712(1.0001)
2023/09/15 03:58:21 - INFO - root -   Epoch: [116/500][320/346], lr: 0.00000266 	 loss = 0.2190(0.9995)
2023/09/15 03:58:37 - INFO - root -   Epoch: [116/500][340/346], lr: 0.00000266 	 loss = 1.8311(1.0019)
2023/09/15 03:58:39 - INFO - root -   Epoch: [116/500] 	 loss = 0.9986
2023/09/15 03:58:39 - INFO - root -   train_accuracy = 0.6171
2023/09/15 03:58:47 - INFO - root -   Epoch: [117/500][0/346], lr: 0.00000267 	 loss = 1.5798(1.5798)
2023/09/15 03:59:04 - INFO - root -   Epoch: [117/500][20/346], lr: 0.00000267 	 loss = 1.3308(0.9521)
2023/09/15 03:59:25 - INFO - root -   Epoch: [117/500][40/346], lr: 0.00000267 	 loss = 0.9208(0.9500)
2023/09/15 03:59:41 - INFO - root -   Epoch: [117/500][60/346], lr: 0.00000267 	 loss = 0.5865(0.9612)
2023/09/15 04:00:02 - INFO - root -   Epoch: [117/500][80/346], lr: 0.00000267 	 loss = 1.5555(1.0183)
2023/09/15 04:00:19 - INFO - root -   Epoch: [117/500][100/346], lr: 0.00000267 	 loss = 0.4787(1.0189)
2023/09/15 04:00:40 - INFO - root -   Epoch: [117/500][120/346], lr: 0.00000267 	 loss = 0.7158(1.0378)
2023/09/15 04:00:56 - INFO - root -   Epoch: [117/500][140/346], lr: 0.00000267 	 loss = 2.1353(1.0412)
2023/09/15 04:01:17 - INFO - root -   Epoch: [117/500][160/346], lr: 0.00000267 	 loss = 1.0367(1.0227)
2023/09/15 04:01:34 - INFO - root -   Epoch: [117/500][180/346], lr: 0.00000267 	 loss = 0.8825(1.0116)
2023/09/15 04:01:54 - INFO - root -   Epoch: [117/500][200/346], lr: 0.00000267 	 loss = 0.1100(0.9901)
2023/09/15 04:02:11 - INFO - root -   Epoch: [117/500][220/346], lr: 0.00000267 	 loss = 2.0545(0.9908)
2023/09/15 04:02:32 - INFO - root -   Epoch: [117/500][240/346], lr: 0.00000267 	 loss = 0.3854(0.9854)
2023/09/15 04:02:48 - INFO - root -   Epoch: [117/500][260/346], lr: 0.00000267 	 loss = 0.9440(0.9777)
2023/09/15 04:03:09 - INFO - root -   Epoch: [117/500][280/346], lr: 0.00000267 	 loss = 0.5559(0.9753)
2023/09/15 04:03:26 - INFO - root -   Epoch: [117/500][300/346], lr: 0.00000267 	 loss = 1.5236(0.9861)
2023/09/15 04:03:47 - INFO - root -   Epoch: [117/500][320/346], lr: 0.00000267 	 loss = 0.1451(0.9888)
2023/09/15 04:04:03 - INFO - root -   Epoch: [117/500][340/346], lr: 0.00000267 	 loss = 1.8990(0.9911)
2023/09/15 04:04:05 - INFO - root -   Epoch: [117/500] 	 loss = 0.9878
2023/09/15 04:04:05 - INFO - root -   train_accuracy = 0.6228
2023/09/15 04:04:13 - INFO - root -   Epoch: [118/500][0/346], lr: 0.00000269 	 loss = 1.7406(1.7406)
2023/09/15 04:04:30 - INFO - root -   Epoch: [118/500][20/346], lr: 0.00000269 	 loss = 1.1411(0.9260)
2023/09/15 04:04:51 - INFO - root -   Epoch: [118/500][40/346], lr: 0.00000269 	 loss = 0.7398(0.9299)
2023/09/15 04:05:07 - INFO - root -   Epoch: [118/500][60/346], lr: 0.00000269 	 loss = 0.5742(0.9419)
2023/09/15 04:05:28 - INFO - root -   Epoch: [118/500][80/346], lr: 0.00000269 	 loss = 1.3757(1.0142)
2023/09/15 04:05:45 - INFO - root -   Epoch: [118/500][100/346], lr: 0.00000269 	 loss = 0.3217(1.0091)
2023/09/15 04:06:05 - INFO - root -   Epoch: [118/500][120/346], lr: 0.00000269 	 loss = 1.0889(1.0326)
2023/09/15 04:06:22 - INFO - root -   Epoch: [118/500][140/346], lr: 0.00000269 	 loss = 2.2302(1.0349)
2023/09/15 04:06:43 - INFO - root -   Epoch: [118/500][160/346], lr: 0.00000269 	 loss = 1.1484(1.0167)
2023/09/15 04:06:59 - INFO - root -   Epoch: [118/500][180/346], lr: 0.00000269 	 loss = 1.0735(1.0074)
2023/09/15 04:07:20 - INFO - root -   Epoch: [118/500][200/346], lr: 0.00000269 	 loss = 0.1273(0.9833)
2023/09/15 04:07:37 - INFO - root -   Epoch: [118/500][220/346], lr: 0.00000269 	 loss = 1.5901(0.9748)
2023/09/15 04:07:58 - INFO - root -   Epoch: [118/500][240/346], lr: 0.00000269 	 loss = 0.4620(0.9779)
2023/09/15 04:08:14 - INFO - root -   Epoch: [118/500][260/346], lr: 0.00000269 	 loss = 1.0882(0.9714)
2023/09/15 04:08:35 - INFO - root -   Epoch: [118/500][280/346], lr: 0.00000269 	 loss = 0.9352(0.9643)
2023/09/15 04:08:52 - INFO - root -   Epoch: [118/500][300/346], lr: 0.00000269 	 loss = 1.3087(0.9690)
2023/09/15 04:09:13 - INFO - root -   Epoch: [118/500][320/346], lr: 0.00000269 	 loss = 0.1473(0.9695)
2023/09/15 04:09:29 - INFO - root -   Epoch: [118/500][340/346], lr: 0.00000269 	 loss = 1.5539(0.9710)
2023/09/15 04:09:31 - INFO - root -   Epoch: [118/500] 	 loss = 0.9674
2023/09/15 04:09:31 - INFO - root -   train_accuracy = 0.6344
2023/09/15 04:09:39 - INFO - root -   Epoch: [119/500][0/346], lr: 0.00000270 	 loss = 1.6252(1.6252)
2023/09/15 04:09:56 - INFO - root -   Epoch: [119/500][20/346], lr: 0.00000270 	 loss = 1.1851(0.9335)
2023/09/15 04:10:16 - INFO - root -   Epoch: [119/500][40/346], lr: 0.00000270 	 loss = 0.5953(0.9051)
2023/09/15 04:10:33 - INFO - root -   Epoch: [119/500][60/346], lr: 0.00000270 	 loss = 0.8305(0.9134)
2023/09/15 04:10:54 - INFO - root -   Epoch: [119/500][80/346], lr: 0.00000270 	 loss = 1.4899(0.9773)
2023/09/15 04:11:10 - INFO - root -   Epoch: [119/500][100/346], lr: 0.00000270 	 loss = 0.4217(0.9761)
2023/09/15 04:11:31 - INFO - root -   Epoch: [119/500][120/346], lr: 0.00000270 	 loss = 0.9964(1.0018)
2023/09/15 04:11:48 - INFO - root -   Epoch: [119/500][140/346], lr: 0.00000270 	 loss = 2.5851(1.0226)
2023/09/15 04:12:09 - INFO - root -   Epoch: [119/500][160/346], lr: 0.00000270 	 loss = 1.1480(0.9992)
2023/09/15 04:12:25 - INFO - root -   Epoch: [119/500][180/346], lr: 0.00000270 	 loss = 0.7830(0.9857)
2023/09/15 04:12:46 - INFO - root -   Epoch: [119/500][200/346], lr: 0.00000270 	 loss = 0.1010(0.9645)
2023/09/15 04:13:03 - INFO - root -   Epoch: [119/500][220/346], lr: 0.00000270 	 loss = 1.7364(0.9640)
2023/09/15 04:13:24 - INFO - root -   Epoch: [119/500][240/346], lr: 0.00000270 	 loss = 0.4195(0.9706)
2023/09/15 04:13:41 - INFO - root -   Epoch: [119/500][260/346], lr: 0.00000270 	 loss = 0.8855(0.9630)
2023/09/15 04:14:01 - INFO - root -   Epoch: [119/500][280/346], lr: 0.00000270 	 loss = 1.2365(0.9602)
2023/09/15 04:14:17 - INFO - root -   Epoch: [119/500][300/346], lr: 0.00000270 	 loss = 1.4604(0.9688)
2023/09/15 04:14:38 - INFO - root -   Epoch: [119/500][320/346], lr: 0.00000270 	 loss = 0.1253(0.9673)
2023/09/15 04:14:54 - INFO - root -   Epoch: [119/500][340/346], lr: 0.00000270 	 loss = 1.6899(0.9675)
2023/09/15 04:14:56 - INFO - root -   Epoch: [119/500] 	 loss = 0.9648
2023/09/15 04:16:19 - INFO - root -   precision = 0.5575
2023/09/15 04:16:19 - INFO - root -   eval_loss = 1.1053
2023/09/15 04:16:20 - INFO - root -   train_accuracy = 0.6373
2023/09/15 04:16:28 - INFO - root -   Epoch: [120/500][0/346], lr: 0.00000272 	 loss = 1.7620(1.7620)
2023/09/15 04:16:45 - INFO - root -   Epoch: [120/500][20/346], lr: 0.00000272 	 loss = 1.1105(0.9128)
2023/09/15 04:17:05 - INFO - root -   Epoch: [120/500][40/346], lr: 0.00000272 	 loss = 0.7125(0.9107)
2023/09/15 04:17:22 - INFO - root -   Epoch: [120/500][60/346], lr: 0.00000272 	 loss = 0.7411(0.9165)
2023/09/15 04:17:43 - INFO - root -   Epoch: [120/500][80/346], lr: 0.00000272 	 loss = 1.5094(0.9899)
2023/09/15 04:17:59 - INFO - root -   Epoch: [120/500][100/346], lr: 0.00000272 	 loss = 0.3255(0.9791)
2023/09/15 04:18:20 - INFO - root -   Epoch: [120/500][120/346], lr: 0.00000272 	 loss = 1.1595(0.9972)
2023/09/15 04:18:37 - INFO - root -   Epoch: [120/500][140/346], lr: 0.00000272 	 loss = 1.8659(1.0074)
2023/09/15 04:18:58 - INFO - root -   Epoch: [120/500][160/346], lr: 0.00000272 	 loss = 1.0737(0.9881)
2023/09/15 04:19:15 - INFO - root -   Epoch: [120/500][180/346], lr: 0.00000272 	 loss = 1.0622(0.9847)
2023/09/15 04:19:36 - INFO - root -   Epoch: [120/500][200/346], lr: 0.00000272 	 loss = 0.1459(0.9610)
2023/09/15 04:19:52 - INFO - root -   Epoch: [120/500][220/346], lr: 0.00000272 	 loss = 1.7764(0.9538)
2023/09/15 04:20:13 - INFO - root -   Epoch: [120/500][240/346], lr: 0.00000272 	 loss = 0.3764(0.9544)
2023/09/15 04:20:30 - INFO - root -   Epoch: [120/500][260/346], lr: 0.00000272 	 loss = 1.2108(0.9507)
2023/09/15 04:20:51 - INFO - root -   Epoch: [120/500][280/346], lr: 0.00000272 	 loss = 0.9751(0.9478)
2023/09/15 04:21:07 - INFO - root -   Epoch: [120/500][300/346], lr: 0.00000272 	 loss = 1.2277(0.9524)
2023/09/15 04:21:29 - INFO - root -   Epoch: [120/500][320/346], lr: 0.00000272 	 loss = 0.1681(0.9509)
2023/09/15 04:21:45 - INFO - root -   Epoch: [120/500][340/346], lr: 0.00000272 	 loss = 1.7576(0.9496)
2023/09/15 04:21:47 - INFO - root -   Epoch: [120/500] 	 loss = 0.9464
2023/09/15 04:21:47 - INFO - root -   train_accuracy = 0.6315
2023/09/15 04:21:55 - INFO - root -   Epoch: [121/500][0/346], lr: 0.00000273 	 loss = 2.6281(2.6281)
2023/09/15 04:22:12 - INFO - root -   Epoch: [121/500][20/346], lr: 0.00000273 	 loss = 1.0000(1.0445)
2023/09/15 04:22:33 - INFO - root -   Epoch: [121/500][40/346], lr: 0.00000273 	 loss = 1.0800(0.9951)
2023/09/15 04:22:49 - INFO - root -   Epoch: [121/500][60/346], lr: 0.00000273 	 loss = 0.8510(0.9758)
2023/09/15 04:23:10 - INFO - root -   Epoch: [121/500][80/346], lr: 0.00000273 	 loss = 1.5608(1.0174)
2023/09/15 04:23:27 - INFO - root -   Epoch: [121/500][100/346], lr: 0.00000273 	 loss = 0.3172(0.9955)
2023/09/15 04:23:46 - INFO - root -   Epoch: [121/500][120/346], lr: 0.00000273 	 loss = 0.8549(1.0210)
2023/09/15 04:24:04 - INFO - root -   Epoch: [121/500][140/346], lr: 0.00000273 	 loss = 2.5775(1.0281)
2023/09/15 04:24:23 - INFO - root -   Epoch: [121/500][160/346], lr: 0.00000273 	 loss = 1.3667(1.0020)
2023/09/15 04:24:41 - INFO - root -   Epoch: [121/500][180/346], lr: 0.00000273 	 loss = 0.6798(0.9909)
2023/09/15 04:24:59 - INFO - root -   Epoch: [121/500][200/346], lr: 0.00000273 	 loss = 0.0887(0.9638)
2023/09/15 04:25:19 - INFO - root -   Epoch: [121/500][220/346], lr: 0.00000273 	 loss = 1.7684(0.9620)
2023/09/15 04:25:36 - INFO - root -   Epoch: [121/500][240/346], lr: 0.00000273 	 loss = 0.3275(0.9646)
2023/09/15 04:25:56 - INFO - root -   Epoch: [121/500][260/346], lr: 0.00000273 	 loss = 1.1253(0.9517)
2023/09/15 04:26:14 - INFO - root -   Epoch: [121/500][280/346], lr: 0.00000273 	 loss = 0.8011(0.9461)
2023/09/15 04:26:34 - INFO - root -   Epoch: [121/500][300/346], lr: 0.00000273 	 loss = 0.9774(0.9518)
2023/09/15 04:26:52 - INFO - root -   Epoch: [121/500][320/346], lr: 0.00000273 	 loss = 0.2473(0.9517)
2023/09/15 04:27:11 - INFO - root -   Epoch: [121/500][340/346], lr: 0.00000273 	 loss = 1.8147(0.9501)
2023/09/15 04:27:13 - INFO - root -   Epoch: [121/500] 	 loss = 0.9465
2023/09/15 04:27:13 - INFO - root -   train_accuracy = 0.6460
2023/09/15 04:27:22 - INFO - root -   Epoch: [122/500][0/346], lr: 0.00000275 	 loss = 2.1841(2.1841)
2023/09/15 04:27:39 - INFO - root -   Epoch: [122/500][20/346], lr: 0.00000275 	 loss = 1.0944(0.9590)
2023/09/15 04:28:00 - INFO - root -   Epoch: [122/500][40/346], lr: 0.00000275 	 loss = 0.8931(0.9387)
2023/09/15 04:28:16 - INFO - root -   Epoch: [122/500][60/346], lr: 0.00000275 	 loss = 0.7589(0.9284)
2023/09/15 04:28:37 - INFO - root -   Epoch: [122/500][80/346], lr: 0.00000275 	 loss = 1.5176(0.9885)
2023/09/15 04:28:54 - INFO - root -   Epoch: [122/500][100/346], lr: 0.00000275 	 loss = 0.3612(0.9735)
2023/09/15 04:29:15 - INFO - root -   Epoch: [122/500][120/346], lr: 0.00000275 	 loss = 0.7995(0.9924)
2023/09/15 04:29:31 - INFO - root -   Epoch: [122/500][140/346], lr: 0.00000275 	 loss = 1.8016(0.9941)
2023/09/15 04:29:52 - INFO - root -   Epoch: [122/500][160/346], lr: 0.00000275 	 loss = 1.1967(0.9822)
2023/09/15 04:30:09 - INFO - root -   Epoch: [122/500][180/346], lr: 0.00000275 	 loss = 0.6612(0.9639)
2023/09/15 04:30:29 - INFO - root -   Epoch: [122/500][200/346], lr: 0.00000275 	 loss = 0.1168(0.9403)
2023/09/15 04:30:46 - INFO - root -   Epoch: [122/500][220/346], lr: 0.00000275 	 loss = 1.8332(0.9393)
2023/09/15 04:31:07 - INFO - root -   Epoch: [122/500][240/346], lr: 0.00000275 	 loss = 0.4348(0.9413)
2023/09/15 04:31:23 - INFO - root -   Epoch: [122/500][260/346], lr: 0.00000275 	 loss = 1.0944(0.9332)
2023/09/15 04:31:44 - INFO - root -   Epoch: [122/500][280/346], lr: 0.00000275 	 loss = 1.1329(0.9255)
2023/09/15 04:32:01 - INFO - root -   Epoch: [122/500][300/346], lr: 0.00000275 	 loss = 1.0190(0.9262)
2023/09/15 04:32:22 - INFO - root -   Epoch: [122/500][320/346], lr: 0.00000275 	 loss = 0.1666(0.9219)
2023/09/15 04:32:37 - INFO - root -   Epoch: [122/500][340/346], lr: 0.00000275 	 loss = 1.7750(0.9229)
2023/09/15 04:32:39 - INFO - root -   Epoch: [122/500] 	 loss = 0.9214
2023/09/15 04:32:39 - INFO - root -   train_accuracy = 0.6546
2023/09/15 04:32:48 - INFO - root -   Epoch: [123/500][0/346], lr: 0.00000276 	 loss = 2.1401(2.1401)
2023/09/15 04:33:04 - INFO - root -   Epoch: [123/500][20/346], lr: 0.00000276 	 loss = 1.0687(1.0044)
2023/09/15 04:33:25 - INFO - root -   Epoch: [123/500][40/346], lr: 0.00000276 	 loss = 0.9860(0.9566)
2023/09/15 04:33:42 - INFO - root -   Epoch: [123/500][60/346], lr: 0.00000276 	 loss = 0.7952(0.9413)
2023/09/15 04:34:03 - INFO - root -   Epoch: [123/500][80/346], lr: 0.00000276 	 loss = 1.4287(0.9861)
2023/09/15 04:34:20 - INFO - root -   Epoch: [123/500][100/346], lr: 0.00000276 	 loss = 0.3982(0.9687)
2023/09/15 04:34:41 - INFO - root -   Epoch: [123/500][120/346], lr: 0.00000276 	 loss = 0.9804(0.9835)
2023/09/15 04:34:57 - INFO - root -   Epoch: [123/500][140/346], lr: 0.00000276 	 loss = 2.3662(0.9879)
2023/09/15 04:35:18 - INFO - root -   Epoch: [123/500][160/346], lr: 0.00000276 	 loss = 1.4702(0.9735)
2023/09/15 04:35:35 - INFO - root -   Epoch: [123/500][180/346], lr: 0.00000276 	 loss = 0.6759(0.9612)
2023/09/15 04:35:56 - INFO - root -   Epoch: [123/500][200/346], lr: 0.00000276 	 loss = 0.1204(0.9333)
2023/09/15 04:36:12 - INFO - root -   Epoch: [123/500][220/346], lr: 0.00000276 	 loss = 1.7362(0.9297)
2023/09/15 04:36:33 - INFO - root -   Epoch: [123/500][240/346], lr: 0.00000276 	 loss = 0.3100(0.9271)
2023/09/15 04:36:50 - INFO - root -   Epoch: [123/500][260/346], lr: 0.00000276 	 loss = 1.0696(0.9183)
2023/09/15 04:37:11 - INFO - root -   Epoch: [123/500][280/346], lr: 0.00000276 	 loss = 0.6362(0.9120)
2023/09/15 04:37:28 - INFO - root -   Epoch: [123/500][300/346], lr: 0.00000276 	 loss = 1.4834(0.9195)
2023/09/15 04:37:49 - INFO - root -   Epoch: [123/500][320/346], lr: 0.00000276 	 loss = 0.2196(0.9177)
2023/09/15 04:38:05 - INFO - root -   Epoch: [123/500][340/346], lr: 0.00000276 	 loss = 1.2731(0.9139)
2023/09/15 04:38:07 - INFO - root -   Epoch: [123/500] 	 loss = 0.9089
2023/09/15 04:38:07 - INFO - root -   train_accuracy = 0.6460
2023/09/15 04:38:16 - INFO - root -   Epoch: [124/500][0/346], lr: 0.00000277 	 loss = 2.4900(2.4900)
2023/09/15 04:38:33 - INFO - root -   Epoch: [124/500][20/346], lr: 0.00000277 	 loss = 1.0426(1.1325)
2023/09/15 04:38:54 - INFO - root -   Epoch: [124/500][40/346], lr: 0.00000277 	 loss = 1.0643(1.0031)
2023/09/15 04:39:10 - INFO - root -   Epoch: [124/500][60/346], lr: 0.00000277 	 loss = 0.3885(0.9762)
2023/09/15 04:39:32 - INFO - root -   Epoch: [124/500][80/346], lr: 0.00000277 	 loss = 1.7662(0.9942)
2023/09/15 04:39:49 - INFO - root -   Epoch: [124/500][100/346], lr: 0.00000277 	 loss = 0.4040(0.9692)
2023/09/15 04:40:09 - INFO - root -   Epoch: [124/500][120/346], lr: 0.00000277 	 loss = 0.9520(0.9858)
2023/09/15 04:40:26 - INFO - root -   Epoch: [124/500][140/346], lr: 0.00000277 	 loss = 2.1130(0.9876)
2023/09/15 04:40:47 - INFO - root -   Epoch: [124/500][160/346], lr: 0.00000277 	 loss = 1.3496(0.9715)
2023/09/15 04:41:04 - INFO - root -   Epoch: [124/500][180/346], lr: 0.00000277 	 loss = 1.0554(0.9563)
2023/09/15 04:41:25 - INFO - root -   Epoch: [124/500][200/346], lr: 0.00000277 	 loss = 0.1062(0.9272)
2023/09/15 04:41:42 - INFO - root -   Epoch: [124/500][220/346], lr: 0.00000277 	 loss = 1.6693(0.9199)
2023/09/15 04:42:04 - INFO - root -   Epoch: [124/500][240/346], lr: 0.00000277 	 loss = 0.3774(0.9214)
2023/09/15 04:42:20 - INFO - root -   Epoch: [124/500][260/346], lr: 0.00000277 	 loss = 0.8790(0.9084)
2023/09/15 04:42:41 - INFO - root -   Epoch: [124/500][280/346], lr: 0.00000277 	 loss = 0.8031(0.9033)
2023/09/15 04:42:58 - INFO - root -   Epoch: [124/500][300/346], lr: 0.00000277 	 loss = 1.3901(0.9151)
2023/09/15 04:43:19 - INFO - root -   Epoch: [124/500][320/346], lr: 0.00000277 	 loss = 0.1112(0.9156)
2023/09/15 04:43:36 - INFO - root -   Epoch: [124/500][340/346], lr: 0.00000277 	 loss = 1.5146(0.9144)
2023/09/15 04:43:38 - INFO - root -   Epoch: [124/500] 	 loss = 0.9119
2023/09/15 04:45:00 - INFO - root -   precision = 0.5345
2023/09/15 04:45:00 - INFO - root -   eval_loss = 1.1464
2023/09/15 04:45:01 - INFO - root -   train_accuracy = 0.6590
2023/09/15 04:45:09 - INFO - root -   Epoch: [125/500][0/346], lr: 0.00000279 	 loss = 1.6433(1.6433)
2023/09/15 04:45:26 - INFO - root -   Epoch: [125/500][20/346], lr: 0.00000279 	 loss = 1.1943(0.9842)
2023/09/15 04:45:47 - INFO - root -   Epoch: [125/500][40/346], lr: 0.00000279 	 loss = 1.1461(0.9413)
2023/09/15 04:46:03 - INFO - root -   Epoch: [125/500][60/346], lr: 0.00000279 	 loss = 0.4778(0.9281)
2023/09/15 04:46:24 - INFO - root -   Epoch: [125/500][80/346], lr: 0.00000279 	 loss = 1.4707(0.9681)
2023/09/15 04:46:41 - INFO - root -   Epoch: [125/500][100/346], lr: 0.00000279 	 loss = 0.4385(0.9603)
2023/09/15 04:47:02 - INFO - root -   Epoch: [125/500][120/346], lr: 0.00000279 	 loss = 1.1887(0.9791)
2023/09/15 04:47:18 - INFO - root -   Epoch: [125/500][140/346], lr: 0.00000279 	 loss = 2.9956(0.9829)
2023/09/15 04:47:39 - INFO - root -   Epoch: [125/500][160/346], lr: 0.00000279 	 loss = 1.5734(0.9670)
2023/09/15 04:47:56 - INFO - root -   Epoch: [125/500][180/346], lr: 0.00000279 	 loss = 0.6951(0.9536)
2023/09/15 04:48:16 - INFO - root -   Epoch: [125/500][200/346], lr: 0.00000279 	 loss = 0.1504(0.9267)
2023/09/15 04:48:33 - INFO - root -   Epoch: [125/500][220/346], lr: 0.00000279 	 loss = 1.5438(0.9231)
2023/09/15 04:48:54 - INFO - root -   Epoch: [125/500][240/346], lr: 0.00000279 	 loss = 0.4625(0.9256)
2023/09/15 04:49:10 - INFO - root -   Epoch: [125/500][260/346], lr: 0.00000279 	 loss = 1.2218(0.9182)
2023/09/15 04:49:31 - INFO - root -   Epoch: [125/500][280/346], lr: 0.00000279 	 loss = 0.8700(0.9118)
2023/09/15 04:49:48 - INFO - root -   Epoch: [125/500][300/346], lr: 0.00000279 	 loss = 1.0871(0.9142)
2023/09/15 04:50:09 - INFO - root -   Epoch: [125/500][320/346], lr: 0.00000279 	 loss = 0.1466(0.9148)
2023/09/15 04:50:25 - INFO - root -   Epoch: [125/500][340/346], lr: 0.00000279 	 loss = 1.4910(0.9097)
2023/09/15 04:50:27 - INFO - root -   Epoch: [125/500] 	 loss = 0.9065
2023/09/15 04:50:27 - INFO - root -   train_accuracy = 0.6604
2023/09/15 04:50:35 - INFO - root -   Epoch: [126/500][0/346], lr: 0.00000280 	 loss = 2.1822(2.1822)
2023/09/15 04:50:52 - INFO - root -   Epoch: [126/500][20/346], lr: 0.00000280 	 loss = 1.0609(0.8998)
2023/09/15 04:51:13 - INFO - root -   Epoch: [126/500][40/346], lr: 0.00000280 	 loss = 1.0596(0.8907)
2023/09/15 04:51:29 - INFO - root -   Epoch: [126/500][60/346], lr: 0.00000280 	 loss = 0.6652(0.8929)
2023/09/15 04:51:50 - INFO - root -   Epoch: [126/500][80/346], lr: 0.00000280 	 loss = 1.5625(0.9228)
2023/09/15 04:52:07 - INFO - root -   Epoch: [126/500][100/346], lr: 0.00000280 	 loss = 0.3806(0.9142)
2023/09/15 04:52:28 - INFO - root -   Epoch: [126/500][120/346], lr: 0.00000280 	 loss = 0.6424(0.9330)
2023/09/15 04:52:44 - INFO - root -   Epoch: [126/500][140/346], lr: 0.00000280 	 loss = 2.1912(0.9326)
2023/09/15 04:53:05 - INFO - root -   Epoch: [126/500][160/346], lr: 0.00000280 	 loss = 1.4202(0.9217)
2023/09/15 04:53:22 - INFO - root -   Epoch: [126/500][180/346], lr: 0.00000280 	 loss = 0.6797(0.9107)
2023/09/15 04:53:43 - INFO - root -   Epoch: [126/500][200/346], lr: 0.00000280 	 loss = 0.0991(0.8929)
2023/09/15 04:53:59 - INFO - root -   Epoch: [126/500][220/346], lr: 0.00000280 	 loss = 2.1290(0.8885)
2023/09/15 04:54:21 - INFO - root -   Epoch: [126/500][240/346], lr: 0.00000280 	 loss = 0.2363(0.8923)
2023/09/15 04:54:37 - INFO - root -   Epoch: [126/500][260/346], lr: 0.00000280 	 loss = 1.4345(0.8829)
2023/09/15 04:54:58 - INFO - root -   Epoch: [126/500][280/346], lr: 0.00000280 	 loss = 0.8335(0.8800)
2023/09/15 04:55:15 - INFO - root -   Epoch: [126/500][300/346], lr: 0.00000280 	 loss = 1.4792(0.8844)
2023/09/15 04:55:36 - INFO - root -   Epoch: [126/500][320/346], lr: 0.00000280 	 loss = 0.1855(0.8847)
2023/09/15 04:55:52 - INFO - root -   Epoch: [126/500][340/346], lr: 0.00000280 	 loss = 1.4296(0.8871)
2023/09/15 04:55:54 - INFO - root -   Epoch: [126/500] 	 loss = 0.8843
2023/09/15 04:55:54 - INFO - root -   train_accuracy = 0.6647
2023/09/15 04:56:02 - INFO - root -   Epoch: [127/500][0/346], lr: 0.00000282 	 loss = 1.5906(1.5906)
2023/09/15 04:56:19 - INFO - root -   Epoch: [127/500][20/346], lr: 0.00000282 	 loss = 0.9948(0.8922)
2023/09/15 04:56:40 - INFO - root -   Epoch: [127/500][40/346], lr: 0.00000282 	 loss = 1.0646(0.8575)
2023/09/15 04:56:56 - INFO - root -   Epoch: [127/500][60/346], lr: 0.00000282 	 loss = 0.5399(0.8644)
2023/09/15 04:57:17 - INFO - root -   Epoch: [127/500][80/346], lr: 0.00000282 	 loss = 1.7583(0.8995)
2023/09/15 04:57:34 - INFO - root -   Epoch: [127/500][100/346], lr: 0.00000282 	 loss = 0.2932(0.8980)
2023/09/15 04:57:54 - INFO - root -   Epoch: [127/500][120/346], lr: 0.00000282 	 loss = 0.6051(0.9224)
2023/09/15 04:58:11 - INFO - root -   Epoch: [127/500][140/346], lr: 0.00000282 	 loss = 1.7740(0.9222)
2023/09/15 04:58:32 - INFO - root -   Epoch: [127/500][160/346], lr: 0.00000282 	 loss = 1.4065(0.9088)
2023/09/15 04:58:49 - INFO - root -   Epoch: [127/500][180/346], lr: 0.00000282 	 loss = 0.7354(0.8957)
2023/09/15 04:59:10 - INFO - root -   Epoch: [127/500][200/346], lr: 0.00000282 	 loss = 0.0579(0.8696)
2023/09/15 04:59:26 - INFO - root -   Epoch: [127/500][220/346], lr: 0.00000282 	 loss = 1.7154(0.8649)
2023/09/15 04:59:47 - INFO - root -   Epoch: [127/500][240/346], lr: 0.00000282 	 loss = 0.3789(0.8704)
2023/09/15 05:00:04 - INFO - root -   Epoch: [127/500][260/346], lr: 0.00000282 	 loss = 1.4391(0.8679)
2023/09/15 05:00:25 - INFO - root -   Epoch: [127/500][280/346], lr: 0.00000282 	 loss = 1.1824(0.8672)
2023/09/15 05:00:41 - INFO - root -   Epoch: [127/500][300/346], lr: 0.00000282 	 loss = 0.8321(0.8702)
2023/09/15 05:01:02 - INFO - root -   Epoch: [127/500][320/346], lr: 0.00000282 	 loss = 0.2554(0.8700)
2023/09/15 05:01:19 - INFO - root -   Epoch: [127/500][340/346], lr: 0.00000282 	 loss = 1.4198(0.8693)
2023/09/15 05:01:21 - INFO - root -   Epoch: [127/500] 	 loss = 0.8662
2023/09/15 05:01:21 - INFO - root -   train_accuracy = 0.6777
2023/09/15 05:01:29 - INFO - root -   Epoch: [128/500][0/346], lr: 0.00000283 	 loss = 1.7650(1.7650)
2023/09/15 05:01:45 - INFO - root -   Epoch: [128/500][20/346], lr: 0.00000283 	 loss = 1.3155(0.8975)
2023/09/15 05:02:06 - INFO - root -   Epoch: [128/500][40/346], lr: 0.00000283 	 loss = 0.9984(0.8633)
2023/09/15 05:02:23 - INFO - root -   Epoch: [128/500][60/346], lr: 0.00000283 	 loss = 0.3470(0.8859)
2023/09/15 05:02:44 - INFO - root -   Epoch: [128/500][80/346], lr: 0.00000283 	 loss = 1.7111(0.9224)
2023/09/15 05:03:00 - INFO - root -   Epoch: [128/500][100/346], lr: 0.00000283 	 loss = 0.3020(0.9108)
2023/09/15 05:03:21 - INFO - root -   Epoch: [128/500][120/346], lr: 0.00000283 	 loss = 0.6959(0.9230)
2023/09/15 05:03:38 - INFO - root -   Epoch: [128/500][140/346], lr: 0.00000283 	 loss = 2.3892(0.9362)
2023/09/15 05:03:59 - INFO - root -   Epoch: [128/500][160/346], lr: 0.00000283 	 loss = 1.6506(0.9284)
2023/09/15 05:04:16 - INFO - root -   Epoch: [128/500][180/346], lr: 0.00000283 	 loss = 0.6833(0.9209)
2023/09/15 05:04:37 - INFO - root -   Epoch: [128/500][200/346], lr: 0.00000283 	 loss = 0.0561(0.8928)
2023/09/15 05:04:53 - INFO - root -   Epoch: [128/500][220/346], lr: 0.00000283 	 loss = 1.5909(0.8864)
2023/09/15 05:05:14 - INFO - root -   Epoch: [128/500][240/346], lr: 0.00000283 	 loss = 0.3676(0.8849)
2023/09/15 05:05:31 - INFO - root -   Epoch: [128/500][260/346], lr: 0.00000283 	 loss = 1.1695(0.8749)
2023/09/15 05:05:52 - INFO - root -   Epoch: [128/500][280/346], lr: 0.00000283 	 loss = 0.7533(0.8682)
2023/09/15 05:06:08 - INFO - root -   Epoch: [128/500][300/346], lr: 0.00000283 	 loss = 1.0570(0.8747)
2023/09/15 05:06:29 - INFO - root -   Epoch: [128/500][320/346], lr: 0.00000283 	 loss = 0.1966(0.8748)
2023/09/15 05:06:46 - INFO - root -   Epoch: [128/500][340/346], lr: 0.00000283 	 loss = 1.2311(0.8750)
2023/09/15 05:06:48 - INFO - root -   Epoch: [128/500] 	 loss = 0.8722
2023/09/15 05:06:48 - INFO - root -   train_accuracy = 0.6734
2023/09/15 05:06:56 - INFO - root -   Epoch: [129/500][0/346], lr: 0.00000285 	 loss = 2.2172(2.2172)
2023/09/15 05:07:13 - INFO - root -   Epoch: [129/500][20/346], lr: 0.00000285 	 loss = 1.0639(0.9302)
2023/09/15 05:07:33 - INFO - root -   Epoch: [129/500][40/346], lr: 0.00000285 	 loss = 1.1066(0.8640)
2023/09/15 05:07:50 - INFO - root -   Epoch: [129/500][60/346], lr: 0.00000285 	 loss = 0.6209(0.8940)
2023/09/15 05:08:11 - INFO - root -   Epoch: [129/500][80/346], lr: 0.00000285 	 loss = 1.6450(0.9072)
2023/09/15 05:08:28 - INFO - root -   Epoch: [129/500][100/346], lr: 0.00000285 	 loss = 0.2754(0.9084)
2023/09/15 05:08:49 - INFO - root -   Epoch: [129/500][120/346], lr: 0.00000285 	 loss = 0.8384(0.9321)
2023/09/15 05:09:05 - INFO - root -   Epoch: [129/500][140/346], lr: 0.00000285 	 loss = 2.1664(0.9387)
2023/09/15 05:09:26 - INFO - root -   Epoch: [129/500][160/346], lr: 0.00000285 	 loss = 1.6786(0.9272)
2023/09/15 05:09:43 - INFO - root -   Epoch: [129/500][180/346], lr: 0.00000285 	 loss = 0.6157(0.9162)
2023/09/15 05:10:04 - INFO - root -   Epoch: [129/500][200/346], lr: 0.00000285 	 loss = 0.2408(0.8923)
2023/09/15 05:10:21 - INFO - root -   Epoch: [129/500][220/346], lr: 0.00000285 	 loss = 1.9376(0.8794)
2023/09/15 05:10:42 - INFO - root -   Epoch: [129/500][240/346], lr: 0.00000285 	 loss = 0.3660(0.8802)
2023/09/15 05:10:58 - INFO - root -   Epoch: [129/500][260/346], lr: 0.00000285 	 loss = 1.0762(0.8680)
2023/09/15 05:11:19 - INFO - root -   Epoch: [129/500][280/346], lr: 0.00000285 	 loss = 0.9267(0.8615)
2023/09/15 05:11:36 - INFO - root -   Epoch: [129/500][300/346], lr: 0.00000285 	 loss = 1.3502(0.8608)
2023/09/15 05:11:57 - INFO - root -   Epoch: [129/500][320/346], lr: 0.00000285 	 loss = 0.1945(0.8587)
2023/09/15 05:12:13 - INFO - root -   Epoch: [129/500][340/346], lr: 0.00000285 	 loss = 1.9855(0.8588)
2023/09/15 05:12:15 - INFO - root -   Epoch: [129/500] 	 loss = 0.8556
2023/09/15 05:13:37 - INFO - root -   precision = 0.5345
2023/09/15 05:13:37 - INFO - root -   eval_loss = 1.2244
2023/09/15 05:13:38 - INFO - root -   train_accuracy = 0.6806
2023/09/15 05:13:47 - INFO - root -   Epoch: [130/500][0/346], lr: 0.00000286 	 loss = 1.8004(1.8004)
2023/09/15 05:14:03 - INFO - root -   Epoch: [130/500][20/346], lr: 0.00000286 	 loss = 1.1831(0.8901)
2023/09/15 05:14:24 - INFO - root -   Epoch: [130/500][40/346], lr: 0.00000286 	 loss = 0.9715(0.8715)
2023/09/15 05:14:41 - INFO - root -   Epoch: [130/500][60/346], lr: 0.00000286 	 loss = 0.6732(0.8805)
2023/09/15 05:15:02 - INFO - root -   Epoch: [130/500][80/346], lr: 0.00000286 	 loss = 1.5889(0.9062)
2023/09/15 05:15:18 - INFO - root -   Epoch: [130/500][100/346], lr: 0.00000286 	 loss = 0.1866(0.8981)
2023/09/15 05:15:39 - INFO - root -   Epoch: [130/500][120/346], lr: 0.00000286 	 loss = 0.7695(0.9232)
2023/09/15 05:15:56 - INFO - root -   Epoch: [130/500][140/346], lr: 0.00000286 	 loss = 2.3998(0.9359)
2023/09/15 05:16:17 - INFO - root -   Epoch: [130/500][160/346], lr: 0.00000286 	 loss = 1.5763(0.9347)
2023/09/15 05:16:33 - INFO - root -   Epoch: [130/500][180/346], lr: 0.00000286 	 loss = 0.6569(0.9199)
2023/09/15 05:16:54 - INFO - root -   Epoch: [130/500][200/346], lr: 0.00000286 	 loss = 0.0957(0.8907)
2023/09/15 05:17:11 - INFO - root -   Epoch: [130/500][220/346], lr: 0.00000286 	 loss = 1.5174(0.8816)
2023/09/15 05:17:32 - INFO - root -   Epoch: [130/500][240/346], lr: 0.00000286 	 loss = 0.5429(0.8921)
2023/09/15 05:17:48 - INFO - root -   Epoch: [130/500][260/346], lr: 0.00000286 	 loss = 1.2393(0.8818)
2023/09/15 05:18:09 - INFO - root -   Epoch: [130/500][280/346], lr: 0.00000286 	 loss = 1.0427(0.8770)
2023/09/15 05:18:26 - INFO - root -   Epoch: [130/500][300/346], lr: 0.00000286 	 loss = 1.2747(0.8783)
2023/09/15 05:18:47 - INFO - root -   Epoch: [130/500][320/346], lr: 0.00000286 	 loss = 0.1067(0.8766)
2023/09/15 05:19:04 - INFO - root -   Epoch: [130/500][340/346], lr: 0.00000286 	 loss = 1.3676(0.8765)
2023/09/15 05:19:06 - INFO - root -   Epoch: [130/500] 	 loss = 0.8755
2023/09/15 05:19:06 - INFO - root -   train_accuracy = 0.6777
2023/09/15 05:19:14 - INFO - root -   Epoch: [131/500][0/346], lr: 0.00000287 	 loss = 1.8770(1.8770)
2023/09/15 05:19:31 - INFO - root -   Epoch: [131/500][20/346], lr: 0.00000287 	 loss = 0.9845(0.8172)
2023/09/15 05:19:51 - INFO - root -   Epoch: [131/500][40/346], lr: 0.00000287 	 loss = 1.9943(0.8669)
2023/09/15 05:20:08 - INFO - root -   Epoch: [131/500][60/346], lr: 0.00000287 	 loss = 0.6300(0.8953)
2023/09/15 05:20:29 - INFO - root -   Epoch: [131/500][80/346], lr: 0.00000287 	 loss = 1.4888(0.9027)
2023/09/15 05:20:45 - INFO - root -   Epoch: [131/500][100/346], lr: 0.00000287 	 loss = 0.4476(0.8845)
2023/09/15 05:21:06 - INFO - root -   Epoch: [131/500][120/346], lr: 0.00000287 	 loss = 0.6833(0.9205)
2023/09/15 05:21:22 - INFO - root -   Epoch: [131/500][140/346], lr: 0.00000287 	 loss = 2.6756(0.9241)
2023/09/15 05:21:43 - INFO - root -   Epoch: [131/500][160/346], lr: 0.00000287 	 loss = 1.4753(0.9278)
2023/09/15 05:21:59 - INFO - root -   Epoch: [131/500][180/346], lr: 0.00000287 	 loss = 0.5513(0.9167)
2023/09/15 05:22:20 - INFO - root -   Epoch: [131/500][200/346], lr: 0.00000287 	 loss = 0.0727(0.8898)
2023/09/15 05:22:36 - INFO - root -   Epoch: [131/500][220/346], lr: 0.00000287 	 loss = 1.3995(0.8770)
2023/09/15 05:22:57 - INFO - root -   Epoch: [131/500][240/346], lr: 0.00000287 	 loss = 0.3903(0.8713)
2023/09/15 05:23:13 - INFO - root -   Epoch: [131/500][260/346], lr: 0.00000287 	 loss = 1.5879(0.8619)
2023/09/15 05:23:34 - INFO - root -   Epoch: [131/500][280/346], lr: 0.00000287 	 loss = 1.0401(0.8640)
2023/09/15 05:23:51 - INFO - root -   Epoch: [131/500][300/346], lr: 0.00000287 	 loss = 0.9088(0.8660)
2023/09/15 05:24:12 - INFO - root -   Epoch: [131/500][320/346], lr: 0.00000287 	 loss = 0.1272(0.8655)
2023/09/15 05:24:27 - INFO - root -   Epoch: [131/500][340/346], lr: 0.00000287 	 loss = 1.2919(0.8677)
2023/09/15 05:24:29 - INFO - root -   Epoch: [131/500] 	 loss = 0.8642
2023/09/15 05:24:29 - INFO - root -   train_accuracy = 0.6734
2023/09/15 05:24:37 - INFO - root -   Epoch: [132/500][0/346], lr: 0.00000289 	 loss = 1.9321(1.9321)
2023/09/15 05:24:54 - INFO - root -   Epoch: [132/500][20/346], lr: 0.00000289 	 loss = 1.0069(0.8368)
2023/09/15 05:25:15 - INFO - root -   Epoch: [132/500][40/346], lr: 0.00000289 	 loss = 0.9171(0.8523)
2023/09/15 05:25:31 - INFO - root -   Epoch: [132/500][60/346], lr: 0.00000289 	 loss = 0.5554(0.8629)
2023/09/15 05:25:52 - INFO - root -   Epoch: [132/500][80/346], lr: 0.00000289 	 loss = 2.1630(0.9007)
2023/09/15 05:26:09 - INFO - root -   Epoch: [132/500][100/346], lr: 0.00000289 	 loss = 0.3742(0.8920)
2023/09/15 05:26:30 - INFO - root -   Epoch: [132/500][120/346], lr: 0.00000289 	 loss = 0.6587(0.8980)
2023/09/15 05:26:46 - INFO - root -   Epoch: [132/500][140/346], lr: 0.00000289 	 loss = 1.7676(0.9202)
2023/09/15 05:27:07 - INFO - root -   Epoch: [132/500][160/346], lr: 0.00000289 	 loss = 1.5369(0.9180)
2023/09/15 05:27:24 - INFO - root -   Epoch: [132/500][180/346], lr: 0.00000289 	 loss = 0.5046(0.9091)
2023/09/15 05:27:44 - INFO - root -   Epoch: [132/500][200/346], lr: 0.00000289 	 loss = 0.0594(0.8801)
2023/09/15 05:28:01 - INFO - root -   Epoch: [132/500][220/346], lr: 0.00000289 	 loss = 1.8901(0.8749)
2023/09/15 05:28:22 - INFO - root -   Epoch: [132/500][240/346], lr: 0.00000289 	 loss = 0.3788(0.8674)
2023/09/15 05:28:39 - INFO - root -   Epoch: [132/500][260/346], lr: 0.00000289 	 loss = 1.7701(0.8618)
2023/09/15 05:29:00 - INFO - root -   Epoch: [132/500][280/346], lr: 0.00000289 	 loss = 0.6712(0.8569)
2023/09/15 05:29:16 - INFO - root -   Epoch: [132/500][300/346], lr: 0.00000289 	 loss = 1.5465(0.8537)
2023/09/15 05:29:37 - INFO - root -   Epoch: [132/500][320/346], lr: 0.00000289 	 loss = 0.1213(0.8569)
2023/09/15 05:29:54 - INFO - root -   Epoch: [132/500][340/346], lr: 0.00000289 	 loss = 1.3718(0.8513)
2023/09/15 05:29:56 - INFO - root -   Epoch: [132/500] 	 loss = 0.8479
2023/09/15 05:29:56 - INFO - root -   train_accuracy = 0.6720
2023/09/15 05:30:04 - INFO - root -   Epoch: [133/500][0/346], lr: 0.00000290 	 loss = 1.9029(1.9029)
2023/09/15 05:30:21 - INFO - root -   Epoch: [133/500][20/346], lr: 0.00000290 	 loss = 0.6059(0.8589)
2023/09/15 05:30:42 - INFO - root -   Epoch: [133/500][40/346], lr: 0.00000290 	 loss = 0.8000(0.8085)
2023/09/15 05:30:58 - INFO - root -   Epoch: [133/500][60/346], lr: 0.00000290 	 loss = 0.7061(0.8234)
2023/09/15 05:31:19 - INFO - root -   Epoch: [133/500][80/346], lr: 0.00000290 	 loss = 1.5328(0.8808)
2023/09/15 05:31:36 - INFO - root -   Epoch: [133/500][100/346], lr: 0.00000290 	 loss = 0.1668(0.8755)
2023/09/15 05:31:57 - INFO - root -   Epoch: [133/500][120/346], lr: 0.00000290 	 loss = 0.8456(0.9128)
2023/09/15 05:32:14 - INFO - root -   Epoch: [133/500][140/346], lr: 0.00000290 	 loss = 2.2703(0.9135)
2023/09/15 05:32:35 - INFO - root -   Epoch: [133/500][160/346], lr: 0.00000290 	 loss = 1.4199(0.8964)
2023/09/15 05:32:51 - INFO - root -   Epoch: [133/500][180/346], lr: 0.00000290 	 loss = 1.0455(0.8862)
2023/09/15 05:33:12 - INFO - root -   Epoch: [133/500][200/346], lr: 0.00000290 	 loss = 0.0684(0.8522)
2023/09/15 05:33:29 - INFO - root -   Epoch: [133/500][220/346], lr: 0.00000290 	 loss = 1.9881(0.8432)
2023/09/15 05:33:50 - INFO - root -   Epoch: [133/500][240/346], lr: 0.00000290 	 loss = 0.3438(0.8397)
2023/09/15 05:34:06 - INFO - root -   Epoch: [133/500][260/346], lr: 0.00000290 	 loss = 0.6629(0.8328)
2023/09/15 05:34:27 - INFO - root -   Epoch: [133/500][280/346], lr: 0.00000290 	 loss = 0.4366(0.8308)
2023/09/15 05:34:44 - INFO - root -   Epoch: [133/500][300/346], lr: 0.00000290 	 loss = 2.0290(0.8383)
2023/09/15 05:35:05 - INFO - root -   Epoch: [133/500][320/346], lr: 0.00000290 	 loss = 0.1700(0.8390)
2023/09/15 05:35:21 - INFO - root -   Epoch: [133/500][340/346], lr: 0.00000290 	 loss = 1.1807(0.8377)
2023/09/15 05:35:24 - INFO - root -   Epoch: [133/500] 	 loss = 0.8346
2023/09/15 05:35:24 - INFO - root -   train_accuracy = 0.7023
2023/09/15 05:35:32 - INFO - root -   Epoch: [134/500][0/346], lr: 0.00000292 	 loss = 1.7317(1.7317)
2023/09/15 05:35:48 - INFO - root -   Epoch: [134/500][20/346], lr: 0.00000292 	 loss = 0.8153(0.7737)
2023/09/15 05:36:09 - INFO - root -   Epoch: [134/500][40/346], lr: 0.00000292 	 loss = 0.5040(0.7570)
2023/09/15 05:36:26 - INFO - root -   Epoch: [134/500][60/346], lr: 0.00000292 	 loss = 0.6190(0.7967)
2023/09/15 05:36:47 - INFO - root -   Epoch: [134/500][80/346], lr: 0.00000292 	 loss = 1.8890(0.8338)
2023/09/15 05:37:03 - INFO - root -   Epoch: [134/500][100/346], lr: 0.00000292 	 loss = 0.2454(0.8398)
2023/09/15 05:37:24 - INFO - root -   Epoch: [134/500][120/346], lr: 0.00000292 	 loss = 0.5002(0.8378)
2023/09/15 05:37:41 - INFO - root -   Epoch: [134/500][140/346], lr: 0.00000292 	 loss = 2.3679(0.8691)
2023/09/15 05:38:01 - INFO - root -   Epoch: [134/500][160/346], lr: 0.00000292 	 loss = 1.4602(0.8741)
2023/09/15 05:38:18 - INFO - root -   Epoch: [134/500][180/346], lr: 0.00000292 	 loss = 0.5452(0.8567)
2023/09/15 05:38:39 - INFO - root -   Epoch: [134/500][200/346], lr: 0.00000292 	 loss = 0.1015(0.8315)
2023/09/15 05:38:56 - INFO - root -   Epoch: [134/500][220/346], lr: 0.00000292 	 loss = 1.6801(0.8296)
2023/09/15 05:39:17 - INFO - root -   Epoch: [134/500][240/346], lr: 0.00000292 	 loss = 0.2249(0.8209)
2023/09/15 05:39:33 - INFO - root -   Epoch: [134/500][260/346], lr: 0.00000292 	 loss = 1.2790(0.8142)
2023/09/15 05:39:54 - INFO - root -   Epoch: [134/500][280/346], lr: 0.00000292 	 loss = 0.7054(0.8129)
2023/09/15 05:40:11 - INFO - root -   Epoch: [134/500][300/346], lr: 0.00000292 	 loss = 0.9474(0.8200)
2023/09/15 05:40:32 - INFO - root -   Epoch: [134/500][320/346], lr: 0.00000292 	 loss = 0.1628(0.8210)
2023/09/15 05:40:48 - INFO - root -   Epoch: [134/500][340/346], lr: 0.00000292 	 loss = 1.2458(0.8207)
2023/09/15 05:40:50 - INFO - root -   Epoch: [134/500] 	 loss = 0.8172
2023/09/15 05:42:13 - INFO - root -   precision = 0.5287
2023/09/15 05:42:13 - INFO - root -   eval_loss = 1.2297
2023/09/15 05:42:14 - INFO - root -   train_accuracy = 0.6864
2023/09/15 05:42:22 - INFO - root -   Epoch: [135/500][0/346], lr: 0.00000293 	 loss = 1.9835(1.9835)
2023/09/15 05:42:39 - INFO - root -   Epoch: [135/500][20/346], lr: 0.00000293 	 loss = 1.0962(0.7424)
2023/09/15 05:43:00 - INFO - root -   Epoch: [135/500][40/346], lr: 0.00000293 	 loss = 0.4126(0.7377)
2023/09/15 05:43:16 - INFO - root -   Epoch: [135/500][60/346], lr: 0.00000293 	 loss = 0.5689(0.7472)
2023/09/15 05:43:37 - INFO - root -   Epoch: [135/500][80/346], lr: 0.00000293 	 loss = 1.6789(0.7931)
2023/09/15 05:43:54 - INFO - root -   Epoch: [135/500][100/346], lr: 0.00000293 	 loss = 0.4175(0.8076)
2023/09/15 05:44:15 - INFO - root -   Epoch: [135/500][120/346], lr: 0.00000293 	 loss = 0.6860(0.8328)
2023/09/15 05:44:32 - INFO - root -   Epoch: [135/500][140/346], lr: 0.00000293 	 loss = 2.2848(0.8333)
2023/09/15 05:44:52 - INFO - root -   Epoch: [135/500][160/346], lr: 0.00000293 	 loss = 1.7380(0.8320)
2023/09/15 05:45:08 - INFO - root -   Epoch: [135/500][180/346], lr: 0.00000293 	 loss = 0.4468(0.8194)
2023/09/15 05:45:29 - INFO - root -   Epoch: [135/500][200/346], lr: 0.00000293 	 loss = 0.1083(0.7968)
2023/09/15 05:45:45 - INFO - root -   Epoch: [135/500][220/346], lr: 0.00000293 	 loss = 1.6273(0.7842)
2023/09/15 05:46:06 - INFO - root -   Epoch: [135/500][240/346], lr: 0.00000293 	 loss = 0.3392(0.7762)
2023/09/15 05:46:22 - INFO - root -   Epoch: [135/500][260/346], lr: 0.00000293 	 loss = 1.6349(0.7755)
2023/09/15 05:46:43 - INFO - root -   Epoch: [135/500][280/346], lr: 0.00000293 	 loss = 0.7530(0.7807)
2023/09/15 05:47:00 - INFO - root -   Epoch: [135/500][300/346], lr: 0.00000293 	 loss = 1.2823(0.7797)
2023/09/15 05:47:20 - INFO - root -   Epoch: [135/500][320/346], lr: 0.00000293 	 loss = 0.2367(0.7841)
2023/09/15 05:47:35 - INFO - root -   Epoch: [135/500][340/346], lr: 0.00000293 	 loss = 1.5567(0.7818)
2023/09/15 05:47:37 - INFO - root -   Epoch: [135/500] 	 loss = 0.7802
2023/09/15 05:47:37 - INFO - root -   train_accuracy = 0.7066
2023/09/15 05:47:45 - INFO - root -   Epoch: [136/500][0/346], lr: 0.00000295 	 loss = 1.5725(1.5725)
2023/09/15 05:48:02 - INFO - root -   Epoch: [136/500][20/346], lr: 0.00000295 	 loss = 0.7095(0.8225)
2023/09/15 05:48:23 - INFO - root -   Epoch: [136/500][40/346], lr: 0.00000295 	 loss = 0.9884(0.8119)
2023/09/15 05:48:39 - INFO - root -   Epoch: [136/500][60/346], lr: 0.00000295 	 loss = 0.7078(0.8598)
2023/09/15 05:49:00 - INFO - root -   Epoch: [136/500][80/346], lr: 0.00000295 	 loss = 2.3759(0.8906)
2023/09/15 05:49:17 - INFO - root -   Epoch: [136/500][100/346], lr: 0.00000295 	 loss = 0.2118(0.8901)
2023/09/15 05:49:37 - INFO - root -   Epoch: [136/500][120/346], lr: 0.00000295 	 loss = 0.5724(0.8967)
2023/09/15 05:49:54 - INFO - root -   Epoch: [136/500][140/346], lr: 0.00000295 	 loss = 2.4574(0.8999)
2023/09/15 05:50:14 - INFO - root -   Epoch: [136/500][160/346], lr: 0.00000295 	 loss = 1.8486(0.8886)
2023/09/15 05:50:31 - INFO - root -   Epoch: [136/500][180/346], lr: 0.00000295 	 loss = 0.3983(0.8679)
2023/09/15 05:50:51 - INFO - root -   Epoch: [136/500][200/346], lr: 0.00000295 	 loss = 0.0499(0.8395)
2023/09/15 05:51:09 - INFO - root -   Epoch: [136/500][220/346], lr: 0.00000295 	 loss = 1.5854(0.8420)
2023/09/15 05:51:27 - INFO - root -   Epoch: [136/500][240/346], lr: 0.00000295 	 loss = 0.3434(0.8496)
2023/09/15 05:51:46 - INFO - root -   Epoch: [136/500][260/346], lr: 0.00000295 	 loss = 1.3092(0.8389)
2023/09/15 05:52:04 - INFO - root -   Epoch: [136/500][280/346], lr: 0.00000295 	 loss = 0.5649(0.8337)
2023/09/15 05:52:24 - INFO - root -   Epoch: [136/500][300/346], lr: 0.00000295 	 loss = 1.1824(0.8312)
2023/09/15 05:52:41 - INFO - root -   Epoch: [136/500][320/346], lr: 0.00000295 	 loss = 0.1506(0.8276)
2023/09/15 05:53:01 - INFO - root -   Epoch: [136/500][340/346], lr: 0.00000295 	 loss = 1.3883(0.8221)
2023/09/15 05:53:03 - INFO - root -   Epoch: [136/500] 	 loss = 0.8183
2023/09/15 05:53:03 - INFO - root -   train_accuracy = 0.6908
2023/09/15 05:53:11 - INFO - root -   Epoch: [137/500][0/346], lr: 0.00000296 	 loss = 1.5598(1.5598)
2023/09/15 05:53:27 - INFO - root -   Epoch: [137/500][20/346], lr: 0.00000296 	 loss = 0.9420(0.7600)
2023/09/15 05:53:48 - INFO - root -   Epoch: [137/500][40/346], lr: 0.00000296 	 loss = 0.7320(0.7353)
2023/09/15 05:54:05 - INFO - root -   Epoch: [137/500][60/346], lr: 0.00000296 	 loss = 0.3979(0.7530)
2023/09/15 05:54:26 - INFO - root -   Epoch: [137/500][80/346], lr: 0.00000296 	 loss = 1.8730(0.7793)
2023/09/15 05:54:43 - INFO - root -   Epoch: [137/500][100/346], lr: 0.00000296 	 loss = 0.1359(0.7632)
2023/09/15 05:55:04 - INFO - root -   Epoch: [137/500][120/346], lr: 0.00000296 	 loss = 0.4837(0.7832)
2023/09/15 05:55:20 - INFO - root -   Epoch: [137/500][140/346], lr: 0.00000296 	 loss = 2.2154(0.8090)
2023/09/15 05:55:41 - INFO - root -   Epoch: [137/500][160/346], lr: 0.00000296 	 loss = 1.3900(0.7993)
2023/09/15 05:55:58 - INFO - root -   Epoch: [137/500][180/346], lr: 0.00000296 	 loss = 0.5324(0.7931)
2023/09/15 05:56:18 - INFO - root -   Epoch: [137/500][200/346], lr: 0.00000296 	 loss = 0.0739(0.7698)
2023/09/15 05:56:35 - INFO - root -   Epoch: [137/500][220/346], lr: 0.00000296 	 loss = 1.9525(0.7806)
2023/09/15 05:56:56 - INFO - root -   Epoch: [137/500][240/346], lr: 0.00000296 	 loss = 0.2029(0.7748)
2023/09/15 05:57:12 - INFO - root -   Epoch: [137/500][260/346], lr: 0.00000296 	 loss = 1.0895(0.7692)
2023/09/15 05:57:33 - INFO - root -   Epoch: [137/500][280/346], lr: 0.00000296 	 loss = 0.6022(0.7616)
2023/09/15 05:57:49 - INFO - root -   Epoch: [137/500][300/346], lr: 0.00000296 	 loss = 1.0188(0.7703)
2023/09/15 05:58:10 - INFO - root -   Epoch: [137/500][320/346], lr: 0.00000296 	 loss = 0.1378(0.7772)
2023/09/15 05:58:26 - INFO - root -   Epoch: [137/500][340/346], lr: 0.00000296 	 loss = 1.1730(0.7706)
2023/09/15 05:58:28 - INFO - root -   Epoch: [137/500] 	 loss = 0.7690
2023/09/15 05:58:28 - INFO - root -   train_accuracy = 0.7153
2023/09/15 05:58:36 - INFO - root -   Epoch: [138/500][0/346], lr: 0.00000297 	 loss = 1.9135(1.9135)
2023/09/15 05:58:53 - INFO - root -   Epoch: [138/500][20/346], lr: 0.00000297 	 loss = 0.8855(0.8703)
2023/09/15 05:59:13 - INFO - root -   Epoch: [138/500][40/346], lr: 0.00000297 	 loss = 1.2471(0.8845)
2023/09/15 05:59:30 - INFO - root -   Epoch: [138/500][60/346], lr: 0.00000297 	 loss = 0.3115(0.8426)
2023/09/15 05:59:51 - INFO - root -   Epoch: [138/500][80/346], lr: 0.00000297 	 loss = 2.0290(0.8632)
2023/09/15 06:00:08 - INFO - root -   Epoch: [138/500][100/346], lr: 0.00000297 	 loss = 0.2241(0.8422)
2023/09/15 06:00:29 - INFO - root -   Epoch: [138/500][120/346], lr: 0.00000297 	 loss = 0.5828(0.8519)
2023/09/15 06:00:45 - INFO - root -   Epoch: [138/500][140/346], lr: 0.00000297 	 loss = 2.0725(0.8543)
2023/09/15 06:01:06 - INFO - root -   Epoch: [138/500][160/346], lr: 0.00000297 	 loss = 1.4326(0.8374)
2023/09/15 06:01:23 - INFO - root -   Epoch: [138/500][180/346], lr: 0.00000297 	 loss = 0.2430(0.8224)
2023/09/15 06:01:43 - INFO - root -   Epoch: [138/500][200/346], lr: 0.00000297 	 loss = 0.0762(0.7944)
2023/09/15 06:02:00 - INFO - root -   Epoch: [138/500][220/346], lr: 0.00000297 	 loss = 1.4582(0.7860)
2023/09/15 06:02:21 - INFO - root -   Epoch: [138/500][240/346], lr: 0.00000297 	 loss = 0.3073(0.7764)
2023/09/15 06:02:38 - INFO - root -   Epoch: [138/500][260/346], lr: 0.00000297 	 loss = 1.6212(0.7687)
2023/09/15 06:02:59 - INFO - root -   Epoch: [138/500][280/346], lr: 0.00000297 	 loss = 0.9788(0.7630)
2023/09/15 06:03:15 - INFO - root -   Epoch: [138/500][300/346], lr: 0.00000297 	 loss = 1.1379(0.7581)
2023/09/15 06:03:36 - INFO - root -   Epoch: [138/500][320/346], lr: 0.00000297 	 loss = 0.2501(0.7621)
2023/09/15 06:03:52 - INFO - root -   Epoch: [138/500][340/346], lr: 0.00000297 	 loss = 1.0200(0.7605)
2023/09/15 06:03:54 - INFO - root -   Epoch: [138/500] 	 loss = 0.7585
2023/09/15 06:03:54 - INFO - root -   train_accuracy = 0.7095
2023/09/15 06:04:03 - INFO - root -   Epoch: [139/500][0/346], lr: 0.00000299 	 loss = 1.7013(1.7013)
2023/09/15 06:04:19 - INFO - root -   Epoch: [139/500][20/346], lr: 0.00000299 	 loss = 0.6237(0.7686)
2023/09/15 06:04:41 - INFO - root -   Epoch: [139/500][40/346], lr: 0.00000299 	 loss = 1.0370(0.7739)
2023/09/15 06:04:57 - INFO - root -   Epoch: [139/500][60/346], lr: 0.00000299 	 loss = 0.3903(0.7689)
2023/09/15 06:05:18 - INFO - root -   Epoch: [139/500][80/346], lr: 0.00000299 	 loss = 1.6206(0.8024)
2023/09/15 06:05:35 - INFO - root -   Epoch: [139/500][100/346], lr: 0.00000299 	 loss = 0.1775(0.7936)
2023/09/15 06:05:56 - INFO - root -   Epoch: [139/500][120/346], lr: 0.00000299 	 loss = 0.6929(0.8106)
2023/09/15 06:06:13 - INFO - root -   Epoch: [139/500][140/346], lr: 0.00000299 	 loss = 3.0737(0.8299)
2023/09/15 06:06:34 - INFO - root -   Epoch: [139/500][160/346], lr: 0.00000299 	 loss = 2.2987(0.8295)
2023/09/15 06:06:51 - INFO - root -   Epoch: [139/500][180/346], lr: 0.00000299 	 loss = 0.3433(0.8199)
2023/09/15 06:07:12 - INFO - root -   Epoch: [139/500][200/346], lr: 0.00000299 	 loss = 0.3313(0.7974)
2023/09/15 06:07:29 - INFO - root -   Epoch: [139/500][220/346], lr: 0.00000299 	 loss = 1.9067(0.7893)
2023/09/15 06:07:50 - INFO - root -   Epoch: [139/500][240/346], lr: 0.00000299 	 loss = 0.3650(0.7793)
2023/09/15 06:08:06 - INFO - root -   Epoch: [139/500][260/346], lr: 0.00000299 	 loss = 1.1241(0.7693)
2023/09/15 06:08:28 - INFO - root -   Epoch: [139/500][280/346], lr: 0.00000299 	 loss = 0.5945(0.7661)
2023/09/15 06:08:44 - INFO - root -   Epoch: [139/500][300/346], lr: 0.00000299 	 loss = 1.6046(0.7603)
2023/09/15 06:09:05 - INFO - root -   Epoch: [139/500][320/346], lr: 0.00000299 	 loss = 0.2017(0.7631)
2023/09/15 06:09:22 - INFO - root -   Epoch: [139/500][340/346], lr: 0.00000299 	 loss = 1.0842(0.7591)
2023/09/15 06:09:24 - INFO - root -   Epoch: [139/500] 	 loss = 0.7553
2023/09/15 06:10:47 - INFO - root -   precision = 0.5345
2023/09/15 06:10:47 - INFO - root -   eval_loss = 1.3707
2023/09/15 06:10:48 - INFO - root -   train_accuracy = 0.7066
2023/09/15 06:10:56 - INFO - root -   Epoch: [140/500][0/346], lr: 0.00000300 	 loss = 1.2507(1.2507)
2023/09/15 06:11:13 - INFO - root -   Epoch: [140/500][20/346], lr: 0.00000300 	 loss = 0.8734(0.7156)
2023/09/15 06:11:33 - INFO - root -   Epoch: [140/500][40/346], lr: 0.00000300 	 loss = 0.2865(0.7864)
2023/09/15 06:11:50 - INFO - root -   Epoch: [140/500][60/346], lr: 0.00000300 	 loss = 1.0290(0.7979)
2023/09/15 06:12:11 - INFO - root -   Epoch: [140/500][80/346], lr: 0.00000300 	 loss = 1.5438(0.8184)
2023/09/15 06:12:27 - INFO - root -   Epoch: [140/500][100/346], lr: 0.00000300 	 loss = 0.1126(0.7908)
2023/09/15 06:12:48 - INFO - root -   Epoch: [140/500][120/346], lr: 0.00000300 	 loss = 0.6334(0.7986)
2023/09/15 06:13:04 - INFO - root -   Epoch: [140/500][140/346], lr: 0.00000300 	 loss = 2.0559(0.8208)
2023/09/15 06:13:25 - INFO - root -   Epoch: [140/500][160/346], lr: 0.00000300 	 loss = 1.9257(0.8247)
2023/09/15 06:13:42 - INFO - root -   Epoch: [140/500][180/346], lr: 0.00000300 	 loss = 0.3988(0.8150)
2023/09/15 06:14:03 - INFO - root -   Epoch: [140/500][200/346], lr: 0.00000300 	 loss = 0.0479(0.7838)
2023/09/15 06:14:19 - INFO - root -   Epoch: [140/500][220/346], lr: 0.00000300 	 loss = 1.5002(0.7695)
2023/09/15 06:14:40 - INFO - root -   Epoch: [140/500][240/346], lr: 0.00000300 	 loss = 0.3746(0.7559)
2023/09/15 06:14:57 - INFO - root -   Epoch: [140/500][260/346], lr: 0.00000300 	 loss = 1.2852(0.7479)
2023/09/15 06:15:18 - INFO - root -   Epoch: [140/500][280/346], lr: 0.00000300 	 loss = 0.5238(0.7382)
2023/09/15 06:15:35 - INFO - root -   Epoch: [140/500][300/346], lr: 0.00000300 	 loss = 1.1785(0.7413)
2023/09/15 06:15:56 - INFO - root -   Epoch: [140/500][320/346], lr: 0.00000300 	 loss = 0.0858(0.7448)
2023/09/15 06:16:12 - INFO - root -   Epoch: [140/500][340/346], lr: 0.00000300 	 loss = 1.1113(0.7411)
2023/09/15 06:16:14 - INFO - root -   Epoch: [140/500] 	 loss = 0.7377
2023/09/15 06:16:14 - INFO - root -   train_accuracy = 0.7095
2023/09/15 06:16:23 - INFO - root -   Epoch: [141/500][0/346], lr: 0.00000302 	 loss = 1.4807(1.4807)
2023/09/15 06:16:39 - INFO - root -   Epoch: [141/500][20/346], lr: 0.00000302 	 loss = 0.6693(0.6989)
2023/09/15 06:17:00 - INFO - root -   Epoch: [141/500][40/346], lr: 0.00000302 	 loss = 0.4729(0.7117)
2023/09/15 06:17:17 - INFO - root -   Epoch: [141/500][60/346], lr: 0.00000302 	 loss = 0.6611(0.7108)
2023/09/15 06:17:38 - INFO - root -   Epoch: [141/500][80/346], lr: 0.00000302 	 loss = 2.1660(0.7396)
2023/09/15 06:17:54 - INFO - root -   Epoch: [141/500][100/346], lr: 0.00000302 	 loss = 0.0911(0.7406)
2023/09/15 06:18:15 - INFO - root -   Epoch: [141/500][120/346], lr: 0.00000302 	 loss = 0.5604(0.7755)
2023/09/15 06:18:32 - INFO - root -   Epoch: [141/500][140/346], lr: 0.00000302 	 loss = 2.6185(0.7883)
2023/09/15 06:18:52 - INFO - root -   Epoch: [141/500][160/346], lr: 0.00000302 	 loss = 1.8925(0.7814)
2023/09/15 06:19:09 - INFO - root -   Epoch: [141/500][180/346], lr: 0.00000302 	 loss = 0.2359(0.7742)
2023/09/15 06:19:30 - INFO - root -   Epoch: [141/500][200/346], lr: 0.00000302 	 loss = 0.0589(0.7439)
2023/09/15 06:19:46 - INFO - root -   Epoch: [141/500][220/346], lr: 0.00000302 	 loss = 1.5363(0.7430)
2023/09/15 06:20:07 - INFO - root -   Epoch: [141/500][240/346], lr: 0.00000302 	 loss = 0.2467(0.7395)
2023/09/15 06:20:23 - INFO - root -   Epoch: [141/500][260/346], lr: 0.00000302 	 loss = 1.3903(0.7316)
2023/09/15 06:20:44 - INFO - root -   Epoch: [141/500][280/346], lr: 0.00000302 	 loss = 0.6131(0.7227)
2023/09/15 06:21:01 - INFO - root -   Epoch: [141/500][300/346], lr: 0.00000302 	 loss = 1.1847(0.7206)
2023/09/15 06:21:21 - INFO - root -   Epoch: [141/500][320/346], lr: 0.00000302 	 loss = 0.0900(0.7213)
2023/09/15 06:21:37 - INFO - root -   Epoch: [141/500][340/346], lr: 0.00000302 	 loss = 1.3531(0.7238)
2023/09/15 06:21:39 - INFO - root -   Epoch: [141/500] 	 loss = 0.7209
2023/09/15 06:21:39 - INFO - root -   train_accuracy = 0.7283
2023/09/15 06:21:47 - INFO - root -   Epoch: [142/500][0/346], lr: 0.00000303 	 loss = 1.7982(1.7982)
2023/09/15 06:22:04 - INFO - root -   Epoch: [142/500][20/346], lr: 0.00000303 	 loss = 0.7192(0.6771)
2023/09/15 06:22:25 - INFO - root -   Epoch: [142/500][40/346], lr: 0.00000303 	 loss = 0.6322(0.6799)
2023/09/15 06:22:42 - INFO - root -   Epoch: [142/500][60/346], lr: 0.00000303 	 loss = 0.5135(0.6985)
2023/09/15 06:23:03 - INFO - root -   Epoch: [142/500][80/346], lr: 0.00000303 	 loss = 3.0138(0.7301)
2023/09/15 06:23:19 - INFO - root -   Epoch: [142/500][100/346], lr: 0.00000303 	 loss = 0.0988(0.7535)
2023/09/15 06:23:40 - INFO - root -   Epoch: [142/500][120/346], lr: 0.00000303 	 loss = 0.3651(0.7610)
2023/09/15 06:23:57 - INFO - root -   Epoch: [142/500][140/346], lr: 0.00000303 	 loss = 2.5930(0.7869)
2023/09/15 06:24:18 - INFO - root -   Epoch: [142/500][160/346], lr: 0.00000303 	 loss = 1.8048(0.7963)
2023/09/15 06:24:34 - INFO - root -   Epoch: [142/500][180/346], lr: 0.00000303 	 loss = 0.4239(0.7826)
2023/09/15 06:24:55 - INFO - root -   Epoch: [142/500][200/346], lr: 0.00000303 	 loss = 0.0819(0.7552)
2023/09/15 06:25:12 - INFO - root -   Epoch: [142/500][220/346], lr: 0.00000303 	 loss = 1.6213(0.7550)
2023/09/15 06:25:33 - INFO - root -   Epoch: [142/500][240/346], lr: 0.00000303 	 loss = 0.8271(0.7390)
2023/09/15 06:25:49 - INFO - root -   Epoch: [142/500][260/346], lr: 0.00000303 	 loss = 1.6705(0.7292)
2023/09/15 06:26:10 - INFO - root -   Epoch: [142/500][280/346], lr: 0.00000303 	 loss = 0.5403(0.7185)
2023/09/15 06:26:27 - INFO - root -   Epoch: [142/500][300/346], lr: 0.00000303 	 loss = 0.9727(0.7159)
2023/09/15 06:26:48 - INFO - root -   Epoch: [142/500][320/346], lr: 0.00000303 	 loss = 0.3308(0.7154)
2023/09/15 06:27:05 - INFO - root -   Epoch: [142/500][340/346], lr: 0.00000303 	 loss = 1.3285(0.7174)
2023/09/15 06:27:07 - INFO - root -   Epoch: [142/500] 	 loss = 0.7140
2023/09/15 06:27:07 - INFO - root -   train_accuracy = 0.7240
2023/09/15 06:27:15 - INFO - root -   Epoch: [143/500][0/346], lr: 0.00000305 	 loss = 1.2831(1.2831)
2023/09/15 06:27:31 - INFO - root -   Epoch: [143/500][20/346], lr: 0.00000305 	 loss = 0.5571(0.7453)
2023/09/15 06:27:53 - INFO - root -   Epoch: [143/500][40/346], lr: 0.00000305 	 loss = 0.3783(0.7454)
2023/09/15 06:28:09 - INFO - root -   Epoch: [143/500][60/346], lr: 0.00000305 	 loss = 0.4338(0.7219)
2023/09/15 06:28:30 - INFO - root -   Epoch: [143/500][80/346], lr: 0.00000305 	 loss = 1.3976(0.7275)
2023/09/15 06:28:47 - INFO - root -   Epoch: [143/500][100/346], lr: 0.00000305 	 loss = 0.0279(0.7332)
2023/09/15 06:29:08 - INFO - root -   Epoch: [143/500][120/346], lr: 0.00000305 	 loss = 0.5034(0.7687)
2023/09/15 06:29:24 - INFO - root -   Epoch: [143/500][140/346], lr: 0.00000305 	 loss = 2.1063(0.7770)
2023/09/15 06:29:45 - INFO - root -   Epoch: [143/500][160/346], lr: 0.00000305 	 loss = 1.4832(0.7707)
2023/09/15 06:30:02 - INFO - root -   Epoch: [143/500][180/346], lr: 0.00000305 	 loss = 0.5037(0.7641)
2023/09/15 06:30:23 - INFO - root -   Epoch: [143/500][200/346], lr: 0.00000305 	 loss = 0.0335(0.7322)
2023/09/15 06:30:40 - INFO - root -   Epoch: [143/500][220/346], lr: 0.00000305 	 loss = 1.9095(0.7341)
2023/09/15 06:31:01 - INFO - root -   Epoch: [143/500][240/346], lr: 0.00000305 	 loss = 0.3987(0.7305)
2023/09/15 06:31:17 - INFO - root -   Epoch: [143/500][260/346], lr: 0.00000305 	 loss = 1.8654(0.7236)
2023/09/15 06:31:38 - INFO - root -   Epoch: [143/500][280/346], lr: 0.00000305 	 loss = 0.3548(0.7167)
2023/09/15 06:31:55 - INFO - root -   Epoch: [143/500][300/346], lr: 0.00000305 	 loss = 1.0826(0.7205)
2023/09/15 06:32:15 - INFO - root -   Epoch: [143/500][320/346], lr: 0.00000305 	 loss = 0.3059(0.7207)
2023/09/15 06:32:32 - INFO - root -   Epoch: [143/500][340/346], lr: 0.00000305 	 loss = 0.8502(0.7143)
2023/09/15 06:32:34 - INFO - root -   Epoch: [143/500] 	 loss = 0.7101
2023/09/15 06:32:34 - INFO - root -   train_accuracy = 0.7283
2023/09/15 06:32:42 - INFO - root -   Epoch: [144/500][0/346], lr: 0.00000306 	 loss = 1.0570(1.0570)
2023/09/15 06:32:59 - INFO - root -   Epoch: [144/500][20/346], lr: 0.00000306 	 loss = 0.9826(0.6547)
2023/09/15 06:33:20 - INFO - root -   Epoch: [144/500][40/346], lr: 0.00000306 	 loss = 0.2176(0.7004)
2023/09/15 06:33:37 - INFO - root -   Epoch: [144/500][60/346], lr: 0.00000306 	 loss = 0.1230(0.6961)
2023/09/15 06:33:58 - INFO - root -   Epoch: [144/500][80/346], lr: 0.00000306 	 loss = 1.4835(0.6955)
2023/09/15 06:34:15 - INFO - root -   Epoch: [144/500][100/346], lr: 0.00000306 	 loss = 0.1168(0.6926)
2023/09/15 06:34:36 - INFO - root -   Epoch: [144/500][120/346], lr: 0.00000306 	 loss = 0.3388(0.7116)
2023/09/15 06:34:52 - INFO - root -   Epoch: [144/500][140/346], lr: 0.00000306 	 loss = 3.0700(0.7382)
2023/09/15 06:35:13 - INFO - root -   Epoch: [144/500][160/346], lr: 0.00000306 	 loss = 2.1064(0.7620)
2023/09/15 06:35:30 - INFO - root -   Epoch: [144/500][180/346], lr: 0.00000306 	 loss = 0.3351(0.7521)
2023/09/15 06:35:51 - INFO - root -   Epoch: [144/500][200/346], lr: 0.00000306 	 loss = 0.1077(0.7244)
2023/09/15 06:36:08 - INFO - root -   Epoch: [144/500][220/346], lr: 0.00000306 	 loss = 1.4012(0.7173)
2023/09/15 06:36:29 - INFO - root -   Epoch: [144/500][240/346], lr: 0.00000306 	 loss = 0.3188(0.7037)
2023/09/15 06:36:45 - INFO - root -   Epoch: [144/500][260/346], lr: 0.00000306 	 loss = 1.6419(0.6937)
2023/09/15 06:37:06 - INFO - root -   Epoch: [144/500][280/346], lr: 0.00000306 	 loss = 0.4891(0.6863)
2023/09/15 06:37:23 - INFO - root -   Epoch: [144/500][300/346], lr: 0.00000306 	 loss = 0.6091(0.6838)
2023/09/15 06:37:44 - INFO - root -   Epoch: [144/500][320/346], lr: 0.00000306 	 loss = 0.1887(0.6899)
2023/09/15 06:38:01 - INFO - root -   Epoch: [144/500][340/346], lr: 0.00000306 	 loss = 1.1101(0.6894)
2023/09/15 06:38:03 - INFO - root -   Epoch: [144/500] 	 loss = 0.6873
2023/09/15 06:39:25 - INFO - root -   precision = 0.5000
2023/09/15 06:39:25 - INFO - root -   eval_loss = 1.4666
2023/09/15 06:39:27 - INFO - root -   train_accuracy = 0.7298
2023/09/15 06:39:35 - INFO - root -   Epoch: [145/500][0/346], lr: 0.00000307 	 loss = 1.0061(1.0061)
2023/09/15 06:39:51 - INFO - root -   Epoch: [145/500][20/346], lr: 0.00000307 	 loss = 0.7937(0.7074)
2023/09/15 06:40:12 - INFO - root -   Epoch: [145/500][40/346], lr: 0.00000307 	 loss = 0.3952(0.7396)
2023/09/15 06:40:29 - INFO - root -   Epoch: [145/500][60/346], lr: 0.00000307 	 loss = 0.4022(0.7517)
2023/09/15 06:40:49 - INFO - root -   Epoch: [145/500][80/346], lr: 0.00000307 	 loss = 1.2177(0.7416)
2023/09/15 06:41:06 - INFO - root -   Epoch: [145/500][100/346], lr: 0.00000307 	 loss = 0.2888(0.7568)
2023/09/15 06:41:27 - INFO - root -   Epoch: [145/500][120/346], lr: 0.00000307 	 loss = 0.6810(0.7648)
2023/09/15 06:41:43 - INFO - root -   Epoch: [145/500][140/346], lr: 0.00000307 	 loss = 1.8717(0.7970)
2023/09/15 06:42:03 - INFO - root -   Epoch: [145/500][160/346], lr: 0.00000307 	 loss = 1.6789(0.7889)
2023/09/15 06:42:20 - INFO - root -   Epoch: [145/500][180/346], lr: 0.00000307 	 loss = 0.4729(0.7849)
2023/09/15 06:42:40 - INFO - root -   Epoch: [145/500][200/346], lr: 0.00000307 	 loss = 0.0945(0.7454)
2023/09/15 06:42:57 - INFO - root -   Epoch: [145/500][220/346], lr: 0.00000307 	 loss = 1.6964(0.7352)
2023/09/15 06:43:17 - INFO - root -   Epoch: [145/500][240/346], lr: 0.00000307 	 loss = 0.2999(0.7190)
2023/09/15 06:43:34 - INFO - root -   Epoch: [145/500][260/346], lr: 0.00000307 	 loss = 1.7683(0.7182)
2023/09/15 06:43:53 - INFO - root -   Epoch: [145/500][280/346], lr: 0.00000307 	 loss = 0.4403(0.7207)
2023/09/15 06:44:11 - INFO - root -   Epoch: [145/500][300/346], lr: 0.00000307 	 loss = 0.6224(0.7169)
2023/09/15 06:44:29 - INFO - root -   Epoch: [145/500][320/346], lr: 0.00000307 	 loss = 0.0998(0.7151)
2023/09/15 06:44:47 - INFO - root -   Epoch: [145/500][340/346], lr: 0.00000307 	 loss = 1.1706(0.7109)
2023/09/15 06:44:49 - INFO - root -   Epoch: [145/500] 	 loss = 0.7074
2023/09/15 06:44:49 - INFO - root -   train_accuracy = 0.7312
2023/09/15 06:44:58 - INFO - root -   Epoch: [146/500][0/346], lr: 0.00000309 	 loss = 1.4850(1.4850)
2023/09/15 06:45:14 - INFO - root -   Epoch: [146/500][20/346], lr: 0.00000309 	 loss = 0.8251(0.7131)
2023/09/15 06:45:35 - INFO - root -   Epoch: [146/500][40/346], lr: 0.00000309 	 loss = 0.8482(0.7212)
2023/09/15 06:45:52 - INFO - root -   Epoch: [146/500][60/346], lr: 0.00000309 	 loss = 0.1592(0.7357)
2023/09/15 06:46:13 - INFO - root -   Epoch: [146/500][80/346], lr: 0.00000309 	 loss = 2.1751(0.7489)
2023/09/15 06:46:30 - INFO - root -   Epoch: [146/500][100/346], lr: 0.00000309 	 loss = 0.1053(0.7506)
2023/09/15 06:46:51 - INFO - root -   Epoch: [146/500][120/346], lr: 0.00000309 	 loss = 0.4716(0.7591)
2023/09/15 06:47:07 - INFO - root -   Epoch: [146/500][140/346], lr: 0.00000309 	 loss = 3.1076(0.7777)
2023/09/15 06:47:28 - INFO - root -   Epoch: [146/500][160/346], lr: 0.00000309 	 loss = 3.9659(0.7854)
2023/09/15 06:47:45 - INFO - root -   Epoch: [146/500][180/346], lr: 0.00000309 	 loss = 0.3926(0.7742)
2023/09/15 06:48:06 - INFO - root -   Epoch: [146/500][200/346], lr: 0.00000309 	 loss = 0.0445(0.7382)
2023/09/15 06:48:23 - INFO - root -   Epoch: [146/500][220/346], lr: 0.00000309 	 loss = 1.5557(0.7295)
2023/09/15 06:48:44 - INFO - root -   Epoch: [146/500][240/346], lr: 0.00000309 	 loss = 0.4828(0.7262)
2023/09/15 06:49:01 - INFO - root -   Epoch: [146/500][260/346], lr: 0.00000309 	 loss = 1.5687(0.7201)
2023/09/15 06:49:21 - INFO - root -   Epoch: [146/500][280/346], lr: 0.00000309 	 loss = 0.4028(0.7183)
2023/09/15 06:49:38 - INFO - root -   Epoch: [146/500][300/346], lr: 0.00000309 	 loss = 0.9878(0.7251)
2023/09/15 06:49:59 - INFO - root -   Epoch: [146/500][320/346], lr: 0.00000309 	 loss = 0.0913(0.7245)
2023/09/15 06:50:15 - INFO - root -   Epoch: [146/500][340/346], lr: 0.00000309 	 loss = 1.0965(0.7217)
2023/09/15 06:50:17 - INFO - root -   Epoch: [146/500] 	 loss = 0.7189
2023/09/15 06:50:17 - INFO - root -   train_accuracy = 0.7197
2023/09/15 06:50:26 - INFO - root -   Epoch: [147/500][0/346], lr: 0.00000310 	 loss = 0.9161(0.9161)
2023/09/15 06:50:42 - INFO - root -   Epoch: [147/500][20/346], lr: 0.00000310 	 loss = 0.7520(0.6243)
2023/09/15 06:51:03 - INFO - root -   Epoch: [147/500][40/346], lr: 0.00000310 	 loss = 0.3303(0.6580)
2023/09/15 06:51:20 - INFO - root -   Epoch: [147/500][60/346], lr: 0.00000310 	 loss = 1.5367(0.6973)
2023/09/15 06:51:40 - INFO - root -   Epoch: [147/500][80/346], lr: 0.00000310 	 loss = 1.4165(0.7313)
2023/09/15 06:51:57 - INFO - root -   Epoch: [147/500][100/346], lr: 0.00000310 	 loss = 0.2634(0.7218)
2023/09/15 06:52:18 - INFO - root -   Epoch: [147/500][120/346], lr: 0.00000310 	 loss = 0.4310(0.7372)
2023/09/15 06:52:34 - INFO - root -   Epoch: [147/500][140/346], lr: 0.00000310 	 loss = 1.5945(0.7348)
2023/09/15 06:52:55 - INFO - root -   Epoch: [147/500][160/346], lr: 0.00000310 	 loss = 2.0085(0.7266)
2023/09/15 06:53:12 - INFO - root -   Epoch: [147/500][180/346], lr: 0.00000310 	 loss = 0.3126(0.7097)
2023/09/15 06:53:33 - INFO - root -   Epoch: [147/500][200/346], lr: 0.00000310 	 loss = 0.0075(0.6852)
2023/09/15 06:53:49 - INFO - root -   Epoch: [147/500][220/346], lr: 0.00000310 	 loss = 1.2578(0.6785)
2023/09/15 06:54:10 - INFO - root -   Epoch: [147/500][240/346], lr: 0.00000310 	 loss = 0.2924(0.6660)
2023/09/15 06:54:27 - INFO - root -   Epoch: [147/500][260/346], lr: 0.00000310 	 loss = 1.5273(0.6651)
2023/09/15 06:54:48 - INFO - root -   Epoch: [147/500][280/346], lr: 0.00000310 	 loss = 1.0166(0.6613)
2023/09/15 06:55:04 - INFO - root -   Epoch: [147/500][300/346], lr: 0.00000310 	 loss = 0.6487(0.6562)
2023/09/15 06:55:25 - INFO - root -   Epoch: [147/500][320/346], lr: 0.00000310 	 loss = 0.1071(0.6609)
2023/09/15 06:55:41 - INFO - root -   Epoch: [147/500][340/346], lr: 0.00000310 	 loss = 1.1219(0.6625)
2023/09/15 06:55:43 - INFO - root -   Epoch: [147/500] 	 loss = 0.6600
2023/09/15 06:55:43 - INFO - root -   train_accuracy = 0.7399
2023/09/15 06:55:52 - INFO - root -   Epoch: [148/500][0/346], lr: 0.00000312 	 loss = 2.6379(2.6379)
2023/09/15 06:56:09 - INFO - root -   Epoch: [148/500][20/346], lr: 0.00000312 	 loss = 0.5956(0.8892)
2023/09/15 06:56:30 - INFO - root -   Epoch: [148/500][40/346], lr: 0.00000312 	 loss = 1.6219(0.7883)
2023/09/15 06:56:47 - INFO - root -   Epoch: [148/500][60/346], lr: 0.00000312 	 loss = 0.2429(0.7582)
2023/09/15 06:57:08 - INFO - root -   Epoch: [148/500][80/346], lr: 0.00000312 	 loss = 1.9202(0.7528)
2023/09/15 06:57:25 - INFO - root -   Epoch: [148/500][100/346], lr: 0.00000312 	 loss = 0.1224(0.7173)
2023/09/15 06:57:46 - INFO - root -   Epoch: [148/500][120/346], lr: 0.00000312 	 loss = 0.4708(0.7275)
2023/09/15 06:58:03 - INFO - root -   Epoch: [148/500][140/346], lr: 0.00000312 	 loss = 2.3092(0.7716)
2023/09/15 06:58:24 - INFO - root -   Epoch: [148/500][160/346], lr: 0.00000312 	 loss = 1.5924(0.7524)
2023/09/15 06:58:41 - INFO - root -   Epoch: [148/500][180/346], lr: 0.00000312 	 loss = 0.3601(0.7361)
2023/09/15 06:59:02 - INFO - root -   Epoch: [148/500][200/346], lr: 0.00000312 	 loss = 0.0420(0.7015)
2023/09/15 06:59:19 - INFO - root -   Epoch: [148/500][220/346], lr: 0.00000312 	 loss = 1.2717(0.6924)
2023/09/15 06:59:40 - INFO - root -   Epoch: [148/500][240/346], lr: 0.00000312 	 loss = 0.1846(0.6798)
2023/09/15 06:59:56 - INFO - root -   Epoch: [148/500][260/346], lr: 0.00000312 	 loss = 1.4382(0.6676)
2023/09/15 07:00:18 - INFO - root -   Epoch: [148/500][280/346], lr: 0.00000312 	 loss = 0.7768(0.6639)
2023/09/15 07:00:34 - INFO - root -   Epoch: [148/500][300/346], lr: 0.00000312 	 loss = 0.8133(0.6645)
2023/09/15 07:00:55 - INFO - root -   Epoch: [148/500][320/346], lr: 0.00000312 	 loss = 0.0675(0.6675)
2023/09/15 07:01:12 - INFO - root -   Epoch: [148/500][340/346], lr: 0.00000312 	 loss = 1.3941(0.6634)
2023/09/15 07:01:14 - INFO - root -   Epoch: [148/500] 	 loss = 0.6613
2023/09/15 07:01:14 - INFO - root -   train_accuracy = 0.7702
2023/09/15 07:01:22 - INFO - root -   Epoch: [149/500][0/346], lr: 0.00000313 	 loss = 1.6199(1.6199)
2023/09/15 07:01:39 - INFO - root -   Epoch: [149/500][20/346], lr: 0.00000313 	 loss = 0.6752(0.6180)
2023/09/15 07:02:00 - INFO - root -   Epoch: [149/500][40/346], lr: 0.00000313 	 loss = 0.6562(0.6819)
2023/09/15 07:02:16 - INFO - root -   Epoch: [149/500][60/346], lr: 0.00000313 	 loss = 0.2936(0.6661)
2023/09/15 07:02:37 - INFO - root -   Epoch: [149/500][80/346], lr: 0.00000313 	 loss = 1.7254(0.6858)
2023/09/15 07:02:54 - INFO - root -   Epoch: [149/500][100/346], lr: 0.00000313 	 loss = 0.1611(0.6681)
2023/09/15 07:03:15 - INFO - root -   Epoch: [149/500][120/346], lr: 0.00000313 	 loss = 0.4889(0.6683)
2023/09/15 07:03:31 - INFO - root -   Epoch: [149/500][140/346], lr: 0.00000313 	 loss = 2.3658(0.6852)
2023/09/15 07:03:52 - INFO - root -   Epoch: [149/500][160/346], lr: 0.00000313 	 loss = 2.2866(0.6835)
2023/09/15 07:04:09 - INFO - root -   Epoch: [149/500][180/346], lr: 0.00000313 	 loss = 0.1537(0.6719)
2023/09/15 07:04:30 - INFO - root -   Epoch: [149/500][200/346], lr: 0.00000313 	 loss = 0.0125(0.6418)
2023/09/15 07:04:46 - INFO - root -   Epoch: [149/500][220/346], lr: 0.00000313 	 loss = 1.5910(0.6453)
2023/09/15 07:05:07 - INFO - root -   Epoch: [149/500][240/346], lr: 0.00000313 	 loss = 0.3676(0.6326)
2023/09/15 07:05:23 - INFO - root -   Epoch: [149/500][260/346], lr: 0.00000313 	 loss = 1.9205(0.6263)
2023/09/15 07:05:44 - INFO - root -   Epoch: [149/500][280/346], lr: 0.00000313 	 loss = 0.3986(0.6214)
2023/09/15 07:06:01 - INFO - root -   Epoch: [149/500][300/346], lr: 0.00000313 	 loss = 1.0351(0.6206)
2023/09/15 07:06:21 - INFO - root -   Epoch: [149/500][320/346], lr: 0.00000313 	 loss = 0.1969(0.6303)
2023/09/15 07:06:37 - INFO - root -   Epoch: [149/500][340/346], lr: 0.00000313 	 loss = 1.2778(0.6276)
2023/09/15 07:06:39 - INFO - root -   Epoch: [149/500] 	 loss = 0.6245
2023/09/15 07:08:02 - INFO - root -   precision = 0.4885
2023/09/15 07:08:02 - INFO - root -   eval_loss = 1.6231
2023/09/15 07:08:03 - INFO - root -   train_accuracy = 0.7659
2023/09/15 07:08:12 - INFO - root -   Epoch: [150/500][0/346], lr: 0.00000315 	 loss = 1.3160(1.3160)
2023/09/15 07:08:28 - INFO - root -   Epoch: [150/500][20/346], lr: 0.00000315 	 loss = 0.3890(0.5267)
2023/09/15 07:08:49 - INFO - root -   Epoch: [150/500][40/346], lr: 0.00000315 	 loss = 0.5041(0.5565)
2023/09/15 07:09:06 - INFO - root -   Epoch: [150/500][60/346], lr: 0.00000315 	 loss = 0.1917(0.5747)
2023/09/15 07:09:27 - INFO - root -   Epoch: [150/500][80/346], lr: 0.00000315 	 loss = 1.8362(0.6092)
2023/09/15 07:09:43 - INFO - root -   Epoch: [150/500][100/346], lr: 0.00000315 	 loss = 0.1495(0.6230)
2023/09/15 07:10:04 - INFO - root -   Epoch: [150/500][120/346], lr: 0.00000315 	 loss = 0.6194(0.6483)
2023/09/15 07:10:21 - INFO - root -   Epoch: [150/500][140/346], lr: 0.00000315 	 loss = 1.8101(0.6567)
2023/09/15 07:10:41 - INFO - root -   Epoch: [150/500][160/346], lr: 0.00000315 	 loss = 2.1376(0.6562)
2023/09/15 07:10:58 - INFO - root -   Epoch: [150/500][180/346], lr: 0.00000315 	 loss = 0.0904(0.6543)
2023/09/15 07:11:19 - INFO - root -   Epoch: [150/500][200/346], lr: 0.00000315 	 loss = 0.0281(0.6229)
2023/09/15 07:11:36 - INFO - root -   Epoch: [150/500][220/346], lr: 0.00000315 	 loss = 1.0978(0.6214)
2023/09/15 07:11:57 - INFO - root -   Epoch: [150/500][240/346], lr: 0.00000315 	 loss = 0.3685(0.6048)
2023/09/15 07:12:13 - INFO - root -   Epoch: [150/500][260/346], lr: 0.00000315 	 loss = 1.6126(0.6052)
2023/09/15 07:12:34 - INFO - root -   Epoch: [150/500][280/346], lr: 0.00000315 	 loss = 0.1084(0.6414)
2023/09/15 07:12:51 - INFO - root -   Epoch: [150/500][300/346], lr: 0.00000315 	 loss = 0.8182(0.6463)
2023/09/15 07:13:12 - INFO - root -   Epoch: [150/500][320/346], lr: 0.00000315 	 loss = 0.0859(0.6466)
2023/09/15 07:13:28 - INFO - root -   Epoch: [150/500][340/346], lr: 0.00000315 	 loss = 1.5505(0.6484)
2023/09/15 07:13:30 - INFO - root -   Epoch: [150/500] 	 loss = 0.6444
2023/09/15 07:13:30 - INFO - root -   train_accuracy = 0.7616
2023/09/15 07:13:39 - INFO - root -   Epoch: [151/500][0/346], lr: 0.00000316 	 loss = 1.0444(1.0444)
2023/09/15 07:13:55 - INFO - root -   Epoch: [151/500][20/346], lr: 0.00000316 	 loss = 0.6085(0.5975)
2023/09/15 07:14:16 - INFO - root -   Epoch: [151/500][40/346], lr: 0.00000316 	 loss = 1.2893(0.6596)
2023/09/15 07:14:33 - INFO - root -   Epoch: [151/500][60/346], lr: 0.00000316 	 loss = 0.2458(0.6602)
2023/09/15 07:14:54 - INFO - root -   Epoch: [151/500][80/346], lr: 0.00000316 	 loss = 1.9530(0.6746)
2023/09/15 07:15:10 - INFO - root -   Epoch: [151/500][100/346], lr: 0.00000316 	 loss = 0.6867(0.6802)
2023/09/15 07:15:31 - INFO - root -   Epoch: [151/500][120/346], lr: 0.00000316 	 loss = 0.3042(0.6908)
2023/09/15 07:15:48 - INFO - root -   Epoch: [151/500][140/346], lr: 0.00000316 	 loss = 2.3604(0.7059)
2023/09/15 07:16:09 - INFO - root -   Epoch: [151/500][160/346], lr: 0.00000316 	 loss = 1.9745(0.7009)
2023/09/15 07:16:26 - INFO - root -   Epoch: [151/500][180/346], lr: 0.00000316 	 loss = 0.2562(0.6885)
2023/09/15 07:16:47 - INFO - root -   Epoch: [151/500][200/346], lr: 0.00000316 	 loss = 0.2076(0.6610)
2023/09/15 07:17:03 - INFO - root -   Epoch: [151/500][220/346], lr: 0.00000316 	 loss = 1.7318(0.6567)
2023/09/15 07:17:24 - INFO - root -   Epoch: [151/500][240/346], lr: 0.00000316 	 loss = 0.8062(0.6352)
2023/09/15 07:17:41 - INFO - root -   Epoch: [151/500][260/346], lr: 0.00000316 	 loss = 1.8931(0.6319)
2023/09/15 07:18:02 - INFO - root -   Epoch: [151/500][280/346], lr: 0.00000316 	 loss = 0.5717(0.6373)
2023/09/15 07:18:18 - INFO - root -   Epoch: [151/500][300/346], lr: 0.00000316 	 loss = 0.7195(0.6341)
2023/09/15 07:18:39 - INFO - root -   Epoch: [151/500][320/346], lr: 0.00000316 	 loss = 0.1017(0.6321)
2023/09/15 07:18:56 - INFO - root -   Epoch: [151/500][340/346], lr: 0.00000316 	 loss = 1.1824(0.6267)
2023/09/15 07:18:58 - INFO - root -   Epoch: [151/500] 	 loss = 0.6220
2023/09/15 07:18:58 - INFO - root -   train_accuracy = 0.7702
2023/09/15 07:19:06 - INFO - root -   Epoch: [152/500][0/346], lr: 0.00000317 	 loss = 1.0443(1.0443)
2023/09/15 07:19:23 - INFO - root -   Epoch: [152/500][20/346], lr: 0.00000317 	 loss = 0.4977(0.5421)
2023/09/15 07:19:43 - INFO - root -   Epoch: [152/500][40/346], lr: 0.00000317 	 loss = 0.3935(0.5717)
2023/09/15 07:20:00 - INFO - root -   Epoch: [152/500][60/346], lr: 0.00000317 	 loss = 0.1670(0.5755)
2023/09/15 07:20:21 - INFO - root -   Epoch: [152/500][80/346], lr: 0.00000317 	 loss = 3.0412(0.5995)
2023/09/15 07:20:37 - INFO - root -   Epoch: [152/500][100/346], lr: 0.00000317 	 loss = 0.0364(0.5984)
2023/09/15 07:20:58 - INFO - root -   Epoch: [152/500][120/346], lr: 0.00000317 	 loss = 0.1715(0.6070)
2023/09/15 07:21:15 - INFO - root -   Epoch: [152/500][140/346], lr: 0.00000317 	 loss = 2.2407(0.6376)
2023/09/15 07:21:36 - INFO - root -   Epoch: [152/500][160/346], lr: 0.00000317 	 loss = 1.8494(0.6664)
2023/09/15 07:21:52 - INFO - root -   Epoch: [152/500][180/346], lr: 0.00000317 	 loss = 0.2024(0.6581)
2023/09/15 07:22:13 - INFO - root -   Epoch: [152/500][200/346], lr: 0.00000317 	 loss = 0.0283(0.6283)
2023/09/15 07:22:29 - INFO - root -   Epoch: [152/500][220/346], lr: 0.00000317 	 loss = 1.4411(0.6237)
2023/09/15 07:22:50 - INFO - root -   Epoch: [152/500][240/346], lr: 0.00000317 	 loss = 0.2773(0.6059)
2023/09/15 07:23:07 - INFO - root -   Epoch: [152/500][260/346], lr: 0.00000317 	 loss = 1.7576(0.6044)
2023/09/15 07:23:28 - INFO - root -   Epoch: [152/500][280/346], lr: 0.00000317 	 loss = 1.0133(0.6061)
2023/09/15 07:23:44 - INFO - root -   Epoch: [152/500][300/346], lr: 0.00000317 	 loss = 0.6496(0.5994)
2023/09/15 07:24:05 - INFO - root -   Epoch: [152/500][320/346], lr: 0.00000317 	 loss = 0.0577(0.6045)
2023/09/15 07:24:20 - INFO - root -   Epoch: [152/500][340/346], lr: 0.00000317 	 loss = 0.9987(0.6084)
2023/09/15 07:24:23 - INFO - root -   Epoch: [152/500] 	 loss = 0.6054
2023/09/15 07:24:23 - INFO - root -   train_accuracy = 0.7876
2023/09/15 07:24:31 - INFO - root -   Epoch: [153/500][0/346], lr: 0.00000319 	 loss = 1.1489(1.1489)
2023/09/15 07:24:47 - INFO - root -   Epoch: [153/500][20/346], lr: 0.00000319 	 loss = 1.0611(0.6823)
2023/09/15 07:25:08 - INFO - root -   Epoch: [153/500][40/346], lr: 0.00000319 	 loss = 0.1944(0.6357)
2023/09/15 07:25:25 - INFO - root -   Epoch: [153/500][60/346], lr: 0.00000319 	 loss = 0.2945(0.6359)
2023/09/15 07:25:46 - INFO - root -   Epoch: [153/500][80/346], lr: 0.00000319 	 loss = 1.5678(0.6204)
2023/09/15 07:26:02 - INFO - root -   Epoch: [153/500][100/346], lr: 0.00000319 	 loss = 0.2168(0.6047)
2023/09/15 07:26:23 - INFO - root -   Epoch: [153/500][120/346], lr: 0.00000319 	 loss = 0.3386(0.6034)
2023/09/15 07:26:39 - INFO - root -   Epoch: [153/500][140/346], lr: 0.00000319 	 loss = 2.0593(0.6192)
2023/09/15 07:27:00 - INFO - root -   Epoch: [153/500][160/346], lr: 0.00000319 	 loss = 2.1261(0.6189)
2023/09/15 07:27:17 - INFO - root -   Epoch: [153/500][180/346], lr: 0.00000319 	 loss = 0.0792(0.6116)
2023/09/15 07:27:38 - INFO - root -   Epoch: [153/500][200/346], lr: 0.00000319 	 loss = 0.0053(0.5806)
2023/09/15 07:27:54 - INFO - root -   Epoch: [153/500][220/346], lr: 0.00000319 	 loss = 1.2444(0.5727)
2023/09/15 07:28:16 - INFO - root -   Epoch: [153/500][240/346], lr: 0.00000319 	 loss = 0.2187(0.5658)
2023/09/15 07:28:32 - INFO - root -   Epoch: [153/500][260/346], lr: 0.00000319 	 loss = 1.4222(0.5584)
2023/09/15 07:28:53 - INFO - root -   Epoch: [153/500][280/346], lr: 0.00000319 	 loss = 0.3889(0.5524)
2023/09/15 07:29:10 - INFO - root -   Epoch: [153/500][300/346], lr: 0.00000319 	 loss = 1.1121(0.5448)
2023/09/15 07:29:30 - INFO - root -   Epoch: [153/500][320/346], lr: 0.00000319 	 loss = 0.1033(0.5633)
2023/09/15 07:29:47 - INFO - root -   Epoch: [153/500][340/346], lr: 0.00000319 	 loss = 1.2371(0.5648)
2023/09/15 07:29:49 - INFO - root -   Epoch: [153/500] 	 loss = 0.5636
2023/09/15 07:29:49 - INFO - root -   train_accuracy = 0.7832
2023/09/15 07:29:57 - INFO - root -   Epoch: [154/500][0/346], lr: 0.00000320 	 loss = 0.7811(0.7811)
2023/09/15 07:30:14 - INFO - root -   Epoch: [154/500][20/346], lr: 0.00000320 	 loss = 0.8455(0.5564)
2023/09/15 07:30:35 - INFO - root -   Epoch: [154/500][40/346], lr: 0.00000320 	 loss = 0.3308(0.5894)
2023/09/15 07:30:51 - INFO - root -   Epoch: [154/500][60/346], lr: 0.00000320 	 loss = 0.3999(0.5585)
2023/09/15 07:31:12 - INFO - root -   Epoch: [154/500][80/346], lr: 0.00000320 	 loss = 1.1903(0.5942)
2023/09/15 07:31:29 - INFO - root -   Epoch: [154/500][100/346], lr: 0.00000320 	 loss = 0.0217(0.6088)
2023/09/15 07:31:50 - INFO - root -   Epoch: [154/500][120/346], lr: 0.00000320 	 loss = 0.2357(0.6260)
2023/09/15 07:32:06 - INFO - root -   Epoch: [154/500][140/346], lr: 0.00000320 	 loss = 1.3152(0.6338)
2023/09/15 07:32:27 - INFO - root -   Epoch: [154/500][160/346], lr: 0.00000320 	 loss = 2.6222(0.6337)
2023/09/15 07:32:44 - INFO - root -   Epoch: [154/500][180/346], lr: 0.00000320 	 loss = 0.1482(0.6246)
2023/09/15 07:33:05 - INFO - root -   Epoch: [154/500][200/346], lr: 0.00000320 	 loss = 0.0387(0.5971)
2023/09/15 07:33:21 - INFO - root -   Epoch: [154/500][220/346], lr: 0.00000320 	 loss = 1.3443(0.6061)
2023/09/15 07:33:42 - INFO - root -   Epoch: [154/500][240/346], lr: 0.00000320 	 loss = 0.2774(0.6156)
2023/09/15 07:33:59 - INFO - root -   Epoch: [154/500][260/346], lr: 0.00000320 	 loss = 1.2887(0.6104)
2023/09/15 07:34:20 - INFO - root -   Epoch: [154/500][280/346], lr: 0.00000320 	 loss = 0.4821(0.5990)
2023/09/15 07:34:37 - INFO - root -   Epoch: [154/500][300/346], lr: 0.00000320 	 loss = 0.7809(0.5976)
2023/09/15 07:34:58 - INFO - root -   Epoch: [154/500][320/346], lr: 0.00000320 	 loss = 0.0792(0.5971)
2023/09/15 07:35:14 - INFO - root -   Epoch: [154/500][340/346], lr: 0.00000320 	 loss = 0.9539(0.5915)
2023/09/15 07:35:16 - INFO - root -   Epoch: [154/500] 	 loss = 0.5891
2023/09/15 07:36:39 - INFO - root -   precision = 0.4598
2023/09/15 07:36:39 - INFO - root -   eval_loss = 1.5776
2023/09/15 07:36:40 - INFO - root -   train_accuracy = 0.7645
2023/09/15 07:36:49 - INFO - root -   Epoch: [155/500][0/346], lr: 0.00000322 	 loss = 1.0370(1.0370)
2023/09/15 07:37:05 - INFO - root -   Epoch: [155/500][20/346], lr: 0.00000322 	 loss = 0.8123(0.6484)
2023/09/15 07:37:26 - INFO - root -   Epoch: [155/500][40/346], lr: 0.00000322 	 loss = 0.7276(0.6064)
2023/09/15 07:37:42 - INFO - root -   Epoch: [155/500][60/346], lr: 0.00000322 	 loss = 0.4429(0.6041)
2023/09/15 07:38:03 - INFO - root -   Epoch: [155/500][80/346], lr: 0.00000322 	 loss = 1.8156(0.6149)
2023/09/15 07:38:20 - INFO - root -   Epoch: [155/500][100/346], lr: 0.00000322 	 loss = 0.0617(0.6153)
2023/09/15 07:38:41 - INFO - root -   Epoch: [155/500][120/346], lr: 0.00000322 	 loss = 0.2589(0.6213)
2023/09/15 07:38:57 - INFO - root -   Epoch: [155/500][140/346], lr: 0.00000322 	 loss = 1.7243(0.6219)
2023/09/15 07:39:18 - INFO - root -   Epoch: [155/500][160/346], lr: 0.00000322 	 loss = 2.2466(0.6090)
2023/09/15 07:39:35 - INFO - root -   Epoch: [155/500][180/346], lr: 0.00000322 	 loss = 0.1524(0.6023)
2023/09/15 07:39:56 - INFO - root -   Epoch: [155/500][200/346], lr: 0.00000322 	 loss = 0.0033(0.5784)
2023/09/15 07:40:12 - INFO - root -   Epoch: [155/500][220/346], lr: 0.00000322 	 loss = 1.2770(0.5809)
2023/09/15 07:40:33 - INFO - root -   Epoch: [155/500][240/346], lr: 0.00000322 	 loss = 0.3103(0.5611)
2023/09/15 07:40:50 - INFO - root -   Epoch: [155/500][260/346], lr: 0.00000322 	 loss = 1.6812(0.5614)
2023/09/15 07:41:11 - INFO - root -   Epoch: [155/500][280/346], lr: 0.00000322 	 loss = 0.4126(0.5497)
2023/09/15 07:41:28 - INFO - root -   Epoch: [155/500][300/346], lr: 0.00000322 	 loss = 0.4825(0.5455)
2023/09/15 07:41:49 - INFO - root -   Epoch: [155/500][320/346], lr: 0.00000322 	 loss = 0.0160(0.5518)
2023/09/15 07:42:05 - INFO - root -   Epoch: [155/500][340/346], lr: 0.00000322 	 loss = 1.0449(0.5513)
2023/09/15 07:42:07 - INFO - root -   Epoch: [155/500] 	 loss = 0.5495
2023/09/15 07:42:07 - INFO - root -   train_accuracy = 0.7861
2023/09/15 07:42:15 - INFO - root -   Epoch: [156/500][0/346], lr: 0.00000323 	 loss = 0.7862(0.7862)
2023/09/15 07:42:32 - INFO - root -   Epoch: [156/500][20/346], lr: 0.00000323 	 loss = 2.0627(0.6784)
2023/09/15 07:42:53 - INFO - root -   Epoch: [156/500][40/346], lr: 0.00000323 	 loss = 0.1908(0.6215)
2023/09/15 07:43:10 - INFO - root -   Epoch: [156/500][60/346], lr: 0.00000323 	 loss = 0.6138(0.7120)
2023/09/15 07:43:31 - INFO - root -   Epoch: [156/500][80/346], lr: 0.00000323 	 loss = 1.4071(0.7035)
2023/09/15 07:43:48 - INFO - root -   Epoch: [156/500][100/346], lr: 0.00000323 	 loss = 0.0775(0.6759)
2023/09/15 07:44:09 - INFO - root -   Epoch: [156/500][120/346], lr: 0.00000323 	 loss = 0.2885(0.6607)
2023/09/15 07:44:25 - INFO - root -   Epoch: [156/500][140/346], lr: 0.00000323 	 loss = 1.3850(0.6602)
2023/09/15 07:44:46 - INFO - root -   Epoch: [156/500][160/346], lr: 0.00000323 	 loss = 2.0847(0.6584)
2023/09/15 07:45:03 - INFO - root -   Epoch: [156/500][180/346], lr: 0.00000323 	 loss = 0.0760(0.6716)
2023/09/15 07:45:24 - INFO - root -   Epoch: [156/500][200/346], lr: 0.00000323 	 loss = 0.0073(0.6392)
2023/09/15 07:45:41 - INFO - root -   Epoch: [156/500][220/346], lr: 0.00000323 	 loss = 1.3327(0.6318)
2023/09/15 07:46:02 - INFO - root -   Epoch: [156/500][240/346], lr: 0.00000323 	 loss = 0.3359(0.6092)
2023/09/15 07:46:19 - INFO - root -   Epoch: [156/500][260/346], lr: 0.00000323 	 loss = 1.7086(0.6061)
2023/09/15 07:46:40 - INFO - root -   Epoch: [156/500][280/346], lr: 0.00000323 	 loss = 0.5919(0.5972)
2023/09/15 07:46:57 - INFO - root -   Epoch: [156/500][300/346], lr: 0.00000323 	 loss = 0.7992(0.5975)
2023/09/15 07:47:18 - INFO - root -   Epoch: [156/500][320/346], lr: 0.00000323 	 loss = 0.0696(0.6034)
2023/09/15 07:47:34 - INFO - root -   Epoch: [156/500][340/346], lr: 0.00000323 	 loss = 1.0143(0.6026)
2023/09/15 07:47:36 - INFO - root -   Epoch: [156/500] 	 loss = 0.5976
2023/09/15 07:47:36 - INFO - root -   train_accuracy = 0.7702
2023/09/15 07:47:45 - INFO - root -   Epoch: [157/500][0/346], lr: 0.00000325 	 loss = 0.9271(0.9271)
2023/09/15 07:48:01 - INFO - root -   Epoch: [157/500][20/346], lr: 0.00000325 	 loss = 0.6334(0.5624)
2023/09/15 07:48:22 - INFO - root -   Epoch: [157/500][40/346], lr: 0.00000325 	 loss = 0.3129(0.5712)
2023/09/15 07:48:39 - INFO - root -   Epoch: [157/500][60/346], lr: 0.00000325 	 loss = 0.0847(0.5798)
2023/09/15 07:48:59 - INFO - root -   Epoch: [157/500][80/346], lr: 0.00000325 	 loss = 1.3753(0.5994)
2023/09/15 07:49:16 - INFO - root -   Epoch: [157/500][100/346], lr: 0.00000325 	 loss = 0.0422(0.5901)
2023/09/15 07:49:37 - INFO - root -   Epoch: [157/500][120/346], lr: 0.00000325 	 loss = 1.2730(0.6426)
2023/09/15 07:49:53 - INFO - root -   Epoch: [157/500][140/346], lr: 0.00000325 	 loss = 1.1631(0.6851)
2023/09/15 07:50:14 - INFO - root -   Epoch: [157/500][160/346], lr: 0.00000325 	 loss = 1.9300(0.6784)
2023/09/15 07:50:31 - INFO - root -   Epoch: [157/500][180/346], lr: 0.00000325 	 loss = 0.2467(0.6638)
2023/09/15 07:50:52 - INFO - root -   Epoch: [157/500][200/346], lr: 0.00000325 	 loss = 0.0136(0.6322)
2023/09/15 07:51:09 - INFO - root -   Epoch: [157/500][220/346], lr: 0.00000325 	 loss = 1.5874(0.6242)
2023/09/15 07:51:30 - INFO - root -   Epoch: [157/500][240/346], lr: 0.00000325 	 loss = 0.4819(0.6228)
2023/09/15 07:51:46 - INFO - root -   Epoch: [157/500][260/346], lr: 0.00000325 	 loss = 1.1839(0.6154)
2023/09/15 07:52:07 - INFO - root -   Epoch: [157/500][280/346], lr: 0.00000325 	 loss = 0.6820(0.6021)
2023/09/15 07:52:24 - INFO - root -   Epoch: [157/500][300/346], lr: 0.00000325 	 loss = 0.6416(0.5912)
2023/09/15 07:52:45 - INFO - root -   Epoch: [157/500][320/346], lr: 0.00000325 	 loss = 0.0552(0.5944)
2023/09/15 07:53:02 - INFO - root -   Epoch: [157/500][340/346], lr: 0.00000325 	 loss = 1.3116(0.5917)
2023/09/15 07:53:04 - INFO - root -   Epoch: [157/500] 	 loss = 0.5886
2023/09/15 07:53:04 - INFO - root -   train_accuracy = 0.7659
2023/09/15 07:53:12 - INFO - root -   Epoch: [158/500][0/346], lr: 0.00000326 	 loss = 0.4723(0.4723)
2023/09/15 07:53:29 - INFO - root -   Epoch: [158/500][20/346], lr: 0.00000326 	 loss = 0.3088(0.4544)
2023/09/15 07:53:50 - INFO - root -   Epoch: [158/500][40/346], lr: 0.00000326 	 loss = 0.5172(0.4837)
2023/09/15 07:54:06 - INFO - root -   Epoch: [158/500][60/346], lr: 0.00000326 	 loss = 0.3946(0.4982)
2023/09/15 07:54:27 - INFO - root -   Epoch: [158/500][80/346], lr: 0.00000326 	 loss = 1.2386(0.5040)
2023/09/15 07:54:44 - INFO - root -   Epoch: [158/500][100/346], lr: 0.00000326 	 loss = 0.1111(0.4994)
2023/09/15 07:55:05 - INFO - root -   Epoch: [158/500][120/346], lr: 0.00000326 	 loss = 0.4684(0.5064)
2023/09/15 07:55:22 - INFO - root -   Epoch: [158/500][140/346], lr: 0.00000326 	 loss = 0.6109(0.5270)
2023/09/15 07:55:43 - INFO - root -   Epoch: [158/500][160/346], lr: 0.00000326 	 loss = 2.1372(0.5260)
2023/09/15 07:56:00 - INFO - root -   Epoch: [158/500][180/346], lr: 0.00000326 	 loss = 0.1721(0.5232)
2023/09/15 07:56:21 - INFO - root -   Epoch: [158/500][200/346], lr: 0.00000326 	 loss = 0.0031(0.4964)
2023/09/15 07:56:37 - INFO - root -   Epoch: [158/500][220/346], lr: 0.00000326 	 loss = 1.0957(0.4962)
2023/09/15 07:56:58 - INFO - root -   Epoch: [158/500][240/346], lr: 0.00000326 	 loss = 0.3597(0.4839)
2023/09/15 07:57:15 - INFO - root -   Epoch: [158/500][260/346], lr: 0.00000326 	 loss = 2.0891(0.5036)
2023/09/15 07:57:36 - INFO - root -   Epoch: [158/500][280/346], lr: 0.00000326 	 loss = 0.6251(0.4992)
2023/09/15 07:57:53 - INFO - root -   Epoch: [158/500][300/346], lr: 0.00000326 	 loss = 0.5480(0.4960)
2023/09/15 07:58:14 - INFO - root -   Epoch: [158/500][320/346], lr: 0.00000326 	 loss = 0.0337(0.4963)
2023/09/15 07:58:30 - INFO - root -   Epoch: [158/500][340/346], lr: 0.00000326 	 loss = 0.9662(0.4968)
2023/09/15 07:58:32 - INFO - root -   Epoch: [158/500] 	 loss = 0.4975
2023/09/15 07:58:32 - INFO - root -   train_accuracy = 0.8194
2023/09/15 07:58:41 - INFO - root -   Epoch: [159/500][0/346], lr: 0.00000327 	 loss = 0.2084(0.2084)
2023/09/15 07:58:57 - INFO - root -   Epoch: [159/500][20/346], lr: 0.00000327 	 loss = 0.2552(0.4345)
2023/09/15 07:59:18 - INFO - root -   Epoch: [159/500][40/346], lr: 0.00000327 	 loss = 0.5071(0.4654)
2023/09/15 07:59:35 - INFO - root -   Epoch: [159/500][60/346], lr: 0.00000327 	 loss = 0.2593(0.4867)
2023/09/15 07:59:56 - INFO - root -   Epoch: [159/500][80/346], lr: 0.00000327 	 loss = 1.1675(0.4968)
2023/09/15 08:00:12 - INFO - root -   Epoch: [159/500][100/346], lr: 0.00000327 	 loss = 0.0972(0.4982)
2023/09/15 08:00:33 - INFO - root -   Epoch: [159/500][120/346], lr: 0.00000327 	 loss = 0.2357(0.4987)
2023/09/15 08:00:50 - INFO - root -   Epoch: [159/500][140/346], lr: 0.00000327 	 loss = 0.8117(0.5243)
2023/09/15 08:01:11 - INFO - root -   Epoch: [159/500][160/346], lr: 0.00000327 	 loss = 2.0316(0.5216)
2023/09/15 08:01:27 - INFO - root -   Epoch: [159/500][180/346], lr: 0.00000327 	 loss = 0.2003(0.5212)
2023/09/15 08:01:48 - INFO - root -   Epoch: [159/500][200/346], lr: 0.00000327 	 loss = 0.0011(0.4982)
2023/09/15 08:02:05 - INFO - root -   Epoch: [159/500][220/346], lr: 0.00000327 	 loss = 1.2744(0.5032)
2023/09/15 08:02:26 - INFO - root -   Epoch: [159/500][240/346], lr: 0.00000327 	 loss = 0.3140(0.4857)
2023/09/15 08:02:42 - INFO - root -   Epoch: [159/500][260/346], lr: 0.00000327 	 loss = 2.4709(0.4989)
2023/09/15 08:03:03 - INFO - root -   Epoch: [159/500][280/346], lr: 0.00000327 	 loss = 0.3855(0.4861)
2023/09/15 08:03:20 - INFO - root -   Epoch: [159/500][300/346], lr: 0.00000327 	 loss = 0.3542(0.4810)
2023/09/15 08:03:41 - INFO - root -   Epoch: [159/500][320/346], lr: 0.00000327 	 loss = 0.0255(0.4917)
2023/09/15 08:03:57 - INFO - root -   Epoch: [159/500][340/346], lr: 0.00000327 	 loss = 0.9281(0.4964)
2023/09/15 08:03:59 - INFO - root -   Epoch: [159/500] 	 loss = 0.4944
2023/09/15 08:05:22 - INFO - root -   precision = 0.4713
2023/09/15 08:05:22 - INFO - root -   eval_loss = 2.0614
2023/09/15 08:05:23 - INFO - root -   train_accuracy = 0.8208
2023/09/15 08:05:31 - INFO - root -   Epoch: [160/500][0/346], lr: 0.00000329 	 loss = 1.4221(1.4221)
2023/09/15 08:05:48 - INFO - root -   Epoch: [160/500][20/346], lr: 0.00000329 	 loss = 0.4167(0.8972)
2023/09/15 08:06:09 - INFO - root -   Epoch: [160/500][40/346], lr: 0.00000329 	 loss = 0.4558(0.7287)
2023/09/15 08:06:25 - INFO - root -   Epoch: [160/500][60/346], lr: 0.00000329 	 loss = 0.2272(0.6548)
2023/09/15 08:06:46 - INFO - root -   Epoch: [160/500][80/346], lr: 0.00000329 	 loss = 1.1283(0.6374)
2023/09/15 08:07:02 - INFO - root -   Epoch: [160/500][100/346], lr: 0.00000329 	 loss = 0.1857(0.6265)
2023/09/15 08:07:23 - INFO - root -   Epoch: [160/500][120/346], lr: 0.00000329 	 loss = 0.3978(0.6323)
2023/09/15 08:07:40 - INFO - root -   Epoch: [160/500][140/346], lr: 0.00000329 	 loss = 1.0791(0.6247)
2023/09/15 08:08:01 - INFO - root -   Epoch: [160/500][160/346], lr: 0.00000329 	 loss = 1.9146(0.6335)
2023/09/15 08:08:18 - INFO - root -   Epoch: [160/500][180/346], lr: 0.00000329 	 loss = 0.1434(0.6396)
2023/09/15 08:08:38 - INFO - root -   Epoch: [160/500][200/346], lr: 0.00000329 	 loss = 0.8545(0.6184)
2023/09/15 08:08:55 - INFO - root -   Epoch: [160/500][220/346], lr: 0.00000329 	 loss = 1.8110(0.6161)
2023/09/15 08:09:16 - INFO - root -   Epoch: [160/500][240/346], lr: 0.00000329 	 loss = 0.3290(0.6033)
2023/09/15 08:09:32 - INFO - root -   Epoch: [160/500][260/346], lr: 0.00000329 	 loss = 2.2241(0.6115)
2023/09/15 08:09:54 - INFO - root -   Epoch: [160/500][280/346], lr: 0.00000329 	 loss = 0.5560(0.5974)
2023/09/15 08:10:10 - INFO - root -   Epoch: [160/500][300/346], lr: 0.00000329 	 loss = 0.6842(0.5895)
2023/09/15 08:10:31 - INFO - root -   Epoch: [160/500][320/346], lr: 0.00000329 	 loss = 0.0147(0.5773)
2023/09/15 08:10:47 - INFO - root -   Epoch: [160/500][340/346], lr: 0.00000329 	 loss = 1.1317(0.5701)
2023/09/15 08:10:49 - INFO - root -   Epoch: [160/500] 	 loss = 0.5671
2023/09/15 08:10:49 - INFO - root -   train_accuracy = 0.7803
2023/09/15 08:10:58 - INFO - root -   Epoch: [161/500][0/346], lr: 0.00000330 	 loss = 0.7692(0.7692)
2023/09/15 08:11:14 - INFO - root -   Epoch: [161/500][20/346], lr: 0.00000330 	 loss = 0.3137(0.4552)
2023/09/15 08:11:35 - INFO - root -   Epoch: [161/500][40/346], lr: 0.00000330 	 loss = 0.3074(0.4731)
2023/09/15 08:11:52 - INFO - root -   Epoch: [161/500][60/346], lr: 0.00000330 	 loss = 1.2406(0.5000)
2023/09/15 08:12:12 - INFO - root -   Epoch: [161/500][80/346], lr: 0.00000330 	 loss = 0.4798(0.4803)
2023/09/15 08:12:29 - INFO - root -   Epoch: [161/500][100/346], lr: 0.00000330 	 loss = 0.0492(0.4888)
2023/09/15 08:12:49 - INFO - root -   Epoch: [161/500][120/346], lr: 0.00000330 	 loss = 0.3380(0.4824)
2023/09/15 08:13:06 - INFO - root -   Epoch: [161/500][140/346], lr: 0.00000330 	 loss = 1.1008(0.4874)
2023/09/15 08:13:26 - INFO - root -   Epoch: [161/500][160/346], lr: 0.00000330 	 loss = 2.3715(0.5064)
2023/09/15 08:13:42 - INFO - root -   Epoch: [161/500][180/346], lr: 0.00000330 	 loss = 0.2870(0.5243)
2023/09/15 08:14:03 - INFO - root -   Epoch: [161/500][200/346], lr: 0.00000330 	 loss = 0.0030(0.4976)
2023/09/15 08:14:20 - INFO - root -   Epoch: [161/500][220/346], lr: 0.00000330 	 loss = 1.0734(0.5052)
2023/09/15 08:14:40 - INFO - root -   Epoch: [161/500][240/346], lr: 0.00000330 	 loss = 0.2988(0.4900)
2023/09/15 08:14:57 - INFO - root -   Epoch: [161/500][260/346], lr: 0.00000330 	 loss = 2.0529(0.4988)
2023/09/15 08:15:18 - INFO - root -   Epoch: [161/500][280/346], lr: 0.00000330 	 loss = 0.5473(0.4885)
2023/09/15 08:15:34 - INFO - root -   Epoch: [161/500][300/346], lr: 0.00000330 	 loss = 0.4662(0.4809)
2023/09/15 08:15:55 - INFO - root -   Epoch: [161/500][320/346], lr: 0.00000330 	 loss = 0.0455(0.4845)
2023/09/15 08:16:10 - INFO - root -   Epoch: [161/500][340/346], lr: 0.00000330 	 loss = 1.3963(0.4787)
2023/09/15 08:16:13 - INFO - root -   Epoch: [161/500] 	 loss = 0.4758
2023/09/15 08:16:13 - INFO - root -   train_accuracy = 0.8194
2023/09/15 08:16:21 - INFO - root -   Epoch: [162/500][0/346], lr: 0.00000332 	 loss = 0.8754(0.8754)
2023/09/15 08:16:37 - INFO - root -   Epoch: [162/500][20/346], lr: 0.00000332 	 loss = 0.1699(0.4586)
2023/09/15 08:16:58 - INFO - root -   Epoch: [162/500][40/346], lr: 0.00000332 	 loss = 0.6467(0.4445)
2023/09/15 08:17:15 - INFO - root -   Epoch: [162/500][60/346], lr: 0.00000332 	 loss = 0.0551(0.5428)
2023/09/15 08:17:36 - INFO - root -   Epoch: [162/500][80/346], lr: 0.00000332 	 loss = 0.6296(0.5146)
2023/09/15 08:17:52 - INFO - root -   Epoch: [162/500][100/346], lr: 0.00000332 	 loss = 0.0427(0.5126)
2023/09/15 08:18:13 - INFO - root -   Epoch: [162/500][120/346], lr: 0.00000332 	 loss = 0.4434(0.5308)
2023/09/15 08:18:30 - INFO - root -   Epoch: [162/500][140/346], lr: 0.00000332 	 loss = 0.6337(0.5293)
2023/09/15 08:18:51 - INFO - root -   Epoch: [162/500][160/346], lr: 0.00000332 	 loss = 2.7160(0.5320)
2023/09/15 08:19:07 - INFO - root -   Epoch: [162/500][180/346], lr: 0.00000332 	 loss = 0.2981(0.5385)
2023/09/15 08:19:28 - INFO - root -   Epoch: [162/500][200/346], lr: 0.00000332 	 loss = 0.0029(0.5143)
2023/09/15 08:19:45 - INFO - root -   Epoch: [162/500][220/346], lr: 0.00000332 	 loss = 1.1496(0.5349)
2023/09/15 08:20:06 - INFO - root -   Epoch: [162/500][240/346], lr: 0.00000332 	 loss = 0.2543(0.6071)
2023/09/15 08:20:22 - INFO - root -   Epoch: [162/500][260/346], lr: 0.00000332 	 loss = 1.1872(0.6288)
2023/09/15 08:20:43 - INFO - root -   Epoch: [162/500][280/346], lr: 0.00000332 	 loss = 0.7047(0.6188)
2023/09/15 08:21:00 - INFO - root -   Epoch: [162/500][300/346], lr: 0.00000332 	 loss = 0.4856(0.6460)
2023/09/15 08:21:21 - INFO - root -   Epoch: [162/500][320/346], lr: 0.00000332 	 loss = 0.0404(0.6518)
2023/09/15 08:21:37 - INFO - root -   Epoch: [162/500][340/346], lr: 0.00000332 	 loss = 1.3021(0.6427)
2023/09/15 08:21:39 - INFO - root -   Epoch: [162/500] 	 loss = 0.6385
2023/09/15 08:21:39 - INFO - root -   train_accuracy = 0.7645
2023/09/15 08:21:48 - INFO - root -   Epoch: [163/500][0/346], lr: 0.00000333 	 loss = 0.9892(0.9892)
2023/09/15 08:22:04 - INFO - root -   Epoch: [163/500][20/346], lr: 0.00000333 	 loss = 0.4564(0.5177)
2023/09/15 08:22:25 - INFO - root -   Epoch: [163/500][40/346], lr: 0.00000333 	 loss = 0.5616(0.4991)
2023/09/15 08:22:41 - INFO - root -   Epoch: [163/500][60/346], lr: 0.00000333 	 loss = 0.3958(0.5196)
2023/09/15 08:23:02 - INFO - root -   Epoch: [163/500][80/346], lr: 0.00000333 	 loss = 0.4003(0.5101)
2023/09/15 08:23:18 - INFO - root -   Epoch: [163/500][100/346], lr: 0.00000333 	 loss = 0.1585(0.5195)
2023/09/15 08:23:39 - INFO - root -   Epoch: [163/500][120/346], lr: 0.00000333 	 loss = 0.2628(0.5176)
2023/09/15 08:23:55 - INFO - root -   Epoch: [163/500][140/346], lr: 0.00000333 	 loss = 0.7742(0.5204)
2023/09/15 08:24:16 - INFO - root -   Epoch: [163/500][160/346], lr: 0.00000333 	 loss = 2.3701(0.5080)
2023/09/15 08:24:32 - INFO - root -   Epoch: [163/500][180/346], lr: 0.00000333 	 loss = 0.0887(0.4957)
2023/09/15 08:24:53 - INFO - root -   Epoch: [163/500][200/346], lr: 0.00000333 	 loss = 0.0005(0.4745)
2023/09/15 08:25:09 - INFO - root -   Epoch: [163/500][220/346], lr: 0.00000333 	 loss = 0.8751(0.4822)
2023/09/15 08:25:30 - INFO - root -   Epoch: [163/500][240/346], lr: 0.00000333 	 loss = 0.2597(0.4690)
2023/09/15 08:25:46 - INFO - root -   Epoch: [163/500][260/346], lr: 0.00000333 	 loss = 1.8987(0.4677)
2023/09/15 08:26:07 - INFO - root -   Epoch: [163/500][280/346], lr: 0.00000333 	 loss = 0.7418(0.4582)
2023/09/15 08:26:24 - INFO - root -   Epoch: [163/500][300/346], lr: 0.00000333 	 loss = 0.4715(0.4557)
2023/09/15 08:26:45 - INFO - root -   Epoch: [163/500][320/346], lr: 0.00000333 	 loss = 0.0545(0.4619)
2023/09/15 08:27:00 - INFO - root -   Epoch: [163/500][340/346], lr: 0.00000333 	 loss = 0.9206(0.4667)
2023/09/15 08:27:02 - INFO - root -   Epoch: [163/500] 	 loss = 0.4683
2023/09/15 08:27:02 - INFO - root -   train_accuracy = 0.8251
2023/09/15 08:27:11 - INFO - root -   Epoch: [164/500][0/346], lr: 0.00000335 	 loss = 0.2419(0.2419)
2023/09/15 08:27:27 - INFO - root -   Epoch: [164/500][20/346], lr: 0.00000335 	 loss = 0.4236(0.5674)
2023/09/15 08:27:48 - INFO - root -   Epoch: [164/500][40/346], lr: 0.00000335 	 loss = 0.4561(0.5321)
2023/09/15 08:28:05 - INFO - root -   Epoch: [164/500][60/346], lr: 0.00000335 	 loss = 0.5679(0.5467)
2023/09/15 08:28:26 - INFO - root -   Epoch: [164/500][80/346], lr: 0.00000335 	 loss = 0.3911(0.5316)
2023/09/15 08:28:43 - INFO - root -   Epoch: [164/500][100/346], lr: 0.00000335 	 loss = 0.4592(0.5126)
2023/09/15 08:29:04 - INFO - root -   Epoch: [164/500][120/346], lr: 0.00000335 	 loss = 0.1811(0.4965)
2023/09/15 08:29:21 - INFO - root -   Epoch: [164/500][140/346], lr: 0.00000335 	 loss = 0.3746(0.4913)
2023/09/15 08:29:42 - INFO - root -   Epoch: [164/500][160/346], lr: 0.00000335 	 loss = 2.3442(0.5001)
2023/09/15 08:29:58 - INFO - root -   Epoch: [164/500][180/346], lr: 0.00000335 	 loss = 0.1940(0.5072)
2023/09/15 08:30:19 - INFO - root -   Epoch: [164/500][200/346], lr: 0.00000335 	 loss = 0.0007(0.4883)
2023/09/15 08:30:36 - INFO - root -   Epoch: [164/500][220/346], lr: 0.00000335 	 loss = 0.7837(0.4875)
2023/09/15 08:30:56 - INFO - root -   Epoch: [164/500][240/346], lr: 0.00000335 	 loss = 0.1851(0.4657)
2023/09/15 08:31:13 - INFO - root -   Epoch: [164/500][260/346], lr: 0.00000335 	 loss = 2.6061(0.4767)
2023/09/15 08:31:34 - INFO - root -   Epoch: [164/500][280/346], lr: 0.00000335 	 loss = 0.6509(0.4706)
2023/09/15 08:31:51 - INFO - root -   Epoch: [164/500][300/346], lr: 0.00000335 	 loss = 0.3653(0.4807)
2023/09/15 08:32:11 - INFO - root -   Epoch: [164/500][320/346], lr: 0.00000335 	 loss = 0.0476(0.5111)
2023/09/15 08:32:27 - INFO - root -   Epoch: [164/500][340/346], lr: 0.00000335 	 loss = 1.2619(0.5364)
2023/09/15 08:32:29 - INFO - root -   Epoch: [164/500] 	 loss = 0.5327
2023/09/15 08:33:52 - INFO - root -   precision = 0.4368
2023/09/15 08:33:52 - INFO - root -   eval_loss = 1.6094
2023/09/15 08:33:53 - INFO - root -   train_accuracy = 0.8136
2023/09/15 08:34:01 - INFO - root -   Epoch: [165/500][0/346], lr: 0.00000336 	 loss = 0.3178(0.3178)
2023/09/15 08:34:17 - INFO - root -   Epoch: [165/500][20/346], lr: 0.00000336 	 loss = 0.1763(0.5040)
2023/09/15 08:34:38 - INFO - root -   Epoch: [165/500][40/346], lr: 0.00000336 	 loss = 0.8183(0.5116)
2023/09/15 08:34:55 - INFO - root -   Epoch: [165/500][60/346], lr: 0.00000336 	 loss = 0.1112(0.5474)
2023/09/15 08:35:15 - INFO - root -   Epoch: [165/500][80/346], lr: 0.00000336 	 loss = 0.6600(0.5271)
2023/09/15 08:35:32 - INFO - root -   Epoch: [165/500][100/346], lr: 0.00000336 	 loss = 0.0615(0.5187)
2023/09/15 08:35:52 - INFO - root -   Epoch: [165/500][120/346], lr: 0.00000336 	 loss = 0.2259(0.5040)
2023/09/15 08:36:09 - INFO - root -   Epoch: [165/500][140/346], lr: 0.00000336 	 loss = 0.7299(0.4951)
2023/09/15 08:36:29 - INFO - root -   Epoch: [165/500][160/346], lr: 0.00000336 	 loss = 2.0340(0.5038)
2023/09/15 08:36:46 - INFO - root -   Epoch: [165/500][180/346], lr: 0.00000336 	 loss = 0.3667(0.4987)
2023/09/15 08:37:07 - INFO - root -   Epoch: [165/500][200/346], lr: 0.00000336 	 loss = 0.0017(0.4790)
2023/09/15 08:37:23 - INFO - root -   Epoch: [165/500][220/346], lr: 0.00000336 	 loss = 1.0235(0.4981)
2023/09/15 08:37:44 - INFO - root -   Epoch: [165/500][240/346], lr: 0.00000336 	 loss = 0.2120(0.4764)
2023/09/15 08:38:00 - INFO - root -   Epoch: [165/500][260/346], lr: 0.00000336 	 loss = 1.1978(0.4691)
2023/09/15 08:38:21 - INFO - root -   Epoch: [165/500][280/346], lr: 0.00000336 	 loss = 0.2411(0.4572)
2023/09/15 08:38:38 - INFO - root -   Epoch: [165/500][300/346], lr: 0.00000336 	 loss = 0.5451(0.4513)
2023/09/15 08:38:58 - INFO - root -   Epoch: [165/500][320/346], lr: 0.00000336 	 loss = 0.0272(0.4610)
2023/09/15 08:39:15 - INFO - root -   Epoch: [165/500][340/346], lr: 0.00000336 	 loss = 1.1789(0.4627)
2023/09/15 08:39:17 - INFO - root -   Epoch: [165/500] 	 loss = 0.4630
2023/09/15 08:39:17 - INFO - root -   train_accuracy = 0.8338
2023/09/15 08:39:26 - INFO - root -   Epoch: [166/500][0/346], lr: 0.00000337 	 loss = 0.8977(0.8977)
2023/09/15 08:39:42 - INFO - root -   Epoch: [166/500][20/346], lr: 0.00000337 	 loss = 0.2896(0.4467)
2023/09/15 08:40:03 - INFO - root -   Epoch: [166/500][40/346], lr: 0.00000337 	 loss = 0.4451(0.4573)
2023/09/15 08:40:20 - INFO - root -   Epoch: [166/500][60/346], lr: 0.00000337 	 loss = 0.2216(0.4593)
2023/09/15 08:40:40 - INFO - root -   Epoch: [166/500][80/346], lr: 0.00000337 	 loss = 0.3309(0.4415)
2023/09/15 08:40:57 - INFO - root -   Epoch: [166/500][100/346], lr: 0.00000337 	 loss = 0.1558(0.4477)
2023/09/15 08:41:18 - INFO - root -   Epoch: [166/500][120/346], lr: 0.00000337 	 loss = 0.2676(0.4473)
2023/09/15 08:41:34 - INFO - root -   Epoch: [166/500][140/346], lr: 0.00000337 	 loss = 0.5486(0.4375)
2023/09/15 08:41:55 - INFO - root -   Epoch: [166/500][160/346], lr: 0.00000337 	 loss = 2.7218(0.4380)
2023/09/15 08:42:12 - INFO - root -   Epoch: [166/500][180/346], lr: 0.00000337 	 loss = 0.2175(0.4605)
2023/09/15 08:42:33 - INFO - root -   Epoch: [166/500][200/346], lr: 0.00000337 	 loss = 0.0012(0.4403)
2023/09/15 08:42:49 - INFO - root -   Epoch: [166/500][220/346], lr: 0.00000337 	 loss = 1.0725(0.4433)
2023/09/15 08:43:10 - INFO - root -   Epoch: [166/500][240/346], lr: 0.00000337 	 loss = 0.0949(0.4389)
2023/09/15 08:43:27 - INFO - root -   Epoch: [166/500][260/346], lr: 0.00000337 	 loss = 1.9611(0.4543)
2023/09/15 08:43:48 - INFO - root -   Epoch: [166/500][280/346], lr: 0.00000337 	 loss = 1.2757(0.4665)
2023/09/15 08:44:04 - INFO - root -   Epoch: [166/500][300/346], lr: 0.00000337 	 loss = 0.6980(0.4609)
2023/09/15 08:44:25 - INFO - root -   Epoch: [166/500][320/346], lr: 0.00000337 	 loss = 0.0145(0.4599)
2023/09/15 08:44:41 - INFO - root -   Epoch: [166/500][340/346], lr: 0.00000337 	 loss = 0.7455(0.4564)
2023/09/15 08:44:44 - INFO - root -   Epoch: [166/500] 	 loss = 0.4530
2023/09/15 08:44:44 - INFO - root -   train_accuracy = 0.8382
2023/09/15 08:44:52 - INFO - root -   Epoch: [167/500][0/346], lr: 0.00000339 	 loss = 1.9077(1.9077)
2023/09/15 08:45:08 - INFO - root -   Epoch: [167/500][20/346], lr: 0.00000339 	 loss = 0.3507(0.5359)
2023/09/15 08:45:29 - INFO - root -   Epoch: [167/500][40/346], lr: 0.00000339 	 loss = 0.4168(0.4832)
2023/09/15 08:45:45 - INFO - root -   Epoch: [167/500][60/346], lr: 0.00000339 	 loss = 0.4592(0.5112)
2023/09/15 08:46:06 - INFO - root -   Epoch: [167/500][80/346], lr: 0.00000339 	 loss = 0.1637(0.4711)
2023/09/15 08:46:23 - INFO - root -   Epoch: [167/500][100/346], lr: 0.00000339 	 loss = 0.0401(0.4663)
2023/09/15 08:46:44 - INFO - root -   Epoch: [167/500][120/346], lr: 0.00000339 	 loss = 0.2784(0.4720)
2023/09/15 08:47:00 - INFO - root -   Epoch: [167/500][140/346], lr: 0.00000339 	 loss = 0.4289(0.4705)
2023/09/15 08:47:21 - INFO - root -   Epoch: [167/500][160/346], lr: 0.00000339 	 loss = 2.7065(0.4632)
2023/09/15 08:47:38 - INFO - root -   Epoch: [167/500][180/346], lr: 0.00000339 	 loss = 0.2291(0.4614)
2023/09/15 08:47:59 - INFO - root -   Epoch: [167/500][200/346], lr: 0.00000339 	 loss = 0.0004(0.4434)
2023/09/15 08:48:15 - INFO - root -   Epoch: [167/500][220/346], lr: 0.00000339 	 loss = 0.3981(0.4367)
2023/09/15 08:48:36 - INFO - root -   Epoch: [167/500][240/346], lr: 0.00000339 	 loss = 0.0861(0.4242)
2023/09/15 08:48:53 - INFO - root -   Epoch: [167/500][260/346], lr: 0.00000339 	 loss = 2.2157(0.4385)
2023/09/15 08:49:13 - INFO - root -   Epoch: [167/500][280/346], lr: 0.00000339 	 loss = 0.1145(0.4467)
2023/09/15 08:49:30 - INFO - root -   Epoch: [167/500][300/346], lr: 0.00000339 	 loss = 0.4525(0.4626)
2023/09/15 08:49:51 - INFO - root -   Epoch: [167/500][320/346], lr: 0.00000339 	 loss = 0.0336(0.4747)
2023/09/15 08:50:07 - INFO - root -   Epoch: [167/500][340/346], lr: 0.00000339 	 loss = 1.0113(0.4706)
2023/09/15 08:50:09 - INFO - root -   Epoch: [167/500] 	 loss = 0.4689
2023/09/15 08:50:09 - INFO - root -   train_accuracy = 0.8396
2023/09/15 08:50:18 - INFO - root -   Epoch: [168/500][0/346], lr: 0.00000340 	 loss = 1.2774(1.2774)
2023/09/15 08:50:34 - INFO - root -   Epoch: [168/500][20/346], lr: 0.00000340 	 loss = 0.4407(0.5850)
2023/09/15 08:50:55 - INFO - root -   Epoch: [168/500][40/346], lr: 0.00000340 	 loss = 0.4849(0.5169)
2023/09/15 08:51:11 - INFO - root -   Epoch: [168/500][60/346], lr: 0.00000340 	 loss = 0.3134(0.5058)
2023/09/15 08:51:32 - INFO - root -   Epoch: [168/500][80/346], lr: 0.00000340 	 loss = 0.3968(0.5634)
2023/09/15 08:51:48 - INFO - root -   Epoch: [168/500][100/346], lr: 0.00000340 	 loss = 0.0148(0.5384)
2023/09/15 08:52:09 - INFO - root -   Epoch: [168/500][120/346], lr: 0.00000340 	 loss = 0.5559(0.5133)
2023/09/15 08:52:26 - INFO - root -   Epoch: [168/500][140/346], lr: 0.00000340 	 loss = 0.5837(0.5030)
2023/09/15 08:52:47 - INFO - root -   Epoch: [168/500][160/346], lr: 0.00000340 	 loss = 1.3387(0.4797)
2023/09/15 08:53:03 - INFO - root -   Epoch: [168/500][180/346], lr: 0.00000340 	 loss = 0.1266(0.4715)
2023/09/15 08:53:24 - INFO - root -   Epoch: [168/500][200/346], lr: 0.00000340 	 loss = 0.0002(0.4439)
2023/09/15 08:53:41 - INFO - root -   Epoch: [168/500][220/346], lr: 0.00000340 	 loss = 1.3921(0.4498)
2023/09/15 08:54:02 - INFO - root -   Epoch: [168/500][240/346], lr: 0.00000340 	 loss = 0.0479(0.4370)
2023/09/15 08:54:18 - INFO - root -   Epoch: [168/500][260/346], lr: 0.00000340 	 loss = 1.5684(0.4365)
2023/09/15 08:54:39 - INFO - root -   Epoch: [168/500][280/346], lr: 0.00000340 	 loss = 0.5458(0.4247)
2023/09/15 08:54:55 - INFO - root -   Epoch: [168/500][300/346], lr: 0.00000340 	 loss = 0.4472(0.4251)
2023/09/15 08:55:16 - INFO - root -   Epoch: [168/500][320/346], lr: 0.00000340 	 loss = 0.0294(0.4317)
2023/09/15 08:55:33 - INFO - root -   Epoch: [168/500][340/346], lr: 0.00000340 	 loss = 1.5319(0.4336)
2023/09/15 08:55:35 - INFO - root -   Epoch: [168/500] 	 loss = 0.4351
2023/09/15 08:55:35 - INFO - root -   train_accuracy = 0.8382
2023/09/15 08:55:43 - INFO - root -   Epoch: [169/500][0/346], lr: 0.00000342 	 loss = 0.3013(0.3013)
2023/09/15 08:56:00 - INFO - root -   Epoch: [169/500][20/346], lr: 0.00000342 	 loss = 0.3723(0.3816)
2023/09/15 08:56:21 - INFO - root -   Epoch: [169/500][40/346], lr: 0.00000342 	 loss = 0.4709(0.4210)
2023/09/15 08:56:37 - INFO - root -   Epoch: [169/500][60/346], lr: 0.00000342 	 loss = 0.3853(0.4449)
2023/09/15 08:56:58 - INFO - root -   Epoch: [169/500][80/346], lr: 0.00000342 	 loss = 0.2806(0.4322)
2023/09/15 08:57:15 - INFO - root -   Epoch: [169/500][100/346], lr: 0.00000342 	 loss = 0.1025(0.4271)
2023/09/15 08:57:36 - INFO - root -   Epoch: [169/500][120/346], lr: 0.00000342 	 loss = 0.4593(0.4361)
2023/09/15 08:57:52 - INFO - root -   Epoch: [169/500][140/346], lr: 0.00000342 	 loss = 0.1921(0.4270)
2023/09/15 08:58:13 - INFO - root -   Epoch: [169/500][160/346], lr: 0.00000342 	 loss = 2.3086(0.4182)
2023/09/15 08:58:30 - INFO - root -   Epoch: [169/500][180/346], lr: 0.00000342 	 loss = 0.0836(0.4166)
2023/09/15 08:58:51 - INFO - root -   Epoch: [169/500][200/346], lr: 0.00000342 	 loss = 0.0002(0.3965)
2023/09/15 08:59:07 - INFO - root -   Epoch: [169/500][220/346], lr: 0.00000342 	 loss = 0.9421(0.3945)
2023/09/15 08:59:28 - INFO - root -   Epoch: [169/500][240/346], lr: 0.00000342 	 loss = 0.1678(0.3944)
2023/09/15 08:59:45 - INFO - root -   Epoch: [169/500][260/346], lr: 0.00000342 	 loss = 1.0210(0.3991)
2023/09/15 09:00:06 - INFO - root -   Epoch: [169/500][280/346], lr: 0.00000342 	 loss = 0.5701(0.3980)
2023/09/15 09:00:23 - INFO - root -   Epoch: [169/500][300/346], lr: 0.00000342 	 loss = 0.4067(0.3992)
2023/09/15 09:00:44 - INFO - root -   Epoch: [169/500][320/346], lr: 0.00000342 	 loss = 0.6011(0.4118)
2023/09/15 09:01:00 - INFO - root -   Epoch: [169/500][340/346], lr: 0.00000342 	 loss = 1.0982(0.4120)
2023/09/15 09:01:02 - INFO - root -   Epoch: [169/500] 	 loss = 0.4105
2023/09/15 09:02:24 - INFO - root -   precision = 0.4310
2023/09/15 09:02:24 - INFO - root -   eval_loss = 1.8351
2023/09/15 09:02:25 - INFO - root -   train_accuracy = 0.8425
2023/09/15 09:02:34 - INFO - root -   Epoch: [170/500][0/346], lr: 0.00000343 	 loss = 0.1655(0.1655)
2023/09/15 09:02:51 - INFO - root -   Epoch: [170/500][20/346], lr: 0.00000343 	 loss = 0.1271(0.3446)
2023/09/15 09:03:11 - INFO - root -   Epoch: [170/500][40/346], lr: 0.00000343 	 loss = 0.5661(0.3417)
2023/09/15 09:03:27 - INFO - root -   Epoch: [170/500][60/346], lr: 0.00000343 	 loss = 0.8796(0.4369)
2023/09/15 09:03:48 - INFO - root -   Epoch: [170/500][80/346], lr: 0.00000343 	 loss = 0.2950(0.4501)
2023/09/15 09:04:04 - INFO - root -   Epoch: [170/500][100/346], lr: 0.00000343 	 loss = 0.0277(0.4496)
2023/09/15 09:04:25 - INFO - root -   Epoch: [170/500][120/346], lr: 0.00000343 	 loss = 0.3309(0.4514)
2023/09/15 09:04:41 - INFO - root -   Epoch: [170/500][140/346], lr: 0.00000343 	 loss = 0.4858(0.4405)
2023/09/15 09:05:01 - INFO - root -   Epoch: [170/500][160/346], lr: 0.00000343 	 loss = 2.2935(0.4494)
2023/09/15 09:05:18 - INFO - root -   Epoch: [170/500][180/346], lr: 0.00000343 	 loss = 0.9869(0.4567)
2023/09/15 09:05:38 - INFO - root -   Epoch: [170/500][200/346], lr: 0.00000343 	 loss = 0.0001(0.4284)
2023/09/15 09:05:55 - INFO - root -   Epoch: [170/500][220/346], lr: 0.00000343 	 loss = 1.1426(0.4380)
2023/09/15 09:06:15 - INFO - root -   Epoch: [170/500][240/346], lr: 0.00000343 	 loss = 0.1126(0.4317)
2023/09/15 09:06:31 - INFO - root -   Epoch: [170/500][260/346], lr: 0.00000343 	 loss = 1.9493(0.4431)
2023/09/15 09:06:52 - INFO - root -   Epoch: [170/500][280/346], lr: 0.00000343 	 loss = 0.1674(0.4357)
2023/09/15 09:07:08 - INFO - root -   Epoch: [170/500][300/346], lr: 0.00000343 	 loss = 0.2153(0.4281)
2023/09/15 09:07:28 - INFO - root -   Epoch: [170/500][320/346], lr: 0.00000343 	 loss = 0.0052(0.4267)
2023/09/15 09:07:44 - INFO - root -   Epoch: [170/500][340/346], lr: 0.00000343 	 loss = 0.6808(0.4188)
2023/09/15 09:07:46 - INFO - root -   Epoch: [170/500] 	 loss = 0.4185
2023/09/15 09:07:46 - INFO - root -   train_accuracy = 0.8410
2023/09/15 09:07:54 - INFO - root -   Epoch: [171/500][0/346], lr: 0.00000345 	 loss = 4.0504(4.0504)
2023/09/15 09:08:11 - INFO - root -   Epoch: [171/500][20/346], lr: 0.00000345 	 loss = 0.2818(0.6690)
2023/09/15 09:08:32 - INFO - root -   Epoch: [171/500][40/346], lr: 0.00000345 	 loss = 0.1933(0.6802)
2023/09/15 09:08:48 - INFO - root -   Epoch: [171/500][60/346], lr: 0.00000345 	 loss = 0.2534(0.6108)
2023/09/15 09:09:09 - INFO - root -   Epoch: [171/500][80/346], lr: 0.00000345 	 loss = 0.3147(0.5962)
2023/09/15 09:09:26 - INFO - root -   Epoch: [171/500][100/346], lr: 0.00000345 	 loss = 0.2099(0.5777)
2023/09/15 09:09:47 - INFO - root -   Epoch: [171/500][120/346], lr: 0.00000345 	 loss = 0.5719(0.5676)
2023/09/15 09:10:03 - INFO - root -   Epoch: [171/500][140/346], lr: 0.00000345 	 loss = 0.3287(0.5455)
2023/09/15 09:10:24 - INFO - root -   Epoch: [171/500][160/346], lr: 0.00000345 	 loss = 2.0802(0.5148)
2023/09/15 09:10:41 - INFO - root -   Epoch: [171/500][180/346], lr: 0.00000345 	 loss = 0.0510(0.4992)
2023/09/15 09:11:02 - INFO - root -   Epoch: [171/500][200/346], lr: 0.00000345 	 loss = 0.0009(0.4710)
2023/09/15 09:11:18 - INFO - root -   Epoch: [171/500][220/346], lr: 0.00000345 	 loss = 0.3443(0.4682)
2023/09/15 09:11:38 - INFO - root -   Epoch: [171/500][240/346], lr: 0.00000345 	 loss = 0.0850(0.4473)
2023/09/15 09:11:54 - INFO - root -   Epoch: [171/500][260/346], lr: 0.00000345 	 loss = 0.5885(0.4611)
2023/09/15 09:12:15 - INFO - root -   Epoch: [171/500][280/346], lr: 0.00000345 	 loss = 0.3235(0.4582)
2023/09/15 09:12:31 - INFO - root -   Epoch: [171/500][300/346], lr: 0.00000345 	 loss = 0.3132(0.4466)
2023/09/15 09:12:52 - INFO - root -   Epoch: [171/500][320/346], lr: 0.00000345 	 loss = 0.0074(0.4432)
2023/09/15 09:13:07 - INFO - root -   Epoch: [171/500][340/346], lr: 0.00000345 	 loss = 0.9881(0.4326)
2023/09/15 09:13:09 - INFO - root -   Epoch: [171/500] 	 loss = 0.4303
2023/09/15 09:13:09 - INFO - root -   train_accuracy = 0.8454
2023/09/15 09:13:18 - INFO - root -   Epoch: [172/500][0/346], lr: 0.00000346 	 loss = 0.1046(0.1046)
2023/09/15 09:13:35 - INFO - root -   Epoch: [172/500][20/346], lr: 0.00000346 	 loss = 0.3874(0.3502)
2023/09/15 09:13:55 - INFO - root -   Epoch: [172/500][40/346], lr: 0.00000346 	 loss = 0.1482(0.3111)
2023/09/15 09:14:12 - INFO - root -   Epoch: [172/500][60/346], lr: 0.00000346 	 loss = 0.0938(0.3084)
2023/09/15 09:14:32 - INFO - root -   Epoch: [172/500][80/346], lr: 0.00000346 	 loss = 0.2502(0.3011)
2023/09/15 09:14:49 - INFO - root -   Epoch: [172/500][100/346], lr: 0.00000346 	 loss = 0.0506(0.3071)
2023/09/15 09:15:09 - INFO - root -   Epoch: [172/500][120/346], lr: 0.00000346 	 loss = 0.1704(0.3123)
2023/09/15 09:15:26 - INFO - root -   Epoch: [172/500][140/346], lr: 0.00000346 	 loss = 0.2189(0.3168)
2023/09/15 09:15:46 - INFO - root -   Epoch: [172/500][160/346], lr: 0.00000346 	 loss = 2.2679(0.3075)
2023/09/15 09:16:03 - INFO - root -   Epoch: [172/500][180/346], lr: 0.00000346 	 loss = 0.0346(0.3173)
2023/09/15 09:16:23 - INFO - root -   Epoch: [172/500][200/346], lr: 0.00000346 	 loss = 0.0003(0.3097)
2023/09/15 09:16:40 - INFO - root -   Epoch: [172/500][220/346], lr: 0.00000346 	 loss = 0.9889(0.3227)
2023/09/15 09:17:01 - INFO - root -   Epoch: [172/500][240/346], lr: 0.00000346 	 loss = 0.1160(0.3143)
2023/09/15 09:17:17 - INFO - root -   Epoch: [172/500][260/346], lr: 0.00000346 	 loss = 0.7700(0.3244)
2023/09/15 09:17:38 - INFO - root -   Epoch: [172/500][280/346], lr: 0.00000346 	 loss = 0.1240(0.3242)
2023/09/15 09:17:55 - INFO - root -   Epoch: [172/500][300/346], lr: 0.00000346 	 loss = 0.6848(0.3267)
2023/09/15 09:18:15 - INFO - root -   Epoch: [172/500][320/346], lr: 0.00000346 	 loss = 0.0237(0.3366)
2023/09/15 09:18:32 - INFO - root -   Epoch: [172/500][340/346], lr: 0.00000346 	 loss = 1.2514(0.3615)
2023/09/15 09:18:34 - INFO - root -   Epoch: [172/500] 	 loss = 0.3607
2023/09/15 09:18:34 - INFO - root -   train_accuracy = 0.8699
2023/09/15 09:18:42 - INFO - root -   Epoch: [173/500][0/346], lr: 0.00000347 	 loss = 0.3845(0.3845)
2023/09/15 09:18:58 - INFO - root -   Epoch: [173/500][20/346], lr: 0.00000347 	 loss = 0.1494(0.4162)
2023/09/15 09:19:19 - INFO - root -   Epoch: [173/500][40/346], lr: 0.00000347 	 loss = 0.1453(0.4554)
2023/09/15 09:19:36 - INFO - root -   Epoch: [173/500][60/346], lr: 0.00000347 	 loss = 0.4805(0.4305)
2023/09/15 09:19:57 - INFO - root -   Epoch: [173/500][80/346], lr: 0.00000347 	 loss = 0.0810(0.4025)
2023/09/15 09:20:13 - INFO - root -   Epoch: [173/500][100/346], lr: 0.00000347 	 loss = 0.4134(0.4008)
2023/09/15 09:20:34 - INFO - root -   Epoch: [173/500][120/346], lr: 0.00000347 	 loss = 0.1245(0.4028)
2023/09/15 09:20:51 - INFO - root -   Epoch: [173/500][140/346], lr: 0.00000347 	 loss = 0.2008(0.3852)
2023/09/15 09:21:12 - INFO - root -   Epoch: [173/500][160/346], lr: 0.00000347 	 loss = 3.3528(0.4252)
2023/09/15 09:21:29 - INFO - root -   Epoch: [173/500][180/346], lr: 0.00000347 	 loss = 0.4347(0.4281)
2023/09/15 09:21:49 - INFO - root -   Epoch: [173/500][200/346], lr: 0.00000347 	 loss = 0.0007(0.4069)
2023/09/15 09:22:06 - INFO - root -   Epoch: [173/500][220/346], lr: 0.00000347 	 loss = 0.5616(0.4033)
2023/09/15 09:22:27 - INFO - root -   Epoch: [173/500][240/346], lr: 0.00000347 	 loss = 0.2188(0.3883)
2023/09/15 09:22:44 - INFO - root -   Epoch: [173/500][260/346], lr: 0.00000347 	 loss = 2.5814(0.4076)
2023/09/15 09:23:05 - INFO - root -   Epoch: [173/500][280/346], lr: 0.00000347 	 loss = 0.1637(0.4008)
2023/09/15 09:23:21 - INFO - root -   Epoch: [173/500][300/346], lr: 0.00000347 	 loss = 0.1826(0.3903)
2023/09/15 09:23:42 - INFO - root -   Epoch: [173/500][320/346], lr: 0.00000347 	 loss = 0.0176(0.3880)
2023/09/15 09:23:58 - INFO - root -   Epoch: [173/500][340/346], lr: 0.00000347 	 loss = 0.5487(0.3812)
2023/09/15 09:24:00 - INFO - root -   Epoch: [173/500] 	 loss = 0.3790
2023/09/15 09:24:00 - INFO - root -   train_accuracy = 0.8671
2023/09/15 09:24:08 - INFO - root -   Epoch: [174/500][0/346], lr: 0.00000349 	 loss = 0.4868(0.4868)
2023/09/15 09:24:24 - INFO - root -   Epoch: [174/500][20/346], lr: 0.00000349 	 loss = 0.4793(0.5617)
2023/09/15 09:24:45 - INFO - root -   Epoch: [174/500][40/346], lr: 0.00000349 	 loss = 0.1498(0.4937)
2023/09/15 09:25:02 - INFO - root -   Epoch: [174/500][60/346], lr: 0.00000349 	 loss = 0.0251(0.4784)
2023/09/15 09:25:23 - INFO - root -   Epoch: [174/500][80/346], lr: 0.00000349 	 loss = 0.5289(0.4503)
2023/09/15 09:25:39 - INFO - root -   Epoch: [174/500][100/346], lr: 0.00000349 	 loss = 0.0813(0.4715)
2023/09/15 09:26:00 - INFO - root -   Epoch: [174/500][120/346], lr: 0.00000349 	 loss = 0.2643(0.4582)
2023/09/15 09:26:17 - INFO - root -   Epoch: [174/500][140/346], lr: 0.00000349 	 loss = 0.1838(0.4360)
2023/09/15 09:26:38 - INFO - root -   Epoch: [174/500][160/346], lr: 0.00000349 	 loss = 1.7338(0.4334)
2023/09/15 09:26:55 - INFO - root -   Epoch: [174/500][180/346], lr: 0.00000349 	 loss = 0.3396(0.4352)
2023/09/15 09:27:15 - INFO - root -   Epoch: [174/500][200/346], lr: 0.00000349 	 loss = 0.0000(0.4127)
2023/09/15 09:27:32 - INFO - root -   Epoch: [174/500][220/346], lr: 0.00000349 	 loss = 0.3207(0.4289)
2023/09/15 09:27:53 - INFO - root -   Epoch: [174/500][240/346], lr: 0.00000349 	 loss = 1.3971(0.4606)
2023/09/15 09:28:10 - INFO - root -   Epoch: [174/500][260/346], lr: 0.00000349 	 loss = 0.6828(0.6264)
2023/09/15 09:28:31 - INFO - root -   Epoch: [174/500][280/346], lr: 0.00000349 	 loss = 2.7754(0.6686)
2023/09/15 09:28:47 - INFO - root -   Epoch: [174/500][300/346], lr: 0.00000349 	 loss = 1.8285(0.7003)
2023/09/15 09:29:08 - INFO - root -   Epoch: [174/500][320/346], lr: 0.00000349 	 loss = 0.1250(0.7141)
2023/09/15 09:29:24 - INFO - root -   Epoch: [174/500][340/346], lr: 0.00000349 	 loss = 1.4083(0.7160)
2023/09/15 09:29:26 - INFO - root -   Epoch: [174/500] 	 loss = 0.7112
2023/09/15 09:30:49 - INFO - root -   precision = 0.4713
2023/09/15 09:30:49 - INFO - root -   eval_loss = 1.4068
2023/09/15 09:30:50 - INFO - root -   train_accuracy = 0.7818
2023/09/15 09:30:58 - INFO - root -   Epoch: [175/500][0/346], lr: 0.00000350 	 loss = 1.5942(1.5942)
2023/09/15 09:31:15 - INFO - root -   Epoch: [175/500][20/346], lr: 0.00000350 	 loss = 0.5230(0.6932)
2023/09/15 09:31:36 - INFO - root -   Epoch: [175/500][40/346], lr: 0.00000350 	 loss = 1.3010(0.6490)
2023/09/15 09:31:52 - INFO - root -   Epoch: [175/500][60/346], lr: 0.00000350 	 loss = 0.3900(0.6466)
2023/09/15 09:32:13 - INFO - root -   Epoch: [175/500][80/346], lr: 0.00000350 	 loss = 0.7399(0.6459)
2023/09/15 09:32:30 - INFO - root -   Epoch: [175/500][100/346], lr: 0.00000350 	 loss = 0.0453(0.5982)
2023/09/15 09:32:51 - INFO - root -   Epoch: [175/500][120/346], lr: 0.00000350 	 loss = 0.4124(0.5900)
2023/09/15 09:33:07 - INFO - root -   Epoch: [175/500][140/346], lr: 0.00000350 	 loss = 0.4628(0.5597)
2023/09/15 09:33:28 - INFO - root -   Epoch: [175/500][160/346], lr: 0.00000350 	 loss = 1.5398(0.5360)
2023/09/15 09:33:45 - INFO - root -   Epoch: [175/500][180/346], lr: 0.00000350 	 loss = 0.3075(0.5503)
2023/09/15 09:34:06 - INFO - root -   Epoch: [175/500][200/346], lr: 0.00000350 	 loss = 0.0005(0.5263)
2023/09/15 09:34:22 - INFO - root -   Epoch: [175/500][220/346], lr: 0.00000350 	 loss = 1.1056(0.5246)
2023/09/15 09:34:43 - INFO - root -   Epoch: [175/500][240/346], lr: 0.00000350 	 loss = 0.0868(0.5068)
2023/09/15 09:35:00 - INFO - root -   Epoch: [175/500][260/346], lr: 0.00000350 	 loss = 0.3691(0.4925)
2023/09/15 09:35:20 - INFO - root -   Epoch: [175/500][280/346], lr: 0.00000350 	 loss = 0.8268(0.4860)
2023/09/15 09:35:37 - INFO - root -   Epoch: [175/500][300/346], lr: 0.00000350 	 loss = 0.2652(0.4777)
2023/09/15 09:35:58 - INFO - root -   Epoch: [175/500][320/346], lr: 0.00000350 	 loss = 0.0038(0.4831)
2023/09/15 09:36:15 - INFO - root -   Epoch: [175/500][340/346], lr: 0.00000350 	 loss = 0.4481(0.4752)
2023/09/15 09:36:17 - INFO - root -   Epoch: [175/500] 	 loss = 0.4709
2023/09/15 09:36:17 - INFO - root -   train_accuracy = 0.8266
2023/09/15 09:36:25 - INFO - root -   Epoch: [176/500][0/346], lr: 0.00000352 	 loss = 0.2787(0.2787)
2023/09/15 09:36:41 - INFO - root -   Epoch: [176/500][20/346], lr: 0.00000352 	 loss = 0.3552(0.3029)
2023/09/15 09:37:02 - INFO - root -   Epoch: [176/500][40/346], lr: 0.00000352 	 loss = 0.3093(0.3367)
2023/09/15 09:37:19 - INFO - root -   Epoch: [176/500][60/346], lr: 0.00000352 	 loss = 0.0529(0.3393)
2023/09/15 09:37:40 - INFO - root -   Epoch: [176/500][80/346], lr: 0.00000352 	 loss = 0.4258(0.3530)
2023/09/15 09:37:56 - INFO - root -   Epoch: [176/500][100/346], lr: 0.00000352 	 loss = 0.0890(0.3664)
2023/09/15 09:38:17 - INFO - root -   Epoch: [176/500][120/346], lr: 0.00000352 	 loss = 0.1706(0.3573)
2023/09/15 09:38:34 - INFO - root -   Epoch: [176/500][140/346], lr: 0.00000352 	 loss = 0.1383(0.3487)
2023/09/15 09:38:55 - INFO - root -   Epoch: [176/500][160/346], lr: 0.00000352 	 loss = 1.6580(0.3468)
2023/09/15 09:39:12 - INFO - root -   Epoch: [176/500][180/346], lr: 0.00000352 	 loss = 0.0329(0.3629)
2023/09/15 09:39:33 - INFO - root -   Epoch: [176/500][200/346], lr: 0.00000352 	 loss = 0.0002(0.3602)
2023/09/15 09:39:49 - INFO - root -   Epoch: [176/500][220/346], lr: 0.00000352 	 loss = 0.8340(0.3703)
2023/09/15 09:40:10 - INFO - root -   Epoch: [176/500][240/346], lr: 0.00000352 	 loss = 0.3168(0.3905)
2023/09/15 09:40:27 - INFO - root -   Epoch: [176/500][260/346], lr: 0.00000352 	 loss = 0.9006(0.4315)
2023/09/15 09:40:47 - INFO - root -   Epoch: [176/500][280/346], lr: 0.00000352 	 loss = 0.6013(0.4273)
2023/09/15 09:41:04 - INFO - root -   Epoch: [176/500][300/346], lr: 0.00000352 	 loss = 0.2613(0.4158)
2023/09/15 09:41:25 - INFO - root -   Epoch: [176/500][320/346], lr: 0.00000352 	 loss = 0.0087(0.4058)
2023/09/15 09:41:41 - INFO - root -   Epoch: [176/500][340/346], lr: 0.00000352 	 loss = 0.4856(0.3973)
2023/09/15 09:41:43 - INFO - root -   Epoch: [176/500] 	 loss = 0.3941
2023/09/15 09:41:43 - INFO - root -   train_accuracy = 0.8540
2023/09/15 09:41:52 - INFO - root -   Epoch: [177/500][0/346], lr: 0.00000353 	 loss = 0.3994(0.3994)
2023/09/15 09:42:08 - INFO - root -   Epoch: [177/500][20/346], lr: 0.00000353 	 loss = 0.3819(0.3533)
2023/09/15 09:42:29 - INFO - root -   Epoch: [177/500][40/346], lr: 0.00000353 	 loss = 0.4768(0.3242)
2023/09/15 09:42:45 - INFO - root -   Epoch: [177/500][60/346], lr: 0.00000353 	 loss = 0.0966(0.3210)
2023/09/15 09:43:06 - INFO - root -   Epoch: [177/500][80/346], lr: 0.00000353 	 loss = 0.5309(0.3025)
2023/09/15 09:43:23 - INFO - root -   Epoch: [177/500][100/346], lr: 0.00000353 	 loss = 0.0474(0.3245)
2023/09/15 09:43:44 - INFO - root -   Epoch: [177/500][120/346], lr: 0.00000353 	 loss = 0.0899(0.3203)
2023/09/15 09:44:00 - INFO - root -   Epoch: [177/500][140/346], lr: 0.00000353 	 loss = 0.6328(0.3128)
2023/09/15 09:44:21 - INFO - root -   Epoch: [177/500][160/346], lr: 0.00000353 	 loss = 2.7103(0.3723)
2023/09/15 09:44:38 - INFO - root -   Epoch: [177/500][180/346], lr: 0.00000353 	 loss = 0.0261(0.3730)
2023/09/15 09:44:59 - INFO - root -   Epoch: [177/500][200/346], lr: 0.00000353 	 loss = 0.0004(0.3601)
2023/09/15 09:45:15 - INFO - root -   Epoch: [177/500][220/346], lr: 0.00000353 	 loss = 0.6852(0.3553)
2023/09/15 09:45:37 - INFO - root -   Epoch: [177/500][240/346], lr: 0.00000353 	 loss = 0.0502(0.3614)
2023/09/15 09:45:53 - INFO - root -   Epoch: [177/500][260/346], lr: 0.00000353 	 loss = 1.2037(0.3698)
2023/09/15 09:46:14 - INFO - root -   Epoch: [177/500][280/346], lr: 0.00000353 	 loss = 0.2397(0.3609)
2023/09/15 09:46:30 - INFO - root -   Epoch: [177/500][300/346], lr: 0.00000353 	 loss = 0.6962(0.3540)
2023/09/15 09:46:51 - INFO - root -   Epoch: [177/500][320/346], lr: 0.00000353 	 loss = 0.0385(0.3484)
2023/09/15 09:47:07 - INFO - root -   Epoch: [177/500][340/346], lr: 0.00000353 	 loss = 0.2901(0.3442)
2023/09/15 09:47:09 - INFO - root -   Epoch: [177/500] 	 loss = 0.3417
2023/09/15 09:47:09 - INFO - root -   train_accuracy = 0.8786
2023/09/15 09:47:17 - INFO - root -   Epoch: [178/500][0/346], lr: 0.00000355 	 loss = 0.2806(0.2806)
2023/09/15 09:47:33 - INFO - root -   Epoch: [178/500][20/346], lr: 0.00000355 	 loss = 0.1625(0.3672)
2023/09/15 09:47:54 - INFO - root -   Epoch: [178/500][40/346], lr: 0.00000355 	 loss = 0.6905(0.3409)
2023/09/15 09:48:11 - INFO - root -   Epoch: [178/500][60/346], lr: 0.00000355 	 loss = 0.0731(0.3217)
2023/09/15 09:48:32 - INFO - root -   Epoch: [178/500][80/346], lr: 0.00000355 	 loss = 0.5530(0.3351)
2023/09/15 09:48:49 - INFO - root -   Epoch: [178/500][100/346], lr: 0.00000355 	 loss = 0.0390(0.3615)
2023/09/15 09:49:10 - INFO - root -   Epoch: [178/500][120/346], lr: 0.00000355 	 loss = 0.0589(0.3451)
2023/09/15 09:49:26 - INFO - root -   Epoch: [178/500][140/346], lr: 0.00000355 	 loss = 0.0320(0.3615)
2023/09/15 09:49:47 - INFO - root -   Epoch: [178/500][160/346], lr: 0.00000355 	 loss = 1.8142(0.3463)
2023/09/15 09:50:04 - INFO - root -   Epoch: [178/500][180/346], lr: 0.00000355 	 loss = 0.0777(0.3553)
2023/09/15 09:50:25 - INFO - root -   Epoch: [178/500][200/346], lr: 0.00000355 	 loss = 0.0004(0.3463)
2023/09/15 09:50:42 - INFO - root -   Epoch: [178/500][220/346], lr: 0.00000355 	 loss = 0.1690(0.3577)
2023/09/15 09:51:03 - INFO - root -   Epoch: [178/500][240/346], lr: 0.00000355 	 loss = 0.0703(0.3414)
2023/09/15 09:51:20 - INFO - root -   Epoch: [178/500][260/346], lr: 0.00000355 	 loss = 0.1340(0.3369)
2023/09/15 09:51:41 - INFO - root -   Epoch: [178/500][280/346], lr: 0.00000355 	 loss = 0.3193(0.3374)
2023/09/15 09:51:57 - INFO - root -   Epoch: [178/500][300/346], lr: 0.00000355 	 loss = 0.2927(0.3306)
2023/09/15 09:52:18 - INFO - root -   Epoch: [178/500][320/346], lr: 0.00000355 	 loss = 0.0049(0.3276)
2023/09/15 09:52:35 - INFO - root -   Epoch: [178/500][340/346], lr: 0.00000355 	 loss = 0.3857(0.3414)
2023/09/15 09:52:37 - INFO - root -   Epoch: [178/500] 	 loss = 0.3403
2023/09/15 09:52:37 - INFO - root -   train_accuracy = 0.8699
2023/09/15 09:52:45 - INFO - root -   Epoch: [179/500][0/346], lr: 0.00000356 	 loss = 1.0923(1.0923)
2023/09/15 09:53:02 - INFO - root -   Epoch: [179/500][20/346], lr: 0.00000356 	 loss = 0.5214(0.8852)
2023/09/15 09:53:23 - INFO - root -   Epoch: [179/500][40/346], lr: 0.00000356 	 loss = 1.0857(0.6145)
2023/09/15 09:53:39 - INFO - root -   Epoch: [179/500][60/346], lr: 0.00000356 	 loss = 0.1777(0.5150)
2023/09/15 09:54:00 - INFO - root -   Epoch: [179/500][80/346], lr: 0.00000356 	 loss = 0.3017(0.4739)
2023/09/15 09:54:17 - INFO - root -   Epoch: [179/500][100/346], lr: 0.00000356 	 loss = 0.0295(0.4232)
2023/09/15 09:54:38 - INFO - root -   Epoch: [179/500][120/346], lr: 0.00000356 	 loss = 0.0496(0.3939)
2023/09/15 09:54:55 - INFO - root -   Epoch: [179/500][140/346], lr: 0.00000356 	 loss = 0.0454(0.3652)
2023/09/15 09:55:15 - INFO - root -   Epoch: [179/500][160/346], lr: 0.00000356 	 loss = 1.6663(0.3577)
2023/09/15 09:55:32 - INFO - root -   Epoch: [179/500][180/346], lr: 0.00000356 	 loss = 0.0220(0.3524)
2023/09/15 09:55:53 - INFO - root -   Epoch: [179/500][200/346], lr: 0.00000356 	 loss = 0.0004(0.3310)
2023/09/15 09:56:10 - INFO - root -   Epoch: [179/500][220/346], lr: 0.00000356 	 loss = 0.1480(0.3386)
2023/09/15 09:56:31 - INFO - root -   Epoch: [179/500][240/346], lr: 0.00000356 	 loss = 0.0098(0.3267)
2023/09/15 09:56:47 - INFO - root -   Epoch: [179/500][260/346], lr: 0.00000356 	 loss = 1.1482(0.3293)
2023/09/15 09:57:08 - INFO - root -   Epoch: [179/500][280/346], lr: 0.00000356 	 loss = 0.3853(0.3221)
2023/09/15 09:57:24 - INFO - root -   Epoch: [179/500][300/346], lr: 0.00000356 	 loss = 0.1361(0.3106)
2023/09/15 09:57:45 - INFO - root -   Epoch: [179/500][320/346], lr: 0.00000356 	 loss = 0.0032(0.3315)
2023/09/15 09:58:02 - INFO - root -   Epoch: [179/500][340/346], lr: 0.00000356 	 loss = 0.1599(0.3223)
2023/09/15 09:58:04 - INFO - root -   Epoch: [179/500] 	 loss = 0.3213
2023/09/15 09:59:26 - INFO - root -   precision = 0.5345
2023/09/15 09:59:26 - INFO - root -   eval_loss = 2.6988
2023/09/15 09:59:27 - INFO - root -   train_accuracy = 0.8902
2023/09/15 09:59:36 - INFO - root -   Epoch: [180/500][0/346], lr: 0.00000358 	 loss = 1.5124(1.5124)
2023/09/15 09:59:52 - INFO - root -   Epoch: [180/500][20/346], lr: 0.00000358 	 loss = 0.1352(0.8138)
2023/09/15 10:00:13 - INFO - root -   Epoch: [180/500][40/346], lr: 0.00000358 	 loss = 0.2923(0.6292)
2023/09/15 10:00:31 - INFO - root -   Epoch: [180/500][60/346], lr: 0.00000358 	 loss = 6.3133(0.7092)
2023/09/15 10:00:50 - INFO - root -   Epoch: [180/500][80/346], lr: 0.00000358 	 loss = 0.2241(0.6300)
2023/09/15 10:01:07 - INFO - root -   Epoch: [180/500][100/346], lr: 0.00000358 	 loss = 0.0263(0.5869)
2023/09/15 10:01:27 - INFO - root -   Epoch: [180/500][120/346], lr: 0.00000358 	 loss = 0.4612(0.5476)
2023/09/15 10:01:44 - INFO - root -   Epoch: [180/500][140/346], lr: 0.00000358 	 loss = 0.1941(0.5091)
2023/09/15 10:02:03 - INFO - root -   Epoch: [180/500][160/346], lr: 0.00000358 	 loss = 2.3761(0.4949)
2023/09/15 10:02:20 - INFO - root -   Epoch: [180/500][180/346], lr: 0.00000358 	 loss = 0.1137(0.4696)
2023/09/15 10:02:40 - INFO - root -   Epoch: [180/500][200/346], lr: 0.00000358 	 loss = 0.0003(0.4358)
2023/09/15 10:02:57 - INFO - root -   Epoch: [180/500][220/346], lr: 0.00000358 	 loss = 0.5872(0.4257)
2023/09/15 10:03:17 - INFO - root -   Epoch: [180/500][240/346], lr: 0.00000358 	 loss = 0.0310(0.4114)
2023/09/15 10:03:34 - INFO - root -   Epoch: [180/500][260/346], lr: 0.00000358 	 loss = 0.8482(0.4189)
2023/09/15 10:03:54 - INFO - root -   Epoch: [180/500][280/346], lr: 0.00000358 	 loss = 0.2424(0.4067)
2023/09/15 10:04:11 - INFO - root -   Epoch: [180/500][300/346], lr: 0.00000358 	 loss = 0.1894(0.3922)
2023/09/15 10:04:31 - INFO - root -   Epoch: [180/500][320/346], lr: 0.00000358 	 loss = 0.0147(0.3809)
2023/09/15 10:04:47 - INFO - root -   Epoch: [180/500][340/346], lr: 0.00000358 	 loss = 0.2070(0.3700)
2023/09/15 10:04:49 - INFO - root -   Epoch: [180/500] 	 loss = 0.3674
2023/09/15 10:04:49 - INFO - root -   train_accuracy = 0.8728
2023/09/15 10:04:57 - INFO - root -   Epoch: [181/500][0/346], lr: 0.00000359 	 loss = 0.1873(0.1873)
2023/09/15 10:05:14 - INFO - root -   Epoch: [181/500][20/346], lr: 0.00000359 	 loss = 0.1616(0.2587)
2023/09/15 10:05:35 - INFO - root -   Epoch: [181/500][40/346], lr: 0.00000359 	 loss = 0.0910(0.2341)
2023/09/15 10:05:51 - INFO - root -   Epoch: [181/500][60/346], lr: 0.00000359 	 loss = 0.0629(0.2949)
2023/09/15 10:06:12 - INFO - root -   Epoch: [181/500][80/346], lr: 0.00000359 	 loss = 0.1250(0.2849)
2023/09/15 10:06:29 - INFO - root -   Epoch: [181/500][100/346], lr: 0.00000359 	 loss = 0.0390(0.2908)
2023/09/15 10:06:49 - INFO - root -   Epoch: [181/500][120/346], lr: 0.00000359 	 loss = 0.1445(0.3034)
2023/09/15 10:07:06 - INFO - root -   Epoch: [181/500][140/346], lr: 0.00000359 	 loss = 0.2630(0.2912)
2023/09/15 10:07:27 - INFO - root -   Epoch: [181/500][160/346], lr: 0.00000359 	 loss = 2.0511(0.2796)
2023/09/15 10:07:44 - INFO - root -   Epoch: [181/500][180/346], lr: 0.00000359 	 loss = 0.0197(0.2899)
2023/09/15 10:08:05 - INFO - root -   Epoch: [181/500][200/346], lr: 0.00000359 	 loss = 0.0002(0.2961)
2023/09/15 10:08:21 - INFO - root -   Epoch: [181/500][220/346], lr: 0.00000359 	 loss = 0.3646(0.3245)
2023/09/15 10:08:42 - INFO - root -   Epoch: [181/500][240/346], lr: 0.00000359 	 loss = 0.1177(0.3245)
2023/09/15 10:08:59 - INFO - root -   Epoch: [181/500][260/346], lr: 0.00000359 	 loss = 0.5683(0.3399)
2023/09/15 10:09:20 - INFO - root -   Epoch: [181/500][280/346], lr: 0.00000359 	 loss = 0.3405(0.3289)
2023/09/15 10:09:36 - INFO - root -   Epoch: [181/500][300/346], lr: 0.00000359 	 loss = 0.4455(0.3221)
2023/09/15 10:09:57 - INFO - root -   Epoch: [181/500][320/346], lr: 0.00000359 	 loss = 0.0066(0.3202)
2023/09/15 10:10:13 - INFO - root -   Epoch: [181/500][340/346], lr: 0.00000359 	 loss = 0.2275(0.3280)
2023/09/15 10:10:15 - INFO - root -   Epoch: [181/500] 	 loss = 0.3251
2023/09/15 10:10:15 - INFO - root -   train_accuracy = 0.8801
2023/09/15 10:10:24 - INFO - root -   Epoch: [182/500][0/346], lr: 0.00000360 	 loss = 0.2238(0.2238)
2023/09/15 10:10:40 - INFO - root -   Epoch: [182/500][20/346], lr: 0.00000360 	 loss = 0.1991(0.2363)
2023/09/15 10:11:01 - INFO - root -   Epoch: [182/500][40/346], lr: 0.00000360 	 loss = 0.0795(0.1826)
2023/09/15 10:11:18 - INFO - root -   Epoch: [182/500][60/346], lr: 0.00000360 	 loss = 0.0281(0.2111)
2023/09/15 10:11:39 - INFO - root -   Epoch: [182/500][80/346], lr: 0.00000360 	 loss = 0.1119(0.2340)
2023/09/15 10:11:55 - INFO - root -   Epoch: [182/500][100/346], lr: 0.00000360 	 loss = 0.0047(0.2452)
2023/09/15 10:12:16 - INFO - root -   Epoch: [182/500][120/346], lr: 0.00000360 	 loss = 0.1881(0.2915)
2023/09/15 10:12:33 - INFO - root -   Epoch: [182/500][140/346], lr: 0.00000360 	 loss = 0.0517(0.2850)
2023/09/15 10:12:54 - INFO - root -   Epoch: [182/500][160/346], lr: 0.00000360 	 loss = 1.4111(0.2837)
2023/09/15 10:13:10 - INFO - root -   Epoch: [182/500][180/346], lr: 0.00000360 	 loss = 0.1022(0.2889)
2023/09/15 10:13:31 - INFO - root -   Epoch: [182/500][200/346], lr: 0.00000360 	 loss = 0.0002(0.2851)
2023/09/15 10:13:48 - INFO - root -   Epoch: [182/500][220/346], lr: 0.00000360 	 loss = 0.2288(0.3218)
2023/09/15 10:14:09 - INFO - root -   Epoch: [182/500][240/346], lr: 0.00000360 	 loss = 0.1087(0.3348)
2023/09/15 10:14:25 - INFO - root -   Epoch: [182/500][260/346], lr: 0.00000360 	 loss = 0.2783(0.3290)
2023/09/15 10:14:46 - INFO - root -   Epoch: [182/500][280/346], lr: 0.00000360 	 loss = 0.7245(0.3298)
2023/09/15 10:15:03 - INFO - root -   Epoch: [182/500][300/346], lr: 0.00000360 	 loss = 4.2337(0.3301)
2023/09/15 10:15:24 - INFO - root -   Epoch: [182/500][320/346], lr: 0.00000360 	 loss = 0.0074(0.3270)
2023/09/15 10:15:40 - INFO - root -   Epoch: [182/500][340/346], lr: 0.00000360 	 loss = 0.2051(0.3197)
2023/09/15 10:15:42 - INFO - root -   Epoch: [182/500] 	 loss = 0.3169
2023/09/15 10:15:42 - INFO - root -   train_accuracy = 0.8844
2023/09/15 10:15:51 - INFO - root -   Epoch: [183/500][0/346], lr: 0.00000362 	 loss = 1.3207(1.3207)
2023/09/15 10:16:07 - INFO - root -   Epoch: [183/500][20/346], lr: 0.00000362 	 loss = 0.0649(0.2471)
2023/09/15 10:16:28 - INFO - root -   Epoch: [183/500][40/346], lr: 0.00000362 	 loss = 0.1289(0.2007)
2023/09/15 10:16:44 - INFO - root -   Epoch: [183/500][60/346], lr: 0.00000362 	 loss = 0.0055(0.2125)
2023/09/15 10:17:05 - INFO - root -   Epoch: [183/500][80/346], lr: 0.00000362 	 loss = 0.6608(0.2216)
2023/09/15 10:17:22 - INFO - root -   Epoch: [183/500][100/346], lr: 0.00000362 	 loss = 0.2863(0.2701)
2023/09/15 10:17:43 - INFO - root -   Epoch: [183/500][120/346], lr: 0.00000362 	 loss = 0.1136(0.2831)
2023/09/15 10:17:59 - INFO - root -   Epoch: [183/500][140/346], lr: 0.00000362 	 loss = 0.1583(0.2768)
2023/09/15 10:18:20 - INFO - root -   Epoch: [183/500][160/346], lr: 0.00000362 	 loss = 1.4808(0.2716)
2023/09/15 10:18:36 - INFO - root -   Epoch: [183/500][180/346], lr: 0.00000362 	 loss = 0.0508(0.2727)
2023/09/15 10:18:57 - INFO - root -   Epoch: [183/500][200/346], lr: 0.00000362 	 loss = 0.0000(0.2606)
2023/09/15 10:19:14 - INFO - root -   Epoch: [183/500][220/346], lr: 0.00000362 	 loss = 0.4065(0.2636)
2023/09/15 10:19:35 - INFO - root -   Epoch: [183/500][240/346], lr: 0.00000362 	 loss = 0.0098(0.2718)
2023/09/15 10:19:52 - INFO - root -   Epoch: [183/500][260/346], lr: 0.00000362 	 loss = 0.6656(0.2709)
2023/09/15 10:20:13 - INFO - root -   Epoch: [183/500][280/346], lr: 0.00000362 	 loss = 0.7359(0.2689)
2023/09/15 10:20:29 - INFO - root -   Epoch: [183/500][300/346], lr: 0.00000362 	 loss = 0.0433(0.2653)
2023/09/15 10:20:50 - INFO - root -   Epoch: [183/500][320/346], lr: 0.00000362 	 loss = 0.0032(0.2674)
2023/09/15 10:21:07 - INFO - root -   Epoch: [183/500][340/346], lr: 0.00000362 	 loss = 0.3583(0.2735)
2023/09/15 10:21:09 - INFO - root -   Epoch: [183/500] 	 loss = 0.2708
2023/09/15 10:21:09 - INFO - root -   train_accuracy = 0.9032
2023/09/15 10:21:17 - INFO - root -   Epoch: [184/500][0/346], lr: 0.00000363 	 loss = 0.5622(0.5622)
2023/09/15 10:21:34 - INFO - root -   Epoch: [184/500][20/346], lr: 0.00000363 	 loss = 0.3607(0.3452)
2023/09/15 10:21:55 - INFO - root -   Epoch: [184/500][40/346], lr: 0.00000363 	 loss = 0.1102(0.2169)
2023/09/15 10:22:11 - INFO - root -   Epoch: [184/500][60/346], lr: 0.00000363 	 loss = 0.0141(0.2297)
2023/09/15 10:22:32 - INFO - root -   Epoch: [184/500][80/346], lr: 0.00000363 	 loss = 0.0784(0.2277)
2023/09/15 10:22:49 - INFO - root -   Epoch: [184/500][100/346], lr: 0.00000363 	 loss = 0.0183(0.2213)
2023/09/15 10:23:09 - INFO - root -   Epoch: [184/500][120/346], lr: 0.00000363 	 loss = 0.4451(0.2350)
2023/09/15 10:23:26 - INFO - root -   Epoch: [184/500][140/346], lr: 0.00000363 	 loss = 0.0048(0.2456)
2023/09/15 10:23:47 - INFO - root -   Epoch: [184/500][160/346], lr: 0.00000363 	 loss = 2.5057(0.2404)
2023/09/15 10:24:04 - INFO - root -   Epoch: [184/500][180/346], lr: 0.00000363 	 loss = 0.0195(0.2482)
2023/09/15 10:24:25 - INFO - root -   Epoch: [184/500][200/346], lr: 0.00000363 	 loss = 0.1650(0.2390)
2023/09/15 10:24:41 - INFO - root -   Epoch: [184/500][220/346], lr: 0.00000363 	 loss = 0.4872(0.2425)
2023/09/15 10:25:02 - INFO - root -   Epoch: [184/500][240/346], lr: 0.00000363 	 loss = 0.0999(0.2386)
2023/09/15 10:25:19 - INFO - root -   Epoch: [184/500][260/346], lr: 0.00000363 	 loss = 0.2800(0.2512)
2023/09/15 10:25:40 - INFO - root -   Epoch: [184/500][280/346], lr: 0.00000363 	 loss = 0.1427(0.2520)
2023/09/15 10:25:57 - INFO - root -   Epoch: [184/500][300/346], lr: 0.00000363 	 loss = 0.4140(0.2435)
2023/09/15 10:26:18 - INFO - root -   Epoch: [184/500][320/346], lr: 0.00000363 	 loss = 0.0011(0.2465)
2023/09/15 10:26:34 - INFO - root -   Epoch: [184/500][340/346], lr: 0.00000363 	 loss = 0.0737(0.2474)
2023/09/15 10:26:36 - INFO - root -   Epoch: [184/500] 	 loss = 0.2454
2023/09/15 10:27:59 - INFO - root -   precision = 0.4540
2023/09/15 10:27:59 - INFO - root -   eval_loss = 2.7162
2023/09/15 10:28:00 - INFO - root -   train_accuracy = 0.9147
2023/09/15 10:28:08 - INFO - root -   Epoch: [185/500][0/346], lr: 0.00000365 	 loss = 0.1437(0.1437)
2023/09/15 10:28:24 - INFO - root -   Epoch: [185/500][20/346], lr: 0.00000365 	 loss = 0.7320(0.3782)
2023/09/15 10:28:45 - INFO - root -   Epoch: [185/500][40/346], lr: 0.00000365 	 loss = 0.0993(0.3706)
2023/09/15 10:29:02 - INFO - root -   Epoch: [185/500][60/346], lr: 0.00000365 	 loss = 0.0102(0.3199)
2023/09/15 10:29:22 - INFO - root -   Epoch: [185/500][80/346], lr: 0.00000365 	 loss = 0.0423(0.3086)
2023/09/15 10:29:38 - INFO - root -   Epoch: [185/500][100/346], lr: 0.00000365 	 loss = 0.0307(0.3114)
2023/09/15 10:29:59 - INFO - root -   Epoch: [185/500][120/346], lr: 0.00000365 	 loss = 0.0470(0.3017)
2023/09/15 10:30:16 - INFO - root -   Epoch: [185/500][140/346], lr: 0.00000365 	 loss = 0.0200(0.2860)
2023/09/15 10:30:36 - INFO - root -   Epoch: [185/500][160/346], lr: 0.00000365 	 loss = 1.5029(0.3008)
2023/09/15 10:30:53 - INFO - root -   Epoch: [185/500][180/346], lr: 0.00000365 	 loss = 0.0485(0.2942)
2023/09/15 10:31:13 - INFO - root -   Epoch: [185/500][200/346], lr: 0.00000365 	 loss = 0.0002(0.2750)
2023/09/15 10:31:30 - INFO - root -   Epoch: [185/500][220/346], lr: 0.00000365 	 loss = 0.2887(0.2853)
2023/09/15 10:31:51 - INFO - root -   Epoch: [185/500][240/346], lr: 0.00000365 	 loss = 0.0405(0.2738)
2023/09/15 10:32:07 - INFO - root -   Epoch: [185/500][260/346], lr: 0.00000365 	 loss = 0.9257(0.2900)
2023/09/15 10:32:27 - INFO - root -   Epoch: [185/500][280/346], lr: 0.00000365 	 loss = 0.1459(0.2762)
2023/09/15 10:32:44 - INFO - root -   Epoch: [185/500][300/346], lr: 0.00000365 	 loss = 0.1702(0.2643)
2023/09/15 10:33:04 - INFO - root -   Epoch: [185/500][320/346], lr: 0.00000365 	 loss = 0.0110(0.2735)
2023/09/15 10:33:21 - INFO - root -   Epoch: [185/500][340/346], lr: 0.00000365 	 loss = 0.1625(0.2697)
2023/09/15 10:33:23 - INFO - root -   Epoch: [185/500] 	 loss = 0.2669
2023/09/15 10:33:23 - INFO - root -   train_accuracy = 0.9118
2023/09/15 10:33:31 - INFO - root -   Epoch: [186/500][0/346], lr: 0.00000366 	 loss = 0.8309(0.8309)
2023/09/15 10:33:47 - INFO - root -   Epoch: [186/500][20/346], lr: 0.00000366 	 loss = 0.0934(0.2508)
2023/09/15 10:34:08 - INFO - root -   Epoch: [186/500][40/346], lr: 0.00000366 	 loss = 0.0327(0.2076)
2023/09/15 10:34:24 - INFO - root -   Epoch: [186/500][60/346], lr: 0.00000366 	 loss = 0.0115(0.2165)
2023/09/15 10:34:45 - INFO - root -   Epoch: [186/500][80/346], lr: 0.00000366 	 loss = 0.1112(0.2059)
2023/09/15 10:35:01 - INFO - root -   Epoch: [186/500][100/346], lr: 0.00000366 	 loss = 0.3148(0.2199)
2023/09/15 10:35:22 - INFO - root -   Epoch: [186/500][120/346], lr: 0.00000366 	 loss = 0.1976(0.2582)
2023/09/15 10:35:38 - INFO - root -   Epoch: [186/500][140/346], lr: 0.00000366 	 loss = 0.0229(0.2522)
2023/09/15 10:35:58 - INFO - root -   Epoch: [186/500][160/346], lr: 0.00000366 	 loss = 2.4571(0.2489)
2023/09/15 10:36:15 - INFO - root -   Epoch: [186/500][180/346], lr: 0.00000366 	 loss = 0.0208(0.2745)
2023/09/15 10:36:35 - INFO - root -   Epoch: [186/500][200/346], lr: 0.00000366 	 loss = 0.0004(0.2642)
2023/09/15 10:36:52 - INFO - root -   Epoch: [186/500][220/346], lr: 0.00000366 	 loss = 0.1471(0.2636)
2023/09/15 10:37:12 - INFO - root -   Epoch: [186/500][240/346], lr: 0.00000366 	 loss = 0.0603(0.2653)
2023/09/15 10:37:29 - INFO - root -   Epoch: [186/500][260/346], lr: 0.00000366 	 loss = 0.1840(0.2687)
2023/09/15 10:37:49 - INFO - root -   Epoch: [186/500][280/346], lr: 0.00000366 	 loss = 0.1692(0.2589)
2023/09/15 10:38:06 - INFO - root -   Epoch: [186/500][300/346], lr: 0.00000366 	 loss = 0.0526(0.2539)
2023/09/15 10:38:26 - INFO - root -   Epoch: [186/500][320/346], lr: 0.00000366 	 loss = 0.0036(0.2551)
2023/09/15 10:38:42 - INFO - root -   Epoch: [186/500][340/346], lr: 0.00000366 	 loss = 0.0584(0.2488)
2023/09/15 10:38:44 - INFO - root -   Epoch: [186/500] 	 loss = 0.2486
2023/09/15 10:38:44 - INFO - root -   train_accuracy = 0.9205
2023/09/15 10:38:53 - INFO - root -   Epoch: [187/500][0/346], lr: 0.00000368 	 loss = 0.0362(0.0362)
2023/09/15 10:39:09 - INFO - root -   Epoch: [187/500][20/346], lr: 0.00000368 	 loss = 0.1805(0.1591)
2023/09/15 10:39:30 - INFO - root -   Epoch: [187/500][40/346], lr: 0.00000368 	 loss = 0.0625(0.1430)
2023/09/15 10:39:46 - INFO - root -   Epoch: [187/500][60/346], lr: 0.00000368 	 loss = 0.3614(0.1901)
2023/09/15 10:40:07 - INFO - root -   Epoch: [187/500][80/346], lr: 0.00000368 	 loss = 0.0509(0.2294)
2023/09/15 10:40:23 - INFO - root -   Epoch: [187/500][100/346], lr: 0.00000368 	 loss = 0.0284(0.3324)
2023/09/15 10:40:44 - INFO - root -   Epoch: [187/500][120/346], lr: 0.00000368 	 loss = 0.1431(0.3257)
2023/09/15 10:41:00 - INFO - root -   Epoch: [187/500][140/346], lr: 0.00000368 	 loss = 0.0129(0.3253)
2023/09/15 10:41:21 - INFO - root -   Epoch: [187/500][160/346], lr: 0.00000368 	 loss = 1.6249(0.3180)
2023/09/15 10:41:37 - INFO - root -   Epoch: [187/500][180/346], lr: 0.00000368 	 loss = 0.5317(0.3098)
2023/09/15 10:41:58 - INFO - root -   Epoch: [187/500][200/346], lr: 0.00000368 	 loss = 0.0019(0.2925)
2023/09/15 10:42:14 - INFO - root -   Epoch: [187/500][220/346], lr: 0.00000368 	 loss = 0.1109(0.2862)
2023/09/15 10:42:35 - INFO - root -   Epoch: [187/500][240/346], lr: 0.00000368 	 loss = 0.0209(0.2744)
2023/09/15 10:42:51 - INFO - root -   Epoch: [187/500][260/346], lr: 0.00000368 	 loss = 0.0983(0.2827)
2023/09/15 10:43:12 - INFO - root -   Epoch: [187/500][280/346], lr: 0.00000368 	 loss = 0.1748(0.2718)
2023/09/15 10:43:28 - INFO - root -   Epoch: [187/500][300/346], lr: 0.00000368 	 loss = 0.0074(0.2623)
2023/09/15 10:43:49 - INFO - root -   Epoch: [187/500][320/346], lr: 0.00000368 	 loss = 0.0032(0.2625)
2023/09/15 10:44:05 - INFO - root -   Epoch: [187/500][340/346], lr: 0.00000368 	 loss = 0.0603(0.2529)
2023/09/15 10:44:07 - INFO - root -   Epoch: [187/500] 	 loss = 0.2524
2023/09/15 10:44:07 - INFO - root -   train_accuracy = 0.9191
2023/09/15 10:44:16 - INFO - root -   Epoch: [188/500][0/346], lr: 0.00000369 	 loss = 0.0252(0.0252)
2023/09/15 10:44:32 - INFO - root -   Epoch: [188/500][20/346], lr: 0.00000369 	 loss = 0.0126(0.2346)
2023/09/15 10:44:52 - INFO - root -   Epoch: [188/500][40/346], lr: 0.00000369 	 loss = 0.0462(0.2157)
2023/09/15 10:45:09 - INFO - root -   Epoch: [188/500][60/346], lr: 0.00000369 	 loss = 0.0035(0.2209)
2023/09/15 10:45:29 - INFO - root -   Epoch: [188/500][80/346], lr: 0.00000369 	 loss = 0.7905(0.5358)
2023/09/15 10:45:45 - INFO - root -   Epoch: [188/500][100/346], lr: 0.00000369 	 loss = 0.1135(0.5199)
2023/09/15 10:46:06 - INFO - root -   Epoch: [188/500][120/346], lr: 0.00000369 	 loss = 0.4802(0.4923)
2023/09/15 10:46:23 - INFO - root -   Epoch: [188/500][140/346], lr: 0.00000369 	 loss = 0.0248(0.4605)
2023/09/15 10:46:43 - INFO - root -   Epoch: [188/500][160/346], lr: 0.00000369 	 loss = 1.7393(0.4575)
2023/09/15 10:47:00 - INFO - root -   Epoch: [188/500][180/346], lr: 0.00000369 	 loss = 0.1403(0.4326)
2023/09/15 10:47:20 - INFO - root -   Epoch: [188/500][200/346], lr: 0.00000369 	 loss = 0.0020(0.4039)
2023/09/15 10:47:36 - INFO - root -   Epoch: [188/500][220/346], lr: 0.00000369 	 loss = 0.1714(0.3885)
2023/09/15 10:47:57 - INFO - root -   Epoch: [188/500][240/346], lr: 0.00000369 	 loss = 0.0301(0.3658)
2023/09/15 10:48:13 - INFO - root -   Epoch: [188/500][260/346], lr: 0.00000369 	 loss = 1.0322(0.3535)
2023/09/15 10:48:34 - INFO - root -   Epoch: [188/500][280/346], lr: 0.00000369 	 loss = 0.3528(0.3378)
2023/09/15 10:48:50 - INFO - root -   Epoch: [188/500][300/346], lr: 0.00000369 	 loss = 0.2600(0.3528)
2023/09/15 10:49:10 - INFO - root -   Epoch: [188/500][320/346], lr: 0.00000369 	 loss = 0.0712(0.3393)
2023/09/15 10:49:27 - INFO - root -   Epoch: [188/500][340/346], lr: 0.00000369 	 loss = 0.0929(0.3289)
2023/09/15 10:49:29 - INFO - root -   Epoch: [188/500] 	 loss = 0.3292
2023/09/15 10:49:29 - INFO - root -   train_accuracy = 0.8960
2023/09/15 10:49:37 - INFO - root -   Epoch: [189/500][0/346], lr: 0.00000370 	 loss = 0.0436(0.0436)
2023/09/15 10:49:53 - INFO - root -   Epoch: [189/500][20/346], lr: 0.00000370 	 loss = 0.1153(0.1565)
2023/09/15 10:50:14 - INFO - root -   Epoch: [189/500][40/346], lr: 0.00000370 	 loss = 0.0780(0.1437)
2023/09/15 10:50:30 - INFO - root -   Epoch: [189/500][60/346], lr: 0.00000370 	 loss = 0.0948(0.1556)
2023/09/15 10:50:50 - INFO - root -   Epoch: [189/500][80/346], lr: 0.00000370 	 loss = 0.0662(0.1679)
2023/09/15 10:51:06 - INFO - root -   Epoch: [189/500][100/346], lr: 0.00000370 	 loss = 0.0056(0.1828)
2023/09/15 10:51:27 - INFO - root -   Epoch: [189/500][120/346], lr: 0.00000370 	 loss = 0.0078(0.1813)
2023/09/15 10:51:43 - INFO - root -   Epoch: [189/500][140/346], lr: 0.00000370 	 loss = 0.1668(0.1776)
2023/09/15 10:52:04 - INFO - root -   Epoch: [189/500][160/346], lr: 0.00000370 	 loss = 2.7313(0.1804)
2023/09/15 10:52:20 - INFO - root -   Epoch: [189/500][180/346], lr: 0.00000370 	 loss = 0.0067(0.2031)
2023/09/15 10:52:41 - INFO - root -   Epoch: [189/500][200/346], lr: 0.00000370 	 loss = 0.0003(0.1908)
2023/09/15 10:52:57 - INFO - root -   Epoch: [189/500][220/346], lr: 0.00000370 	 loss = 0.1643(0.1836)
2023/09/15 10:53:17 - INFO - root -   Epoch: [189/500][240/346], lr: 0.00000370 	 loss = 0.0085(0.2013)
2023/09/15 10:53:33 - INFO - root -   Epoch: [189/500][260/346], lr: 0.00000370 	 loss = 0.2097(0.2051)
2023/09/15 10:53:54 - INFO - root -   Epoch: [189/500][280/346], lr: 0.00000370 	 loss = 0.4422(0.2035)
2023/09/15 10:54:10 - INFO - root -   Epoch: [189/500][300/346], lr: 0.00000370 	 loss = 0.0609(0.1958)
2023/09/15 10:54:31 - INFO - root -   Epoch: [189/500][320/346], lr: 0.00000370 	 loss = 0.0073(0.1961)
2023/09/15 10:54:46 - INFO - root -   Epoch: [189/500][340/346], lr: 0.00000370 	 loss = 0.0796(0.2195)
2023/09/15 10:54:48 - INFO - root -   Epoch: [189/500] 	 loss = 0.2182
2023/09/15 10:56:09 - INFO - root -   precision = 0.3851
2023/09/15 10:56:09 - INFO - root -   eval_loss = 3.2893
2023/09/15 10:56:10 - INFO - root -   train_accuracy = 0.9220
2023/09/15 10:56:18 - INFO - root -   Epoch: [190/500][0/346], lr: 0.00000372 	 loss = 0.5345(0.5345)
2023/09/15 10:56:35 - INFO - root -   Epoch: [190/500][20/346], lr: 0.00000372 	 loss = 0.3926(0.7022)
2023/09/15 10:56:55 - INFO - root -   Epoch: [190/500][40/346], lr: 0.00000372 	 loss = 0.0597(0.6305)
2023/09/15 10:57:11 - INFO - root -   Epoch: [190/500][60/346], lr: 0.00000372 	 loss = 0.0413(0.5574)
2023/09/15 10:57:32 - INFO - root -   Epoch: [190/500][80/346], lr: 0.00000372 	 loss = 0.1120(0.5066)
2023/09/15 10:57:48 - INFO - root -   Epoch: [190/500][100/346], lr: 0.00000372 	 loss = 0.0212(0.5493)
2023/09/15 10:58:08 - INFO - root -   Epoch: [190/500][120/346], lr: 0.00000372 	 loss = 0.8358(0.5503)
2023/09/15 10:58:24 - INFO - root -   Epoch: [190/500][140/346], lr: 0.00000372 	 loss = 0.1921(0.5018)
2023/09/15 10:58:45 - INFO - root -   Epoch: [190/500][160/346], lr: 0.00000372 	 loss = 1.5615(0.4956)
2023/09/15 10:59:01 - INFO - root -   Epoch: [190/500][180/346], lr: 0.00000372 	 loss = 0.3526(0.4804)
2023/09/15 10:59:22 - INFO - root -   Epoch: [190/500][200/346], lr: 0.00000372 	 loss = 0.0006(0.4540)
2023/09/15 10:59:38 - INFO - root -   Epoch: [190/500][220/346], lr: 0.00000372 	 loss = 0.0949(0.4337)
2023/09/15 10:59:59 - INFO - root -   Epoch: [190/500][240/346], lr: 0.00000372 	 loss = 0.0792(0.4104)
2023/09/15 11:00:15 - INFO - root -   Epoch: [190/500][260/346], lr: 0.00000372 	 loss = 0.1618(0.3893)
2023/09/15 11:00:36 - INFO - root -   Epoch: [190/500][280/346], lr: 0.00000372 	 loss = 0.0936(0.3785)
2023/09/15 11:00:52 - INFO - root -   Epoch: [190/500][300/346], lr: 0.00000372 	 loss = 0.2572(0.3586)
2023/09/15 11:01:13 - INFO - root -   Epoch: [190/500][320/346], lr: 0.00000372 	 loss = 0.0050(0.3524)
2023/09/15 11:01:29 - INFO - root -   Epoch: [190/500][340/346], lr: 0.00000372 	 loss = 0.9125(0.3451)
2023/09/15 11:01:31 - INFO - root -   Epoch: [190/500] 	 loss = 0.3582
2023/09/15 11:01:31 - INFO - root -   train_accuracy = 0.8829
2023/09/15 11:01:39 - INFO - root -   Epoch: [191/500][0/346], lr: 0.00000373 	 loss = 0.3470(0.3470)
2023/09/15 11:01:56 - INFO - root -   Epoch: [191/500][20/346], lr: 0.00000373 	 loss = 0.4460(0.4398)
2023/09/15 11:02:17 - INFO - root -   Epoch: [191/500][40/346], lr: 0.00000373 	 loss = 0.0782(0.3846)
2023/09/15 11:02:33 - INFO - root -   Epoch: [191/500][60/346], lr: 0.00000373 	 loss = 0.0165(0.3542)
2023/09/15 11:02:54 - INFO - root -   Epoch: [191/500][80/346], lr: 0.00000373 	 loss = 0.2303(0.3188)
2023/09/15 11:03:11 - INFO - root -   Epoch: [191/500][100/346], lr: 0.00000373 	 loss = 0.0046(0.3082)
2023/09/15 11:03:32 - INFO - root -   Epoch: [191/500][120/346], lr: 0.00000373 	 loss = 0.0924(0.2802)
2023/09/15 11:03:48 - INFO - root -   Epoch: [191/500][140/346], lr: 0.00000373 	 loss = 0.0321(0.2693)
2023/09/15 11:04:09 - INFO - root -   Epoch: [191/500][160/346], lr: 0.00000373 	 loss = 1.7126(0.2636)
2023/09/15 11:04:26 - INFO - root -   Epoch: [191/500][180/346], lr: 0.00000373 	 loss = 0.0043(0.2566)
2023/09/15 11:04:46 - INFO - root -   Epoch: [191/500][200/346], lr: 0.00000373 	 loss = 0.0002(0.2412)
2023/09/15 11:05:03 - INFO - root -   Epoch: [191/500][220/346], lr: 0.00000373 	 loss = 0.0384(0.2358)
2023/09/15 11:05:24 - INFO - root -   Epoch: [191/500][240/346], lr: 0.00000373 	 loss = 0.0115(0.2254)
2023/09/15 11:05:40 - INFO - root -   Epoch: [191/500][260/346], lr: 0.00000373 	 loss = 0.1656(0.2279)
2023/09/15 11:06:01 - INFO - root -   Epoch: [191/500][280/346], lr: 0.00000373 	 loss = 0.3291(0.2214)
2023/09/15 11:06:17 - INFO - root -   Epoch: [191/500][300/346], lr: 0.00000373 	 loss = 0.0023(0.2191)
2023/09/15 11:06:38 - INFO - root -   Epoch: [191/500][320/346], lr: 0.00000373 	 loss = 5.8357(0.2316)
2023/09/15 11:06:55 - INFO - root -   Epoch: [191/500][340/346], lr: 0.00000373 	 loss = 0.1764(0.2250)
2023/09/15 11:06:57 - INFO - root -   Epoch: [191/500] 	 loss = 0.2222
2023/09/15 11:06:57 - INFO - root -   train_accuracy = 0.9249
2023/09/15 11:07:05 - INFO - root -   Epoch: [192/500][0/346], lr: 0.00000375 	 loss = 0.1058(0.1058)
2023/09/15 11:07:21 - INFO - root -   Epoch: [192/500][20/346], lr: 0.00000375 	 loss = 0.1720(0.2406)
2023/09/15 11:07:42 - INFO - root -   Epoch: [192/500][40/346], lr: 0.00000375 	 loss = 0.0471(0.1873)
2023/09/15 11:07:59 - INFO - root -   Epoch: [192/500][60/346], lr: 0.00000375 	 loss = 1.9930(0.2837)
2023/09/15 11:08:19 - INFO - root -   Epoch: [192/500][80/346], lr: 0.00000375 	 loss = 0.0694(0.2642)
2023/09/15 11:08:36 - INFO - root -   Epoch: [192/500][100/346], lr: 0.00000375 	 loss = 0.2032(0.2621)
2023/09/15 11:08:57 - INFO - root -   Epoch: [192/500][120/346], lr: 0.00000375 	 loss = 0.0202(0.3262)
2023/09/15 11:09:13 - INFO - root -   Epoch: [192/500][140/346], lr: 0.00000375 	 loss = 0.0047(0.3235)
2023/09/15 11:09:34 - INFO - root -   Epoch: [192/500][160/346], lr: 0.00000375 	 loss = 1.2559(0.3058)
2023/09/15 11:09:50 - INFO - root -   Epoch: [192/500][180/346], lr: 0.00000375 	 loss = 0.0302(0.2979)
2023/09/15 11:10:11 - INFO - root -   Epoch: [192/500][200/346], lr: 0.00000375 	 loss = 0.0102(0.2783)
2023/09/15 11:10:28 - INFO - root -   Epoch: [192/500][220/346], lr: 0.00000375 	 loss = 0.0763(0.2635)
2023/09/15 11:10:49 - INFO - root -   Epoch: [192/500][240/346], lr: 0.00000375 	 loss = 0.0269(0.2614)
2023/09/15 11:11:05 - INFO - root -   Epoch: [192/500][260/346], lr: 0.00000375 	 loss = 0.0679(0.2534)
2023/09/15 11:11:26 - INFO - root -   Epoch: [192/500][280/346], lr: 0.00000375 	 loss = 0.0719(0.2409)
2023/09/15 11:11:43 - INFO - root -   Epoch: [192/500][300/346], lr: 0.00000375 	 loss = 0.0285(0.2271)
2023/09/15 11:12:04 - INFO - root -   Epoch: [192/500][320/346], lr: 0.00000375 	 loss = 0.0030(0.2192)
2023/09/15 11:12:20 - INFO - root -   Epoch: [192/500][340/346], lr: 0.00000375 	 loss = 0.0168(0.2120)
2023/09/15 11:12:22 - INFO - root -   Epoch: [192/500] 	 loss = 0.2099
2023/09/15 11:12:22 - INFO - root -   train_accuracy = 0.9436
2023/09/15 11:12:30 - INFO - root -   Epoch: [193/500][0/346], lr: 0.00000376 	 loss = 0.0413(0.0413)
2023/09/15 11:12:46 - INFO - root -   Epoch: [193/500][20/346], lr: 0.00000376 	 loss = 0.1258(0.1175)
2023/09/15 11:13:07 - INFO - root -   Epoch: [193/500][40/346], lr: 0.00000376 	 loss = 0.0278(0.1076)
2023/09/15 11:13:23 - INFO - root -   Epoch: [193/500][60/346], lr: 0.00000376 	 loss = 0.0019(0.1702)
2023/09/15 11:13:44 - INFO - root -   Epoch: [193/500][80/346], lr: 0.00000376 	 loss = 0.0121(0.1889)
2023/09/15 11:14:01 - INFO - root -   Epoch: [193/500][100/346], lr: 0.00000376 	 loss = 0.0058(0.1891)
2023/09/15 11:14:21 - INFO - root -   Epoch: [193/500][120/346], lr: 0.00000376 	 loss = 0.7884(0.2364)
2023/09/15 11:14:38 - INFO - root -   Epoch: [193/500][140/346], lr: 0.00000376 	 loss = 0.4628(0.2338)
2023/09/15 11:14:58 - INFO - root -   Epoch: [193/500][160/346], lr: 0.00000376 	 loss = 1.3094(0.2205)
2023/09/15 11:15:14 - INFO - root -   Epoch: [193/500][180/346], lr: 0.00000376 	 loss = 0.0183(0.2532)
2023/09/15 11:15:34 - INFO - root -   Epoch: [193/500][200/346], lr: 0.00000376 	 loss = 0.0004(0.2397)
2023/09/15 11:15:51 - INFO - root -   Epoch: [193/500][220/346], lr: 0.00000376 	 loss = 0.0481(0.2475)
2023/09/15 11:16:11 - INFO - root -   Epoch: [193/500][240/346], lr: 0.00000376 	 loss = 0.0054(0.2392)
2023/09/15 11:16:28 - INFO - root -   Epoch: [193/500][260/346], lr: 0.00000376 	 loss = 0.1394(0.2330)
2023/09/15 11:16:47 - INFO - root -   Epoch: [193/500][280/346], lr: 0.00000376 	 loss = 0.0722(0.2258)
2023/09/15 11:17:05 - INFO - root -   Epoch: [193/500][300/346], lr: 0.00000376 	 loss = 0.0946(0.2233)
2023/09/15 11:17:24 - INFO - root -   Epoch: [193/500][320/346], lr: 0.00000376 	 loss = 0.0004(0.2154)
2023/09/15 11:17:42 - INFO - root -   Epoch: [193/500][340/346], lr: 0.00000376 	 loss = 0.0652(0.2081)
2023/09/15 11:17:44 - INFO - root -   Epoch: [193/500] 	 loss = 0.2061
2023/09/15 11:17:44 - INFO - root -   train_accuracy = 0.9263
2023/09/15 11:17:52 - INFO - root -   Epoch: [194/500][0/346], lr: 0.00000378 	 loss = 0.2878(0.2878)
2023/09/15 11:18:08 - INFO - root -   Epoch: [194/500][20/346], lr: 0.00000378 	 loss = 0.0255(0.6103)
2023/09/15 11:18:29 - INFO - root -   Epoch: [194/500][40/346], lr: 0.00000378 	 loss = 0.0563(0.4046)
2023/09/15 11:18:46 - INFO - root -   Epoch: [194/500][60/346], lr: 0.00000378 	 loss = 0.0432(0.3347)
2023/09/15 11:19:06 - INFO - root -   Epoch: [194/500][80/346], lr: 0.00000378 	 loss = 0.9343(0.3510)
2023/09/15 11:19:23 - INFO - root -   Epoch: [194/500][100/346], lr: 0.00000378 	 loss = 0.1513(0.3855)
2023/09/15 11:19:44 - INFO - root -   Epoch: [194/500][120/346], lr: 0.00000378 	 loss = 0.0699(0.3547)
2023/09/15 11:20:00 - INFO - root -   Epoch: [194/500][140/346], lr: 0.00000378 	 loss = 0.0068(0.3261)
2023/09/15 11:20:21 - INFO - root -   Epoch: [194/500][160/346], lr: 0.00000378 	 loss = 1.7140(0.3107)
2023/09/15 11:20:38 - INFO - root -   Epoch: [194/500][180/346], lr: 0.00000378 	 loss = 0.0030(0.3079)
2023/09/15 11:20:59 - INFO - root -   Epoch: [194/500][200/346], lr: 0.00000378 	 loss = 0.0005(0.2826)
2023/09/15 11:21:15 - INFO - root -   Epoch: [194/500][220/346], lr: 0.00000378 	 loss = 0.0645(0.2633)
2023/09/15 11:21:36 - INFO - root -   Epoch: [194/500][240/346], lr: 0.00000378 	 loss = 0.0267(0.2502)
2023/09/15 11:21:53 - INFO - root -   Epoch: [194/500][260/346], lr: 0.00000378 	 loss = 0.7243(0.2470)
2023/09/15 11:22:14 - INFO - root -   Epoch: [194/500][280/346], lr: 0.00000378 	 loss = 0.5563(0.2409)
2023/09/15 11:22:30 - INFO - root -   Epoch: [194/500][300/346], lr: 0.00000378 	 loss = 0.0301(0.2597)
2023/09/15 11:22:51 - INFO - root -   Epoch: [194/500][320/346], lr: 0.00000378 	 loss = 0.0381(0.2641)
2023/09/15 11:23:07 - INFO - root -   Epoch: [194/500][340/346], lr: 0.00000378 	 loss = 0.1103(0.2582)
2023/09/15 11:23:09 - INFO - root -   Epoch: [194/500] 	 loss = 0.2647
2023/09/15 11:24:31 - INFO - root -   precision = 0.5460
2023/09/15 11:24:31 - INFO - root -   eval_loss = 2.3912
2023/09/15 11:24:32 - INFO - root -   train_accuracy = 0.9162
2023/09/15 11:24:40 - INFO - root -   Epoch: [195/500][0/346], lr: 0.00000379 	 loss = 0.2192(0.2192)
2023/09/15 11:24:57 - INFO - root -   Epoch: [195/500][20/346], lr: 0.00000379 	 loss = 0.0776(0.1419)
2023/09/15 11:25:18 - INFO - root -   Epoch: [195/500][40/346], lr: 0.00000379 	 loss = 0.0219(0.1754)
2023/09/15 11:25:35 - INFO - root -   Epoch: [195/500][60/346], lr: 0.00000379 	 loss = 0.0190(0.1869)
2023/09/15 11:25:56 - INFO - root -   Epoch: [195/500][80/346], lr: 0.00000379 	 loss = 0.0979(0.2279)
2023/09/15 11:26:12 - INFO - root -   Epoch: [195/500][100/346], lr: 0.00000379 	 loss = 0.0050(0.2280)
2023/09/15 11:26:33 - INFO - root -   Epoch: [195/500][120/346], lr: 0.00000379 	 loss = 0.0241(0.2126)
2023/09/15 11:26:49 - INFO - root -   Epoch: [195/500][140/346], lr: 0.00000379 	 loss = 0.0561(0.1950)
2023/09/15 11:27:10 - INFO - root -   Epoch: [195/500][160/346], lr: 0.00000379 	 loss = 0.5579(0.1882)
2023/09/15 11:27:26 - INFO - root -   Epoch: [195/500][180/346], lr: 0.00000379 	 loss = 0.0192(0.1902)
2023/09/15 11:27:47 - INFO - root -   Epoch: [195/500][200/346], lr: 0.00000379 	 loss = 0.0001(0.1812)
2023/09/15 11:28:03 - INFO - root -   Epoch: [195/500][220/346], lr: 0.00000379 	 loss = 0.0160(0.1752)
2023/09/15 11:28:24 - INFO - root -   Epoch: [195/500][240/346], lr: 0.00000379 	 loss = 0.0042(0.1656)
2023/09/15 11:28:41 - INFO - root -   Epoch: [195/500][260/346], lr: 0.00000379 	 loss = 0.3199(0.1653)
2023/09/15 11:29:01 - INFO - root -   Epoch: [195/500][280/346], lr: 0.00000379 	 loss = 0.7802(0.1822)
2023/09/15 11:29:18 - INFO - root -   Epoch: [195/500][300/346], lr: 0.00000379 	 loss = 2.0607(0.1825)
2023/09/15 11:29:38 - INFO - root -   Epoch: [195/500][320/346], lr: 0.00000379 	 loss = 0.0155(0.1762)
2023/09/15 11:29:54 - INFO - root -   Epoch: [195/500][340/346], lr: 0.00000379 	 loss = 0.0888(0.1746)
2023/09/15 11:29:56 - INFO - root -   Epoch: [195/500] 	 loss = 0.1727
2023/09/15 11:29:56 - INFO - root -   train_accuracy = 0.9335
2023/09/15 11:30:05 - INFO - root -   Epoch: [196/500][0/346], lr: 0.00000380 	 loss = 0.1489(0.1489)
2023/09/15 11:30:21 - INFO - root -   Epoch: [196/500][20/346], lr: 0.00000380 	 loss = 0.1504(0.0952)
2023/09/15 11:30:42 - INFO - root -   Epoch: [196/500][40/346], lr: 0.00000380 	 loss = 0.0023(0.0815)
2023/09/15 11:30:58 - INFO - root -   Epoch: [196/500][60/346], lr: 0.00000380 	 loss = 0.0058(0.0982)
2023/09/15 11:31:19 - INFO - root -   Epoch: [196/500][80/346], lr: 0.00000380 	 loss = 0.0157(0.3788)
2023/09/15 11:31:36 - INFO - root -   Epoch: [196/500][100/346], lr: 0.00000380 	 loss = 0.1221(0.6070)
2023/09/15 11:31:56 - INFO - root -   Epoch: [196/500][120/346], lr: 0.00000380 	 loss = 0.2573(0.6098)
2023/09/15 11:32:13 - INFO - root -   Epoch: [196/500][140/346], lr: 0.00000380 	 loss = 0.1556(0.5600)
2023/09/15 11:32:34 - INFO - root -   Epoch: [196/500][160/346], lr: 0.00000380 	 loss = 0.6801(0.5316)
2023/09/15 11:32:50 - INFO - root -   Epoch: [196/500][180/346], lr: 0.00000380 	 loss = 0.0109(0.4989)
2023/09/15 11:33:11 - INFO - root -   Epoch: [196/500][200/346], lr: 0.00000380 	 loss = 0.0028(0.4586)
2023/09/15 11:33:27 - INFO - root -   Epoch: [196/500][220/346], lr: 0.00000380 	 loss = 0.2172(0.4278)
2023/09/15 11:33:48 - INFO - root -   Epoch: [196/500][240/346], lr: 0.00000380 	 loss = 0.0093(0.4004)
2023/09/15 11:34:04 - INFO - root -   Epoch: [196/500][260/346], lr: 0.00000380 	 loss = 0.3712(0.3791)
2023/09/15 11:34:25 - INFO - root -   Epoch: [196/500][280/346], lr: 0.00000380 	 loss = 0.1241(0.3732)
2023/09/15 11:34:41 - INFO - root -   Epoch: [196/500][300/346], lr: 0.00000380 	 loss = 0.7549(0.3545)
2023/09/15 11:35:01 - INFO - root -   Epoch: [196/500][320/346], lr: 0.00000380 	 loss = 0.0134(0.3399)
2023/09/15 11:35:17 - INFO - root -   Epoch: [196/500][340/346], lr: 0.00000380 	 loss = 0.0648(0.3288)
2023/09/15 11:35:20 - INFO - root -   Epoch: [196/500] 	 loss = 0.3258
2023/09/15 11:35:20 - INFO - root -   train_accuracy = 0.9162
2023/09/15 11:35:28 - INFO - root -   Epoch: [197/500][0/346], lr: 0.00000382 	 loss = 0.0831(0.0831)
2023/09/15 11:35:44 - INFO - root -   Epoch: [197/500][20/346], lr: 0.00000382 	 loss = 0.0734(0.0971)
2023/09/15 11:36:05 - INFO - root -   Epoch: [197/500][40/346], lr: 0.00000382 	 loss = 0.0476(0.0949)
2023/09/15 11:36:21 - INFO - root -   Epoch: [197/500][60/346], lr: 0.00000382 	 loss = 0.0080(0.1135)
2023/09/15 11:36:42 - INFO - root -   Epoch: [197/500][80/346], lr: 0.00000382 	 loss = 0.5428(0.1588)
2023/09/15 11:36:59 - INFO - root -   Epoch: [197/500][100/346], lr: 0.00000382 	 loss = 0.0115(0.1675)
2023/09/15 11:37:19 - INFO - root -   Epoch: [197/500][120/346], lr: 0.00000382 	 loss = 0.0634(0.2090)
2023/09/15 11:37:36 - INFO - root -   Epoch: [197/500][140/346], lr: 0.00000382 	 loss = 0.0104(0.1981)
2023/09/15 11:37:56 - INFO - root -   Epoch: [197/500][160/346], lr: 0.00000382 	 loss = 1.3075(0.1890)
2023/09/15 11:38:13 - INFO - root -   Epoch: [197/500][180/346], lr: 0.00000382 	 loss = 0.0028(0.1849)
2023/09/15 11:38:34 - INFO - root -   Epoch: [197/500][200/346], lr: 0.00000382 	 loss = 0.0005(0.1730)
2023/09/15 11:38:50 - INFO - root -   Epoch: [197/500][220/346], lr: 0.00000382 	 loss = 0.0498(0.1761)
2023/09/15 11:39:10 - INFO - root -   Epoch: [197/500][240/346], lr: 0.00000382 	 loss = 0.0136(0.1738)
2023/09/15 11:39:27 - INFO - root -   Epoch: [197/500][260/346], lr: 0.00000382 	 loss = 0.0414(0.1653)
2023/09/15 11:39:47 - INFO - root -   Epoch: [197/500][280/346], lr: 0.00000382 	 loss = 0.0655(0.1760)
2023/09/15 11:40:04 - INFO - root -   Epoch: [197/500][300/346], lr: 0.00000382 	 loss = 0.0342(0.1671)
2023/09/15 11:40:24 - INFO - root -   Epoch: [197/500][320/346], lr: 0.00000382 	 loss = 0.0016(0.1603)
2023/09/15 11:40:40 - INFO - root -   Epoch: [197/500][340/346], lr: 0.00000382 	 loss = 0.3154(0.1557)
2023/09/15 11:40:42 - INFO - root -   Epoch: [197/500] 	 loss = 0.1539
2023/09/15 11:40:42 - INFO - root -   train_accuracy = 0.9581
2023/09/15 11:40:51 - INFO - root -   Epoch: [198/500][0/346], lr: 0.00000383 	 loss = 0.1569(0.1569)
2023/09/15 11:41:07 - INFO - root -   Epoch: [198/500][20/346], lr: 0.00000383 	 loss = 0.1665(0.1201)
2023/09/15 11:41:28 - INFO - root -   Epoch: [198/500][40/346], lr: 0.00000383 	 loss = 0.3483(0.0844)
2023/09/15 11:41:44 - INFO - root -   Epoch: [198/500][60/346], lr: 0.00000383 	 loss = 0.0114(0.0856)
2023/09/15 11:42:05 - INFO - root -   Epoch: [198/500][80/346], lr: 0.00000383 	 loss = 0.0139(0.0988)
2023/09/15 11:42:21 - INFO - root -   Epoch: [198/500][100/346], lr: 0.00000383 	 loss = 0.0005(0.1307)
2023/09/15 11:42:42 - INFO - root -   Epoch: [198/500][120/346], lr: 0.00000383 	 loss = 0.0033(0.1316)
2023/09/15 11:42:58 - INFO - root -   Epoch: [198/500][140/346], lr: 0.00000383 	 loss = 0.1027(0.1183)
2023/09/15 11:43:19 - INFO - root -   Epoch: [198/500][160/346], lr: 0.00000383 	 loss = 0.4678(0.1229)
2023/09/15 11:43:36 - INFO - root -   Epoch: [198/500][180/346], lr: 0.00000383 	 loss = 0.0025(0.1370)
2023/09/15 11:43:57 - INFO - root -   Epoch: [198/500][200/346], lr: 0.00000383 	 loss = 0.0000(0.1253)
2023/09/15 11:44:13 - INFO - root -   Epoch: [198/500][220/346], lr: 0.00000383 	 loss = 0.0969(0.1179)
2023/09/15 11:44:34 - INFO - root -   Epoch: [198/500][240/346], lr: 0.00000383 	 loss = 0.0062(0.1111)
2023/09/15 11:44:50 - INFO - root -   Epoch: [198/500][260/346], lr: 0.00000383 	 loss = 0.0963(0.1073)
2023/09/15 11:45:11 - INFO - root -   Epoch: [198/500][280/346], lr: 0.00000383 	 loss = 0.6489(0.1107)
2023/09/15 11:45:27 - INFO - root -   Epoch: [198/500][300/346], lr: 0.00000383 	 loss = 0.4693(0.1169)
2023/09/15 11:45:48 - INFO - root -   Epoch: [198/500][320/346], lr: 0.00000383 	 loss = 0.0046(0.1121)
2023/09/15 11:46:04 - INFO - root -   Epoch: [198/500][340/346], lr: 0.00000383 	 loss = 0.0113(0.1087)
2023/09/15 11:46:06 - INFO - root -   Epoch: [198/500] 	 loss = 0.1078
2023/09/15 11:46:06 - INFO - root -   train_accuracy = 0.9668
2023/09/15 11:46:14 - INFO - root -   Epoch: [199/500][0/346], lr: 0.00000385 	 loss = 0.0520(0.0520)
2023/09/15 11:46:31 - INFO - root -   Epoch: [199/500][20/346], lr: 0.00000385 	 loss = 0.0117(0.0639)
2023/09/15 11:46:51 - INFO - root -   Epoch: [199/500][40/346], lr: 0.00000385 	 loss = 0.0086(0.0879)
2023/09/15 11:47:08 - INFO - root -   Epoch: [199/500][60/346], lr: 0.00000385 	 loss = 0.0350(0.1489)
2023/09/15 11:47:28 - INFO - root -   Epoch: [199/500][80/346], lr: 0.00000385 	 loss = 0.1227(0.1609)
2023/09/15 11:47:45 - INFO - root -   Epoch: [199/500][100/346], lr: 0.00000385 	 loss = 0.0128(0.1687)
2023/09/15 11:48:05 - INFO - root -   Epoch: [199/500][120/346], lr: 0.00000385 	 loss = 0.0477(0.1845)
2023/09/15 11:48:22 - INFO - root -   Epoch: [199/500][140/346], lr: 0.00000385 	 loss = 0.0186(0.1691)
2023/09/15 11:48:42 - INFO - root -   Epoch: [199/500][160/346], lr: 0.00000385 	 loss = 0.2446(0.1769)
2023/09/15 11:48:59 - INFO - root -   Epoch: [199/500][180/346], lr: 0.00000385 	 loss = 0.0014(0.1710)
2023/09/15 11:49:20 - INFO - root -   Epoch: [199/500][200/346], lr: 0.00000385 	 loss = 0.0003(0.1614)
2023/09/15 11:49:36 - INFO - root -   Epoch: [199/500][220/346], lr: 0.00000385 	 loss = 0.0099(0.1620)
2023/09/15 11:49:57 - INFO - root -   Epoch: [199/500][240/346], lr: 0.00000385 	 loss = 0.0037(0.1529)
2023/09/15 11:50:13 - INFO - root -   Epoch: [199/500][260/346], lr: 0.00000385 	 loss = 0.1283(0.1479)
2023/09/15 11:50:34 - INFO - root -   Epoch: [199/500][280/346], lr: 0.00000385 	 loss = 0.4624(0.1419)
2023/09/15 11:50:50 - INFO - root -   Epoch: [199/500][300/346], lr: 0.00000385 	 loss = 0.0107(0.1334)
2023/09/15 11:51:11 - INFO - root -   Epoch: [199/500][320/346], lr: 0.00000385 	 loss = 0.0031(0.1287)
2023/09/15 11:51:27 - INFO - root -   Epoch: [199/500][340/346], lr: 0.00000385 	 loss = 0.0180(0.1256)
2023/09/15 11:51:29 - INFO - root -   Epoch: [199/500] 	 loss = 0.1241
2023/09/15 11:52:50 - INFO - root -   precision = 0.5517
2023/09/15 11:52:50 - INFO - root -   eval_loss = 2.9936
2023/09/15 11:52:51 - INFO - root -   train_accuracy = 0.9624
2023/09/15 11:53:00 - INFO - root -   Epoch: [200/500][0/346], lr: 0.00000386 	 loss = 0.0017(0.0017)
2023/09/15 11:53:16 - INFO - root -   Epoch: [200/500][20/346], lr: 0.00000386 	 loss = 0.0189(0.1131)
2023/09/15 11:53:36 - INFO - root -   Epoch: [200/500][40/346], lr: 0.00000386 	 loss = 0.0298(0.1743)
2023/09/15 11:53:52 - INFO - root -   Epoch: [200/500][60/346], lr: 0.00000386 	 loss = 0.0175(0.2166)
2023/09/15 11:54:12 - INFO - root -   Epoch: [200/500][80/346], lr: 0.00000386 	 loss = 0.0050(0.1918)
2023/09/15 11:54:28 - INFO - root -   Epoch: [200/500][100/346], lr: 0.00000386 	 loss = 0.0214(0.2751)
2023/09/15 11:54:49 - INFO - root -   Epoch: [200/500][120/346], lr: 0.00000386 	 loss = 0.0767(0.2506)
2023/09/15 11:55:05 - INFO - root -   Epoch: [200/500][140/346], lr: 0.00000386 	 loss = 0.0564(0.2326)
2023/09/15 11:55:25 - INFO - root -   Epoch: [200/500][160/346], lr: 0.00000386 	 loss = 3.0053(0.2306)
2023/09/15 11:55:41 - INFO - root -   Epoch: [200/500][180/346], lr: 0.00000386 	 loss = 0.0120(0.2153)
2023/09/15 11:56:02 - INFO - root -   Epoch: [200/500][200/346], lr: 0.00000386 	 loss = 0.0001(0.1958)
2023/09/15 11:56:18 - INFO - root -   Epoch: [200/500][220/346], lr: 0.00000386 	 loss = 0.0193(0.2091)
2023/09/15 11:56:39 - INFO - root -   Epoch: [200/500][240/346], lr: 0.00000386 	 loss = 0.0016(0.2191)
2023/09/15 11:56:55 - INFO - root -   Epoch: [200/500][260/346], lr: 0.00000386 	 loss = 0.1896(0.2126)
2023/09/15 11:57:15 - INFO - root -   Epoch: [200/500][280/346], lr: 0.00000386 	 loss = 0.1914(0.2056)
2023/09/15 11:57:32 - INFO - root -   Epoch: [200/500][300/346], lr: 0.00000386 	 loss = 0.0259(0.2000)
2023/09/15 11:57:52 - INFO - root -   Epoch: [200/500][320/346], lr: 0.00000386 	 loss = 0.0010(0.2069)
2023/09/15 11:58:07 - INFO - root -   Epoch: [200/500][340/346], lr: 0.00000386 	 loss = 0.3995(0.2003)
2023/09/15 11:58:09 - INFO - root -   Epoch: [200/500] 	 loss = 0.1975
2023/09/15 11:58:09 - INFO - root -   train_accuracy = 0.9408
2023/09/15 11:58:17 - INFO - root -   Epoch: [201/500][0/346], lr: 0.00000388 	 loss = 0.0043(0.0043)
2023/09/15 11:58:33 - INFO - root -   Epoch: [201/500][20/346], lr: 0.00000388 	 loss = 0.0085(0.0982)
2023/09/15 11:58:53 - INFO - root -   Epoch: [201/500][40/346], lr: 0.00000388 	 loss = 0.0957(0.1311)
2023/09/15 11:59:09 - INFO - root -   Epoch: [201/500][60/346], lr: 0.00000388 	 loss = 0.0046(0.1091)
2023/09/15 11:59:28 - INFO - root -   Epoch: [201/500][80/346], lr: 0.00000388 	 loss = 0.2515(0.1947)
2023/09/15 11:59:44 - INFO - root -   Epoch: [201/500][100/346], lr: 0.00000388 	 loss = 0.0014(0.1663)
2023/09/15 12:00:04 - INFO - root -   Epoch: [201/500][120/346], lr: 0.00000388 	 loss = 0.0226(0.1500)
2023/09/15 12:00:20 - INFO - root -   Epoch: [201/500][140/346], lr: 0.00000388 	 loss = 0.0015(0.1632)
2023/09/15 12:00:40 - INFO - root -   Epoch: [201/500][160/346], lr: 0.00000388 	 loss = 1.6329(0.1610)
2023/09/15 12:00:56 - INFO - root -   Epoch: [201/500][180/346], lr: 0.00000388 	 loss = 0.0559(0.1629)
2023/09/15 12:01:15 - INFO - root -   Epoch: [201/500][200/346], lr: 0.00000388 	 loss = 0.0000(0.1802)
2023/09/15 12:01:31 - INFO - root -   Epoch: [201/500][220/346], lr: 0.00000388 	 loss = 0.0603(0.2090)
2023/09/15 12:01:51 - INFO - root -   Epoch: [201/500][240/346], lr: 0.00000388 	 loss = 0.0329(0.1978)
2023/09/15 12:02:07 - INFO - root -   Epoch: [201/500][260/346], lr: 0.00000388 	 loss = 0.0615(0.1920)
2023/09/15 12:02:26 - INFO - root -   Epoch: [201/500][280/346], lr: 0.00000388 	 loss = 0.1065(0.1880)
2023/09/15 12:02:42 - INFO - root -   Epoch: [201/500][300/346], lr: 0.00000388 	 loss = 0.0522(0.1929)
2023/09/15 12:03:01 - INFO - root -   Epoch: [201/500][320/346], lr: 0.00000388 	 loss = 0.0000(0.2050)
2023/09/15 12:03:17 - INFO - root -   Epoch: [201/500][340/346], lr: 0.00000388 	 loss = 0.0106(0.2194)
2023/09/15 12:03:19 - INFO - root -   Epoch: [201/500] 	 loss = 0.2164
2023/09/15 12:03:19 - INFO - root -   train_accuracy = 0.9350
2023/09/15 12:03:27 - INFO - root -   Epoch: [202/500][0/346], lr: 0.00000389 	 loss = 0.0100(0.0100)
2023/09/15 12:03:43 - INFO - root -   Epoch: [202/500][20/346], lr: 0.00000389 	 loss = 0.0135(0.1100)
2023/09/15 12:04:03 - INFO - root -   Epoch: [202/500][40/346], lr: 0.00000389 	 loss = 0.0166(0.0986)
2023/09/15 12:04:19 - INFO - root -   Epoch: [202/500][60/346], lr: 0.00000389 	 loss = 0.1935(0.2130)
2023/09/15 12:04:39 - INFO - root -   Epoch: [202/500][80/346], lr: 0.00000389 	 loss = 0.0747(0.2586)
2023/09/15 12:04:55 - INFO - root -   Epoch: [202/500][100/346], lr: 0.00000389 	 loss = 0.0653(0.2498)
2023/09/15 12:05:15 - INFO - root -   Epoch: [202/500][120/346], lr: 0.00000389 	 loss = 0.5691(0.2436)
2023/09/15 12:05:31 - INFO - root -   Epoch: [202/500][140/346], lr: 0.00000389 	 loss = 0.4865(0.2315)
2023/09/15 12:05:50 - INFO - root -   Epoch: [202/500][160/346], lr: 0.00000389 	 loss = 1.4416(0.2245)
2023/09/15 12:06:06 - INFO - root -   Epoch: [202/500][180/346], lr: 0.00000389 	 loss = 0.0837(0.2294)
2023/09/15 12:06:26 - INFO - root -   Epoch: [202/500][200/346], lr: 0.00000389 	 loss = 1.6615(0.2238)
2023/09/15 12:06:42 - INFO - root -   Epoch: [202/500][220/346], lr: 0.00000389 	 loss = 0.0765(0.2360)
2023/09/15 12:07:01 - INFO - root -   Epoch: [202/500][240/346], lr: 0.00000389 	 loss = 0.0069(0.2238)
2023/09/15 12:07:17 - INFO - root -   Epoch: [202/500][260/346], lr: 0.00000389 	 loss = 0.1185(0.2143)
2023/09/15 12:07:37 - INFO - root -   Epoch: [202/500][280/346], lr: 0.00000389 	 loss = 0.1377(0.2064)
2023/09/15 12:07:53 - INFO - root -   Epoch: [202/500][300/346], lr: 0.00000389 	 loss = 0.0122(0.2116)
2023/09/15 12:08:12 - INFO - root -   Epoch: [202/500][320/346], lr: 0.00000389 	 loss = 0.0005(0.2036)
2023/09/15 12:08:27 - INFO - root -   Epoch: [202/500][340/346], lr: 0.00000389 	 loss = 0.0987(0.1985)
2023/09/15 12:08:29 - INFO - root -   Epoch: [202/500] 	 loss = 0.1962
2023/09/15 12:08:29 - INFO - root -   train_accuracy = 0.9292
2023/09/15 12:08:38 - INFO - root -   Epoch: [203/500][0/346], lr: 0.00000390 	 loss = 0.0262(0.0262)
2023/09/15 12:08:54 - INFO - root -   Epoch: [203/500][20/346], lr: 0.00000390 	 loss = 0.3044(0.0775)
2023/09/15 12:09:13 - INFO - root -   Epoch: [203/500][40/346], lr: 0.00000390 	 loss = 0.0016(0.1319)
2023/09/15 12:09:29 - INFO - root -   Epoch: [203/500][60/346], lr: 0.00000390 	 loss = 0.0030(0.1719)
2023/09/15 12:09:49 - INFO - root -   Epoch: [203/500][80/346], lr: 0.00000390 	 loss = 0.0439(0.1751)
2023/09/15 12:10:05 - INFO - root -   Epoch: [203/500][100/346], lr: 0.00000390 	 loss = 0.0025(0.1643)
2023/09/15 12:10:25 - INFO - root -   Epoch: [203/500][120/346], lr: 0.00000390 	 loss = 0.0239(0.1632)
2023/09/15 12:10:41 - INFO - root -   Epoch: [203/500][140/346], lr: 0.00000390 	 loss = 0.0083(0.1519)
2023/09/15 12:11:01 - INFO - root -   Epoch: [203/500][160/346], lr: 0.00000390 	 loss = 0.5866(0.1485)
2023/09/15 12:11:16 - INFO - root -   Epoch: [203/500][180/346], lr: 0.00000390 	 loss = 0.6949(0.1550)
2023/09/15 12:11:36 - INFO - root -   Epoch: [203/500][200/346], lr: 0.00000390 	 loss = 0.0003(0.1468)
2023/09/15 12:11:52 - INFO - root -   Epoch: [203/500][220/346], lr: 0.00000390 	 loss = 1.4673(0.1811)
2023/09/15 12:12:12 - INFO - root -   Epoch: [203/500][240/346], lr: 0.00000390 	 loss = 0.0085(0.1719)
2023/09/15 12:12:28 - INFO - root -   Epoch: [203/500][260/346], lr: 0.00000390 	 loss = 0.8053(0.1660)
2023/09/15 12:12:48 - INFO - root -   Epoch: [203/500][280/346], lr: 0.00000390 	 loss = 0.0410(0.1623)
2023/09/15 12:13:03 - INFO - root -   Epoch: [203/500][300/346], lr: 0.00000390 	 loss = 0.0414(0.1557)
2023/09/15 12:13:23 - INFO - root -   Epoch: [203/500][320/346], lr: 0.00000390 	 loss = 0.0018(0.1483)
2023/09/15 12:13:39 - INFO - root -   Epoch: [203/500][340/346], lr: 0.00000390 	 loss = 0.0796(0.1467)
2023/09/15 12:13:41 - INFO - root -   Epoch: [203/500] 	 loss = 0.1449
2023/09/15 12:13:41 - INFO - root -   train_accuracy = 0.9523
2023/09/15 12:13:49 - INFO - root -   Epoch: [204/500][0/346], lr: 0.00000392 	 loss = 0.0189(0.0189)
2023/09/15 12:14:05 - INFO - root -   Epoch: [204/500][20/346], lr: 0.00000392 	 loss = 0.7016(0.1161)
2023/09/15 12:14:25 - INFO - root -   Epoch: [204/500][40/346], lr: 0.00000392 	 loss = 0.0206(0.0729)
2023/09/15 12:14:41 - INFO - root -   Epoch: [204/500][60/346], lr: 0.00000392 	 loss = 0.0056(0.1875)
2023/09/15 12:15:01 - INFO - root -   Epoch: [204/500][80/346], lr: 0.00000392 	 loss = 0.0079(0.1700)
2023/09/15 12:15:17 - INFO - root -   Epoch: [204/500][100/346], lr: 0.00000392 	 loss = 0.0006(0.2533)
2023/09/15 12:15:37 - INFO - root -   Epoch: [204/500][120/346], lr: 0.00000392 	 loss = 0.0130(0.2270)
2023/09/15 12:15:52 - INFO - root -   Epoch: [204/500][140/346], lr: 0.00000392 	 loss = 1.5284(0.2405)
2023/09/15 12:16:12 - INFO - root -   Epoch: [204/500][160/346], lr: 0.00000392 	 loss = 1.3353(0.2337)
2023/09/15 12:16:28 - INFO - root -   Epoch: [204/500][180/346], lr: 0.00000392 	 loss = 0.0080(0.2315)
2023/09/15 12:16:48 - INFO - root -   Epoch: [204/500][200/346], lr: 0.00000392 	 loss = 0.0001(0.2130)
2023/09/15 12:17:03 - INFO - root -   Epoch: [204/500][220/346], lr: 0.00000392 	 loss = 0.0461(0.1957)
2023/09/15 12:17:23 - INFO - root -   Epoch: [204/500][240/346], lr: 0.00000392 	 loss = 0.0042(0.1823)
2023/09/15 12:17:39 - INFO - root -   Epoch: [204/500][260/346], lr: 0.00000392 	 loss = 0.0977(0.1729)
2023/09/15 12:17:59 - INFO - root -   Epoch: [204/500][280/346], lr: 0.00000392 	 loss = 0.3747(0.1682)
2023/09/15 12:18:14 - INFO - root -   Epoch: [204/500][300/346], lr: 0.00000392 	 loss = 0.0663(0.1720)
2023/09/15 12:18:34 - INFO - root -   Epoch: [204/500][320/346], lr: 0.00000392 	 loss = 0.0005(0.1643)
2023/09/15 12:18:49 - INFO - root -   Epoch: [204/500][340/346], lr: 0.00000392 	 loss = 0.0024(0.1621)
2023/09/15 12:18:51 - INFO - root -   Epoch: [204/500] 	 loss = 0.1604
2023/09/15 12:20:10 - INFO - root -   precision = 0.5000
2023/09/15 12:20:10 - INFO - root -   eval_loss = 2.6566
2023/09/15 12:20:11 - INFO - root -   train_accuracy = 0.9581
2023/09/15 12:20:19 - INFO - root -   Epoch: [205/500][0/346], lr: 0.00000393 	 loss = 0.0008(0.0008)
2023/09/15 12:20:35 - INFO - root -   Epoch: [205/500][20/346], lr: 0.00000393 	 loss = 0.0127(0.1385)
2023/09/15 12:20:55 - INFO - root -   Epoch: [205/500][40/346], lr: 0.00000393 	 loss = 0.0011(0.1263)
2023/09/15 12:21:11 - INFO - root -   Epoch: [205/500][60/346], lr: 0.00000393 	 loss = 0.0057(0.1011)
2023/09/15 12:21:31 - INFO - root -   Epoch: [205/500][80/346], lr: 0.00000393 	 loss = 0.0025(0.0873)
2023/09/15 12:21:47 - INFO - root -   Epoch: [205/500][100/346], lr: 0.00000393 	 loss = 0.0009(0.1115)
2023/09/15 12:22:06 - INFO - root -   Epoch: [205/500][120/346], lr: 0.00000393 	 loss = 1.0360(0.1528)
2023/09/15 12:22:22 - INFO - root -   Epoch: [205/500][140/346], lr: 0.00000393 	 loss = 0.0144(0.1488)
2023/09/15 12:22:41 - INFO - root -   Epoch: [205/500][160/346], lr: 0.00000393 	 loss = 0.5170(0.1379)
2023/09/15 12:22:58 - INFO - root -   Epoch: [205/500][180/346], lr: 0.00000393 	 loss = 0.0080(0.1405)
2023/09/15 12:23:16 - INFO - root -   Epoch: [205/500][200/346], lr: 0.00000393 	 loss = 0.0018(0.1327)
2023/09/15 12:23:33 - INFO - root -   Epoch: [205/500][220/346], lr: 0.00000393 	 loss = 0.0320(0.1255)
2023/09/15 12:23:50 - INFO - root -   Epoch: [205/500][240/346], lr: 0.00000393 	 loss = 0.0013(0.1189)
2023/09/15 12:24:08 - INFO - root -   Epoch: [205/500][260/346], lr: 0.00000393 	 loss = 0.0432(0.1155)
2023/09/15 12:24:25 - INFO - root -   Epoch: [205/500][280/346], lr: 0.00000393 	 loss = 0.0687(0.1135)
2023/09/15 12:24:44 - INFO - root -   Epoch: [205/500][300/346], lr: 0.00000393 	 loss = 0.0064(0.1123)
2023/09/15 12:25:00 - INFO - root -   Epoch: [205/500][320/346], lr: 0.00000393 	 loss = 0.0003(0.1155)
2023/09/15 12:25:19 - INFO - root -   Epoch: [205/500][340/346], lr: 0.00000393 	 loss = 0.0043(0.1140)
2023/09/15 12:25:21 - INFO - root -   Epoch: [205/500] 	 loss = 0.1169
2023/09/15 12:25:21 - INFO - root -   train_accuracy = 0.9624
2023/09/15 12:25:29 - INFO - root -   Epoch: [206/500][0/346], lr: 0.00000395 	 loss = 0.0022(0.0022)
2023/09/15 12:25:45 - INFO - root -   Epoch: [206/500][20/346], lr: 0.00000395 	 loss = 0.1342(0.0519)
2023/09/15 12:26:05 - INFO - root -   Epoch: [206/500][40/346], lr: 0.00000395 	 loss = 0.0309(0.0438)
2023/09/15 12:26:21 - INFO - root -   Epoch: [206/500][60/346], lr: 0.00000395 	 loss = 0.0243(0.0487)
2023/09/15 12:26:40 - INFO - root -   Epoch: [206/500][80/346], lr: 0.00000395 	 loss = 0.1959(0.0518)
2023/09/15 12:26:56 - INFO - root -   Epoch: [206/500][100/346], lr: 0.00000395 	 loss = 0.0562(0.0557)
2023/09/15 12:27:16 - INFO - root -   Epoch: [206/500][120/346], lr: 0.00000395 	 loss = 0.0227(0.0543)
2023/09/15 12:27:32 - INFO - root -   Epoch: [206/500][140/346], lr: 0.00000395 	 loss = 0.0047(0.0599)
2023/09/15 12:27:52 - INFO - root -   Epoch: [206/500][160/346], lr: 0.00000395 	 loss = 0.5959(0.0596)
2023/09/15 12:28:08 - INFO - root -   Epoch: [206/500][180/346], lr: 0.00000395 	 loss = 0.0605(0.0616)
2023/09/15 12:28:28 - INFO - root -   Epoch: [206/500][200/346], lr: 0.00000395 	 loss = 0.0000(0.0619)
2023/09/15 12:28:43 - INFO - root -   Epoch: [206/500][220/346], lr: 0.00000395 	 loss = 0.0171(0.0618)
2023/09/15 12:29:03 - INFO - root -   Epoch: [206/500][240/346], lr: 0.00000395 	 loss = 0.0006(0.0587)
2023/09/15 12:29:19 - INFO - root -   Epoch: [206/500][260/346], lr: 0.00000395 	 loss = 0.0358(0.0578)
2023/09/15 12:29:38 - INFO - root -   Epoch: [206/500][280/346], lr: 0.00000395 	 loss = 0.0229(0.0693)
2023/09/15 12:29:54 - INFO - root -   Epoch: [206/500][300/346], lr: 0.00000395 	 loss = 0.0650(0.0658)
2023/09/15 12:30:14 - INFO - root -   Epoch: [206/500][320/346], lr: 0.00000395 	 loss = 0.0309(0.0640)
2023/09/15 12:30:30 - INFO - root -   Epoch: [206/500][340/346], lr: 0.00000395 	 loss = 0.0016(0.0642)
2023/09/15 12:30:32 - INFO - root -   Epoch: [206/500] 	 loss = 0.0635
2023/09/15 12:30:32 - INFO - root -   train_accuracy = 0.9798
2023/09/15 12:30:40 - INFO - root -   Epoch: [207/500][0/346], lr: 0.00000396 	 loss = 0.0032(0.0032)
2023/09/15 12:30:56 - INFO - root -   Epoch: [207/500][20/346], lr: 0.00000396 	 loss = 0.0635(0.1384)
2023/09/15 12:31:16 - INFO - root -   Epoch: [207/500][40/346], lr: 0.00000396 	 loss = 0.0010(0.2456)
2023/09/15 12:31:32 - INFO - root -   Epoch: [207/500][60/346], lr: 0.00000396 	 loss = 0.0131(0.2580)
2023/09/15 12:31:52 - INFO - root -   Epoch: [207/500][80/346], lr: 0.00000396 	 loss = 0.0567(0.2505)
2023/09/15 12:32:08 - INFO - root -   Epoch: [207/500][100/346], lr: 0.00000396 	 loss = 0.0363(0.2171)
2023/09/15 12:32:28 - INFO - root -   Epoch: [207/500][120/346], lr: 0.00000396 	 loss = 0.0025(0.1882)
2023/09/15 12:32:44 - INFO - root -   Epoch: [207/500][140/346], lr: 0.00000396 	 loss = 0.0010(0.1681)
2023/09/15 12:33:03 - INFO - root -   Epoch: [207/500][160/346], lr: 0.00000396 	 loss = 0.9873(0.1566)
2023/09/15 12:33:19 - INFO - root -   Epoch: [207/500][180/346], lr: 0.00000396 	 loss = 0.0061(0.1480)
2023/09/15 12:33:39 - INFO - root -   Epoch: [207/500][200/346], lr: 0.00000396 	 loss = 0.0001(0.1367)
2023/09/15 12:33:55 - INFO - root -   Epoch: [207/500][220/346], lr: 0.00000396 	 loss = 0.0460(0.1298)
2023/09/15 12:34:15 - INFO - root -   Epoch: [207/500][240/346], lr: 0.00000396 	 loss = 0.0003(0.1258)
2023/09/15 12:34:30 - INFO - root -   Epoch: [207/500][260/346], lr: 0.00000396 	 loss = 0.0824(0.1208)
2023/09/15 12:34:50 - INFO - root -   Epoch: [207/500][280/346], lr: 0.00000396 	 loss = 0.7883(0.1209)
2023/09/15 12:35:06 - INFO - root -   Epoch: [207/500][300/346], lr: 0.00000396 	 loss = 0.0322(0.1260)
2023/09/15 12:35:25 - INFO - root -   Epoch: [207/500][320/346], lr: 0.00000396 	 loss = 0.0011(0.1314)
2023/09/15 12:35:41 - INFO - root -   Epoch: [207/500][340/346], lr: 0.00000396 	 loss = 0.0016(0.1332)
2023/09/15 12:35:43 - INFO - root -   Epoch: [207/500] 	 loss = 0.1315
2023/09/15 12:35:43 - INFO - root -   train_accuracy = 0.9523
2023/09/15 12:35:51 - INFO - root -   Epoch: [208/500][0/346], lr: 0.00000398 	 loss = 0.1901(0.1901)
2023/09/15 12:36:07 - INFO - root -   Epoch: [208/500][20/346], lr: 0.00000398 	 loss = 0.0047(0.3870)
2023/09/15 12:36:27 - INFO - root -   Epoch: [208/500][40/346], lr: 0.00000398 	 loss = 0.0071(0.3993)
2023/09/15 12:36:42 - INFO - root -   Epoch: [208/500][60/346], lr: 0.00000398 	 loss = 0.0726(0.3742)
2023/09/15 12:37:02 - INFO - root -   Epoch: [208/500][80/346], lr: 0.00000398 	 loss = 0.0168(0.2977)
2023/09/15 12:37:18 - INFO - root -   Epoch: [208/500][100/346], lr: 0.00000398 	 loss = 0.0267(0.2764)
2023/09/15 12:37:38 - INFO - root -   Epoch: [208/500][120/346], lr: 0.00000398 	 loss = 0.0012(0.2374)
2023/09/15 12:37:54 - INFO - root -   Epoch: [208/500][140/346], lr: 0.00000398 	 loss = 2.1399(0.2288)
2023/09/15 12:38:14 - INFO - root -   Epoch: [208/500][160/346], lr: 0.00000398 	 loss = 0.0347(0.2140)
2023/09/15 12:38:30 - INFO - root -   Epoch: [208/500][180/346], lr: 0.00000398 	 loss = 0.1008(0.2198)
2023/09/15 12:38:49 - INFO - root -   Epoch: [208/500][200/346], lr: 0.00000398 	 loss = 0.0000(0.2526)
2023/09/15 12:39:05 - INFO - root -   Epoch: [208/500][220/346], lr: 0.00000398 	 loss = 0.0534(0.2796)
2023/09/15 12:39:25 - INFO - root -   Epoch: [208/500][240/346], lr: 0.00000398 	 loss = 0.0120(0.2621)
2023/09/15 12:39:41 - INFO - root -   Epoch: [208/500][260/346], lr: 0.00000398 	 loss = 0.3798(0.2516)
2023/09/15 12:40:01 - INFO - root -   Epoch: [208/500][280/346], lr: 0.00000398 	 loss = 1.2370(0.2533)
2023/09/15 12:40:17 - INFO - root -   Epoch: [208/500][300/346], lr: 0.00000398 	 loss = 0.1441(0.2515)
2023/09/15 12:40:37 - INFO - root -   Epoch: [208/500][320/346], lr: 0.00000398 	 loss = 0.0003(0.2431)
2023/09/15 12:40:52 - INFO - root -   Epoch: [208/500][340/346], lr: 0.00000398 	 loss = 0.4556(0.2396)
2023/09/15 12:40:54 - INFO - root -   Epoch: [208/500] 	 loss = 0.2366
2023/09/15 12:40:54 - INFO - root -   train_accuracy = 0.9263
2023/09/15 12:41:02 - INFO - root -   Epoch: [209/500][0/346], lr: 0.00000399 	 loss = 1.7070(1.7070)
2023/09/15 12:41:18 - INFO - root -   Epoch: [209/500][20/346], lr: 0.00000399 	 loss = 0.1165(0.2987)
2023/09/15 12:41:38 - INFO - root -   Epoch: [209/500][40/346], lr: 0.00000399 	 loss = 0.0071(0.2731)
2023/09/15 12:41:54 - INFO - root -   Epoch: [209/500][60/346], lr: 0.00000399 	 loss = 0.0541(0.1997)
2023/09/15 12:42:13 - INFO - root -   Epoch: [209/500][80/346], lr: 0.00000399 	 loss = 0.0106(0.1785)
2023/09/15 12:42:29 - INFO - root -   Epoch: [209/500][100/346], lr: 0.00000399 	 loss = 0.0096(0.1742)
2023/09/15 12:42:49 - INFO - root -   Epoch: [209/500][120/346], lr: 0.00000399 	 loss = 0.0268(0.1660)
2023/09/15 12:43:05 - INFO - root -   Epoch: [209/500][140/346], lr: 0.00000399 	 loss = 0.0088(0.1531)
2023/09/15 12:43:25 - INFO - root -   Epoch: [209/500][160/346], lr: 0.00000399 	 loss = 0.8872(0.1514)
2023/09/15 12:43:41 - INFO - root -   Epoch: [209/500][180/346], lr: 0.00000399 	 loss = 0.0459(0.1482)
2023/09/15 12:44:00 - INFO - root -   Epoch: [209/500][200/346], lr: 0.00000399 	 loss = 0.0004(0.1393)
2023/09/15 12:44:16 - INFO - root -   Epoch: [209/500][220/346], lr: 0.00000399 	 loss = 0.0064(0.1445)
2023/09/15 12:44:36 - INFO - root -   Epoch: [209/500][240/346], lr: 0.00000399 	 loss = 0.0013(0.1485)
2023/09/15 12:44:51 - INFO - root -   Epoch: [209/500][260/346], lr: 0.00000399 	 loss = 0.1200(0.1786)
2023/09/15 12:45:11 - INFO - root -   Epoch: [209/500][280/346], lr: 0.00000399 	 loss = 0.0212(0.1857)
2023/09/15 12:45:27 - INFO - root -   Epoch: [209/500][300/346], lr: 0.00000399 	 loss = 0.0655(0.1961)
2023/09/15 12:45:46 - INFO - root -   Epoch: [209/500][320/346], lr: 0.00000399 	 loss = 0.0093(0.1980)
2023/09/15 12:46:01 - INFO - root -   Epoch: [209/500][340/346], lr: 0.00000399 	 loss = 0.0899(0.1895)
2023/09/15 12:46:03 - INFO - root -   Epoch: [209/500] 	 loss = 0.1884
2023/09/15 12:47:22 - INFO - root -   precision = 0.5172
2023/09/15 12:47:22 - INFO - root -   eval_loss = 2.4031
2023/09/15 12:47:23 - INFO - root -   train_accuracy = 0.9306
2023/09/15 12:47:31 - INFO - root -   Epoch: [210/500][0/346], lr: 0.00000400 	 loss = 0.0055(0.0055)
2023/09/15 12:47:47 - INFO - root -   Epoch: [210/500][20/346], lr: 0.00000400 	 loss = 0.0398(0.0630)
2023/09/15 12:48:08 - INFO - root -   Epoch: [210/500][40/346], lr: 0.00000400 	 loss = 0.0015(0.0615)
2023/09/15 12:48:24 - INFO - root -   Epoch: [210/500][60/346], lr: 0.00000400 	 loss = 0.0069(0.0607)
2023/09/15 12:48:44 - INFO - root -   Epoch: [210/500][80/346], lr: 0.00000400 	 loss = 0.0146(0.0605)
2023/09/15 12:49:00 - INFO - root -   Epoch: [210/500][100/346], lr: 0.00000400 	 loss = 0.0042(0.0910)
2023/09/15 12:49:20 - INFO - root -   Epoch: [210/500][120/346], lr: 0.00000400 	 loss = 1.4371(0.1831)
2023/09/15 12:49:37 - INFO - root -   Epoch: [210/500][140/346], lr: 0.00000400 	 loss = 0.1169(0.1751)
2023/09/15 12:49:57 - INFO - root -   Epoch: [210/500][160/346], lr: 0.00000400 	 loss = 1.4733(0.1652)
2023/09/15 12:50:13 - INFO - root -   Epoch: [210/500][180/346], lr: 0.00000400 	 loss = 0.1831(0.1788)
2023/09/15 12:50:33 - INFO - root -   Epoch: [210/500][200/346], lr: 0.00000400 	 loss = 0.0003(0.1963)
2023/09/15 12:50:50 - INFO - root -   Epoch: [210/500][220/346], lr: 0.00000400 	 loss = 0.1007(0.1872)
2023/09/15 12:51:10 - INFO - root -   Epoch: [210/500][240/346], lr: 0.00000400 	 loss = 0.0022(0.1875)
2023/09/15 12:51:26 - INFO - root -   Epoch: [210/500][260/346], lr: 0.00000400 	 loss = 0.1161(0.1787)
2023/09/15 12:51:47 - INFO - root -   Epoch: [210/500][280/346], lr: 0.00000400 	 loss = 0.0014(0.1711)
2023/09/15 12:52:03 - INFO - root -   Epoch: [210/500][300/346], lr: 0.00000400 	 loss = 0.0807(0.1629)
2023/09/15 12:52:23 - INFO - root -   Epoch: [210/500][320/346], lr: 0.00000400 	 loss = 0.0068(0.1574)
2023/09/15 12:52:39 - INFO - root -   Epoch: [210/500][340/346], lr: 0.00000400 	 loss = 0.0090(0.1536)
2023/09/15 12:52:41 - INFO - root -   Epoch: [210/500] 	 loss = 0.1522
2023/09/15 12:52:41 - INFO - root -   train_accuracy = 0.9566
2023/09/15 12:52:50 - INFO - root -   Epoch: [211/500][0/346], lr: 0.00000402 	 loss = 0.0148(0.0148)
2023/09/15 12:53:06 - INFO - root -   Epoch: [211/500][20/346], lr: 0.00000402 	 loss = 0.0584(0.0264)
2023/09/15 12:53:26 - INFO - root -   Epoch: [211/500][40/346], lr: 0.00000402 	 loss = 0.0087(0.0482)
2023/09/15 12:53:42 - INFO - root -   Epoch: [211/500][60/346], lr: 0.00000402 	 loss = 0.0036(0.0496)
2023/09/15 12:54:02 - INFO - root -   Epoch: [211/500][80/346], lr: 0.00000402 	 loss = 0.2031(0.0515)
2023/09/15 12:54:18 - INFO - root -   Epoch: [211/500][100/346], lr: 0.00000402 	 loss = 0.0400(0.0586)
2023/09/15 12:54:39 - INFO - root -   Epoch: [211/500][120/346], lr: 0.00000402 	 loss = 0.0022(0.0540)
2023/09/15 12:54:55 - INFO - root -   Epoch: [211/500][140/346], lr: 0.00000402 	 loss = 0.0171(0.0520)
2023/09/15 12:55:15 - INFO - root -   Epoch: [211/500][160/346], lr: 0.00000402 	 loss = 0.3546(0.0524)
2023/09/15 12:55:32 - INFO - root -   Epoch: [211/500][180/346], lr: 0.00000402 	 loss = 1.5185(0.0806)
2023/09/15 12:55:51 - INFO - root -   Epoch: [211/500][200/346], lr: 0.00000402 	 loss = 0.0001(0.0828)
2023/09/15 12:56:08 - INFO - root -   Epoch: [211/500][220/346], lr: 0.00000402 	 loss = 0.1168(0.0810)
2023/09/15 12:56:27 - INFO - root -   Epoch: [211/500][240/346], lr: 0.00000402 	 loss = 0.0029(0.0798)
2023/09/15 12:56:44 - INFO - root -   Epoch: [211/500][260/346], lr: 0.00000402 	 loss = 0.1055(0.0749)
2023/09/15 12:57:03 - INFO - root -   Epoch: [211/500][280/346], lr: 0.00000402 	 loss = 0.0302(0.0716)
2023/09/15 12:57:21 - INFO - root -   Epoch: [211/500][300/346], lr: 0.00000402 	 loss = 0.0071(0.0676)
2023/09/15 12:57:39 - INFO - root -   Epoch: [211/500][320/346], lr: 0.00000402 	 loss = 0.0015(0.0656)
2023/09/15 12:57:56 - INFO - root -   Epoch: [211/500][340/346], lr: 0.00000402 	 loss = 0.0238(0.0632)
2023/09/15 12:57:58 - INFO - root -   Epoch: [211/500] 	 loss = 0.0624
2023/09/15 12:57:58 - INFO - root -   train_accuracy = 0.9841
2023/09/15 12:58:07 - INFO - root -   Epoch: [212/500][0/346], lr: 0.00000403 	 loss = 0.0193(0.0193)
2023/09/15 12:58:23 - INFO - root -   Epoch: [212/500][20/346], lr: 0.00000403 	 loss = 0.0177(0.0227)
2023/09/15 12:58:44 - INFO - root -   Epoch: [212/500][40/346], lr: 0.00000403 	 loss = 0.0037(0.0222)
2023/09/15 12:59:00 - INFO - root -   Epoch: [212/500][60/346], lr: 0.00000403 	 loss = 0.0016(0.0207)
2023/09/15 12:59:20 - INFO - root -   Epoch: [212/500][80/346], lr: 0.00000403 	 loss = 0.1069(0.0268)
2023/09/15 12:59:37 - INFO - root -   Epoch: [212/500][100/346], lr: 0.00000403 	 loss = 0.0216(0.0292)
2023/09/15 12:59:57 - INFO - root -   Epoch: [212/500][120/346], lr: 0.00000403 	 loss = 0.0042(0.0526)
2023/09/15 13:00:13 - INFO - root -   Epoch: [212/500][140/346], lr: 0.00000403 	 loss = 0.0758(0.0720)
2023/09/15 13:00:33 - INFO - root -   Epoch: [212/500][160/346], lr: 0.00000403 	 loss = 0.2409(0.0678)
2023/09/15 13:00:50 - INFO - root -   Epoch: [212/500][180/346], lr: 0.00000403 	 loss = 0.0003(0.0773)
2023/09/15 13:01:10 - INFO - root -   Epoch: [212/500][200/346], lr: 0.00000403 	 loss = 0.0000(0.0863)
2023/09/15 13:01:26 - INFO - root -   Epoch: [212/500][220/346], lr: 0.00000403 	 loss = 0.0750(0.0833)
2023/09/15 13:01:46 - INFO - root -   Epoch: [212/500][240/346], lr: 0.00000403 	 loss = 0.0738(0.0997)
2023/09/15 13:02:03 - INFO - root -   Epoch: [212/500][260/346], lr: 0.00000403 	 loss = 0.4653(0.1121)
2023/09/15 13:02:23 - INFO - root -   Epoch: [212/500][280/346], lr: 0.00000403 	 loss = 0.0063(0.1065)
2023/09/15 13:02:39 - INFO - root -   Epoch: [212/500][300/346], lr: 0.00000403 	 loss = 0.0391(0.1010)
2023/09/15 13:02:59 - INFO - root -   Epoch: [212/500][320/346], lr: 0.00000403 	 loss = 0.0002(0.0992)
2023/09/15 13:03:15 - INFO - root -   Epoch: [212/500][340/346], lr: 0.00000403 	 loss = 0.6696(0.1065)
2023/09/15 13:03:17 - INFO - root -   Epoch: [212/500] 	 loss = 0.1053
2023/09/15 13:03:17 - INFO - root -   train_accuracy = 0.9711
2023/09/15 13:03:25 - INFO - root -   Epoch: [213/500][0/346], lr: 0.00000405 	 loss = 0.0070(0.0070)
2023/09/15 13:03:41 - INFO - root -   Epoch: [213/500][20/346], lr: 0.00000405 	 loss = 0.1514(0.0733)
2023/09/15 13:04:01 - INFO - root -   Epoch: [213/500][40/346], lr: 0.00000405 	 loss = 0.0125(0.0684)
2023/09/15 13:04:18 - INFO - root -   Epoch: [213/500][60/346], lr: 0.00000405 	 loss = 0.0024(0.0557)
2023/09/15 13:04:38 - INFO - root -   Epoch: [213/500][80/346], lr: 0.00000405 	 loss = 0.0183(0.1379)
2023/09/15 13:04:54 - INFO - root -   Epoch: [213/500][100/346], lr: 0.00000405 	 loss = 0.0707(0.1649)
2023/09/15 13:05:15 - INFO - root -   Epoch: [213/500][120/346], lr: 0.00000405 	 loss = 0.0050(0.1537)
2023/09/15 13:05:31 - INFO - root -   Epoch: [213/500][140/346], lr: 0.00000405 	 loss = 1.1883(0.1506)
2023/09/15 13:05:51 - INFO - root -   Epoch: [213/500][160/346], lr: 0.00000405 	 loss = 0.1428(0.1609)
2023/09/15 13:06:07 - INFO - root -   Epoch: [213/500][180/346], lr: 0.00000405 	 loss = 1.0667(0.1755)
2023/09/15 13:06:28 - INFO - root -   Epoch: [213/500][200/346], lr: 0.00000405 	 loss = 0.0003(0.1747)
2023/09/15 13:06:44 - INFO - root -   Epoch: [213/500][220/346], lr: 0.00000405 	 loss = 0.0110(0.1706)
2023/09/15 13:07:04 - INFO - root -   Epoch: [213/500][240/346], lr: 0.00000405 	 loss = 0.0563(0.1615)
2023/09/15 13:07:20 - INFO - root -   Epoch: [213/500][260/346], lr: 0.00000405 	 loss = 1.2300(0.1599)
2023/09/15 13:07:40 - INFO - root -   Epoch: [213/500][280/346], lr: 0.00000405 	 loss = 0.0379(0.1550)
2023/09/15 13:07:56 - INFO - root -   Epoch: [213/500][300/346], lr: 0.00000405 	 loss = 0.0117(0.1488)
2023/09/15 13:08:16 - INFO - root -   Epoch: [213/500][320/346], lr: 0.00000405 	 loss = 0.0002(0.1404)
2023/09/15 13:08:31 - INFO - root -   Epoch: [213/500][340/346], lr: 0.00000405 	 loss = 0.1737(0.1339)
2023/09/15 13:08:33 - INFO - root -   Epoch: [213/500] 	 loss = 0.1330
2023/09/15 13:08:33 - INFO - root -   train_accuracy = 0.9480
2023/09/15 13:08:42 - INFO - root -   Epoch: [214/500][0/346], lr: 0.00000406 	 loss = 0.0436(0.0436)
2023/09/15 13:08:58 - INFO - root -   Epoch: [214/500][20/346], lr: 0.00000406 	 loss = 0.1980(0.0694)
2023/09/15 13:09:19 - INFO - root -   Epoch: [214/500][40/346], lr: 0.00000406 	 loss = 0.0392(0.0462)
2023/09/15 13:09:35 - INFO - root -   Epoch: [214/500][60/346], lr: 0.00000406 	 loss = 0.0045(0.0780)
2023/09/15 13:09:56 - INFO - root -   Epoch: [214/500][80/346], lr: 0.00000406 	 loss = 0.0052(0.0880)
2023/09/15 13:10:12 - INFO - root -   Epoch: [214/500][100/346], lr: 0.00000406 	 loss = 0.0052(0.0968)
2023/09/15 13:10:33 - INFO - root -   Epoch: [214/500][120/346], lr: 0.00000406 	 loss = 0.0049(0.0842)
2023/09/15 13:10:49 - INFO - root -   Epoch: [214/500][140/346], lr: 0.00000406 	 loss = 0.0059(0.0879)
2023/09/15 13:11:10 - INFO - root -   Epoch: [214/500][160/346], lr: 0.00000406 	 loss = 0.0382(0.0824)
2023/09/15 13:11:27 - INFO - root -   Epoch: [214/500][180/346], lr: 0.00000406 	 loss = 0.0045(0.0828)
2023/09/15 13:11:47 - INFO - root -   Epoch: [214/500][200/346], lr: 0.00000406 	 loss = 0.0109(0.0884)
2023/09/15 13:12:04 - INFO - root -   Epoch: [214/500][220/346], lr: 0.00000406 	 loss = 0.1497(0.0896)
2023/09/15 13:12:24 - INFO - root -   Epoch: [214/500][240/346], lr: 0.00000406 	 loss = 0.0016(0.0960)
2023/09/15 13:12:41 - INFO - root -   Epoch: [214/500][260/346], lr: 0.00000406 	 loss = 2.7921(0.1151)
2023/09/15 13:13:01 - INFO - root -   Epoch: [214/500][280/346], lr: 0.00000406 	 loss = 1.1077(0.1408)
2023/09/15 13:13:18 - INFO - root -   Epoch: [214/500][300/346], lr: 0.00000406 	 loss = 0.0119(0.1467)
2023/09/15 13:13:37 - INFO - root -   Epoch: [214/500][320/346], lr: 0.00000406 	 loss = 0.0000(0.1437)
2023/09/15 13:13:54 - INFO - root -   Epoch: [214/500][340/346], lr: 0.00000406 	 loss = 0.6855(0.1444)
2023/09/15 13:13:56 - INFO - root -   Epoch: [214/500] 	 loss = 0.1434
2023/09/15 13:15:17 - INFO - root -   precision = 0.3736
2023/09/15 13:15:17 - INFO - root -   eval_loss = 3.7948
2023/09/15 13:15:18 - INFO - root -   train_accuracy = 0.9494
2023/09/15 13:15:26 - INFO - root -   Epoch: [215/500][0/346], lr: 0.00000408 	 loss = 0.0145(0.0145)
2023/09/15 13:15:42 - INFO - root -   Epoch: [215/500][20/346], lr: 0.00000408 	 loss = 0.0654(0.0837)
2023/09/15 13:16:03 - INFO - root -   Epoch: [215/500][40/346], lr: 0.00000408 	 loss = 0.0372(0.1004)
2023/09/15 13:16:19 - INFO - root -   Epoch: [215/500][60/346], lr: 0.00000408 	 loss = 0.0674(0.1790)
2023/09/15 13:16:39 - INFO - root -   Epoch: [215/500][80/346], lr: 0.00000408 	 loss = 0.0158(0.1454)
2023/09/15 13:16:55 - INFO - root -   Epoch: [215/500][100/346], lr: 0.00000408 	 loss = 0.0026(0.1392)
2023/09/15 13:17:16 - INFO - root -   Epoch: [215/500][120/346], lr: 0.00000408 	 loss = 0.0036(0.1231)
2023/09/15 13:17:32 - INFO - root -   Epoch: [215/500][140/346], lr: 0.00000408 	 loss = 0.1426(0.1117)
2023/09/15 13:17:52 - INFO - root -   Epoch: [215/500][160/346], lr: 0.00000408 	 loss = 0.2601(0.1076)
2023/09/15 13:18:08 - INFO - root -   Epoch: [215/500][180/346], lr: 0.00000408 	 loss = 0.0072(0.1045)
2023/09/15 13:18:29 - INFO - root -   Epoch: [215/500][200/346], lr: 0.00000408 	 loss = 0.0020(0.1029)
2023/09/15 13:18:45 - INFO - root -   Epoch: [215/500][220/346], lr: 0.00000408 	 loss = 0.0401(0.0985)
2023/09/15 13:19:05 - INFO - root -   Epoch: [215/500][240/346], lr: 0.00000408 	 loss = 0.0983(0.1107)
2023/09/15 13:19:21 - INFO - root -   Epoch: [215/500][260/346], lr: 0.00000408 	 loss = 0.0374(0.1073)
2023/09/15 13:19:41 - INFO - root -   Epoch: [215/500][280/346], lr: 0.00000408 	 loss = 0.0478(0.1024)
2023/09/15 13:19:57 - INFO - root -   Epoch: [215/500][300/346], lr: 0.00000408 	 loss = 0.0002(0.0967)
2023/09/15 13:20:18 - INFO - root -   Epoch: [215/500][320/346], lr: 0.00000408 	 loss = 0.0001(0.0930)
2023/09/15 13:20:34 - INFO - root -   Epoch: [215/500][340/346], lr: 0.00000408 	 loss = 0.0143(0.1077)
2023/09/15 13:20:36 - INFO - root -   Epoch: [215/500] 	 loss = 0.1066
2023/09/15 13:20:36 - INFO - root -   train_accuracy = 0.9639
2023/09/15 13:20:44 - INFO - root -   Epoch: [216/500][0/346], lr: 0.00000409 	 loss = 0.0068(0.0068)
2023/09/15 13:21:00 - INFO - root -   Epoch: [216/500][20/346], lr: 0.00000409 	 loss = 0.2791(0.0470)
2023/09/15 13:21:19 - INFO - root -   Epoch: [216/500][40/346], lr: 0.00000409 	 loss = 0.0026(0.0635)
2023/09/15 13:21:35 - INFO - root -   Epoch: [216/500][60/346], lr: 0.00000409 	 loss = 0.0039(0.0575)
2023/09/15 13:21:54 - INFO - root -   Epoch: [216/500][80/346], lr: 0.00000409 	 loss = 0.0254(0.1101)
2023/09/15 13:22:10 - INFO - root -   Epoch: [216/500][100/346], lr: 0.00000409 	 loss = 0.0135(0.1029)
2023/09/15 13:22:29 - INFO - root -   Epoch: [216/500][120/346], lr: 0.00000409 	 loss = 0.0017(0.0960)
2023/09/15 13:22:46 - INFO - root -   Epoch: [216/500][140/346], lr: 0.00000409 	 loss = 0.0278(0.1050)
2023/09/15 13:23:04 - INFO - root -   Epoch: [216/500][160/346], lr: 0.00000409 	 loss = 0.1295(0.0957)
2023/09/15 13:23:21 - INFO - root -   Epoch: [216/500][180/346], lr: 0.00000409 	 loss = 0.4310(0.1006)
2023/09/15 13:23:39 - INFO - root -   Epoch: [216/500][200/346], lr: 0.00000409 	 loss = 0.0001(0.0959)
2023/09/15 13:23:56 - INFO - root -   Epoch: [216/500][220/346], lr: 0.00000409 	 loss = 0.0131(0.0930)
2023/09/15 13:24:14 - INFO - root -   Epoch: [216/500][240/346], lr: 0.00000409 	 loss = 0.0009(0.1055)
2023/09/15 13:24:31 - INFO - root -   Epoch: [216/500][260/346], lr: 0.00000409 	 loss = 0.4131(0.0999)
2023/09/15 13:24:49 - INFO - root -   Epoch: [216/500][280/346], lr: 0.00000409 	 loss = 0.0175(0.0955)
2023/09/15 13:25:07 - INFO - root -   Epoch: [216/500][300/346], lr: 0.00000409 	 loss = 0.0007(0.0978)
2023/09/15 13:25:24 - INFO - root -   Epoch: [216/500][320/346], lr: 0.00000409 	 loss = 1.8345(0.1402)
2023/09/15 13:25:41 - INFO - root -   Epoch: [216/500][340/346], lr: 0.00000409 	 loss = 0.0256(0.1498)
2023/09/15 13:25:43 - INFO - root -   Epoch: [216/500] 	 loss = 0.1520
2023/09/15 13:25:43 - INFO - root -   train_accuracy = 0.9523
2023/09/15 13:25:52 - INFO - root -   Epoch: [217/500][0/346], lr: 0.00000410 	 loss = 2.1862(2.1862)
2023/09/15 13:26:08 - INFO - root -   Epoch: [217/500][20/346], lr: 0.00000410 	 loss = 0.0200(0.4462)
2023/09/15 13:26:28 - INFO - root -   Epoch: [217/500][40/346], lr: 0.00000410 	 loss = 0.4616(0.3640)
2023/09/15 13:26:44 - INFO - root -   Epoch: [217/500][60/346], lr: 0.00000410 	 loss = 0.0310(0.2714)
2023/09/15 13:27:04 - INFO - root -   Epoch: [217/500][80/346], lr: 0.00000410 	 loss = 0.0288(0.2219)
2023/09/15 13:27:20 - INFO - root -   Epoch: [217/500][100/346], lr: 0.00000410 	 loss = 0.0196(0.2869)
2023/09/15 13:27:40 - INFO - root -   Epoch: [217/500][120/346], lr: 0.00000410 	 loss = 0.0333(0.2556)
2023/09/15 13:27:56 - INFO - root -   Epoch: [217/500][140/346], lr: 0.00000410 	 loss = 0.0502(0.2286)
2023/09/15 13:28:16 - INFO - root -   Epoch: [217/500][160/346], lr: 0.00000410 	 loss = 0.2994(0.2213)
2023/09/15 13:28:31 - INFO - root -   Epoch: [217/500][180/346], lr: 0.00000410 	 loss = 0.2400(0.2302)
2023/09/15 13:28:51 - INFO - root -   Epoch: [217/500][200/346], lr: 0.00000410 	 loss = 0.0000(0.2094)
2023/09/15 13:29:07 - INFO - root -   Epoch: [217/500][220/346], lr: 0.00000410 	 loss = 0.0742(0.1989)
2023/09/15 13:29:26 - INFO - root -   Epoch: [217/500][240/346], lr: 0.00000410 	 loss = 0.0028(0.1862)
2023/09/15 13:29:43 - INFO - root -   Epoch: [217/500][260/346], lr: 0.00000410 	 loss = 0.0043(0.1759)
2023/09/15 13:30:02 - INFO - root -   Epoch: [217/500][280/346], lr: 0.00000410 	 loss = 0.0348(0.1690)
2023/09/15 13:30:18 - INFO - root -   Epoch: [217/500][300/346], lr: 0.00000410 	 loss = 0.0050(0.1790)
2023/09/15 13:30:37 - INFO - root -   Epoch: [217/500][320/346], lr: 0.00000410 	 loss = 0.0014(0.1702)
2023/09/15 13:30:52 - INFO - root -   Epoch: [217/500][340/346], lr: 0.00000410 	 loss = 0.0799(0.1634)
2023/09/15 13:30:54 - INFO - root -   Epoch: [217/500] 	 loss = 0.1618
2023/09/15 13:30:54 - INFO - root -   train_accuracy = 0.9552
2023/09/15 13:31:02 - INFO - root -   Epoch: [218/500][0/346], lr: 0.00000412 	 loss = 0.0831(0.0831)
2023/09/15 13:31:18 - INFO - root -   Epoch: [218/500][20/346], lr: 0.00000412 	 loss = 0.0317(0.0315)
2023/09/15 13:31:38 - INFO - root -   Epoch: [218/500][40/346], lr: 0.00000412 	 loss = 0.0001(0.0269)
2023/09/15 13:31:54 - INFO - root -   Epoch: [218/500][60/346], lr: 0.00000412 	 loss = 0.0984(0.0255)
2023/09/15 13:32:14 - INFO - root -   Epoch: [218/500][80/346], lr: 0.00000412 	 loss = 0.0112(0.0298)
2023/09/15 13:32:30 - INFO - root -   Epoch: [218/500][100/346], lr: 0.00000412 	 loss = 0.0488(0.0333)
2023/09/15 13:32:50 - INFO - root -   Epoch: [218/500][120/346], lr: 0.00000412 	 loss = 0.0193(0.0445)
2023/09/15 13:33:06 - INFO - root -   Epoch: [218/500][140/346], lr: 0.00000412 	 loss = 0.0048(0.0407)
2023/09/15 13:33:25 - INFO - root -   Epoch: [218/500][160/346], lr: 0.00000412 	 loss = 0.4356(0.0408)
2023/09/15 13:33:41 - INFO - root -   Epoch: [218/500][180/346], lr: 0.00000412 	 loss = 0.4243(0.0559)
2023/09/15 13:34:00 - INFO - root -   Epoch: [218/500][200/346], lr: 0.00000412 	 loss = 0.0007(0.0613)
2023/09/15 13:34:17 - INFO - root -   Epoch: [218/500][220/346], lr: 0.00000412 	 loss = 0.0534(0.0709)
2023/09/15 13:34:35 - INFO - root -   Epoch: [218/500][240/346], lr: 0.00000412 	 loss = 0.0015(0.0678)
2023/09/15 13:34:52 - INFO - root -   Epoch: [218/500][260/346], lr: 0.00000412 	 loss = 0.0044(0.0647)
2023/09/15 13:35:10 - INFO - root -   Epoch: [218/500][280/346], lr: 0.00000412 	 loss = 0.0924(0.0614)
2023/09/15 13:35:28 - INFO - root -   Epoch: [218/500][300/346], lr: 0.00000412 	 loss = 0.0080(0.0630)
2023/09/15 13:35:45 - INFO - root -   Epoch: [218/500][320/346], lr: 0.00000412 	 loss = 0.0003(0.0639)
2023/09/15 13:36:03 - INFO - root -   Epoch: [218/500][340/346], lr: 0.00000412 	 loss = 0.0011(0.0657)
2023/09/15 13:36:05 - INFO - root -   Epoch: [218/500] 	 loss = 0.0650
2023/09/15 13:36:05 - INFO - root -   train_accuracy = 0.9827
2023/09/15 13:36:14 - INFO - root -   Epoch: [219/500][0/346], lr: 0.00000413 	 loss = 0.0007(0.0007)
2023/09/15 13:36:30 - INFO - root -   Epoch: [219/500][20/346], lr: 0.00000413 	 loss = 0.0055(0.0203)
2023/09/15 13:36:50 - INFO - root -   Epoch: [219/500][40/346], lr: 0.00000413 	 loss = 0.0018(0.1254)
2023/09/15 13:37:06 - INFO - root -   Epoch: [219/500][60/346], lr: 0.00000413 	 loss = 0.0293(0.1154)
2023/09/15 13:37:26 - INFO - root -   Epoch: [219/500][80/346], lr: 0.00000413 	 loss = 0.3165(0.0996)
2023/09/15 13:37:42 - INFO - root -   Epoch: [219/500][100/346], lr: 0.00000413 	 loss = 0.0039(0.1237)
2023/09/15 13:38:02 - INFO - root -   Epoch: [219/500][120/346], lr: 0.00000413 	 loss = 0.0032(0.1275)
2023/09/15 13:38:18 - INFO - root -   Epoch: [219/500][140/346], lr: 0.00000413 	 loss = 0.0115(0.1139)
2023/09/15 13:38:38 - INFO - root -   Epoch: [219/500][160/346], lr: 0.00000413 	 loss = 0.1151(0.1139)
2023/09/15 13:38:54 - INFO - root -   Epoch: [219/500][180/346], lr: 0.00000413 	 loss = 0.0001(0.1224)
2023/09/15 13:39:14 - INFO - root -   Epoch: [219/500][200/346], lr: 0.00000413 	 loss = 0.0000(0.1187)
2023/09/15 13:39:30 - INFO - root -   Epoch: [219/500][220/346], lr: 0.00000413 	 loss = 0.0613(0.1094)
2023/09/15 13:39:50 - INFO - root -   Epoch: [219/500][240/346], lr: 0.00000413 	 loss = 0.0191(0.1075)
2023/09/15 13:40:06 - INFO - root -   Epoch: [219/500][260/346], lr: 0.00000413 	 loss = 0.0271(0.1035)
2023/09/15 13:40:26 - INFO - root -   Epoch: [219/500][280/346], lr: 0.00000413 	 loss = 0.0104(0.0994)
2023/09/15 13:40:42 - INFO - root -   Epoch: [219/500][300/346], lr: 0.00000413 	 loss = 0.0855(0.1163)
2023/09/15 13:41:01 - INFO - root -   Epoch: [219/500][320/346], lr: 0.00000413 	 loss = 0.0001(0.1123)
2023/09/15 13:41:17 - INFO - root -   Epoch: [219/500][340/346], lr: 0.00000413 	 loss = 0.0991(0.1070)
2023/09/15 13:41:19 - INFO - root -   Epoch: [219/500] 	 loss = 0.1056
2023/09/15 13:42:38 - INFO - root -   precision = 0.5172
2023/09/15 13:42:38 - INFO - root -   eval_loss = 2.9829
2023/09/15 13:42:39 - INFO - root -   train_accuracy = 0.9783
2023/09/15 13:42:47 - INFO - root -   Epoch: [220/500][0/346], lr: 0.00000415 	 loss = 0.0060(0.0060)
2023/09/15 13:43:03 - INFO - root -   Epoch: [220/500][20/346], lr: 0.00000415 	 loss = 0.0474(0.0252)
2023/09/15 13:43:23 - INFO - root -   Epoch: [220/500][40/346], lr: 0.00000415 	 loss = 0.0078(0.0571)
2023/09/15 13:43:41 - INFO - root -   Epoch: [220/500][60/346], lr: 0.00000415 	 loss = 0.7811(0.1552)
2023/09/15 13:43:59 - INFO - root -   Epoch: [220/500][80/346], lr: 0.00000415 	 loss = 0.0161(0.1764)
2023/09/15 13:44:17 - INFO - root -   Epoch: [220/500][100/346], lr: 0.00000415 	 loss = 0.0247(0.1810)
2023/09/15 13:44:35 - INFO - root -   Epoch: [220/500][120/346], lr: 0.00000415 	 loss = 0.0027(0.2503)
2023/09/15 13:44:53 - INFO - root -   Epoch: [220/500][140/346], lr: 0.00000415 	 loss = 0.0038(0.2232)
2023/09/15 13:45:11 - INFO - root -   Epoch: [220/500][160/346], lr: 0.00000415 	 loss = 0.4725(0.2111)
2023/09/15 13:45:28 - INFO - root -   Epoch: [220/500][180/346], lr: 0.00000415 	 loss = 0.2391(0.1909)
2023/09/15 13:45:47 - INFO - root -   Epoch: [220/500][200/346], lr: 0.00000415 	 loss = 0.0025(0.1864)
2023/09/15 13:46:03 - INFO - root -   Epoch: [220/500][220/346], lr: 0.00000415 	 loss = 0.0391(0.1799)
2023/09/15 13:46:22 - INFO - root -   Epoch: [220/500][240/346], lr: 0.00000415 	 loss = 0.1582(0.1691)
2023/09/15 13:46:39 - INFO - root -   Epoch: [220/500][260/346], lr: 0.00000415 	 loss = 0.2376(0.1612)
2023/09/15 13:46:58 - INFO - root -   Epoch: [220/500][280/346], lr: 0.00000415 	 loss = 0.0039(0.1525)
2023/09/15 13:47:15 - INFO - root -   Epoch: [220/500][300/346], lr: 0.00000415 	 loss = 0.0065(0.1449)
2023/09/15 13:47:33 - INFO - root -   Epoch: [220/500][320/346], lr: 0.00000415 	 loss = 0.0003(0.1379)
2023/09/15 13:47:49 - INFO - root -   Epoch: [220/500][340/346], lr: 0.00000415 	 loss = 0.0114(0.1320)
2023/09/15 13:47:51 - INFO - root -   Epoch: [220/500] 	 loss = 0.1307
2023/09/15 13:47:51 - INFO - root -   train_accuracy = 0.9465
2023/09/15 13:47:59 - INFO - root -   Epoch: [221/500][0/346], lr: 0.00000416 	 loss = 1.7568(1.7568)
2023/09/15 13:48:15 - INFO - root -   Epoch: [221/500][20/346], lr: 0.00000416 	 loss = 0.2397(0.1993)
2023/09/15 13:48:35 - INFO - root -   Epoch: [221/500][40/346], lr: 0.00000416 	 loss = 0.0060(0.1666)
2023/09/15 13:48:51 - INFO - root -   Epoch: [221/500][60/346], lr: 0.00000416 	 loss = 0.0010(0.1319)
2023/09/15 13:49:11 - INFO - root -   Epoch: [221/500][80/346], lr: 0.00000416 	 loss = 0.0073(0.1251)
2023/09/15 13:49:27 - INFO - root -   Epoch: [221/500][100/346], lr: 0.00000416 	 loss = 0.0014(0.1234)
2023/09/15 13:49:47 - INFO - root -   Epoch: [221/500][120/346], lr: 0.00000416 	 loss = 0.0074(0.1066)
2023/09/15 13:50:03 - INFO - root -   Epoch: [221/500][140/346], lr: 0.00000416 	 loss = 0.0372(0.0949)
2023/09/15 13:50:22 - INFO - root -   Epoch: [221/500][160/346], lr: 0.00000416 	 loss = 0.9111(0.1102)
2023/09/15 13:50:38 - INFO - root -   Epoch: [221/500][180/346], lr: 0.00000416 	 loss = 0.1829(0.1218)
2023/09/15 13:50:58 - INFO - root -   Epoch: [221/500][200/346], lr: 0.00000416 	 loss = 0.0013(0.1180)
2023/09/15 13:51:13 - INFO - root -   Epoch: [221/500][220/346], lr: 0.00000416 	 loss = 0.0818(0.1163)
2023/09/15 13:51:33 - INFO - root -   Epoch: [221/500][240/346], lr: 0.00000416 	 loss = 0.0028(0.1115)
2023/09/15 13:51:49 - INFO - root -   Epoch: [221/500][260/346], lr: 0.00000416 	 loss = 0.0085(0.1089)
2023/09/15 13:52:09 - INFO - root -   Epoch: [221/500][280/346], lr: 0.00000416 	 loss = 0.0003(0.1141)
2023/09/15 13:52:25 - INFO - root -   Epoch: [221/500][300/346], lr: 0.00000416 	 loss = 0.0161(0.1197)
2023/09/15 13:52:45 - INFO - root -   Epoch: [221/500][320/346], lr: 0.00000416 	 loss = 0.0012(0.1205)
2023/09/15 13:53:00 - INFO - root -   Epoch: [221/500][340/346], lr: 0.00000416 	 loss = 0.0982(0.1226)
2023/09/15 13:53:02 - INFO - root -   Epoch: [221/500] 	 loss = 0.1210
2023/09/15 13:53:02 - INFO - root -   train_accuracy = 0.9494
2023/09/15 13:53:10 - INFO - root -   Epoch: [222/500][0/346], lr: 0.00000418 	 loss = 2.8342(2.8342)
2023/09/15 13:53:26 - INFO - root -   Epoch: [222/500][20/346], lr: 0.00000418 	 loss = 0.1018(0.1881)
2023/09/15 13:53:46 - INFO - root -   Epoch: [222/500][40/346], lr: 0.00000418 	 loss = 0.0033(0.1159)
2023/09/15 13:54:01 - INFO - root -   Epoch: [222/500][60/346], lr: 0.00000418 	 loss = 0.0016(0.0905)
2023/09/15 13:54:21 - INFO - root -   Epoch: [222/500][80/346], lr: 0.00000418 	 loss = 0.0367(0.1223)
2023/09/15 13:54:37 - INFO - root -   Epoch: [222/500][100/346], lr: 0.00000418 	 loss = 0.0042(0.1296)
2023/09/15 13:54:57 - INFO - root -   Epoch: [222/500][120/346], lr: 0.00000418 	 loss = 0.0030(0.1146)
2023/09/15 13:55:13 - INFO - root -   Epoch: [222/500][140/346], lr: 0.00000418 	 loss = 0.0174(0.1042)
2023/09/15 13:55:32 - INFO - root -   Epoch: [222/500][160/346], lr: 0.00000418 	 loss = 0.0417(0.0958)
2023/09/15 13:55:48 - INFO - root -   Epoch: [222/500][180/346], lr: 0.00000418 	 loss = 0.0038(0.0877)
2023/09/15 13:56:08 - INFO - root -   Epoch: [222/500][200/346], lr: 0.00000418 	 loss = 0.0036(0.0899)
2023/09/15 13:56:24 - INFO - root -   Epoch: [222/500][220/346], lr: 0.00000418 	 loss = 0.1427(0.0896)
2023/09/15 13:56:43 - INFO - root -   Epoch: [222/500][240/346], lr: 0.00000418 	 loss = 0.0020(0.0833)
2023/09/15 13:56:59 - INFO - root -   Epoch: [222/500][260/346], lr: 0.00000418 	 loss = 0.0021(0.0784)
2023/09/15 13:57:19 - INFO - root -   Epoch: [222/500][280/346], lr: 0.00000418 	 loss = 0.0062(0.0748)
2023/09/15 13:57:34 - INFO - root -   Epoch: [222/500][300/346], lr: 0.00000418 	 loss = 1.3813(0.0971)
2023/09/15 13:57:54 - INFO - root -   Epoch: [222/500][320/346], lr: 0.00000418 	 loss = 0.0111(0.1249)
2023/09/15 13:58:10 - INFO - root -   Epoch: [222/500][340/346], lr: 0.00000418 	 loss = 0.2943(0.1255)
2023/09/15 13:58:12 - INFO - root -   Epoch: [222/500] 	 loss = 0.1246
2023/09/15 13:58:12 - INFO - root -   train_accuracy = 0.9711
2023/09/15 13:58:20 - INFO - root -   Epoch: [223/500][0/346], lr: 0.00000419 	 loss = 0.0239(0.0239)
2023/09/15 13:58:36 - INFO - root -   Epoch: [223/500][20/346], lr: 0.00000419 	 loss = 0.0083(0.0473)
2023/09/15 13:58:55 - INFO - root -   Epoch: [223/500][40/346], lr: 0.00000419 	 loss = 0.0045(0.0392)
2023/09/15 13:59:12 - INFO - root -   Epoch: [223/500][60/346], lr: 0.00000419 	 loss = 0.0044(0.0370)
2023/09/15 13:59:31 - INFO - root -   Epoch: [223/500][80/346], lr: 0.00000419 	 loss = 0.0060(0.0403)
2023/09/15 13:59:47 - INFO - root -   Epoch: [223/500][100/346], lr: 0.00000419 	 loss = 0.0117(0.0368)
2023/09/15 14:00:07 - INFO - root -   Epoch: [223/500][120/346], lr: 0.00000419 	 loss = 0.3258(0.0383)
2023/09/15 14:00:23 - INFO - root -   Epoch: [223/500][140/346], lr: 0.00000419 	 loss = 0.0613(0.0427)
2023/09/15 14:00:41 - INFO - root -   Epoch: [223/500][160/346], lr: 0.00000419 	 loss = 0.6138(0.0450)
2023/09/15 14:00:58 - INFO - root -   Epoch: [223/500][180/346], lr: 0.00000419 	 loss = 0.8091(0.0697)
2023/09/15 14:01:16 - INFO - root -   Epoch: [223/500][200/346], lr: 0.00000419 	 loss = 0.0014(0.0766)
2023/09/15 14:01:34 - INFO - root -   Epoch: [223/500][220/346], lr: 0.00000419 	 loss = 1.1288(0.0859)
2023/09/15 14:01:51 - INFO - root -   Epoch: [223/500][240/346], lr: 0.00000419 	 loss = 0.0045(0.0843)
2023/09/15 14:02:09 - INFO - root -   Epoch: [223/500][260/346], lr: 0.00000419 	 loss = 0.0550(0.0827)
2023/09/15 14:02:26 - INFO - root -   Epoch: [223/500][280/346], lr: 0.00000419 	 loss = 0.1494(0.0959)
2023/09/15 14:02:44 - INFO - root -   Epoch: [223/500][300/346], lr: 0.00000419 	 loss = 0.1583(0.0912)
2023/09/15 14:03:01 - INFO - root -   Epoch: [223/500][320/346], lr: 0.00000419 	 loss = 0.0071(0.0900)
2023/09/15 14:03:19 - INFO - root -   Epoch: [223/500][340/346], lr: 0.00000419 	 loss = 0.1057(0.0936)
2023/09/15 14:03:21 - INFO - root -   Epoch: [223/500] 	 loss = 0.0939
2023/09/15 14:03:21 - INFO - root -   train_accuracy = 0.9783
2023/09/15 14:03:29 - INFO - root -   Epoch: [224/500][0/346], lr: 0.00000420 	 loss = 0.0682(0.0682)
2023/09/15 14:03:45 - INFO - root -   Epoch: [224/500][20/346], lr: 0.00000420 	 loss = 0.1336(0.1002)
2023/09/15 14:04:05 - INFO - root -   Epoch: [224/500][40/346], lr: 0.00000420 	 loss = 0.0050(0.2260)
2023/09/15 14:04:21 - INFO - root -   Epoch: [224/500][60/346], lr: 0.00000420 	 loss = 0.0327(0.1707)
2023/09/15 14:04:40 - INFO - root -   Epoch: [224/500][80/346], lr: 0.00000420 	 loss = 0.0243(0.1668)
2023/09/15 14:04:57 - INFO - root -   Epoch: [224/500][100/346], lr: 0.00000420 	 loss = 0.0036(0.1704)
2023/09/15 14:05:15 - INFO - root -   Epoch: [224/500][120/346], lr: 0.00000420 	 loss = 0.0497(0.1709)
2023/09/15 14:05:32 - INFO - root -   Epoch: [224/500][140/346], lr: 0.00000420 	 loss = 1.2971(0.1637)
2023/09/15 14:05:50 - INFO - root -   Epoch: [224/500][160/346], lr: 0.00000420 	 loss = 0.3775(0.1519)
2023/09/15 14:06:08 - INFO - root -   Epoch: [224/500][180/346], lr: 0.00000420 	 loss = 0.0089(0.1456)
2023/09/15 14:06:25 - INFO - root -   Epoch: [224/500][200/346], lr: 0.00000420 	 loss = 0.0074(0.1515)
2023/09/15 14:06:43 - INFO - root -   Epoch: [224/500][220/346], lr: 0.00000420 	 loss = 0.0797(0.1669)
2023/09/15 14:07:01 - INFO - root -   Epoch: [224/500][240/346], lr: 0.00000420 	 loss = 0.0066(0.1902)
2023/09/15 14:07:19 - INFO - root -   Epoch: [224/500][260/346], lr: 0.00000420 	 loss = 0.1431(0.1821)
2023/09/15 14:07:36 - INFO - root -   Epoch: [224/500][280/346], lr: 0.00000420 	 loss = 0.4058(0.1732)
2023/09/15 14:07:55 - INFO - root -   Epoch: [224/500][300/346], lr: 0.00000420 	 loss = 0.1286(0.1644)
2023/09/15 14:08:12 - INFO - root -   Epoch: [224/500][320/346], lr: 0.00000420 	 loss = 0.0018(0.1593)
2023/09/15 14:08:30 - INFO - root -   Epoch: [224/500][340/346], lr: 0.00000420 	 loss = 0.0016(0.1550)
2023/09/15 14:08:32 - INFO - root -   Epoch: [224/500] 	 loss = 0.1529
2023/09/15 14:09:51 - INFO - root -   precision = 0.4885
2023/09/15 14:09:51 - INFO - root -   eval_loss = 2.7403
2023/09/15 14:09:52 - INFO - root -   train_accuracy = 0.9610
2023/09/15 14:10:01 - INFO - root -   Epoch: [225/500][0/346], lr: 0.00000422 	 loss = 0.0029(0.0029)
2023/09/15 14:10:17 - INFO - root -   Epoch: [225/500][20/346], lr: 0.00000422 	 loss = 0.0071(0.0109)
2023/09/15 14:10:37 - INFO - root -   Epoch: [225/500][40/346], lr: 0.00000422 	 loss = 0.0087(0.0189)
2023/09/15 14:10:53 - INFO - root -   Epoch: [225/500][60/346], lr: 0.00000422 	 loss = 0.7552(0.0905)
2023/09/15 14:11:13 - INFO - root -   Epoch: [225/500][80/346], lr: 0.00000422 	 loss = 0.1482(0.0841)
2023/09/15 14:11:29 - INFO - root -   Epoch: [225/500][100/346], lr: 0.00000422 	 loss = 0.0322(0.0765)
2023/09/15 14:11:49 - INFO - root -   Epoch: [225/500][120/346], lr: 0.00000422 	 loss = 0.0342(0.0669)
2023/09/15 14:12:05 - INFO - root -   Epoch: [225/500][140/346], lr: 0.00000422 	 loss = 0.4319(0.0666)
2023/09/15 14:12:24 - INFO - root -   Epoch: [225/500][160/346], lr: 0.00000422 	 loss = 0.4669(0.0642)
2023/09/15 14:12:40 - INFO - root -   Epoch: [225/500][180/346], lr: 0.00000422 	 loss = 0.0239(0.0603)
2023/09/15 14:13:00 - INFO - root -   Epoch: [225/500][200/346], lr: 0.00000422 	 loss = 0.0001(0.0655)
2023/09/15 14:13:16 - INFO - root -   Epoch: [225/500][220/346], lr: 0.00000422 	 loss = 0.0821(0.0777)
2023/09/15 14:13:36 - INFO - root -   Epoch: [225/500][240/346], lr: 0.00000422 	 loss = 0.0163(0.1251)
2023/09/15 14:13:51 - INFO - root -   Epoch: [225/500][260/346], lr: 0.00000422 	 loss = 0.2906(0.1220)
2023/09/15 14:14:11 - INFO - root -   Epoch: [225/500][280/346], lr: 0.00000422 	 loss = 0.0643(0.1159)
2023/09/15 14:14:27 - INFO - root -   Epoch: [225/500][300/346], lr: 0.00000422 	 loss = 0.1627(0.1099)
2023/09/15 14:14:46 - INFO - root -   Epoch: [225/500][320/346], lr: 0.00000422 	 loss = 0.0275(0.1046)
2023/09/15 14:15:01 - INFO - root -   Epoch: [225/500][340/346], lr: 0.00000422 	 loss = 0.0085(0.1018)
2023/09/15 14:15:03 - INFO - root -   Epoch: [225/500] 	 loss = 0.1005
2023/09/15 14:15:03 - INFO - root -   train_accuracy = 0.9725
2023/09/15 14:15:11 - INFO - root -   Epoch: [226/500][0/346], lr: 0.00000423 	 loss = 0.0425(0.0425)
2023/09/15 14:15:27 - INFO - root -   Epoch: [226/500][20/346], lr: 0.00000423 	 loss = 0.0444(0.0322)
2023/09/15 14:15:47 - INFO - root -   Epoch: [226/500][40/346], lr: 0.00000423 	 loss = 0.0294(0.0292)
2023/09/15 14:16:03 - INFO - root -   Epoch: [226/500][60/346], lr: 0.00000423 	 loss = 0.0039(0.0279)
2023/09/15 14:16:23 - INFO - root -   Epoch: [226/500][80/346], lr: 0.00000423 	 loss = 0.1196(0.0264)
2023/09/15 14:16:39 - INFO - root -   Epoch: [226/500][100/346], lr: 0.00000423 	 loss = 0.0015(0.0288)
2023/09/15 14:16:58 - INFO - root -   Epoch: [226/500][120/346], lr: 0.00000423 	 loss = 0.0028(0.0421)
2023/09/15 14:17:14 - INFO - root -   Epoch: [226/500][140/346], lr: 0.00000423 	 loss = 0.0203(0.0437)
2023/09/15 14:17:34 - INFO - root -   Epoch: [226/500][160/346], lr: 0.00000423 	 loss = 0.4390(0.0476)
2023/09/15 14:17:49 - INFO - root -   Epoch: [226/500][180/346], lr: 0.00000423 	 loss = 0.0579(0.0471)
2023/09/15 14:18:09 - INFO - root -   Epoch: [226/500][200/346], lr: 0.00000423 	 loss = 0.0002(0.0490)
2023/09/15 14:18:25 - INFO - root -   Epoch: [226/500][220/346], lr: 0.00000423 	 loss = 0.5529(0.0578)
2023/09/15 14:18:44 - INFO - root -   Epoch: [226/500][240/346], lr: 0.00000423 	 loss = 0.0368(0.0677)
2023/09/15 14:19:00 - INFO - root -   Epoch: [226/500][260/346], lr: 0.00000423 	 loss = 0.1658(0.0698)
2023/09/15 14:19:19 - INFO - root -   Epoch: [226/500][280/346], lr: 0.00000423 	 loss = 0.0005(0.0704)
2023/09/15 14:19:35 - INFO - root -   Epoch: [226/500][300/346], lr: 0.00000423 	 loss = 0.0070(0.0700)
2023/09/15 14:19:54 - INFO - root -   Epoch: [226/500][320/346], lr: 0.00000423 	 loss = 0.0005(0.0697)
2023/09/15 14:20:10 - INFO - root -   Epoch: [226/500][340/346], lr: 0.00000423 	 loss = 0.0561(0.0671)
2023/09/15 14:20:12 - INFO - root -   Epoch: [226/500] 	 loss = 0.0665
2023/09/15 14:20:12 - INFO - root -   train_accuracy = 0.9769
2023/09/15 14:20:20 - INFO - root -   Epoch: [227/500][0/346], lr: 0.00000425 	 loss = 0.0310(0.0310)
2023/09/15 14:20:36 - INFO - root -   Epoch: [227/500][20/346], lr: 0.00000425 	 loss = 0.0132(0.0129)
2023/09/15 14:20:56 - INFO - root -   Epoch: [227/500][40/346], lr: 0.00000425 	 loss = 0.3494(0.0741)
2023/09/15 14:21:11 - INFO - root -   Epoch: [227/500][60/346], lr: 0.00000425 	 loss = 0.0005(0.0742)
2023/09/15 14:21:31 - INFO - root -   Epoch: [227/500][80/346], lr: 0.00000425 	 loss = 0.0511(0.0629)
2023/09/15 14:21:47 - INFO - root -   Epoch: [227/500][100/346], lr: 0.00000425 	 loss = 0.0005(0.0675)
2023/09/15 14:22:06 - INFO - root -   Epoch: [227/500][120/346], lr: 0.00000425 	 loss = 0.0018(0.0616)
2023/09/15 14:22:22 - INFO - root -   Epoch: [227/500][140/346], lr: 0.00000425 	 loss = 0.0122(0.0630)
2023/09/15 14:22:41 - INFO - root -   Epoch: [227/500][160/346], lr: 0.00000425 	 loss = 1.0956(0.0645)
2023/09/15 14:22:58 - INFO - root -   Epoch: [227/500][180/346], lr: 0.00000425 	 loss = 0.0014(0.0767)
2023/09/15 14:23:16 - INFO - root -   Epoch: [227/500][200/346], lr: 0.00000425 	 loss = 0.0001(0.0759)
2023/09/15 14:23:33 - INFO - root -   Epoch: [227/500][220/346], lr: 0.00000425 	 loss = 2.5581(0.1109)
2023/09/15 14:23:51 - INFO - root -   Epoch: [227/500][240/346], lr: 0.00000425 	 loss = 0.0007(0.1435)
2023/09/15 14:24:08 - INFO - root -   Epoch: [227/500][260/346], lr: 0.00000425 	 loss = 2.2543(0.1560)
2023/09/15 14:24:26 - INFO - root -   Epoch: [227/500][280/346], lr: 0.00000425 	 loss = 0.0006(0.1556)
2023/09/15 14:24:43 - INFO - root -   Epoch: [227/500][300/346], lr: 0.00000425 	 loss = 0.0563(0.1635)
2023/09/15 14:25:01 - INFO - root -   Epoch: [227/500][320/346], lr: 0.00000425 	 loss = 0.0003(0.1599)
2023/09/15 14:25:17 - INFO - root -   Epoch: [227/500][340/346], lr: 0.00000425 	 loss = 0.3242(0.1549)
2023/09/15 14:25:19 - INFO - root -   Epoch: [227/500] 	 loss = 0.1538
2023/09/15 14:25:19 - INFO - root -   train_accuracy = 0.9494
2023/09/15 14:25:27 - INFO - root -   Epoch: [228/500][0/346], lr: 0.00000426 	 loss = 0.0051(0.0051)
2023/09/15 14:25:43 - INFO - root -   Epoch: [228/500][20/346], lr: 0.00000426 	 loss = 0.0095(0.0666)
2023/09/15 14:26:02 - INFO - root -   Epoch: [228/500][40/346], lr: 0.00000426 	 loss = 0.8113(0.1110)
2023/09/15 14:26:18 - INFO - root -   Epoch: [228/500][60/346], lr: 0.00000426 	 loss = 0.0056(0.1127)
2023/09/15 14:26:37 - INFO - root -   Epoch: [228/500][80/346], lr: 0.00000426 	 loss = 0.0060(0.0948)
2023/09/15 14:26:54 - INFO - root -   Epoch: [228/500][100/346], lr: 0.00000426 	 loss = 0.0007(0.0844)
2023/09/15 14:27:12 - INFO - root -   Epoch: [228/500][120/346], lr: 0.00000426 	 loss = 0.0003(0.0765)
2023/09/15 14:27:29 - INFO - root -   Epoch: [228/500][140/346], lr: 0.00000426 	 loss = 0.0195(0.0734)
2023/09/15 14:27:46 - INFO - root -   Epoch: [228/500][160/346], lr: 0.00000426 	 loss = 0.0418(0.0692)
2023/09/15 14:28:04 - INFO - root -   Epoch: [228/500][180/346], lr: 0.00000426 	 loss = 0.5760(0.0681)
2023/09/15 14:28:21 - INFO - root -   Epoch: [228/500][200/346], lr: 0.00000426 	 loss = 0.0000(0.0639)
2023/09/15 14:28:40 - INFO - root -   Epoch: [228/500][220/346], lr: 0.00000426 	 loss = 0.4810(0.0729)
2023/09/15 14:28:56 - INFO - root -   Epoch: [228/500][240/346], lr: 0.00000426 	 loss = 0.0035(0.0700)
2023/09/15 14:29:15 - INFO - root -   Epoch: [228/500][260/346], lr: 0.00000426 	 loss = 0.0242(0.0793)
2023/09/15 14:29:31 - INFO - root -   Epoch: [228/500][280/346], lr: 0.00000426 	 loss = 0.0092(0.0922)
2023/09/15 14:29:50 - INFO - root -   Epoch: [228/500][300/346], lr: 0.00000426 	 loss = 0.0172(0.0995)
2023/09/15 14:30:06 - INFO - root -   Epoch: [228/500][320/346], lr: 0.00000426 	 loss = 0.1711(0.0961)
2023/09/15 14:30:25 - INFO - root -   Epoch: [228/500][340/346], lr: 0.00000426 	 loss = 0.0099(0.0924)
2023/09/15 14:30:27 - INFO - root -   Epoch: [228/500] 	 loss = 0.0914
2023/09/15 14:30:27 - INFO - root -   train_accuracy = 0.9668
2023/09/15 14:30:35 - INFO - root -   Epoch: [229/500][0/346], lr: 0.00000428 	 loss = 0.0157(0.0157)
2023/09/15 14:30:51 - INFO - root -   Epoch: [229/500][20/346], lr: 0.00000428 	 loss = 0.0073(0.0624)
2023/09/15 14:31:11 - INFO - root -   Epoch: [229/500][40/346], lr: 0.00000428 	 loss = 0.0003(0.0627)
2023/09/15 14:31:27 - INFO - root -   Epoch: [229/500][60/346], lr: 0.00000428 	 loss = 0.0379(0.0721)
2023/09/15 14:31:46 - INFO - root -   Epoch: [229/500][80/346], lr: 0.00000428 	 loss = 0.0034(0.0797)
2023/09/15 14:32:02 - INFO - root -   Epoch: [229/500][100/346], lr: 0.00000428 	 loss = 0.0019(0.0793)
2023/09/15 14:32:22 - INFO - root -   Epoch: [229/500][120/346], lr: 0.00000428 	 loss = 0.0017(0.0726)
2023/09/15 14:32:38 - INFO - root -   Epoch: [229/500][140/346], lr: 0.00000428 	 loss = 0.1791(0.0717)
2023/09/15 14:32:57 - INFO - root -   Epoch: [229/500][160/346], lr: 0.00000428 	 loss = 0.3205(0.0668)
2023/09/15 14:33:13 - INFO - root -   Epoch: [229/500][180/346], lr: 0.00000428 	 loss = 0.0328(0.0780)
2023/09/15 14:33:33 - INFO - root -   Epoch: [229/500][200/346], lr: 0.00000428 	 loss = 0.0001(0.0791)
2023/09/15 14:33:48 - INFO - root -   Epoch: [229/500][220/346], lr: 0.00000428 	 loss = 0.1424(0.0927)
2023/09/15 14:34:08 - INFO - root -   Epoch: [229/500][240/346], lr: 0.00000428 	 loss = 0.0135(0.1019)
2023/09/15 14:34:24 - INFO - root -   Epoch: [229/500][260/346], lr: 0.00000428 	 loss = 0.0018(0.1100)
2023/09/15 14:34:43 - INFO - root -   Epoch: [229/500][280/346], lr: 0.00000428 	 loss = 0.1762(0.1083)
2023/09/15 14:34:59 - INFO - root -   Epoch: [229/500][300/346], lr: 0.00000428 	 loss = 0.0273(0.1052)
2023/09/15 14:35:19 - INFO - root -   Epoch: [229/500][320/346], lr: 0.00000428 	 loss = 0.0002(0.1010)
2023/09/15 14:35:33 - INFO - root -   Epoch: [229/500][340/346], lr: 0.00000428 	 loss = 0.0169(0.0958)
2023/09/15 14:35:35 - INFO - root -   Epoch: [229/500] 	 loss = 0.0946
2023/09/15 14:36:54 - INFO - root -   precision = 0.5057
2023/09/15 14:36:54 - INFO - root -   eval_loss = 3.2851
2023/09/15 14:36:55 - INFO - root -   train_accuracy = 0.9783
2023/09/15 14:37:03 - INFO - root -   Epoch: [230/500][0/346], lr: 0.00000429 	 loss = 0.0031(0.0031)
2023/09/15 14:37:20 - INFO - root -   Epoch: [230/500][20/346], lr: 0.00000429 	 loss = 0.0091(0.0247)
2023/09/15 14:37:39 - INFO - root -   Epoch: [230/500][40/346], lr: 0.00000429 	 loss = 0.2117(0.0282)
2023/09/15 14:37:56 - INFO - root -   Epoch: [230/500][60/346], lr: 0.00000429 	 loss = 0.0018(0.0246)
2023/09/15 14:38:15 - INFO - root -   Epoch: [230/500][80/346], lr: 0.00000429 	 loss = 0.0335(0.0244)
2023/09/15 14:38:32 - INFO - root -   Epoch: [230/500][100/346], lr: 0.00000429 	 loss = 0.0042(0.0270)
2023/09/15 14:38:50 - INFO - root -   Epoch: [230/500][120/346], lr: 0.00000429 	 loss = 0.0008(0.0280)
2023/09/15 14:39:08 - INFO - root -   Epoch: [230/500][140/346], lr: 0.00000429 	 loss = 0.0032(0.0395)
2023/09/15 14:39:26 - INFO - root -   Epoch: [230/500][160/346], lr: 0.00000429 	 loss = 1.8824(0.0510)
2023/09/15 14:39:44 - INFO - root -   Epoch: [230/500][180/346], lr: 0.00000429 	 loss = 0.0021(0.0583)
2023/09/15 14:40:02 - INFO - root -   Epoch: [230/500][200/346], lr: 0.00000429 	 loss = 0.0002(0.0611)
2023/09/15 14:40:20 - INFO - root -   Epoch: [230/500][220/346], lr: 0.00000429 	 loss = 0.0023(0.0635)
2023/09/15 14:40:39 - INFO - root -   Epoch: [230/500][240/346], lr: 0.00000429 	 loss = 0.0082(0.0798)
2023/09/15 14:40:56 - INFO - root -   Epoch: [230/500][260/346], lr: 0.00000429 	 loss = 0.0010(0.0821)
2023/09/15 14:41:15 - INFO - root -   Epoch: [230/500][280/346], lr: 0.00000429 	 loss = 0.0073(0.0768)
2023/09/15 14:41:32 - INFO - root -   Epoch: [230/500][300/346], lr: 0.00000429 	 loss = 0.0562(0.0950)
2023/09/15 14:41:51 - INFO - root -   Epoch: [230/500][320/346], lr: 0.00000429 	 loss = 0.0003(0.0963)
2023/09/15 14:42:08 - INFO - root -   Epoch: [230/500][340/346], lr: 0.00000429 	 loss = 0.0557(0.0936)
2023/09/15 14:42:10 - INFO - root -   Epoch: [230/500] 	 loss = 0.0933
2023/09/15 14:42:10 - INFO - root -   train_accuracy = 0.9682
2023/09/15 14:42:18 - INFO - root -   Epoch: [231/500][0/346], lr: 0.00000430 	 loss = 0.0053(0.0053)
2023/09/15 14:42:35 - INFO - root -   Epoch: [231/500][20/346], lr: 0.00000430 	 loss = 0.0086(0.0432)
2023/09/15 14:42:56 - INFO - root -   Epoch: [231/500][40/346], lr: 0.00000430 	 loss = 0.0008(0.1407)
2023/09/15 14:43:12 - INFO - root -   Epoch: [231/500][60/346], lr: 0.00000430 	 loss = 0.0036(0.1032)
2023/09/15 14:43:33 - INFO - root -   Epoch: [231/500][80/346], lr: 0.00000430 	 loss = 0.1609(0.0823)
2023/09/15 14:43:50 - INFO - root -   Epoch: [231/500][100/346], lr: 0.00000430 	 loss = 0.0008(0.0701)
2023/09/15 14:44:10 - INFO - root -   Epoch: [231/500][120/346], lr: 0.00000430 	 loss = 0.0001(0.0761)
2023/09/15 14:44:27 - INFO - root -   Epoch: [231/500][140/346], lr: 0.00000430 	 loss = 0.0212(0.0862)
2023/09/15 14:44:48 - INFO - root -   Epoch: [231/500][160/346], lr: 0.00000430 	 loss = 0.2334(0.0805)
2023/09/15 14:45:05 - INFO - root -   Epoch: [231/500][180/346], lr: 0.00000430 	 loss = 0.0068(0.0738)
2023/09/15 14:45:26 - INFO - root -   Epoch: [231/500][200/346], lr: 0.00000430 	 loss = 0.0001(0.0743)
2023/09/15 14:45:43 - INFO - root -   Epoch: [231/500][220/346], lr: 0.00000430 	 loss = 1.6840(0.0885)
2023/09/15 14:46:04 - INFO - root -   Epoch: [231/500][240/346], lr: 0.00000430 	 loss = 0.0084(0.0831)
2023/09/15 14:46:21 - INFO - root -   Epoch: [231/500][260/346], lr: 0.00000430 	 loss = 0.0039(0.0780)
2023/09/15 14:46:42 - INFO - root -   Epoch: [231/500][280/346], lr: 0.00000430 	 loss = 0.0029(0.0740)
2023/09/15 14:46:58 - INFO - root -   Epoch: [231/500][300/346], lr: 0.00000430 	 loss = 0.0020(0.0694)
2023/09/15 14:47:19 - INFO - root -   Epoch: [231/500][320/346], lr: 0.00000430 	 loss = 0.0007(0.0668)
2023/09/15 14:47:36 - INFO - root -   Epoch: [231/500][340/346], lr: 0.00000430 	 loss = 0.2421(0.0677)
2023/09/15 14:47:38 - INFO - root -   Epoch: [231/500] 	 loss = 0.0668
2023/09/15 14:47:38 - INFO - root -   train_accuracy = 0.9812
2023/09/15 14:47:46 - INFO - root -   Epoch: [232/500][0/346], lr: 0.00000432 	 loss = 0.2610(0.2610)
2023/09/15 14:48:03 - INFO - root -   Epoch: [232/500][20/346], lr: 0.00000432 	 loss = 0.0031(0.1012)
2023/09/15 14:48:24 - INFO - root -   Epoch: [232/500][40/346], lr: 0.00000432 	 loss = 0.0016(0.2280)
2023/09/15 14:48:40 - INFO - root -   Epoch: [232/500][60/346], lr: 0.00000432 	 loss = 0.0131(0.2802)
2023/09/15 14:49:01 - INFO - root -   Epoch: [232/500][80/346], lr: 0.00000432 	 loss = 0.0441(0.3105)
2023/09/15 14:49:18 - INFO - root -   Epoch: [232/500][100/346], lr: 0.00000432 	 loss = 0.0149(0.2943)
2023/09/15 14:49:39 - INFO - root -   Epoch: [232/500][120/346], lr: 0.00000432 	 loss = 0.0047(0.2655)
2023/09/15 14:49:56 - INFO - root -   Epoch: [232/500][140/346], lr: 0.00000432 	 loss = 0.1872(0.2364)
2023/09/15 14:50:17 - INFO - root -   Epoch: [232/500][160/346], lr: 0.00000432 	 loss = 0.0163(0.2299)
2023/09/15 14:50:34 - INFO - root -   Epoch: [232/500][180/346], lr: 0.00000432 	 loss = 0.0064(0.2110)
2023/09/15 14:50:55 - INFO - root -   Epoch: [232/500][200/346], lr: 0.00000432 	 loss = 0.0000(0.1912)
2023/09/15 14:51:11 - INFO - root -   Epoch: [232/500][220/346], lr: 0.00000432 	 loss = 0.0062(0.1838)
2023/09/15 14:51:32 - INFO - root -   Epoch: [232/500][240/346], lr: 0.00000432 	 loss = 0.0006(0.1732)
2023/09/15 14:51:49 - INFO - root -   Epoch: [232/500][260/346], lr: 0.00000432 	 loss = 0.0029(0.1614)
2023/09/15 14:52:10 - INFO - root -   Epoch: [232/500][280/346], lr: 0.00000432 	 loss = 0.0063(0.1539)
2023/09/15 14:52:27 - INFO - root -   Epoch: [232/500][300/346], lr: 0.00000432 	 loss = 0.0061(0.1444)
2023/09/15 14:52:48 - INFO - root -   Epoch: [232/500][320/346], lr: 0.00000432 	 loss = 0.0002(0.1372)
2023/09/15 14:53:04 - INFO - root -   Epoch: [232/500][340/346], lr: 0.00000432 	 loss = 0.0051(0.1300)
2023/09/15 14:53:06 - INFO - root -   Epoch: [232/500] 	 loss = 0.1285
2023/09/15 14:53:06 - INFO - root -   train_accuracy = 0.9639
2023/09/15 14:53:15 - INFO - root -   Epoch: [233/500][0/346], lr: 0.00000433 	 loss = 0.0003(0.0003)
2023/09/15 14:53:31 - INFO - root -   Epoch: [233/500][20/346], lr: 0.00000433 	 loss = 0.0042(0.0499)
2023/09/15 14:53:52 - INFO - root -   Epoch: [233/500][40/346], lr: 0.00000433 	 loss = 0.0075(0.1050)
2023/09/15 14:54:09 - INFO - root -   Epoch: [233/500][60/346], lr: 0.00000433 	 loss = 0.0003(0.0869)
2023/09/15 14:54:30 - INFO - root -   Epoch: [233/500][80/346], lr: 0.00000433 	 loss = 1.1593(0.0852)
2023/09/15 14:54:46 - INFO - root -   Epoch: [233/500][100/346], lr: 0.00000433 	 loss = 0.0017(0.0947)
2023/09/15 14:55:07 - INFO - root -   Epoch: [233/500][120/346], lr: 0.00000433 	 loss = 0.0047(0.0815)
2023/09/15 14:55:24 - INFO - root -   Epoch: [233/500][140/346], lr: 0.00000433 	 loss = 0.0910(0.0730)
2023/09/15 14:55:44 - INFO - root -   Epoch: [233/500][160/346], lr: 0.00000433 	 loss = 0.1369(0.0663)
2023/09/15 14:56:01 - INFO - root -   Epoch: [233/500][180/346], lr: 0.00000433 	 loss = 0.0092(0.0654)
2023/09/15 14:56:22 - INFO - root -   Epoch: [233/500][200/346], lr: 0.00000433 	 loss = 0.0003(0.0716)
2023/09/15 14:56:38 - INFO - root -   Epoch: [233/500][220/346], lr: 0.00000433 	 loss = 0.0005(0.0739)
2023/09/15 14:56:59 - INFO - root -   Epoch: [233/500][240/346], lr: 0.00000433 	 loss = 0.0006(0.0715)
2023/09/15 14:57:16 - INFO - root -   Epoch: [233/500][260/346], lr: 0.00000433 	 loss = 0.2849(0.0757)
2023/09/15 14:57:37 - INFO - root -   Epoch: [233/500][280/346], lr: 0.00000433 	 loss = 0.0011(0.0713)
2023/09/15 14:57:54 - INFO - root -   Epoch: [233/500][300/346], lr: 0.00000433 	 loss = 0.0384(0.0669)
2023/09/15 14:58:15 - INFO - root -   Epoch: [233/500][320/346], lr: 0.00000433 	 loss = 0.0001(0.0773)
2023/09/15 14:58:31 - INFO - root -   Epoch: [233/500][340/346], lr: 0.00000433 	 loss = 0.0132(0.0800)
2023/09/15 14:58:33 - INFO - root -   Epoch: [233/500] 	 loss = 0.0792
2023/09/15 14:58:33 - INFO - root -   train_accuracy = 0.9754
2023/09/15 14:58:41 - INFO - root -   Epoch: [234/500][0/346], lr: 0.00000435 	 loss = 0.2469(0.2469)
2023/09/15 14:58:58 - INFO - root -   Epoch: [234/500][20/346], lr: 0.00000435 	 loss = 0.0089(0.4999)
2023/09/15 14:59:19 - INFO - root -   Epoch: [234/500][40/346], lr: 0.00000435 	 loss = 0.0224(0.4456)
2023/09/15 14:59:36 - INFO - root -   Epoch: [234/500][60/346], lr: 0.00000435 	 loss = 0.0015(0.4305)
2023/09/15 14:59:57 - INFO - root -   Epoch: [234/500][80/346], lr: 0.00000435 	 loss = 0.0273(0.3473)
2023/09/15 15:00:14 - INFO - root -   Epoch: [234/500][100/346], lr: 0.00000435 	 loss = 0.0005(0.2937)
2023/09/15 15:00:35 - INFO - root -   Epoch: [234/500][120/346], lr: 0.00000435 	 loss = 0.0027(0.2531)
2023/09/15 15:00:52 - INFO - root -   Epoch: [234/500][140/346], lr: 0.00000435 	 loss = 0.1117(0.2219)
2023/09/15 15:01:13 - INFO - root -   Epoch: [234/500][160/346], lr: 0.00000435 	 loss = 0.0221(0.2057)
2023/09/15 15:01:30 - INFO - root -   Epoch: [234/500][180/346], lr: 0.00000435 	 loss = 0.0003(0.1850)
2023/09/15 15:01:51 - INFO - root -   Epoch: [234/500][200/346], lr: 0.00000435 	 loss = 0.0001(0.1696)
2023/09/15 15:02:07 - INFO - root -   Epoch: [234/500][220/346], lr: 0.00000435 	 loss = 0.0130(0.1568)
2023/09/15 15:02:28 - INFO - root -   Epoch: [234/500][240/346], lr: 0.00000435 	 loss = 0.0007(0.1464)
2023/09/15 15:02:45 - INFO - root -   Epoch: [234/500][260/346], lr: 0.00000435 	 loss = 0.0032(0.1359)
2023/09/15 15:03:06 - INFO - root -   Epoch: [234/500][280/346], lr: 0.00000435 	 loss = 0.0023(0.1273)
2023/09/15 15:03:23 - INFO - root -   Epoch: [234/500][300/346], lr: 0.00000435 	 loss = 0.0202(0.1238)
2023/09/15 15:03:44 - INFO - root -   Epoch: [234/500][320/346], lr: 0.00000435 	 loss = 0.0000(0.1247)
2023/09/15 15:04:01 - INFO - root -   Epoch: [234/500][340/346], lr: 0.00000435 	 loss = 0.7352(0.1353)
2023/09/15 15:04:03 - INFO - root -   Epoch: [234/500] 	 loss = 0.1338
2023/09/15 15:05:26 - INFO - root -   precision = 0.4770
2023/09/15 15:05:26 - INFO - root -   eval_loss = 3.3052
2023/09/15 15:05:27 - INFO - root -   train_accuracy = 0.9639
2023/09/15 15:05:35 - INFO - root -   Epoch: [235/500][0/346], lr: 0.00000436 	 loss = 0.4329(0.4329)
2023/09/15 15:05:52 - INFO - root -   Epoch: [235/500][20/346], lr: 0.00000436 	 loss = 0.0839(0.0729)
2023/09/15 15:06:13 - INFO - root -   Epoch: [235/500][40/346], lr: 0.00000436 	 loss = 0.0018(0.2009)
2023/09/15 15:06:29 - INFO - root -   Epoch: [235/500][60/346], lr: 0.00000436 	 loss = 1.0399(0.3186)
2023/09/15 15:06:50 - INFO - root -   Epoch: [235/500][80/346], lr: 0.00000436 	 loss = 0.0023(0.2774)
2023/09/15 15:07:06 - INFO - root -   Epoch: [235/500][100/346], lr: 0.00000436 	 loss = 0.0012(0.2456)
2023/09/15 15:07:27 - INFO - root -   Epoch: [235/500][120/346], lr: 0.00000436 	 loss = 0.0046(0.2107)
2023/09/15 15:07:44 - INFO - root -   Epoch: [235/500][140/346], lr: 0.00000436 	 loss = 0.6444(0.1930)
2023/09/15 15:08:05 - INFO - root -   Epoch: [235/500][160/346], lr: 0.00000436 	 loss = 0.7108(0.1970)
2023/09/15 15:08:21 - INFO - root -   Epoch: [235/500][180/346], lr: 0.00000436 	 loss = 0.0148(0.1784)
2023/09/15 15:08:42 - INFO - root -   Epoch: [235/500][200/346], lr: 0.00000436 	 loss = 0.0002(0.1632)
2023/09/15 15:08:58 - INFO - root -   Epoch: [235/500][220/346], lr: 0.00000436 	 loss = 0.0653(0.1517)
2023/09/15 15:09:19 - INFO - root -   Epoch: [235/500][240/346], lr: 0.00000436 	 loss = 0.0041(0.1419)
2023/09/15 15:09:36 - INFO - root -   Epoch: [235/500][260/346], lr: 0.00000436 	 loss = 0.0117(0.1322)
2023/09/15 15:09:57 - INFO - root -   Epoch: [235/500][280/346], lr: 0.00000436 	 loss = 0.0176(0.1258)
2023/09/15 15:10:13 - INFO - root -   Epoch: [235/500][300/346], lr: 0.00000436 	 loss = 0.0096(0.1182)
2023/09/15 15:10:34 - INFO - root -   Epoch: [235/500][320/346], lr: 0.00000436 	 loss = 0.0002(0.1144)
2023/09/15 15:10:50 - INFO - root -   Epoch: [235/500][340/346], lr: 0.00000436 	 loss = 0.0681(0.1119)
2023/09/15 15:10:52 - INFO - root -   Epoch: [235/500] 	 loss = 0.1104
2023/09/15 15:10:52 - INFO - root -   train_accuracy = 0.9653
2023/09/15 15:11:00 - INFO - root -   Epoch: [236/500][0/346], lr: 0.00000438 	 loss = 0.0006(0.0006)
2023/09/15 15:11:17 - INFO - root -   Epoch: [236/500][20/346], lr: 0.00000438 	 loss = 0.0095(0.0146)
2023/09/15 15:11:38 - INFO - root -   Epoch: [236/500][40/346], lr: 0.00000438 	 loss = 0.0015(0.0120)
2023/09/15 15:11:55 - INFO - root -   Epoch: [236/500][60/346], lr: 0.00000438 	 loss = 0.0028(0.0105)
2023/09/15 15:12:15 - INFO - root -   Epoch: [236/500][80/346], lr: 0.00000438 	 loss = 0.0013(0.0303)
2023/09/15 15:12:32 - INFO - root -   Epoch: [236/500][100/346], lr: 0.00000438 	 loss = 0.0037(0.0404)
2023/09/15 15:12:53 - INFO - root -   Epoch: [236/500][120/346], lr: 0.00000438 	 loss = 0.0014(0.0351)
2023/09/15 15:13:10 - INFO - root -   Epoch: [236/500][140/346], lr: 0.00000438 	 loss = 0.0001(0.0372)
2023/09/15 15:13:31 - INFO - root -   Epoch: [236/500][160/346], lr: 0.00000438 	 loss = 0.0019(0.0404)
2023/09/15 15:13:48 - INFO - root -   Epoch: [236/500][180/346], lr: 0.00000438 	 loss = 0.0050(0.0486)
2023/09/15 15:14:09 - INFO - root -   Epoch: [236/500][200/346], lr: 0.00000438 	 loss = 0.0000(0.0619)
2023/09/15 15:14:25 - INFO - root -   Epoch: [236/500][220/346], lr: 0.00000438 	 loss = 0.0045(0.0633)
2023/09/15 15:14:47 - INFO - root -   Epoch: [236/500][240/346], lr: 0.00000438 	 loss = 0.0041(0.0651)
2023/09/15 15:15:03 - INFO - root -   Epoch: [236/500][260/346], lr: 0.00000438 	 loss = 0.0830(0.0627)
2023/09/15 15:15:24 - INFO - root -   Epoch: [236/500][280/346], lr: 0.00000438 	 loss = 0.0027(0.0591)
2023/09/15 15:15:41 - INFO - root -   Epoch: [236/500][300/346], lr: 0.00000438 	 loss = 0.0438(0.0559)
2023/09/15 15:16:02 - INFO - root -   Epoch: [236/500][320/346], lr: 0.00000438 	 loss = 0.0005(0.0532)
2023/09/15 15:16:18 - INFO - root -   Epoch: [236/500][340/346], lr: 0.00000438 	 loss = 0.0227(0.0511)
2023/09/15 15:16:20 - INFO - root -   Epoch: [236/500] 	 loss = 0.0504
2023/09/15 15:16:20 - INFO - root -   train_accuracy = 0.9841
2023/09/15 15:16:29 - INFO - root -   Epoch: [237/500][0/346], lr: 0.00000439 	 loss = 0.0005(0.0005)
2023/09/15 15:16:45 - INFO - root -   Epoch: [237/500][20/346], lr: 0.00000439 	 loss = 0.0120(0.0191)
2023/09/15 15:17:06 - INFO - root -   Epoch: [237/500][40/346], lr: 0.00000439 	 loss = 0.0007(0.0130)
2023/09/15 15:17:22 - INFO - root -   Epoch: [237/500][60/346], lr: 0.00000439 	 loss = 0.0002(0.0129)
2023/09/15 15:17:43 - INFO - root -   Epoch: [237/500][80/346], lr: 0.00000439 	 loss = 0.0014(0.0137)
2023/09/15 15:18:00 - INFO - root -   Epoch: [237/500][100/346], lr: 0.00000439 	 loss = 0.0057(0.0201)
2023/09/15 15:18:20 - INFO - root -   Epoch: [237/500][120/346], lr: 0.00000439 	 loss = 0.0009(0.0215)
2023/09/15 15:18:37 - INFO - root -   Epoch: [237/500][140/346], lr: 0.00000439 	 loss = 0.0004(0.0211)
2023/09/15 15:18:58 - INFO - root -   Epoch: [237/500][160/346], lr: 0.00000439 	 loss = 0.1269(0.0209)
2023/09/15 15:19:14 - INFO - root -   Epoch: [237/500][180/346], lr: 0.00000439 	 loss = 0.0066(0.0197)
2023/09/15 15:19:35 - INFO - root -   Epoch: [237/500][200/346], lr: 0.00000439 	 loss = 0.0000(0.0187)
2023/09/15 15:19:52 - INFO - root -   Epoch: [237/500][220/346], lr: 0.00000439 	 loss = 0.0119(0.0276)
2023/09/15 15:20:13 - INFO - root -   Epoch: [237/500][240/346], lr: 0.00000439 	 loss = 0.0003(0.0325)
2023/09/15 15:20:29 - INFO - root -   Epoch: [237/500][260/346], lr: 0.00000439 	 loss = 0.0014(0.0302)
2023/09/15 15:20:50 - INFO - root -   Epoch: [237/500][280/346], lr: 0.00000439 	 loss = 0.0004(0.0307)
2023/09/15 15:21:06 - INFO - root -   Epoch: [237/500][300/346], lr: 0.00000439 	 loss = 0.0007(0.0317)
2023/09/15 15:21:27 - INFO - root -   Epoch: [237/500][320/346], lr: 0.00000439 	 loss = 0.0004(0.0417)
2023/09/15 15:21:44 - INFO - root -   Epoch: [237/500][340/346], lr: 0.00000439 	 loss = 1.6542(0.0528)
2023/09/15 15:21:46 - INFO - root -   Epoch: [237/500] 	 loss = 0.0528
2023/09/15 15:21:46 - INFO - root -   train_accuracy = 0.9870
2023/09/15 15:21:54 - INFO - root -   Epoch: [238/500][0/346], lr: 0.00000440 	 loss = 0.0265(0.0265)
2023/09/15 15:22:11 - INFO - root -   Epoch: [238/500][20/346], lr: 0.00000440 	 loss = 0.0598(0.0281)
2023/09/15 15:22:31 - INFO - root -   Epoch: [238/500][40/346], lr: 0.00000440 	 loss = 0.0014(0.1025)
2023/09/15 15:22:48 - INFO - root -   Epoch: [238/500][60/346], lr: 0.00000440 	 loss = 0.0017(0.2951)
2023/09/15 15:23:09 - INFO - root -   Epoch: [238/500][80/346], lr: 0.00000440 	 loss = 0.0123(0.2607)
2023/09/15 15:23:25 - INFO - root -   Epoch: [238/500][100/346], lr: 0.00000440 	 loss = 0.0026(0.2391)
2023/09/15 15:23:46 - INFO - root -   Epoch: [238/500][120/346], lr: 0.00000440 	 loss = 0.0040(0.2140)
2023/09/15 15:24:02 - INFO - root -   Epoch: [238/500][140/346], lr: 0.00000440 	 loss = 0.0078(0.1894)
2023/09/15 15:24:23 - INFO - root -   Epoch: [238/500][160/346], lr: 0.00000440 	 loss = 0.0717(0.1690)
2023/09/15 15:24:40 - INFO - root -   Epoch: [238/500][180/346], lr: 0.00000440 	 loss = 0.0026(0.1536)
2023/09/15 15:25:00 - INFO - root -   Epoch: [238/500][200/346], lr: 0.00000440 	 loss = 0.0001(0.1501)
2023/09/15 15:25:17 - INFO - root -   Epoch: [238/500][220/346], lr: 0.00000440 	 loss = 0.0014(0.1444)
2023/09/15 15:25:38 - INFO - root -   Epoch: [238/500][240/346], lr: 0.00000440 	 loss = 0.0600(0.1383)
2023/09/15 15:25:56 - INFO - root -   Epoch: [238/500][260/346], lr: 0.00000440 	 loss = 0.1107(0.1378)
2023/09/15 15:26:17 - INFO - root -   Epoch: [238/500][280/346], lr: 0.00000440 	 loss = 0.4969(0.1899)
2023/09/15 15:26:33 - INFO - root -   Epoch: [238/500][300/346], lr: 0.00000440 	 loss = 0.0125(0.1915)
2023/09/15 15:26:54 - INFO - root -   Epoch: [238/500][320/346], lr: 0.00000440 	 loss = 0.0004(0.1832)
2023/09/15 15:27:09 - INFO - root -   Epoch: [238/500][340/346], lr: 0.00000440 	 loss = 0.0458(0.1752)
2023/09/15 15:27:11 - INFO - root -   Epoch: [238/500] 	 loss = 0.1729
2023/09/15 15:27:11 - INFO - root -   train_accuracy = 0.9538
2023/09/15 15:27:20 - INFO - root -   Epoch: [239/500][0/346], lr: 0.00000442 	 loss = 0.0206(0.0206)
2023/09/15 15:27:37 - INFO - root -   Epoch: [239/500][20/346], lr: 0.00000442 	 loss = 0.0220(0.1613)
2023/09/15 15:27:57 - INFO - root -   Epoch: [239/500][40/346], lr: 0.00000442 	 loss = 0.0292(0.1305)
2023/09/15 15:28:14 - INFO - root -   Epoch: [239/500][60/346], lr: 0.00000442 	 loss = 0.1664(0.2004)
2023/09/15 15:28:35 - INFO - root -   Epoch: [239/500][80/346], lr: 0.00000442 	 loss = 0.0494(0.1725)
2023/09/15 15:28:51 - INFO - root -   Epoch: [239/500][100/346], lr: 0.00000442 	 loss = 0.0637(0.1476)
2023/09/15 15:29:12 - INFO - root -   Epoch: [239/500][120/346], lr: 0.00000442 	 loss = 0.0026(0.1289)
2023/09/15 15:29:29 - INFO - root -   Epoch: [239/500][140/346], lr: 0.00000442 	 loss = 0.0227(0.1139)
2023/09/15 15:29:50 - INFO - root -   Epoch: [239/500][160/346], lr: 0.00000442 	 loss = 0.0492(0.1044)
2023/09/15 15:30:07 - INFO - root -   Epoch: [239/500][180/346], lr: 0.00000442 	 loss = 0.0031(0.0975)
2023/09/15 15:30:28 - INFO - root -   Epoch: [239/500][200/346], lr: 0.00000442 	 loss = 0.0000(0.0891)
2023/09/15 15:30:44 - INFO - root -   Epoch: [239/500][220/346], lr: 0.00000442 	 loss = 0.0178(0.0841)
2023/09/15 15:31:05 - INFO - root -   Epoch: [239/500][240/346], lr: 0.00000442 	 loss = 0.0039(0.0788)
2023/09/15 15:31:22 - INFO - root -   Epoch: [239/500][260/346], lr: 0.00000442 	 loss = 0.0018(0.0760)
2023/09/15 15:31:43 - INFO - root -   Epoch: [239/500][280/346], lr: 0.00000442 	 loss = 0.0005(0.0832)
2023/09/15 15:31:59 - INFO - root -   Epoch: [239/500][300/346], lr: 0.00000442 	 loss = 0.0004(0.1138)
2023/09/15 15:32:20 - INFO - root -   Epoch: [239/500][320/346], lr: 0.00000442 	 loss = 0.0006(0.1105)
2023/09/15 15:32:37 - INFO - root -   Epoch: [239/500][340/346], lr: 0.00000442 	 loss = 0.0193(0.1175)
2023/09/15 15:32:39 - INFO - root -   Epoch: [239/500] 	 loss = 0.1161
2023/09/15 15:34:02 - INFO - root -   precision = 0.5287
2023/09/15 15:34:02 - INFO - root -   eval_loss = 2.6705
2023/09/15 15:34:03 - INFO - root -   train_accuracy = 0.9711
2023/09/15 15:34:11 - INFO - root -   Epoch: [240/500][0/346], lr: 0.00000443 	 loss = 0.0244(0.0244)
2023/09/15 15:34:27 - INFO - root -   Epoch: [240/500][20/346], lr: 0.00000443 	 loss = 0.0054(0.0745)
2023/09/15 15:34:48 - INFO - root -   Epoch: [240/500][40/346], lr: 0.00000443 	 loss = 0.0012(0.0819)
2023/09/15 15:35:05 - INFO - root -   Epoch: [240/500][60/346], lr: 0.00000443 	 loss = 0.0006(0.0746)
2023/09/15 15:35:26 - INFO - root -   Epoch: [240/500][80/346], lr: 0.00000443 	 loss = 0.0380(0.0631)
2023/09/15 15:35:42 - INFO - root -   Epoch: [240/500][100/346], lr: 0.00000443 	 loss = 0.0053(0.0570)
2023/09/15 15:36:03 - INFO - root -   Epoch: [240/500][120/346], lr: 0.00000443 	 loss = 0.0100(0.0738)
2023/09/15 15:36:20 - INFO - root -   Epoch: [240/500][140/346], lr: 0.00000443 	 loss = 0.5733(0.0777)
2023/09/15 15:36:40 - INFO - root -   Epoch: [240/500][160/346], lr: 0.00000443 	 loss = 0.0449(0.0875)
2023/09/15 15:36:57 - INFO - root -   Epoch: [240/500][180/346], lr: 0.00000443 	 loss = 0.0025(0.0796)
2023/09/15 15:37:18 - INFO - root -   Epoch: [240/500][200/346], lr: 0.00000443 	 loss = 0.0001(0.0910)
2023/09/15 15:37:35 - INFO - root -   Epoch: [240/500][220/346], lr: 0.00000443 	 loss = 0.0091(0.0862)
2023/09/15 15:37:56 - INFO - root -   Epoch: [240/500][240/346], lr: 0.00000443 	 loss = 0.0238(0.0816)
2023/09/15 15:38:12 - INFO - root -   Epoch: [240/500][260/346], lr: 0.00000443 	 loss = 0.0643(0.0760)
2023/09/15 15:38:33 - INFO - root -   Epoch: [240/500][280/346], lr: 0.00000443 	 loss = 0.0008(0.0715)
2023/09/15 15:38:50 - INFO - root -   Epoch: [240/500][300/346], lr: 0.00000443 	 loss = 0.0450(0.0741)
2023/09/15 15:39:27 - INFO - root -   Epoch: [240/500][320/346], lr: 0.00000443 	 loss = 0.0006(0.0718)
2023/09/15 15:40:03 - INFO - root -   Epoch: [240/500][340/346], lr: 0.00000443 	 loss = 0.0033(0.0681)
2023/09/15 15:40:11 - INFO - root -   Epoch: [240/500] 	 loss = 0.0676
2023/09/15 15:40:11 - INFO - root -   train_accuracy = 0.9855
2023/09/15 15:40:28 - INFO - root -   Epoch: [241/500][0/346], lr: 0.00000445 	 loss = 0.0125(0.0125)
2023/09/15 15:41:00 - INFO - root -   Epoch: [241/500][20/346], lr: 0.00000445 	 loss = 0.0009(0.0148)
2023/09/15 15:41:44 - INFO - root -   Epoch: [241/500][40/346], lr: 0.00000445 	 loss = 0.0005(0.0130)
2023/09/15 15:42:02 - INFO - root -   Epoch: [241/500][60/346], lr: 0.00000445 	 loss = 0.0013(0.0110)
2023/09/15 15:42:23 - INFO - root -   Epoch: [241/500][80/346], lr: 0.00000445 	 loss = 0.0961(0.0162)
2023/09/15 15:42:39 - INFO - root -   Epoch: [241/500][100/346], lr: 0.00000445 	 loss = 0.0026(0.0574)
2023/09/15 15:43:00 - INFO - root -   Epoch: [241/500][120/346], lr: 0.00000445 	 loss = 0.0060(0.0580)
2023/09/15 15:43:17 - INFO - root -   Epoch: [241/500][140/346], lr: 0.00000445 	 loss = 0.0102(0.0511)
2023/09/15 15:43:37 - INFO - root -   Epoch: [241/500][160/346], lr: 0.00000445 	 loss = 0.0144(0.0526)
2023/09/15 15:43:54 - INFO - root -   Epoch: [241/500][180/346], lr: 0.00000445 	 loss = 0.0001(0.0693)
2023/09/15 15:44:15 - INFO - root -   Epoch: [241/500][200/346], lr: 0.00000445 	 loss = 0.0001(0.0673)
2023/09/15 15:44:31 - INFO - root -   Epoch: [241/500][220/346], lr: 0.00000445 	 loss = 0.1785(0.0650)
2023/09/15 15:44:52 - INFO - root -   Epoch: [241/500][240/346], lr: 0.00000445 	 loss = 0.0233(0.0602)
2023/09/15 15:45:09 - INFO - root -   Epoch: [241/500][260/346], lr: 0.00000445 	 loss = 0.0297(0.0575)
2023/09/15 15:45:30 - INFO - root -   Epoch: [241/500][280/346], lr: 0.00000445 	 loss = 0.0493(0.0714)
2023/09/15 15:45:46 - INFO - root -   Epoch: [241/500][300/346], lr: 0.00000445 	 loss = 0.0178(0.0779)
2023/09/15 15:46:07 - INFO - root -   Epoch: [241/500][320/346], lr: 0.00000445 	 loss = 0.0003(0.0762)
2023/09/15 15:46:24 - INFO - root -   Epoch: [241/500][340/346], lr: 0.00000445 	 loss = 0.0074(0.0741)
2023/09/15 15:46:26 - INFO - root -   Epoch: [241/500] 	 loss = 0.0731
2023/09/15 15:46:26 - INFO - root -   train_accuracy = 0.9798
2023/09/15 15:46:34 - INFO - root -   Epoch: [242/500][0/346], lr: 0.00000446 	 loss = 0.0038(0.0038)
2023/09/15 15:46:51 - INFO - root -   Epoch: [242/500][20/346], lr: 0.00000446 	 loss = 0.0025(0.0050)
2023/09/15 15:47:12 - INFO - root -   Epoch: [242/500][40/346], lr: 0.00000446 	 loss = 0.0088(0.0407)
2023/09/15 15:47:28 - INFO - root -   Epoch: [242/500][60/346], lr: 0.00000446 	 loss = 0.0028(0.0337)
2023/09/15 15:47:49 - INFO - root -   Epoch: [242/500][80/346], lr: 0.00000446 	 loss = 0.0002(0.0318)
2023/09/15 15:48:06 - INFO - root -   Epoch: [242/500][100/346], lr: 0.00000446 	 loss = 0.0019(0.0336)
2023/09/15 15:48:27 - INFO - root -   Epoch: [242/500][120/346], lr: 0.00000446 	 loss = 0.0013(0.0309)
2023/09/15 15:48:44 - INFO - root -   Epoch: [242/500][140/346], lr: 0.00000446 	 loss = 0.0073(0.0284)
2023/09/15 15:49:05 - INFO - root -   Epoch: [242/500][160/346], lr: 0.00000446 	 loss = 0.0794(0.0263)
2023/09/15 15:49:21 - INFO - root -   Epoch: [242/500][180/346], lr: 0.00000446 	 loss = 0.0009(0.0256)
2023/09/15 15:49:42 - INFO - root -   Epoch: [242/500][200/346], lr: 0.00000446 	 loss = 0.0000(0.0267)
2023/09/15 15:49:59 - INFO - root -   Epoch: [242/500][220/346], lr: 0.00000446 	 loss = 0.0170(0.0260)
2023/09/15 15:50:20 - INFO - root -   Epoch: [242/500][240/346], lr: 0.00000446 	 loss = 0.0223(0.0324)
2023/09/15 15:50:37 - INFO - root -   Epoch: [242/500][260/346], lr: 0.00000446 	 loss = 0.0069(0.0309)
2023/09/15 15:50:58 - INFO - root -   Epoch: [242/500][280/346], lr: 0.00000446 	 loss = 0.0007(0.0307)
2023/09/15 15:51:14 - INFO - root -   Epoch: [242/500][300/346], lr: 0.00000446 	 loss = 0.0072(0.0290)
2023/09/15 15:51:35 - INFO - root -   Epoch: [242/500][320/346], lr: 0.00000446 	 loss = 0.0003(0.0276)
2023/09/15 15:51:52 - INFO - root -   Epoch: [242/500][340/346], lr: 0.00000446 	 loss = 0.0014(0.0262)
2023/09/15 15:51:54 - INFO - root -   Epoch: [242/500] 	 loss = 0.0258
2023/09/15 15:51:54 - INFO - root -   train_accuracy = 0.9928
2023/09/15 15:52:02 - INFO - root -   Epoch: [243/500][0/346], lr: 0.00000448 	 loss = 0.0268(0.0268)
2023/09/15 15:52:19 - INFO - root -   Epoch: [243/500][20/346], lr: 0.00000448 	 loss = 0.0026(0.0039)
2023/09/15 15:52:40 - INFO - root -   Epoch: [243/500][40/346], lr: 0.00000448 	 loss = 0.0437(0.0401)
2023/09/15 15:52:57 - INFO - root -   Epoch: [243/500][60/346], lr: 0.00000448 	 loss = 0.0009(0.0299)
2023/09/15 15:53:17 - INFO - root -   Epoch: [243/500][80/346], lr: 0.00000448 	 loss = 0.0073(0.0406)
2023/09/15 15:53:34 - INFO - root -   Epoch: [243/500][100/346], lr: 0.00000448 	 loss = 0.0003(0.0621)
2023/09/15 15:53:55 - INFO - root -   Epoch: [243/500][120/346], lr: 0.00000448 	 loss = 0.0041(0.0970)
2023/09/15 15:54:11 - INFO - root -   Epoch: [243/500][140/346], lr: 0.00000448 	 loss = 0.0009(0.0953)
2023/09/15 15:54:32 - INFO - root -   Epoch: [243/500][160/346], lr: 0.00000448 	 loss = 0.0013(0.1237)
2023/09/15 15:54:49 - INFO - root -   Epoch: [243/500][180/346], lr: 0.00000448 	 loss = 0.0006(0.1124)
2023/09/15 15:55:10 - INFO - root -   Epoch: [243/500][200/346], lr: 0.00000448 	 loss = 0.0001(0.1022)
2023/09/15 15:55:26 - INFO - root -   Epoch: [243/500][220/346], lr: 0.00000448 	 loss = 0.5498(0.0960)
2023/09/15 15:55:47 - INFO - root -   Epoch: [243/500][240/346], lr: 0.00000448 	 loss = 0.0250(0.1261)
2023/09/15 15:56:04 - INFO - root -   Epoch: [243/500][260/346], lr: 0.00000448 	 loss = 0.6839(0.1217)
2023/09/15 15:56:25 - INFO - root -   Epoch: [243/500][280/346], lr: 0.00000448 	 loss = 0.0031(0.1163)
2023/09/15 15:56:41 - INFO - root -   Epoch: [243/500][300/346], lr: 0.00000448 	 loss = 0.0002(0.1101)
2023/09/15 15:57:02 - INFO - root -   Epoch: [243/500][320/346], lr: 0.00000448 	 loss = 0.0001(0.1040)
2023/09/15 15:57:19 - INFO - root -   Epoch: [243/500][340/346], lr: 0.00000448 	 loss = 0.0048(0.0985)
2023/09/15 15:57:21 - INFO - root -   Epoch: [243/500] 	 loss = 0.0972
2023/09/15 15:57:21 - INFO - root -   train_accuracy = 0.9754
2023/09/15 15:57:29 - INFO - root -   Epoch: [244/500][0/346], lr: 0.00000449 	 loss = 1.1422(1.1422)
2023/09/15 15:57:46 - INFO - root -   Epoch: [244/500][20/346], lr: 0.00000449 	 loss = 0.0483(0.1597)
2023/09/15 15:58:07 - INFO - root -   Epoch: [244/500][40/346], lr: 0.00000449 	 loss = 0.0159(0.0975)
2023/09/15 15:58:23 - INFO - root -   Epoch: [244/500][60/346], lr: 0.00000449 	 loss = 0.0039(0.0684)
2023/09/15 15:58:44 - INFO - root -   Epoch: [244/500][80/346], lr: 0.00000449 	 loss = 0.0177(0.0557)
2023/09/15 15:59:01 - INFO - root -   Epoch: [244/500][100/346], lr: 0.00000449 	 loss = 0.0010(0.0605)
2023/09/15 15:59:22 - INFO - root -   Epoch: [244/500][120/346], lr: 0.00000449 	 loss = 0.0008(0.0572)
2023/09/15 15:59:38 - INFO - root -   Epoch: [244/500][140/346], lr: 0.00000449 	 loss = 0.0021(0.0528)
2023/09/15 15:59:59 - INFO - root -   Epoch: [244/500][160/346], lr: 0.00000449 	 loss = 0.3403(0.0503)
2023/09/15 16:00:16 - INFO - root -   Epoch: [244/500][180/346], lr: 0.00000449 	 loss = 0.0016(0.0460)
2023/09/15 16:00:37 - INFO - root -   Epoch: [244/500][200/346], lr: 0.00000449 	 loss = 0.0000(0.0433)
2023/09/15 16:00:53 - INFO - root -   Epoch: [244/500][220/346], lr: 0.00000449 	 loss = 0.0039(0.0468)
2023/09/15 16:01:14 - INFO - root -   Epoch: [244/500][240/346], lr: 0.00000449 	 loss = 0.0014(0.0431)
2023/09/15 16:01:31 - INFO - root -   Epoch: [244/500][260/346], lr: 0.00000449 	 loss = 0.0061(0.0413)
2023/09/15 16:01:52 - INFO - root -   Epoch: [244/500][280/346], lr: 0.00000449 	 loss = 0.0008(0.0403)
2023/09/15 16:02:09 - INFO - root -   Epoch: [244/500][300/346], lr: 0.00000449 	 loss = 0.0663(0.0380)
2023/09/15 16:02:30 - INFO - root -   Epoch: [244/500][320/346], lr: 0.00000449 	 loss = 0.0001(0.0360)
2023/09/15 16:02:46 - INFO - root -   Epoch: [244/500][340/346], lr: 0.00000449 	 loss = 0.0028(0.0351)
2023/09/15 16:02:48 - INFO - root -   Epoch: [244/500] 	 loss = 0.0347
2023/09/15 16:04:11 - INFO - root -   precision = 0.4253
2023/09/15 16:04:11 - INFO - root -   eval_loss = 4.6204
2023/09/15 16:04:12 - INFO - root -   train_accuracy = 0.9899
2023/09/15 16:04:20 - INFO - root -   Epoch: [245/500][0/346], lr: 0.00000451 	 loss = 0.0867(0.0867)
2023/09/15 16:04:37 - INFO - root -   Epoch: [245/500][20/346], lr: 0.00000451 	 loss = 0.0509(0.1233)
2023/09/15 16:04:58 - INFO - root -   Epoch: [245/500][40/346], lr: 0.00000451 	 loss = 0.0000(0.0945)
2023/09/15 16:05:14 - INFO - root -   Epoch: [245/500][60/346], lr: 0.00000451 	 loss = 0.0002(0.0775)
2023/09/15 16:05:35 - INFO - root -   Epoch: [245/500][80/346], lr: 0.00000451 	 loss = 0.0020(0.0723)
2023/09/15 16:05:52 - INFO - root -   Epoch: [245/500][100/346], lr: 0.00000451 	 loss = 0.0003(0.0610)
