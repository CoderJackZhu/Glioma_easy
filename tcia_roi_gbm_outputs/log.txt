2023/10/21 23:04:06 - INFO - root -   Num train examples = 169
2023/10/21 23:04:06 - INFO - root -   Num val examples = 43
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:06 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/10/21 23:04:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/10/21 23:04:07 - INFO - root -   backend = nccl
2023/10/21 23:04:07 - INFO - root -   batch_size = 2
2023/10/21 23:04:07 - INFO - root -   dropout = 0.5
2023/10/21 23:04:07 - INFO - root -   epochs = 300
2023/10/21 23:04:07 - INFO - root -   eval_freq = 5
2023/10/21 23:04:07 - INFO - root -   focal_loss = False
2023/10/21 23:04:07 - INFO - root -   input_size = 224
2023/10/21 23:04:07 - INFO - root -   is_pretrained = False
2023/10/21 23:04:07 - INFO - root -   label_smooth = False
2023/10/21 23:04:07 - INFO - root -   local_rank = -1
2023/10/21 23:04:07 - INFO - root -   lr = 1e-05
2023/10/21 23:04:07 - INFO - root -   lr_decay_rate = 0.1
2023/10/21 23:04:07 - INFO - root -   lr_steps = [50, 100]
2023/10/21 23:04:07 - INFO - root -   lr_type = cosine
2023/10/21 23:04:07 - INFO - root -   model_depth = 34
2023/10/21 23:04:07 - INFO - root -   model_name = resnet50
2023/10/21 23:04:07 - INFO - root -   momentum = 0.9
2023/10/21 23:04:07 - INFO - root -   num_classes = 2
2023/10/21 23:04:07 - INFO - root -   output = ./tcia_roi_gbm_outputs
2023/10/21 23:04:07 - INFO - root -   print_freq = 20
2023/10/21 23:04:07 - INFO - root -   resume = 
2023/10/21 23:04:07 - INFO - root -   start_epoch = 0
2023/10/21 23:04:07 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/21 23:04:07 - INFO - root -   tune_from = /media/spgou/DATA/ZYJ/Glioma_easy/tcia_roi_idh_outputs/precision_0.9070_num_210/epoch_210.pth
2023/10/21 23:04:07 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/21 23:04:07 - INFO - root -   warmup_epoch = 20
2023/10/21 23:04:07 - INFO - root -   warmup_multiplier = 100
2023/10/21 23:04:07 - INFO - root -   weight_decay = 0.0005
2023/10/21 23:04:07 - INFO - root -   workers = 8
2023/10/21 23:04:29 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 0.5279(0.5279)
2023/10/21 23:05:43 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 0.6796(0.7025)
2023/10/21 23:07:52 - INFO - root -   Epoch: [0/300][40/84], lr: 0.00000010 	 loss = 0.2308(0.7789)
2023/10/21 23:08:55 - INFO - root -   Epoch: [0/300][60/84], lr: 0.00000010 	 loss = 0.5669(0.7440)
2023/10/21 23:09:44 - INFO - root -   Epoch: [0/300][80/84], lr: 0.00000010 	 loss = 1.1829(0.7651)
2023/10/21 23:09:46 - INFO - root -   Epoch: [0/300] 	 loss = 0.7700
2023/10/21 23:09:46 - INFO - root -   train_accuracy = 0.4702
2023/10/21 23:10:24 - INFO - root -   Epoch: [1/300][0/84], lr: 0.00000011 	 loss = 0.2789(0.2789)
2023/10/21 23:12:15 - INFO - root -   Epoch: [1/300][20/84], lr: 0.00000011 	 loss = 0.4672(0.6723)
2023/10/21 23:13:29 - INFO - root -   Epoch: [1/300][40/84], lr: 0.00000011 	 loss = 0.5008(0.7311)
2023/10/21 23:14:32 - INFO - root -   Epoch: [1/300][60/84], lr: 0.00000011 	 loss = 1.0478(0.7608)
2023/10/21 23:15:15 - INFO - root -   Epoch: [1/300][80/84], lr: 0.00000011 	 loss = 0.4314(0.7760)
2023/10/21 23:15:16 - INFO - root -   Epoch: [1/300] 	 loss = 0.7753
2023/10/21 23:15:16 - INFO - root -   train_accuracy = 0.5238
2023/10/21 23:16:01 - INFO - root -   Epoch: [2/300][0/84], lr: 0.00000011 	 loss = 0.4113(0.4113)
2023/10/21 23:16:53 - INFO - root -   Epoch: [2/300][20/84], lr: 0.00000011 	 loss = 0.9889(0.6471)
2023/10/21 23:17:59 - INFO - root -   Epoch: [2/300][40/84], lr: 0.00000011 	 loss = 0.8320(0.6796)
2023/10/21 23:18:52 - INFO - root -   Epoch: [2/300][60/84], lr: 0.00000011 	 loss = 1.0288(0.6713)
2023/10/21 23:20:41 - INFO - root -   Epoch: [2/300][80/84], lr: 0.00000011 	 loss = 0.5401(0.6991)
2023/10/21 23:20:42 - INFO - root -   Epoch: [2/300] 	 loss = 0.6950
2023/10/21 23:20:42 - INFO - root -   train_accuracy = 0.6071
2023/10/21 23:21:23 - INFO - root -   Epoch: [3/300][0/84], lr: 0.00000012 	 loss = 1.0156(1.0156)
2023/10/21 23:22:14 - INFO - root -   Epoch: [3/300][20/84], lr: 0.00000012 	 loss = 1.1103(0.6609)
2023/10/21 23:23:51 - INFO - root -   Epoch: [3/300][40/84], lr: 0.00000012 	 loss = 0.4354(0.6827)
2023/10/21 23:24:45 - INFO - root -   Epoch: [3/300][60/84], lr: 0.00000012 	 loss = 0.5004(0.7315)
2023/10/21 23:25:35 - INFO - root -   Epoch: [3/300][80/84], lr: 0.00000012 	 loss = 0.5757(0.7416)
2023/10/21 23:25:36 - INFO - root -   Epoch: [3/300] 	 loss = 0.7461
2023/10/21 23:25:36 - INFO - root -   train_accuracy = 0.5536
2023/10/21 23:25:59 - INFO - root -   Epoch: [4/300][0/84], lr: 0.00000012 	 loss = 0.9297(0.9297)
2023/10/21 23:27:06 - INFO - root -   Epoch: [4/300][20/84], lr: 0.00000012 	 loss = 0.6700(0.7879)
2023/10/21 23:28:20 - INFO - root -   Epoch: [4/300][40/84], lr: 0.00000012 	 loss = 0.6029(0.7109)
2023/10/21 23:29:26 - INFO - root -   Epoch: [4/300][60/84], lr: 0.00000012 	 loss = 0.5678(0.6925)
2023/10/21 23:30:18 - INFO - root -   Epoch: [4/300][80/84], lr: 0.00000012 	 loss = 0.3690(0.7090)
2023/10/21 23:30:21 - INFO - root -   Epoch: [4/300] 	 loss = 0.7135
2023/10/21 23:31:17 - INFO - root -   precision = 0.6279
2023/10/21 23:31:17 - INFO - root -   eval_loss = 0.6331
2023/10/21 23:31:17 - INFO - root -   eval_acc = 0.6279
2023/10/21 23:31:18 - INFO - root -   train_accuracy = 0.5595
2023/10/21 23:31:40 - INFO - root -   Epoch: [5/300][0/84], lr: 0.00000013 	 loss = 0.8638(0.8638)
2023/10/21 23:33:09 - INFO - root -   Epoch: [5/300][20/84], lr: 0.00000013 	 loss = 0.7794(0.6813)
2023/10/21 23:34:04 - INFO - root -   Epoch: [5/300][40/84], lr: 0.00000013 	 loss = 0.5858(0.6492)
2023/10/21 23:35:23 - INFO - root -   Epoch: [5/300][60/84], lr: 0.00000013 	 loss = 0.3807(0.6877)
2023/10/21 23:35:51 - INFO - root -   Epoch: [5/300][80/84], lr: 0.00000013 	 loss = 0.6665(0.7109)
2023/10/21 23:35:53 - INFO - root -   Epoch: [5/300] 	 loss = 0.7061
2023/10/21 23:35:53 - INFO - root -   train_accuracy = 0.5595
2023/10/21 23:36:15 - INFO - root -   Epoch: [6/300][0/84], lr: 0.00000014 	 loss = 1.1527(1.1527)
2023/10/21 23:37:28 - INFO - root -   Epoch: [6/300][20/84], lr: 0.00000014 	 loss = 0.7185(0.6901)
2023/10/21 23:38:19 - INFO - root -   Epoch: [6/300][40/84], lr: 0.00000014 	 loss = 0.7709(0.6771)
2023/10/21 23:39:25 - INFO - root -   Epoch: [6/300][60/84], lr: 0.00000014 	 loss = 0.3783(0.6919)
2023/10/21 23:40:19 - INFO - root -   Epoch: [6/300][80/84], lr: 0.00000014 	 loss = 0.6034(0.6952)
2023/10/21 23:40:23 - INFO - root -   Epoch: [6/300] 	 loss = 0.6966
2023/10/21 23:40:23 - INFO - root -   train_accuracy = 0.6012
2023/10/21 23:41:01 - INFO - root -   Epoch: [7/300][0/84], lr: 0.00000014 	 loss = 0.4738(0.4738)
2023/10/21 23:42:00 - INFO - root -   Epoch: [7/300][20/84], lr: 0.00000014 	 loss = 1.2251(0.6689)
2023/10/21 23:43:16 - INFO - root -   Epoch: [7/300][40/84], lr: 0.00000014 	 loss = 0.3401(0.6638)
2023/10/21 23:44:17 - INFO - root -   Epoch: [7/300][60/84], lr: 0.00000014 	 loss = 2.1141(0.7005)
2023/10/21 23:45:14 - INFO - root -   Epoch: [7/300][80/84], lr: 0.00000014 	 loss = 0.5090(0.6947)
2023/10/21 23:45:15 - INFO - root -   Epoch: [7/300] 	 loss = 0.6997
2023/10/21 23:45:15 - INFO - root -   train_accuracy = 0.5774
2023/10/21 23:45:37 - INFO - root -   Epoch: [8/300][0/84], lr: 0.00000015 	 loss = 0.4942(0.4942)
2023/10/21 23:46:50 - INFO - root -   Epoch: [8/300][20/84], lr: 0.00000015 	 loss = 0.4651(0.7292)
2023/10/21 23:47:53 - INFO - root -   Epoch: [8/300][40/84], lr: 0.00000015 	 loss = 0.6699(0.6801)
2023/10/21 23:49:01 - INFO - root -   Epoch: [8/300][60/84], lr: 0.00000015 	 loss = 0.7962(0.6833)
2023/10/21 23:49:44 - INFO - root -   Epoch: [8/300][80/84], lr: 0.00000015 	 loss = 0.3385(0.6619)
2023/10/21 23:49:47 - INFO - root -   Epoch: [8/300] 	 loss = 0.6649
2023/10/21 23:49:47 - INFO - root -   train_accuracy = 0.6190
2023/10/21 23:50:18 - INFO - root -   Epoch: [9/300][0/84], lr: 0.00000015 	 loss = 0.3685(0.3685)
2023/10/21 23:51:06 - INFO - root -   Epoch: [9/300][20/84], lr: 0.00000015 	 loss = 0.5893(0.6999)
2023/10/21 23:52:37 - INFO - root -   Epoch: [9/300][40/84], lr: 0.00000015 	 loss = 0.2698(0.6206)
2023/10/21 23:53:20 - INFO - root -   Epoch: [9/300][60/84], lr: 0.00000015 	 loss = 0.3367(0.6160)
2023/10/21 23:54:15 - INFO - root -   Epoch: [9/300][80/84], lr: 0.00000015 	 loss = 0.5178(0.6171)
2023/10/21 23:54:16 - INFO - root -   Epoch: [9/300] 	 loss = 0.6094
2023/10/21 23:55:14 - INFO - root -   precision = 0.6279
2023/10/21 23:55:14 - INFO - root -   eval_loss = 0.6018
2023/10/21 23:55:14 - INFO - root -   eval_acc = 0.6279
2023/10/21 23:55:15 - INFO - root -   train_accuracy = 0.6429
2023/10/21 23:55:37 - INFO - root -   Epoch: [10/300][0/84], lr: 0.00000016 	 loss = 0.7131(0.7131)
2023/10/21 23:57:05 - INFO - root -   Epoch: [10/300][20/84], lr: 0.00000016 	 loss = 0.7508(0.6714)
2023/10/21 23:58:01 - INFO - root -   Epoch: [10/300][40/84], lr: 0.00000016 	 loss = 0.6036(0.6608)
2023/10/21 23:59:22 - INFO - root -   Epoch: [10/300][60/84], lr: 0.00000016 	 loss = 0.5675(0.6353)
2023/10/22 00:00:03 - INFO - root -   Epoch: [10/300][80/84], lr: 0.00000016 	 loss = 0.3772(0.6620)
2023/10/22 00:00:10 - INFO - root -   Epoch: [10/300] 	 loss = 0.6582
2023/10/22 00:00:10 - INFO - root -   train_accuracy = 0.6012
2023/10/22 00:00:32 - INFO - root -   Epoch: [11/300][0/84], lr: 0.00000016 	 loss = 0.9258(0.9258)
2023/10/22 00:01:47 - INFO - root -   Epoch: [11/300][20/84], lr: 0.00000016 	 loss = 0.9690(0.8370)
2023/10/22 00:02:51 - INFO - root -   Epoch: [11/300][40/84], lr: 0.00000016 	 loss = 0.8179(0.7322)
2023/10/22 00:04:06 - INFO - root -   Epoch: [11/300][60/84], lr: 0.00000016 	 loss = 0.5012(0.6916)
2023/10/22 00:04:59 - INFO - root -   Epoch: [11/300][80/84], lr: 0.00000016 	 loss = 0.4437(0.6864)
2023/10/22 00:05:00 - INFO - root -   Epoch: [11/300] 	 loss = 0.6924
2023/10/22 00:05:00 - INFO - root -   train_accuracy = 0.5893
2023/10/22 00:05:30 - INFO - root -   Epoch: [12/300][0/84], lr: 0.00000017 	 loss = 0.3151(0.3151)
2023/10/22 00:06:29 - INFO - root -   Epoch: [12/300][20/84], lr: 0.00000017 	 loss = 1.3636(0.7281)
2023/10/22 00:07:44 - INFO - root -   Epoch: [12/300][40/84], lr: 0.00000017 	 loss = 0.4989(0.6277)
2023/10/22 00:08:42 - INFO - root -   Epoch: [12/300][60/84], lr: 0.00000017 	 loss = 0.1730(0.6182)
2023/10/22 00:09:33 - INFO - root -   Epoch: [12/300][80/84], lr: 0.00000017 	 loss = 0.2760(0.6533)
2023/10/22 00:09:36 - INFO - root -   Epoch: [12/300] 	 loss = 0.6603
2023/10/22 00:09:36 - INFO - root -   train_accuracy = 0.6488
2023/10/22 00:09:58 - INFO - root -   Epoch: [13/300][0/84], lr: 0.00000018 	 loss = 0.8446(0.8446)
2023/10/22 00:10:56 - INFO - root -   Epoch: [13/300][20/84], lr: 0.00000018 	 loss = 0.7201(0.6382)
2023/10/22 00:12:08 - INFO - root -   Epoch: [13/300][40/84], lr: 0.00000018 	 loss = 0.4753(0.6630)
2023/10/22 00:13:23 - INFO - root -   Epoch: [13/300][60/84], lr: 0.00000018 	 loss = 1.1452(0.6391)
2023/10/22 00:14:01 - INFO - root -   Epoch: [13/300][80/84], lr: 0.00000018 	 loss = 0.1953(0.6433)
2023/10/22 00:14:05 - INFO - root -   Epoch: [13/300] 	 loss = 0.6344
2023/10/22 00:14:05 - INFO - root -   train_accuracy = 0.6429
2023/10/22 00:14:26 - INFO - root -   Epoch: [14/300][0/84], lr: 0.00000018 	 loss = 0.9755(0.9755)
2023/10/22 00:15:42 - INFO - root -   Epoch: [14/300][20/84], lr: 0.00000018 	 loss = 0.2633(0.6707)
2023/10/22 00:16:48 - INFO - root -   Epoch: [14/300][40/84], lr: 0.00000018 	 loss = 0.2503(0.6810)
2023/10/22 00:17:50 - INFO - root -   Epoch: [14/300][60/84], lr: 0.00000018 	 loss = 1.1492(0.7108)
2023/10/22 00:18:46 - INFO - root -   Epoch: [14/300][80/84], lr: 0.00000018 	 loss = 1.7650(0.7097)
2023/10/22 00:18:48 - INFO - root -   Epoch: [14/300] 	 loss = 0.7058
2023/10/22 00:19:46 - INFO - root -   precision = 0.6977
2023/10/22 00:19:46 - INFO - root -   eval_loss = 0.5393
2023/10/22 00:19:46 - INFO - root -   eval_acc = 0.6977
2023/10/22 00:19:47 - INFO - root -   train_accuracy = 0.5952
2023/10/22 00:20:17 - INFO - root -   Epoch: [15/300][0/84], lr: 0.00000019 	 loss = 0.4171(0.4171)
2023/10/22 00:21:08 - INFO - root -   Epoch: [15/300][20/84], lr: 0.00000019 	 loss = 0.3756(0.6519)
2023/10/22 00:22:24 - INFO - root -   Epoch: [15/300][40/84], lr: 0.00000019 	 loss = 0.2056(0.6312)
2023/10/22 00:23:14 - INFO - root -   Epoch: [15/300][60/84], lr: 0.00000019 	 loss = 0.4546(0.6656)
2023/10/22 00:24:11 - INFO - root -   Epoch: [15/300][80/84], lr: 0.00000019 	 loss = 0.4325(0.6523)
2023/10/22 00:24:12 - INFO - root -   Epoch: [15/300] 	 loss = 0.6492
2023/10/22 00:24:12 - INFO - root -   train_accuracy = 0.6310
2023/10/22 00:24:34 - INFO - root -   Epoch: [16/300][0/84], lr: 0.00000019 	 loss = 0.5427(0.5427)
2023/10/22 00:25:56 - INFO - root -   Epoch: [16/300][20/84], lr: 0.00000019 	 loss = 0.4260(0.7659)
2023/10/22 00:26:51 - INFO - root -   Epoch: [16/300][40/84], lr: 0.00000019 	 loss = 0.2634(0.6737)
2023/10/22 00:28:09 - INFO - root -   Epoch: [16/300][60/84], lr: 0.00000019 	 loss = 0.4750(0.6472)
2023/10/22 00:28:50 - INFO - root -   Epoch: [16/300][80/84], lr: 0.00000019 	 loss = 0.3097(0.6411)
2023/10/22 00:28:51 - INFO - root -   Epoch: [16/300] 	 loss = 0.6371
2023/10/22 00:28:51 - INFO - root -   train_accuracy = 0.6131
2023/10/22 00:29:13 - INFO - root -   Epoch: [17/300][0/84], lr: 0.00000020 	 loss = 0.4903(0.4903)
2023/10/22 00:30:26 - INFO - root -   Epoch: [17/300][20/84], lr: 0.00000020 	 loss = 0.6601(0.7232)
2023/10/22 00:31:46 - INFO - root -   Epoch: [17/300][40/84], lr: 0.00000020 	 loss = 0.5318(0.6941)
2023/10/22 00:32:48 - INFO - root -   Epoch: [17/300][60/84], lr: 0.00000020 	 loss = 0.2175(0.6707)
2023/10/22 00:33:44 - INFO - root -   Epoch: [17/300][80/84], lr: 0.00000020 	 loss = 0.4550(0.6742)
2023/10/22 00:33:45 - INFO - root -   Epoch: [17/300] 	 loss = 0.6745
2023/10/22 00:33:45 - INFO - root -   train_accuracy = 0.6012
2023/10/22 00:34:14 - INFO - root -   Epoch: [18/300][0/84], lr: 0.00000021 	 loss = 0.6580(0.6580)
2023/10/22 00:35:18 - INFO - root -   Epoch: [18/300][20/84], lr: 0.00000021 	 loss = 0.7239(0.7574)
2023/10/22 00:36:20 - INFO - root -   Epoch: [18/300][40/84], lr: 0.00000021 	 loss = 0.7656(0.6281)
2023/10/22 00:37:39 - INFO - root -   Epoch: [18/300][60/84], lr: 0.00000021 	 loss = 0.9721(0.6868)
2023/10/22 00:38:22 - INFO - root -   Epoch: [18/300][80/84], lr: 0.00000021 	 loss = 0.3708(0.6800)
2023/10/22 00:38:23 - INFO - root -   Epoch: [18/300] 	 loss = 0.6785
2023/10/22 00:38:23 - INFO - root -   train_accuracy = 0.6071
2023/10/22 00:38:55 - INFO - root -   Epoch: [19/300][0/84], lr: 0.00000021 	 loss = 0.5807(0.5807)
2023/10/22 00:39:49 - INFO - root -   Epoch: [19/300][20/84], lr: 0.00000021 	 loss = 0.7454(0.6463)
2023/10/22 00:41:02 - INFO - root -   Epoch: [19/300][40/84], lr: 0.00000021 	 loss = 0.3310(0.6158)
2023/10/22 00:42:10 - INFO - root -   Epoch: [19/300][60/84], lr: 0.00000021 	 loss = 0.7065(0.6224)
2023/10/22 00:42:58 - INFO - root -   Epoch: [19/300][80/84], lr: 0.00000021 	 loss = 0.8928(0.6135)
2023/10/22 00:43:00 - INFO - root -   Epoch: [19/300] 	 loss = 0.6257
2023/10/22 00:43:57 - INFO - root -   precision = 0.6977
2023/10/22 00:43:57 - INFO - root -   eval_loss = 0.5459
2023/10/22 00:43:57 - INFO - root -   eval_acc = 0.6977
2023/10/22 00:43:58 - INFO - root -   train_accuracy = 0.6726
2023/10/22 00:44:28 - INFO - root -   Epoch: [20/300][0/84], lr: 0.00000022 	 loss = 0.3931(0.3931)
2023/10/22 00:45:46 - INFO - root -   Epoch: [20/300][20/84], lr: 0.00000022 	 loss = 0.3247(0.6457)
2023/10/22 00:46:46 - INFO - root -   Epoch: [20/300][40/84], lr: 0.00000022 	 loss = 0.4369(0.5858)
2023/10/22 00:47:52 - INFO - root -   Epoch: [20/300][60/84], lr: 0.00000022 	 loss = 0.3815(0.5333)
2023/10/22 00:48:42 - INFO - root -   Epoch: [20/300][80/84], lr: 0.00000022 	 loss = 0.8777(0.5477)
2023/10/22 00:48:45 - INFO - root -   Epoch: [20/300] 	 loss = 0.5575
2023/10/22 00:48:45 - INFO - root -   train_accuracy = 0.7202
2023/10/22 00:49:06 - INFO - root -   Epoch: [21/300][0/84], lr: 0.00000022 	 loss = 0.5339(0.5339)
2023/10/22 00:50:21 - INFO - root -   Epoch: [21/300][20/84], lr: 0.00000022 	 loss = 1.0151(0.6117)
2023/10/22 00:51:26 - INFO - root -   Epoch: [21/300][40/84], lr: 0.00000022 	 loss = 0.3452(0.5909)
2023/10/22 00:52:34 - INFO - root -   Epoch: [21/300][60/84], lr: 0.00000022 	 loss = 0.3665(0.5453)
2023/10/22 00:53:17 - INFO - root -   Epoch: [21/300][80/84], lr: 0.00000022 	 loss = 0.3942(0.5884)
2023/10/22 00:53:18 - INFO - root -   Epoch: [21/300] 	 loss = 0.5970
2023/10/22 00:53:18 - INFO - root -   train_accuracy = 0.7262
2023/10/22 00:53:40 - INFO - root -   Epoch: [22/300][0/84], lr: 0.00000023 	 loss = 0.9537(0.9537)
2023/10/22 00:54:47 - INFO - root -   Epoch: [22/300][20/84], lr: 0.00000023 	 loss = 0.1943(0.4918)
2023/10/22 00:55:50 - INFO - root -   Epoch: [22/300][40/84], lr: 0.00000023 	 loss = 0.4231(0.4942)
2023/10/22 00:57:11 - INFO - root -   Epoch: [22/300][60/84], lr: 0.00000023 	 loss = 0.1609(0.5413)
2023/10/22 00:57:56 - INFO - root -   Epoch: [22/300][80/84], lr: 0.00000023 	 loss = 0.4897(0.5719)
2023/10/22 00:58:02 - INFO - root -   Epoch: [22/300] 	 loss = 0.5930
2023/10/22 00:58:02 - INFO - root -   train_accuracy = 0.7024
2023/10/22 00:58:23 - INFO - root -   Epoch: [23/300][0/84], lr: 0.00000024 	 loss = 0.3209(0.3209)
2023/10/22 00:59:29 - INFO - root -   Epoch: [23/300][20/84], lr: 0.00000024 	 loss = 0.6576(0.5944)
2023/10/22 01:00:43 - INFO - root -   Epoch: [23/300][40/84], lr: 0.00000024 	 loss = 1.0198(0.6225)
2023/10/22 01:01:47 - INFO - root -   Epoch: [23/300][60/84], lr: 0.00000024 	 loss = 0.2584(0.6311)
2023/10/22 01:02:37 - INFO - root -   Epoch: [23/300][80/84], lr: 0.00000024 	 loss = 2.0143(0.6492)
2023/10/22 01:02:38 - INFO - root -   Epoch: [23/300] 	 loss = 0.6531
2023/10/22 01:02:38 - INFO - root -   train_accuracy = 0.6726
2023/10/22 01:03:07 - INFO - root -   Epoch: [24/300][0/84], lr: 0.00000024 	 loss = 1.1012(1.1012)
2023/10/22 01:04:14 - INFO - root -   Epoch: [24/300][20/84], lr: 0.00000024 	 loss = 0.8564(0.6272)
2023/10/22 01:05:23 - INFO - root -   Epoch: [24/300][40/84], lr: 0.00000024 	 loss = 0.2719(0.5777)
2023/10/22 01:06:28 - INFO - root -   Epoch: [24/300][60/84], lr: 0.00000024 	 loss = 0.8497(0.6242)
2023/10/22 01:07:22 - INFO - root -   Epoch: [24/300][80/84], lr: 0.00000024 	 loss = 1.0180(0.6236)
2023/10/22 01:07:24 - INFO - root -   Epoch: [24/300] 	 loss = 0.6304
2023/10/22 01:08:20 - INFO - root -   precision = 0.7442
2023/10/22 01:08:20 - INFO - root -   eval_loss = 0.5090
2023/10/22 01:08:20 - INFO - root -   eval_acc = 0.7442
2023/10/22 01:08:21 - INFO - root -   train_accuracy = 0.6548
2023/10/22 01:08:42 - INFO - root -   Epoch: [25/300][0/84], lr: 0.00000025 	 loss = 0.3913(0.3913)
2023/10/22 01:09:50 - INFO - root -   Epoch: [25/300][20/84], lr: 0.00000025 	 loss = 0.5509(0.5384)
2023/10/22 01:10:53 - INFO - root -   Epoch: [25/300][40/84], lr: 0.00000025 	 loss = 1.1589(0.5808)
2023/10/22 01:12:03 - INFO - root -   Epoch: [25/300][60/84], lr: 0.00000025 	 loss = 0.6397(0.5868)
2023/10/22 01:12:50 - INFO - root -   Epoch: [25/300][80/84], lr: 0.00000025 	 loss = 0.4648(0.6049)
2023/10/22 01:12:53 - INFO - root -   Epoch: [25/300] 	 loss = 0.6180
2023/10/22 01:12:53 - INFO - root -   train_accuracy = 0.6548
2023/10/22 01:13:22 - INFO - root -   Epoch: [26/300][0/84], lr: 0.00000025 	 loss = 1.1634(1.1634)
2023/10/22 01:14:28 - INFO - root -   Epoch: [26/300][20/84], lr: 0.00000025 	 loss = 0.5291(0.6495)
2023/10/22 01:15:34 - INFO - root -   Epoch: [26/300][40/84], lr: 0.00000025 	 loss = 0.2934(0.6050)
2023/10/22 01:16:58 - INFO - root -   Epoch: [26/300][60/84], lr: 0.00000025 	 loss = 0.8412(0.5930)
2023/10/22 01:17:42 - INFO - root -   Epoch: [26/300][80/84], lr: 0.00000025 	 loss = 0.1104(0.5805)
2023/10/22 01:17:46 - INFO - root -   Epoch: [26/300] 	 loss = 0.5798
2023/10/22 01:17:46 - INFO - root -   train_accuracy = 0.6667
2023/10/22 01:18:25 - INFO - root -   Epoch: [27/300][0/84], lr: 0.00000026 	 loss = 0.2511(0.2511)
2023/10/22 01:19:25 - INFO - root -   Epoch: [27/300][20/84], lr: 0.00000026 	 loss = 0.4738(0.6590)
2023/10/22 01:20:29 - INFO - root -   Epoch: [27/300][40/84], lr: 0.00000026 	 loss = 0.3078(0.6729)
2023/10/22 01:21:42 - INFO - root -   Epoch: [27/300][60/84], lr: 0.00000026 	 loss = 0.3428(0.6328)
2023/10/22 01:22:30 - INFO - root -   Epoch: [27/300][80/84], lr: 0.00000026 	 loss = 0.2985(0.6081)
2023/10/22 01:22:32 - INFO - root -   Epoch: [27/300] 	 loss = 0.6168
2023/10/22 01:22:32 - INFO - root -   train_accuracy = 0.6190
2023/10/22 01:22:54 - INFO - root -   Epoch: [28/300][0/84], lr: 0.00000027 	 loss = 0.6338(0.6338)
2023/10/22 01:24:02 - INFO - root -   Epoch: [28/300][20/84], lr: 0.00000027 	 loss = 1.3245(0.6716)
2023/10/22 01:25:19 - INFO - root -   Epoch: [28/300][40/84], lr: 0.00000027 	 loss = 0.2657(0.6368)
2023/10/22 01:26:13 - INFO - root -   Epoch: [28/300][60/84], lr: 0.00000027 	 loss = 0.6988(0.6455)
2023/10/22 01:27:12 - INFO - root -   Epoch: [28/300][80/84], lr: 0.00000027 	 loss = 0.1665(0.6507)
2023/10/22 01:27:14 - INFO - root -   Epoch: [28/300] 	 loss = 0.6433
2023/10/22 01:27:14 - INFO - root -   train_accuracy = 0.6369
2023/10/22 01:27:36 - INFO - root -   Epoch: [29/300][0/84], lr: 0.00000027 	 loss = 0.2483(0.2483)
2023/10/22 01:28:44 - INFO - root -   Epoch: [29/300][20/84], lr: 0.00000027 	 loss = 0.8281(0.5484)
2023/10/22 01:29:46 - INFO - root -   Epoch: [29/300][40/84], lr: 0.00000027 	 loss = 0.2426(0.6010)
2023/10/22 01:31:02 - INFO - root -   Epoch: [29/300][60/84], lr: 0.00000027 	 loss = 0.1323(0.5493)
2023/10/22 01:31:39 - INFO - root -   Epoch: [29/300][80/84], lr: 0.00000027 	 loss = 0.6051(0.5553)
2023/10/22 01:31:41 - INFO - root -   Epoch: [29/300] 	 loss = 0.5726
2023/10/22 01:32:38 - INFO - root -   precision = 0.7209
2023/10/22 01:32:38 - INFO - root -   eval_loss = 0.4891
2023/10/22 01:32:38 - INFO - root -   eval_acc = 0.7209
2023/10/22 01:32:39 - INFO - root -   train_accuracy = 0.7024
2023/10/22 01:33:00 - INFO - root -   Epoch: [30/300][0/84], lr: 0.00000028 	 loss = 0.5137(0.5137)
2023/10/22 01:34:15 - INFO - root -   Epoch: [30/300][20/84], lr: 0.00000028 	 loss = 0.7956(0.6638)
2023/10/22 01:35:28 - INFO - root -   Epoch: [30/300][40/84], lr: 0.00000028 	 loss = 1.1834(0.6433)
2023/10/22 01:36:27 - INFO - root -   Epoch: [30/300][60/84], lr: 0.00000028 	 loss = 0.4743(0.6529)
2023/10/22 01:37:16 - INFO - root -   Epoch: [30/300][80/84], lr: 0.00000028 	 loss = 0.9542(0.6609)
2023/10/22 01:37:17 - INFO - root -   Epoch: [30/300] 	 loss = 0.6720
2023/10/22 01:37:17 - INFO - root -   train_accuracy = 0.6071
2023/10/22 01:37:40 - INFO - root -   Epoch: [31/300][0/84], lr: 0.00000028 	 loss = 1.0311(1.0311)
2023/10/22 01:38:52 - INFO - root -   Epoch: [31/300][20/84], lr: 0.00000028 	 loss = 0.9647(0.6819)
2023/10/22 01:40:01 - INFO - root -   Epoch: [31/300][40/84], lr: 0.00000028 	 loss = 0.4180(0.6304)
2023/10/22 01:41:08 - INFO - root -   Epoch: [31/300][60/84], lr: 0.00000028 	 loss = 0.4839(0.5769)
2023/10/22 01:41:59 - INFO - root -   Epoch: [31/300][80/84], lr: 0.00000028 	 loss = 0.6910(0.5671)
2023/10/22 01:42:01 - INFO - root -   Epoch: [31/300] 	 loss = 0.5677
2023/10/22 01:42:01 - INFO - root -   train_accuracy = 0.6905
2023/10/22 01:42:22 - INFO - root -   Epoch: [32/300][0/84], lr: 0.00000029 	 loss = 0.4075(0.4075)
2023/10/22 01:43:33 - INFO - root -   Epoch: [32/300][20/84], lr: 0.00000029 	 loss = 0.8486(0.5449)
2023/10/22 01:44:43 - INFO - root -   Epoch: [32/300][40/84], lr: 0.00000029 	 loss = 0.4258(0.5834)
2023/10/22 01:45:37 - INFO - root -   Epoch: [32/300][60/84], lr: 0.00000029 	 loss = 0.4912(0.5667)
2023/10/22 01:46:31 - INFO - root -   Epoch: [32/300][80/84], lr: 0.00000029 	 loss = 0.2383(0.5513)
2023/10/22 01:46:33 - INFO - root -   Epoch: [32/300] 	 loss = 0.5536
2023/10/22 01:46:33 - INFO - root -   train_accuracy = 0.7262
2023/10/22 01:46:55 - INFO - root -   Epoch: [33/300][0/84], lr: 0.00000029 	 loss = 0.7582(0.7582)
2023/10/22 01:47:53 - INFO - root -   Epoch: [33/300][20/84], lr: 0.00000029 	 loss = 0.6318(0.5984)
2023/10/22 01:49:01 - INFO - root -   Epoch: [33/300][40/84], lr: 0.00000029 	 loss = 0.1910(0.5692)
2023/10/22 01:50:19 - INFO - root -   Epoch: [33/300][60/84], lr: 0.00000029 	 loss = 0.4486(0.5530)
2023/10/22 01:51:09 - INFO - root -   Epoch: [33/300][80/84], lr: 0.00000029 	 loss = 0.0985(0.5586)
2023/10/22 01:51:15 - INFO - root -   Epoch: [33/300] 	 loss = 0.5567
2023/10/22 01:51:15 - INFO - root -   train_accuracy = 0.6964
2023/10/22 01:51:37 - INFO - root -   Epoch: [34/300][0/84], lr: 0.00000030 	 loss = 1.2834(1.2834)
2023/10/22 01:52:40 - INFO - root -   Epoch: [34/300][20/84], lr: 0.00000030 	 loss = 0.3405(0.6360)
2023/10/22 01:54:01 - INFO - root -   Epoch: [34/300][40/84], lr: 0.00000030 	 loss = 0.3074(0.6319)
2023/10/22 01:54:58 - INFO - root -   Epoch: [34/300][60/84], lr: 0.00000030 	 loss = 0.7170(0.5955)
2023/10/22 01:55:52 - INFO - root -   Epoch: [34/300][80/84], lr: 0.00000030 	 loss = 0.1048(0.5818)
2023/10/22 01:55:57 - INFO - root -   Epoch: [34/300] 	 loss = 0.5866
2023/10/22 01:56:54 - INFO - root -   precision = 0.7907
2023/10/22 01:56:54 - INFO - root -   eval_loss = 0.4381
2023/10/22 01:56:54 - INFO - root -   eval_acc = 0.7907
2023/10/22 01:56:55 - INFO - root -   train_accuracy = 0.6726
2023/10/22 01:57:16 - INFO - root -   Epoch: [35/300][0/84], lr: 0.00000031 	 loss = 0.1424(0.1424)
2023/10/22 01:58:22 - INFO - root -   Epoch: [35/300][20/84], lr: 0.00000031 	 loss = 0.6618(0.6918)
2023/10/22 01:59:45 - INFO - root -   Epoch: [35/300][40/84], lr: 0.00000031 	 loss = 0.4100(0.6312)
2023/10/22 02:00:37 - INFO - root -   Epoch: [35/300][60/84], lr: 0.00000031 	 loss = 0.2152(0.6084)
2023/10/22 02:01:35 - INFO - root -   Epoch: [35/300][80/84], lr: 0.00000031 	 loss = 0.1696(0.6009)
2023/10/22 02:01:37 - INFO - root -   Epoch: [35/300] 	 loss = 0.5964
2023/10/22 02:01:37 - INFO - root -   train_accuracy = 0.6905
2023/10/22 02:02:07 - INFO - root -   Epoch: [36/300][0/84], lr: 0.00000031 	 loss = 1.3010(1.3010)
2023/10/22 02:03:07 - INFO - root -   Epoch: [36/300][20/84], lr: 0.00000031 	 loss = 1.0559(0.4940)
2023/10/22 02:04:15 - INFO - root -   Epoch: [36/300][40/84], lr: 0.00000031 	 loss = 0.9717(0.5166)
2023/10/22 02:05:14 - INFO - root -   Epoch: [36/300][60/84], lr: 0.00000031 	 loss = 0.1174(0.5305)
2023/10/22 02:06:09 - INFO - root -   Epoch: [36/300][80/84], lr: 0.00000031 	 loss = 0.8167(0.5154)
2023/10/22 02:06:10 - INFO - root -   Epoch: [36/300] 	 loss = 0.5096
2023/10/22 02:06:10 - INFO - root -   train_accuracy = 0.7202
2023/10/22 02:06:41 - INFO - root -   Epoch: [37/300][0/84], lr: 0.00000032 	 loss = 0.5241(0.5241)
2023/10/22 02:07:55 - INFO - root -   Epoch: [37/300][20/84], lr: 0.00000032 	 loss = 0.2828(0.6582)
2023/10/22 02:08:56 - INFO - root -   Epoch: [37/300][40/84], lr: 0.00000032 	 loss = 0.1222(0.6068)
2023/10/22 02:10:01 - INFO - root -   Epoch: [37/300][60/84], lr: 0.00000032 	 loss = 0.1192(0.5970)
2023/10/22 02:10:54 - INFO - root -   Epoch: [37/300][80/84], lr: 0.00000032 	 loss = 0.1277(0.6090)
2023/10/22 02:10:55 - INFO - root -   Epoch: [37/300] 	 loss = 0.5963
2023/10/22 02:10:55 - INFO - root -   train_accuracy = 0.6786
2023/10/22 02:11:17 - INFO - root -   Epoch: [38/300][0/84], lr: 0.00000032 	 loss = 0.4595(0.4595)
2023/10/22 02:12:51 - INFO - root -   Epoch: [38/300][20/84], lr: 0.00000032 	 loss = 0.0967(0.6095)
2023/10/22 02:13:49 - INFO - root -   Epoch: [38/300][40/84], lr: 0.00000032 	 loss = 0.5425(0.5985)
2023/10/22 02:15:02 - INFO - root -   Epoch: [38/300][60/84], lr: 0.00000032 	 loss = 0.3985(0.6067)
2023/10/22 02:15:38 - INFO - root -   Epoch: [38/300][80/84], lr: 0.00000032 	 loss = 1.2510(0.6295)
2023/10/22 02:15:40 - INFO - root -   Epoch: [38/300] 	 loss = 0.6262
2023/10/22 02:15:40 - INFO - root -   train_accuracy = 0.6786
2023/10/22 02:16:01 - INFO - root -   Epoch: [39/300][0/84], lr: 0.00000033 	 loss = 0.6989(0.6989)
2023/10/22 02:17:06 - INFO - root -   Epoch: [39/300][20/84], lr: 0.00000033 	 loss = 0.5162(0.6410)
2023/10/22 02:18:02 - INFO - root -   Epoch: [39/300][40/84], lr: 0.00000033 	 loss = 0.2774(0.6165)
2023/10/22 02:19:08 - INFO - root -   Epoch: [39/300][60/84], lr: 0.00000033 	 loss = 0.3669(0.6590)
2023/10/22 02:20:07 - INFO - root -   Epoch: [39/300][80/84], lr: 0.00000033 	 loss = 0.1516(0.6009)
2023/10/22 02:20:08 - INFO - root -   Epoch: [39/300] 	 loss = 0.5964
2023/10/22 02:21:05 - INFO - root -   precision = 0.8605
2023/10/22 02:21:05 - INFO - root -   eval_loss = 0.4312
2023/10/22 02:21:05 - INFO - root -   eval_acc = 0.8605
2023/10/22 02:21:06 - INFO - root -   train_accuracy = 0.6964
2023/10/22 02:21:36 - INFO - root -   Epoch: [40/300][0/84], lr: 0.00000034 	 loss = 0.7830(0.7830)
2023/10/22 02:22:45 - INFO - root -   Epoch: [40/300][20/84], lr: 0.00000034 	 loss = 0.2865(0.5684)
2023/10/22 02:23:51 - INFO - root -   Epoch: [40/300][40/84], lr: 0.00000034 	 loss = 0.4730(0.5429)
2023/10/22 02:25:10 - INFO - root -   Epoch: [40/300][60/84], lr: 0.00000034 	 loss = 0.1984(0.5429)
2023/10/22 02:25:50 - INFO - root -   Epoch: [40/300][80/84], lr: 0.00000034 	 loss = 0.6262(0.5393)
2023/10/22 02:25:55 - INFO - root -   Epoch: [40/300] 	 loss = 0.5484
2023/10/22 02:25:55 - INFO - root -   train_accuracy = 0.6905
2023/10/22 02:26:16 - INFO - root -   Epoch: [41/300][0/84], lr: 0.00000034 	 loss = 0.6842(0.6842)
2023/10/22 02:27:27 - INFO - root -   Epoch: [41/300][20/84], lr: 0.00000034 	 loss = 1.9469(0.5763)
2023/10/22 02:28:42 - INFO - root -   Epoch: [41/300][40/84], lr: 0.00000034 	 loss = 0.1654(0.5718)
2023/10/22 02:29:39 - INFO - root -   Epoch: [41/300][60/84], lr: 0.00000034 	 loss = 0.7449(0.5466)
2023/10/22 02:30:35 - INFO - root -   Epoch: [41/300][80/84], lr: 0.00000034 	 loss = 0.1030(0.5564)
2023/10/22 02:30:36 - INFO - root -   Epoch: [41/300] 	 loss = 0.5668
2023/10/22 02:30:36 - INFO - root -   train_accuracy = 0.7024
2023/10/22 02:30:57 - INFO - root -   Epoch: [42/300][0/84], lr: 0.00000035 	 loss = 0.0640(0.0640)
2023/10/22 02:31:57 - INFO - root -   Epoch: [42/300][20/84], lr: 0.00000035 	 loss = 0.7006(0.6182)
2023/10/22 02:33:18 - INFO - root -   Epoch: [42/300][40/84], lr: 0.00000035 	 loss = 0.2990(0.5549)
2023/10/22 02:34:22 - INFO - root -   Epoch: [42/300][60/84], lr: 0.00000035 	 loss = 0.5561(0.5368)
2023/10/22 02:35:22 - INFO - root -   Epoch: [42/300][80/84], lr: 0.00000035 	 loss = 0.7931(0.5544)
2023/10/22 02:35:23 - INFO - root -   Epoch: [42/300] 	 loss = 0.5606
2023/10/22 02:35:23 - INFO - root -   train_accuracy = 0.7262
2023/10/22 02:36:00 - INFO - root -   Epoch: [43/300][0/84], lr: 0.00000035 	 loss = 0.5873(0.5873)
2023/10/22 02:37:01 - INFO - root -   Epoch: [43/300][20/84], lr: 0.00000035 	 loss = 0.7224(0.5507)
2023/10/22 02:38:00 - INFO - root -   Epoch: [43/300][40/84], lr: 0.00000035 	 loss = 0.3684(0.6288)
2023/10/22 02:39:06 - INFO - root -   Epoch: [43/300][60/84], lr: 0.00000035 	 loss = 0.0411(0.5547)
2023/10/22 02:39:58 - INFO - root -   Epoch: [43/300][80/84], lr: 0.00000035 	 loss = 0.3399(0.5630)
2023/10/22 02:40:00 - INFO - root -   Epoch: [43/300] 	 loss = 0.5554
2023/10/22 02:40:00 - INFO - root -   train_accuracy = 0.6905
2023/10/22 02:40:21 - INFO - root -   Epoch: [44/300][0/84], lr: 0.00000036 	 loss = 0.4525(0.4525)
2023/10/22 02:41:42 - INFO - root -   Epoch: [44/300][20/84], lr: 0.00000036 	 loss = 0.4052(0.4675)
2023/10/22 02:42:46 - INFO - root -   Epoch: [44/300][40/84], lr: 0.00000036 	 loss = 0.4366(0.5166)
2023/10/22 02:44:05 - INFO - root -   Epoch: [44/300][60/84], lr: 0.00000036 	 loss = 0.2554(0.4852)
2023/10/22 02:44:47 - INFO - root -   Epoch: [44/300][80/84], lr: 0.00000036 	 loss = 0.4001(0.4864)
2023/10/22 02:44:48 - INFO - root -   Epoch: [44/300] 	 loss = 0.4962
2023/10/22 02:45:45 - INFO - root -   precision = 0.8837
2023/10/22 02:45:45 - INFO - root -   eval_loss = 0.3379
2023/10/22 02:45:45 - INFO - root -   eval_acc = 0.8837
2023/10/22 02:45:46 - INFO - root -   train_accuracy = 0.7202
2023/10/22 02:46:08 - INFO - root -   Epoch: [45/300][0/84], lr: 0.00000037 	 loss = 0.7434(0.7434)
2023/10/22 02:47:16 - INFO - root -   Epoch: [45/300][20/84], lr: 0.00000037 	 loss = 0.5093(0.5413)
2023/10/22 02:48:38 - INFO - root -   Epoch: [45/300][40/84], lr: 0.00000037 	 loss = 0.6075(0.5330)
2023/10/22 02:49:32 - INFO - root -   Epoch: [45/300][60/84], lr: 0.00000037 	 loss = 0.1046(0.5584)
2023/10/22 02:50:20 - INFO - root -   Epoch: [45/300][80/84], lr: 0.00000037 	 loss = 0.4368(0.5529)
2023/10/22 02:50:21 - INFO - root -   Epoch: [45/300] 	 loss = 0.5554
2023/10/22 02:50:21 - INFO - root -   train_accuracy = 0.6905
2023/10/22 02:50:52 - INFO - root -   Epoch: [46/300][0/84], lr: 0.00000037 	 loss = 1.0568(1.0568)
2023/10/22 02:52:04 - INFO - root -   Epoch: [46/300][20/84], lr: 0.00000037 	 loss = 0.7665(0.6232)
2023/10/22 02:53:00 - INFO - root -   Epoch: [46/300][40/84], lr: 0.00000037 	 loss = 0.0790(0.5921)
2023/10/22 02:54:16 - INFO - root -   Epoch: [46/300][60/84], lr: 0.00000037 	 loss = 0.4976(0.5802)
2023/10/22 02:54:55 - INFO - root -   Epoch: [46/300][80/84], lr: 0.00000037 	 loss = 0.8342(0.5513)
2023/10/22 02:54:58 - INFO - root -   Epoch: [46/300] 	 loss = 0.5518
2023/10/22 02:54:58 - INFO - root -   train_accuracy = 0.6310
2023/10/22 02:55:20 - INFO - root -   Epoch: [47/300][0/84], lr: 0.00000038 	 loss = 0.6157(0.6157)
2023/10/22 02:56:28 - INFO - root -   Epoch: [47/300][20/84], lr: 0.00000038 	 loss = 0.4697(0.5032)
2023/10/22 02:57:25 - INFO - root -   Epoch: [47/300][40/84], lr: 0.00000038 	 loss = 0.4602(0.5128)
2023/10/22 02:58:29 - INFO - root -   Epoch: [47/300][60/84], lr: 0.00000038 	 loss = 0.2808(0.4930)
2023/10/22 02:59:25 - INFO - root -   Epoch: [47/300][80/84], lr: 0.00000038 	 loss = 0.1017(0.5075)
2023/10/22 02:59:27 - INFO - root -   Epoch: [47/300] 	 loss = 0.5148
2023/10/22 02:59:27 - INFO - root -   train_accuracy = 0.7381
2023/10/22 02:59:58 - INFO - root -   Epoch: [48/300][0/84], lr: 0.00000038 	 loss = 0.2027(0.2027)
2023/10/22 03:01:16 - INFO - root -   Epoch: [48/300][20/84], lr: 0.00000038 	 loss = 0.3390(0.5121)
2023/10/22 03:02:12 - INFO - root -   Epoch: [48/300][40/84], lr: 0.00000038 	 loss = 0.0641(0.5433)
2023/10/22 03:03:25 - INFO - root -   Epoch: [48/300][60/84], lr: 0.00000038 	 loss = 0.1699(0.5441)
2023/10/22 03:04:12 - INFO - root -   Epoch: [48/300][80/84], lr: 0.00000038 	 loss = 0.3615(0.5781)
2023/10/22 03:04:16 - INFO - root -   Epoch: [48/300] 	 loss = 0.5740
2023/10/22 03:04:16 - INFO - root -   train_accuracy = 0.6964
2023/10/22 03:04:37 - INFO - root -   Epoch: [49/300][0/84], lr: 0.00000039 	 loss = 0.3045(0.3045)
2023/10/22 03:05:38 - INFO - root -   Epoch: [49/300][20/84], lr: 0.00000039 	 loss = 1.8426(0.5068)
2023/10/22 03:06:46 - INFO - root -   Epoch: [49/300][40/84], lr: 0.00000039 	 loss = 0.0467(0.4930)
2023/10/22 03:07:42 - INFO - root -   Epoch: [49/300][60/84], lr: 0.00000039 	 loss = 0.8135(0.4805)
2023/10/22 03:08:46 - INFO - root -   Epoch: [49/300][80/84], lr: 0.00000039 	 loss = 0.2271(0.4853)
2023/10/22 03:08:47 - INFO - root -   Epoch: [49/300] 	 loss = 0.4920
2023/10/22 03:09:44 - INFO - root -   precision = 0.8605
2023/10/22 03:09:44 - INFO - root -   eval_loss = 0.3338
2023/10/22 03:09:44 - INFO - root -   eval_acc = 0.8605
2023/10/22 03:09:45 - INFO - root -   train_accuracy = 0.7440
2023/10/22 03:10:06 - INFO - root -   Epoch: [50/300][0/84], lr: 0.00000039 	 loss = 0.5193(0.5193)
2023/10/22 03:11:30 - INFO - root -   Epoch: [50/300][20/84], lr: 0.00000039 	 loss = 2.6187(0.5025)
2023/10/22 03:12:21 - INFO - root -   Epoch: [50/300][40/84], lr: 0.00000039 	 loss = 0.0595(0.4674)
2023/10/22 03:13:29 - INFO - root -   Epoch: [50/300][60/84], lr: 0.00000039 	 loss = 0.1061(0.4813)
2023/10/22 03:14:18 - INFO - root -   Epoch: [50/300][80/84], lr: 0.00000039 	 loss = 0.1327(0.4932)
2023/10/22 03:14:20 - INFO - root -   Epoch: [50/300] 	 loss = 0.4901
2023/10/22 03:14:20 - INFO - root -   train_accuracy = 0.7857
2023/10/22 03:14:49 - INFO - root -   Epoch: [51/300][0/84], lr: 0.00000040 	 loss = 0.1916(0.1916)
2023/10/22 03:16:04 - INFO - root -   Epoch: [51/300][20/84], lr: 0.00000040 	 loss = 1.2694(0.6035)
2023/10/22 03:17:20 - INFO - root -   Epoch: [51/300][40/84], lr: 0.00000040 	 loss = 0.6659(0.5234)
2023/10/22 03:18:19 - INFO - root -   Epoch: [51/300][60/84], lr: 0.00000040 	 loss = 0.1261(0.5450)
2023/10/22 03:19:05 - INFO - root -   Epoch: [51/300][80/84], lr: 0.00000040 	 loss = 0.1095(0.5303)
2023/10/22 03:19:06 - INFO - root -   Epoch: [51/300] 	 loss = 0.5439
2023/10/22 03:19:06 - INFO - root -   train_accuracy = 0.7143
2023/10/22 03:19:36 - INFO - root -   Epoch: [52/300][0/84], lr: 0.00000041 	 loss = 0.1468(0.1468)
2023/10/22 03:20:49 - INFO - root -   Epoch: [52/300][20/84], lr: 0.00000041 	 loss = 0.5366(0.5474)
2023/10/22 03:21:49 - INFO - root -   Epoch: [52/300][40/84], lr: 0.00000041 	 loss = 0.0354(0.4910)
2023/10/22 03:22:53 - INFO - root -   Epoch: [52/300][60/84], lr: 0.00000041 	 loss = 0.4743(0.4747)
2023/10/22 03:23:41 - INFO - root -   Epoch: [52/300][80/84], lr: 0.00000041 	 loss = 1.9879(0.4720)
2023/10/22 03:23:43 - INFO - root -   Epoch: [52/300] 	 loss = 0.4768
2023/10/22 03:23:43 - INFO - root -   train_accuracy = 0.7679
2023/10/22 03:24:04 - INFO - root -   Epoch: [53/300][0/84], lr: 0.00000041 	 loss = 0.1191(0.1191)
2023/10/22 03:25:18 - INFO - root -   Epoch: [53/300][20/84], lr: 0.00000041 	 loss = 1.0676(0.6864)
2023/10/22 03:26:19 - INFO - root -   Epoch: [53/300][40/84], lr: 0.00000041 	 loss = 0.0510(0.6272)
2023/10/22 03:27:25 - INFO - root -   Epoch: [53/300][60/84], lr: 0.00000041 	 loss = 0.0343(0.5861)
2023/10/22 03:28:18 - INFO - root -   Epoch: [53/300][80/84], lr: 0.00000041 	 loss = 0.2769(0.5641)
2023/10/22 03:28:20 - INFO - root -   Epoch: [53/300] 	 loss = 0.5596
2023/10/22 03:28:20 - INFO - root -   train_accuracy = 0.6726
2023/10/22 03:28:49 - INFO - root -   Epoch: [54/300][0/84], lr: 0.00000042 	 loss = 0.3483(0.3483)
2023/10/22 03:30:00 - INFO - root -   Epoch: [54/300][20/84], lr: 0.00000042 	 loss = 0.1605(0.3536)
2023/10/22 03:31:02 - INFO - root -   Epoch: [54/300][40/84], lr: 0.00000042 	 loss = 0.7162(0.4501)
2023/10/22 03:32:07 - INFO - root -   Epoch: [54/300][60/84], lr: 0.00000042 	 loss = 0.3307(0.4640)
2023/10/22 03:33:07 - INFO - root -   Epoch: [54/300][80/84], lr: 0.00000042 	 loss = 0.2471(0.4382)
2023/10/22 03:33:09 - INFO - root -   Epoch: [54/300] 	 loss = 0.4494
2023/10/22 03:34:06 - INFO - root -   precision = 0.8837
2023/10/22 03:34:06 - INFO - root -   eval_loss = 0.3132
2023/10/22 03:34:06 - INFO - root -   eval_acc = 0.8837
2023/10/22 03:34:07 - INFO - root -   train_accuracy = 0.7619
2023/10/22 03:34:37 - INFO - root -   Epoch: [55/300][0/84], lr: 0.00000042 	 loss = 0.3493(0.3493)
2023/10/22 03:35:41 - INFO - root -   Epoch: [55/300][20/84], lr: 0.00000042 	 loss = 0.6496(0.4897)
2023/10/22 03:36:39 - INFO - root -   Epoch: [55/300][40/84], lr: 0.00000042 	 loss = 0.0176(0.4234)
2023/10/22 03:38:02 - INFO - root -   Epoch: [55/300][60/84], lr: 0.00000042 	 loss = 0.3951(0.4513)
2023/10/22 03:38:48 - INFO - root -   Epoch: [55/300][80/84], lr: 0.00000042 	 loss = 0.5373(0.4787)
2023/10/22 03:38:50 - INFO - root -   Epoch: [55/300] 	 loss = 0.4796
2023/10/22 03:38:50 - INFO - root -   train_accuracy = 0.7440
2023/10/22 03:39:35 - INFO - root -   Epoch: [56/300][0/84], lr: 0.00000043 	 loss = 0.3283(0.3283)
2023/10/22 03:40:33 - INFO - root -   Epoch: [56/300][20/84], lr: 0.00000043 	 loss = 0.5999(0.5133)
2023/10/22 03:41:33 - INFO - root -   Epoch: [56/300][40/84], lr: 0.00000043 	 loss = 0.1944(0.5022)
2023/10/22 03:42:40 - INFO - root -   Epoch: [56/300][60/84], lr: 0.00000043 	 loss = 0.2530(0.5316)
2023/10/22 03:43:33 - INFO - root -   Epoch: [56/300][80/84], lr: 0.00000043 	 loss = 0.5438(0.5456)
2023/10/22 03:43:34 - INFO - root -   Epoch: [56/300] 	 loss = 0.5440
2023/10/22 03:43:34 - INFO - root -   train_accuracy = 0.6905
2023/10/22 03:44:14 - INFO - root -   Epoch: [57/300][0/84], lr: 0.00000044 	 loss = 1.3571(1.3571)
2023/10/22 03:45:01 - INFO - root -   Epoch: [57/300][20/84], lr: 0.00000044 	 loss = 0.8595(0.7162)
2023/10/22 03:46:10 - INFO - root -   Epoch: [57/300][40/84], lr: 0.00000044 	 loss = 0.3356(0.5776)
2023/10/22 03:47:16 - INFO - root -   Epoch: [57/300][60/84], lr: 0.00000044 	 loss = 0.1173(0.5151)
2023/10/22 03:48:12 - INFO - root -   Epoch: [57/300][80/84], lr: 0.00000044 	 loss = 0.3665(0.4719)
2023/10/22 03:48:13 - INFO - root -   Epoch: [57/300] 	 loss = 0.4808
2023/10/22 03:48:13 - INFO - root -   train_accuracy = 0.7440
2023/10/22 03:48:43 - INFO - root -   Epoch: [58/300][0/84], lr: 0.00000044 	 loss = 0.4254(0.4254)
2023/10/22 03:49:51 - INFO - root -   Epoch: [58/300][20/84], lr: 0.00000044 	 loss = 0.6258(0.6123)
2023/10/22 03:50:49 - INFO - root -   Epoch: [58/300][40/84], lr: 0.00000044 	 loss = 0.7721(0.5475)
2023/10/22 03:51:49 - INFO - root -   Epoch: [58/300][60/84], lr: 0.00000044 	 loss = 0.1060(0.5437)
2023/10/22 03:52:47 - INFO - root -   Epoch: [58/300][80/84], lr: 0.00000044 	 loss = 0.1975(0.5237)
2023/10/22 03:52:50 - INFO - root -   Epoch: [58/300] 	 loss = 0.5255
2023/10/22 03:52:50 - INFO - root -   train_accuracy = 0.7619
2023/10/22 03:53:20 - INFO - root -   Epoch: [59/300][0/84], lr: 0.00000045 	 loss = 0.5200(0.5200)
2023/10/22 03:54:15 - INFO - root -   Epoch: [59/300][20/84], lr: 0.00000045 	 loss = 0.8622(0.5826)
2023/10/22 03:55:24 - INFO - root -   Epoch: [59/300][40/84], lr: 0.00000045 	 loss = 0.0663(0.4864)
2023/10/22 03:56:37 - INFO - root -   Epoch: [59/300][60/84], lr: 0.00000045 	 loss = 0.1205(0.4853)
2023/10/22 03:57:27 - INFO - root -   Epoch: [59/300][80/84], lr: 0.00000045 	 loss = 0.2484(0.4706)
2023/10/22 03:57:29 - INFO - root -   Epoch: [59/300] 	 loss = 0.4724
2023/10/22 03:58:25 - INFO - root -   precision = 0.8605
2023/10/22 03:58:25 - INFO - root -   eval_loss = 0.2991
2023/10/22 03:58:25 - INFO - root -   eval_acc = 0.8605
2023/10/22 03:58:26 - INFO - root -   train_accuracy = 0.7619
2023/10/22 03:58:48 - INFO - root -   Epoch: [60/300][0/84], lr: 0.00000045 	 loss = 0.1076(0.1076)
2023/10/22 03:59:48 - INFO - root -   Epoch: [60/300][20/84], lr: 0.00000045 	 loss = 0.7383(0.5578)
2023/10/22 04:00:56 - INFO - root -   Epoch: [60/300][40/84], lr: 0.00000045 	 loss = 0.1920(0.5020)
2023/10/22 04:02:02 - INFO - root -   Epoch: [60/300][60/84], lr: 0.00000045 	 loss = 0.0712(0.4553)
2023/10/22 04:02:59 - INFO - root -   Epoch: [60/300][80/84], lr: 0.00000045 	 loss = 0.4851(0.4708)
2023/10/22 04:03:00 - INFO - root -   Epoch: [60/300] 	 loss = 0.4882
2023/10/22 04:03:00 - INFO - root -   train_accuracy = 0.7560
2023/10/22 04:03:29 - INFO - root -   Epoch: [61/300][0/84], lr: 0.00000046 	 loss = 1.1788(1.1788)
2023/10/22 04:04:28 - INFO - root -   Epoch: [61/300][20/84], lr: 0.00000046 	 loss = 2.0287(0.6625)
2023/10/22 04:05:56 - INFO - root -   Epoch: [61/300][40/84], lr: 0.00000046 	 loss = 0.0402(0.6010)
2023/10/22 04:06:52 - INFO - root -   Epoch: [61/300][60/84], lr: 0.00000046 	 loss = 0.2562(0.5941)
2023/10/22 04:07:39 - INFO - root -   Epoch: [61/300][80/84], lr: 0.00000046 	 loss = 0.0858(0.5652)
2023/10/22 04:07:40 - INFO - root -   Epoch: [61/300] 	 loss = 0.5597
2023/10/22 04:07:40 - INFO - root -   train_accuracy = 0.6905
2023/10/22 04:08:02 - INFO - root -   Epoch: [62/300][0/84], lr: 0.00000047 	 loss = 0.1650(0.1650)
2023/10/22 04:09:17 - INFO - root -   Epoch: [62/300][20/84], lr: 0.00000047 	 loss = 1.0941(0.5567)
2023/10/22 04:10:07 - INFO - root -   Epoch: [62/300][40/84], lr: 0.00000047 	 loss = 0.0879(0.5409)
2023/10/22 04:11:22 - INFO - root -   Epoch: [62/300][60/84], lr: 0.00000047 	 loss = 0.0500(0.4858)
2023/10/22 04:12:07 - INFO - root -   Epoch: [62/300][80/84], lr: 0.00000047 	 loss = 0.9865(0.5068)
2023/10/22 04:12:12 - INFO - root -   Epoch: [62/300] 	 loss = 0.5032
2023/10/22 04:12:12 - INFO - root -   train_accuracy = 0.7321
2023/10/22 04:12:35 - INFO - root -   Epoch: [63/300][0/84], lr: 0.00000047 	 loss = 0.1011(0.1011)
2023/10/22 04:13:40 - INFO - root -   Epoch: [63/300][20/84], lr: 0.00000047 	 loss = 0.3224(0.3532)
2023/10/22 04:14:40 - INFO - root -   Epoch: [63/300][40/84], lr: 0.00000047 	 loss = 0.2966(0.4000)
2023/10/22 04:16:08 - INFO - root -   Epoch: [63/300][60/84], lr: 0.00000047 	 loss = 0.3159(0.4238)
2023/10/22 04:16:48 - INFO - root -   Epoch: [63/300][80/84], lr: 0.00000047 	 loss = 0.2494(0.4195)
2023/10/22 04:16:50 - INFO - root -   Epoch: [63/300] 	 loss = 0.4136
2023/10/22 04:16:50 - INFO - root -   train_accuracy = 0.7976
2023/10/22 04:17:19 - INFO - root -   Epoch: [64/300][0/84], lr: 0.00000048 	 loss = 0.7967(0.7967)
2023/10/22 04:18:18 - INFO - root -   Epoch: [64/300][20/84], lr: 0.00000048 	 loss = 1.2215(0.6162)
2023/10/22 04:19:50 - INFO - root -   Epoch: [64/300][40/84], lr: 0.00000048 	 loss = 0.3336(0.4774)
2023/10/22 04:20:43 - INFO - root -   Epoch: [64/300][60/84], lr: 0.00000048 	 loss = 0.0749(0.5088)
2023/10/22 04:21:36 - INFO - root -   Epoch: [64/300][80/84], lr: 0.00000048 	 loss = 0.5828(0.5195)
2023/10/22 04:21:38 - INFO - root -   Epoch: [64/300] 	 loss = 0.5200
2023/10/22 04:22:35 - INFO - root -   precision = 0.8140
2023/10/22 04:22:35 - INFO - root -   eval_loss = 0.3917
2023/10/22 04:22:35 - INFO - root -   eval_acc = 0.8140
2023/10/22 04:22:36 - INFO - root -   train_accuracy = 0.7143
2023/10/22 04:23:07 - INFO - root -   Epoch: [65/300][0/84], lr: 0.00000048 	 loss = 0.2030(0.2030)
2023/10/22 04:24:10 - INFO - root -   Epoch: [65/300][20/84], lr: 0.00000048 	 loss = 1.4445(0.5548)
2023/10/22 04:25:24 - INFO - root -   Epoch: [65/300][40/84], lr: 0.00000048 	 loss = 0.0395(0.4993)
2023/10/22 04:26:21 - INFO - root -   Epoch: [65/300][60/84], lr: 0.00000048 	 loss = 0.0560(0.4894)
2023/10/22 04:27:09 - INFO - root -   Epoch: [65/300][80/84], lr: 0.00000048 	 loss = 0.7734(0.4904)
2023/10/22 04:27:11 - INFO - root -   Epoch: [65/300] 	 loss = 0.5007
2023/10/22 04:27:11 - INFO - root -   train_accuracy = 0.7619
2023/10/22 04:27:41 - INFO - root -   Epoch: [66/300][0/84], lr: 0.00000049 	 loss = 0.4932(0.4932)
2023/10/22 04:28:46 - INFO - root -   Epoch: [66/300][20/84], lr: 0.00000049 	 loss = 0.6232(0.5987)
2023/10/22 04:30:03 - INFO - root -   Epoch: [66/300][40/84], lr: 0.00000049 	 loss = 0.4209(0.5699)
2023/10/22 04:30:57 - INFO - root -   Epoch: [66/300][60/84], lr: 0.00000049 	 loss = 0.0629(0.5204)
2023/10/22 04:31:50 - INFO - root -   Epoch: [66/300][80/84], lr: 0.00000049 	 loss = 0.2286(0.4919)
2023/10/22 04:31:51 - INFO - root -   Epoch: [66/300] 	 loss = 0.4971
2023/10/22 04:31:51 - INFO - root -   train_accuracy = 0.7560
2023/10/22 04:32:21 - INFO - root -   Epoch: [67/300][0/84], lr: 0.00000049 	 loss = 0.7163(0.7163)
2023/10/22 04:33:35 - INFO - root -   Epoch: [67/300][20/84], lr: 0.00000049 	 loss = 0.1253(0.4822)
2023/10/22 04:34:42 - INFO - root -   Epoch: [67/300][40/84], lr: 0.00000049 	 loss = 0.2016(0.4408)
2023/10/22 04:35:41 - INFO - root -   Epoch: [67/300][60/84], lr: 0.00000049 	 loss = 0.2368(0.4601)
2023/10/22 04:36:29 - INFO - root -   Epoch: [67/300][80/84], lr: 0.00000049 	 loss = 0.5700(0.4645)
2023/10/22 04:36:30 - INFO - root -   Epoch: [67/300] 	 loss = 0.4738
2023/10/22 04:36:30 - INFO - root -   train_accuracy = 0.7857
2023/10/22 04:36:52 - INFO - root -   Epoch: [68/300][0/84], lr: 0.00000050 	 loss = 0.1583(0.1583)
2023/10/22 04:37:57 - INFO - root -   Epoch: [68/300][20/84], lr: 0.00000050 	 loss = 0.7473(0.5316)
2023/10/22 04:38:58 - INFO - root -   Epoch: [68/300][40/84], lr: 0.00000050 	 loss = 0.2246(0.4910)
2023/10/22 04:40:25 - INFO - root -   Epoch: [68/300][60/84], lr: 0.00000050 	 loss = 0.0485(0.4677)
2023/10/22 04:41:08 - INFO - root -   Epoch: [68/300][80/84], lr: 0.00000050 	 loss = 0.3508(0.5031)
2023/10/22 04:41:13 - INFO - root -   Epoch: [68/300] 	 loss = 0.4999
2023/10/22 04:41:13 - INFO - root -   train_accuracy = 0.7321
2023/10/22 04:41:34 - INFO - root -   Epoch: [69/300][0/84], lr: 0.00000051 	 loss = 0.4858(0.4858)
2023/10/22 04:42:43 - INFO - root -   Epoch: [69/300][20/84], lr: 0.00000051 	 loss = 0.3630(0.5173)
2023/10/22 04:43:35 - INFO - root -   Epoch: [69/300][40/84], lr: 0.00000051 	 loss = 0.0150(0.4718)
2023/10/22 04:44:58 - INFO - root -   Epoch: [69/300][60/84], lr: 0.00000051 	 loss = 0.1619(0.4765)
2023/10/22 04:45:50 - INFO - root -   Epoch: [69/300][80/84], lr: 0.00000051 	 loss = 0.0389(0.4726)
2023/10/22 04:45:51 - INFO - root -   Epoch: [69/300] 	 loss = 0.4721
2023/10/22 04:46:48 - INFO - root -   precision = 0.8372
2023/10/22 04:46:48 - INFO - root -   eval_loss = 0.2836
2023/10/22 04:46:48 - INFO - root -   eval_acc = 0.8372
2023/10/22 04:46:49 - INFO - root -   train_accuracy = 0.7202
2023/10/22 04:47:11 - INFO - root -   Epoch: [70/300][0/84], lr: 0.00000051 	 loss = 0.0657(0.0657)
2023/10/22 04:48:09 - INFO - root -   Epoch: [70/300][20/84], lr: 0.00000051 	 loss = 0.9962(0.4082)
2023/10/22 04:49:23 - INFO - root -   Epoch: [70/300][40/84], lr: 0.00000051 	 loss = 0.2124(0.4463)
2023/10/22 04:50:55 - INFO - root -   Epoch: [70/300][60/84], lr: 0.00000051 	 loss = 0.7008(0.4689)
2023/10/22 04:51:29 - INFO - root -   Epoch: [70/300][80/84], lr: 0.00000051 	 loss = 0.6610(0.4775)
2023/10/22 04:51:32 - INFO - root -   Epoch: [70/300] 	 loss = 0.4840
2023/10/22 04:51:32 - INFO - root -   train_accuracy = 0.7202
2023/10/22 04:52:10 - INFO - root -   Epoch: [71/300][0/84], lr: 0.00000052 	 loss = 0.5258(0.5258)
2023/10/22 04:53:10 - INFO - root -   Epoch: [71/300][20/84], lr: 0.00000052 	 loss = 0.8817(0.4316)
2023/10/22 04:54:29 - INFO - root -   Epoch: [71/300][40/84], lr: 0.00000052 	 loss = 0.9339(0.4542)
2023/10/22 04:55:40 - INFO - root -   Epoch: [71/300][60/84], lr: 0.00000052 	 loss = 0.2482(0.5158)
2023/10/22 04:56:21 - INFO - root -   Epoch: [71/300][80/84], lr: 0.00000052 	 loss = 0.0877(0.5183)
2023/10/22 04:56:25 - INFO - root -   Epoch: [71/300] 	 loss = 0.5192
2023/10/22 04:56:25 - INFO - root -   train_accuracy = 0.7262
2023/10/22 04:56:46 - INFO - root -   Epoch: [72/300][0/84], lr: 0.00000052 	 loss = 0.4037(0.4037)
2023/10/22 04:57:55 - INFO - root -   Epoch: [72/300][20/84], lr: 0.00000052 	 loss = 1.1510(0.5093)
2023/10/22 04:59:04 - INFO - root -   Epoch: [72/300][40/84], lr: 0.00000052 	 loss = 0.0144(0.5115)
2023/10/22 05:00:09 - INFO - root -   Epoch: [72/300][60/84], lr: 0.00000052 	 loss = 0.0622(0.5202)
2023/10/22 05:01:08 - INFO - root -   Epoch: [72/300][80/84], lr: 0.00000052 	 loss = 0.2264(0.4759)
2023/10/22 05:01:09 - INFO - root -   Epoch: [72/300] 	 loss = 0.4851
2023/10/22 05:01:09 - INFO - root -   train_accuracy = 0.7321
2023/10/22 05:01:39 - INFO - root -   Epoch: [73/300][0/84], lr: 0.00000053 	 loss = 0.5276(0.5276)
2023/10/22 05:02:49 - INFO - root -   Epoch: [73/300][20/84], lr: 0.00000053 	 loss = 0.2130(0.4446)
2023/10/22 05:03:47 - INFO - root -   Epoch: [73/300][40/84], lr: 0.00000053 	 loss = 0.1541(0.4555)
2023/10/22 05:04:52 - INFO - root -   Epoch: [73/300][60/84], lr: 0.00000053 	 loss = 0.0381(0.4493)
2023/10/22 05:05:47 - INFO - root -   Epoch: [73/300][80/84], lr: 0.00000053 	 loss = 0.1640(0.4460)
2023/10/22 05:05:48 - INFO - root -   Epoch: [73/300] 	 loss = 0.4462
2023/10/22 05:05:48 - INFO - root -   train_accuracy = 0.7500
2023/10/22 05:06:19 - INFO - root -   Epoch: [74/300][0/84], lr: 0.00000054 	 loss = 0.2215(0.2215)
2023/10/22 05:07:18 - INFO - root -   Epoch: [74/300][20/84], lr: 0.00000054 	 loss = 0.9155(0.4146)
2023/10/22 05:08:25 - INFO - root -   Epoch: [74/300][40/84], lr: 0.00000054 	 loss = 0.6249(0.4132)
2023/10/22 05:09:49 - INFO - root -   Epoch: [74/300][60/84], lr: 0.00000054 	 loss = 0.0860(0.4136)
2023/10/22 05:10:26 - INFO - root -   Epoch: [74/300][80/84], lr: 0.00000054 	 loss = 0.2037(0.4297)
2023/10/22 05:10:30 - INFO - root -   Epoch: [74/300] 	 loss = 0.4281
2023/10/22 05:11:28 - INFO - root -   precision = 0.8372
2023/10/22 05:11:28 - INFO - root -   eval_loss = 0.3446
2023/10/22 05:11:28 - INFO - root -   eval_acc = 0.8372
2023/10/22 05:11:29 - INFO - root -   train_accuracy = 0.7976
2023/10/22 05:12:05 - INFO - root -   Epoch: [75/300][0/84], lr: 0.00000054 	 loss = 0.4056(0.4056)
2023/10/22 05:13:06 - INFO - root -   Epoch: [75/300][20/84], lr: 0.00000054 	 loss = 0.4287(0.4298)
2023/10/22 05:14:29 - INFO - root -   Epoch: [75/300][40/84], lr: 0.00000054 	 loss = 0.7342(0.4273)
2023/10/22 05:15:33 - INFO - root -   Epoch: [75/300][60/84], lr: 0.00000054 	 loss = 0.6441(0.4224)
2023/10/22 05:16:09 - INFO - root -   Epoch: [75/300][80/84], lr: 0.00000054 	 loss = 0.1234(0.4388)
2023/10/22 05:16:11 - INFO - root -   Epoch: [75/300] 	 loss = 0.4323
2023/10/22 05:16:11 - INFO - root -   train_accuracy = 0.7798
2023/10/22 05:16:32 - INFO - root -   Epoch: [76/300][0/84], lr: 0.00000055 	 loss = 0.3046(0.3046)
2023/10/22 05:17:39 - INFO - root -   Epoch: [76/300][20/84], lr: 0.00000055 	 loss = 1.1284(0.5134)
2023/10/22 05:18:40 - INFO - root -   Epoch: [76/300][40/84], lr: 0.00000055 	 loss = 0.7445(0.5957)
2023/10/22 05:19:55 - INFO - root -   Epoch: [76/300][60/84], lr: 0.00000055 	 loss = 0.2488(0.5096)
2023/10/22 05:20:48 - INFO - root -   Epoch: [76/300][80/84], lr: 0.00000055 	 loss = 0.5538(0.5391)
2023/10/22 05:20:49 - INFO - root -   Epoch: [76/300] 	 loss = 0.5367
2023/10/22 05:20:49 - INFO - root -   train_accuracy = 0.7262
2023/10/22 05:21:10 - INFO - root -   Epoch: [77/300][0/84], lr: 0.00000055 	 loss = 0.1309(0.1309)
2023/10/22 05:22:33 - INFO - root -   Epoch: [77/300][20/84], lr: 0.00000055 	 loss = 0.9834(0.5216)
2023/10/22 05:23:41 - INFO - root -   Epoch: [77/300][40/84], lr: 0.00000055 	 loss = 0.3981(0.5083)
2023/10/22 05:24:47 - INFO - root -   Epoch: [77/300][60/84], lr: 0.00000055 	 loss = 0.3432(0.4675)
2023/10/22 05:25:25 - INFO - root -   Epoch: [77/300][80/84], lr: 0.00000055 	 loss = 0.2007(0.5106)
2023/10/22 05:25:26 - INFO - root -   Epoch: [77/300] 	 loss = 0.5121
2023/10/22 05:25:26 - INFO - root -   train_accuracy = 0.7202
2023/10/22 05:25:48 - INFO - root -   Epoch: [78/300][0/84], lr: 0.00000056 	 loss = 0.2604(0.2604)
2023/10/22 05:27:01 - INFO - root -   Epoch: [78/300][20/84], lr: 0.00000056 	 loss = 1.7708(0.5456)
2023/10/22 05:28:04 - INFO - root -   Epoch: [78/300][40/84], lr: 0.00000056 	 loss = 0.3121(0.4816)
2023/10/22 05:29:25 - INFO - root -   Epoch: [78/300][60/84], lr: 0.00000056 	 loss = 0.7311(0.4909)
2023/10/22 05:30:02 - INFO - root -   Epoch: [78/300][80/84], lr: 0.00000056 	 loss = 1.1647(0.4602)
2023/10/22 05:30:04 - INFO - root -   Epoch: [78/300] 	 loss = 0.4725
2023/10/22 05:30:04 - INFO - root -   train_accuracy = 0.7619
2023/10/22 05:30:34 - INFO - root -   Epoch: [79/300][0/84], lr: 0.00000057 	 loss = 0.1981(0.1981)
2023/10/22 05:31:33 - INFO - root -   Epoch: [79/300][20/84], lr: 0.00000057 	 loss = 2.2926(0.6873)
2023/10/22 05:32:45 - INFO - root -   Epoch: [79/300][40/84], lr: 0.00000057 	 loss = 0.4217(0.6609)
2023/10/22 05:33:45 - INFO - root -   Epoch: [79/300][60/84], lr: 0.00000057 	 loss = 0.6852(0.5942)
2023/10/22 05:34:47 - INFO - root -   Epoch: [79/300][80/84], lr: 0.00000057 	 loss = 0.3843(0.5458)
2023/10/22 05:34:49 - INFO - root -   Epoch: [79/300] 	 loss = 0.5349
2023/10/22 05:35:46 - INFO - root -   precision = 0.8837
2023/10/22 05:35:46 - INFO - root -   eval_loss = 0.2904
2023/10/22 05:35:46 - INFO - root -   eval_acc = 0.8837
2023/10/22 05:35:47 - INFO - root -   train_accuracy = 0.7202
2023/10/22 05:36:08 - INFO - root -   Epoch: [80/300][0/84], lr: 0.00000057 	 loss = 0.1371(0.1371)
2023/10/22 05:37:25 - INFO - root -   Epoch: [80/300][20/84], lr: 0.00000057 	 loss = 0.6972(0.5404)
2023/10/22 05:38:24 - INFO - root -   Epoch: [80/300][40/84], lr: 0.00000057 	 loss = 0.2816(0.4939)
2023/10/22 05:39:41 - INFO - root -   Epoch: [80/300][60/84], lr: 0.00000057 	 loss = 0.1332(0.4717)
2023/10/22 05:40:16 - INFO - root -   Epoch: [80/300][80/84], lr: 0.00000057 	 loss = 0.5895(0.4587)
2023/10/22 05:40:20 - INFO - root -   Epoch: [80/300] 	 loss = 0.4482
2023/10/22 05:40:20 - INFO - root -   train_accuracy = 0.7798
2023/10/22 05:40:42 - INFO - root -   Epoch: [81/300][0/84], lr: 0.00000058 	 loss = 0.2497(0.2497)
2023/10/22 05:41:57 - INFO - root -   Epoch: [81/300][20/84], lr: 0.00000058 	 loss = 0.8996(0.6370)
2023/10/22 05:42:54 - INFO - root -   Epoch: [81/300][40/84], lr: 0.00000058 	 loss = 0.0472(0.4544)
2023/10/22 05:43:48 - INFO - root -   Epoch: [81/300][60/84], lr: 0.00000058 	 loss = 0.1572(0.4440)
2023/10/22 05:44:46 - INFO - root -   Epoch: [81/300][80/84], lr: 0.00000058 	 loss = 0.0894(0.4712)
2023/10/22 05:44:47 - INFO - root -   Epoch: [81/300] 	 loss = 0.4729
2023/10/22 05:44:47 - INFO - root -   train_accuracy = 0.7619
2023/10/22 05:45:17 - INFO - root -   Epoch: [82/300][0/84], lr: 0.00000058 	 loss = 0.5346(0.5346)
2023/10/22 05:46:16 - INFO - root -   Epoch: [82/300][20/84], lr: 0.00000058 	 loss = 1.3794(0.5850)
2023/10/22 05:47:16 - INFO - root -   Epoch: [82/300][40/84], lr: 0.00000058 	 loss = 0.1042(0.5069)
2023/10/22 05:48:31 - INFO - root -   Epoch: [82/300][60/84], lr: 0.00000058 	 loss = 0.4566(0.4785)
2023/10/22 05:49:20 - INFO - root -   Epoch: [82/300][80/84], lr: 0.00000058 	 loss = 0.5520(0.5039)
2023/10/22 05:49:27 - INFO - root -   Epoch: [82/300] 	 loss = 0.5053
2023/10/22 05:49:27 - INFO - root -   train_accuracy = 0.7202
2023/10/22 05:49:49 - INFO - root -   Epoch: [83/300][0/84], lr: 0.00000059 	 loss = 0.1161(0.1161)
2023/10/22 05:51:11 - INFO - root -   Epoch: [83/300][20/84], lr: 0.00000059 	 loss = 0.8581(0.5795)
2023/10/22 05:52:23 - INFO - root -   Epoch: [83/300][40/84], lr: 0.00000059 	 loss = 0.2333(0.5629)
2023/10/22 05:53:22 - INFO - root -   Epoch: [83/300][60/84], lr: 0.00000059 	 loss = 0.6308(0.5178)
2023/10/22 05:54:07 - INFO - root -   Epoch: [83/300][80/84], lr: 0.00000059 	 loss = 0.3306(0.5541)
2023/10/22 05:54:08 - INFO - root -   Epoch: [83/300] 	 loss = 0.5469
2023/10/22 05:54:08 - INFO - root -   train_accuracy = 0.7024
2023/10/22 05:54:30 - INFO - root -   Epoch: [84/300][0/84], lr: 0.00000060 	 loss = 0.6052(0.6052)
2023/10/22 05:55:39 - INFO - root -   Epoch: [84/300][20/84], lr: 0.00000060 	 loss = 1.0110(0.5674)
2023/10/22 05:56:33 - INFO - root -   Epoch: [84/300][40/84], lr: 0.00000060 	 loss = 0.0384(0.4527)
2023/10/22 05:57:44 - INFO - root -   Epoch: [84/300][60/84], lr: 0.00000060 	 loss = 0.1993(0.4360)
2023/10/22 05:58:36 - INFO - root -   Epoch: [84/300][80/84], lr: 0.00000060 	 loss = 0.6286(0.4571)
2023/10/22 05:58:37 - INFO - root -   Epoch: [84/300] 	 loss = 0.4493
2023/10/22 05:59:34 - INFO - root -   precision = 0.8837
2023/10/22 05:59:34 - INFO - root -   eval_loss = 0.3156
2023/10/22 05:59:34 - INFO - root -   eval_acc = 0.8837
2023/10/22 05:59:35 - INFO - root -   train_accuracy = 0.8095
2023/10/22 05:59:57 - INFO - root -   Epoch: [85/300][0/84], lr: 0.00000060 	 loss = 0.3143(0.3143)
2023/10/22 06:01:08 - INFO - root -   Epoch: [85/300][20/84], lr: 0.00000060 	 loss = 1.1006(0.5311)
2023/10/22 06:02:05 - INFO - root -   Epoch: [85/300][40/84], lr: 0.00000060 	 loss = 0.0480(0.4670)
2023/10/22 06:03:08 - INFO - root -   Epoch: [85/300][60/84], lr: 0.00000060 	 loss = 0.0217(0.4408)
2023/10/22 06:04:06 - INFO - root -   Epoch: [85/300][80/84], lr: 0.00000060 	 loss = 0.0898(0.4588)
2023/10/22 06:04:09 - INFO - root -   Epoch: [85/300] 	 loss = 0.4685
2023/10/22 06:04:09 - INFO - root -   train_accuracy = 0.7619
2023/10/22 06:04:31 - INFO - root -   Epoch: [86/300][0/84], lr: 0.00000061 	 loss = 0.1147(0.1147)
2023/10/22 06:05:36 - INFO - root -   Epoch: [86/300][20/84], lr: 0.00000061 	 loss = 1.5264(0.4763)
2023/10/22 06:06:43 - INFO - root -   Epoch: [86/300][40/84], lr: 0.00000061 	 loss = 0.1471(0.4421)
2023/10/22 06:08:02 - INFO - root -   Epoch: [86/300][60/84], lr: 0.00000061 	 loss = 0.3285(0.4701)
2023/10/22 06:08:51 - INFO - root -   Epoch: [86/300][80/84], lr: 0.00000061 	 loss = 0.5754(0.4407)
2023/10/22 06:08:52 - INFO - root -   Epoch: [86/300] 	 loss = 0.4354
2023/10/22 06:08:52 - INFO - root -   train_accuracy = 0.7798
2023/10/22 06:09:14 - INFO - root -   Epoch: [87/300][0/84], lr: 0.00000061 	 loss = 0.1911(0.1911)
2023/10/22 06:10:21 - INFO - root -   Epoch: [87/300][20/84], lr: 0.00000061 	 loss = 0.4194(0.5499)
2023/10/22 06:11:40 - INFO - root -   Epoch: [87/300][40/84], lr: 0.00000061 	 loss = 0.1255(0.4431)
2023/10/22 06:12:30 - INFO - root -   Epoch: [87/300][60/84], lr: 0.00000061 	 loss = 0.2102(0.4799)
2023/10/22 06:13:25 - INFO - root -   Epoch: [87/300][80/84], lr: 0.00000061 	 loss = 0.0960(0.4715)
2023/10/22 06:13:30 - INFO - root -   Epoch: [87/300] 	 loss = 0.4752
2023/10/22 06:13:30 - INFO - root -   train_accuracy = 0.7738
2023/10/22 06:13:52 - INFO - root -   Epoch: [88/300][0/84], lr: 0.00000062 	 loss = 0.0840(0.0840)
2023/10/22 06:15:00 - INFO - root -   Epoch: [88/300][20/84], lr: 0.00000062 	 loss = 0.5005(0.5284)
2023/10/22 06:16:07 - INFO - root -   Epoch: [88/300][40/84], lr: 0.00000062 	 loss = 0.1574(0.4628)
2023/10/22 06:17:13 - INFO - root -   Epoch: [88/300][60/84], lr: 0.00000062 	 loss = 0.5443(0.4970)
2023/10/22 06:18:04 - INFO - root -   Epoch: [88/300][80/84], lr: 0.00000062 	 loss = 0.1413(0.4998)
2023/10/22 06:18:09 - INFO - root -   Epoch: [88/300] 	 loss = 0.4935
2023/10/22 06:18:09 - INFO - root -   train_accuracy = 0.7143
2023/10/22 06:18:39 - INFO - root -   Epoch: [89/300][0/84], lr: 0.00000062 	 loss = 0.2374(0.2374)
2023/10/22 06:19:45 - INFO - root -   Epoch: [89/300][20/84], lr: 0.00000062 	 loss = 0.4716(0.5314)
2023/10/22 06:21:23 - INFO - root -   Epoch: [89/300][40/84], lr: 0.00000062 	 loss = 0.1928(0.5253)
2023/10/22 06:22:07 - INFO - root -   Epoch: [89/300][60/84], lr: 0.00000062 	 loss = 0.0537(0.4849)
2023/10/22 06:23:02 - INFO - root -   Epoch: [89/300][80/84], lr: 0.00000062 	 loss = 0.0657(0.5140)
2023/10/22 06:23:04 - INFO - root -   Epoch: [89/300] 	 loss = 0.5014
2023/10/22 06:24:01 - INFO - root -   precision = 0.9070
2023/10/22 06:24:01 - INFO - root -   eval_loss = 0.2818
2023/10/22 06:24:01 - INFO - root -   eval_acc = 0.9070
2023/10/22 06:24:02 - INFO - root -   train_accuracy = 0.7262
2023/10/22 06:24:32 - INFO - root -   Epoch: [90/300][0/84], lr: 0.00000063 	 loss = 0.1487(0.1487)
2023/10/22 06:25:39 - INFO - root -   Epoch: [90/300][20/84], lr: 0.00000063 	 loss = 0.9103(0.4859)
2023/10/22 06:26:56 - INFO - root -   Epoch: [90/300][40/84], lr: 0.00000063 	 loss = 0.2398(0.4949)
2023/10/22 06:27:54 - INFO - root -   Epoch: [90/300][60/84], lr: 0.00000063 	 loss = 0.2468(0.5025)
2023/10/22 06:28:50 - INFO - root -   Epoch: [90/300][80/84], lr: 0.00000063 	 loss = 0.6906(0.5031)
2023/10/22 06:28:52 - INFO - root -   Epoch: [90/300] 	 loss = 0.4989
2023/10/22 06:28:52 - INFO - root -   train_accuracy = 0.7619
2023/10/22 06:29:13 - INFO - root -   Epoch: [91/300][0/84], lr: 0.00000064 	 loss = 0.0578(0.0578)
2023/10/22 06:30:13 - INFO - root -   Epoch: [91/300][20/84], lr: 0.00000064 	 loss = 0.7962(0.4065)
2023/10/22 06:31:23 - INFO - root -   Epoch: [91/300][40/84], lr: 0.00000064 	 loss = 0.1207(0.4486)
2023/10/22 06:32:27 - INFO - root -   Epoch: [91/300][60/84], lr: 0.00000064 	 loss = 0.3331(0.4285)
2023/10/22 06:33:17 - INFO - root -   Epoch: [91/300][80/84], lr: 0.00000064 	 loss = 0.8452(0.4293)
2023/10/22 06:33:21 - INFO - root -   Epoch: [91/300] 	 loss = 0.4230
2023/10/22 06:33:21 - INFO - root -   train_accuracy = 0.7738
2023/10/22 06:33:43 - INFO - root -   Epoch: [92/300][0/84], lr: 0.00000064 	 loss = 0.0416(0.0416)
2023/10/22 06:34:55 - INFO - root -   Epoch: [92/300][20/84], lr: 0.00000064 	 loss = 0.1681(0.5197)
2023/10/22 06:35:54 - INFO - root -   Epoch: [92/300][40/84], lr: 0.00000064 	 loss = 0.0485(0.5528)
2023/10/22 06:37:01 - INFO - root -   Epoch: [92/300][60/84], lr: 0.00000064 	 loss = 0.0617(0.5320)
2023/10/22 06:37:55 - INFO - root -   Epoch: [92/300][80/84], lr: 0.00000064 	 loss = 0.0484(0.5323)
2023/10/22 06:37:59 - INFO - root -   Epoch: [92/300] 	 loss = 0.5250
2023/10/22 06:37:59 - INFO - root -   train_accuracy = 0.7143
2023/10/22 06:38:30 - INFO - root -   Epoch: [93/300][0/84], lr: 0.00000065 	 loss = 0.1649(0.1649)
2023/10/22 06:39:30 - INFO - root -   Epoch: [93/300][20/84], lr: 0.00000065 	 loss = 0.3743(0.4374)
2023/10/22 06:40:46 - INFO - root -   Epoch: [93/300][40/84], lr: 0.00000065 	 loss = 0.1310(0.4170)
2023/10/22 06:41:42 - INFO - root -   Epoch: [93/300][60/84], lr: 0.00000065 	 loss = 0.2929(0.4038)
2023/10/22 06:42:27 - INFO - root -   Epoch: [93/300][80/84], lr: 0.00000065 	 loss = 0.2312(0.4147)
2023/10/22 06:42:31 - INFO - root -   Epoch: [93/300] 	 loss = 0.4130
2023/10/22 06:42:31 - INFO - root -   train_accuracy = 0.7857
2023/10/22 06:43:00 - INFO - root -   Epoch: [94/300][0/84], lr: 0.00000065 	 loss = 0.6539(0.6539)
2023/10/22 06:44:07 - INFO - root -   Epoch: [94/300][20/84], lr: 0.00000065 	 loss = 0.6813(0.5341)
2023/10/22 06:45:13 - INFO - root -   Epoch: [94/300][40/84], lr: 0.00000065 	 loss = 0.0651(0.4960)
2023/10/22 06:46:32 - INFO - root -   Epoch: [94/300][60/84], lr: 0.00000065 	 loss = 0.4687(0.5152)
2023/10/22 06:47:13 - INFO - root -   Epoch: [94/300][80/84], lr: 0.00000065 	 loss = 0.3530(0.5169)
2023/10/22 06:47:19 - INFO - root -   Epoch: [94/300] 	 loss = 0.5231
2023/10/22 06:48:15 - INFO - root -   precision = 0.8837
2023/10/22 06:48:15 - INFO - root -   eval_loss = 0.3025
2023/10/22 06:48:15 - INFO - root -   eval_acc = 0.8837
2023/10/22 06:48:16 - INFO - root -   train_accuracy = 0.7143
2023/10/22 06:48:37 - INFO - root -   Epoch: [95/300][0/84], lr: 0.00000066 	 loss = 0.0639(0.0639)
2023/10/22 06:49:44 - INFO - root -   Epoch: [95/300][20/84], lr: 0.00000066 	 loss = 0.5727(0.5947)
2023/10/22 06:50:34 - INFO - root -   Epoch: [95/300][40/84], lr: 0.00000066 	 loss = 0.1859(0.4675)
2023/10/22 06:51:52 - INFO - root -   Epoch: [95/300][60/84], lr: 0.00000066 	 loss = 0.0878(0.4645)
2023/10/22 06:52:43 - INFO - root -   Epoch: [95/300][80/84], lr: 0.00000066 	 loss = 0.4666(0.4659)
2023/10/22 06:52:44 - INFO - root -   Epoch: [95/300] 	 loss = 0.4521
2023/10/22 06:52:44 - INFO - root -   train_accuracy = 0.7738
2023/10/22 06:53:06 - INFO - root -   Epoch: [96/300][0/84], lr: 0.00000067 	 loss = 0.0766(0.0766)
2023/10/22 06:54:11 - INFO - root -   Epoch: [96/300][20/84], lr: 0.00000067 	 loss = 0.3262(0.3681)
2023/10/22 06:55:21 - INFO - root -   Epoch: [96/300][40/84], lr: 0.00000067 	 loss = 0.3919(0.3965)
2023/10/22 06:56:20 - INFO - root -   Epoch: [96/300][60/84], lr: 0.00000067 	 loss = 0.0618(0.3915)
2023/10/22 06:57:13 - INFO - root -   Epoch: [96/300][80/84], lr: 0.00000067 	 loss = 0.6480(0.4218)
2023/10/22 06:57:15 - INFO - root -   Epoch: [96/300] 	 loss = 0.4142
2023/10/22 06:57:15 - INFO - root -   train_accuracy = 0.8095
2023/10/22 06:57:45 - INFO - root -   Epoch: [97/300][0/84], lr: 0.00000067 	 loss = 0.1901(0.1901)
2023/10/22 06:58:49 - INFO - root -   Epoch: [97/300][20/84], lr: 0.00000067 	 loss = 0.8340(0.5463)
2023/10/22 07:00:14 - INFO - root -   Epoch: [97/300][40/84], lr: 0.00000067 	 loss = 0.9957(0.4868)
2023/10/22 07:01:02 - INFO - root -   Epoch: [97/300][60/84], lr: 0.00000067 	 loss = 1.3266(0.4494)
2023/10/22 07:01:54 - INFO - root -   Epoch: [97/300][80/84], lr: 0.00000067 	 loss = 0.4571(0.4711)
2023/10/22 07:01:55 - INFO - root -   Epoch: [97/300] 	 loss = 0.4675
2023/10/22 07:01:55 - INFO - root -   train_accuracy = 0.7440
2023/10/22 07:02:17 - INFO - root -   Epoch: [98/300][0/84], lr: 0.00000068 	 loss = 1.0092(1.0092)
2023/10/22 07:03:31 - INFO - root -   Epoch: [98/300][20/84], lr: 0.00000068 	 loss = 0.2174(0.4703)
2023/10/22 07:04:45 - INFO - root -   Epoch: [98/300][40/84], lr: 0.00000068 	 loss = 0.1441(0.4720)
2023/10/22 07:05:33 - INFO - root -   Epoch: [98/300][60/84], lr: 0.00000068 	 loss = 0.1762(0.4557)
2023/10/22 07:06:24 - INFO - root -   Epoch: [98/300][80/84], lr: 0.00000068 	 loss = 0.3014(0.4413)
2023/10/22 07:06:25 - INFO - root -   Epoch: [98/300] 	 loss = 0.4446
2023/10/22 07:06:25 - INFO - root -   train_accuracy = 0.7976
2023/10/22 07:06:47 - INFO - root -   Epoch: [99/300][0/84], lr: 0.00000068 	 loss = 0.3936(0.3936)
2023/10/22 07:08:21 - INFO - root -   Epoch: [99/300][20/84], lr: 0.00000068 	 loss = 0.6333(0.4469)
2023/10/22 07:09:15 - INFO - root -   Epoch: [99/300][40/84], lr: 0.00000068 	 loss = 0.2832(0.4089)
2023/10/22 07:10:13 - INFO - root -   Epoch: [99/300][60/84], lr: 0.00000068 	 loss = 0.0282(0.3789)
2023/10/22 07:11:03 - INFO - root -   Epoch: [99/300][80/84], lr: 0.00000068 	 loss = 0.2069(0.4079)
2023/10/22 07:11:05 - INFO - root -   Epoch: [99/300] 	 loss = 0.4175
2023/10/22 07:12:01 - INFO - root -   precision = 0.9070
2023/10/22 07:12:01 - INFO - root -   eval_loss = 0.2575
2023/10/22 07:12:01 - INFO - root -   eval_acc = 0.9070
2023/10/22 07:12:02 - INFO - root -   train_accuracy = 0.8155
2023/10/22 07:12:40 - INFO - root -   Epoch: [100/300][0/84], lr: 0.00000069 	 loss = 0.3373(0.3373)
2023/10/22 07:13:35 - INFO - root -   Epoch: [100/300][20/84], lr: 0.00000069 	 loss = 0.8635(0.4992)
2023/10/22 07:14:45 - INFO - root -   Epoch: [100/300][40/84], lr: 0.00000069 	 loss = 0.1943(0.5149)
2023/10/22 07:16:04 - INFO - root -   Epoch: [100/300][60/84], lr: 0.00000069 	 loss = 0.0542(0.4612)
2023/10/22 07:16:42 - INFO - root -   Epoch: [100/300][80/84], lr: 0.00000069 	 loss = 0.2175(0.4767)
2023/10/22 07:16:46 - INFO - root -   Epoch: [100/300] 	 loss = 0.4721
2023/10/22 07:16:46 - INFO - root -   train_accuracy = 0.7679
2023/10/22 07:17:07 - INFO - root -   Epoch: [101/300][0/84], lr: 0.00000070 	 loss = 0.0998(0.0998)
2023/10/22 07:18:21 - INFO - root -   Epoch: [101/300][20/84], lr: 0.00000070 	 loss = 0.1378(0.3401)
2023/10/22 07:19:16 - INFO - root -   Epoch: [101/300][40/84], lr: 0.00000070 	 loss = 0.0248(0.4236)
2023/10/22 07:20:46 - INFO - root -   Epoch: [101/300][60/84], lr: 0.00000070 	 loss = 0.1840(0.4419)
2023/10/22 07:21:21 - INFO - root -   Epoch: [101/300][80/84], lr: 0.00000070 	 loss = 1.3957(0.4719)
2023/10/22 07:21:26 - INFO - root -   Epoch: [101/300] 	 loss = 0.4582
2023/10/22 07:21:26 - INFO - root -   train_accuracy = 0.7500
2023/10/22 07:21:55 - INFO - root -   Epoch: [102/300][0/84], lr: 0.00000070 	 loss = 0.2362(0.2362)
2023/10/22 07:22:53 - INFO - root -   Epoch: [102/300][20/84], lr: 0.00000070 	 loss = 0.7192(0.4474)
2023/10/22 07:24:01 - INFO - root -   Epoch: [102/300][40/84], lr: 0.00000070 	 loss = 0.0470(0.4106)
2023/10/22 07:25:07 - INFO - root -   Epoch: [102/300][60/84], lr: 0.00000070 	 loss = 0.1953(0.3805)
2023/10/22 07:26:00 - INFO - root -   Epoch: [102/300][80/84], lr: 0.00000070 	 loss = 0.5128(0.3765)
2023/10/22 07:26:04 - INFO - root -   Epoch: [102/300] 	 loss = 0.3782
2023/10/22 07:26:04 - INFO - root -   train_accuracy = 0.8214
2023/10/22 07:26:26 - INFO - root -   Epoch: [103/300][0/84], lr: 0.00000071 	 loss = 0.0627(0.0627)
2023/10/22 07:27:26 - INFO - root -   Epoch: [103/300][20/84], lr: 0.00000071 	 loss = 0.7584(0.3646)
2023/10/22 07:29:00 - INFO - root -   Epoch: [103/300][40/84], lr: 0.00000071 	 loss = 0.0372(0.4076)
2023/10/22 07:29:54 - INFO - root -   Epoch: [103/300][60/84], lr: 0.00000071 	 loss = 0.1399(0.4290)
2023/10/22 07:30:37 - INFO - root -   Epoch: [103/300][80/84], lr: 0.00000071 	 loss = 0.1938(0.3915)
2023/10/22 07:30:38 - INFO - root -   Epoch: [103/300] 	 loss = 0.4028
2023/10/22 07:30:38 - INFO - root -   train_accuracy = 0.7857
2023/10/22 07:31:00 - INFO - root -   Epoch: [104/300][0/84], lr: 0.00000071 	 loss = 0.1407(0.1407)
2023/10/22 07:32:06 - INFO - root -   Epoch: [104/300][20/84], lr: 0.00000071 	 loss = 0.4447(0.4499)
2023/10/22 07:33:16 - INFO - root -   Epoch: [104/300][40/84], lr: 0.00000071 	 loss = 0.0768(0.4000)
2023/10/22 07:34:18 - INFO - root -   Epoch: [104/300][60/84], lr: 0.00000071 	 loss = 0.5616(0.4572)
2023/10/22 07:35:15 - INFO - root -   Epoch: [104/300][80/84], lr: 0.00000071 	 loss = 0.5286(0.4660)
2023/10/22 07:35:17 - INFO - root -   Epoch: [104/300] 	 loss = 0.4599
2023/10/22 07:36:13 - INFO - root -   precision = 0.9070
2023/10/22 07:36:13 - INFO - root -   eval_loss = 0.2665
2023/10/22 07:36:13 - INFO - root -   eval_acc = 0.9070
2023/10/22 07:36:14 - INFO - root -   train_accuracy = 0.7798
2023/10/22 07:36:35 - INFO - root -   Epoch: [105/300][0/84], lr: 0.00000072 	 loss = 0.0651(0.0651)
2023/10/22 07:37:49 - INFO - root -   Epoch: [105/300][20/84], lr: 0.00000072 	 loss = 0.0797(0.4471)
2023/10/22 07:38:58 - INFO - root -   Epoch: [105/300][40/84], lr: 0.00000072 	 loss = 0.6260(0.3660)
2023/10/22 07:40:09 - INFO - root -   Epoch: [105/300][60/84], lr: 0.00000072 	 loss = 0.1569(0.3898)
2023/10/22 07:40:54 - INFO - root -   Epoch: [105/300][80/84], lr: 0.00000072 	 loss = 0.1534(0.3608)
2023/10/22 07:40:58 - INFO - root -   Epoch: [105/300] 	 loss = 0.3734
2023/10/22 07:40:58 - INFO - root -   train_accuracy = 0.8214
2023/10/22 07:41:20 - INFO - root -   Epoch: [106/300][0/84], lr: 0.00000072 	 loss = 0.0314(0.0314)
2023/10/22 07:42:28 - INFO - root -   Epoch: [106/300][20/84], lr: 0.00000072 	 loss = 0.1701(0.4158)
2023/10/22 07:43:26 - INFO - root -   Epoch: [106/300][40/84], lr: 0.00000072 	 loss = 0.6537(0.3972)
2023/10/22 07:44:34 - INFO - root -   Epoch: [106/300][60/84], lr: 0.00000072 	 loss = 0.1767(0.3967)
2023/10/22 07:45:26 - INFO - root -   Epoch: [106/300][80/84], lr: 0.00000072 	 loss = 0.3626(0.4347)
2023/10/22 07:45:28 - INFO - root -   Epoch: [106/300] 	 loss = 0.4336
2023/10/22 07:45:28 - INFO - root -   train_accuracy = 0.7798
2023/10/22 07:45:50 - INFO - root -   Epoch: [107/300][0/84], lr: 0.00000073 	 loss = 0.2954(0.2954)
2023/10/22 07:46:57 - INFO - root -   Epoch: [107/300][20/84], lr: 0.00000073 	 loss = 0.6551(0.4613)
2023/10/22 07:48:05 - INFO - root -   Epoch: [107/300][40/84], lr: 0.00000073 	 loss = 0.5762(0.4933)
2023/10/22 07:49:29 - INFO - root -   Epoch: [107/300][60/84], lr: 0.00000073 	 loss = 0.2555(0.4601)
2023/10/22 07:50:04 - INFO - root -   Epoch: [107/300][80/84], lr: 0.00000073 	 loss = 0.4086(0.4548)
2023/10/22 07:50:05 - INFO - root -   Epoch: [107/300] 	 loss = 0.4769
2023/10/22 07:50:05 - INFO - root -   train_accuracy = 0.7500
2023/10/22 07:50:43 - INFO - root -   Epoch: [108/300][0/84], lr: 0.00000074 	 loss = 0.2855(0.2855)
2023/10/22 07:51:39 - INFO - root -   Epoch: [108/300][20/84], lr: 0.00000074 	 loss = 0.4714(0.4868)
2023/10/22 07:52:59 - INFO - root -   Epoch: [108/300][40/84], lr: 0.00000074 	 loss = 0.1511(0.3902)
2023/10/22 07:53:47 - INFO - root -   Epoch: [108/300][60/84], lr: 0.00000074 	 loss = 0.4793(0.4006)
2023/10/22 07:54:45 - INFO - root -   Epoch: [108/300][80/84], lr: 0.00000074 	 loss = 0.3761(0.4058)
2023/10/22 07:54:46 - INFO - root -   Epoch: [108/300] 	 loss = 0.4059
2023/10/22 07:54:46 - INFO - root -   train_accuracy = 0.7917
2023/10/22 07:55:23 - INFO - root -   Epoch: [109/300][0/84], lr: 0.00000074 	 loss = 0.5928(0.5928)
2023/10/22 07:56:23 - INFO - root -   Epoch: [109/300][20/84], lr: 0.00000074 	 loss = 0.7046(0.5935)
2023/10/22 07:57:38 - INFO - root -   Epoch: [109/300][40/84], lr: 0.00000074 	 loss = 0.4007(0.5684)
2023/10/22 07:58:29 - INFO - root -   Epoch: [109/300][60/84], lr: 0.00000074 	 loss = 0.0460(0.5174)
2023/10/22 07:59:26 - INFO - root -   Epoch: [109/300][80/84], lr: 0.00000074 	 loss = 0.1976(0.5027)
2023/10/22 07:59:29 - INFO - root -   Epoch: [109/300] 	 loss = 0.4899
2023/10/22 08:00:26 - INFO - root -   precision = 0.8837
2023/10/22 08:00:26 - INFO - root -   eval_loss = 0.2483
2023/10/22 08:00:26 - INFO - root -   eval_acc = 0.8837
2023/10/22 08:00:27 - INFO - root -   train_accuracy = 0.7381
2023/10/22 08:00:49 - INFO - root -   Epoch: [110/300][0/84], lr: 0.00000075 	 loss = 0.0586(0.0586)
2023/10/22 08:01:48 - INFO - root -   Epoch: [110/300][20/84], lr: 0.00000075 	 loss = 0.7566(0.4559)
2023/10/22 08:03:07 - INFO - root -   Epoch: [110/300][40/84], lr: 0.00000075 	 loss = 0.5688(0.4077)
2023/10/22 08:04:32 - INFO - root -   Epoch: [110/300][60/84], lr: 0.00000075 	 loss = 0.2951(0.4198)
2023/10/22 08:05:06 - INFO - root -   Epoch: [110/300][80/84], lr: 0.00000075 	 loss = 0.9308(0.4228)
2023/10/22 08:05:07 - INFO - root -   Epoch: [110/300] 	 loss = 0.4191
2023/10/22 08:05:07 - INFO - root -   train_accuracy = 0.7917
2023/10/22 08:05:29 - INFO - root -   Epoch: [111/300][0/84], lr: 0.00000075 	 loss = 0.0747(0.0747)
2023/10/22 08:06:43 - INFO - root -   Epoch: [111/300][20/84], lr: 0.00000075 	 loss = 0.1203(0.4771)
2023/10/22 08:07:49 - INFO - root -   Epoch: [111/300][40/84], lr: 0.00000075 	 loss = 0.1307(0.4620)
2023/10/22 08:09:07 - INFO - root -   Epoch: [111/300][60/84], lr: 0.00000075 	 loss = 0.4114(0.4671)
2023/10/22 08:10:00 - INFO - root -   Epoch: [111/300][80/84], lr: 0.00000075 	 loss = 0.0696(0.4235)
2023/10/22 08:10:01 - INFO - root -   Epoch: [111/300] 	 loss = 0.4191
2023/10/22 08:10:01 - INFO - root -   train_accuracy = 0.8155
2023/10/22 08:10:30 - INFO - root -   Epoch: [112/300][0/84], lr: 0.00000076 	 loss = 0.4134(0.4134)
2023/10/22 08:11:28 - INFO - root -   Epoch: [112/300][20/84], lr: 0.00000076 	 loss = 0.3811(0.4248)
2023/10/22 08:12:43 - INFO - root -   Epoch: [112/300][40/84], lr: 0.00000076 	 loss = 0.4326(0.4385)
2023/10/22 08:13:41 - INFO - root -   Epoch: [112/300][60/84], lr: 0.00000076 	 loss = 0.2443(0.4438)
2023/10/22 08:14:37 - INFO - root -   Epoch: [112/300][80/84], lr: 0.00000076 	 loss = 0.2169(0.4531)
2023/10/22 08:14:39 - INFO - root -   Epoch: [112/300] 	 loss = 0.4576
2023/10/22 08:14:39 - INFO - root -   train_accuracy = 0.7619
2023/10/22 08:15:01 - INFO - root -   Epoch: [113/300][0/84], lr: 0.00000077 	 loss = 0.0518(0.0518)
2023/10/22 08:16:07 - INFO - root -   Epoch: [113/300][20/84], lr: 0.00000077 	 loss = 0.5206(0.4088)
2023/10/22 08:17:13 - INFO - root -   Epoch: [113/300][40/84], lr: 0.00000077 	 loss = 0.0569(0.4145)
2023/10/22 08:18:32 - INFO - root -   Epoch: [113/300][60/84], lr: 0.00000077 	 loss = 0.0578(0.3752)
2023/10/22 08:19:17 - INFO - root -   Epoch: [113/300][80/84], lr: 0.00000077 	 loss = 0.0668(0.3946)
2023/10/22 08:19:18 - INFO - root -   Epoch: [113/300] 	 loss = 0.3903
2023/10/22 08:19:18 - INFO - root -   train_accuracy = 0.8214
2023/10/22 08:19:49 - INFO - root -   Epoch: [114/300][0/84], lr: 0.00000077 	 loss = 0.8116(0.8116)
2023/10/22 08:20:54 - INFO - root -   Epoch: [114/300][20/84], lr: 0.00000077 	 loss = 0.3773(0.5360)
2023/10/22 08:22:08 - INFO - root -   Epoch: [114/300][40/84], lr: 0.00000077 	 loss = 0.0442(0.4659)
2023/10/22 08:23:14 - INFO - root -   Epoch: [114/300][60/84], lr: 0.00000077 	 loss = 0.1033(0.4833)
2023/10/22 08:23:59 - INFO - root -   Epoch: [114/300][80/84], lr: 0.00000077 	 loss = 0.1162(0.5010)
2023/10/22 08:24:00 - INFO - root -   Epoch: [114/300] 	 loss = 0.4986
2023/10/22 08:24:59 - INFO - root -   precision = 0.8837
2023/10/22 08:24:59 - INFO - root -   eval_loss = 0.2675
2023/10/22 08:24:59 - INFO - root -   eval_acc = 0.8837
2023/10/22 08:25:00 - INFO - root -   train_accuracy = 0.7262
2023/10/22 08:25:21 - INFO - root -   Epoch: [115/300][0/84], lr: 0.00000078 	 loss = 0.0852(0.0852)
2023/10/22 08:26:29 - INFO - root -   Epoch: [115/300][20/84], lr: 0.00000078 	 loss = 0.5744(0.3125)
2023/10/22 08:27:49 - INFO - root -   Epoch: [115/300][40/84], lr: 0.00000078 	 loss = 0.5217(0.3469)
2023/10/22 08:28:49 - INFO - root -   Epoch: [115/300][60/84], lr: 0.00000078 	 loss = 0.7768(0.3383)
2023/10/22 08:29:37 - INFO - root -   Epoch: [115/300][80/84], lr: 0.00000078 	 loss = 0.0598(0.3784)
2023/10/22 08:29:41 - INFO - root -   Epoch: [115/300] 	 loss = 0.3839
2023/10/22 08:29:41 - INFO - root -   train_accuracy = 0.7917
2023/10/22 08:30:12 - INFO - root -   Epoch: [116/300][0/84], lr: 0.00000078 	 loss = 0.0247(0.0247)
2023/10/22 08:31:16 - INFO - root -   Epoch: [116/300][20/84], lr: 0.00000078 	 loss = 0.2494(0.4457)
2023/10/22 08:32:22 - INFO - root -   Epoch: [116/300][40/84], lr: 0.00000078 	 loss = 0.3766(0.3864)
2023/10/22 08:33:19 - INFO - root -   Epoch: [116/300][60/84], lr: 0.00000078 	 loss = 0.1620(0.3887)
2023/10/22 08:34:12 - INFO - root -   Epoch: [116/300][80/84], lr: 0.00000078 	 loss = 0.3013(0.4270)
2023/10/22 08:34:13 - INFO - root -   Epoch: [116/300] 	 loss = 0.4341
2023/10/22 08:34:13 - INFO - root -   train_accuracy = 0.8036
2023/10/22 08:34:42 - INFO - root -   Epoch: [117/300][0/84], lr: 0.00000079 	 loss = 0.1506(0.1506)
2023/10/22 08:35:41 - INFO - root -   Epoch: [117/300][20/84], lr: 0.00000079 	 loss = 0.8877(0.3491)
2023/10/22 08:36:49 - INFO - root -   Epoch: [117/300][40/84], lr: 0.00000079 	 loss = 0.1132(0.3639)
2023/10/22 08:37:48 - INFO - root -   Epoch: [117/300][60/84], lr: 0.00000079 	 loss = 0.4150(0.3784)
2023/10/22 08:38:45 - INFO - root -   Epoch: [117/300][80/84], lr: 0.00000079 	 loss = 0.0803(0.3722)
2023/10/22 08:38:46 - INFO - root -   Epoch: [117/300] 	 loss = 0.3745
2023/10/22 08:38:46 - INFO - root -   train_accuracy = 0.8036
2023/10/22 08:39:16 - INFO - root -   Epoch: [118/300][0/84], lr: 0.00000080 	 loss = 0.1174(0.1174)
2023/10/22 08:40:19 - INFO - root -   Epoch: [118/300][20/84], lr: 0.00000080 	 loss = 0.1933(0.3212)
2023/10/22 08:41:39 - INFO - root -   Epoch: [118/300][40/84], lr: 0.00000080 	 loss = 0.1265(0.3207)
2023/10/22 08:42:37 - INFO - root -   Epoch: [118/300][60/84], lr: 0.00000080 	 loss = 1.1379(0.3288)
2023/10/22 08:43:19 - INFO - root -   Epoch: [118/300][80/84], lr: 0.00000080 	 loss = 0.7183(0.3677)
2023/10/22 08:43:21 - INFO - root -   Epoch: [118/300] 	 loss = 0.3614
2023/10/22 08:43:21 - INFO - root -   train_accuracy = 0.8095
2023/10/22 08:43:57 - INFO - root -   Epoch: [119/300][0/84], lr: 0.00000080 	 loss = 0.5918(0.5918)
2023/10/22 08:44:54 - INFO - root -   Epoch: [119/300][20/84], lr: 0.00000080 	 loss = 0.3067(0.4199)
2023/10/22 08:46:22 - INFO - root -   Epoch: [119/300][40/84], lr: 0.00000080 	 loss = 0.7957(0.3614)
2023/10/22 08:47:14 - INFO - root -   Epoch: [119/300][60/84], lr: 0.00000080 	 loss = 0.3233(0.3720)
2023/10/22 08:48:05 - INFO - root -   Epoch: [119/300][80/84], lr: 0.00000080 	 loss = 0.4500(0.4055)
2023/10/22 08:48:07 - INFO - root -   Epoch: [119/300] 	 loss = 0.4131
2023/10/22 08:49:03 - INFO - root -   precision = 0.9302
2023/10/22 08:49:03 - INFO - root -   eval_loss = 0.2749
2023/10/22 08:49:03 - INFO - root -   eval_acc = 0.9302
2023/10/22 08:49:04 - INFO - root -   train_accuracy = 0.7738
2023/10/22 08:49:33 - INFO - root -   Epoch: [120/300][0/84], lr: 0.00000081 	 loss = 0.0838(0.0838)
2023/10/22 08:50:31 - INFO - root -   Epoch: [120/300][20/84], lr: 0.00000081 	 loss = 0.9873(0.4415)
2023/10/22 08:51:48 - INFO - root -   Epoch: [120/300][40/84], lr: 0.00000081 	 loss = 0.1387(0.4258)
2023/10/22 08:52:38 - INFO - root -   Epoch: [120/300][60/84], lr: 0.00000081 	 loss = 0.0362(0.3914)
2023/10/22 08:53:45 - INFO - root -   Epoch: [120/300][80/84], lr: 0.00000081 	 loss = 0.1278(0.4218)
2023/10/22 08:53:46 - INFO - root -   Epoch: [120/300] 	 loss = 0.4180
2023/10/22 08:53:46 - INFO - root -   train_accuracy = 0.7560
2023/10/22 08:54:16 - INFO - root -   Epoch: [121/300][0/84], lr: 0.00000081 	 loss = 0.3648(0.3648)
2023/10/22 08:55:20 - INFO - root -   Epoch: [121/300][20/84], lr: 0.00000081 	 loss = 0.1781(0.3558)
2023/10/22 08:56:43 - INFO - root -   Epoch: [121/300][40/84], lr: 0.00000081 	 loss = 0.1386(0.4155)
2023/10/22 08:57:32 - INFO - root -   Epoch: [121/300][60/84], lr: 0.00000081 	 loss = 0.7779(0.4045)
2023/10/22 08:58:25 - INFO - root -   Epoch: [121/300][80/84], lr: 0.00000081 	 loss = 0.1062(0.4691)
2023/10/22 08:58:27 - INFO - root -   Epoch: [121/300] 	 loss = 0.4610
2023/10/22 08:58:27 - INFO - root -   train_accuracy = 0.7560
2023/10/22 08:59:05 - INFO - root -   Epoch: [122/300][0/84], lr: 0.00000082 	 loss = 0.5227(0.5227)
2023/10/22 09:00:03 - INFO - root -   Epoch: [122/300][20/84], lr: 0.00000082 	 loss = 0.6962(0.3609)
2023/10/22 09:01:14 - INFO - root -   Epoch: [122/300][40/84], lr: 0.00000082 	 loss = 0.0302(0.3665)
2023/10/22 09:02:05 - INFO - root -   Epoch: [122/300][60/84], lr: 0.00000082 	 loss = 0.2066(0.3781)
2023/10/22 09:03:01 - INFO - root -   Epoch: [122/300][80/84], lr: 0.00000082 	 loss = 0.4447(0.3847)
2023/10/22 09:03:02 - INFO - root -   Epoch: [122/300] 	 loss = 0.3779
2023/10/22 09:03:02 - INFO - root -   train_accuracy = 0.8333
2023/10/22 09:03:33 - INFO - root -   Epoch: [123/300][0/84], lr: 0.00000082 	 loss = 0.3200(0.3200)
2023/10/22 09:04:44 - INFO - root -   Epoch: [123/300][20/84], lr: 0.00000082 	 loss = 0.8403(0.4503)
2023/10/22 09:05:35 - INFO - root -   Epoch: [123/300][40/84], lr: 0.00000082 	 loss = 0.1148(0.3469)
2023/10/22 09:07:05 - INFO - root -   Epoch: [123/300][60/84], lr: 0.00000082 	 loss = 0.0553(0.3463)
2023/10/22 09:07:45 - INFO - root -   Epoch: [123/300][80/84], lr: 0.00000082 	 loss = 0.0169(0.3590)
2023/10/22 09:07:51 - INFO - root -   Epoch: [123/300] 	 loss = 0.3768
2023/10/22 09:07:51 - INFO - root -   train_accuracy = 0.7976
2023/10/22 09:08:21 - INFO - root -   Epoch: [124/300][0/84], lr: 0.00000083 	 loss = 0.1962(0.1962)
2023/10/22 09:09:19 - INFO - root -   Epoch: [124/300][20/84], lr: 0.00000083 	 loss = 0.2878(0.4286)
2023/10/22 09:10:33 - INFO - root -   Epoch: [124/300][40/84], lr: 0.00000083 	 loss = 0.0304(0.4270)
2023/10/22 09:11:40 - INFO - root -   Epoch: [124/300][60/84], lr: 0.00000083 	 loss = 0.5851(0.4657)
2023/10/22 09:12:27 - INFO - root -   Epoch: [124/300][80/84], lr: 0.00000083 	 loss = 0.7984(0.4511)
2023/10/22 09:12:30 - INFO - root -   Epoch: [124/300] 	 loss = 0.4459
2023/10/22 09:13:27 - INFO - root -   precision = 0.8140
2023/10/22 09:13:27 - INFO - root -   eval_loss = 0.3695
2023/10/22 09:13:27 - INFO - root -   eval_acc = 0.8140
2023/10/22 09:13:28 - INFO - root -   train_accuracy = 0.7440
2023/10/22 09:13:58 - INFO - root -   Epoch: [125/300][0/84], lr: 0.00000084 	 loss = 0.4530(0.4530)
2023/10/22 09:14:55 - INFO - root -   Epoch: [125/300][20/84], lr: 0.00000084 	 loss = 0.1239(0.5091)
2023/10/22 09:15:55 - INFO - root -   Epoch: [125/300][40/84], lr: 0.00000084 	 loss = 0.6285(0.4695)
2023/10/22 09:17:00 - INFO - root -   Epoch: [125/300][60/84], lr: 0.00000084 	 loss = 0.1110(0.4592)
2023/10/22 09:17:52 - INFO - root -   Epoch: [125/300][80/84], lr: 0.00000084 	 loss = 0.1791(0.4426)
2023/10/22 09:17:55 - INFO - root -   Epoch: [125/300] 	 loss = 0.4413
2023/10/22 09:17:55 - INFO - root -   train_accuracy = 0.7679
2023/10/22 09:18:17 - INFO - root -   Epoch: [126/300][0/84], lr: 0.00000084 	 loss = 0.0793(0.0793)
2023/10/22 09:19:33 - INFO - root -   Epoch: [126/300][20/84], lr: 0.00000084 	 loss = 0.4803(0.3785)
2023/10/22 09:20:43 - INFO - root -   Epoch: [126/300][40/84], lr: 0.00000084 	 loss = 0.3039(0.3454)
2023/10/22 09:21:51 - INFO - root -   Epoch: [126/300][60/84], lr: 0.00000084 	 loss = 0.2933(0.3491)
2023/10/22 09:22:37 - INFO - root -   Epoch: [126/300][80/84], lr: 0.00000084 	 loss = 0.2238(0.3549)
2023/10/22 09:22:40 - INFO - root -   Epoch: [126/300] 	 loss = 0.3517
2023/10/22 09:22:40 - INFO - root -   train_accuracy = 0.8393
2023/10/22 09:23:12 - INFO - root -   Epoch: [127/300][0/84], lr: 0.00000085 	 loss = 0.5457(0.5457)
2023/10/22 09:24:13 - INFO - root -   Epoch: [127/300][20/84], lr: 0.00000085 	 loss = 0.2038(0.4277)
2023/10/22 09:25:29 - INFO - root -   Epoch: [127/300][40/84], lr: 0.00000085 	 loss = 0.1212(0.3633)
2023/10/22 09:26:19 - INFO - root -   Epoch: [127/300][60/84], lr: 0.00000085 	 loss = 0.3265(0.3645)
2023/10/22 09:27:16 - INFO - root -   Epoch: [127/300][80/84], lr: 0.00000085 	 loss = 0.0704(0.4072)
2023/10/22 09:27:18 - INFO - root -   Epoch: [127/300] 	 loss = 0.3990
2023/10/22 09:27:18 - INFO - root -   train_accuracy = 0.8036
2023/10/22 09:27:39 - INFO - root -   Epoch: [128/300][0/84], lr: 0.00000085 	 loss = 0.0420(0.0420)
2023/10/22 09:29:08 - INFO - root -   Epoch: [128/300][20/84], lr: 0.00000085 	 loss = 0.6462(0.4627)
2023/10/22 09:29:50 - INFO - root -   Epoch: [128/300][40/84], lr: 0.00000085 	 loss = 0.0555(0.4091)
2023/10/22 09:31:05 - INFO - root -   Epoch: [128/300][60/84], lr: 0.00000085 	 loss = 0.0618(0.4887)
2023/10/22 09:31:57 - INFO - root -   Epoch: [128/300][80/84], lr: 0.00000085 	 loss = 0.0787(0.4518)
2023/10/22 09:32:02 - INFO - root -   Epoch: [128/300] 	 loss = 0.4595
2023/10/22 09:32:02 - INFO - root -   train_accuracy = 0.7679
2023/10/22 09:32:40 - INFO - root -   Epoch: [129/300][0/84], lr: 0.00000086 	 loss = 0.6229(0.6229)
2023/10/22 09:33:49 - INFO - root -   Epoch: [129/300][20/84], lr: 0.00000086 	 loss = 0.1145(0.5317)
2023/10/22 09:34:52 - INFO - root -   Epoch: [129/300][40/84], lr: 0.00000086 	 loss = 0.4787(0.4974)
2023/10/22 09:36:13 - INFO - root -   Epoch: [129/300][60/84], lr: 0.00000086 	 loss = 0.1952(0.4457)
2023/10/22 09:36:56 - INFO - root -   Epoch: [129/300][80/84], lr: 0.00000086 	 loss = 0.2977(0.4110)
2023/10/22 09:36:58 - INFO - root -   Epoch: [129/300] 	 loss = 0.4000
2023/10/22 09:37:54 - INFO - root -   precision = 0.8837
2023/10/22 09:37:54 - INFO - root -   eval_loss = 0.3494
2023/10/22 09:37:54 - INFO - root -   eval_acc = 0.8837
2023/10/22 09:37:55 - INFO - root -   train_accuracy = 0.7798
2023/10/22 09:38:26 - INFO - root -   Epoch: [130/300][0/84], lr: 0.00000087 	 loss = 0.6022(0.6022)
2023/10/22 09:39:20 - INFO - root -   Epoch: [130/300][20/84], lr: 0.00000087 	 loss = 0.3744(0.4533)
2023/10/22 09:40:42 - INFO - root -   Epoch: [130/300][40/84], lr: 0.00000087 	 loss = 0.6203(0.4419)
2023/10/22 09:41:32 - INFO - root -   Epoch: [130/300][60/84], lr: 0.00000087 	 loss = 0.1539(0.4289)
2023/10/22 09:42:29 - INFO - root -   Epoch: [130/300][80/84], lr: 0.00000087 	 loss = 0.0785(0.4083)
2023/10/22 09:42:30 - INFO - root -   Epoch: [130/300] 	 loss = 0.3972
2023/10/22 09:42:30 - INFO - root -   train_accuracy = 0.7857
2023/10/22 09:42:51 - INFO - root -   Epoch: [131/300][0/84], lr: 0.00000087 	 loss = 0.0676(0.0676)
2023/10/22 09:43:59 - INFO - root -   Epoch: [131/300][20/84], lr: 0.00000087 	 loss = 0.4287(0.4309)
2023/10/22 09:45:31 - INFO - root -   Epoch: [131/300][40/84], lr: 0.00000087 	 loss = 0.1851(0.4509)
2023/10/22 09:46:29 - INFO - root -   Epoch: [131/300][60/84], lr: 0.00000087 	 loss = 0.0667(0.4608)
2023/10/22 09:47:20 - INFO - root -   Epoch: [131/300][80/84], lr: 0.00000087 	 loss = 0.4153(0.4948)
2023/10/22 09:47:21 - INFO - root -   Epoch: [131/300] 	 loss = 0.4895
2023/10/22 09:47:21 - INFO - root -   train_accuracy = 0.7202
2023/10/22 09:47:51 - INFO - root -   Epoch: [132/300][0/84], lr: 0.00000088 	 loss = 0.0363(0.0363)
2023/10/22 09:48:56 - INFO - root -   Epoch: [132/300][20/84], lr: 0.00000088 	 loss = 0.2979(0.4876)
2023/10/22 09:49:50 - INFO - root -   Epoch: [132/300][40/84], lr: 0.00000088 	 loss = 0.0414(0.4518)
2023/10/22 09:51:04 - INFO - root -   Epoch: [132/300][60/84], lr: 0.00000088 	 loss = 0.7360(0.4802)
2023/10/22 09:51:57 - INFO - root -   Epoch: [132/300][80/84], lr: 0.00000088 	 loss = 0.0522(0.4650)
2023/10/22 09:52:00 - INFO - root -   Epoch: [132/300] 	 loss = 0.4622
2023/10/22 09:52:00 - INFO - root -   train_accuracy = 0.7560
2023/10/22 09:52:29 - INFO - root -   Epoch: [133/300][0/84], lr: 0.00000088 	 loss = 0.0548(0.0548)
2023/10/22 09:53:26 - INFO - root -   Epoch: [133/300][20/84], lr: 0.00000088 	 loss = 0.1458(0.3866)
2023/10/22 09:54:26 - INFO - root -   Epoch: [133/300][40/84], lr: 0.00000088 	 loss = 0.0264(0.3425)
2023/10/22 09:55:45 - INFO - root -   Epoch: [133/300][60/84], lr: 0.00000088 	 loss = 0.3138(0.3661)
2023/10/22 09:56:19 - INFO - root -   Epoch: [133/300][80/84], lr: 0.00000088 	 loss = 0.1555(0.3565)
2023/10/22 09:56:23 - INFO - root -   Epoch: [133/300] 	 loss = 0.3658
2023/10/22 09:56:23 - INFO - root -   train_accuracy = 0.8155
2023/10/22 09:56:45 - INFO - root -   Epoch: [134/300][0/84], lr: 0.00000089 	 loss = 0.1046(0.1046)
2023/10/22 09:58:03 - INFO - root -   Epoch: [134/300][20/84], lr: 0.00000089 	 loss = 0.5130(0.3090)
2023/10/22 09:58:58 - INFO - root -   Epoch: [134/300][40/84], lr: 0.00000089 	 loss = 0.0427(0.2810)
2023/10/22 10:00:32 - INFO - root -   Epoch: [134/300][60/84], lr: 0.00000089 	 loss = 0.3708(0.3209)
2023/10/22 10:01:04 - INFO - root -   Epoch: [134/300][80/84], lr: 0.00000089 	 loss = 0.0417(0.3378)
2023/10/22 10:01:05 - INFO - root -   Epoch: [134/300] 	 loss = 0.3378
2023/10/22 10:02:01 - INFO - root -   precision = 0.8837
2023/10/22 10:02:01 - INFO - root -   eval_loss = 0.2831
2023/10/22 10:02:01 - INFO - root -   eval_acc = 0.8837
2023/10/22 10:02:02 - INFO - root -   train_accuracy = 0.8333
2023/10/22 10:02:33 - INFO - root -   Epoch: [135/300][0/84], lr: 0.00000090 	 loss = 0.9527(0.9527)
2023/10/22 10:03:32 - INFO - root -   Epoch: [135/300][20/84], lr: 0.00000090 	 loss = 0.5429(0.4816)
2023/10/22 10:04:58 - INFO - root -   Epoch: [135/300][40/84], lr: 0.00000090 	 loss = 1.3492(0.4773)
2023/10/22 10:05:53 - INFO - root -   Epoch: [135/300][60/84], lr: 0.00000090 	 loss = 0.1975(0.4373)
2023/10/22 10:06:46 - INFO - root -   Epoch: [135/300][80/84], lr: 0.00000090 	 loss = 1.4491(0.4340)
2023/10/22 10:06:49 - INFO - root -   Epoch: [135/300] 	 loss = 0.4280
2023/10/22 10:06:49 - INFO - root -   train_accuracy = 0.7679
2023/10/22 10:07:19 - INFO - root -   Epoch: [136/300][0/84], lr: 0.00000090 	 loss = 2.1068(2.1068)
2023/10/22 10:08:32 - INFO - root -   Epoch: [136/300][20/84], lr: 0.00000090 	 loss = 0.4837(0.5113)
2023/10/22 10:09:26 - INFO - root -   Epoch: [136/300][40/84], lr: 0.00000090 	 loss = 0.4494(0.4637)
2023/10/22 10:10:33 - INFO - root -   Epoch: [136/300][60/84], lr: 0.00000090 	 loss = 0.4076(0.4361)
2023/10/22 10:11:34 - INFO - root -   Epoch: [136/300][80/84], lr: 0.00000090 	 loss = 1.4562(0.4436)
2023/10/22 10:11:35 - INFO - root -   Epoch: [136/300] 	 loss = 0.4383
2023/10/22 10:11:35 - INFO - root -   train_accuracy = 0.7798
2023/10/22 10:12:05 - INFO - root -   Epoch: [137/300][0/84], lr: 0.00000091 	 loss = 0.4158(0.4158)
2023/10/22 10:13:12 - INFO - root -   Epoch: [137/300][20/84], lr: 0.00000091 	 loss = 2.7205(0.4856)
2023/10/22 10:14:21 - INFO - root -   Epoch: [137/300][40/84], lr: 0.00000091 	 loss = 0.2614(0.4628)
2023/10/22 10:15:36 - INFO - root -   Epoch: [137/300][60/84], lr: 0.00000091 	 loss = 0.4596(0.4274)
2023/10/22 10:16:24 - INFO - root -   Epoch: [137/300][80/84], lr: 0.00000091 	 loss = 0.2831(0.4575)
2023/10/22 10:16:25 - INFO - root -   Epoch: [137/300] 	 loss = 0.4575
2023/10/22 10:16:25 - INFO - root -   train_accuracy = 0.7321
2023/10/22 10:16:46 - INFO - root -   Epoch: [138/300][0/84], lr: 0.00000091 	 loss = 0.0367(0.0367)
2023/10/22 10:17:47 - INFO - root -   Epoch: [138/300][20/84], lr: 0.00000091 	 loss = 0.0831(0.3005)
2023/10/22 10:19:10 - INFO - root -   Epoch: [138/300][40/84], lr: 0.00000091 	 loss = 0.9261(0.3465)
2023/10/22 10:20:03 - INFO - root -   Epoch: [138/300][60/84], lr: 0.00000091 	 loss = 0.1754(0.3470)
2023/10/22 10:21:03 - INFO - root -   Epoch: [138/300][80/84], lr: 0.00000091 	 loss = 0.2986(0.3350)
2023/10/22 10:21:04 - INFO - root -   Epoch: [138/300] 	 loss = 0.3378
2023/10/22 10:21:04 - INFO - root -   train_accuracy = 0.8393
2023/10/22 10:21:34 - INFO - root -   Epoch: [139/300][0/84], lr: 0.00000092 	 loss = 0.5557(0.5557)
2023/10/22 10:22:38 - INFO - root -   Epoch: [139/300][20/84], lr: 0.00000092 	 loss = 0.5694(0.4075)
2023/10/22 10:23:29 - INFO - root -   Epoch: [139/300][40/84], lr: 0.00000092 	 loss = 0.0243(0.3809)
2023/10/22 10:24:47 - INFO - root -   Epoch: [139/300][60/84], lr: 0.00000092 	 loss = 0.2531(0.3905)
2023/10/22 10:25:31 - INFO - root -   Epoch: [139/300][80/84], lr: 0.00000092 	 loss = 0.3516(0.3552)
2023/10/22 10:25:33 - INFO - root -   Epoch: [139/300] 	 loss = 0.3564
2023/10/22 10:26:29 - INFO - root -   precision = 0.8372
2023/10/22 10:26:29 - INFO - root -   eval_loss = 0.2981
2023/10/22 10:26:29 - INFO - root -   eval_acc = 0.8372
2023/10/22 10:26:30 - INFO - root -   train_accuracy = 0.8393
2023/10/22 10:27:09 - INFO - root -   Epoch: [140/300][0/84], lr: 0.00000093 	 loss = 0.9283(0.9283)
2023/10/22 10:28:06 - INFO - root -   Epoch: [140/300][20/84], lr: 0.00000093 	 loss = 1.0002(0.4630)
2023/10/22 10:29:29 - INFO - root -   Epoch: [140/300][40/84], lr: 0.00000093 	 loss = 0.2503(0.3295)
2023/10/22 10:30:23 - INFO - root -   Epoch: [140/300][60/84], lr: 0.00000093 	 loss = 0.4882(0.3699)
2023/10/22 10:31:11 - INFO - root -   Epoch: [140/300][80/84], lr: 0.00000093 	 loss = 0.0199(0.3708)
2023/10/22 10:31:12 - INFO - root -   Epoch: [140/300] 	 loss = 0.3739
2023/10/22 10:31:12 - INFO - root -   train_accuracy = 0.7917
2023/10/22 10:31:34 - INFO - root -   Epoch: [141/300][0/84], lr: 0.00000093 	 loss = 0.0846(0.0846)
2023/10/22 10:32:40 - INFO - root -   Epoch: [141/300][20/84], lr: 0.00000093 	 loss = 2.8469(0.4530)
2023/10/22 10:33:39 - INFO - root -   Epoch: [141/300][40/84], lr: 0.00000093 	 loss = 0.0351(0.4333)
2023/10/22 10:34:55 - INFO - root -   Epoch: [141/300][60/84], lr: 0.00000093 	 loss = 0.2665(0.4328)
2023/10/22 10:35:38 - INFO - root -   Epoch: [141/300][80/84], lr: 0.00000093 	 loss = 0.3427(0.4887)
2023/10/22 10:35:42 - INFO - root -   Epoch: [141/300] 	 loss = 0.4804
2023/10/22 10:35:42 - INFO - root -   train_accuracy = 0.7857
2023/10/22 10:36:20 - INFO - root -   Epoch: [142/300][0/84], lr: 0.00000094 	 loss = 0.7112(0.7112)
2023/10/22 10:37:23 - INFO - root -   Epoch: [142/300][20/84], lr: 0.00000094 	 loss = 0.3936(0.5219)
2023/10/22 10:38:45 - INFO - root -   Epoch: [142/300][40/84], lr: 0.00000094 	 loss = 0.7046(0.4285)
2023/10/22 10:39:30 - INFO - root -   Epoch: [142/300][60/84], lr: 0.00000094 	 loss = 0.1727(0.4249)
2023/10/22 10:40:27 - INFO - root -   Epoch: [142/300][80/84], lr: 0.00000094 	 loss = 2.0812(0.4390)
2023/10/22 10:40:29 - INFO - root -   Epoch: [142/300] 	 loss = 0.4329
2023/10/22 10:40:29 - INFO - root -   train_accuracy = 0.7679
2023/10/22 10:40:58 - INFO - root -   Epoch: [143/300][0/84], lr: 0.00000094 	 loss = 0.0691(0.0691)
2023/10/22 10:41:58 - INFO - root -   Epoch: [143/300][20/84], lr: 0.00000094 	 loss = 0.0571(0.4572)
2023/10/22 10:43:01 - INFO - root -   Epoch: [143/300][40/84], lr: 0.00000094 	 loss = 0.1573(0.3336)
2023/10/22 10:44:09 - INFO - root -   Epoch: [143/300][60/84], lr: 0.00000094 	 loss = 0.0204(0.3218)
2023/10/22 10:45:02 - INFO - root -   Epoch: [143/300][80/84], lr: 0.00000094 	 loss = 0.0706(0.3211)
2023/10/22 10:45:03 - INFO - root -   Epoch: [143/300] 	 loss = 0.3219
2023/10/22 10:45:03 - INFO - root -   train_accuracy = 0.8155
2023/10/22 10:45:33 - INFO - root -   Epoch: [144/300][0/84], lr: 0.00000095 	 loss = 0.3184(0.3184)
2023/10/22 10:46:44 - INFO - root -   Epoch: [144/300][20/84], lr: 0.00000095 	 loss = 1.0125(0.4178)
2023/10/22 10:47:42 - INFO - root -   Epoch: [144/300][40/84], lr: 0.00000095 	 loss = 0.4632(0.4087)
2023/10/22 10:49:13 - INFO - root -   Epoch: [144/300][60/84], lr: 0.00000095 	 loss = 0.9394(0.4308)
2023/10/22 10:49:52 - INFO - root -   Epoch: [144/300][80/84], lr: 0.00000095 	 loss = 0.0267(0.4164)
2023/10/22 10:49:54 - INFO - root -   Epoch: [144/300] 	 loss = 0.4165
2023/10/22 10:50:52 - INFO - root -   precision = 0.8837
2023/10/22 10:50:52 - INFO - root -   eval_loss = 0.3481
2023/10/22 10:50:52 - INFO - root -   eval_acc = 0.8837
2023/10/22 10:50:53 - INFO - root -   train_accuracy = 0.7857
2023/10/22 10:51:15 - INFO - root -   Epoch: [145/300][0/84], lr: 0.00000095 	 loss = 0.8582(0.8582)
2023/10/22 10:52:17 - INFO - root -   Epoch: [145/300][20/84], lr: 0.00000095 	 loss = 0.1141(0.3573)
2023/10/22 10:53:30 - INFO - root -   Epoch: [145/300][40/84], lr: 0.00000095 	 loss = 0.0295(0.3758)
2023/10/22 10:54:29 - INFO - root -   Epoch: [145/300][60/84], lr: 0.00000095 	 loss = 0.5143(0.3631)
2023/10/22 10:55:26 - INFO - root -   Epoch: [145/300][80/84], lr: 0.00000095 	 loss = 0.0243(0.3427)
2023/10/22 10:55:27 - INFO - root -   Epoch: [145/300] 	 loss = 0.3531
2023/10/22 10:55:27 - INFO - root -   train_accuracy = 0.8214
2023/10/22 10:55:49 - INFO - root -   Epoch: [146/300][0/84], lr: 0.00000096 	 loss = 0.0940(0.0940)
2023/10/22 10:57:03 - INFO - root -   Epoch: [146/300][20/84], lr: 0.00000096 	 loss = 0.1894(0.5838)
2023/10/22 10:58:04 - INFO - root -   Epoch: [146/300][40/84], lr: 0.00000096 	 loss = 0.3296(0.4476)
2023/10/22 10:59:24 - INFO - root -   Epoch: [146/300][60/84], lr: 0.00000096 	 loss = 0.0357(0.4051)
2023/10/22 11:00:05 - INFO - root -   Epoch: [146/300][80/84], lr: 0.00000096 	 loss = 0.1455(0.3698)
2023/10/22 11:00:09 - INFO - root -   Epoch: [146/300] 	 loss = 0.3678
2023/10/22 11:00:09 - INFO - root -   train_accuracy = 0.7917
2023/10/22 11:00:30 - INFO - root -   Epoch: [147/300][0/84], lr: 0.00000097 	 loss = 0.0146(0.0146)
2023/10/22 11:01:37 - INFO - root -   Epoch: [147/300][20/84], lr: 0.00000097 	 loss = 0.1597(0.3785)
2023/10/22 11:02:49 - INFO - root -   Epoch: [147/300][40/84], lr: 0.00000097 	 loss = 0.4100(0.3238)
2023/10/22 11:03:54 - INFO - root -   Epoch: [147/300][60/84], lr: 0.00000097 	 loss = 0.0854(0.3395)
2023/10/22 11:04:55 - INFO - root -   Epoch: [147/300][80/84], lr: 0.00000097 	 loss = 0.0591(0.3208)
2023/10/22 11:04:56 - INFO - root -   Epoch: [147/300] 	 loss = 0.3202
2023/10/22 11:04:56 - INFO - root -   train_accuracy = 0.8274
2023/10/22 11:05:35 - INFO - root -   Epoch: [148/300][0/84], lr: 0.00000097 	 loss = 0.8384(0.8384)
2023/10/22 11:06:26 - INFO - root -   Epoch: [148/300][20/84], lr: 0.00000097 	 loss = 0.1420(0.3775)
2023/10/22 11:07:26 - INFO - root -   Epoch: [148/300][40/84], lr: 0.00000097 	 loss = 0.0957(0.4156)
2023/10/22 11:08:45 - INFO - root -   Epoch: [148/300][60/84], lr: 0.00000097 	 loss = 0.2390(0.3829)
2023/10/22 11:09:32 - INFO - root -   Epoch: [148/300][80/84], lr: 0.00000097 	 loss = 0.1001(0.4125)
2023/10/22 11:09:35 - INFO - root -   Epoch: [148/300] 	 loss = 0.4033
2023/10/22 11:09:35 - INFO - root -   train_accuracy = 0.7738
2023/10/22 11:10:05 - INFO - root -   Epoch: [149/300][0/84], lr: 0.00000098 	 loss = 0.6581(0.6581)
2023/10/22 11:11:03 - INFO - root -   Epoch: [149/300][20/84], lr: 0.00000098 	 loss = 0.0645(0.3298)
2023/10/22 11:12:03 - INFO - root -   Epoch: [149/300][40/84], lr: 0.00000098 	 loss = 0.0399(0.3704)
2023/10/22 11:13:01 - INFO - root -   Epoch: [149/300][60/84], lr: 0.00000098 	 loss = 0.1711(0.3187)
2023/10/22 11:13:58 - INFO - root -   Epoch: [149/300][80/84], lr: 0.00000098 	 loss = 0.0167(0.3532)
2023/10/22 11:13:59 - INFO - root -   Epoch: [149/300] 	 loss = 0.3518
2023/10/22 11:14:56 - INFO - root -   precision = 0.8837
2023/10/22 11:14:56 - INFO - root -   eval_loss = 0.4090
2023/10/22 11:14:56 - INFO - root -   eval_acc = 0.8837
2023/10/22 11:14:57 - INFO - root -   train_accuracy = 0.7976
2023/10/22 11:15:27 - INFO - root -   Epoch: [150/300][0/84], lr: 0.00000098 	 loss = 0.5541(0.5541)
2023/10/22 11:16:50 - INFO - root -   Epoch: [150/300][20/84], lr: 0.00000098 	 loss = 0.6981(0.3707)
2023/10/22 11:17:58 - INFO - root -   Epoch: [150/300][40/84], lr: 0.00000098 	 loss = 0.0709(0.3579)
2023/10/22 11:19:05 - INFO - root -   Epoch: [150/300][60/84], lr: 0.00000098 	 loss = 0.1407(0.3495)
2023/10/22 11:19:49 - INFO - root -   Epoch: [150/300][80/84], lr: 0.00000098 	 loss = 0.0744(0.3853)
2023/10/22 11:19:51 - INFO - root -   Epoch: [150/300] 	 loss = 0.3876
2023/10/22 11:19:51 - INFO - root -   train_accuracy = 0.8036
2023/10/22 11:20:13 - INFO - root -   Epoch: [151/300][0/84], lr: 0.00000099 	 loss = 0.0488(0.0488)
2023/10/22 11:21:28 - INFO - root -   Epoch: [151/300][20/84], lr: 0.00000099 	 loss = 0.3815(0.4406)
2023/10/22 11:22:50 - INFO - root -   Epoch: [151/300][40/84], lr: 0.00000099 	 loss = 0.2271(0.4503)
2023/10/22 11:23:53 - INFO - root -   Epoch: [151/300][60/84], lr: 0.00000099 	 loss = 0.0845(0.4601)
2023/10/22 11:24:41 - INFO - root -   Epoch: [151/300][80/84], lr: 0.00000099 	 loss = 0.2163(0.4200)
2023/10/22 11:24:42 - INFO - root -   Epoch: [151/300] 	 loss = 0.4188
2023/10/22 11:24:42 - INFO - root -   train_accuracy = 0.7679
2023/10/22 11:25:11 - INFO - root -   Epoch: [152/300][0/84], lr: 0.00000100 	 loss = 0.1741(0.1741)
2023/10/22 11:26:19 - INFO - root -   Epoch: [152/300][20/84], lr: 0.00000100 	 loss = 0.1269(0.4198)
2023/10/22 11:27:18 - INFO - root -   Epoch: [152/300][40/84], lr: 0.00000100 	 loss = 0.1488(0.4150)
2023/10/22 11:28:26 - INFO - root -   Epoch: [152/300][60/84], lr: 0.00000100 	 loss = 0.0133(0.3995)
2023/10/22 11:29:19 - INFO - root -   Epoch: [152/300][80/84], lr: 0.00000100 	 loss = 0.0507(0.3702)
2023/10/22 11:29:20 - INFO - root -   Epoch: [152/300] 	 loss = 0.3643
2023/10/22 11:29:20 - INFO - root -   train_accuracy = 0.8155
2023/10/22 11:29:42 - INFO - root -   Epoch: [153/300][0/84], lr: 0.00000100 	 loss = 0.0637(0.0637)
2023/10/22 11:30:48 - INFO - root -   Epoch: [153/300][20/84], lr: 0.00000100 	 loss = 0.2227(0.4301)
2023/10/22 11:31:59 - INFO - root -   Epoch: [153/300][40/84], lr: 0.00000100 	 loss = 0.0353(0.3786)
2023/10/22 11:33:19 - INFO - root -   Epoch: [153/300][60/84], lr: 0.00000100 	 loss = 0.4693(0.3607)
2023/10/22 11:34:03 - INFO - root -   Epoch: [153/300][80/84], lr: 0.00000100 	 loss = 1.5110(0.3612)
2023/10/22 11:34:04 - INFO - root -   Epoch: [153/300] 	 loss = 0.3534
2023/10/22 11:34:04 - INFO - root -   train_accuracy = 0.8214
2023/10/22 11:34:33 - INFO - root -   Epoch: [154/300][0/84], lr: 0.00000101 	 loss = 0.7450(0.7450)
2023/10/22 11:35:39 - INFO - root -   Epoch: [154/300][20/84], lr: 0.00000101 	 loss = 0.5323(0.3579)
2023/10/22 11:36:38 - INFO - root -   Epoch: [154/300][40/84], lr: 0.00000101 	 loss = 0.1728(0.3755)
2023/10/22 11:37:47 - INFO - root -   Epoch: [154/300][60/84], lr: 0.00000101 	 loss = 0.1728(0.3307)
2023/10/22 11:38:53 - INFO - root -   Epoch: [154/300][80/84], lr: 0.00000101 	 loss = 0.5338(0.3529)
2023/10/22 11:38:54 - INFO - root -   Epoch: [154/300] 	 loss = 0.3582
2023/10/22 11:39:51 - INFO - root -   precision = 0.8605
2023/10/22 11:39:51 - INFO - root -   eval_loss = 0.3010
2023/10/22 11:39:51 - INFO - root -   eval_acc = 0.8605
2023/10/22 11:39:52 - INFO - root -   train_accuracy = 0.8155
2023/10/22 11:40:21 - INFO - root -   Epoch: [155/300][0/84], lr: 0.00000101 	 loss = 0.5514(0.5514)
2023/10/22 11:41:27 - INFO - root -   Epoch: [155/300][20/84], lr: 0.00000101 	 loss = 0.1499(0.4167)
2023/10/22 11:42:30 - INFO - root -   Epoch: [155/300][40/84], lr: 0.00000101 	 loss = 0.0487(0.3196)
2023/10/22 11:43:40 - INFO - root -   Epoch: [155/300][60/84], lr: 0.00000101 	 loss = 1.8774(0.4177)
2023/10/22 11:44:20 - INFO - root -   Epoch: [155/300][80/84], lr: 0.00000101 	 loss = 0.9984(0.3961)
2023/10/22 11:44:23 - INFO - root -   Epoch: [155/300] 	 loss = 0.3864
2023/10/22 11:44:23 - INFO - root -   train_accuracy = 0.8333
2023/10/22 11:44:45 - INFO - root -   Epoch: [156/300][0/84], lr: 0.00000102 	 loss = 0.0225(0.0225)
2023/10/22 11:45:52 - INFO - root -   Epoch: [156/300][20/84], lr: 0.00000102 	 loss = 1.4632(0.5465)
2023/10/22 11:47:10 - INFO - root -   Epoch: [156/300][40/84], lr: 0.00000102 	 loss = 0.1941(0.4631)
2023/10/22 11:48:17 - INFO - root -   Epoch: [156/300][60/84], lr: 0.00000102 	 loss = 0.0851(0.4643)
2023/10/22 11:48:54 - INFO - root -   Epoch: [156/300][80/84], lr: 0.00000102 	 loss = 0.1456(0.4072)
2023/10/22 11:48:55 - INFO - root -   Epoch: [156/300] 	 loss = 0.4007
2023/10/22 11:48:55 - INFO - root -   train_accuracy = 0.8512
2023/10/22 11:49:17 - INFO - root -   Epoch: [157/300][0/84], lr: 0.00000103 	 loss = 0.0199(0.0199)
2023/10/22 11:50:24 - INFO - root -   Epoch: [157/300][20/84], lr: 0.00000103 	 loss = 0.0310(0.3677)
2023/10/22 11:51:26 - INFO - root -   Epoch: [157/300][40/84], lr: 0.00000103 	 loss = 0.6618(0.4136)
2023/10/22 11:52:48 - INFO - root -   Epoch: [157/300][60/84], lr: 0.00000103 	 loss = 0.3717(0.3792)
2023/10/22 11:53:40 - INFO - root -   Epoch: [157/300][80/84], lr: 0.00000103 	 loss = 0.4000(0.4162)
2023/10/22 11:53:46 - INFO - root -   Epoch: [157/300] 	 loss = 0.4073
2023/10/22 11:53:46 - INFO - root -   train_accuracy = 0.7798
2023/10/22 11:54:08 - INFO - root -   Epoch: [158/300][0/84], lr: 0.00000103 	 loss = 0.0623(0.0623)
2023/10/22 11:55:07 - INFO - root -   Epoch: [158/300][20/84], lr: 0.00000103 	 loss = 0.9865(0.4352)
2023/10/22 11:56:24 - INFO - root -   Epoch: [158/300][40/84], lr: 0.00000103 	 loss = 0.8105(0.4590)
2023/10/22 11:57:29 - INFO - root -   Epoch: [158/300][60/84], lr: 0.00000103 	 loss = 0.1039(0.4392)
2023/10/22 11:58:18 - INFO - root -   Epoch: [158/300][80/84], lr: 0.00000103 	 loss = 0.4504(0.4354)
2023/10/22 11:58:24 - INFO - root -   Epoch: [158/300] 	 loss = 0.4303
2023/10/22 11:58:24 - INFO - root -   train_accuracy = 0.7917
2023/10/22 11:58:45 - INFO - root -   Epoch: [159/300][0/84], lr: 0.00000104 	 loss = 0.0527(0.0527)
2023/10/22 11:59:59 - INFO - root -   Epoch: [159/300][20/84], lr: 0.00000104 	 loss = 0.1303(0.3710)
2023/10/22 12:01:10 - INFO - root -   Epoch: [159/300][40/84], lr: 0.00000104 	 loss = 0.3406(0.3987)
2023/10/22 12:02:15 - INFO - root -   Epoch: [159/300][60/84], lr: 0.00000104 	 loss = 0.0258(0.3622)
2023/10/22 12:03:16 - INFO - root -   Epoch: [159/300][80/84], lr: 0.00000104 	 loss = 0.1442(0.3830)
2023/10/22 12:03:17 - INFO - root -   Epoch: [159/300] 	 loss = 0.3849
2023/10/22 12:04:13 - INFO - root -   precision = 0.8837
2023/10/22 12:04:13 - INFO - root -   eval_loss = 0.3036
2023/10/22 12:04:13 - INFO - root -   eval_acc = 0.8837
2023/10/22 12:04:14 - INFO - root -   train_accuracy = 0.8155
2023/10/22 12:04:44 - INFO - root -   Epoch: [160/300][0/84], lr: 0.00000104 	 loss = 0.1008(0.1008)
2023/10/22 12:05:43 - INFO - root -   Epoch: [160/300][20/84], lr: 0.00000104 	 loss = 0.7357(0.3147)
2023/10/22 12:06:56 - INFO - root -   Epoch: [160/300][40/84], lr: 0.00000104 	 loss = 0.0344(0.3369)
2023/10/22 12:07:59 - INFO - root -   Epoch: [160/300][60/84], lr: 0.00000104 	 loss = 0.2278(0.3113)
2023/10/22 12:08:54 - INFO - root -   Epoch: [160/300][80/84], lr: 0.00000104 	 loss = 0.7919(0.3470)
2023/10/22 12:08:55 - INFO - root -   Epoch: [160/300] 	 loss = 0.3566
2023/10/22 12:08:55 - INFO - root -   train_accuracy = 0.8036
2023/10/22 12:09:33 - INFO - root -   Epoch: [161/300][0/84], lr: 0.00000105 	 loss = 0.7952(0.7952)
2023/10/22 12:10:25 - INFO - root -   Epoch: [161/300][20/84], lr: 0.00000105 	 loss = 0.3575(0.3940)
2023/10/22 12:11:40 - INFO - root -   Epoch: [161/300][40/84], lr: 0.00000105 	 loss = 0.5330(0.3560)
2023/10/22 12:12:41 - INFO - root -   Epoch: [161/300][60/84], lr: 0.00000105 	 loss = 0.0287(0.3394)
2023/10/22 12:13:33 - INFO - root -   Epoch: [161/300][80/84], lr: 0.00000105 	 loss = 0.1669(0.3330)
2023/10/22 12:13:36 - INFO - root -   Epoch: [161/300] 	 loss = 0.3297
2023/10/22 12:13:36 - INFO - root -   train_accuracy = 0.8333
2023/10/22 12:13:58 - INFO - root -   Epoch: [162/300][0/84], lr: 0.00000105 	 loss = 0.1017(0.1017)
2023/10/22 12:15:05 - INFO - root -   Epoch: [162/300][20/84], lr: 0.00000105 	 loss = 0.4593(0.3821)
2023/10/22 12:16:09 - INFO - root -   Epoch: [162/300][40/84], lr: 0.00000105 	 loss = 0.0174(0.4155)
2023/10/22 12:17:18 - INFO - root -   Epoch: [162/300][60/84], lr: 0.00000105 	 loss = 0.0799(0.4368)
2023/10/22 12:18:15 - INFO - root -   Epoch: [162/300][80/84], lr: 0.00000105 	 loss = 0.5841(0.4189)
2023/10/22 12:18:18 - INFO - root -   Epoch: [162/300] 	 loss = 0.4141
2023/10/22 12:18:18 - INFO - root -   train_accuracy = 0.7738
2023/10/22 12:18:39 - INFO - root -   Epoch: [163/300][0/84], lr: 0.00000106 	 loss = 0.0649(0.0649)
2023/10/22 12:19:52 - INFO - root -   Epoch: [163/300][20/84], lr: 0.00000106 	 loss = 0.2785(0.2827)
2023/10/22 12:20:53 - INFO - root -   Epoch: [163/300][40/84], lr: 0.00000106 	 loss = 0.0086(0.3635)
2023/10/22 12:21:59 - INFO - root -   Epoch: [163/300][60/84], lr: 0.00000106 	 loss = 0.4370(0.3603)
2023/10/22 12:22:49 - INFO - root -   Epoch: [163/300][80/84], lr: 0.00000106 	 loss = 0.0798(0.3616)
2023/10/22 12:22:53 - INFO - root -   Epoch: [163/300] 	 loss = 0.3571
2023/10/22 12:22:53 - INFO - root -   train_accuracy = 0.8214
2023/10/22 12:23:14 - INFO - root -   Epoch: [164/300][0/84], lr: 0.00000107 	 loss = 0.0155(0.0155)
2023/10/22 12:24:27 - INFO - root -   Epoch: [164/300][20/84], lr: 0.00000107 	 loss = 0.0852(0.3034)
2023/10/22 12:25:41 - INFO - root -   Epoch: [164/300][40/84], lr: 0.00000107 	 loss = 0.0246(0.3807)
2023/10/22 12:26:39 - INFO - root -   Epoch: [164/300][60/84], lr: 0.00000107 	 loss = 0.7572(0.3923)
2023/10/22 12:27:25 - INFO - root -   Epoch: [164/300][80/84], lr: 0.00000107 	 loss = 0.7565(0.4123)
2023/10/22 12:27:29 - INFO - root -   Epoch: [164/300] 	 loss = 0.4056
2023/10/22 12:28:26 - INFO - root -   precision = 0.8605
2023/10/22 12:28:26 - INFO - root -   eval_loss = 0.3156
2023/10/22 12:28:26 - INFO - root -   eval_acc = 0.8605
2023/10/22 12:28:27 - INFO - root -   train_accuracy = 0.7619
2023/10/22 12:28:56 - INFO - root -   Epoch: [165/300][0/84], lr: 0.00000107 	 loss = 0.2928(0.2928)
2023/10/22 12:29:49 - INFO - root -   Epoch: [165/300][20/84], lr: 0.00000107 	 loss = 0.1651(0.4281)
2023/10/22 12:31:05 - INFO - root -   Epoch: [165/300][40/84], lr: 0.00000107 	 loss = 0.4188(0.4203)
2023/10/22 12:32:18 - INFO - root -   Epoch: [165/300][60/84], lr: 0.00000107 	 loss = 0.6775(0.4338)
2023/10/22 12:33:17 - INFO - root -   Epoch: [165/300][80/84], lr: 0.00000107 	 loss = 0.0859(0.4721)
2023/10/22 12:33:19 - INFO - root -   Epoch: [165/300] 	 loss = 0.4682
2023/10/22 12:33:19 - INFO - root -   train_accuracy = 0.7500
2023/10/22 12:33:48 - INFO - root -   Epoch: [166/300][0/84], lr: 0.00000108 	 loss = 0.0520(0.0520)
2023/10/22 12:34:49 - INFO - root -   Epoch: [166/300][20/84], lr: 0.00000108 	 loss = 2.5333(0.5307)
2023/10/22 12:35:58 - INFO - root -   Epoch: [166/300][40/84], lr: 0.00000108 	 loss = 0.0391(0.4885)
2023/10/22 12:37:20 - INFO - root -   Epoch: [166/300][60/84], lr: 0.00000108 	 loss = 0.0782(0.4303)
2023/10/22 12:38:03 - INFO - root -   Epoch: [166/300][80/84], lr: 0.00000108 	 loss = 0.4358(0.4181)
2023/10/22 12:38:07 - INFO - root -   Epoch: [166/300] 	 loss = 0.4121
2023/10/22 12:38:07 - INFO - root -   train_accuracy = 0.7857
2023/10/22 12:38:46 - INFO - root -   Epoch: [167/300][0/84], lr: 0.00000108 	 loss = 0.3304(0.3304)
2023/10/22 12:39:43 - INFO - root -   Epoch: [167/300][20/84], lr: 0.00000108 	 loss = 0.1057(0.3494)
2023/10/22 12:40:39 - INFO - root -   Epoch: [167/300][40/84], lr: 0.00000108 	 loss = 0.0981(0.3318)
2023/10/22 12:41:46 - INFO - root -   Epoch: [167/300][60/84], lr: 0.00000108 	 loss = 0.2315(0.3069)
2023/10/22 12:42:42 - INFO - root -   Epoch: [167/300][80/84], lr: 0.00000108 	 loss = 0.6399(0.3381)
2023/10/22 12:42:43 - INFO - root -   Epoch: [167/300] 	 loss = 0.3361
2023/10/22 12:42:43 - INFO - root -   train_accuracy = 0.8214
2023/10/22 12:43:06 - INFO - root -   Epoch: [168/300][0/84], lr: 0.00000109 	 loss = 0.0282(0.0282)
2023/10/22 12:44:12 - INFO - root -   Epoch: [168/300][20/84], lr: 0.00000109 	 loss = 0.2169(0.4460)
2023/10/22 12:45:30 - INFO - root -   Epoch: [168/300][40/84], lr: 0.00000109 	 loss = 0.8479(0.4176)
2023/10/22 12:46:37 - INFO - root -   Epoch: [168/300][60/84], lr: 0.00000109 	 loss = 0.7845(0.4577)
2023/10/22 12:47:19 - INFO - root -   Epoch: [168/300][80/84], lr: 0.00000109 	 loss = 0.1325(0.4242)
2023/10/22 12:47:20 - INFO - root -   Epoch: [168/300] 	 loss = 0.4196
2023/10/22 12:47:20 - INFO - root -   train_accuracy = 0.8036
2023/10/22 12:47:42 - INFO - root -   Epoch: [169/300][0/84], lr: 0.00000110 	 loss = 0.0091(0.0091)
2023/10/22 12:48:50 - INFO - root -   Epoch: [169/300][20/84], lr: 0.00000110 	 loss = 2.4673(0.3885)
2023/10/22 12:49:49 - INFO - root -   Epoch: [169/300][40/84], lr: 0.00000110 	 loss = 0.3040(0.3418)
2023/10/22 12:51:05 - INFO - root -   Epoch: [169/300][60/84], lr: 0.00000110 	 loss = 0.3580(0.3686)
2023/10/22 12:51:53 - INFO - root -   Epoch: [169/300][80/84], lr: 0.00000110 	 loss = 0.3692(0.3588)
2023/10/22 12:51:54 - INFO - root -   Epoch: [169/300] 	 loss = 0.3526
2023/10/22 12:52:51 - INFO - root -   precision = 0.8605
2023/10/22 12:52:51 - INFO - root -   eval_loss = 0.2846
2023/10/22 12:52:51 - INFO - root -   eval_acc = 0.8605
2023/10/22 12:52:52 - INFO - root -   train_accuracy = 0.8750
2023/10/22 12:53:22 - INFO - root -   Epoch: [170/300][0/84], lr: 0.00000110 	 loss = 0.2662(0.2662)
2023/10/22 12:54:30 - INFO - root -   Epoch: [170/300][20/84], lr: 0.00000110 	 loss = 0.0280(0.4559)
2023/10/22 12:55:37 - INFO - root -   Epoch: [170/300][40/84], lr: 0.00000110 	 loss = 0.0262(0.3480)
2023/10/22 12:56:44 - INFO - root -   Epoch: [170/300][60/84], lr: 0.00000110 	 loss = 0.0190(0.3289)
2023/10/22 12:57:35 - INFO - root -   Epoch: [170/300][80/84], lr: 0.00000110 	 loss = 0.0552(0.3410)
2023/10/22 12:57:39 - INFO - root -   Epoch: [170/300] 	 loss = 0.3389
2023/10/22 12:57:39 - INFO - root -   train_accuracy = 0.7917
2023/10/22 12:58:01 - INFO - root -   Epoch: [171/300][0/84], lr: 0.00000111 	 loss = 0.0040(0.0040)
2023/10/22 12:59:15 - INFO - root -   Epoch: [171/300][20/84], lr: 0.00000111 	 loss = 0.0789(0.2733)
2023/10/22 13:00:16 - INFO - root -   Epoch: [171/300][40/84], lr: 0.00000111 	 loss = 0.0927(0.3346)
2023/10/22 13:01:23 - INFO - root -   Epoch: [171/300][60/84], lr: 0.00000111 	 loss = 0.0695(0.3217)
2023/10/22 13:02:22 - INFO - root -   Epoch: [171/300][80/84], lr: 0.00000111 	 loss = 0.8546(0.3796)
2023/10/22 13:02:23 - INFO - root -   Epoch: [171/300] 	 loss = 0.3716
2023/10/22 13:02:23 - INFO - root -   train_accuracy = 0.8155
2023/10/22 13:02:45 - INFO - root -   Epoch: [172/300][0/84], lr: 0.00000111 	 loss = 0.0595(0.0595)
2023/10/22 13:03:58 - INFO - root -   Epoch: [172/300][20/84], lr: 0.00000111 	 loss = 0.0428(0.2873)
2023/10/22 13:05:12 - INFO - root -   Epoch: [172/300][40/84], lr: 0.00000111 	 loss = 0.8179(0.3432)
2023/10/22 13:06:01 - INFO - root -   Epoch: [172/300][60/84], lr: 0.00000111 	 loss = 0.5278(0.3364)
2023/10/22 13:06:56 - INFO - root -   Epoch: [172/300][80/84], lr: 0.00000111 	 loss = 0.0319(0.3555)
2023/10/22 13:06:58 - INFO - root -   Epoch: [172/300] 	 loss = 0.3483
2023/10/22 13:06:58 - INFO - root -   train_accuracy = 0.8036
2023/10/22 13:07:20 - INFO - root -   Epoch: [173/300][0/84], lr: 0.00000112 	 loss = 0.0230(0.0230)
2023/10/22 13:08:33 - INFO - root -   Epoch: [173/300][20/84], lr: 0.00000112 	 loss = 0.2120(0.4020)
2023/10/22 13:09:32 - INFO - root -   Epoch: [173/300][40/84], lr: 0.00000112 	 loss = 0.5977(0.4076)
2023/10/22 13:10:38 - INFO - root -   Epoch: [173/300][60/84], lr: 0.00000112 	 loss = 0.3180(0.3987)
2023/10/22 13:11:29 - INFO - root -   Epoch: [173/300][80/84], lr: 0.00000112 	 loss = 1.3769(0.3835)
2023/10/22 13:11:30 - INFO - root -   Epoch: [173/300] 	 loss = 0.3751
2023/10/22 13:11:30 - INFO - root -   train_accuracy = 0.8571
2023/10/22 13:11:52 - INFO - root -   Epoch: [174/300][0/84], lr: 0.00000113 	 loss = 0.0210(0.0210)
2023/10/22 13:13:04 - INFO - root -   Epoch: [174/300][20/84], lr: 0.00000113 	 loss = 0.4789(0.3154)
2023/10/22 13:14:00 - INFO - root -   Epoch: [174/300][40/84], lr: 0.00000113 	 loss = 0.0790(0.3346)
2023/10/22 13:15:08 - INFO - root -   Epoch: [174/300][60/84], lr: 0.00000113 	 loss = 0.0597(0.2908)
2023/10/22 13:16:00 - INFO - root -   Epoch: [174/300][80/84], lr: 0.00000113 	 loss = 0.5589(0.2957)
2023/10/22 13:16:04 - INFO - root -   Epoch: [174/300] 	 loss = 0.2981
2023/10/22 13:17:00 - INFO - root -   precision = 0.8605
2023/10/22 13:17:00 - INFO - root -   eval_loss = 0.4040
2023/10/22 13:17:00 - INFO - root -   eval_acc = 0.8605
2023/10/22 13:17:01 - INFO - root -   train_accuracy = 0.8631
2023/10/22 13:17:24 - INFO - root -   Epoch: [175/300][0/84], lr: 0.00000113 	 loss = 0.0646(0.0646)
2023/10/22 13:18:46 - INFO - root -   Epoch: [175/300][20/84], lr: 0.00000113 	 loss = 0.8612(0.2843)
2023/10/22 13:19:29 - INFO - root -   Epoch: [175/300][40/84], lr: 0.00000113 	 loss = 0.0182(0.2619)
2023/10/22 13:20:41 - INFO - root -   Epoch: [175/300][60/84], lr: 0.00000113 	 loss = 0.2734(0.3188)
2023/10/22 13:21:34 - INFO - root -   Epoch: [175/300][80/84], lr: 0.00000113 	 loss = 0.1101(0.3436)
2023/10/22 13:21:36 - INFO - root -   Epoch: [175/300] 	 loss = 0.3410
2023/10/22 13:21:36 - INFO - root -   train_accuracy = 0.8333
2023/10/22 13:22:17 - INFO - root -   Epoch: [176/300][0/84], lr: 0.00000114 	 loss = 0.0665(0.0665)
2023/10/22 13:23:19 - INFO - root -   Epoch: [176/300][20/84], lr: 0.00000114 	 loss = 1.1855(0.3185)
2023/10/22 13:24:33 - INFO - root -   Epoch: [176/300][40/84], lr: 0.00000114 	 loss = 0.0102(0.3619)
2023/10/22 13:25:22 - INFO - root -   Epoch: [176/300][60/84], lr: 0.00000114 	 loss = 0.1019(0.3710)
2023/10/22 13:26:25 - INFO - root -   Epoch: [176/300][80/84], lr: 0.00000114 	 loss = 0.0397(0.3664)
2023/10/22 13:26:26 - INFO - root -   Epoch: [176/300] 	 loss = 0.3638
2023/10/22 13:26:26 - INFO - root -   train_accuracy = 0.8214
2023/10/22 13:26:48 - INFO - root -   Epoch: [177/300][0/84], lr: 0.00000114 	 loss = 0.0160(0.0160)
2023/10/22 13:27:55 - INFO - root -   Epoch: [177/300][20/84], lr: 0.00000114 	 loss = 0.1286(0.4313)
2023/10/22 13:28:58 - INFO - root -   Epoch: [177/300][40/84], lr: 0.00000114 	 loss = 0.0298(0.3806)
2023/10/22 13:30:22 - INFO - root -   Epoch: [177/300][60/84], lr: 0.00000114 	 loss = 0.1149(0.4363)
2023/10/22 13:31:06 - INFO - root -   Epoch: [177/300][80/84], lr: 0.00000114 	 loss = 0.1284(0.4340)
2023/10/22 13:31:09 - INFO - root -   Epoch: [177/300] 	 loss = 0.4294
2023/10/22 13:31:09 - INFO - root -   train_accuracy = 0.7560
2023/10/22 13:31:38 - INFO - root -   Epoch: [178/300][0/84], lr: 0.00000115 	 loss = 0.0180(0.0180)
2023/10/22 13:32:31 - INFO - root -   Epoch: [178/300][20/84], lr: 0.00000115 	 loss = 0.5200(0.3625)
2023/10/22 13:33:45 - INFO - root -   Epoch: [178/300][40/84], lr: 0.00000115 	 loss = 0.0411(0.3960)
2023/10/22 13:34:58 - INFO - root -   Epoch: [178/300][60/84], lr: 0.00000115 	 loss = 0.4093(0.3633)
2023/10/22 13:35:47 - INFO - root -   Epoch: [178/300][80/84], lr: 0.00000115 	 loss = 0.2588(0.3491)
2023/10/22 13:35:49 - INFO - root -   Epoch: [178/300] 	 loss = 0.3480
2023/10/22 13:35:49 - INFO - root -   train_accuracy = 0.8155
2023/10/22 13:36:27 - INFO - root -   Epoch: [179/300][0/84], lr: 0.00000115 	 loss = 0.6187(0.6187)
2023/10/22 13:37:25 - INFO - root -   Epoch: [179/300][20/84], lr: 0.00000115 	 loss = 0.3291(0.3377)
2023/10/22 13:38:34 - INFO - root -   Epoch: [179/300][40/84], lr: 0.00000115 	 loss = 0.0643(0.3114)
2023/10/22 13:39:45 - INFO - root -   Epoch: [179/300][60/84], lr: 0.00000115 	 loss = 0.0406(0.3408)
2023/10/22 13:40:44 - INFO - root -   Epoch: [179/300][80/84], lr: 0.00000115 	 loss = 0.5371(0.3562)
2023/10/22 13:40:45 - INFO - root -   Epoch: [179/300] 	 loss = 0.3439
2023/10/22 13:41:42 - INFO - root -   precision = 0.8605
2023/10/22 13:41:42 - INFO - root -   eval_loss = 0.2846
2023/10/22 13:41:42 - INFO - root -   eval_acc = 0.8605
2023/10/22 13:41:43 - INFO - root -   train_accuracy = 0.8095
2023/10/22 13:42:14 - INFO - root -   Epoch: [180/300][0/84], lr: 0.00000116 	 loss = 0.5618(0.5618)
2023/10/22 13:43:15 - INFO - root -   Epoch: [180/300][20/84], lr: 0.00000116 	 loss = 0.1822(0.4696)
2023/10/22 13:44:31 - INFO - root -   Epoch: [180/300][40/84], lr: 0.00000116 	 loss = 0.0084(0.4087)
2023/10/22 13:45:27 - INFO - root -   Epoch: [180/300][60/84], lr: 0.00000116 	 loss = 0.1611(0.4076)
2023/10/22 13:46:19 - INFO - root -   Epoch: [180/300][80/84], lr: 0.00000116 	 loss = 0.0787(0.3720)
2023/10/22 13:46:21 - INFO - root -   Epoch: [180/300] 	 loss = 0.3662
2023/10/22 13:46:21 - INFO - root -   train_accuracy = 0.8095
2023/10/22 13:46:59 - INFO - root -   Epoch: [181/300][0/84], lr: 0.00000117 	 loss = 1.6294(1.6294)
2023/10/22 13:47:51 - INFO - root -   Epoch: [181/300][20/84], lr: 0.00000117 	 loss = 0.0884(0.3299)
2023/10/22 13:49:09 - INFO - root -   Epoch: [181/300][40/84], lr: 0.00000117 	 loss = 0.4282(0.3485)
2023/10/22 13:50:00 - INFO - root -   Epoch: [181/300][60/84], lr: 0.00000117 	 loss = 0.0543(0.4004)
2023/10/22 13:50:55 - INFO - root -   Epoch: [181/300][80/84], lr: 0.00000117 	 loss = 0.1530(0.3858)
2023/10/22 13:50:57 - INFO - root -   Epoch: [181/300] 	 loss = 0.3852
2023/10/22 13:50:57 - INFO - root -   train_accuracy = 0.8155
2023/10/22 13:51:19 - INFO - root -   Epoch: [182/300][0/84], lr: 0.00000117 	 loss = 0.3184(0.3184)
2023/10/22 13:52:26 - INFO - root -   Epoch: [182/300][20/84], lr: 0.00000117 	 loss = 0.2352(0.3073)
2023/10/22 13:53:34 - INFO - root -   Epoch: [182/300][40/84], lr: 0.00000117 	 loss = 0.0779(0.3103)
2023/10/22 13:54:53 - INFO - root -   Epoch: [182/300][60/84], lr: 0.00000117 	 loss = 0.1045(0.2876)
2023/10/22 13:55:33 - INFO - root -   Epoch: [182/300][80/84], lr: 0.00000117 	 loss = 0.7457(0.3010)
2023/10/22 13:55:37 - INFO - root -   Epoch: [182/300] 	 loss = 0.3054
2023/10/22 13:55:37 - INFO - root -   train_accuracy = 0.8690
2023/10/22 13:55:59 - INFO - root -   Epoch: [183/300][0/84], lr: 0.00000118 	 loss = 0.0119(0.0119)
2023/10/22 13:57:13 - INFO - root -   Epoch: [183/300][20/84], lr: 0.00000118 	 loss = 0.1299(0.2691)
2023/10/22 13:58:26 - INFO - root -   Epoch: [183/300][40/84], lr: 0.00000118 	 loss = 0.0831(0.3246)
2023/10/22 13:59:49 - INFO - root -   Epoch: [183/300][60/84], lr: 0.00000118 	 loss = 0.2243(0.3630)
2023/10/22 14:00:25 - INFO - root -   Epoch: [183/300][80/84], lr: 0.00000118 	 loss = 0.0155(0.3386)
2023/10/22 14:00:27 - INFO - root -   Epoch: [183/300] 	 loss = 0.3291
2023/10/22 14:00:27 - INFO - root -   train_accuracy = 0.8214
2023/10/22 14:01:03 - INFO - root -   Epoch: [184/300][0/84], lr: 0.00000118 	 loss = 0.2076(0.2076)
2023/10/22 14:02:05 - INFO - root -   Epoch: [184/300][20/84], lr: 0.00000118 	 loss = 0.0276(0.2596)
2023/10/22 14:03:20 - INFO - root -   Epoch: [184/300][40/84], lr: 0.00000118 	 loss = 0.5743(0.3066)
2023/10/22 14:04:15 - INFO - root -   Epoch: [184/300][60/84], lr: 0.00000118 	 loss = 0.0677(0.2803)
2023/10/22 14:05:00 - INFO - root -   Epoch: [184/300][80/84], lr: 0.00000118 	 loss = 1.8635(0.3050)
2023/10/22 14:05:01 - INFO - root -   Epoch: [184/300] 	 loss = 0.2986
2023/10/22 14:05:58 - INFO - root -   precision = 0.8372
2023/10/22 14:05:58 - INFO - root -   eval_loss = 0.3220
2023/10/22 14:05:58 - INFO - root -   eval_acc = 0.8372
2023/10/22 14:05:59 - INFO - root -   train_accuracy = 0.8393
2023/10/22 14:06:21 - INFO - root -   Epoch: [185/300][0/84], lr: 0.00000119 	 loss = 0.0125(0.0125)
2023/10/22 14:07:40 - INFO - root -   Epoch: [185/300][20/84], lr: 0.00000119 	 loss = 0.2744(0.3563)
2023/10/22 14:08:48 - INFO - root -   Epoch: [185/300][40/84], lr: 0.00000119 	 loss = 0.0632(0.3802)
2023/10/22 14:09:42 - INFO - root -   Epoch: [185/300][60/84], lr: 0.00000119 	 loss = 0.1247(0.3797)
2023/10/22 14:10:36 - INFO - root -   Epoch: [185/300][80/84], lr: 0.00000119 	 loss = 0.3025(0.3916)
2023/10/22 14:10:37 - INFO - root -   Epoch: [185/300] 	 loss = 0.3821
2023/10/22 14:10:37 - INFO - root -   train_accuracy = 0.8214
2023/10/22 14:10:59 - INFO - root -   Epoch: [186/300][0/84], lr: 0.00000120 	 loss = 0.1443(0.1443)
2023/10/22 14:12:07 - INFO - root -   Epoch: [186/300][20/84], lr: 0.00000120 	 loss = 2.0218(0.3654)
2023/10/22 14:13:07 - INFO - root -   Epoch: [186/300][40/84], lr: 0.00000120 	 loss = 0.3088(0.3214)
2023/10/22 14:14:19 - INFO - root -   Epoch: [186/300][60/84], lr: 0.00000120 	 loss = 0.2968(0.3436)
2023/10/22 14:15:05 - INFO - root -   Epoch: [186/300][80/84], lr: 0.00000120 	 loss = 0.0352(0.3253)
2023/10/22 14:15:07 - INFO - root -   Epoch: [186/300] 	 loss = 0.3331
2023/10/22 14:15:07 - INFO - root -   train_accuracy = 0.8452
2023/10/22 14:15:29 - INFO - root -   Epoch: [187/300][0/84], lr: 0.00000120 	 loss = 0.0029(0.0029)
2023/10/22 14:16:44 - INFO - root -   Epoch: [187/300][20/84], lr: 0.00000120 	 loss = 0.3580(0.3154)
2023/10/22 14:17:42 - INFO - root -   Epoch: [187/300][40/84], lr: 0.00000120 	 loss = 0.0725(0.3261)
2023/10/22 14:18:50 - INFO - root -   Epoch: [187/300][60/84], lr: 0.00000120 	 loss = 0.0190(0.3199)
2023/10/22 14:19:43 - INFO - root -   Epoch: [187/300][80/84], lr: 0.00000120 	 loss = 0.9814(0.3311)
2023/10/22 14:19:44 - INFO - root -   Epoch: [187/300] 	 loss = 0.3257
2023/10/22 14:19:44 - INFO - root -   train_accuracy = 0.8333
2023/10/22 14:20:13 - INFO - root -   Epoch: [188/300][0/84], lr: 0.00000121 	 loss = 0.4459(0.4459)
2023/10/22 14:21:19 - INFO - root -   Epoch: [188/300][20/84], lr: 0.00000121 	 loss = 0.0976(0.4093)
2023/10/22 14:22:20 - INFO - root -   Epoch: [188/300][40/84], lr: 0.00000121 	 loss = 0.0080(0.3569)
2023/10/22 14:23:16 - INFO - root -   Epoch: [188/300][60/84], lr: 0.00000121 	 loss = 0.1822(0.3537)
2023/10/22 14:24:21 - INFO - root -   Epoch: [188/300][80/84], lr: 0.00000121 	 loss = 0.0091(0.3358)
2023/10/22 14:24:22 - INFO - root -   Epoch: [188/300] 	 loss = 0.3296
2023/10/22 14:24:22 - INFO - root -   train_accuracy = 0.8155
2023/10/22 14:24:52 - INFO - root -   Epoch: [189/300][0/84], lr: 0.00000121 	 loss = 0.1447(0.1447)
2023/10/22 14:26:06 - INFO - root -   Epoch: [189/300][20/84], lr: 0.00000121 	 loss = 0.2151(0.4429)
2023/10/22 14:27:16 - INFO - root -   Epoch: [189/300][40/84], lr: 0.00000121 	 loss = 0.1598(0.3693)
2023/10/22 14:28:05 - INFO - root -   Epoch: [189/300][60/84], lr: 0.00000121 	 loss = 0.4326(0.3522)
2023/10/22 14:29:02 - INFO - root -   Epoch: [189/300][80/84], lr: 0.00000121 	 loss = 0.0220(0.3489)
2023/10/22 14:29:03 - INFO - root -   Epoch: [189/300] 	 loss = 0.3399
2023/10/22 14:29:59 - INFO - root -   precision = 0.8605
2023/10/22 14:29:59 - INFO - root -   eval_loss = 0.3448
2023/10/22 14:29:59 - INFO - root -   eval_acc = 0.8605
2023/10/22 14:30:00 - INFO - root -   train_accuracy = 0.8274
2023/10/22 14:30:32 - INFO - root -   Epoch: [190/300][0/84], lr: 0.00000122 	 loss = 0.3866(0.3866)
2023/10/22 14:31:32 - INFO - root -   Epoch: [190/300][20/84], lr: 0.00000122 	 loss = 0.1426(0.3506)
2023/10/22 14:32:46 - INFO - root -   Epoch: [190/300][40/84], lr: 0.00000122 	 loss = 0.3141(0.3425)
2023/10/22 14:33:51 - INFO - root -   Epoch: [190/300][60/84], lr: 0.00000122 	 loss = 0.0138(0.3284)
2023/10/22 14:34:30 - INFO - root -   Epoch: [190/300][80/84], lr: 0.00000122 	 loss = 0.0288(0.3556)
2023/10/22 14:34:34 - INFO - root -   Epoch: [190/300] 	 loss = 0.3511
2023/10/22 14:34:34 - INFO - root -   train_accuracy = 0.7976
2023/10/22 14:35:06 - INFO - root -   Epoch: [191/300][0/84], lr: 0.00000123 	 loss = 0.2214(0.2214)
2023/10/22 14:35:57 - INFO - root -   Epoch: [191/300][20/84], lr: 0.00000123 	 loss = 2.4012(0.3610)
2023/10/22 14:37:14 - INFO - root -   Epoch: [191/300][40/84], lr: 0.00000123 	 loss = 0.2777(0.3314)
2023/10/22 14:38:21 - INFO - root -   Epoch: [191/300][60/84], lr: 0.00000123 	 loss = 0.0649(0.3251)
2023/10/22 14:39:16 - INFO - root -   Epoch: [191/300][80/84], lr: 0.00000123 	 loss = 0.0540(0.3262)
2023/10/22 14:39:17 - INFO - root -   Epoch: [191/300] 	 loss = 0.3417
2023/10/22 14:39:17 - INFO - root -   train_accuracy = 0.8155
2023/10/22 14:39:40 - INFO - root -   Epoch: [192/300][0/84], lr: 0.00000123 	 loss = 0.0304(0.0304)
2023/10/22 14:40:44 - INFO - root -   Epoch: [192/300][20/84], lr: 0.00000123 	 loss = 0.8661(0.2592)
2023/10/22 14:42:07 - INFO - root -   Epoch: [192/300][40/84], lr: 0.00000123 	 loss = 0.0186(0.3462)
2023/10/22 14:42:57 - INFO - root -   Epoch: [192/300][60/84], lr: 0.00000123 	 loss = 0.0958(0.3101)
2023/10/22 14:43:52 - INFO - root -   Epoch: [192/300][80/84], lr: 0.00000123 	 loss = 0.1375(0.3694)
2023/10/22 14:43:54 - INFO - root -   Epoch: [192/300] 	 loss = 0.3693
2023/10/22 14:43:54 - INFO - root -   train_accuracy = 0.8274
2023/10/22 14:44:23 - INFO - root -   Epoch: [193/300][0/84], lr: 0.00000124 	 loss = 0.1637(0.1637)
2023/10/22 14:45:36 - INFO - root -   Epoch: [193/300][20/84], lr: 0.00000124 	 loss = 0.0505(0.2707)
2023/10/22 14:46:35 - INFO - root -   Epoch: [193/300][40/84], lr: 0.00000124 	 loss = 0.0029(0.2605)
2023/10/22 14:47:52 - INFO - root -   Epoch: [193/300][60/84], lr: 0.00000124 	 loss = 0.4492(0.2871)
2023/10/22 14:48:26 - INFO - root -   Epoch: [193/300][80/84], lr: 0.00000124 	 loss = 0.0214(0.2826)
2023/10/22 14:48:27 - INFO - root -   Epoch: [193/300] 	 loss = 0.2767
2023/10/22 14:48:27 - INFO - root -   train_accuracy = 0.8333
2023/10/22 14:48:48 - INFO - root -   Epoch: [194/300][0/84], lr: 0.00000124 	 loss = 0.0466(0.0466)
2023/10/22 14:49:55 - INFO - root -   Epoch: [194/300][20/84], lr: 0.00000124 	 loss = 0.9008(0.2810)
2023/10/22 14:51:20 - INFO - root -   Epoch: [194/300][40/84], lr: 0.00000124 	 loss = 0.1973(0.3411)
2023/10/22 14:52:15 - INFO - root -   Epoch: [194/300][60/84], lr: 0.00000124 	 loss = 0.0471(0.3468)
2023/10/22 14:53:04 - INFO - root -   Epoch: [194/300][80/84], lr: 0.00000124 	 loss = 0.4401(0.3736)
2023/10/22 14:53:05 - INFO - root -   Epoch: [194/300] 	 loss = 0.3615
2023/10/22 14:54:02 - INFO - root -   precision = 0.8837
2023/10/22 14:54:02 - INFO - root -   eval_loss = 0.3433
2023/10/22 14:54:02 - INFO - root -   eval_acc = 0.8837
2023/10/22 14:54:03 - INFO - root -   train_accuracy = 0.8095
2023/10/22 14:54:35 - INFO - root -   Epoch: [195/300][0/84], lr: 0.00000125 	 loss = 0.3020(0.3020)
2023/10/22 14:55:32 - INFO - root -   Epoch: [195/300][20/84], lr: 0.00000125 	 loss = 0.0342(0.2622)
2023/10/22 14:56:48 - INFO - root -   Epoch: [195/300][40/84], lr: 0.00000125 	 loss = 0.6472(0.4047)
2023/10/22 14:57:51 - INFO - root -   Epoch: [195/300][60/84], lr: 0.00000125 	 loss = 0.2236(0.4097)
2023/10/22 14:58:45 - INFO - root -   Epoch: [195/300][80/84], lr: 0.00000125 	 loss = 0.0876(0.3938)
2023/10/22 14:58:46 - INFO - root -   Epoch: [195/300] 	 loss = 0.3859
2023/10/22 14:58:46 - INFO - root -   train_accuracy = 0.8214
2023/10/22 14:59:16 - INFO - root -   Epoch: [196/300][0/84], lr: 0.00000126 	 loss = 0.4806(0.4806)
2023/10/22 15:00:31 - INFO - root -   Epoch: [196/300][20/84], lr: 0.00000126 	 loss = 0.0787(0.4192)
2023/10/22 15:01:31 - INFO - root -   Epoch: [196/300][40/84], lr: 0.00000126 	 loss = 0.0147(0.4032)
2023/10/22 15:03:01 - INFO - root -   Epoch: [196/300][60/84], lr: 0.00000126 	 loss = 0.0293(0.3695)
2023/10/22 15:03:31 - INFO - root -   Epoch: [196/300][80/84], lr: 0.00000126 	 loss = 0.0365(0.3564)
2023/10/22 15:03:37 - INFO - root -   Epoch: [196/300] 	 loss = 0.3567
2023/10/22 15:03:37 - INFO - root -   train_accuracy = 0.8095
2023/10/22 15:03:59 - INFO - root -   Epoch: [197/300][0/84], lr: 0.00000126 	 loss = 0.0138(0.0138)
2023/10/22 15:05:12 - INFO - root -   Epoch: [197/300][20/84], lr: 0.00000126 	 loss = 0.0194(0.3021)
2023/10/22 15:06:33 - INFO - root -   Epoch: [197/300][40/84], lr: 0.00000126 	 loss = 0.8004(0.3959)
2023/10/22 15:07:21 - INFO - root -   Epoch: [197/300][60/84], lr: 0.00000126 	 loss = 0.1482(0.3669)
2023/10/22 15:08:21 - INFO - root -   Epoch: [197/300][80/84], lr: 0.00000126 	 loss = 0.7590(0.3343)
2023/10/22 15:08:22 - INFO - root -   Epoch: [197/300] 	 loss = 0.3418
2023/10/22 15:08:22 - INFO - root -   train_accuracy = 0.8333
2023/10/22 15:08:44 - INFO - root -   Epoch: [198/300][0/84], lr: 0.00000127 	 loss = 0.0267(0.0267)
2023/10/22 15:09:50 - INFO - root -   Epoch: [198/300][20/84], lr: 0.00000127 	 loss = 0.0544(0.3617)
2023/10/22 15:11:16 - INFO - root -   Epoch: [198/300][40/84], lr: 0.00000127 	 loss = 0.7054(0.3393)
2023/10/22 15:12:09 - INFO - root -   Epoch: [198/300][60/84], lr: 0.00000127 	 loss = 0.0212(0.3183)
2023/10/22 15:13:00 - INFO - root -   Epoch: [198/300][80/84], lr: 0.00000127 	 loss = 0.5502(0.3065)
2023/10/22 15:13:01 - INFO - root -   Epoch: [198/300] 	 loss = 0.3055
2023/10/22 15:13:01 - INFO - root -   train_accuracy = 0.8274
2023/10/22 15:13:23 - INFO - root -   Epoch: [199/300][0/84], lr: 0.00000127 	 loss = 0.0125(0.0125)
2023/10/22 15:14:34 - INFO - root -   Epoch: [199/300][20/84], lr: 0.00000127 	 loss = 0.0770(0.3226)
2023/10/22 15:15:41 - INFO - root -   Epoch: [199/300][40/84], lr: 0.00000127 	 loss = 0.0409(0.3603)
2023/10/22 15:16:48 - INFO - root -   Epoch: [199/300][60/84], lr: 0.00000127 	 loss = 0.5010(0.3878)
2023/10/22 15:17:43 - INFO - root -   Epoch: [199/300][80/84], lr: 0.00000127 	 loss = 0.0329(0.4030)
2023/10/22 15:17:44 - INFO - root -   Epoch: [199/300] 	 loss = 0.3937
2023/10/22 15:18:40 - INFO - root -   precision = 0.9070
2023/10/22 15:18:40 - INFO - root -   eval_loss = 0.2949
2023/10/22 15:18:40 - INFO - root -   eval_acc = 0.9070
2023/10/22 15:18:41 - INFO - root -   train_accuracy = 0.7917
2023/10/22 15:19:13 - INFO - root -   Epoch: [200/300][0/84], lr: 0.00000128 	 loss = 0.5490(0.5490)
2023/10/22 15:20:23 - INFO - root -   Epoch: [200/300][20/84], lr: 0.00000128 	 loss = 0.3267(0.4124)
2023/10/22 15:21:12 - INFO - root -   Epoch: [200/300][40/84], lr: 0.00000128 	 loss = 0.0685(0.3754)
2023/10/22 15:22:34 - INFO - root -   Epoch: [200/300][60/84], lr: 0.00000128 	 loss = 0.0329(0.3547)
2023/10/22 15:23:17 - INFO - root -   Epoch: [200/300][80/84], lr: 0.00000128 	 loss = 0.0231(0.3204)
2023/10/22 15:23:18 - INFO - root -   Epoch: [200/300] 	 loss = 0.3128
2023/10/22 15:23:18 - INFO - root -   train_accuracy = 0.8214
2023/10/22 15:23:40 - INFO - root -   Epoch: [201/300][0/84], lr: 0.00000128 	 loss = 0.0187(0.0187)
2023/10/22 15:24:46 - INFO - root -   Epoch: [201/300][20/84], lr: 0.00000128 	 loss = 0.0448(0.2659)
2023/10/22 15:25:52 - INFO - root -   Epoch: [201/300][40/84], lr: 0.00000128 	 loss = 0.0101(0.2407)
2023/10/22 15:27:05 - INFO - root -   Epoch: [201/300][60/84], lr: 0.00000128 	 loss = 0.0933(0.2658)
2023/10/22 15:27:54 - INFO - root -   Epoch: [201/300][80/84], lr: 0.00000128 	 loss = 1.2560(0.3057)
2023/10/22 15:27:56 - INFO - root -   Epoch: [201/300] 	 loss = 0.3000
2023/10/22 15:27:56 - INFO - root -   train_accuracy = 0.8393
2023/10/22 15:28:24 - INFO - root -   Epoch: [202/300][0/84], lr: 0.00000129 	 loss = 1.5553(1.5553)
2023/10/22 15:29:33 - INFO - root -   Epoch: [202/300][20/84], lr: 0.00000129 	 loss = 0.4504(0.3943)
2023/10/22 15:30:25 - INFO - root -   Epoch: [202/300][40/84], lr: 0.00000129 	 loss = 0.4373(0.3941)
2023/10/22 15:31:57 - INFO - root -   Epoch: [202/300][60/84], lr: 0.00000129 	 loss = 0.2228(0.3556)
2023/10/22 15:32:43 - INFO - root -   Epoch: [202/300][80/84], lr: 0.00000129 	 loss = 0.0205(0.3125)
2023/10/22 15:32:48 - INFO - root -   Epoch: [202/300] 	 loss = 0.3158
2023/10/22 15:32:48 - INFO - root -   train_accuracy = 0.8095
2023/10/22 15:33:10 - INFO - root -   Epoch: [203/300][0/84], lr: 0.00000130 	 loss = 0.0033(0.0033)
2023/10/22 15:34:22 - INFO - root -   Epoch: [203/300][20/84], lr: 0.00000130 	 loss = 0.0953(0.2722)
2023/10/22 15:35:12 - INFO - root -   Epoch: [203/300][40/84], lr: 0.00000130 	 loss = 0.0552(0.2996)
2023/10/22 15:36:26 - INFO - root -   Epoch: [203/300][60/84], lr: 0.00000130 	 loss = 0.0947(0.2927)
2023/10/22 15:37:15 - INFO - root -   Epoch: [203/300][80/84], lr: 0.00000130 	 loss = 0.0231(0.3118)
2023/10/22 15:37:17 - INFO - root -   Epoch: [203/300] 	 loss = 0.3146
2023/10/22 15:37:17 - INFO - root -   train_accuracy = 0.8571
2023/10/22 15:37:48 - INFO - root -   Epoch: [204/300][0/84], lr: 0.00000130 	 loss = 0.5670(0.5670)
2023/10/22 15:38:45 - INFO - root -   Epoch: [204/300][20/84], lr: 0.00000130 	 loss = 0.3010(0.2680)
2023/10/22 15:39:56 - INFO - root -   Epoch: [204/300][40/84], lr: 0.00000130 	 loss = 0.0242(0.2610)
2023/10/22 15:40:48 - INFO - root -   Epoch: [204/300][60/84], lr: 0.00000130 	 loss = 0.0111(0.2914)
2023/10/22 15:41:44 - INFO - root -   Epoch: [204/300][80/84], lr: 0.00000130 	 loss = 0.0445(0.2950)
2023/10/22 15:41:45 - INFO - root -   Epoch: [204/300] 	 loss = 0.2963
2023/10/22 15:42:42 - INFO - root -   precision = 0.9070
2023/10/22 15:42:42 - INFO - root -   eval_loss = 0.2405
2023/10/22 15:42:42 - INFO - root -   eval_acc = 0.9070
2023/10/22 15:42:43 - INFO - root -   train_accuracy = 0.8810
2023/10/22 15:43:13 - INFO - root -   Epoch: [205/300][0/84], lr: 0.00000131 	 loss = 0.3700(0.3700)
2023/10/22 15:44:13 - INFO - root -   Epoch: [205/300][20/84], lr: 0.00000131 	 loss = 0.1306(0.2665)
2023/10/22 15:45:20 - INFO - root -   Epoch: [205/300][40/84], lr: 0.00000131 	 loss = 0.0250(0.3288)
2023/10/22 15:46:37 - INFO - root -   Epoch: [205/300][60/84], lr: 0.00000131 	 loss = 0.0300(0.3017)
2023/10/22 15:47:16 - INFO - root -   Epoch: [205/300][80/84], lr: 0.00000131 	 loss = 0.0630(0.3078)
2023/10/22 15:47:19 - INFO - root -   Epoch: [205/300] 	 loss = 0.3106
2023/10/22 15:47:19 - INFO - root -   train_accuracy = 0.8571
2023/10/22 15:47:56 - INFO - root -   Epoch: [206/300][0/84], lr: 0.00000131 	 loss = 0.6061(0.6061)
2023/10/22 15:48:54 - INFO - root -   Epoch: [206/300][20/84], lr: 0.00000131 	 loss = 1.3411(0.3676)
2023/10/22 15:49:56 - INFO - root -   Epoch: [206/300][40/84], lr: 0.00000131 	 loss = 0.3017(0.4009)
2023/10/22 15:51:07 - INFO - root -   Epoch: [206/300][60/84], lr: 0.00000131 	 loss = 0.0103(0.3755)
2023/10/22 15:51:47 - INFO - root -   Epoch: [206/300][80/84], lr: 0.00000131 	 loss = 0.3408(0.3641)
2023/10/22 15:51:50 - INFO - root -   Epoch: [206/300] 	 loss = 0.3630
2023/10/22 15:51:50 - INFO - root -   train_accuracy = 0.8155
2023/10/22 15:52:11 - INFO - root -   Epoch: [207/300][0/84], lr: 0.00000132 	 loss = 0.0039(0.0039)
2023/10/22 15:53:18 - INFO - root -   Epoch: [207/300][20/84], lr: 0.00000132 	 loss = 0.6184(0.2921)
2023/10/22 15:54:36 - INFO - root -   Epoch: [207/300][40/84], lr: 0.00000132 	 loss = 0.0389(0.2959)
2023/10/22 15:55:35 - INFO - root -   Epoch: [207/300][60/84], lr: 0.00000132 	 loss = 0.0467(0.3145)
2023/10/22 15:56:29 - INFO - root -   Epoch: [207/300][80/84], lr: 0.00000132 	 loss = 0.0637(0.3233)
2023/10/22 15:56:32 - INFO - root -   Epoch: [207/300] 	 loss = 0.3229
2023/10/22 15:56:32 - INFO - root -   train_accuracy = 0.8452
2023/10/22 15:56:55 - INFO - root -   Epoch: [208/300][0/84], lr: 0.00000133 	 loss = 0.0283(0.0283)
2023/10/22 15:58:06 - INFO - root -   Epoch: [208/300][20/84], lr: 0.00000133 	 loss = 0.1259(0.2972)
2023/10/22 15:59:21 - INFO - root -   Epoch: [208/300][40/84], lr: 0.00000133 	 loss = 0.1368(0.3001)
2023/10/22 16:00:22 - INFO - root -   Epoch: [208/300][60/84], lr: 0.00000133 	 loss = 0.3024(0.3010)
2023/10/22 16:01:09 - INFO - root -   Epoch: [208/300][80/84], lr: 0.00000133 	 loss = 0.2315(0.3109)
2023/10/22 16:01:11 - INFO - root -   Epoch: [208/300] 	 loss = 0.3101
2023/10/22 16:01:11 - INFO - root -   train_accuracy = 0.8393
2023/10/22 16:01:40 - INFO - root -   Epoch: [209/300][0/84], lr: 0.00000133 	 loss = 0.7280(0.7280)
2023/10/22 16:02:38 - INFO - root -   Epoch: [209/300][20/84], lr: 0.00000133 	 loss = 0.0061(0.2631)
2023/10/22 16:03:39 - INFO - root -   Epoch: [209/300][40/84], lr: 0.00000133 	 loss = 0.0121(0.2832)
2023/10/22 16:04:46 - INFO - root -   Epoch: [209/300][60/84], lr: 0.00000133 	 loss = 0.2608(0.3112)
2023/10/22 16:05:45 - INFO - root -   Epoch: [209/300][80/84], lr: 0.00000133 	 loss = 0.1559(0.3144)
2023/10/22 16:05:46 - INFO - root -   Epoch: [209/300] 	 loss = 0.3138
2023/10/22 16:06:43 - INFO - root -   precision = 0.8837
2023/10/22 16:06:43 - INFO - root -   eval_loss = 0.4881
2023/10/22 16:06:43 - INFO - root -   eval_acc = 0.8837
2023/10/22 16:06:44 - INFO - root -   train_accuracy = 0.8393
2023/10/22 16:07:13 - INFO - root -   Epoch: [210/300][0/84], lr: 0.00000134 	 loss = 0.0072(0.0072)
2023/10/22 16:08:31 - INFO - root -   Epoch: [210/300][20/84], lr: 0.00000134 	 loss = 3.5007(0.5031)
2023/10/22 16:09:23 - INFO - root -   Epoch: [210/300][40/84], lr: 0.00000134 	 loss = 0.0338(0.3688)
2023/10/22 16:10:45 - INFO - root -   Epoch: [210/300][60/84], lr: 0.00000134 	 loss = 0.0218(0.3499)
2023/10/22 16:11:30 - INFO - root -   Epoch: [210/300][80/84], lr: 0.00000134 	 loss = 0.0050(0.3302)
2023/10/22 16:11:31 - INFO - root -   Epoch: [210/300] 	 loss = 0.3202
2023/10/22 16:11:31 - INFO - root -   train_accuracy = 0.8512
2023/10/22 16:12:00 - INFO - root -   Epoch: [211/300][0/84], lr: 0.00000134 	 loss = 0.8482(0.8482)
2023/10/22 16:13:06 - INFO - root -   Epoch: [211/300][20/84], lr: 0.00000134 	 loss = 0.0386(0.4643)
2023/10/22 16:14:22 - INFO - root -   Epoch: [211/300][40/84], lr: 0.00000134 	 loss = 0.8522(0.4129)
2023/10/22 16:15:12 - INFO - root -   Epoch: [211/300][60/84], lr: 0.00000134 	 loss = 0.0955(0.3666)
2023/10/22 16:16:10 - INFO - root -   Epoch: [211/300][80/84], lr: 0.00000134 	 loss = 0.9026(0.3698)
2023/10/22 16:16:11 - INFO - root -   Epoch: [211/300] 	 loss = 0.3593
2023/10/22 16:16:11 - INFO - root -   train_accuracy = 0.8155
2023/10/22 16:16:33 - INFO - root -   Epoch: [212/300][0/84], lr: 0.00000135 	 loss = 0.0069(0.0069)
2023/10/22 16:17:45 - INFO - root -   Epoch: [212/300][20/84], lr: 0.00000135 	 loss = 0.1406(0.3621)
2023/10/22 16:18:37 - INFO - root -   Epoch: [212/300][40/84], lr: 0.00000135 	 loss = 0.3419(0.3690)
2023/10/22 16:20:02 - INFO - root -   Epoch: [212/300][60/84], lr: 0.00000135 	 loss = 0.0534(0.3317)
2023/10/22 16:20:44 - INFO - root -   Epoch: [212/300][80/84], lr: 0.00000135 	 loss = 0.0835(0.3456)
2023/10/22 16:20:47 - INFO - root -   Epoch: [212/300] 	 loss = 0.3378
2023/10/22 16:20:47 - INFO - root -   train_accuracy = 0.8393
2023/10/22 16:21:17 - INFO - root -   Epoch: [213/300][0/84], lr: 0.00000136 	 loss = 0.4722(0.4722)
2023/10/22 16:22:22 - INFO - root -   Epoch: [213/300][20/84], lr: 0.00000136 	 loss = 1.3464(0.2654)
2023/10/22 16:23:31 - INFO - root -   Epoch: [213/300][40/84], lr: 0.00000136 	 loss = 0.5380(0.3849)
2023/10/22 16:24:30 - INFO - root -   Epoch: [213/300][60/84], lr: 0.00000136 	 loss = 0.0396(0.3654)
2023/10/22 16:25:27 - INFO - root -   Epoch: [213/300][80/84], lr: 0.00000136 	 loss = 0.1292(0.3403)
2023/10/22 16:25:28 - INFO - root -   Epoch: [213/300] 	 loss = 0.3408
2023/10/22 16:25:28 - INFO - root -   train_accuracy = 0.8393
2023/10/22 16:25:57 - INFO - root -   Epoch: [214/300][0/84], lr: 0.00000136 	 loss = 0.0039(0.0039)
2023/10/22 16:26:57 - INFO - root -   Epoch: [214/300][20/84], lr: 0.00000136 	 loss = 0.0257(0.3349)
2023/10/22 16:28:12 - INFO - root -   Epoch: [214/300][40/84], lr: 0.00000136 	 loss = 0.0212(0.3181)
2023/10/22 16:29:09 - INFO - root -   Epoch: [214/300][60/84], lr: 0.00000136 	 loss = 0.2177(0.2985)
2023/10/22 16:29:56 - INFO - root -   Epoch: [214/300][80/84], lr: 0.00000136 	 loss = 0.0157(0.2941)
2023/10/22 16:29:57 - INFO - root -   Epoch: [214/300] 	 loss = 0.2858
2023/10/22 16:30:54 - INFO - root -   precision = 0.8837
2023/10/22 16:30:54 - INFO - root -   eval_loss = 0.3365
2023/10/22 16:30:54 - INFO - root -   eval_acc = 0.8837
2023/10/22 16:30:55 - INFO - root -   train_accuracy = 0.8571
2023/10/22 16:31:17 - INFO - root -   Epoch: [215/300][0/84], lr: 0.00000137 	 loss = 0.0247(0.0247)
2023/10/22 16:32:23 - INFO - root -   Epoch: [215/300][20/84], lr: 0.00000137 	 loss = 0.4309(0.3012)
2023/10/22 16:33:39 - INFO - root -   Epoch: [215/300][40/84], lr: 0.00000137 	 loss = 0.5183(0.2615)
2023/10/22 16:34:40 - INFO - root -   Epoch: [215/300][60/84], lr: 0.00000137 	 loss = 0.6642(0.2924)
2023/10/22 16:35:36 - INFO - root -   Epoch: [215/300][80/84], lr: 0.00000137 	 loss = 0.2607(0.3247)
2023/10/22 16:35:38 - INFO - root -   Epoch: [215/300] 	 loss = 0.3223
2023/10/22 16:35:38 - INFO - root -   train_accuracy = 0.8214
2023/10/22 16:36:07 - INFO - root -   Epoch: [216/300][0/84], lr: 0.00000137 	 loss = 0.7846(0.7846)
2023/10/22 16:37:16 - INFO - root -   Epoch: [216/300][20/84], lr: 0.00000137 	 loss = 0.4663(0.3672)
2023/10/22 16:38:21 - INFO - root -   Epoch: [216/300][40/84], lr: 0.00000137 	 loss = 0.0207(0.3179)
2023/10/22 16:39:31 - INFO - root -   Epoch: [216/300][60/84], lr: 0.00000137 	 loss = 0.4656(0.2828)
2023/10/22 16:40:14 - INFO - root -   Epoch: [216/300][80/84], lr: 0.00000137 	 loss = 0.0236(0.3346)
2023/10/22 16:40:16 - INFO - root -   Epoch: [216/300] 	 loss = 0.3316
2023/10/22 16:40:16 - INFO - root -   train_accuracy = 0.8393
2023/10/22 16:40:46 - INFO - root -   Epoch: [217/300][0/84], lr: 0.00000138 	 loss = 0.0233(0.0233)
2023/10/22 16:41:42 - INFO - root -   Epoch: [217/300][20/84], lr: 0.00000138 	 loss = 0.0509(0.2986)
2023/10/22 16:42:53 - INFO - root -   Epoch: [217/300][40/84], lr: 0.00000138 	 loss = 0.0051(0.2707)
2023/10/22 16:43:49 - INFO - root -   Epoch: [217/300][60/84], lr: 0.00000138 	 loss = 0.0170(0.2432)
2023/10/22 16:44:39 - INFO - root -   Epoch: [217/300][80/84], lr: 0.00000138 	 loss = 0.8651(0.2698)
2023/10/22 16:44:40 - INFO - root -   Epoch: [217/300] 	 loss = 0.2620
2023/10/22 16:44:40 - INFO - root -   train_accuracy = 0.8512
2023/10/22 16:45:03 - INFO - root -   Epoch: [218/300][0/84], lr: 0.00000138 	 loss = 0.0324(0.0324)
2023/10/22 16:46:14 - INFO - root -   Epoch: [218/300][20/84], lr: 0.00000138 	 loss = 0.2505(0.3591)
2023/10/22 16:47:43 - INFO - root -   Epoch: [218/300][40/84], lr: 0.00000138 	 loss = 0.2440(0.3406)
2023/10/22 16:48:44 - INFO - root -   Epoch: [218/300][60/84], lr: 0.00000138 	 loss = 0.3123(0.3039)
2023/10/22 16:49:26 - INFO - root -   Epoch: [218/300][80/84], lr: 0.00000138 	 loss = 0.2033(0.3081)
2023/10/22 16:49:27 - INFO - root -   Epoch: [218/300] 	 loss = 0.3046
2023/10/22 16:49:27 - INFO - root -   train_accuracy = 0.8452
2023/10/22 16:49:48 - INFO - root -   Epoch: [219/300][0/84], lr: 0.00000139 	 loss = 0.2306(0.2306)
2023/10/22 16:50:53 - INFO - root -   Epoch: [219/300][20/84], lr: 0.00000139 	 loss = 0.1067(0.2712)
2023/10/22 16:52:12 - INFO - root -   Epoch: [219/300][40/84], lr: 0.00000139 	 loss = 0.0165(0.2653)
2023/10/22 16:53:09 - INFO - root -   Epoch: [219/300][60/84], lr: 0.00000139 	 loss = 0.1572(0.2620)
2023/10/22 16:53:58 - INFO - root -   Epoch: [219/300][80/84], lr: 0.00000139 	 loss = 0.5264(0.2748)
2023/10/22 16:54:00 - INFO - root -   Epoch: [219/300] 	 loss = 0.2718
2023/10/22 16:54:58 - INFO - root -   precision = 0.8372
2023/10/22 16:54:58 - INFO - root -   eval_loss = 0.4143
2023/10/22 16:54:58 - INFO - root -   eval_acc = 0.8372
2023/10/22 16:54:59 - INFO - root -   train_accuracy = 0.8750
2023/10/22 16:55:21 - INFO - root -   Epoch: [220/300][0/84], lr: 0.00000140 	 loss = 0.0053(0.0053)
2023/10/22 16:56:43 - INFO - root -   Epoch: [220/300][20/84], lr: 0.00000140 	 loss = 0.3071(0.2886)
2023/10/22 16:57:34 - INFO - root -   Epoch: [220/300][40/84], lr: 0.00000140 	 loss = 0.0134(0.2947)
2023/10/22 16:58:48 - INFO - root -   Epoch: [220/300][60/84], lr: 0.00000140 	 loss = 0.4220(0.2765)
2023/10/22 16:59:28 - INFO - root -   Epoch: [220/300][80/84], lr: 0.00000140 	 loss = 0.1176(0.2798)
2023/10/22 16:59:30 - INFO - root -   Epoch: [220/300] 	 loss = 0.2755
2023/10/22 16:59:30 - INFO - root -   train_accuracy = 0.8869
2023/10/22 17:00:00 - INFO - root -   Epoch: [221/300][0/84], lr: 0.00000140 	 loss = 0.1678(0.1678)
2023/10/22 17:01:13 - INFO - root -   Epoch: [221/300][20/84], lr: 0.00000140 	 loss = 0.3619(0.2282)
2023/10/22 17:02:10 - INFO - root -   Epoch: [221/300][40/84], lr: 0.00000140 	 loss = 0.0200(0.2408)
2023/10/22 17:03:24 - INFO - root -   Epoch: [221/300][60/84], lr: 0.00000140 	 loss = 0.0260(0.2479)
2023/10/22 17:04:09 - INFO - root -   Epoch: [221/300][80/84], lr: 0.00000140 	 loss = 0.2089(0.2903)
2023/10/22 17:04:13 - INFO - root -   Epoch: [221/300] 	 loss = 0.3106
2023/10/22 17:04:13 - INFO - root -   train_accuracy = 0.8333
2023/10/22 17:04:35 - INFO - root -   Epoch: [222/300][0/84], lr: 0.00000141 	 loss = 0.0016(0.0016)
2023/10/22 17:05:49 - INFO - root -   Epoch: [222/300][20/84], lr: 0.00000141 	 loss = 0.0225(0.3433)
2023/10/22 17:06:54 - INFO - root -   Epoch: [222/300][40/84], lr: 0.00000141 	 loss = 0.0209(0.3721)
2023/10/22 17:07:58 - INFO - root -   Epoch: [222/300][60/84], lr: 0.00000141 	 loss = 1.4464(0.3687)
2023/10/22 17:08:41 - INFO - root -   Epoch: [222/300][80/84], lr: 0.00000141 	 loss = 0.1172(0.3230)
2023/10/22 17:08:46 - INFO - root -   Epoch: [222/300] 	 loss = 0.3222
2023/10/22 17:08:46 - INFO - root -   train_accuracy = 0.8512
2023/10/22 17:09:15 - INFO - root -   Epoch: [223/300][0/84], lr: 0.00000141 	 loss = 0.0111(0.0111)
2023/10/22 17:10:14 - INFO - root -   Epoch: [223/300][20/84], lr: 0.00000141 	 loss = 0.1890(0.2354)
2023/10/22 17:11:14 - INFO - root -   Epoch: [223/300][40/84], lr: 0.00000141 	 loss = 0.0311(0.2384)
2023/10/22 17:12:40 - INFO - root -   Epoch: [223/300][60/84], lr: 0.00000141 	 loss = 0.5057(0.2991)
2023/10/22 17:13:23 - INFO - root -   Epoch: [223/300][80/84], lr: 0.00000141 	 loss = 0.1079(0.2893)
2023/10/22 17:13:25 - INFO - root -   Epoch: [223/300] 	 loss = 0.2823
2023/10/22 17:13:25 - INFO - root -   train_accuracy = 0.8452
2023/10/22 17:13:48 - INFO - root -   Epoch: [224/300][0/84], lr: 0.00000142 	 loss = 0.0014(0.0014)
2023/10/22 17:14:55 - INFO - root -   Epoch: [224/300][20/84], lr: 0.00000142 	 loss = 0.1632(0.1839)
2023/10/22 17:16:19 - INFO - root -   Epoch: [224/300][40/84], lr: 0.00000142 	 loss = 0.0256(0.2615)
2023/10/22 17:17:01 - INFO - root -   Epoch: [224/300][60/84], lr: 0.00000142 	 loss = 0.2106(0.2902)
2023/10/22 17:17:57 - INFO - root -   Epoch: [224/300][80/84], lr: 0.00000142 	 loss = 0.2882(0.2849)
2023/10/22 17:17:59 - INFO - root -   Epoch: [224/300] 	 loss = 0.2810
2023/10/22 17:18:56 - INFO - root -   precision = 0.8837
2023/10/22 17:18:56 - INFO - root -   eval_loss = 0.2895
2023/10/22 17:18:56 - INFO - root -   eval_acc = 0.8837
2023/10/22 17:18:57 - INFO - root -   train_accuracy = 0.8690
2023/10/22 17:19:19 - INFO - root -   Epoch: [225/300][0/84], lr: 0.00000143 	 loss = 0.0614(0.0614)
2023/10/22 17:20:25 - INFO - root -   Epoch: [225/300][20/84], lr: 0.00000143 	 loss = 0.1993(0.2483)
2023/10/22 17:21:35 - INFO - root -   Epoch: [225/300][40/84], lr: 0.00000143 	 loss = 1.0187(0.3737)
2023/10/22 17:22:38 - INFO - root -   Epoch: [225/300][60/84], lr: 0.00000143 	 loss = 0.0186(0.3313)
2023/10/22 17:23:34 - INFO - root -   Epoch: [225/300][80/84], lr: 0.00000143 	 loss = 0.2811(0.3020)
2023/10/22 17:23:38 - INFO - root -   Epoch: [225/300] 	 loss = 0.2924
2023/10/22 17:23:38 - INFO - root -   train_accuracy = 0.8571
2023/10/22 17:24:17 - INFO - root -   Epoch: [226/300][0/84], lr: 0.00000143 	 loss = 0.9637(0.9637)
2023/10/22 17:25:13 - INFO - root -   Epoch: [226/300][20/84], lr: 0.00000143 	 loss = 0.3023(0.2207)
2023/10/22 17:26:13 - INFO - root -   Epoch: [226/300][40/84], lr: 0.00000143 	 loss = 0.0194(0.2986)
2023/10/22 17:27:24 - INFO - root -   Epoch: [226/300][60/84], lr: 0.00000143 	 loss = 0.4050(0.2322)
2023/10/22 17:28:05 - INFO - root -   Epoch: [226/300][80/84], lr: 0.00000143 	 loss = 0.1448(0.2723)
2023/10/22 17:28:13 - INFO - root -   Epoch: [226/300] 	 loss = 0.2716
2023/10/22 17:28:13 - INFO - root -   train_accuracy = 0.8690
2023/10/22 17:28:34 - INFO - root -   Epoch: [227/300][0/84], lr: 0.00000144 	 loss = 0.0073(0.0073)
2023/10/22 17:29:34 - INFO - root -   Epoch: [227/300][20/84], lr: 0.00000144 	 loss = 0.2451(0.3468)
2023/10/22 17:30:45 - INFO - root -   Epoch: [227/300][40/84], lr: 0.00000144 	 loss = 0.0047(0.3473)
2023/10/22 17:31:55 - INFO - root -   Epoch: [227/300][60/84], lr: 0.00000144 	 loss = 0.1498(0.3446)
2023/10/22 17:32:48 - INFO - root -   Epoch: [227/300][80/84], lr: 0.00000144 	 loss = 0.5910(0.3734)
2023/10/22 17:32:51 - INFO - root -   Epoch: [227/300] 	 loss = 0.3617
2023/10/22 17:32:51 - INFO - root -   train_accuracy = 0.8155
2023/10/22 17:33:12 - INFO - root -   Epoch: [228/300][0/84], lr: 0.00000144 	 loss = 0.0044(0.0044)
2023/10/22 17:34:25 - INFO - root -   Epoch: [228/300][20/84], lr: 0.00000144 	 loss = 0.4998(0.2552)
2023/10/22 17:35:31 - INFO - root -   Epoch: [228/300][40/84], lr: 0.00000144 	 loss = 0.0183(0.2225)
2023/10/22 17:36:37 - INFO - root -   Epoch: [228/300][60/84], lr: 0.00000144 	 loss = 0.0573(0.1760)
2023/10/22 17:37:25 - INFO - root -   Epoch: [228/300][80/84], lr: 0.00000144 	 loss = 0.0243(0.2181)
2023/10/22 17:37:26 - INFO - root -   Epoch: [228/300] 	 loss = 0.2104
2023/10/22 17:37:26 - INFO - root -   train_accuracy = 0.8988
2023/10/22 17:37:48 - INFO - root -   Epoch: [229/300][0/84], lr: 0.00000145 	 loss = 0.0007(0.0007)
2023/10/22 17:39:13 - INFO - root -   Epoch: [229/300][20/84], lr: 0.00000145 	 loss = 0.0343(0.4151)
2023/10/22 17:40:02 - INFO - root -   Epoch: [229/300][40/84], lr: 0.00000145 	 loss = 0.0430(0.3683)
2023/10/22 17:41:18 - INFO - root -   Epoch: [229/300][60/84], lr: 0.00000145 	 loss = 0.0995(0.3242)
2023/10/22 17:42:03 - INFO - root -   Epoch: [229/300][80/84], lr: 0.00000145 	 loss = 0.0268(0.3206)
2023/10/22 17:42:08 - INFO - root -   Epoch: [229/300] 	 loss = 0.3286
2023/10/22 17:43:06 - INFO - root -   precision = 0.7907
2023/10/22 17:43:06 - INFO - root -   eval_loss = 0.4637
2023/10/22 17:43:06 - INFO - root -   eval_acc = 0.7907
2023/10/22 17:43:07 - INFO - root -   train_accuracy = 0.7976
2023/10/22 17:43:37 - INFO - root -   Epoch: [230/300][0/84], lr: 0.00000146 	 loss = 0.0350(0.0350)
2023/10/22 17:44:37 - INFO - root -   Epoch: [230/300][20/84], lr: 0.00000146 	 loss = 0.0214(0.1786)
2023/10/22 17:45:48 - INFO - root -   Epoch: [230/300][40/84], lr: 0.00000146 	 loss = 0.0053(0.2260)
2023/10/22 17:46:44 - INFO - root -   Epoch: [230/300][60/84], lr: 0.00000146 	 loss = 0.0082(0.2440)
2023/10/22 17:47:49 - INFO - root -   Epoch: [230/300][80/84], lr: 0.00000146 	 loss = 0.4174(0.2479)
2023/10/22 17:47:50 - INFO - root -   Epoch: [230/300] 	 loss = 0.2585
2023/10/22 17:47:50 - INFO - root -   train_accuracy = 0.8690
2023/10/22 17:48:19 - INFO - root -   Epoch: [231/300][0/84], lr: 0.00000146 	 loss = 0.0013(0.0013)
2023/10/22 17:49:32 - INFO - root -   Epoch: [231/300][20/84], lr: 0.00000146 	 loss = 0.5083(0.3097)
2023/10/22 17:50:36 - INFO - root -   Epoch: [231/300][40/84], lr: 0.00000146 	 loss = 0.4511(0.2889)
2023/10/22 17:51:34 - INFO - root -   Epoch: [231/300][60/84], lr: 0.00000146 	 loss = 0.2222(0.3228)
2023/10/22 17:52:35 - INFO - root -   Epoch: [231/300][80/84], lr: 0.00000146 	 loss = 0.3406(0.3120)
2023/10/22 17:52:36 - INFO - root -   Epoch: [231/300] 	 loss = 0.3059
2023/10/22 17:52:36 - INFO - root -   train_accuracy = 0.8452
2023/10/22 17:53:14 - INFO - root -   Epoch: [232/300][0/84], lr: 0.00000147 	 loss = 0.8292(0.8292)
2023/10/22 17:54:05 - INFO - root -   Epoch: [232/300][20/84], lr: 0.00000147 	 loss = 0.0252(0.3589)
2023/10/22 17:55:34 - INFO - root -   Epoch: [232/300][40/84], lr: 0.00000147 	 loss = 1.1674(0.3252)
2023/10/22 17:56:27 - INFO - root -   Epoch: [232/300][60/84], lr: 0.00000147 	 loss = 0.0189(0.3027)
2023/10/22 17:57:10 - INFO - root -   Epoch: [232/300][80/84], lr: 0.00000147 	 loss = 0.1246(0.3514)
2023/10/22 17:57:12 - INFO - root -   Epoch: [232/300] 	 loss = 0.3493
2023/10/22 17:57:12 - INFO - root -   train_accuracy = 0.8333
2023/10/22 17:57:33 - INFO - root -   Epoch: [233/300][0/84], lr: 0.00000147 	 loss = 0.0094(0.0094)
2023/10/22 17:58:33 - INFO - root -   Epoch: [233/300][20/84], lr: 0.00000147 	 loss = 0.0271(0.4174)
2023/10/22 18:00:02 - INFO - root -   Epoch: [233/300][40/84], lr: 0.00000147 	 loss = 1.1539(0.3828)
2023/10/22 18:00:53 - INFO - root -   Epoch: [233/300][60/84], lr: 0.00000147 	 loss = 0.2165(0.3459)
2023/10/22 18:01:56 - INFO - root -   Epoch: [233/300][80/84], lr: 0.00000147 	 loss = 2.7102(0.3551)
2023/10/22 18:01:57 - INFO - root -   Epoch: [233/300] 	 loss = 0.3549
2023/10/22 18:01:57 - INFO - root -   train_accuracy = 0.7976
2023/10/22 18:02:27 - INFO - root -   Epoch: [234/300][0/84], lr: 0.00000148 	 loss = 0.6627(0.6627)
2023/10/22 18:03:33 - INFO - root -   Epoch: [234/300][20/84], lr: 0.00000148 	 loss = 0.0444(0.2502)
2023/10/22 18:04:38 - INFO - root -   Epoch: [234/300][40/84], lr: 0.00000148 	 loss = 0.0562(0.2866)
2023/10/22 18:05:54 - INFO - root -   Epoch: [234/300][60/84], lr: 0.00000148 	 loss = 0.0212(0.2844)
2023/10/22 18:06:35 - INFO - root -   Epoch: [234/300][80/84], lr: 0.00000148 	 loss = 0.0590(0.2764)
2023/10/22 18:06:37 - INFO - root -   Epoch: [234/300] 	 loss = 0.2669
2023/10/22 18:07:33 - INFO - root -   precision = 0.8605
2023/10/22 18:07:33 - INFO - root -   eval_loss = 0.5958
2023/10/22 18:07:33 - INFO - root -   eval_acc = 0.8605
2023/10/22 18:07:34 - INFO - root -   train_accuracy = 0.8393
2023/10/22 18:07:56 - INFO - root -   Epoch: [235/300][0/84], lr: 0.00000148 	 loss = 0.0079(0.0079)
2023/10/22 18:09:09 - INFO - root -   Epoch: [235/300][20/84], lr: 0.00000148 	 loss = 0.6725(0.3339)
2023/10/22 18:10:08 - INFO - root -   Epoch: [235/300][40/84], lr: 0.00000148 	 loss = 0.0406(0.2931)
2023/10/22 18:11:28 - INFO - root -   Epoch: [235/300][60/84], lr: 0.00000148 	 loss = 0.3908(0.2662)
2023/10/22 18:12:16 - INFO - root -   Epoch: [235/300][80/84], lr: 0.00000148 	 loss = 0.0530(0.3090)
2023/10/22 18:12:17 - INFO - root -   Epoch: [235/300] 	 loss = 0.2990
2023/10/22 18:12:17 - INFO - root -   train_accuracy = 0.8869
2023/10/22 18:12:55 - INFO - root -   Epoch: [236/300][0/84], lr: 0.00000149 	 loss = 0.4681(0.4681)
2023/10/22 18:13:46 - INFO - root -   Epoch: [236/300][20/84], lr: 0.00000149 	 loss = 0.0426(0.2737)
2023/10/22 18:14:53 - INFO - root -   Epoch: [236/300][40/84], lr: 0.00000149 	 loss = 0.0177(0.2935)
2023/10/22 18:16:00 - INFO - root -   Epoch: [236/300][60/84], lr: 0.00000149 	 loss = 0.0715(0.2757)
2023/10/22 18:16:50 - INFO - root -   Epoch: [236/300][80/84], lr: 0.00000149 	 loss = 0.0205(0.3106)
2023/10/22 18:16:51 - INFO - root -   Epoch: [236/300] 	 loss = 0.3003
2023/10/22 18:16:51 - INFO - root -   train_accuracy = 0.8333
2023/10/22 18:17:13 - INFO - root -   Epoch: [237/300][0/84], lr: 0.00000150 	 loss = 0.0059(0.0059)
2023/10/22 18:18:32 - INFO - root -   Epoch: [237/300][20/84], lr: 0.00000150 	 loss = 4.2995(0.5267)
2023/10/22 18:19:37 - INFO - root -   Epoch: [237/300][40/84], lr: 0.00000150 	 loss = 0.6527(0.4228)
2023/10/22 18:20:36 - INFO - root -   Epoch: [237/300][60/84], lr: 0.00000150 	 loss = 0.0254(0.3769)
2023/10/22 18:21:22 - INFO - root -   Epoch: [237/300][80/84], lr: 0.00000150 	 loss = 0.0035(0.3394)
2023/10/22 18:21:25 - INFO - root -   Epoch: [237/300] 	 loss = 0.3447
2023/10/22 18:21:25 - INFO - root -   train_accuracy = 0.8333
2023/10/22 18:21:55 - INFO - root -   Epoch: [238/300][0/84], lr: 0.00000150 	 loss = 0.2544(0.2544)
2023/10/22 18:22:56 - INFO - root -   Epoch: [238/300][20/84], lr: 0.00000150 	 loss = 0.4640(0.2464)
2023/10/22 18:24:12 - INFO - root -   Epoch: [238/300][40/84], lr: 0.00000150 	 loss = 0.2498(0.3021)
2023/10/22 18:25:27 - INFO - root -   Epoch: [238/300][60/84], lr: 0.00000150 	 loss = 0.6033(0.3117)
2023/10/22 18:26:11 - INFO - root -   Epoch: [238/300][80/84], lr: 0.00000150 	 loss = 0.1010(0.3069)
2023/10/22 18:26:13 - INFO - root -   Epoch: [238/300] 	 loss = 0.2980
2023/10/22 18:26:13 - INFO - root -   train_accuracy = 0.8512
2023/10/22 18:26:43 - INFO - root -   Epoch: [239/300][0/84], lr: 0.00000151 	 loss = 0.4488(0.4488)
2023/10/22 18:27:41 - INFO - root -   Epoch: [239/300][20/84], lr: 0.00000151 	 loss = 0.0084(0.2695)
2023/10/22 18:28:58 - INFO - root -   Epoch: [239/300][40/84], lr: 0.00000151 	 loss = 0.0142(0.3034)
2023/10/22 18:29:56 - INFO - root -   Epoch: [239/300][60/84], lr: 0.00000151 	 loss = 0.0094(0.3263)
2023/10/22 18:30:44 - INFO - root -   Epoch: [239/300][80/84], lr: 0.00000151 	 loss = 0.0271(0.2832)
2023/10/22 18:30:46 - INFO - root -   Epoch: [239/300] 	 loss = 0.2734
2023/10/22 18:31:43 - INFO - root -   precision = 0.8837
2023/10/22 18:31:43 - INFO - root -   eval_loss = 0.3342
2023/10/22 18:31:43 - INFO - root -   eval_acc = 0.8837
2023/10/22 18:31:44 - INFO - root -   train_accuracy = 0.8333
2023/10/22 18:32:14 - INFO - root -   Epoch: [240/300][0/84], lr: 0.00000151 	 loss = 0.0024(0.0024)
2023/10/22 18:33:13 - INFO - root -   Epoch: [240/300][20/84], lr: 0.00000151 	 loss = 0.2437(0.3216)
2023/10/22 18:34:14 - INFO - root -   Epoch: [240/300][40/84], lr: 0.00000151 	 loss = 0.1889(0.3198)
2023/10/22 18:35:20 - INFO - root -   Epoch: [240/300][60/84], lr: 0.00000151 	 loss = 0.0641(0.3083)
2023/10/22 18:36:05 - INFO - root -   Epoch: [240/300][80/84], lr: 0.00000151 	 loss = 0.1754(0.3274)
2023/10/22 18:36:06 - INFO - root -   Epoch: [240/300] 	 loss = 0.3272
2023/10/22 18:36:06 - INFO - root -   train_accuracy = 0.8393
2023/10/22 18:36:36 - INFO - root -   Epoch: [241/300][0/84], lr: 0.00000152 	 loss = 0.2735(0.2735)
2023/10/22 18:37:34 - INFO - root -   Epoch: [241/300][20/84], lr: 0.00000152 	 loss = 0.1439(0.3345)
2023/10/22 18:38:35 - INFO - root -   Epoch: [241/300][40/84], lr: 0.00000152 	 loss = 0.1741(0.3335)
2023/10/22 18:39:42 - INFO - root -   Epoch: [241/300][60/84], lr: 0.00000152 	 loss = 0.0383(0.2992)
2023/10/22 18:40:41 - INFO - root -   Epoch: [241/300][80/84], lr: 0.00000152 	 loss = 0.3140(0.3260)
2023/10/22 18:40:42 - INFO - root -   Epoch: [241/300] 	 loss = 0.3316
2023/10/22 18:40:42 - INFO - root -   train_accuracy = 0.8393
2023/10/22 18:41:04 - INFO - root -   Epoch: [242/300][0/84], lr: 0.00000153 	 loss = 0.1653(0.1653)
2023/10/22 18:42:11 - INFO - root -   Epoch: [242/300][20/84], lr: 0.00000153 	 loss = 0.0673(0.3417)
2023/10/22 18:43:33 - INFO - root -   Epoch: [242/300][40/84], lr: 0.00000153 	 loss = 0.0099(0.3157)
2023/10/22 18:44:42 - INFO - root -   Epoch: [242/300][60/84], lr: 0.00000153 	 loss = 0.0015(0.2755)
2023/10/22 18:45:26 - INFO - root -   Epoch: [242/300][80/84], lr: 0.00000153 	 loss = 0.2713(0.2782)
2023/10/22 18:45:28 - INFO - root -   Epoch: [242/300] 	 loss = 0.2685
2023/10/22 18:45:28 - INFO - root -   train_accuracy = 0.8512
2023/10/22 18:45:49 - INFO - root -   Epoch: [243/300][0/84], lr: 0.00000153 	 loss = 0.0418(0.0418)
2023/10/22 18:46:59 - INFO - root -   Epoch: [243/300][20/84], lr: 0.00000153 	 loss = 0.4934(0.3917)
2023/10/22 18:48:24 - INFO - root -   Epoch: [243/300][40/84], lr: 0.00000153 	 loss = 0.4527(0.3431)
2023/10/22 18:49:25 - INFO - root -   Epoch: [243/300][60/84], lr: 0.00000153 	 loss = 0.1944(0.3298)
2023/10/22 18:50:14 - INFO - root -   Epoch: [243/300][80/84], lr: 0.00000153 	 loss = 0.0218(0.3117)
2023/10/22 18:50:15 - INFO - root -   Epoch: [243/300] 	 loss = 0.3035
2023/10/22 18:50:15 - INFO - root -   train_accuracy = 0.8155
2023/10/22 18:50:37 - INFO - root -   Epoch: [244/300][0/84], lr: 0.00000154 	 loss = 0.0015(0.0015)
2023/10/22 18:51:48 - INFO - root -   Epoch: [244/300][20/84], lr: 0.00000154 	 loss = 0.1291(0.2128)
2023/10/22 18:52:39 - INFO - root -   Epoch: [244/300][40/84], lr: 0.00000154 	 loss = 0.9662(0.3204)
2023/10/22 18:54:07 - INFO - root -   Epoch: [244/300][60/84], lr: 0.00000154 	 loss = 0.5758(0.4279)
2023/10/22 18:54:50 - INFO - root -   Epoch: [244/300][80/84], lr: 0.00000154 	 loss = 0.1650(0.4068)
2023/10/22 18:54:52 - INFO - root -   Epoch: [244/300] 	 loss = 0.3999
2023/10/22 18:55:49 - INFO - root -   precision = 0.8837
2023/10/22 18:55:49 - INFO - root -   eval_loss = 0.3333
2023/10/22 18:55:49 - INFO - root -   eval_acc = 0.8837
2023/10/22 18:55:50 - INFO - root -   train_accuracy = 0.7917
2023/10/22 18:56:20 - INFO - root -   Epoch: [245/300][0/84], lr: 0.00000154 	 loss = 0.2488(0.2488)
2023/10/22 18:57:26 - INFO - root -   Epoch: [245/300][20/84], lr: 0.00000154 	 loss = 0.4536(0.3682)
2023/10/22 18:58:51 - INFO - root -   Epoch: [245/300][40/84], lr: 0.00000154 	 loss = 0.8561(0.3722)
2023/10/22 18:59:42 - INFO - root -   Epoch: [245/300][60/84], lr: 0.00000154 	 loss = 0.3942(0.3657)
2023/10/22 19:00:37 - INFO - root -   Epoch: [245/300][80/84], lr: 0.00000154 	 loss = 0.2100(0.3444)
2023/10/22 19:00:39 - INFO - root -   Epoch: [245/300] 	 loss = 0.3478
2023/10/22 19:00:39 - INFO - root -   train_accuracy = 0.8333
2023/10/22 19:01:16 - INFO - root -   Epoch: [246/300][0/84], lr: 0.00000155 	 loss = 0.7237(0.7237)
2023/10/22 19:02:06 - INFO - root -   Epoch: [246/300][20/84], lr: 0.00000155 	 loss = 0.0740(0.2610)
2023/10/22 19:03:07 - INFO - root -   Epoch: [246/300][40/84], lr: 0.00000155 	 loss = 0.0174(0.2687)
2023/10/22 19:04:20 - INFO - root -   Epoch: [246/300][60/84], lr: 0.00000155 	 loss = 0.4901(0.2801)
2023/10/22 19:05:06 - INFO - root -   Epoch: [246/300][80/84], lr: 0.00000155 	 loss = 0.0788(0.2675)
2023/10/22 19:05:09 - INFO - root -   Epoch: [246/300] 	 loss = 0.2585
2023/10/22 19:05:09 - INFO - root -   train_accuracy = 0.8631
2023/10/22 19:05:31 - INFO - root -   Epoch: [247/300][0/84], lr: 0.00000156 	 loss = 0.0017(0.0017)
2023/10/22 19:06:52 - INFO - root -   Epoch: [247/300][20/84], lr: 0.00000156 	 loss = 0.1635(0.3472)
2023/10/22 19:08:01 - INFO - root -   Epoch: [247/300][40/84], lr: 0.00000156 	 loss = 0.0786(0.3258)
2023/10/22 19:09:16 - INFO - root -   Epoch: [247/300][60/84], lr: 0.00000156 	 loss = 0.0596(0.3083)
2023/10/22 19:10:01 - INFO - root -   Epoch: [247/300][80/84], lr: 0.00000156 	 loss = 0.2564(0.3321)
2023/10/22 19:10:02 - INFO - root -   Epoch: [247/300] 	 loss = 0.3388
2023/10/22 19:10:02 - INFO - root -   train_accuracy = 0.8274
2023/10/22 19:10:32 - INFO - root -   Epoch: [248/300][0/84], lr: 0.00000156 	 loss = 0.4396(0.4396)
2023/10/22 19:11:40 - INFO - root -   Epoch: [248/300][20/84], lr: 0.00000156 	 loss = 0.6182(0.3053)
2023/10/22 19:12:47 - INFO - root -   Epoch: [248/300][40/84], lr: 0.00000156 	 loss = 0.0037(0.2701)
2023/10/22 19:14:06 - INFO - root -   Epoch: [248/300][60/84], lr: 0.00000156 	 loss = 0.2344(0.2710)
2023/10/22 19:14:53 - INFO - root -   Epoch: [248/300][80/84], lr: 0.00000156 	 loss = 0.2832(0.3072)
2023/10/22 19:14:55 - INFO - root -   Epoch: [248/300] 	 loss = 0.2970
2023/10/22 19:14:55 - INFO - root -   train_accuracy = 0.8036
2023/10/22 19:15:17 - INFO - root -   Epoch: [249/300][0/84], lr: 0.00000157 	 loss = 0.0040(0.0040)
2023/10/22 19:16:24 - INFO - root -   Epoch: [249/300][20/84], lr: 0.00000157 	 loss = 0.0715(0.2199)
2023/10/22 19:17:15 - INFO - root -   Epoch: [249/300][40/84], lr: 0.00000157 	 loss = 0.0067(0.2220)
2023/10/22 19:18:24 - INFO - root -   Epoch: [249/300][60/84], lr: 0.00000157 	 loss = 0.0164(0.2668)
2023/10/22 19:19:17 - INFO - root -   Epoch: [249/300][80/84], lr: 0.00000157 	 loss = 0.3269(0.2877)
2023/10/22 19:19:21 - INFO - root -   Epoch: [249/300] 	 loss = 0.2867
2023/10/22 19:20:18 - INFO - root -   precision = 0.9070
2023/10/22 19:20:18 - INFO - root -   eval_loss = 0.3056
2023/10/22 19:20:18 - INFO - root -   eval_acc = 0.9070
2023/10/22 19:20:19 - INFO - root -   train_accuracy = 0.8512
2023/10/22 19:20:58 - INFO - root -   Epoch: [250/300][0/84], lr: 0.00000157 	 loss = 0.0195(0.0195)
2023/10/22 19:21:48 - INFO - root -   Epoch: [250/300][20/84], lr: 0.00000157 	 loss = 0.5195(0.2734)
2023/10/22 19:23:01 - INFO - root -   Epoch: [250/300][40/84], lr: 0.00000157 	 loss = 0.0091(0.2921)
2023/10/22 19:23:58 - INFO - root -   Epoch: [250/300][60/84], lr: 0.00000157 	 loss = 0.0461(0.3467)
2023/10/22 19:24:49 - INFO - root -   Epoch: [250/300][80/84], lr: 0.00000157 	 loss = 0.2480(0.3309)
2023/10/22 19:24:53 - INFO - root -   Epoch: [250/300] 	 loss = 0.3288
2023/10/22 19:24:53 - INFO - root -   train_accuracy = 0.7976
2023/10/22 19:25:14 - INFO - root -   Epoch: [251/300][0/84], lr: 0.00000158 	 loss = 0.0009(0.0009)
2023/10/22 19:26:12 - INFO - root -   Epoch: [251/300][20/84], lr: 0.00000158 	 loss = 0.0117(0.3907)
2023/10/22 19:27:35 - INFO - root -   Epoch: [251/300][40/84], lr: 0.00000158 	 loss = 0.1193(0.3495)
2023/10/22 19:28:38 - INFO - root -   Epoch: [251/300][60/84], lr: 0.00000158 	 loss = 0.1167(0.3153)
2023/10/22 19:29:24 - INFO - root -   Epoch: [251/300][80/84], lr: 0.00000158 	 loss = 0.1371(0.2898)
2023/10/22 19:29:28 - INFO - root -   Epoch: [251/300] 	 loss = 0.2906
2023/10/22 19:29:28 - INFO - root -   train_accuracy = 0.8631
2023/10/22 19:29:57 - INFO - root -   Epoch: [252/300][0/84], lr: 0.00000159 	 loss = 0.0766(0.0766)
2023/10/22 19:31:12 - INFO - root -   Epoch: [252/300][20/84], lr: 0.00000159 	 loss = 0.9518(0.5703)
2023/10/22 19:32:11 - INFO - root -   Epoch: [252/300][40/84], lr: 0.00000159 	 loss = 0.0025(0.3965)
2023/10/22 19:33:20 - INFO - root -   Epoch: [252/300][60/84], lr: 0.00000159 	 loss = 0.0124(0.3226)
2023/10/22 19:34:08 - INFO - root -   Epoch: [252/300][80/84], lr: 0.00000159 	 loss = 0.1712(0.3006)
2023/10/22 19:34:10 - INFO - root -   Epoch: [252/300] 	 loss = 0.2900
2023/10/22 19:34:10 - INFO - root -   train_accuracy = 0.8690
2023/10/22 19:34:32 - INFO - root -   Epoch: [253/300][0/84], lr: 0.00000159 	 loss = 0.0152(0.0152)
2023/10/22 19:35:43 - INFO - root -   Epoch: [253/300][20/84], lr: 0.00000159 	 loss = 0.2664(0.2685)
2023/10/22 19:36:42 - INFO - root -   Epoch: [253/300][40/84], lr: 0.00000159 	 loss = 0.1874(0.3045)
2023/10/22 19:37:56 - INFO - root -   Epoch: [253/300][60/84], lr: 0.00000159 	 loss = 0.4356(0.2902)
2023/10/22 19:38:36 - INFO - root -   Epoch: [253/300][80/84], lr: 0.00000159 	 loss = 0.3582(0.2995)
2023/10/22 19:38:37 - INFO - root -   Epoch: [253/300] 	 loss = 0.2897
2023/10/22 19:38:37 - INFO - root -   train_accuracy = 0.8333
2023/10/22 19:38:59 - INFO - root -   Epoch: [254/300][0/84], lr: 0.00000160 	 loss = 0.0013(0.0013)
2023/10/22 19:39:59 - INFO - root -   Epoch: [254/300][20/84], lr: 0.00000160 	 loss = 0.1768(0.3920)
2023/10/22 19:41:18 - INFO - root -   Epoch: [254/300][40/84], lr: 0.00000160 	 loss = 0.0018(0.3504)
2023/10/22 19:42:15 - INFO - root -   Epoch: [254/300][60/84], lr: 0.00000160 	 loss = 0.3520(0.3272)
2023/10/22 19:43:21 - INFO - root -   Epoch: [254/300][80/84], lr: 0.00000160 	 loss = 0.6867(0.3461)
2023/10/22 19:43:22 - INFO - root -   Epoch: [254/300] 	 loss = 0.3422
2023/10/22 19:44:19 - INFO - root -   precision = 0.9070
2023/10/22 19:44:19 - INFO - root -   eval_loss = 0.3610
2023/10/22 19:44:19 - INFO - root -   eval_acc = 0.9070
2023/10/22 19:44:20 - INFO - root -   train_accuracy = 0.8274
2023/10/22 19:44:57 - INFO - root -   Epoch: [255/300][0/84], lr: 0.00000160 	 loss = 0.4351(0.4351)
2023/10/22 19:45:55 - INFO - root -   Epoch: [255/300][20/84], lr: 0.00000160 	 loss = 0.3167(0.4832)
2023/10/22 19:47:18 - INFO - root -   Epoch: [255/300][40/84], lr: 0.00000160 	 loss = 0.4427(0.4081)
2023/10/22 19:48:24 - INFO - root -   Epoch: [255/300][60/84], lr: 0.00000160 	 loss = 0.0571(0.3758)
2023/10/22 19:49:10 - INFO - root -   Epoch: [255/300][80/84], lr: 0.00000160 	 loss = 0.5768(0.3429)
2023/10/22 19:49:11 - INFO - root -   Epoch: [255/300] 	 loss = 0.3396
2023/10/22 19:49:11 - INFO - root -   train_accuracy = 0.8274
2023/10/22 19:49:33 - INFO - root -   Epoch: [256/300][0/84], lr: 0.00000161 	 loss = 0.0186(0.0186)
2023/10/22 19:50:40 - INFO - root -   Epoch: [256/300][20/84], lr: 0.00000161 	 loss = 0.0145(0.1922)
2023/10/22 19:51:50 - INFO - root -   Epoch: [256/300][40/84], lr: 0.00000161 	 loss = 0.0568(0.2697)
2023/10/22 19:53:02 - INFO - root -   Epoch: [256/300][60/84], lr: 0.00000161 	 loss = 0.3116(0.2545)
2023/10/22 19:53:47 - INFO - root -   Epoch: [256/300][80/84], lr: 0.00000161 	 loss = 0.1746(0.2586)
2023/10/22 19:53:48 - INFO - root -   Epoch: [256/300] 	 loss = 0.2587
2023/10/22 19:53:48 - INFO - root -   train_accuracy = 0.8810
2023/10/22 19:54:10 - INFO - root -   Epoch: [257/300][0/84], lr: 0.00000161 	 loss = 0.0019(0.0019)
2023/10/22 19:55:15 - INFO - root -   Epoch: [257/300][20/84], lr: 0.00000161 	 loss = 0.0951(0.2665)
2023/10/22 19:56:07 - INFO - root -   Epoch: [257/300][40/84], lr: 0.00000161 	 loss = 0.0210(0.2389)
2023/10/22 19:57:15 - INFO - root -   Epoch: [257/300][60/84], lr: 0.00000161 	 loss = 0.0018(0.2306)
2023/10/22 19:58:10 - INFO - root -   Epoch: [257/300][80/84], lr: 0.00000161 	 loss = 0.5323(0.2436)
2023/10/22 19:58:11 - INFO - root -   Epoch: [257/300] 	 loss = 0.2475
2023/10/22 19:58:11 - INFO - root -   train_accuracy = 0.8631
2023/10/22 19:58:34 - INFO - root -   Epoch: [258/300][0/84], lr: 0.00000162 	 loss = 0.1668(0.1668)
2023/10/22 19:59:34 - INFO - root -   Epoch: [258/300][20/84], lr: 0.00000162 	 loss = 0.0904(0.3103)
2023/10/22 20:00:45 - INFO - root -   Epoch: [258/300][40/84], lr: 0.00000162 	 loss = 0.0324(0.3200)
2023/10/22 20:01:44 - INFO - root -   Epoch: [258/300][60/84], lr: 0.00000162 	 loss = 0.4547(0.2980)
2023/10/22 20:02:46 - INFO - root -   Epoch: [258/300][80/84], lr: 0.00000162 	 loss = 0.0792(0.3047)
2023/10/22 20:02:47 - INFO - root -   Epoch: [258/300] 	 loss = 0.3071
2023/10/22 20:02:47 - INFO - root -   train_accuracy = 0.8095
2023/10/22 20:03:25 - INFO - root -   Epoch: [259/300][0/84], lr: 0.00000163 	 loss = 0.1343(0.1343)
2023/10/22 20:04:30 - INFO - root -   Epoch: [259/300][20/84], lr: 0.00000163 	 loss = 0.3891(0.2825)
2023/10/22 20:05:35 - INFO - root -   Epoch: [259/300][40/84], lr: 0.00000163 	 loss = 0.0014(0.2495)
2023/10/22 20:06:47 - INFO - root -   Epoch: [259/300][60/84], lr: 0.00000163 	 loss = 0.7990(0.2926)
2023/10/22 20:07:24 - INFO - root -   Epoch: [259/300][80/84], lr: 0.00000163 	 loss = 0.0017(0.2727)
2023/10/22 20:07:31 - INFO - root -   Epoch: [259/300] 	 loss = 0.2654
2023/10/22 20:08:28 - INFO - root -   precision = 0.8605
2023/10/22 20:08:28 - INFO - root -   eval_loss = 0.3865
2023/10/22 20:08:28 - INFO - root -   eval_acc = 0.8605
2023/10/22 20:08:29 - INFO - root -   train_accuracy = 0.8512
2023/10/22 20:08:51 - INFO - root -   Epoch: [260/300][0/84], lr: 0.00000163 	 loss = 0.0022(0.0022)
2023/10/22 20:09:59 - INFO - root -   Epoch: [260/300][20/84], lr: 0.00000163 	 loss = 0.0115(0.2702)
2023/10/22 20:10:59 - INFO - root -   Epoch: [260/300][40/84], lr: 0.00000163 	 loss = 0.0051(0.2670)
2023/10/22 20:12:05 - INFO - root -   Epoch: [260/300][60/84], lr: 0.00000163 	 loss = 0.7472(0.2571)
2023/10/22 20:13:10 - INFO - root -   Epoch: [260/300][80/84], lr: 0.00000163 	 loss = 0.7629(0.2959)
2023/10/22 20:13:11 - INFO - root -   Epoch: [260/300] 	 loss = 0.2952
2023/10/22 20:13:11 - INFO - root -   train_accuracy = 0.8631
2023/10/22 20:13:42 - INFO - root -   Epoch: [261/300][0/84], lr: 0.00000164 	 loss = 0.6208(0.6208)
2023/10/22 20:14:41 - INFO - root -   Epoch: [261/300][20/84], lr: 0.00000164 	 loss = 0.2235(0.2315)
2023/10/22 20:15:43 - INFO - root -   Epoch: [261/300][40/84], lr: 0.00000164 	 loss = 0.1528(0.2727)
2023/10/22 20:16:57 - INFO - root -   Epoch: [261/300][60/84], lr: 0.00000164 	 loss = 0.0075(0.2651)
2023/10/22 20:17:51 - INFO - root -   Epoch: [261/300][80/84], lr: 0.00000164 	 loss = 0.9130(0.2842)
2023/10/22 20:17:55 - INFO - root -   Epoch: [261/300] 	 loss = 0.2868
2023/10/22 20:17:55 - INFO - root -   train_accuracy = 0.8750
2023/10/22 20:18:24 - INFO - root -   Epoch: [262/300][0/84], lr: 0.00000164 	 loss = 0.0112(0.0112)
2023/10/22 20:19:31 - INFO - root -   Epoch: [262/300][20/84], lr: 0.00000164 	 loss = 0.0676(0.3694)
2023/10/22 20:20:39 - INFO - root -   Epoch: [262/300][40/84], lr: 0.00000164 	 loss = 0.0121(0.3526)
2023/10/22 20:21:42 - INFO - root -   Epoch: [262/300][60/84], lr: 0.00000164 	 loss = 0.1739(0.3279)
2023/10/22 20:22:31 - INFO - root -   Epoch: [262/300][80/84], lr: 0.00000164 	 loss = 0.4804(0.3257)
2023/10/22 20:22:32 - INFO - root -   Epoch: [262/300] 	 loss = 0.3194
2023/10/22 20:22:32 - INFO - root -   train_accuracy = 0.7917
2023/10/22 20:22:54 - INFO - root -   Epoch: [263/300][0/84], lr: 0.00000165 	 loss = 0.0083(0.0083)
2023/10/22 20:23:57 - INFO - root -   Epoch: [263/300][20/84], lr: 0.00000165 	 loss = 0.1655(0.2646)
2023/10/22 20:25:15 - INFO - root -   Epoch: [263/300][40/84], lr: 0.00000165 	 loss = 0.0981(0.2611)
2023/10/22 20:26:11 - INFO - root -   Epoch: [263/300][60/84], lr: 0.00000165 	 loss = 0.2343(0.2462)
2023/10/22 20:27:06 - INFO - root -   Epoch: [263/300][80/84], lr: 0.00000165 	 loss = 0.6646(0.2562)
2023/10/22 20:27:07 - INFO - root -   Epoch: [263/300] 	 loss = 0.2515
2023/10/22 20:27:07 - INFO - root -   train_accuracy = 0.8929
2023/10/22 20:27:37 - INFO - root -   Epoch: [264/300][0/84], lr: 0.00000166 	 loss = 0.0012(0.0012)
2023/10/22 20:28:41 - INFO - root -   Epoch: [264/300][20/84], lr: 0.00000166 	 loss = 2.1105(0.3926)
2023/10/22 20:29:37 - INFO - root -   Epoch: [264/300][40/84], lr: 0.00000166 	 loss = 0.0008(0.3176)
2023/10/22 20:31:00 - INFO - root -   Epoch: [264/300][60/84], lr: 0.00000166 	 loss = 0.3617(0.3004)
2023/10/22 20:31:42 - INFO - root -   Epoch: [264/300][80/84], lr: 0.00000166 	 loss = 0.6792(0.3031)
2023/10/22 20:31:47 - INFO - root -   Epoch: [264/300] 	 loss = 0.2957
2023/10/22 20:32:44 - INFO - root -   precision = 0.8605
2023/10/22 20:32:44 - INFO - root -   eval_loss = 0.3941
2023/10/22 20:32:44 - INFO - root -   eval_acc = 0.8605
2023/10/22 20:32:45 - INFO - root -   train_accuracy = 0.8274
2023/10/22 20:33:15 - INFO - root -   Epoch: [265/300][0/84], lr: 0.00000166 	 loss = 0.4489(0.4489)
2023/10/22 20:34:13 - INFO - root -   Epoch: [265/300][20/84], lr: 0.00000166 	 loss = 0.0176(0.3080)
2023/10/22 20:35:13 - INFO - root -   Epoch: [265/300][40/84], lr: 0.00000166 	 loss = 0.0156(0.3206)
2023/10/22 20:36:31 - INFO - root -   Epoch: [265/300][60/84], lr: 0.00000166 	 loss = 0.5831(0.2867)
2023/10/22 20:37:17 - INFO - root -   Epoch: [265/300][80/84], lr: 0.00000166 	 loss = 0.0317(0.2626)
2023/10/22 20:37:19 - INFO - root -   Epoch: [265/300] 	 loss = 0.2591
2023/10/22 20:37:19 - INFO - root -   train_accuracy = 0.8631
2023/10/22 20:37:41 - INFO - root -   Epoch: [266/300][0/84], lr: 0.00000167 	 loss = 0.0458(0.0458)
2023/10/22 20:39:07 - INFO - root -   Epoch: [266/300][20/84], lr: 0.00000167 	 loss = 0.0953(0.2816)
2023/10/22 20:39:57 - INFO - root -   Epoch: [266/300][40/84], lr: 0.00000167 	 loss = 0.0026(0.2563)
2023/10/22 20:41:10 - INFO - root -   Epoch: [266/300][60/84], lr: 0.00000167 	 loss = 0.2945(0.2623)
2023/10/22 20:41:54 - INFO - root -   Epoch: [266/300][80/84], lr: 0.00000167 	 loss = 0.0087(0.2620)
2023/10/22 20:41:59 - INFO - root -   Epoch: [266/300] 	 loss = 0.2589
2023/10/22 20:41:59 - INFO - root -   train_accuracy = 0.8690
2023/10/22 20:42:29 - INFO - root -   Epoch: [267/300][0/84], lr: 0.00000167 	 loss = 0.0019(0.0019)
2023/10/22 20:43:35 - INFO - root -   Epoch: [267/300][20/84], lr: 0.00000167 	 loss = 0.1006(0.2142)
2023/10/22 20:44:46 - INFO - root -   Epoch: [267/300][40/84], lr: 0.00000167 	 loss = 0.0060(0.3018)
2023/10/22 20:45:52 - INFO - root -   Epoch: [267/300][60/84], lr: 0.00000167 	 loss = 0.4523(0.2938)
2023/10/22 20:46:44 - INFO - root -   Epoch: [267/300][80/84], lr: 0.00000167 	 loss = 0.4240(0.2970)
2023/10/22 20:46:46 - INFO - root -   Epoch: [267/300] 	 loss = 0.3077
2023/10/22 20:46:46 - INFO - root -   train_accuracy = 0.8631
2023/10/22 20:47:24 - INFO - root -   Epoch: [268/300][0/84], lr: 0.00000168 	 loss = 0.4416(0.4416)
2023/10/22 20:48:15 - INFO - root -   Epoch: [268/300][20/84], lr: 0.00000168 	 loss = 0.0105(0.2223)
2023/10/22 20:49:33 - INFO - root -   Epoch: [268/300][40/84], lr: 0.00000168 	 loss = 0.3065(0.2691)
2023/10/22 20:50:30 - INFO - root -   Epoch: [268/300][60/84], lr: 0.00000168 	 loss = 0.6456(0.2565)
2023/10/22 20:51:24 - INFO - root -   Epoch: [268/300][80/84], lr: 0.00000168 	 loss = 0.0572(0.2352)
2023/10/22 20:51:25 - INFO - root -   Epoch: [268/300] 	 loss = 0.2335
2023/10/22 20:51:25 - INFO - root -   train_accuracy = 0.8571
2023/10/22 20:51:56 - INFO - root -   Epoch: [269/300][0/84], lr: 0.00000169 	 loss = 0.0008(0.0008)
2023/10/22 20:52:52 - INFO - root -   Epoch: [269/300][20/84], lr: 0.00000169 	 loss = 0.0041(0.0927)
2023/10/22 20:53:44 - INFO - root -   Epoch: [269/300][40/84], lr: 0.00000169 	 loss = 0.0044(0.1581)
2023/10/22 20:54:51 - INFO - root -   Epoch: [269/300][60/84], lr: 0.00000169 	 loss = 0.0429(0.1500)
2023/10/22 20:55:43 - INFO - root -   Epoch: [269/300][80/84], lr: 0.00000169 	 loss = 0.2438(0.1770)
2023/10/22 20:55:45 - INFO - root -   Epoch: [269/300] 	 loss = 0.1789
2023/10/22 20:56:41 - INFO - root -   precision = 0.8837
2023/10/22 20:56:41 - INFO - root -   eval_loss = 0.3870
2023/10/22 20:56:41 - INFO - root -   eval_acc = 0.8837
2023/10/22 20:56:42 - INFO - root -   train_accuracy = 0.9167
2023/10/22 20:57:12 - INFO - root -   Epoch: [270/300][0/84], lr: 0.00000169 	 loss = 0.1931(0.1931)
2023/10/22 20:58:20 - INFO - root -   Epoch: [270/300][20/84], lr: 0.00000169 	 loss = 1.8445(0.3496)
2023/10/22 20:59:39 - INFO - root -   Epoch: [270/300][40/84], lr: 0.00000169 	 loss = 0.4488(0.3589)
2023/10/22 21:00:33 - INFO - root -   Epoch: [270/300][60/84], lr: 0.00000169 	 loss = 0.0185(0.3352)
2023/10/22 21:01:30 - INFO - root -   Epoch: [270/300][80/84], lr: 0.00000169 	 loss = 0.4061(0.3228)
2023/10/22 21:01:32 - INFO - root -   Epoch: [270/300] 	 loss = 0.3192
2023/10/22 21:01:32 - INFO - root -   train_accuracy = 0.8036
2023/10/22 21:02:02 - INFO - root -   Epoch: [271/300][0/84], lr: 0.00000170 	 loss = 0.0121(0.0121)
2023/10/22 21:03:08 - INFO - root -   Epoch: [271/300][20/84], lr: 0.00000170 	 loss = 0.4960(0.3928)
2023/10/22 21:04:21 - INFO - root -   Epoch: [271/300][40/84], lr: 0.00000170 	 loss = 0.0022(0.2804)
2023/10/22 21:05:18 - INFO - root -   Epoch: [271/300][60/84], lr: 0.00000170 	 loss = 0.1260(0.3113)
2023/10/22 21:06:05 - INFO - root -   Epoch: [271/300][80/84], lr: 0.00000170 	 loss = 0.7118(0.3314)
2023/10/22 21:06:13 - INFO - root -   Epoch: [271/300] 	 loss = 0.3333
2023/10/22 21:06:13 - INFO - root -   train_accuracy = 0.8274
2023/10/22 21:06:35 - INFO - root -   Epoch: [272/300][0/84], lr: 0.00000170 	 loss = 0.0167(0.0167)
2023/10/22 21:07:40 - INFO - root -   Epoch: [272/300][20/84], lr: 0.00000170 	 loss = 0.0398(0.2954)
2023/10/22 21:09:01 - INFO - root -   Epoch: [272/300][40/84], lr: 0.00000170 	 loss = 0.0118(0.3027)
2023/10/22 21:09:43 - INFO - root -   Epoch: [272/300][60/84], lr: 0.00000170 	 loss = 0.2218(0.2564)
2023/10/22 21:10:50 - INFO - root -   Epoch: [272/300][80/84], lr: 0.00000170 	 loss = 0.0430(0.2482)
2023/10/22 21:10:52 - INFO - root -   Epoch: [272/300] 	 loss = 0.2458
2023/10/22 21:10:52 - INFO - root -   train_accuracy = 0.8631
2023/10/22 21:11:13 - INFO - root -   Epoch: [273/300][0/84], lr: 0.00000171 	 loss = 0.0492(0.0492)
2023/10/22 21:12:21 - INFO - root -   Epoch: [273/300][20/84], lr: 0.00000171 	 loss = 0.0095(0.2339)
2023/10/22 21:13:33 - INFO - root -   Epoch: [273/300][40/84], lr: 0.00000171 	 loss = 0.3216(0.3183)
2023/10/22 21:14:38 - INFO - root -   Epoch: [273/300][60/84], lr: 0.00000171 	 loss = 0.3688(0.2904)
2023/10/22 21:15:37 - INFO - root -   Epoch: [273/300][80/84], lr: 0.00000171 	 loss = 0.3913(0.2754)
2023/10/22 21:15:39 - INFO - root -   Epoch: [273/300] 	 loss = 0.2843
2023/10/22 21:15:39 - INFO - root -   train_accuracy = 0.8452
2023/10/22 21:16:16 - INFO - root -   Epoch: [274/300][0/84], lr: 0.00000171 	 loss = 0.6186(0.6186)
2023/10/22 21:17:07 - INFO - root -   Epoch: [274/300][20/84], lr: 0.00000171 	 loss = 0.0922(0.3358)
2023/10/22 21:18:02 - INFO - root -   Epoch: [274/300][40/84], lr: 0.00000171 	 loss = 0.3392(0.2640)
2023/10/22 21:19:17 - INFO - root -   Epoch: [274/300][60/84], lr: 0.00000171 	 loss = 0.0042(0.2402)
2023/10/22 21:20:02 - INFO - root -   Epoch: [274/300][80/84], lr: 0.00000171 	 loss = 0.2033(0.2217)
2023/10/22 21:20:04 - INFO - root -   Epoch: [274/300] 	 loss = 0.2190
2023/10/22 21:21:01 - INFO - root -   precision = 0.8372
2023/10/22 21:21:01 - INFO - root -   eval_loss = 0.3698
2023/10/22 21:21:01 - INFO - root -   eval_acc = 0.8372
2023/10/22 21:21:02 - INFO - root -   train_accuracy = 0.8869
2023/10/22 21:21:33 - INFO - root -   Epoch: [275/300][0/84], lr: 0.00000172 	 loss = 0.1763(0.1763)
2023/10/22 21:22:23 - INFO - root -   Epoch: [275/300][20/84], lr: 0.00000172 	 loss = 0.3539(0.3200)
2023/10/22 21:23:45 - INFO - root -   Epoch: [275/300][40/84], lr: 0.00000172 	 loss = 0.0007(0.2402)
2023/10/22 21:24:44 - INFO - root -   Epoch: [275/300][60/84], lr: 0.00000172 	 loss = 0.0238(0.2459)
2023/10/22 21:25:48 - INFO - root -   Epoch: [275/300][80/84], lr: 0.00000172 	 loss = 0.3628(0.2638)
2023/10/22 21:25:49 - INFO - root -   Epoch: [275/300] 	 loss = 0.2678
2023/10/22 21:25:49 - INFO - root -   train_accuracy = 0.8393
2023/10/22 21:26:19 - INFO - root -   Epoch: [276/300][0/84], lr: 0.00000173 	 loss = 0.2222(0.2222)
2023/10/22 21:27:23 - INFO - root -   Epoch: [276/300][20/84], lr: 0.00000173 	 loss = 0.0328(0.2905)
2023/10/22 21:28:28 - INFO - root -   Epoch: [276/300][40/84], lr: 0.00000173 	 loss = 0.0023(0.2407)
2023/10/22 21:29:45 - INFO - root -   Epoch: [276/300][60/84], lr: 0.00000173 	 loss = 0.0073(0.2824)
2023/10/22 21:30:36 - INFO - root -   Epoch: [276/300][80/84], lr: 0.00000173 	 loss = 0.0207(0.2980)
2023/10/22 21:30:37 - INFO - root -   Epoch: [276/300] 	 loss = 0.2899
2023/10/22 21:30:37 - INFO - root -   train_accuracy = 0.8512
2023/10/22 21:30:58 - INFO - root -   Epoch: [277/300][0/84], lr: 0.00000173 	 loss = 0.0005(0.0005)
2023/10/22 21:32:16 - INFO - root -   Epoch: [277/300][20/84], lr: 0.00000173 	 loss = 0.7022(0.3467)
2023/10/22 21:33:13 - INFO - root -   Epoch: [277/300][40/84], lr: 0.00000173 	 loss = 0.0092(0.2742)
2023/10/22 21:34:31 - INFO - root -   Epoch: [277/300][60/84], lr: 0.00000173 	 loss = 0.0053(0.2735)
2023/10/22 21:35:10 - INFO - root -   Epoch: [277/300][80/84], lr: 0.00000173 	 loss = 0.0375(0.2544)
2023/10/22 21:35:13 - INFO - root -   Epoch: [277/300] 	 loss = 0.2585
2023/10/22 21:35:13 - INFO - root -   train_accuracy = 0.8571
2023/10/22 21:35:43 - INFO - root -   Epoch: [278/300][0/84], lr: 0.00000174 	 loss = 0.1572(0.1572)
2023/10/22 21:36:34 - INFO - root -   Epoch: [278/300][20/84], lr: 0.00000174 	 loss = 0.0129(0.1770)
2023/10/22 21:37:44 - INFO - root -   Epoch: [278/300][40/84], lr: 0.00000174 	 loss = 0.7552(0.2156)
2023/10/22 21:38:51 - INFO - root -   Epoch: [278/300][60/84], lr: 0.00000174 	 loss = 0.1954(0.2379)
2023/10/22 21:39:54 - INFO - root -   Epoch: [278/300][80/84], lr: 0.00000174 	 loss = 0.4629(0.2683)
2023/10/22 21:39:55 - INFO - root -   Epoch: [278/300] 	 loss = 0.2640
2023/10/22 21:39:55 - INFO - root -   train_accuracy = 0.8571
2023/10/22 21:40:26 - INFO - root -   Epoch: [279/300][0/84], lr: 0.00000174 	 loss = 0.2873(0.2873)
2023/10/22 21:41:25 - INFO - root -   Epoch: [279/300][20/84], lr: 0.00000174 	 loss = 0.0032(0.2030)
2023/10/22 21:42:21 - INFO - root -   Epoch: [279/300][40/84], lr: 0.00000174 	 loss = 0.0292(0.2621)
2023/10/22 21:43:28 - INFO - root -   Epoch: [279/300][60/84], lr: 0.00000174 	 loss = 0.6057(0.2411)
2023/10/22 21:44:18 - INFO - root -   Epoch: [279/300][80/84], lr: 0.00000174 	 loss = 0.1612(0.2407)
2023/10/22 21:44:21 - INFO - root -   Epoch: [279/300] 	 loss = 0.2341
2023/10/22 21:45:20 - INFO - root -   precision = 0.8837
2023/10/22 21:45:20 - INFO - root -   eval_loss = 0.2811
2023/10/22 21:45:20 - INFO - root -   eval_acc = 0.8837
2023/10/22 21:45:21 - INFO - root -   train_accuracy = 0.8750
2023/10/22 21:45:42 - INFO - root -   Epoch: [280/300][0/84], lr: 0.00000175 	 loss = 0.0192(0.0192)
2023/10/22 21:46:42 - INFO - root -   Epoch: [280/300][20/84], lr: 0.00000175 	 loss = 0.0865(0.3525)
2023/10/22 21:48:04 - INFO - root -   Epoch: [280/300][40/84], lr: 0.00000175 	 loss = 0.0042(0.3285)
2023/10/22 21:49:13 - INFO - root -   Epoch: [280/300][60/84], lr: 0.00000175 	 loss = 0.0024(0.2888)
2023/10/22 21:50:00 - INFO - root -   Epoch: [280/300][80/84], lr: 0.00000175 	 loss = 1.0610(0.2752)
2023/10/22 21:50:01 - INFO - root -   Epoch: [280/300] 	 loss = 0.2663
2023/10/22 21:50:01 - INFO - root -   train_accuracy = 0.8750
2023/10/22 21:50:23 - INFO - root -   Epoch: [281/300][0/84], lr: 0.00000176 	 loss = 0.0002(0.0002)
2023/10/22 21:51:31 - INFO - root -   Epoch: [281/300][20/84], lr: 0.00000176 	 loss = 1.5379(0.2874)
2023/10/22 21:52:40 - INFO - root -   Epoch: [281/300][40/84], lr: 0.00000176 	 loss = 0.0006(0.2672)
2023/10/22 21:53:47 - INFO - root -   Epoch: [281/300][60/84], lr: 0.00000176 	 loss = 0.0037(0.2818)
2023/10/22 21:54:42 - INFO - root -   Epoch: [281/300][80/84], lr: 0.00000176 	 loss = 0.1617(0.2597)
2023/10/22 21:54:43 - INFO - root -   Epoch: [281/300] 	 loss = 0.2591
2023/10/22 21:54:43 - INFO - root -   train_accuracy = 0.8393
2023/10/22 21:55:13 - INFO - root -   Epoch: [282/300][0/84], lr: 0.00000176 	 loss = 1.0294(1.0294)
2023/10/22 21:56:12 - INFO - root -   Epoch: [282/300][20/84], lr: 0.00000176 	 loss = 0.5126(0.3457)
2023/10/22 21:57:29 - INFO - root -   Epoch: [282/300][40/84], lr: 0.00000176 	 loss = 0.0191(0.3476)
2023/10/22 21:58:32 - INFO - root -   Epoch: [282/300][60/84], lr: 0.00000176 	 loss = 0.0587(0.3231)
2023/10/22 21:59:26 - INFO - root -   Epoch: [282/300][80/84], lr: 0.00000176 	 loss = 0.5209(0.3301)
2023/10/22 21:59:27 - INFO - root -   Epoch: [282/300] 	 loss = 0.3216
2023/10/22 21:59:27 - INFO - root -   train_accuracy = 0.8393
2023/10/22 21:59:49 - INFO - root -   Epoch: [283/300][0/84], lr: 0.00000177 	 loss = 0.0032(0.0032)
2023/10/22 22:01:12 - INFO - root -   Epoch: [283/300][20/84], lr: 0.00000177 	 loss = 2.2560(0.4536)
2023/10/22 22:02:07 - INFO - root -   Epoch: [283/300][40/84], lr: 0.00000177 	 loss = 0.0011(0.4492)
2023/10/22 22:03:26 - INFO - root -   Epoch: [283/300][60/84], lr: 0.00000177 	 loss = 0.0492(0.4319)
2023/10/22 22:03:57 - INFO - root -   Epoch: [283/300][80/84], lr: 0.00000177 	 loss = 0.0249(0.3870)
2023/10/22 22:04:01 - INFO - root -   Epoch: [283/300] 	 loss = 0.3773
2023/10/22 22:04:01 - INFO - root -   train_accuracy = 0.8155
2023/10/22 22:04:22 - INFO - root -   Epoch: [284/300][0/84], lr: 0.00000177 	 loss = 0.0408(0.0408)
2023/10/22 22:05:28 - INFO - root -   Epoch: [284/300][20/84], lr: 0.00000177 	 loss = 0.1427(0.3255)
2023/10/22 22:06:35 - INFO - root -   Epoch: [284/300][40/84], lr: 0.00000177 	 loss = 0.5476(0.2698)
2023/10/22 22:07:51 - INFO - root -   Epoch: [284/300][60/84], lr: 0.00000177 	 loss = 0.0712(0.2610)
2023/10/22 22:08:46 - INFO - root -   Epoch: [284/300][80/84], lr: 0.00000177 	 loss = 0.9323(0.2615)
2023/10/22 22:08:49 - INFO - root -   Epoch: [284/300] 	 loss = 0.2621
2023/10/22 22:09:45 - INFO - root -   precision = 0.9070
2023/10/22 22:09:45 - INFO - root -   eval_loss = 0.3374
2023/10/22 22:09:45 - INFO - root -   eval_acc = 0.9070
2023/10/22 22:09:46 - INFO - root -   train_accuracy = 0.8571
2023/10/22 22:10:16 - INFO - root -   Epoch: [285/300][0/84], lr: 0.00000178 	 loss = 0.5874(0.5874)
2023/10/22 22:11:21 - INFO - root -   Epoch: [285/300][20/84], lr: 0.00000178 	 loss = 0.0633(0.2629)
2023/10/22 22:12:28 - INFO - root -   Epoch: [285/300][40/84], lr: 0.00000178 	 loss = 0.2034(0.2860)
2023/10/22 22:13:30 - INFO - root -   Epoch: [285/300][60/84], lr: 0.00000178 	 loss = 0.0100(0.3043)
2023/10/22 22:14:29 - INFO - root -   Epoch: [285/300][80/84], lr: 0.00000178 	 loss = 0.7866(0.2875)
2023/10/22 22:14:30 - INFO - root -   Epoch: [285/300] 	 loss = 0.2927
2023/10/22 22:14:30 - INFO - root -   train_accuracy = 0.8274
2023/10/22 22:14:52 - INFO - root -   Epoch: [286/300][0/84], lr: 0.00000179 	 loss = 0.0010(0.0010)
2023/10/22 22:16:07 - INFO - root -   Epoch: [286/300][20/84], lr: 0.00000179 	 loss = 0.5203(0.1831)
2023/10/22 22:17:05 - INFO - root -   Epoch: [286/300][40/84], lr: 0.00000179 	 loss = 0.0064(0.2199)
2023/10/22 22:18:12 - INFO - root -   Epoch: [286/300][60/84], lr: 0.00000179 	 loss = 0.0741(0.2428)
2023/10/22 22:19:01 - INFO - root -   Epoch: [286/300][80/84], lr: 0.00000179 	 loss = 0.0156(0.2255)
2023/10/22 22:19:02 - INFO - root -   Epoch: [286/300] 	 loss = 0.2177
2023/10/22 22:19:02 - INFO - root -   train_accuracy = 0.8810
2023/10/22 22:19:24 - INFO - root -   Epoch: [287/300][0/84], lr: 0.00000179 	 loss = 0.0003(0.0003)
2023/10/22 22:20:16 - INFO - root -   Epoch: [287/300][20/84], lr: 0.00000179 	 loss = 0.0703(0.1979)
2023/10/22 22:21:48 - INFO - root -   Epoch: [287/300][40/84], lr: 0.00000179 	 loss = 0.6395(0.2105)
2023/10/22 22:22:51 - INFO - root -   Epoch: [287/300][60/84], lr: 0.00000179 	 loss = 0.0015(0.2525)
2023/10/22 22:23:35 - INFO - root -   Epoch: [287/300][80/84], lr: 0.00000179 	 loss = 0.0028(0.2881)
2023/10/22 22:23:39 - INFO - root -   Epoch: [287/300] 	 loss = 0.2996
2023/10/22 22:23:39 - INFO - root -   train_accuracy = 0.8333
2023/10/22 22:24:01 - INFO - root -   Epoch: [288/300][0/84], lr: 0.00000180 	 loss = 0.0001(0.0001)
2023/10/22 22:25:07 - INFO - root -   Epoch: [288/300][20/84], lr: 0.00000180 	 loss = 0.0297(0.2446)
2023/10/22 22:26:06 - INFO - root -   Epoch: [288/300][40/84], lr: 0.00000180 	 loss = 0.7978(0.2953)
2023/10/22 22:27:20 - INFO - root -   Epoch: [288/300][60/84], lr: 0.00000180 	 loss = 0.3448(0.3175)
2023/10/22 22:28:11 - INFO - root -   Epoch: [288/300][80/84], lr: 0.00000180 	 loss = 0.0675(0.2971)
2023/10/22 22:28:21 - INFO - root -   Epoch: [288/300] 	 loss = 0.2903
2023/10/22 22:28:21 - INFO - root -   train_accuracy = 0.8690
2023/10/22 22:28:43 - INFO - root -   Epoch: [289/300][0/84], lr: 0.00000180 	 loss = 0.0017(0.0017)
2023/10/22 22:29:48 - INFO - root -   Epoch: [289/300][20/84], lr: 0.00000180 	 loss = 0.0193(0.1941)
2023/10/22 22:30:58 - INFO - root -   Epoch: [289/300][40/84], lr: 0.00000180 	 loss = 0.0805(0.2215)
2023/10/22 22:32:10 - INFO - root -   Epoch: [289/300][60/84], lr: 0.00000180 	 loss = 0.0292(0.2161)
2023/10/22 22:32:53 - INFO - root -   Epoch: [289/300][80/84], lr: 0.00000180 	 loss = 0.0037(0.2201)
2023/10/22 22:32:57 - INFO - root -   Epoch: [289/300] 	 loss = 0.2227
2023/10/22 22:33:54 - INFO - root -   precision = 0.8837
2023/10/22 22:33:54 - INFO - root -   eval_loss = 0.5219
2023/10/22 22:33:54 - INFO - root -   eval_acc = 0.8837
2023/10/22 22:33:55 - INFO - root -   train_accuracy = 0.8631
2023/10/22 22:34:16 - INFO - root -   Epoch: [290/300][0/84], lr: 0.00000181 	 loss = 0.0598(0.0598)
2023/10/22 22:35:31 - INFO - root -   Epoch: [290/300][20/84], lr: 0.00000181 	 loss = 0.7798(0.3794)
2023/10/22 22:36:46 - INFO - root -   Epoch: [290/300][40/84], lr: 0.00000181 	 loss = 0.4568(0.3163)
2023/10/22 22:38:04 - INFO - root -   Epoch: [290/300][60/84], lr: 0.00000181 	 loss = 0.1726(0.3060)
2023/10/22 22:38:44 - INFO - root -   Epoch: [290/300][80/84], lr: 0.00000181 	 loss = 0.0207(0.3212)
2023/10/22 22:38:45 - INFO - root -   Epoch: [290/300] 	 loss = 0.3144
2023/10/22 22:38:45 - INFO - root -   train_accuracy = 0.8214
2023/10/22 22:39:16 - INFO - root -   Epoch: [291/300][0/84], lr: 0.00000181 	 loss = 0.5499(0.5499)
2023/10/22 22:40:28 - INFO - root -   Epoch: [291/300][20/84], lr: 0.00000181 	 loss = 0.0241(0.2319)
2023/10/22 22:41:22 - INFO - root -   Epoch: [291/300][40/84], lr: 0.00000181 	 loss = 0.0031(0.2393)
2023/10/22 22:42:30 - INFO - root -   Epoch: [291/300][60/84], lr: 0.00000181 	 loss = 0.4659(0.2537)
2023/10/22 22:43:18 - INFO - root -   Epoch: [291/300][80/84], lr: 0.00000181 	 loss = 0.2533(0.2555)
2023/10/22 22:43:27 - INFO - root -   Epoch: [291/300] 	 loss = 0.2625
2023/10/22 22:43:27 - INFO - root -   train_accuracy = 0.8452
2023/10/22 22:44:05 - INFO - root -   Epoch: [292/300][0/84], lr: 0.00000182 	 loss = 0.2406(0.2406)
2023/10/22 22:44:48 - INFO - root -   Epoch: [292/300][20/84], lr: 0.00000182 	 loss = 0.1692(0.1099)
2023/10/22 22:45:57 - INFO - root -   Epoch: [292/300][40/84], lr: 0.00000182 	 loss = 0.0009(0.1866)
2023/10/22 22:47:10 - INFO - root -   Epoch: [292/300][60/84], lr: 0.00000182 	 loss = 0.2538(0.2292)
2023/10/22 22:48:02 - INFO - root -   Epoch: [292/300][80/84], lr: 0.00000182 	 loss = 0.0536(0.2420)
2023/10/22 22:48:03 - INFO - root -   Epoch: [292/300] 	 loss = 0.2374
2023/10/22 22:48:03 - INFO - root -   train_accuracy = 0.8631
2023/10/22 22:48:25 - INFO - root -   Epoch: [293/300][0/84], lr: 0.00000183 	 loss = 0.0007(0.0007)
2023/10/22 22:49:37 - INFO - root -   Epoch: [293/300][20/84], lr: 0.00000183 	 loss = 0.0201(0.2739)
2023/10/22 22:50:36 - INFO - root -   Epoch: [293/300][40/84], lr: 0.00000183 	 loss = 0.0037(0.2525)
2023/10/22 22:51:51 - INFO - root -   Epoch: [293/300][60/84], lr: 0.00000183 	 loss = 0.0008(0.2411)
2023/10/22 22:52:35 - INFO - root -   Epoch: [293/300][80/84], lr: 0.00000183 	 loss = 0.0473(0.2258)
2023/10/22 22:52:37 - INFO - root -   Epoch: [293/300] 	 loss = 0.2300
2023/10/22 22:52:37 - INFO - root -   train_accuracy = 0.8631
2023/10/22 22:53:07 - INFO - root -   Epoch: [294/300][0/84], lr: 0.00000183 	 loss = 0.2320(0.2320)
2023/10/22 22:54:14 - INFO - root -   Epoch: [294/300][20/84], lr: 0.00000183 	 loss = 0.0030(0.2713)
2023/10/22 22:55:15 - INFO - root -   Epoch: [294/300][40/84], lr: 0.00000183 	 loss = 0.0052(0.2296)
2023/10/22 22:56:16 - INFO - root -   Epoch: [294/300][60/84], lr: 0.00000183 	 loss = 0.0008(0.2193)
2023/10/22 22:57:06 - INFO - root -   Epoch: [294/300][80/84], lr: 0.00000183 	 loss = 0.0438(0.2067)
2023/10/22 22:57:07 - INFO - root -   Epoch: [294/300] 	 loss = 0.2124
2023/10/22 22:58:05 - INFO - root -   precision = 0.8605
2023/10/22 22:58:05 - INFO - root -   eval_loss = 0.6083
2023/10/22 22:58:05 - INFO - root -   eval_acc = 0.8605
2023/10/22 22:58:06 - INFO - root -   train_accuracy = 0.8869
2023/10/22 22:58:28 - INFO - root -   Epoch: [295/300][0/84], lr: 0.00000184 	 loss = 0.0009(0.0009)
2023/10/22 22:59:40 - INFO - root -   Epoch: [295/300][20/84], lr: 0.00000184 	 loss = 1.1367(0.2770)
2023/10/22 23:00:33 - INFO - root -   Epoch: [295/300][40/84], lr: 0.00000184 	 loss = 0.0574(0.2911)
2023/10/22 23:01:37 - INFO - root -   Epoch: [295/300][60/84], lr: 0.00000184 	 loss = 0.2574(0.2780)
2023/10/22 23:02:41 - INFO - root -   Epoch: [295/300][80/84], lr: 0.00000184 	 loss = 0.5903(0.2831)
2023/10/22 23:02:43 - INFO - root -   Epoch: [295/300] 	 loss = 0.2771
2023/10/22 23:02:43 - INFO - root -   train_accuracy = 0.8452
2023/10/22 23:03:13 - INFO - root -   Epoch: [296/300][0/84], lr: 0.00000184 	 loss = 0.1171(0.1171)
2023/10/22 23:04:27 - INFO - root -   Epoch: [296/300][20/84], lr: 0.00000184 	 loss = 0.0330(0.2513)
2023/10/22 23:05:34 - INFO - root -   Epoch: [296/300][40/84], lr: 0.00000184 	 loss = 0.0004(0.2353)
2023/10/22 23:06:48 - INFO - root -   Epoch: [296/300][60/84], lr: 0.00000184 	 loss = 0.0055(0.3256)
2023/10/22 23:07:34 - INFO - root -   Epoch: [296/300][80/84], lr: 0.00000184 	 loss = 0.3300(0.2844)
2023/10/22 23:07:35 - INFO - root -   Epoch: [296/300] 	 loss = 0.3011
2023/10/22 23:07:35 - INFO - root -   train_accuracy = 0.8929
2023/10/22 23:08:05 - INFO - root -   Epoch: [297/300][0/84], lr: 0.00000185 	 loss = 0.0072(0.0072)
2023/10/22 23:09:12 - INFO - root -   Epoch: [297/300][20/84], lr: 0.00000185 	 loss = 1.2430(0.4150)
2023/10/22 23:10:11 - INFO - root -   Epoch: [297/300][40/84], lr: 0.00000185 	 loss = 0.0086(0.4121)
2023/10/22 23:11:34 - INFO - root -   Epoch: [297/300][60/84], lr: 0.00000185 	 loss = 0.3630(0.3750)
2023/10/22 23:12:22 - INFO - root -   Epoch: [297/300][80/84], lr: 0.00000185 	 loss = 1.0773(0.3745)
2023/10/22 23:12:29 - INFO - root -   Epoch: [297/300] 	 loss = 0.3862
2023/10/22 23:12:29 - INFO - root -   train_accuracy = 0.7917
2023/10/22 23:12:51 - INFO - root -   Epoch: [298/300][0/84], lr: 0.00000186 	 loss = 0.0021(0.0021)
2023/10/22 23:14:11 - INFO - root -   Epoch: [298/300][20/84], lr: 0.00000186 	 loss = 0.0328(0.3077)
2023/10/22 23:15:13 - INFO - root -   Epoch: [298/300][40/84], lr: 0.00000186 	 loss = 0.0219(0.2470)
2023/10/22 23:16:30 - INFO - root -   Epoch: [298/300][60/84], lr: 0.00000186 	 loss = 0.7136(0.3075)
2023/10/22 23:17:07 - INFO - root -   Epoch: [298/300][80/84], lr: 0.00000186 	 loss = 0.0229(0.2783)
2023/10/22 23:17:08 - INFO - root -   Epoch: [298/300] 	 loss = 0.2738
2023/10/22 23:17:08 - INFO - root -   train_accuracy = 0.8690
2023/10/22 23:17:31 - INFO - root -   Epoch: [299/300][0/84], lr: 0.00000186 	 loss = 0.2325(0.2325)
2023/10/22 23:18:31 - INFO - root -   Epoch: [299/300][20/84], lr: 0.00000186 	 loss = 0.4790(0.2530)
2023/10/22 23:19:48 - INFO - root -   Epoch: [299/300][40/84], lr: 0.00000186 	 loss = 0.0027(0.2540)
2023/10/22 23:20:37 - INFO - root -   Epoch: [299/300][60/84], lr: 0.00000186 	 loss = 0.0309(0.2491)
2023/10/22 23:21:45 - INFO - root -   Epoch: [299/300][80/84], lr: 0.00000186 	 loss = 0.2402(0.2684)
2023/10/22 23:21:46 - INFO - root -   Epoch: [299/300] 	 loss = 0.2658
2023/10/22 23:22:43 - INFO - root -   precision = 0.8837
2023/10/22 23:22:43 - INFO - root -   eval_loss = 0.3861
2023/10/22 23:22:43 - INFO - root -   eval_acc = 0.8837
2023/10/22 23:22:44 - INFO - root -   train_accuracy = 0.8571
