2023/10/30 16:38:21 - INFO - root -   Num train examples = 169
2023/10/30 16:38:21 - INFO - root -   Num val examples = 43
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/10/30 16:38:21 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/10/30 16:38:22 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/10/30 16:38:22 - INFO - root -   backend = nccl
2023/10/30 16:38:22 - INFO - root -   batch_size = 2
2023/10/30 16:38:22 - INFO - root -   dropout = 0.5
2023/10/30 16:38:22 - INFO - root -   epochs = 300
2023/10/30 16:38:22 - INFO - root -   eval_freq = 5
2023/10/30 16:38:22 - INFO - root -   focal_loss = False
2023/10/30 16:38:22 - INFO - root -   input_size = 224
2023/10/30 16:38:22 - INFO - root -   is_pretrained = False
2023/10/30 16:38:22 - INFO - root -   label_smooth = False
2023/10/30 16:38:22 - INFO - root -   local_rank = -1
2023/10/30 16:38:22 - INFO - root -   lr = 1e-05
2023/10/30 16:38:22 - INFO - root -   lr_decay_rate = 0.1
2023/10/30 16:38:22 - INFO - root -   lr_steps = [50, 100]
2023/10/30 16:38:22 - INFO - root -   lr_type = cosine
2023/10/30 16:38:22 - INFO - root -   model_depth = 34
2023/10/30 16:38:22 - INFO - root -   model_name = resnet50
2023/10/30 16:38:22 - INFO - root -   momentum = 0.9
2023/10/30 16:38:22 - INFO - root -   num_classes = 2
2023/10/30 16:38:22 - INFO - root -   output = ./tcia_all_1p19q_outputs
2023/10/30 16:38:22 - INFO - root -   print_freq = 20
2023/10/30 16:38:22 - INFO - root -   resume = 
2023/10/30 16:38:22 - INFO - root -   start_epoch = 0
2023/10/30 16:38:22 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_train_patients.txt
2023/10/30 16:38:22 - INFO - root -   tune_from = 
2023/10/30 16:38:22 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/tcia_test_patients.txt
2023/10/30 16:38:22 - INFO - root -   warmup_epoch = 20
2023/10/30 16:38:22 - INFO - root -   warmup_multiplier = 100
2023/10/30 16:38:22 - INFO - root -   weight_decay = 0.0005
2023/10/30 16:38:22 - INFO - root -   workers = 8
2023/10/30 16:38:45 - INFO - root -   Epoch: [0/300][0/84], lr: 0.00000010 	 loss = 1.3446(1.3446)
2023/10/30 16:40:01 - INFO - root -   Epoch: [0/300][20/84], lr: 0.00000010 	 loss = 0.7123(1.0210)
2023/10/30 16:41:03 - INFO - root -   Epoch: [0/300][40/84], lr: 0.00000010 	 loss = 0.7424(1.0399)
2023/10/30 16:42:30 - INFO - root -   Epoch: [0/300][60/84], lr: 0.00000010 	 loss = 0.3595(0.9756)
2023/10/30 16:43:15 - INFO - root -   Epoch: [0/300][80/84], lr: 0.00000010 	 loss = 1.2427(0.9580)
2023/10/30 16:43:20 - INFO - root -   Epoch: [0/300] 	 loss = 0.9560
2023/10/30 16:43:20 - INFO - root -   train_accuracy = 0.3690
2023/10/30 16:43:42 - INFO - root -   Epoch: [1/300][0/84], lr: 0.00000011 	 loss = 0.6187(0.6187)
2023/10/30 16:44:45 - INFO - root -   Epoch: [1/300][20/84], lr: 0.00000011 	 loss = 0.2726(0.6168)
2023/10/30 16:45:45 - INFO - root -   Epoch: [1/300][40/84], lr: 0.00000011 	 loss = 0.6652(0.5785)
2023/10/30 16:47:02 - INFO - root -   Epoch: [1/300][60/84], lr: 0.00000011 	 loss = 0.2310(0.5765)
2023/10/30 16:47:55 - INFO - root -   Epoch: [1/300][80/84], lr: 0.00000011 	 loss = 0.6043(0.5835)
2023/10/30 16:47:56 - INFO - root -   Epoch: [1/300] 	 loss = 0.5817
2023/10/30 16:47:56 - INFO - root -   train_accuracy = 0.7262
2023/10/30 16:48:17 - INFO - root -   Epoch: [2/300][0/84], lr: 0.00000011 	 loss = 0.5621(0.5621)
2023/10/30 16:49:24 - INFO - root -   Epoch: [2/300][20/84], lr: 0.00000011 	 loss = 0.8501(0.5310)
2023/10/30 16:50:29 - INFO - root -   Epoch: [2/300][40/84], lr: 0.00000011 	 loss = 0.3297(0.4605)
2023/10/30 16:51:40 - INFO - root -   Epoch: [2/300][60/84], lr: 0.00000011 	 loss = 0.4535(0.4915)
2023/10/30 16:52:18 - INFO - root -   Epoch: [2/300][80/84], lr: 0.00000011 	 loss = 0.4556(0.4955)
2023/10/30 16:52:24 - INFO - root -   Epoch: [2/300] 	 loss = 0.4833
2023/10/30 16:52:24 - INFO - root -   train_accuracy = 0.8333
2023/10/30 16:52:53 - INFO - root -   Epoch: [3/300][0/84], lr: 0.00000012 	 loss = 0.3068(0.3068)
2023/10/30 16:53:52 - INFO - root -   Epoch: [3/300][20/84], lr: 0.00000012 	 loss = 0.4998(0.4322)
2023/10/30 16:55:16 - INFO - root -   Epoch: [3/300][40/84], lr: 0.00000012 	 loss = 0.1024(0.4494)
2023/10/30 16:56:12 - INFO - root -   Epoch: [3/300][60/84], lr: 0.00000012 	 loss = 0.2150(0.4671)
2023/10/30 16:57:05 - INFO - root -   Epoch: [3/300][80/84], lr: 0.00000012 	 loss = 0.4742(0.4629)
2023/10/30 16:57:07 - INFO - root -   Epoch: [3/300] 	 loss = 0.4539
2023/10/30 16:57:07 - INFO - root -   train_accuracy = 0.8452
2023/10/30 16:57:28 - INFO - root -   Epoch: [4/300][0/84], lr: 0.00000012 	 loss = 0.1427(0.1427)
2023/10/30 16:58:47 - INFO - root -   Epoch: [4/300][20/84], lr: 0.00000012 	 loss = 1.2247(0.4613)
2023/10/30 16:59:44 - INFO - root -   Epoch: [4/300][40/84], lr: 0.00000012 	 loss = 0.0434(0.3765)
2023/10/30 17:01:14 - INFO - root -   Epoch: [4/300][60/84], lr: 0.00000012 	 loss = 0.1806(0.4185)
2023/10/30 17:01:50 - INFO - root -   Epoch: [4/300][80/84], lr: 0.00000012 	 loss = 0.0926(0.4202)
2023/10/30 17:01:51 - INFO - root -   Epoch: [4/300] 	 loss = 0.4159
2023/10/30 17:02:58 - INFO - root -   precision = 0.8837
2023/10/30 17:02:58 - INFO - root -   eval_loss = 0.3665
2023/10/30 17:02:58 - INFO - root -   eval_acc = 0.8837
2023/10/30 17:02:59 - INFO - root -   train_accuracy = 0.8631
2023/10/30 17:03:21 - INFO - root -   Epoch: [5/300][0/84], lr: 0.00000013 	 loss = 0.2361(0.2361)
2023/10/30 17:04:27 - INFO - root -   Epoch: [5/300][20/84], lr: 0.00000013 	 loss = 1.4371(0.3080)
2023/10/30 17:05:36 - INFO - root -   Epoch: [5/300][40/84], lr: 0.00000013 	 loss = 0.1147(0.3373)
2023/10/30 17:06:44 - INFO - root -   Epoch: [5/300][60/84], lr: 0.00000013 	 loss = 0.1084(0.3972)
2023/10/30 17:07:40 - INFO - root -   Epoch: [5/300][80/84], lr: 0.00000013 	 loss = 0.0924(0.4042)
2023/10/30 17:07:44 - INFO - root -   Epoch: [5/300] 	 loss = 0.3978
2023/10/30 17:07:44 - INFO - root -   train_accuracy = 0.8690
2023/10/30 17:08:22 - INFO - root -   Epoch: [6/300][0/84], lr: 0.00000014 	 loss = 0.1052(0.1052)
2023/10/30 17:09:28 - INFO - root -   Epoch: [6/300][20/84], lr: 0.00000014 	 loss = 1.3930(0.3323)
2023/10/30 17:10:41 - INFO - root -   Epoch: [6/300][40/84], lr: 0.00000014 	 loss = 0.1127(0.3868)
2023/10/30 17:11:37 - INFO - root -   Epoch: [6/300][60/84], lr: 0.00000014 	 loss = 0.3158(0.3814)
2023/10/30 17:12:22 - INFO - root -   Epoch: [6/300][80/84], lr: 0.00000014 	 loss = 0.3753(0.4025)
2023/10/30 17:12:23 - INFO - root -   Epoch: [6/300] 	 loss = 0.3952
2023/10/30 17:12:23 - INFO - root -   train_accuracy = 0.8631
2023/10/30 17:12:53 - INFO - root -   Epoch: [7/300][0/84], lr: 0.00000014 	 loss = 0.2577(0.2577)
2023/10/30 17:14:06 - INFO - root -   Epoch: [7/300][20/84], lr: 0.00000014 	 loss = 1.2953(0.4405)
2023/10/30 17:14:56 - INFO - root -   Epoch: [7/300][40/84], lr: 0.00000014 	 loss = 0.0487(0.4167)
2023/10/30 17:16:29 - INFO - root -   Epoch: [7/300][60/84], lr: 0.00000014 	 loss = 0.2555(0.4390)
2023/10/30 17:17:10 - INFO - root -   Epoch: [7/300][80/84], lr: 0.00000014 	 loss = 0.0594(0.4397)
2023/10/30 17:17:12 - INFO - root -   Epoch: [7/300] 	 loss = 0.4301
2023/10/30 17:17:12 - INFO - root -   train_accuracy = 0.8631
2023/10/30 17:17:42 - INFO - root -   Epoch: [8/300][0/84], lr: 0.00000015 	 loss = 0.1533(0.1533)
2023/10/30 17:18:42 - INFO - root -   Epoch: [8/300][20/84], lr: 0.00000015 	 loss = 1.6750(0.4506)
2023/10/30 17:20:03 - INFO - root -   Epoch: [8/300][40/84], lr: 0.00000015 	 loss = 0.1452(0.4723)
2023/10/30 17:20:54 - INFO - root -   Epoch: [8/300][60/84], lr: 0.00000015 	 loss = 0.0685(0.4791)
2023/10/30 17:21:46 - INFO - root -   Epoch: [8/300][80/84], lr: 0.00000015 	 loss = 0.1158(0.4541)
2023/10/30 17:21:48 - INFO - root -   Epoch: [8/300] 	 loss = 0.4438
2023/10/30 17:21:48 - INFO - root -   train_accuracy = 0.8750
2023/10/30 17:22:17 - INFO - root -   Epoch: [9/300][0/84], lr: 0.00000015 	 loss = 0.1346(0.1346)
2023/10/30 17:23:18 - INFO - root -   Epoch: [9/300][20/84], lr: 0.00000015 	 loss = 1.0292(0.4064)
2023/10/30 17:24:16 - INFO - root -   Epoch: [9/300][40/84], lr: 0.00000015 	 loss = 0.0287(0.4011)
2023/10/30 17:25:34 - INFO - root -   Epoch: [9/300][60/84], lr: 0.00000015 	 loss = 0.4099(0.4416)
2023/10/30 17:26:23 - INFO - root -   Epoch: [9/300][80/84], lr: 0.00000015 	 loss = 0.0756(0.4521)
2023/10/30 17:26:24 - INFO - root -   Epoch: [9/300] 	 loss = 0.4429
2023/10/30 17:27:21 - INFO - root -   precision = 0.8837
2023/10/30 17:27:21 - INFO - root -   eval_loss = 0.3646
2023/10/30 17:27:21 - INFO - root -   eval_acc = 0.8837
2023/10/30 17:27:22 - INFO - root -   train_accuracy = 0.8750
2023/10/30 17:27:43 - INFO - root -   Epoch: [10/300][0/84], lr: 0.00000016 	 loss = 0.1996(0.1996)
2023/10/30 17:29:09 - INFO - root -   Epoch: [10/300][20/84], lr: 0.00000016 	 loss = 1.7393(0.5124)
2023/10/30 17:29:51 - INFO - root -   Epoch: [10/300][40/84], lr: 0.00000016 	 loss = 0.1527(0.4430)
2023/10/30 17:31:18 - INFO - root -   Epoch: [10/300][60/84], lr: 0.00000016 	 loss = 0.1648(0.4452)
2023/10/30 17:32:03 - INFO - root -   Epoch: [10/300][80/84], lr: 0.00000016 	 loss = 0.1728(0.4452)
2023/10/30 17:32:04 - INFO - root -   Epoch: [10/300] 	 loss = 0.4379
2023/10/30 17:32:04 - INFO - root -   train_accuracy = 0.8512
2023/10/30 17:32:26 - INFO - root -   Epoch: [11/300][0/84], lr: 0.00000016 	 loss = 0.0439(0.0439)
2023/10/30 17:33:40 - INFO - root -   Epoch: [11/300][20/84], lr: 0.00000016 	 loss = 1.0528(0.3277)
2023/10/30 17:34:35 - INFO - root -   Epoch: [11/300][40/84], lr: 0.00000016 	 loss = 0.2513(0.3922)
2023/10/30 17:35:50 - INFO - root -   Epoch: [11/300][60/84], lr: 0.00000016 	 loss = 0.2746(0.4417)
2023/10/30 17:36:45 - INFO - root -   Epoch: [11/300][80/84], lr: 0.00000016 	 loss = 0.2286(0.4526)
2023/10/30 17:36:49 - INFO - root -   Epoch: [11/300] 	 loss = 0.4410
2023/10/30 17:36:49 - INFO - root -   train_accuracy = 0.8512
2023/10/30 17:37:11 - INFO - root -   Epoch: [12/300][0/84], lr: 0.00000017 	 loss = 0.0282(0.0282)
2023/10/30 17:38:10 - INFO - root -   Epoch: [12/300][20/84], lr: 0.00000017 	 loss = 0.6294(0.3971)
2023/10/30 17:39:18 - INFO - root -   Epoch: [12/300][40/84], lr: 0.00000017 	 loss = 0.5438(0.4151)
2023/10/30 17:40:31 - INFO - root -   Epoch: [12/300][60/84], lr: 0.00000017 	 loss = 0.0989(0.4364)
2023/10/30 17:41:22 - INFO - root -   Epoch: [12/300][80/84], lr: 0.00000017 	 loss = 0.1339(0.4420)
2023/10/30 17:41:23 - INFO - root -   Epoch: [12/300] 	 loss = 0.4366
2023/10/30 17:41:23 - INFO - root -   train_accuracy = 0.8571
2023/10/30 17:41:45 - INFO - root -   Epoch: [13/300][0/84], lr: 0.00000018 	 loss = 0.1407(0.1407)
2023/10/30 17:43:01 - INFO - root -   Epoch: [13/300][20/84], lr: 0.00000018 	 loss = 1.2935(0.3755)
2023/10/30 17:44:23 - INFO - root -   Epoch: [13/300][40/84], lr: 0.00000018 	 loss = 0.0247(0.3931)
2023/10/30 17:45:21 - INFO - root -   Epoch: [13/300][60/84], lr: 0.00000018 	 loss = 0.1294(0.4284)
2023/10/30 17:46:17 - INFO - root -   Epoch: [13/300][80/84], lr: 0.00000018 	 loss = 0.0341(0.4157)
2023/10/30 17:46:18 - INFO - root -   Epoch: [13/300] 	 loss = 0.4062
2023/10/30 17:46:18 - INFO - root -   train_accuracy = 0.8810
2023/10/30 17:46:48 - INFO - root -   Epoch: [14/300][0/84], lr: 0.00000018 	 loss = 0.0572(0.0572)
2023/10/30 17:47:55 - INFO - root -   Epoch: [14/300][20/84], lr: 0.00000018 	 loss = 1.1987(0.3170)
2023/10/30 17:48:43 - INFO - root -   Epoch: [14/300][40/84], lr: 0.00000018 	 loss = 0.1598(0.3757)
2023/10/30 17:49:57 - INFO - root -   Epoch: [14/300][60/84], lr: 0.00000018 	 loss = 0.2640(0.3895)
2023/10/30 17:50:51 - INFO - root -   Epoch: [14/300][80/84], lr: 0.00000018 	 loss = 0.4278(0.3852)
2023/10/30 17:50:56 - INFO - root -   Epoch: [14/300] 	 loss = 0.3761
2023/10/30 17:51:52 - INFO - root -   precision = 0.8837
2023/10/30 17:51:52 - INFO - root -   eval_loss = 0.3591
2023/10/30 17:51:52 - INFO - root -   eval_acc = 0.8837
2023/10/30 17:51:53 - INFO - root -   train_accuracy = 0.8690
2023/10/30 17:52:15 - INFO - root -   Epoch: [15/300][0/84], lr: 0.00000019 	 loss = 0.0193(0.0193)
2023/10/30 17:53:30 - INFO - root -   Epoch: [15/300][20/84], lr: 0.00000019 	 loss = 0.7276(0.3234)
2023/10/30 17:54:38 - INFO - root -   Epoch: [15/300][40/84], lr: 0.00000019 	 loss = 0.1770(0.3479)
2023/10/30 17:55:38 - INFO - root -   Epoch: [15/300][60/84], lr: 0.00000019 	 loss = 0.3438(0.3856)
2023/10/30 17:56:44 - INFO - root -   Epoch: [15/300][80/84], lr: 0.00000019 	 loss = 0.0959(0.3621)
2023/10/30 17:56:45 - INFO - root -   Epoch: [15/300] 	 loss = 0.3563
2023/10/30 17:56:45 - INFO - root -   train_accuracy = 0.8750
2023/10/30 17:57:06 - INFO - root -   Epoch: [16/300][0/84], lr: 0.00000019 	 loss = 0.0309(0.0309)
2023/10/30 17:58:19 - INFO - root -   Epoch: [16/300][20/84], lr: 0.00000019 	 loss = 1.7617(0.4851)
2023/10/30 17:59:23 - INFO - root -   Epoch: [16/300][40/84], lr: 0.00000019 	 loss = 0.0581(0.4461)
2023/10/30 18:00:29 - INFO - root -   Epoch: [16/300][60/84], lr: 0.00000019 	 loss = 0.1414(0.4360)
2023/10/30 18:01:34 - INFO - root -   Epoch: [16/300][80/84], lr: 0.00000019 	 loss = 0.1205(0.4220)
2023/10/30 18:01:35 - INFO - root -   Epoch: [16/300] 	 loss = 0.4102
2023/10/30 18:01:35 - INFO - root -   train_accuracy = 0.8631
2023/10/30 18:01:57 - INFO - root -   Epoch: [17/300][0/84], lr: 0.00000020 	 loss = 0.1284(0.1284)
2023/10/30 18:03:04 - INFO - root -   Epoch: [17/300][20/84], lr: 0.00000020 	 loss = 0.7940(0.4121)
2023/10/30 18:03:58 - INFO - root -   Epoch: [17/300][40/84], lr: 0.00000020 	 loss = 0.1483(0.3950)
2023/10/30 18:05:29 - INFO - root -   Epoch: [17/300][60/84], lr: 0.00000020 	 loss = 0.0440(0.4262)
2023/10/30 18:06:21 - INFO - root -   Epoch: [17/300][80/84], lr: 0.00000020 	 loss = 0.2161(0.4221)
2023/10/30 18:06:23 - INFO - root -   Epoch: [17/300] 	 loss = 0.4104
2023/10/30 18:06:23 - INFO - root -   train_accuracy = 0.8571
2023/10/30 18:06:53 - INFO - root -   Epoch: [18/300][0/84], lr: 0.00000021 	 loss = 0.0639(0.0639)
2023/10/30 18:07:59 - INFO - root -   Epoch: [18/300][20/84], lr: 0.00000021 	 loss = 1.3136(0.3959)
2023/10/30 18:09:22 - INFO - root -   Epoch: [18/300][40/84], lr: 0.00000021 	 loss = 0.2295(0.3995)
2023/10/30 18:10:21 - INFO - root -   Epoch: [18/300][60/84], lr: 0.00000021 	 loss = 0.2191(0.4189)
2023/10/30 18:11:24 - INFO - root -   Epoch: [18/300][80/84], lr: 0.00000021 	 loss = 0.0750(0.4119)
2023/10/30 18:11:25 - INFO - root -   Epoch: [18/300] 	 loss = 0.4064
2023/10/30 18:11:25 - INFO - root -   train_accuracy = 0.8690
2023/10/30 18:11:54 - INFO - root -   Epoch: [19/300][0/84], lr: 0.00000021 	 loss = 0.0416(0.0416)
2023/10/30 18:12:47 - INFO - root -   Epoch: [19/300][20/84], lr: 0.00000021 	 loss = 0.9989(0.3918)
2023/10/30 18:14:00 - INFO - root -   Epoch: [19/300][40/84], lr: 0.00000021 	 loss = 0.0779(0.3908)
2023/10/30 18:15:11 - INFO - root -   Epoch: [19/300][60/84], lr: 0.00000021 	 loss = 0.2104(0.3864)
2023/10/30 18:15:51 - INFO - root -   Epoch: [19/300][80/84], lr: 0.00000021 	 loss = 0.0651(0.3889)
2023/10/30 18:15:52 - INFO - root -   Epoch: [19/300] 	 loss = 0.3834
2023/10/30 18:16:48 - INFO - root -   precision = 0.8837
2023/10/30 18:16:48 - INFO - root -   eval_loss = 0.3595
2023/10/30 18:16:48 - INFO - root -   eval_acc = 0.8837
2023/10/30 18:16:49 - INFO - root -   train_accuracy = 0.8690
2023/10/30 18:17:20 - INFO - root -   Epoch: [20/300][0/84], lr: 0.00000022 	 loss = 0.0870(0.0870)
2023/10/30 18:18:12 - INFO - root -   Epoch: [20/300][20/84], lr: 0.00000022 	 loss = 0.2493(0.3065)
2023/10/30 18:19:43 - INFO - root -   Epoch: [20/300][40/84], lr: 0.00000022 	 loss = 0.0928(0.3636)
2023/10/30 18:20:39 - INFO - root -   Epoch: [20/300][60/84], lr: 0.00000022 	 loss = 0.2130(0.4059)
2023/10/30 18:21:26 - INFO - root -   Epoch: [20/300][80/84], lr: 0.00000022 	 loss = 0.0515(0.4100)
2023/10/30 18:21:27 - INFO - root -   Epoch: [20/300] 	 loss = 0.4056
2023/10/30 18:21:27 - INFO - root -   train_accuracy = 0.8631
2023/10/30 18:22:04 - INFO - root -   Epoch: [21/300][0/84], lr: 0.00000022 	 loss = 0.0608(0.0608)
2023/10/30 18:23:02 - INFO - root -   Epoch: [21/300][20/84], lr: 0.00000022 	 loss = 0.8344(0.4115)
2023/10/30 18:24:20 - INFO - root -   Epoch: [21/300][40/84], lr: 0.00000022 	 loss = 0.0592(0.4272)
2023/10/30 18:25:17 - INFO - root -   Epoch: [21/300][60/84], lr: 0.00000022 	 loss = 0.1445(0.4484)
2023/10/30 18:26:02 - INFO - root -   Epoch: [21/300][80/84], lr: 0.00000022 	 loss = 0.1171(0.4245)
2023/10/30 18:26:03 - INFO - root -   Epoch: [21/300] 	 loss = 0.4149
2023/10/30 18:26:03 - INFO - root -   train_accuracy = 0.8690
2023/10/30 18:26:41 - INFO - root -   Epoch: [22/300][0/84], lr: 0.00000023 	 loss = 0.3435(0.3435)
2023/10/30 18:27:38 - INFO - root -   Epoch: [22/300][20/84], lr: 0.00000023 	 loss = 0.7534(0.3215)
2023/10/30 18:28:45 - INFO - root -   Epoch: [22/300][40/84], lr: 0.00000023 	 loss = 0.0466(0.3438)
2023/10/30 18:30:01 - INFO - root -   Epoch: [22/300][60/84], lr: 0.00000023 	 loss = 0.0945(0.3894)
2023/10/30 18:30:40 - INFO - root -   Epoch: [22/300][80/84], lr: 0.00000023 	 loss = 0.0372(0.3680)
2023/10/30 18:30:42 - INFO - root -   Epoch: [22/300] 	 loss = 0.3683
2023/10/30 18:30:42 - INFO - root -   train_accuracy = 0.8631
2023/10/30 18:31:12 - INFO - root -   Epoch: [23/300][0/84], lr: 0.00000024 	 loss = 0.2844(0.2844)
2023/10/30 18:32:17 - INFO - root -   Epoch: [23/300][20/84], lr: 0.00000024 	 loss = 0.6411(0.3582)
2023/10/30 18:33:15 - INFO - root -   Epoch: [23/300][40/84], lr: 0.00000024 	 loss = 0.0578(0.3508)
2023/10/30 18:34:24 - INFO - root -   Epoch: [23/300][60/84], lr: 0.00000024 	 loss = 0.1399(0.4138)
2023/10/30 18:35:15 - INFO - root -   Epoch: [23/300][80/84], lr: 0.00000024 	 loss = 0.0706(0.4160)
2023/10/30 18:35:19 - INFO - root -   Epoch: [23/300] 	 loss = 0.4050
2023/10/30 18:35:19 - INFO - root -   train_accuracy = 0.8512
2023/10/30 18:35:40 - INFO - root -   Epoch: [24/300][0/84], lr: 0.00000024 	 loss = 0.0433(0.0433)
2023/10/30 18:36:48 - INFO - root -   Epoch: [24/300][20/84], lr: 0.00000024 	 loss = 0.6112(0.2661)
2023/10/30 18:37:47 - INFO - root -   Epoch: [24/300][40/84], lr: 0.00000024 	 loss = 0.0634(0.3165)
2023/10/30 18:39:03 - INFO - root -   Epoch: [24/300][60/84], lr: 0.00000024 	 loss = 0.0679(0.3658)
2023/10/30 18:39:53 - INFO - root -   Epoch: [24/300][80/84], lr: 0.00000024 	 loss = 0.0434(0.3759)
2023/10/30 18:39:59 - INFO - root -   Epoch: [24/300] 	 loss = 0.3669
2023/10/30 18:40:56 - INFO - root -   precision = 0.8837
2023/10/30 18:40:56 - INFO - root -   eval_loss = 0.3428
2023/10/30 18:40:56 - INFO - root -   eval_acc = 0.8837
2023/10/30 18:40:57 - INFO - root -   train_accuracy = 0.8750
2023/10/30 18:41:28 - INFO - root -   Epoch: [25/300][0/84], lr: 0.00000025 	 loss = 0.2087(0.2087)
2023/10/30 18:42:32 - INFO - root -   Epoch: [25/300][20/84], lr: 0.00000025 	 loss = 0.9683(0.3308)
2023/10/30 18:43:20 - INFO - root -   Epoch: [25/300][40/84], lr: 0.00000025 	 loss = 0.0479(0.3806)
2023/10/30 18:44:31 - INFO - root -   Epoch: [25/300][60/84], lr: 0.00000025 	 loss = 0.0545(0.4202)
2023/10/30 18:45:25 - INFO - root -   Epoch: [25/300][80/84], lr: 0.00000025 	 loss = 0.0638(0.4102)
2023/10/30 18:45:31 - INFO - root -   Epoch: [25/300] 	 loss = 0.3994
2023/10/30 18:45:31 - INFO - root -   train_accuracy = 0.8631
2023/10/30 18:46:00 - INFO - root -   Epoch: [26/300][0/84], lr: 0.00000025 	 loss = 0.1917(0.1917)
2023/10/30 18:47:12 - INFO - root -   Epoch: [26/300][20/84], lr: 0.00000025 	 loss = 0.9160(0.3053)
2023/10/30 18:48:14 - INFO - root -   Epoch: [26/300][40/84], lr: 0.00000025 	 loss = 0.0542(0.3784)
2023/10/30 18:49:47 - INFO - root -   Epoch: [26/300][60/84], lr: 0.00000025 	 loss = 0.0335(0.4109)
2023/10/30 18:50:22 - INFO - root -   Epoch: [26/300][80/84], lr: 0.00000025 	 loss = 0.0750(0.3843)
2023/10/30 18:50:24 - INFO - root -   Epoch: [26/300] 	 loss = 0.3782
2023/10/30 18:50:24 - INFO - root -   train_accuracy = 0.8869
2023/10/30 18:50:53 - INFO - root -   Epoch: [27/300][0/84], lr: 0.00000026 	 loss = 0.0641(0.0641)
2023/10/30 18:51:51 - INFO - root -   Epoch: [27/300][20/84], lr: 0.00000026 	 loss = 0.3814(0.3399)
2023/10/30 18:52:55 - INFO - root -   Epoch: [27/300][40/84], lr: 0.00000026 	 loss = 0.1949(0.3538)
2023/10/30 18:53:55 - INFO - root -   Epoch: [27/300][60/84], lr: 0.00000026 	 loss = 0.0306(0.4197)
2023/10/30 18:54:56 - INFO - root -   Epoch: [27/300][80/84], lr: 0.00000026 	 loss = 0.0680(0.4257)
2023/10/30 18:54:57 - INFO - root -   Epoch: [27/300] 	 loss = 0.4177
2023/10/30 18:54:57 - INFO - root -   train_accuracy = 0.8631
2023/10/30 18:55:19 - INFO - root -   Epoch: [28/300][0/84], lr: 0.00000027 	 loss = 0.1015(0.1015)
2023/10/30 18:56:11 - INFO - root -   Epoch: [28/300][20/84], lr: 0.00000027 	 loss = 0.9031(0.3746)
2023/10/30 18:57:28 - INFO - root -   Epoch: [28/300][40/84], lr: 0.00000027 	 loss = 0.0968(0.3849)
2023/10/30 18:58:46 - INFO - root -   Epoch: [28/300][60/84], lr: 0.00000027 	 loss = 0.0650(0.3950)
2023/10/30 18:59:27 - INFO - root -   Epoch: [28/300][80/84], lr: 0.00000027 	 loss = 0.0508(0.3965)
2023/10/30 18:59:28 - INFO - root -   Epoch: [28/300] 	 loss = 0.3917
2023/10/30 18:59:28 - INFO - root -   train_accuracy = 0.8571
2023/10/30 18:59:49 - INFO - root -   Epoch: [29/300][0/84], lr: 0.00000027 	 loss = 0.0389(0.0389)
2023/10/30 19:01:02 - INFO - root -   Epoch: [29/300][20/84], lr: 0.00000027 	 loss = 0.8902(0.3333)
2023/10/30 19:01:53 - INFO - root -   Epoch: [29/300][40/84], lr: 0.00000027 	 loss = 0.0828(0.3556)
2023/10/30 19:03:02 - INFO - root -   Epoch: [29/300][60/84], lr: 0.00000027 	 loss = 0.1316(0.3814)
2023/10/30 19:04:06 - INFO - root -   Epoch: [29/300][80/84], lr: 0.00000027 	 loss = 0.4088(0.4025)
2023/10/30 19:04:07 - INFO - root -   Epoch: [29/300] 	 loss = 0.4008
2023/10/30 19:05:04 - INFO - root -   precision = 0.8837
2023/10/30 19:05:04 - INFO - root -   eval_loss = 0.3390
2023/10/30 19:05:04 - INFO - root -   eval_acc = 0.8837
2023/10/30 19:05:05 - INFO - root -   train_accuracy = 0.8452
2023/10/30 19:05:42 - INFO - root -   Epoch: [30/300][0/84], lr: 0.00000028 	 loss = 0.3039(0.3039)
2023/10/30 19:06:33 - INFO - root -   Epoch: [30/300][20/84], lr: 0.00000028 	 loss = 0.9806(0.3143)
2023/10/30 19:07:42 - INFO - root -   Epoch: [30/300][40/84], lr: 0.00000028 	 loss = 0.0619(0.3349)
2023/10/30 19:08:48 - INFO - root -   Epoch: [30/300][60/84], lr: 0.00000028 	 loss = 0.0610(0.3701)
2023/10/30 19:09:40 - INFO - root -   Epoch: [30/300][80/84], lr: 0.00000028 	 loss = 0.0516(0.3490)
2023/10/30 19:09:41 - INFO - root -   Epoch: [30/300] 	 loss = 0.3409
2023/10/30 19:09:41 - INFO - root -   train_accuracy = 0.8631
2023/10/30 19:10:23 - INFO - root -   Epoch: [31/300][0/84], lr: 0.00000028 	 loss = 0.0329(0.0329)
2023/10/30 19:11:14 - INFO - root -   Epoch: [31/300][20/84], lr: 0.00000028 	 loss = 1.2088(0.3967)
2023/10/30 19:12:15 - INFO - root -   Epoch: [31/300][40/84], lr: 0.00000028 	 loss = 0.0412(0.3666)
2023/10/30 19:13:13 - INFO - root -   Epoch: [31/300][60/84], lr: 0.00000028 	 loss = 0.0757(0.4010)
2023/10/30 19:14:21 - INFO - root -   Epoch: [31/300][80/84], lr: 0.00000028 	 loss = 0.1795(0.3990)
2023/10/30 19:14:23 - INFO - root -   Epoch: [31/300] 	 loss = 0.3911
2023/10/30 19:14:23 - INFO - root -   train_accuracy = 0.8631
2023/10/30 19:14:44 - INFO - root -   Epoch: [32/300][0/84], lr: 0.00000029 	 loss = 0.0430(0.0430)
2023/10/30 19:15:59 - INFO - root -   Epoch: [32/300][20/84], lr: 0.00000029 	 loss = 0.6219(0.3494)
2023/10/30 19:17:03 - INFO - root -   Epoch: [32/300][40/84], lr: 0.00000029 	 loss = 0.0828(0.3786)
2023/10/30 19:18:09 - INFO - root -   Epoch: [32/300][60/84], lr: 0.00000029 	 loss = 0.0335(0.4002)
2023/10/30 19:18:59 - INFO - root -   Epoch: [32/300][80/84], lr: 0.00000029 	 loss = 0.0857(0.3880)
2023/10/30 19:19:01 - INFO - root -   Epoch: [32/300] 	 loss = 0.3816
2023/10/30 19:19:01 - INFO - root -   train_accuracy = 0.8631
2023/10/30 19:19:31 - INFO - root -   Epoch: [33/300][0/84], lr: 0.00000029 	 loss = 0.0328(0.0328)
2023/10/30 19:20:47 - INFO - root -   Epoch: [33/300][20/84], lr: 0.00000029 	 loss = 0.8574(0.3927)
2023/10/30 19:21:40 - INFO - root -   Epoch: [33/300][40/84], lr: 0.00000029 	 loss = 0.0555(0.3971)
2023/10/30 19:22:52 - INFO - root -   Epoch: [33/300][60/84], lr: 0.00000029 	 loss = 0.0897(0.4162)
2023/10/30 19:23:42 - INFO - root -   Epoch: [33/300][80/84], lr: 0.00000029 	 loss = 0.0547(0.4121)
2023/10/30 19:23:43 - INFO - root -   Epoch: [33/300] 	 loss = 0.4044
2023/10/30 19:23:43 - INFO - root -   train_accuracy = 0.8631
2023/10/30 19:24:13 - INFO - root -   Epoch: [34/300][0/84], lr: 0.00000030 	 loss = 0.3273(0.3273)
2023/10/30 19:25:13 - INFO - root -   Epoch: [34/300][20/84], lr: 0.00000030 	 loss = 1.0498(0.4045)
2023/10/30 19:26:28 - INFO - root -   Epoch: [34/300][40/84], lr: 0.00000030 	 loss = 0.0799(0.3455)
2023/10/30 19:27:26 - INFO - root -   Epoch: [34/300][60/84], lr: 0.00000030 	 loss = 0.0789(0.4084)
2023/10/30 19:28:10 - INFO - root -   Epoch: [34/300][80/84], lr: 0.00000030 	 loss = 0.0414(0.3840)
2023/10/30 19:28:14 - INFO - root -   Epoch: [34/300] 	 loss = 0.3745
2023/10/30 19:29:10 - INFO - root -   precision = 0.8837
2023/10/30 19:29:10 - INFO - root -   eval_loss = 0.3391
2023/10/30 19:29:10 - INFO - root -   eval_acc = 0.8837
2023/10/30 19:29:11 - INFO - root -   train_accuracy = 0.8690
2023/10/30 19:29:33 - INFO - root -   Epoch: [35/300][0/84], lr: 0.00000031 	 loss = 0.0324(0.0324)
2023/10/30 19:30:25 - INFO - root -   Epoch: [35/300][20/84], lr: 0.00000031 	 loss = 0.3141(0.3070)
2023/10/30 19:31:54 - INFO - root -   Epoch: [35/300][40/84], lr: 0.00000031 	 loss = 0.1029(0.3334)
2023/10/30 19:32:46 - INFO - root -   Epoch: [35/300][60/84], lr: 0.00000031 	 loss = 0.1567(0.3732)
2023/10/30 19:33:46 - INFO - root -   Epoch: [35/300][80/84], lr: 0.00000031 	 loss = 0.2297(0.3719)
2023/10/30 19:33:47 - INFO - root -   Epoch: [35/300] 	 loss = 0.3658
2023/10/30 19:33:47 - INFO - root -   train_accuracy = 0.8929
2023/10/30 19:34:18 - INFO - root -   Epoch: [36/300][0/84], lr: 0.00000031 	 loss = 0.0401(0.0401)
2023/10/30 19:35:34 - INFO - root -   Epoch: [36/300][20/84], lr: 0.00000031 	 loss = 1.4658(0.3233)
2023/10/30 19:36:22 - INFO - root -   Epoch: [36/300][40/84], lr: 0.00000031 	 loss = 0.0651(0.3368)
2023/10/30 19:37:36 - INFO - root -   Epoch: [36/300][60/84], lr: 0.00000031 	 loss = 0.0395(0.3740)
2023/10/30 19:38:30 - INFO - root -   Epoch: [36/300][80/84], lr: 0.00000031 	 loss = 0.1418(0.3758)
2023/10/30 19:38:31 - INFO - root -   Epoch: [36/300] 	 loss = 0.3699
2023/10/30 19:38:31 - INFO - root -   train_accuracy = 0.8810
2023/10/30 19:38:53 - INFO - root -   Epoch: [37/300][0/84], lr: 0.00000032 	 loss = 0.0942(0.0942)
2023/10/30 19:40:08 - INFO - root -   Epoch: [37/300][20/84], lr: 0.00000032 	 loss = 1.0836(0.3565)
2023/10/30 19:41:20 - INFO - root -   Epoch: [37/300][40/84], lr: 0.00000032 	 loss = 0.2415(0.3710)
2023/10/30 19:42:17 - INFO - root -   Epoch: [37/300][60/84], lr: 0.00000032 	 loss = 0.0807(0.3816)
2023/10/30 19:43:14 - INFO - root -   Epoch: [37/300][80/84], lr: 0.00000032 	 loss = 0.0277(0.3755)
2023/10/30 19:43:15 - INFO - root -   Epoch: [37/300] 	 loss = 0.3661
2023/10/30 19:43:15 - INFO - root -   train_accuracy = 0.8571
2023/10/30 19:43:36 - INFO - root -   Epoch: [38/300][0/84], lr: 0.00000032 	 loss = 0.0906(0.0906)
2023/10/30 19:44:52 - INFO - root -   Epoch: [38/300][20/84], lr: 0.00000032 	 loss = 0.7150(0.3898)
2023/10/30 19:46:22 - INFO - root -   Epoch: [38/300][40/84], lr: 0.00000032 	 loss = 0.1136(0.3844)
2023/10/30 19:47:02 - INFO - root -   Epoch: [38/300][60/84], lr: 0.00000032 	 loss = 0.1355(0.3911)
2023/10/30 19:47:58 - INFO - root -   Epoch: [38/300][80/84], lr: 0.00000032 	 loss = 0.0302(0.3919)
2023/10/30 19:47:59 - INFO - root -   Epoch: [38/300] 	 loss = 0.3821
2023/10/30 19:47:59 - INFO - root -   train_accuracy = 0.8631
2023/10/30 19:48:22 - INFO - root -   Epoch: [39/300][0/84], lr: 0.00000033 	 loss = 0.1021(0.1021)
2023/10/30 19:49:38 - INFO - root -   Epoch: [39/300][20/84], lr: 0.00000033 	 loss = 1.0258(0.3384)
2023/10/30 19:50:36 - INFO - root -   Epoch: [39/300][40/84], lr: 0.00000033 	 loss = 0.0471(0.3485)
2023/10/30 19:51:51 - INFO - root -   Epoch: [39/300][60/84], lr: 0.00000033 	 loss = 0.2673(0.3819)
2023/10/30 19:52:47 - INFO - root -   Epoch: [39/300][80/84], lr: 0.00000033 	 loss = 0.1111(0.3739)
2023/10/30 19:52:51 - INFO - root -   Epoch: [39/300] 	 loss = 0.3680
2023/10/30 19:53:47 - INFO - root -   precision = 0.8837
2023/10/30 19:53:47 - INFO - root -   eval_loss = 0.3427
2023/10/30 19:53:47 - INFO - root -   eval_acc = 0.8837
2023/10/30 19:53:48 - INFO - root -   train_accuracy = 0.8690
2023/10/30 19:54:10 - INFO - root -   Epoch: [40/300][0/84], lr: 0.00000034 	 loss = 0.0273(0.0273)
2023/10/30 19:55:20 - INFO - root -   Epoch: [40/300][20/84], lr: 0.00000034 	 loss = 0.4567(0.3118)
2023/10/30 19:56:15 - INFO - root -   Epoch: [40/300][40/84], lr: 0.00000034 	 loss = 0.1700(0.3700)
2023/10/30 19:57:34 - INFO - root -   Epoch: [40/300][60/84], lr: 0.00000034 	 loss = 0.0855(0.3661)
2023/10/30 19:58:21 - INFO - root -   Epoch: [40/300][80/84], lr: 0.00000034 	 loss = 0.1183(0.3443)
2023/10/30 19:58:26 - INFO - root -   Epoch: [40/300] 	 loss = 0.3360
2023/10/30 19:58:26 - INFO - root -   train_accuracy = 0.8869
2023/10/30 19:58:48 - INFO - root -   Epoch: [41/300][0/84], lr: 0.00000034 	 loss = 0.0463(0.0463)
2023/10/30 20:00:00 - INFO - root -   Epoch: [41/300][20/84], lr: 0.00000034 	 loss = 0.6768(0.2828)
2023/10/30 20:01:08 - INFO - root -   Epoch: [41/300][40/84], lr: 0.00000034 	 loss = 0.0915(0.3520)
2023/10/30 20:02:23 - INFO - root -   Epoch: [41/300][60/84], lr: 0.00000034 	 loss = 0.1175(0.3828)
2023/10/30 20:03:04 - INFO - root -   Epoch: [41/300][80/84], lr: 0.00000034 	 loss = 0.0567(0.3883)
2023/10/30 20:03:05 - INFO - root -   Epoch: [41/300] 	 loss = 0.3803
2023/10/30 20:03:05 - INFO - root -   train_accuracy = 0.8690
2023/10/30 20:03:34 - INFO - root -   Epoch: [42/300][0/84], lr: 0.00000035 	 loss = 0.1565(0.1565)
2023/10/30 20:04:34 - INFO - root -   Epoch: [42/300][20/84], lr: 0.00000035 	 loss = 0.9486(0.3964)
2023/10/30 20:06:06 - INFO - root -   Epoch: [42/300][40/84], lr: 0.00000035 	 loss = 0.2599(0.4088)
2023/10/30 20:06:46 - INFO - root -   Epoch: [42/300][60/84], lr: 0.00000035 	 loss = 0.2486(0.4323)
2023/10/30 20:07:40 - INFO - root -   Epoch: [42/300][80/84], lr: 0.00000035 	 loss = 0.0952(0.4099)
2023/10/30 20:07:41 - INFO - root -   Epoch: [42/300] 	 loss = 0.4043
2023/10/30 20:07:41 - INFO - root -   train_accuracy = 0.8810
2023/10/30 20:08:03 - INFO - root -   Epoch: [43/300][0/84], lr: 0.00000035 	 loss = 0.1861(0.1861)
2023/10/30 20:09:14 - INFO - root -   Epoch: [43/300][20/84], lr: 0.00000035 	 loss = 0.7229(0.3467)
2023/10/30 20:10:19 - INFO - root -   Epoch: [43/300][40/84], lr: 0.00000035 	 loss = 0.0319(0.3653)
2023/10/30 20:11:39 - INFO - root -   Epoch: [43/300][60/84], lr: 0.00000035 	 loss = 0.1118(0.3888)
2023/10/30 20:12:16 - INFO - root -   Epoch: [43/300][80/84], lr: 0.00000035 	 loss = 0.0348(0.3817)
2023/10/30 20:12:22 - INFO - root -   Epoch: [43/300] 	 loss = 0.3760
2023/10/30 20:12:22 - INFO - root -   train_accuracy = 0.8810
2023/10/30 20:12:44 - INFO - root -   Epoch: [44/300][0/84], lr: 0.00000036 	 loss = 0.0212(0.0212)
2023/10/30 20:13:56 - INFO - root -   Epoch: [44/300][20/84], lr: 0.00000036 	 loss = 0.3312(0.3099)
2023/10/30 20:14:42 - INFO - root -   Epoch: [44/300][40/84], lr: 0.00000036 	 loss = 0.0667(0.3736)
2023/10/30 20:16:12 - INFO - root -   Epoch: [44/300][60/84], lr: 0.00000036 	 loss = 0.0390(0.4429)
2023/10/30 20:16:56 - INFO - root -   Epoch: [44/300][80/84], lr: 0.00000036 	 loss = 0.1539(0.4462)
2023/10/30 20:16:58 - INFO - root -   Epoch: [44/300] 	 loss = 0.4369
2023/10/30 20:17:55 - INFO - root -   precision = 0.8837
2023/10/30 20:17:55 - INFO - root -   eval_loss = 0.3396
2023/10/30 20:17:55 - INFO - root -   eval_acc = 0.8837
2023/10/30 20:17:56 - INFO - root -   train_accuracy = 0.8869
2023/10/30 20:18:34 - INFO - root -   Epoch: [45/300][0/84], lr: 0.00000037 	 loss = 0.2454(0.2454)
2023/10/30 20:19:28 - INFO - root -   Epoch: [45/300][20/84], lr: 0.00000037 	 loss = 1.3421(0.3269)
2023/10/30 20:20:42 - INFO - root -   Epoch: [45/300][40/84], lr: 0.00000037 	 loss = 0.0454(0.3357)
2023/10/30 20:21:53 - INFO - root -   Epoch: [45/300][60/84], lr: 0.00000037 	 loss = 0.0754(0.3579)
2023/10/30 20:22:34 - INFO - root -   Epoch: [45/300][80/84], lr: 0.00000037 	 loss = 0.0684(0.3574)
2023/10/30 20:22:37 - INFO - root -   Epoch: [45/300] 	 loss = 0.3550
2023/10/30 20:22:37 - INFO - root -   train_accuracy = 0.8750
2023/10/30 20:23:14 - INFO - root -   Epoch: [46/300][0/84], lr: 0.00000037 	 loss = 0.1399(0.1399)
2023/10/30 20:24:06 - INFO - root -   Epoch: [46/300][20/84], lr: 0.00000037 	 loss = 1.2901(0.3330)
2023/10/30 20:25:31 - INFO - root -   Epoch: [46/300][40/84], lr: 0.00000037 	 loss = 0.0920(0.3465)
2023/10/30 20:26:33 - INFO - root -   Epoch: [46/300][60/84], lr: 0.00000037 	 loss = 0.0316(0.4198)
2023/10/30 20:27:31 - INFO - root -   Epoch: [46/300][80/84], lr: 0.00000037 	 loss = 0.1100(0.3884)
2023/10/30 20:27:32 - INFO - root -   Epoch: [46/300] 	 loss = 0.3833
2023/10/30 20:27:32 - INFO - root -   train_accuracy = 0.8869
2023/10/30 20:27:53 - INFO - root -   Epoch: [47/300][0/84], lr: 0.00000038 	 loss = 0.0463(0.0463)
2023/10/30 20:28:53 - INFO - root -   Epoch: [47/300][20/84], lr: 0.00000038 	 loss = 0.6409(0.3274)
2023/10/30 20:30:06 - INFO - root -   Epoch: [47/300][40/84], lr: 0.00000038 	 loss = 0.0750(0.3716)
2023/10/30 20:31:20 - INFO - root -   Epoch: [47/300][60/84], lr: 0.00000038 	 loss = 0.0629(0.3874)
2023/10/30 20:32:11 - INFO - root -   Epoch: [47/300][80/84], lr: 0.00000038 	 loss = 0.0573(0.3817)
2023/10/30 20:32:14 - INFO - root -   Epoch: [47/300] 	 loss = 0.3740
2023/10/30 20:32:14 - INFO - root -   train_accuracy = 0.8810
2023/10/30 20:32:36 - INFO - root -   Epoch: [48/300][0/84], lr: 0.00000038 	 loss = 0.0422(0.0422)
2023/10/30 20:33:43 - INFO - root -   Epoch: [48/300][20/84], lr: 0.00000038 	 loss = 1.0226(0.2440)
2023/10/30 20:35:16 - INFO - root -   Epoch: [48/300][40/84], lr: 0.00000038 	 loss = 0.0676(0.2744)
2023/10/30 20:36:05 - INFO - root -   Epoch: [48/300][60/84], lr: 0.00000038 	 loss = 0.0465(0.3520)
2023/10/30 20:36:47 - INFO - root -   Epoch: [48/300][80/84], lr: 0.00000038 	 loss = 0.0326(0.3568)
2023/10/30 20:36:49 - INFO - root -   Epoch: [48/300] 	 loss = 0.3560
2023/10/30 20:36:49 - INFO - root -   train_accuracy = 0.8750
2023/10/30 20:37:19 - INFO - root -   Epoch: [49/300][0/84], lr: 0.00000039 	 loss = 0.0899(0.0899)
2023/10/30 20:38:40 - INFO - root -   Epoch: [49/300][20/84], lr: 0.00000039 	 loss = 0.3458(0.2468)
2023/10/30 20:39:43 - INFO - root -   Epoch: [49/300][40/84], lr: 0.00000039 	 loss = 0.0246(0.2695)
2023/10/30 20:41:02 - INFO - root -   Epoch: [49/300][60/84], lr: 0.00000039 	 loss = 0.1065(0.3350)
2023/10/30 20:41:37 - INFO - root -   Epoch: [49/300][80/84], lr: 0.00000039 	 loss = 0.1367(0.3402)
2023/10/30 20:41:38 - INFO - root -   Epoch: [49/300] 	 loss = 0.3344
2023/10/30 20:42:35 - INFO - root -   precision = 0.8837
2023/10/30 20:42:35 - INFO - root -   eval_loss = 0.3350
2023/10/30 20:42:35 - INFO - root -   eval_acc = 0.8837
2023/10/30 20:42:36 - INFO - root -   train_accuracy = 0.8988
2023/10/30 20:43:06 - INFO - root -   Epoch: [50/300][0/84], lr: 0.00000039 	 loss = 0.0246(0.0246)
2023/10/30 20:44:05 - INFO - root -   Epoch: [50/300][20/84], lr: 0.00000039 	 loss = 0.6362(0.3153)
2023/10/30 20:45:36 - INFO - root -   Epoch: [50/300][40/84], lr: 0.00000039 	 loss = 0.0927(0.3443)
2023/10/30 20:46:39 - INFO - root -   Epoch: [50/300][60/84], lr: 0.00000039 	 loss = 0.2173(0.3573)
2023/10/30 20:47:21 - INFO - root -   Epoch: [50/300][80/84], lr: 0.00000039 	 loss = 0.0444(0.3623)
2023/10/30 20:47:25 - INFO - root -   Epoch: [50/300] 	 loss = 0.3572
2023/10/30 20:47:25 - INFO - root -   train_accuracy = 0.8929
2023/10/30 20:47:47 - INFO - root -   Epoch: [51/300][0/84], lr: 0.00000040 	 loss = 0.0482(0.0482)
2023/10/30 20:48:53 - INFO - root -   Epoch: [51/300][20/84], lr: 0.00000040 	 loss = 0.7777(0.3173)
2023/10/30 20:49:47 - INFO - root -   Epoch: [51/300][40/84], lr: 0.00000040 	 loss = 0.0614(0.3430)
2023/10/30 20:51:12 - INFO - root -   Epoch: [51/300][60/84], lr: 0.00000040 	 loss = 0.0949(0.3787)
2023/10/30 20:51:57 - INFO - root -   Epoch: [51/300][80/84], lr: 0.00000040 	 loss = 0.2222(0.3812)
2023/10/30 20:52:04 - INFO - root -   Epoch: [51/300] 	 loss = 0.3752
2023/10/30 20:52:04 - INFO - root -   train_accuracy = 0.8750
2023/10/30 20:52:26 - INFO - root -   Epoch: [52/300][0/84], lr: 0.00000041 	 loss = 0.0345(0.0345)
2023/10/30 20:53:48 - INFO - root -   Epoch: [52/300][20/84], lr: 0.00000041 	 loss = 1.4398(0.2999)
2023/10/30 20:54:53 - INFO - root -   Epoch: [52/300][40/84], lr: 0.00000041 	 loss = 0.0421(0.3160)
2023/10/30 20:56:01 - INFO - root -   Epoch: [52/300][60/84], lr: 0.00000041 	 loss = 0.1508(0.3303)
2023/10/30 20:56:44 - INFO - root -   Epoch: [52/300][80/84], lr: 0.00000041 	 loss = 0.0217(0.3251)
2023/10/30 20:56:54 - INFO - root -   Epoch: [52/300] 	 loss = 0.3164
2023/10/30 20:56:54 - INFO - root -   train_accuracy = 0.8631
2023/10/30 20:57:25 - INFO - root -   Epoch: [53/300][0/84], lr: 0.00000041 	 loss = 0.0206(0.0206)
2023/10/30 20:58:28 - INFO - root -   Epoch: [53/300][20/84], lr: 0.00000041 	 loss = 0.4144(0.3135)
2023/10/30 20:59:31 - INFO - root -   Epoch: [53/300][40/84], lr: 0.00000041 	 loss = 0.0607(0.2845)
2023/10/30 21:00:34 - INFO - root -   Epoch: [53/300][60/84], lr: 0.00000041 	 loss = 0.1330(0.3665)
2023/10/30 21:01:31 - INFO - root -   Epoch: [53/300][80/84], lr: 0.00000041 	 loss = 0.0552(0.3483)
2023/10/30 21:01:37 - INFO - root -   Epoch: [53/300] 	 loss = 0.3449
2023/10/30 21:01:37 - INFO - root -   train_accuracy = 0.8929
2023/10/30 21:02:15 - INFO - root -   Epoch: [54/300][0/84], lr: 0.00000042 	 loss = 0.0538(0.0538)
2023/10/30 21:02:58 - INFO - root -   Epoch: [54/300][20/84], lr: 0.00000042 	 loss = 0.4246(0.3630)
2023/10/30 21:04:21 - INFO - root -   Epoch: [54/300][40/84], lr: 0.00000042 	 loss = 0.0255(0.3321)
2023/10/30 21:05:17 - INFO - root -   Epoch: [54/300][60/84], lr: 0.00000042 	 loss = 0.1094(0.3414)
2023/10/30 21:06:04 - INFO - root -   Epoch: [54/300][80/84], lr: 0.00000042 	 loss = 0.0194(0.3315)
2023/10/30 21:06:05 - INFO - root -   Epoch: [54/300] 	 loss = 0.3260
2023/10/30 21:07:01 - INFO - root -   precision = 0.8837
2023/10/30 21:07:01 - INFO - root -   eval_loss = 0.3208
2023/10/30 21:07:01 - INFO - root -   eval_acc = 0.8837
2023/10/30 21:07:02 - INFO - root -   train_accuracy = 0.8810
2023/10/30 21:07:23 - INFO - root -   Epoch: [55/300][0/84], lr: 0.00000042 	 loss = 0.0520(0.0520)
2023/10/30 21:08:38 - INFO - root -   Epoch: [55/300][20/84], lr: 0.00000042 	 loss = 0.4535(0.2696)
2023/10/30 21:09:30 - INFO - root -   Epoch: [55/300][40/84], lr: 0.00000042 	 loss = 0.0795(0.2971)
2023/10/30 21:10:38 - INFO - root -   Epoch: [55/300][60/84], lr: 0.00000042 	 loss = 0.0744(0.3316)
2023/10/30 21:11:28 - INFO - root -   Epoch: [55/300][80/84], lr: 0.00000042 	 loss = 0.1755(0.3464)
2023/10/30 21:11:36 - INFO - root -   Epoch: [55/300] 	 loss = 0.3435
2023/10/30 21:11:36 - INFO - root -   train_accuracy = 0.8750
2023/10/30 21:11:58 - INFO - root -   Epoch: [56/300][0/84], lr: 0.00000043 	 loss = 0.0304(0.0304)
2023/10/30 21:12:57 - INFO - root -   Epoch: [56/300][20/84], lr: 0.00000043 	 loss = 0.5638(0.2810)
2023/10/30 21:14:32 - INFO - root -   Epoch: [56/300][40/84], lr: 0.00000043 	 loss = 0.0300(0.3577)
2023/10/30 21:15:20 - INFO - root -   Epoch: [56/300][60/84], lr: 0.00000043 	 loss = 0.0323(0.3987)
2023/10/30 21:16:23 - INFO - root -   Epoch: [56/300][80/84], lr: 0.00000043 	 loss = 0.0830(0.3826)
2023/10/30 21:16:25 - INFO - root -   Epoch: [56/300] 	 loss = 0.3744
2023/10/30 21:16:25 - INFO - root -   train_accuracy = 0.8690
2023/10/30 21:16:46 - INFO - root -   Epoch: [57/300][0/84], lr: 0.00000044 	 loss = 0.0281(0.0281)
2023/10/30 21:17:53 - INFO - root -   Epoch: [57/300][20/84], lr: 0.00000044 	 loss = 0.7091(0.2504)
2023/10/30 21:19:00 - INFO - root -   Epoch: [57/300][40/84], lr: 0.00000044 	 loss = 0.0506(0.3011)
2023/10/30 21:20:07 - INFO - root -   Epoch: [57/300][60/84], lr: 0.00000044 	 loss = 0.2267(0.3429)
2023/10/30 21:21:07 - INFO - root -   Epoch: [57/300][80/84], lr: 0.00000044 	 loss = 0.1241(0.3336)
2023/10/30 21:21:08 - INFO - root -   Epoch: [57/300] 	 loss = 0.3309
2023/10/30 21:21:08 - INFO - root -   train_accuracy = 0.8750
2023/10/30 21:21:39 - INFO - root -   Epoch: [58/300][0/84], lr: 0.00000044 	 loss = 0.0322(0.0322)
2023/10/30 21:22:37 - INFO - root -   Epoch: [58/300][20/84], lr: 0.00000044 	 loss = 0.9949(0.2710)
2023/10/30 21:23:39 - INFO - root -   Epoch: [58/300][40/84], lr: 0.00000044 	 loss = 0.0885(0.3658)
2023/10/30 21:24:58 - INFO - root -   Epoch: [58/300][60/84], lr: 0.00000044 	 loss = 0.0929(0.4187)
2023/10/30 21:25:52 - INFO - root -   Epoch: [58/300][80/84], lr: 0.00000044 	 loss = 0.0121(0.3883)
2023/10/30 21:25:53 - INFO - root -   Epoch: [58/300] 	 loss = 0.3895
2023/10/30 21:25:53 - INFO - root -   train_accuracy = 0.8571
2023/10/30 21:26:31 - INFO - root -   Epoch: [59/300][0/84], lr: 0.00000045 	 loss = 0.2801(0.2801)
2023/10/30 21:27:33 - INFO - root -   Epoch: [59/300][20/84], lr: 0.00000045 	 loss = 1.0894(0.3635)
2023/10/30 21:28:43 - INFO - root -   Epoch: [59/300][40/84], lr: 0.00000045 	 loss = 0.0120(0.3406)
2023/10/30 21:29:43 - INFO - root -   Epoch: [59/300][60/84], lr: 0.00000045 	 loss = 0.0520(0.3767)
2023/10/30 21:30:37 - INFO - root -   Epoch: [59/300][80/84], lr: 0.00000045 	 loss = 0.1056(0.3552)
2023/10/30 21:30:39 - INFO - root -   Epoch: [59/300] 	 loss = 0.3482
2023/10/30 21:31:35 - INFO - root -   precision = 0.8837
2023/10/30 21:31:35 - INFO - root -   eval_loss = 0.3135
2023/10/30 21:31:35 - INFO - root -   eval_acc = 0.8837
2023/10/30 21:31:36 - INFO - root -   train_accuracy = 0.8690
2023/10/30 21:32:05 - INFO - root -   Epoch: [60/300][0/84], lr: 0.00000045 	 loss = 0.1474(0.1474)
2023/10/30 21:33:13 - INFO - root -   Epoch: [60/300][20/84], lr: 0.00000045 	 loss = 1.0226(0.3427)
2023/10/30 21:34:12 - INFO - root -   Epoch: [60/300][40/84], lr: 0.00000045 	 loss = 0.1078(0.3316)
2023/10/30 21:35:26 - INFO - root -   Epoch: [60/300][60/84], lr: 0.00000045 	 loss = 0.1360(0.3976)
2023/10/30 21:36:08 - INFO - root -   Epoch: [60/300][80/84], lr: 0.00000045 	 loss = 0.1026(0.3783)
2023/10/30 21:36:09 - INFO - root -   Epoch: [60/300] 	 loss = 0.3743
2023/10/30 21:36:09 - INFO - root -   train_accuracy = 0.8690
2023/10/30 21:36:38 - INFO - root -   Epoch: [61/300][0/84], lr: 0.00000046 	 loss = 0.1662(0.1662)
2023/10/30 21:37:37 - INFO - root -   Epoch: [61/300][20/84], lr: 0.00000046 	 loss = 0.8100(0.3354)
2023/10/30 21:38:55 - INFO - root -   Epoch: [61/300][40/84], lr: 0.00000046 	 loss = 0.1847(0.3175)
2023/10/30 21:40:04 - INFO - root -   Epoch: [61/300][60/84], lr: 0.00000046 	 loss = 0.0679(0.3347)
2023/10/30 21:40:51 - INFO - root -   Epoch: [61/300][80/84], lr: 0.00000046 	 loss = 0.0418(0.3296)
2023/10/30 21:40:53 - INFO - root -   Epoch: [61/300] 	 loss = 0.3215
2023/10/30 21:40:53 - INFO - root -   train_accuracy = 0.8988
2023/10/30 21:41:15 - INFO - root -   Epoch: [62/300][0/84], lr: 0.00000047 	 loss = 0.0112(0.0112)
2023/10/30 21:42:38 - INFO - root -   Epoch: [62/300][20/84], lr: 0.00000047 	 loss = 0.6911(0.2871)
2023/10/30 21:43:47 - INFO - root -   Epoch: [62/300][40/84], lr: 0.00000047 	 loss = 0.2312(0.2875)
2023/10/30 21:44:48 - INFO - root -   Epoch: [62/300][60/84], lr: 0.00000047 	 loss = 0.0627(0.3524)
2023/10/30 21:45:47 - INFO - root -   Epoch: [62/300][80/84], lr: 0.00000047 	 loss = 0.0171(0.3768)
2023/10/30 21:45:48 - INFO - root -   Epoch: [62/300] 	 loss = 0.3727
2023/10/30 21:45:48 - INFO - root -   train_accuracy = 0.8869
2023/10/30 21:46:18 - INFO - root -   Epoch: [63/300][0/84], lr: 0.00000047 	 loss = 0.0369(0.0369)
2023/10/30 21:47:29 - INFO - root -   Epoch: [63/300][20/84], lr: 0.00000047 	 loss = 0.4874(0.3596)
2023/10/30 21:48:33 - INFO - root -   Epoch: [63/300][40/84], lr: 0.00000047 	 loss = 0.0501(0.3374)
2023/10/30 21:49:39 - INFO - root -   Epoch: [63/300][60/84], lr: 0.00000047 	 loss = 0.0343(0.4169)
2023/10/30 21:50:36 - INFO - root -   Epoch: [63/300][80/84], lr: 0.00000047 	 loss = 0.0464(0.4148)
2023/10/30 21:50:38 - INFO - root -   Epoch: [63/300] 	 loss = 0.4049
2023/10/30 21:50:38 - INFO - root -   train_accuracy = 0.8690
2023/10/30 21:51:16 - INFO - root -   Epoch: [64/300][0/84], lr: 0.00000048 	 loss = 0.0873(0.0873)
2023/10/30 21:52:06 - INFO - root -   Epoch: [64/300][20/84], lr: 0.00000048 	 loss = 0.8571(0.3101)
2023/10/30 21:53:06 - INFO - root -   Epoch: [64/300][40/84], lr: 0.00000048 	 loss = 0.0333(0.3401)
2023/10/30 21:54:21 - INFO - root -   Epoch: [64/300][60/84], lr: 0.00000048 	 loss = 0.1177(0.3485)
2023/10/30 21:55:04 - INFO - root -   Epoch: [64/300][80/84], lr: 0.00000048 	 loss = 0.0311(0.3524)
2023/10/30 21:55:06 - INFO - root -   Epoch: [64/300] 	 loss = 0.3491
2023/10/30 21:56:02 - INFO - root -   precision = 0.8837
2023/10/30 21:56:02 - INFO - root -   eval_loss = 0.3089
2023/10/30 21:56:02 - INFO - root -   eval_acc = 0.8837
2023/10/30 21:56:03 - INFO - root -   train_accuracy = 0.8631
2023/10/30 21:56:33 - INFO - root -   Epoch: [65/300][0/84], lr: 0.00000048 	 loss = 0.1567(0.1567)
2023/10/30 21:57:24 - INFO - root -   Epoch: [65/300][20/84], lr: 0.00000048 	 loss = 1.3052(0.3379)
2023/10/30 21:58:38 - INFO - root -   Epoch: [65/300][40/84], lr: 0.00000048 	 loss = 0.0294(0.2964)
2023/10/30 21:59:31 - INFO - root -   Epoch: [65/300][60/84], lr: 0.00000048 	 loss = 0.1698(0.3313)
2023/10/30 22:00:41 - INFO - root -   Epoch: [65/300][80/84], lr: 0.00000048 	 loss = 0.1064(0.3473)
2023/10/30 22:00:42 - INFO - root -   Epoch: [65/300] 	 loss = 0.3408
2023/10/30 22:00:42 - INFO - root -   train_accuracy = 0.8750
2023/10/30 22:01:04 - INFO - root -   Epoch: [66/300][0/84], lr: 0.00000049 	 loss = 0.4391(0.4391)
2023/10/30 22:02:26 - INFO - root -   Epoch: [66/300][20/84], lr: 0.00000049 	 loss = 1.3157(0.3211)
2023/10/30 22:03:29 - INFO - root -   Epoch: [66/300][40/84], lr: 0.00000049 	 loss = 0.0458(0.2849)
2023/10/30 22:04:36 - INFO - root -   Epoch: [66/300][60/84], lr: 0.00000049 	 loss = 0.1150(0.3083)
2023/10/30 22:05:32 - INFO - root -   Epoch: [66/300][80/84], lr: 0.00000049 	 loss = 0.0356(0.3296)
2023/10/30 22:05:33 - INFO - root -   Epoch: [66/300] 	 loss = 0.3235
2023/10/30 22:05:33 - INFO - root -   train_accuracy = 0.8810
2023/10/30 22:06:03 - INFO - root -   Epoch: [67/300][0/84], lr: 0.00000049 	 loss = 0.0767(0.0767)
2023/10/30 22:07:05 - INFO - root -   Epoch: [67/300][20/84], lr: 0.00000049 	 loss = 0.5871(0.2724)
2023/10/30 22:08:27 - INFO - root -   Epoch: [67/300][40/84], lr: 0.00000049 	 loss = 0.0617(0.2963)
2023/10/30 22:09:25 - INFO - root -   Epoch: [67/300][60/84], lr: 0.00000049 	 loss = 0.3064(0.3492)
2023/10/30 22:10:08 - INFO - root -   Epoch: [67/300][80/84], lr: 0.00000049 	 loss = 0.1336(0.3244)
2023/10/30 22:10:10 - INFO - root -   Epoch: [67/300] 	 loss = 0.3285
2023/10/30 22:10:10 - INFO - root -   train_accuracy = 0.8690
2023/10/30 22:10:47 - INFO - root -   Epoch: [68/300][0/84], lr: 0.00000050 	 loss = 0.1592(0.1592)
2023/10/30 22:11:45 - INFO - root -   Epoch: [68/300][20/84], lr: 0.00000050 	 loss = 0.1090(0.2931)
2023/10/30 22:13:07 - INFO - root -   Epoch: [68/300][40/84], lr: 0.00000050 	 loss = 0.0404(0.3301)
2023/10/30 22:13:54 - INFO - root -   Epoch: [68/300][60/84], lr: 0.00000050 	 loss = 0.1435(0.3788)
2023/10/30 22:14:45 - INFO - root -   Epoch: [68/300][80/84], lr: 0.00000050 	 loss = 0.4094(0.3692)
2023/10/30 22:14:46 - INFO - root -   Epoch: [68/300] 	 loss = 0.3597
2023/10/30 22:14:46 - INFO - root -   train_accuracy = 0.8810
2023/10/30 22:15:16 - INFO - root -   Epoch: [69/300][0/84], lr: 0.00000051 	 loss = 0.0419(0.0419)
2023/10/30 22:16:23 - INFO - root -   Epoch: [69/300][20/84], lr: 0.00000051 	 loss = 1.1298(0.2663)
2023/10/30 22:17:44 - INFO - root -   Epoch: [69/300][40/84], lr: 0.00000051 	 loss = 0.0511(0.3118)
2023/10/30 22:18:27 - INFO - root -   Epoch: [69/300][60/84], lr: 0.00000051 	 loss = 0.0680(0.3518)
2023/10/30 22:19:21 - INFO - root -   Epoch: [69/300][80/84], lr: 0.00000051 	 loss = 0.1009(0.3725)
2023/10/30 22:19:22 - INFO - root -   Epoch: [69/300] 	 loss = 0.3638
2023/10/30 22:20:18 - INFO - root -   precision = 0.8837
2023/10/30 22:20:18 - INFO - root -   eval_loss = 0.3154
2023/10/30 22:20:18 - INFO - root -   eval_acc = 0.8837
2023/10/30 22:20:19 - INFO - root -   train_accuracy = 0.8631
2023/10/30 22:20:41 - INFO - root -   Epoch: [70/300][0/84], lr: 0.00000051 	 loss = 0.0365(0.0365)
2023/10/30 22:21:56 - INFO - root -   Epoch: [70/300][20/84], lr: 0.00000051 	 loss = 0.9984(0.2923)
2023/10/30 22:22:46 - INFO - root -   Epoch: [70/300][40/84], lr: 0.00000051 	 loss = 0.0369(0.3069)
2023/10/30 22:24:07 - INFO - root -   Epoch: [70/300][60/84], lr: 0.00000051 	 loss = 0.2000(0.3270)
2023/10/30 22:24:48 - INFO - root -   Epoch: [70/300][80/84], lr: 0.00000051 	 loss = 0.1069(0.3124)
2023/10/30 22:24:50 - INFO - root -   Epoch: [70/300] 	 loss = 0.3100
2023/10/30 22:24:50 - INFO - root -   train_accuracy = 0.8810
2023/10/30 22:25:11 - INFO - root -   Epoch: [71/300][0/84], lr: 0.00000052 	 loss = 0.0261(0.0261)
2023/10/30 22:26:17 - INFO - root -   Epoch: [71/300][20/84], lr: 0.00000052 	 loss = 1.1145(0.3233)
2023/10/30 22:27:23 - INFO - root -   Epoch: [71/300][40/84], lr: 0.00000052 	 loss = 0.0496(0.3657)
2023/10/30 22:28:46 - INFO - root -   Epoch: [71/300][60/84], lr: 0.00000052 	 loss = 0.0569(0.3680)
2023/10/30 22:29:29 - INFO - root -   Epoch: [71/300][80/84], lr: 0.00000052 	 loss = 0.0524(0.3703)
2023/10/30 22:29:30 - INFO - root -   Epoch: [71/300] 	 loss = 0.3708
2023/10/30 22:29:30 - INFO - root -   train_accuracy = 0.8690
2023/10/30 22:29:52 - INFO - root -   Epoch: [72/300][0/84], lr: 0.00000052 	 loss = 0.0167(0.0167)
2023/10/30 22:30:43 - INFO - root -   Epoch: [72/300][20/84], lr: 0.00000052 	 loss = 1.4056(0.3499)
2023/10/30 22:32:01 - INFO - root -   Epoch: [72/300][40/84], lr: 0.00000052 	 loss = 0.0707(0.3535)
2023/10/30 22:33:11 - INFO - root -   Epoch: [72/300][60/84], lr: 0.00000052 	 loss = 0.0613(0.3865)
2023/10/30 22:33:59 - INFO - root -   Epoch: [72/300][80/84], lr: 0.00000052 	 loss = 0.0324(0.3983)
2023/10/30 22:34:04 - INFO - root -   Epoch: [72/300] 	 loss = 0.3884
2023/10/30 22:34:04 - INFO - root -   train_accuracy = 0.8690
2023/10/30 22:34:25 - INFO - root -   Epoch: [73/300][0/84], lr: 0.00000053 	 loss = 0.0202(0.0202)
2023/10/30 22:35:31 - INFO - root -   Epoch: [73/300][20/84], lr: 0.00000053 	 loss = 1.2253(0.3734)
2023/10/30 22:36:30 - INFO - root -   Epoch: [73/300][40/84], lr: 0.00000053 	 loss = 0.0346(0.3571)
2023/10/30 22:37:56 - INFO - root -   Epoch: [73/300][60/84], lr: 0.00000053 	 loss = 0.2554(0.3654)
2023/10/30 22:38:35 - INFO - root -   Epoch: [73/300][80/84], lr: 0.00000053 	 loss = 0.1053(0.3779)
2023/10/30 22:38:36 - INFO - root -   Epoch: [73/300] 	 loss = 0.3699
2023/10/30 22:38:36 - INFO - root -   train_accuracy = 0.8750
2023/10/30 22:39:07 - INFO - root -   Epoch: [74/300][0/84], lr: 0.00000054 	 loss = 0.0469(0.0469)
2023/10/30 22:40:06 - INFO - root -   Epoch: [74/300][20/84], lr: 0.00000054 	 loss = 0.8665(0.2981)
2023/10/30 22:41:20 - INFO - root -   Epoch: [74/300][40/84], lr: 0.00000054 	 loss = 0.0103(0.3069)
2023/10/30 22:42:29 - INFO - root -   Epoch: [74/300][60/84], lr: 0.00000054 	 loss = 0.1781(0.3133)
2023/10/30 22:43:17 - INFO - root -   Epoch: [74/300][80/84], lr: 0.00000054 	 loss = 0.0585(0.3201)
2023/10/30 22:43:20 - INFO - root -   Epoch: [74/300] 	 loss = 0.3186
2023/10/30 22:44:16 - INFO - root -   precision = 0.8837
2023/10/30 22:44:16 - INFO - root -   eval_loss = 0.2933
2023/10/30 22:44:16 - INFO - root -   eval_acc = 0.8837
2023/10/30 22:44:17 - INFO - root -   train_accuracy = 0.8988
2023/10/30 22:44:47 - INFO - root -   Epoch: [75/300][0/84], lr: 0.00000054 	 loss = 0.1989(0.1989)
2023/10/30 22:45:46 - INFO - root -   Epoch: [75/300][20/84], lr: 0.00000054 	 loss = 0.9209(0.3059)
2023/10/30 22:46:56 - INFO - root -   Epoch: [75/300][40/84], lr: 0.00000054 	 loss = 0.0988(0.3500)
2023/10/30 22:47:58 - INFO - root -   Epoch: [75/300][60/84], lr: 0.00000054 	 loss = 0.0091(0.3961)
2023/10/30 22:48:49 - INFO - root -   Epoch: [75/300][80/84], lr: 0.00000054 	 loss = 0.0155(0.3982)
2023/10/30 22:48:50 - INFO - root -   Epoch: [75/300] 	 loss = 0.3937
2023/10/30 22:48:50 - INFO - root -   train_accuracy = 0.8571
2023/10/30 22:49:12 - INFO - root -   Epoch: [76/300][0/84], lr: 0.00000055 	 loss = 0.0845(0.0845)
2023/10/30 22:50:08 - INFO - root -   Epoch: [76/300][20/84], lr: 0.00000055 	 loss = 0.3928(0.2114)
2023/10/30 22:51:32 - INFO - root -   Epoch: [76/300][40/84], lr: 0.00000055 	 loss = 0.0186(0.3354)
2023/10/30 22:52:24 - INFO - root -   Epoch: [76/300][60/84], lr: 0.00000055 	 loss = 0.1287(0.3452)
2023/10/30 22:53:18 - INFO - root -   Epoch: [76/300][80/84], lr: 0.00000055 	 loss = 0.0335(0.3316)
2023/10/30 22:53:19 - INFO - root -   Epoch: [76/300] 	 loss = 0.3261
2023/10/30 22:53:19 - INFO - root -   train_accuracy = 0.8988
2023/10/30 22:53:40 - INFO - root -   Epoch: [77/300][0/84], lr: 0.00000055 	 loss = 0.0554(0.0554)
2023/10/30 22:54:55 - INFO - root -   Epoch: [77/300][20/84], lr: 0.00000055 	 loss = 0.7936(0.3213)
2023/10/30 22:55:45 - INFO - root -   Epoch: [77/300][40/84], lr: 0.00000055 	 loss = 0.0455(0.3283)
2023/10/30 22:57:00 - INFO - root -   Epoch: [77/300][60/84], lr: 0.00000055 	 loss = 0.0608(0.3355)
2023/10/30 22:58:03 - INFO - root -   Epoch: [77/300][80/84], lr: 0.00000055 	 loss = 0.1055(0.3276)
2023/10/30 22:58:04 - INFO - root -   Epoch: [77/300] 	 loss = 0.3173
2023/10/30 22:58:04 - INFO - root -   train_accuracy = 0.8690
2023/10/30 22:58:26 - INFO - root -   Epoch: [78/300][0/84], lr: 0.00000056 	 loss = 0.2506(0.2506)
2023/10/30 22:59:46 - INFO - root -   Epoch: [78/300][20/84], lr: 0.00000056 	 loss = 0.7461(0.2982)
2023/10/30 23:01:00 - INFO - root -   Epoch: [78/300][40/84], lr: 0.00000056 	 loss = 0.0304(0.3220)
2023/10/30 23:01:54 - INFO - root -   Epoch: [78/300][60/84], lr: 0.00000056 	 loss = 0.1452(0.3409)
2023/10/30 23:02:44 - INFO - root -   Epoch: [78/300][80/84], lr: 0.00000056 	 loss = 0.0092(0.3464)
2023/10/30 23:02:46 - INFO - root -   Epoch: [78/300] 	 loss = 0.3406
2023/10/30 23:02:46 - INFO - root -   train_accuracy = 0.8929
2023/10/30 23:03:08 - INFO - root -   Epoch: [79/300][0/84], lr: 0.00000057 	 loss = 0.0073(0.0073)
2023/10/30 23:04:13 - INFO - root -   Epoch: [79/300][20/84], lr: 0.00000057 	 loss = 0.6076(0.2955)
2023/10/30 23:05:26 - INFO - root -   Epoch: [79/300][40/84], lr: 0.00000057 	 loss = 0.0138(0.3223)
2023/10/30 23:06:27 - INFO - root -   Epoch: [79/300][60/84], lr: 0.00000057 	 loss = 0.0384(0.3436)
2023/10/30 23:07:22 - INFO - root -   Epoch: [79/300][80/84], lr: 0.00000057 	 loss = 0.0455(0.3377)
2023/10/30 23:07:23 - INFO - root -   Epoch: [79/300] 	 loss = 0.3287
2023/10/30 23:08:19 - INFO - root -   precision = 0.8837
2023/10/30 23:08:19 - INFO - root -   eval_loss = 0.2778
2023/10/30 23:08:19 - INFO - root -   eval_acc = 0.8837
2023/10/30 23:08:20 - INFO - root -   train_accuracy = 0.8750
2023/10/30 23:08:51 - INFO - root -   Epoch: [80/300][0/84], lr: 0.00000057 	 loss = 0.0954(0.0954)
2023/10/30 23:09:54 - INFO - root -   Epoch: [80/300][20/84], lr: 0.00000057 	 loss = 0.4245(0.2389)
2023/10/30 23:10:54 - INFO - root -   Epoch: [80/300][40/84], lr: 0.00000057 	 loss = 0.0295(0.2569)
2023/10/30 23:12:04 - INFO - root -   Epoch: [80/300][60/84], lr: 0.00000057 	 loss = 0.1204(0.3275)
2023/10/30 23:12:54 - INFO - root -   Epoch: [80/300][80/84], lr: 0.00000057 	 loss = 0.0151(0.3622)
2023/10/30 23:12:55 - INFO - root -   Epoch: [80/300] 	 loss = 0.3562
2023/10/30 23:12:55 - INFO - root -   train_accuracy = 0.8810
2023/10/30 23:13:26 - INFO - root -   Epoch: [81/300][0/84], lr: 0.00000058 	 loss = 0.0599(0.0599)
2023/10/30 23:14:30 - INFO - root -   Epoch: [81/300][20/84], lr: 0.00000058 	 loss = 1.4164(0.3192)
2023/10/30 23:15:32 - INFO - root -   Epoch: [81/300][40/84], lr: 0.00000058 	 loss = 0.0094(0.3610)
2023/10/30 23:16:53 - INFO - root -   Epoch: [81/300][60/84], lr: 0.00000058 	 loss = 0.2305(0.3859)
2023/10/30 23:17:43 - INFO - root -   Epoch: [81/300][80/84], lr: 0.00000058 	 loss = 0.0937(0.3787)
2023/10/30 23:17:44 - INFO - root -   Epoch: [81/300] 	 loss = 0.3720
2023/10/30 23:17:44 - INFO - root -   train_accuracy = 0.8631
2023/10/30 23:18:06 - INFO - root -   Epoch: [82/300][0/84], lr: 0.00000058 	 loss = 0.0337(0.0337)
2023/10/30 23:19:29 - INFO - root -   Epoch: [82/300][20/84], lr: 0.00000058 	 loss = 0.3720(0.2894)
2023/10/30 23:20:21 - INFO - root -   Epoch: [82/300][40/84], lr: 0.00000058 	 loss = 0.0530(0.3094)
2023/10/30 23:21:29 - INFO - root -   Epoch: [82/300][60/84], lr: 0.00000058 	 loss = 0.0942(0.3650)
2023/10/30 23:22:16 - INFO - root -   Epoch: [82/300][80/84], lr: 0.00000058 	 loss = 0.3280(0.3674)
2023/10/30 23:22:17 - INFO - root -   Epoch: [82/300] 	 loss = 0.3571
2023/10/30 23:22:17 - INFO - root -   train_accuracy = 0.8750
2023/10/30 23:22:39 - INFO - root -   Epoch: [83/300][0/84], lr: 0.00000059 	 loss = 0.0128(0.0128)
2023/10/30 23:23:52 - INFO - root -   Epoch: [83/300][20/84], lr: 0.00000059 	 loss = 0.4582(0.2770)
2023/10/30 23:24:59 - INFO - root -   Epoch: [83/300][40/84], lr: 0.00000059 	 loss = 0.0381(0.3113)
2023/10/30 23:25:58 - INFO - root -   Epoch: [83/300][60/84], lr: 0.00000059 	 loss = 0.0721(0.3279)
2023/10/30 23:26:55 - INFO - root -   Epoch: [83/300][80/84], lr: 0.00000059 	 loss = 0.0614(0.3337)
2023/10/30 23:26:57 - INFO - root -   Epoch: [83/300] 	 loss = 0.3262
2023/10/30 23:26:57 - INFO - root -   train_accuracy = 0.8869
2023/10/30 23:27:18 - INFO - root -   Epoch: [84/300][0/84], lr: 0.00000060 	 loss = 0.1021(0.1021)
2023/10/30 23:28:25 - INFO - root -   Epoch: [84/300][20/84], lr: 0.00000060 	 loss = 0.4790(0.2589)
2023/10/30 23:29:41 - INFO - root -   Epoch: [84/300][40/84], lr: 0.00000060 	 loss = 0.0418(0.2729)
2023/10/30 23:30:36 - INFO - root -   Epoch: [84/300][60/84], lr: 0.00000060 	 loss = 0.0608(0.3303)
2023/10/30 23:31:32 - INFO - root -   Epoch: [84/300][80/84], lr: 0.00000060 	 loss = 0.0544(0.3212)
2023/10/30 23:31:33 - INFO - root -   Epoch: [84/300] 	 loss = 0.3113
2023/10/30 23:32:29 - INFO - root -   precision = 0.8837
2023/10/30 23:32:29 - INFO - root -   eval_loss = 0.2907
2023/10/30 23:32:29 - INFO - root -   eval_acc = 0.8837
2023/10/30 23:32:30 - INFO - root -   train_accuracy = 0.8869
2023/10/30 23:32:51 - INFO - root -   Epoch: [85/300][0/84], lr: 0.00000060 	 loss = 0.0385(0.0385)
2023/10/30 23:34:05 - INFO - root -   Epoch: [85/300][20/84], lr: 0.00000060 	 loss = 0.6259(0.2964)
2023/10/30 23:35:14 - INFO - root -   Epoch: [85/300][40/84], lr: 0.00000060 	 loss = 0.0105(0.3097)
2023/10/30 23:36:23 - INFO - root -   Epoch: [85/300][60/84], lr: 0.00000060 	 loss = 0.0391(0.3838)
2023/10/30 23:37:06 - INFO - root -   Epoch: [85/300][80/84], lr: 0.00000060 	 loss = 0.0646(0.3691)
2023/10/30 23:37:12 - INFO - root -   Epoch: [85/300] 	 loss = 0.3648
2023/10/30 23:37:12 - INFO - root -   train_accuracy = 0.8631
2023/10/30 23:37:42 - INFO - root -   Epoch: [86/300][0/84], lr: 0.00000061 	 loss = 0.0354(0.0354)
2023/10/30 23:38:53 - INFO - root -   Epoch: [86/300][20/84], lr: 0.00000061 	 loss = 1.0060(0.3082)
2023/10/30 23:40:03 - INFO - root -   Epoch: [86/300][40/84], lr: 0.00000061 	 loss = 0.0477(0.3276)
2023/10/30 23:41:11 - INFO - root -   Epoch: [86/300][60/84], lr: 0.00000061 	 loss = 0.0565(0.3363)
2023/10/30 23:41:52 - INFO - root -   Epoch: [86/300][80/84], lr: 0.00000061 	 loss = 0.1045(0.3424)
2023/10/30 23:41:57 - INFO - root -   Epoch: [86/300] 	 loss = 0.3404
2023/10/30 23:41:57 - INFO - root -   train_accuracy = 0.8869
2023/10/30 23:42:26 - INFO - root -   Epoch: [87/300][0/84], lr: 0.00000061 	 loss = 0.0509(0.0509)
2023/10/30 23:43:24 - INFO - root -   Epoch: [87/300][20/84], lr: 0.00000061 	 loss = 0.1667(0.2546)
2023/10/30 23:44:48 - INFO - root -   Epoch: [87/300][40/84], lr: 0.00000061 	 loss = 0.0938(0.2652)
2023/10/30 23:45:48 - INFO - root -   Epoch: [87/300][60/84], lr: 0.00000061 	 loss = 0.0913(0.3319)
2023/10/30 23:46:35 - INFO - root -   Epoch: [87/300][80/84], lr: 0.00000061 	 loss = 0.0242(0.3317)
2023/10/30 23:46:36 - INFO - root -   Epoch: [87/300] 	 loss = 0.3238
2023/10/30 23:46:36 - INFO - root -   train_accuracy = 0.8988
2023/10/30 23:47:07 - INFO - root -   Epoch: [88/300][0/84], lr: 0.00000062 	 loss = 0.0978(0.0978)
2023/10/30 23:48:09 - INFO - root -   Epoch: [88/300][20/84], lr: 0.00000062 	 loss = 0.2109(0.2808)
2023/10/30 23:49:40 - INFO - root -   Epoch: [88/300][40/84], lr: 0.00000062 	 loss = 0.0507(0.2976)
2023/10/30 23:50:23 - INFO - root -   Epoch: [88/300][60/84], lr: 0.00000062 	 loss = 0.0186(0.3415)
2023/10/30 23:51:19 - INFO - root -   Epoch: [88/300][80/84], lr: 0.00000062 	 loss = 0.1399(0.3319)
2023/10/30 23:51:21 - INFO - root -   Epoch: [88/300] 	 loss = 0.3355
2023/10/30 23:51:21 - INFO - root -   train_accuracy = 0.8750
2023/10/30 23:51:51 - INFO - root -   Epoch: [89/300][0/84], lr: 0.00000062 	 loss = 0.1179(0.1179)
2023/10/30 23:52:48 - INFO - root -   Epoch: [89/300][20/84], lr: 0.00000062 	 loss = 0.8701(0.3054)
2023/10/30 23:54:05 - INFO - root -   Epoch: [89/300][40/84], lr: 0.00000062 	 loss = 0.1167(0.2785)
2023/10/30 23:55:16 - INFO - root -   Epoch: [89/300][60/84], lr: 0.00000062 	 loss = 0.0512(0.3081)
2023/10/30 23:55:58 - INFO - root -   Epoch: [89/300][80/84], lr: 0.00000062 	 loss = 0.1231(0.3038)
2023/10/30 23:56:03 - INFO - root -   Epoch: [89/300] 	 loss = 0.2988
2023/10/30 23:56:59 - INFO - root -   precision = 0.8837
2023/10/30 23:56:59 - INFO - root -   eval_loss = 0.2690
2023/10/30 23:56:59 - INFO - root -   eval_acc = 0.8837
2023/10/30 23:57:00 - INFO - root -   train_accuracy = 0.8929
2023/10/30 23:57:52 - INFO - root -   Epoch: [90/300][0/84], lr: 0.00000063 	 loss = 0.1049(0.1049)
2023/10/30 23:58:38 - INFO - root -   Epoch: [90/300][20/84], lr: 0.00000063 	 loss = 1.1676(0.3314)
2023/10/30 23:59:48 - INFO - root -   Epoch: [90/300][40/84], lr: 0.00000063 	 loss = 0.1050(0.3182)
2023/10/31 00:00:51 - INFO - root -   Epoch: [90/300][60/84], lr: 0.00000063 	 loss = 0.0598(0.3331)
2023/10/31 00:01:35 - INFO - root -   Epoch: [90/300][80/84], lr: 0.00000063 	 loss = 0.1384(0.3605)
2023/10/31 00:01:39 - INFO - root -   Epoch: [90/300] 	 loss = 0.3526
2023/10/31 00:01:39 - INFO - root -   train_accuracy = 0.8810
2023/10/31 00:02:09 - INFO - root -   Epoch: [91/300][0/84], lr: 0.00000064 	 loss = 0.0482(0.0482)
2023/10/31 00:03:07 - INFO - root -   Epoch: [91/300][20/84], lr: 0.00000064 	 loss = 1.4042(0.3664)
2023/10/31 00:04:24 - INFO - root -   Epoch: [91/300][40/84], lr: 0.00000064 	 loss = 0.1491(0.3493)
2023/10/31 00:05:30 - INFO - root -   Epoch: [91/300][60/84], lr: 0.00000064 	 loss = 0.0134(0.3514)
2023/10/31 00:06:22 - INFO - root -   Epoch: [91/300][80/84], lr: 0.00000064 	 loss = 0.0216(0.3356)
2023/10/31 00:06:25 - INFO - root -   Epoch: [91/300] 	 loss = 0.3302
2023/10/31 00:06:25 - INFO - root -   train_accuracy = 0.8929
2023/10/31 00:06:46 - INFO - root -   Epoch: [92/300][0/84], lr: 0.00000064 	 loss = 0.0111(0.0111)
2023/10/31 00:07:47 - INFO - root -   Epoch: [92/300][20/84], lr: 0.00000064 	 loss = 0.3234(0.2802)
2023/10/31 00:08:55 - INFO - root -   Epoch: [92/300][40/84], lr: 0.00000064 	 loss = 0.0241(0.3027)
2023/10/31 00:10:29 - INFO - root -   Epoch: [92/300][60/84], lr: 0.00000064 	 loss = 0.1899(0.3579)
2023/10/31 00:11:16 - INFO - root -   Epoch: [92/300][80/84], lr: 0.00000064 	 loss = 0.0493(0.3424)
2023/10/31 00:11:17 - INFO - root -   Epoch: [92/300] 	 loss = 0.3370
2023/10/31 00:11:17 - INFO - root -   train_accuracy = 0.8810
2023/10/31 00:11:39 - INFO - root -   Epoch: [93/300][0/84], lr: 0.00000065 	 loss = 0.0307(0.0307)
2023/10/31 00:12:44 - INFO - root -   Epoch: [93/300][20/84], lr: 0.00000065 	 loss = 0.0947(0.2251)
2023/10/31 00:13:54 - INFO - root -   Epoch: [93/300][40/84], lr: 0.00000065 	 loss = 0.0424(0.3001)
2023/10/31 00:14:55 - INFO - root -   Epoch: [93/300][60/84], lr: 0.00000065 	 loss = 0.0487(0.3324)
2023/10/31 00:15:52 - INFO - root -   Epoch: [93/300][80/84], lr: 0.00000065 	 loss = 0.0127(0.3509)
2023/10/31 00:15:54 - INFO - root -   Epoch: [93/300] 	 loss = 0.3425
2023/10/31 00:15:54 - INFO - root -   train_accuracy = 0.8869
2023/10/31 00:16:32 - INFO - root -   Epoch: [94/300][0/84], lr: 0.00000065 	 loss = 0.0974(0.0974)
2023/10/31 00:17:35 - INFO - root -   Epoch: [94/300][20/84], lr: 0.00000065 	 loss = 1.6076(0.3720)
2023/10/31 00:18:45 - INFO - root -   Epoch: [94/300][40/84], lr: 0.00000065 	 loss = 0.0717(0.3775)
2023/10/31 00:19:45 - INFO - root -   Epoch: [94/300][60/84], lr: 0.00000065 	 loss = 0.0730(0.3710)
2023/10/31 00:20:38 - INFO - root -   Epoch: [94/300][80/84], lr: 0.00000065 	 loss = 0.1313(0.3786)
2023/10/31 00:20:40 - INFO - root -   Epoch: [94/300] 	 loss = 0.3695
2023/10/31 00:21:36 - INFO - root -   precision = 0.8837
2023/10/31 00:21:36 - INFO - root -   eval_loss = 0.2703
2023/10/31 00:21:36 - INFO - root -   eval_acc = 0.8837
2023/10/31 00:21:37 - INFO - root -   train_accuracy = 0.8750
2023/10/31 00:22:06 - INFO - root -   Epoch: [95/300][0/84], lr: 0.00000066 	 loss = 0.2166(0.2166)
2023/10/31 00:23:16 - INFO - root -   Epoch: [95/300][20/84], lr: 0.00000066 	 loss = 1.3051(0.3699)
2023/10/31 00:24:12 - INFO - root -   Epoch: [95/300][40/84], lr: 0.00000066 	 loss = 0.0206(0.3637)
2023/10/31 00:25:47 - INFO - root -   Epoch: [95/300][60/84], lr: 0.00000066 	 loss = 0.3981(0.3836)
2023/10/31 00:26:21 - INFO - root -   Epoch: [95/300][80/84], lr: 0.00000066 	 loss = 0.0354(0.3673)
2023/10/31 00:26:22 - INFO - root -   Epoch: [95/300] 	 loss = 0.3583
2023/10/31 00:26:22 - INFO - root -   train_accuracy = 0.8750
2023/10/31 00:26:43 - INFO - root -   Epoch: [96/300][0/84], lr: 0.00000067 	 loss = 0.0119(0.0119)
2023/10/31 00:27:49 - INFO - root -   Epoch: [96/300][20/84], lr: 0.00000067 	 loss = 0.4150(0.2734)
2023/10/31 00:28:45 - INFO - root -   Epoch: [96/300][40/84], lr: 0.00000067 	 loss = 0.1234(0.2828)
2023/10/31 00:29:47 - INFO - root -   Epoch: [96/300][60/84], lr: 0.00000067 	 loss = 0.0674(0.3130)
2023/10/31 00:30:47 - INFO - root -   Epoch: [96/300][80/84], lr: 0.00000067 	 loss = 0.0539(0.2983)
2023/10/31 00:30:48 - INFO - root -   Epoch: [96/300] 	 loss = 0.2914
2023/10/31 00:30:48 - INFO - root -   train_accuracy = 0.8988
2023/10/31 00:31:19 - INFO - root -   Epoch: [97/300][0/84], lr: 0.00000067 	 loss = 0.1016(0.1016)
2023/10/31 00:32:15 - INFO - root -   Epoch: [97/300][20/84], lr: 0.00000067 	 loss = 0.5224(0.2833)
2023/10/31 00:33:19 - INFO - root -   Epoch: [97/300][40/84], lr: 0.00000067 	 loss = 0.1453(0.2818)
2023/10/31 00:34:32 - INFO - root -   Epoch: [97/300][60/84], lr: 0.00000067 	 loss = 0.1000(0.3024)
2023/10/31 00:35:20 - INFO - root -   Epoch: [97/300][80/84], lr: 0.00000067 	 loss = 0.1636(0.3105)
2023/10/31 00:35:25 - INFO - root -   Epoch: [97/300] 	 loss = 0.3018
2023/10/31 00:35:25 - INFO - root -   train_accuracy = 0.8869
2023/10/31 00:36:02 - INFO - root -   Epoch: [98/300][0/84], lr: 0.00000068 	 loss = 0.0318(0.0318)
2023/10/31 00:36:55 - INFO - root -   Epoch: [98/300][20/84], lr: 0.00000068 	 loss = 0.2219(0.2413)
2023/10/31 00:37:54 - INFO - root -   Epoch: [98/300][40/84], lr: 0.00000068 	 loss = 0.0901(0.3148)
2023/10/31 00:39:06 - INFO - root -   Epoch: [98/300][60/84], lr: 0.00000068 	 loss = 0.0131(0.3032)
2023/10/31 00:39:48 - INFO - root -   Epoch: [98/300][80/84], lr: 0.00000068 	 loss = 0.0168(0.3113)
2023/10/31 00:39:53 - INFO - root -   Epoch: [98/300] 	 loss = 0.3013
2023/10/31 00:39:53 - INFO - root -   train_accuracy = 0.9107
2023/10/31 00:40:24 - INFO - root -   Epoch: [99/300][0/84], lr: 0.00000068 	 loss = 0.1573(0.1573)
2023/10/31 00:41:38 - INFO - root -   Epoch: [99/300][20/84], lr: 0.00000068 	 loss = 1.1514(0.2173)
2023/10/31 00:42:34 - INFO - root -   Epoch: [99/300][40/84], lr: 0.00000068 	 loss = 0.0209(0.2390)
2023/10/31 00:43:54 - INFO - root -   Epoch: [99/300][60/84], lr: 0.00000068 	 loss = 0.0601(0.2679)
2023/10/31 00:44:41 - INFO - root -   Epoch: [99/300][80/84], lr: 0.00000068 	 loss = 0.0292(0.2922)
2023/10/31 00:44:42 - INFO - root -   Epoch: [99/300] 	 loss = 0.2865
2023/10/31 00:45:38 - INFO - root -   precision = 0.8605
2023/10/31 00:45:38 - INFO - root -   eval_loss = 0.2814
2023/10/31 00:45:38 - INFO - root -   eval_acc = 0.8605
2023/10/31 00:45:39 - INFO - root -   train_accuracy = 0.8869
2023/10/31 00:46:00 - INFO - root -   Epoch: [100/300][0/84], lr: 0.00000069 	 loss = 0.0224(0.0224)
2023/10/31 00:47:14 - INFO - root -   Epoch: [100/300][20/84], lr: 0.00000069 	 loss = 1.6173(0.3672)
2023/10/31 00:48:24 - INFO - root -   Epoch: [100/300][40/84], lr: 0.00000069 	 loss = 0.0373(0.3801)
2023/10/31 00:49:26 - INFO - root -   Epoch: [100/300][60/84], lr: 0.00000069 	 loss = 0.0715(0.3784)
2023/10/31 00:50:10 - INFO - root -   Epoch: [100/300][80/84], lr: 0.00000069 	 loss = 0.0258(0.3644)
2023/10/31 00:50:13 - INFO - root -   Epoch: [100/300] 	 loss = 0.3561
2023/10/31 00:50:13 - INFO - root -   train_accuracy = 0.8988
2023/10/31 00:50:43 - INFO - root -   Epoch: [101/300][0/84], lr: 0.00000070 	 loss = 0.0744(0.0744)
2023/10/31 00:51:43 - INFO - root -   Epoch: [101/300][20/84], lr: 0.00000070 	 loss = 0.1981(0.2717)
2023/10/31 00:52:43 - INFO - root -   Epoch: [101/300][40/84], lr: 0.00000070 	 loss = 0.0242(0.3048)
2023/10/31 00:54:05 - INFO - root -   Epoch: [101/300][60/84], lr: 0.00000070 	 loss = 0.0566(0.3400)
2023/10/31 00:54:42 - INFO - root -   Epoch: [101/300][80/84], lr: 0.00000070 	 loss = 0.0186(0.3492)
2023/10/31 00:54:48 - INFO - root -   Epoch: [101/300] 	 loss = 0.3401
2023/10/31 00:54:48 - INFO - root -   train_accuracy = 0.8929
2023/10/31 00:55:10 - INFO - root -   Epoch: [102/300][0/84], lr: 0.00000070 	 loss = 0.0324(0.0324)
2023/10/31 00:56:15 - INFO - root -   Epoch: [102/300][20/84], lr: 0.00000070 	 loss = 0.3474(0.2429)
2023/10/31 00:57:16 - INFO - root -   Epoch: [102/300][40/84], lr: 0.00000070 	 loss = 0.1042(0.2898)
2023/10/31 00:58:53 - INFO - root -   Epoch: [102/300][60/84], lr: 0.00000070 	 loss = 0.0513(0.3286)
2023/10/31 00:59:34 - INFO - root -   Epoch: [102/300][80/84], lr: 0.00000070 	 loss = 0.0436(0.3457)
2023/10/31 00:59:35 - INFO - root -   Epoch: [102/300] 	 loss = 0.3411
2023/10/31 00:59:35 - INFO - root -   train_accuracy = 0.8869
2023/10/31 00:59:57 - INFO - root -   Epoch: [103/300][0/84], lr: 0.00000071 	 loss = 0.0146(0.0146)
2023/10/31 01:01:03 - INFO - root -   Epoch: [103/300][20/84], lr: 0.00000071 	 loss = 1.0131(0.2978)
2023/10/31 01:02:10 - INFO - root -   Epoch: [103/300][40/84], lr: 0.00000071 	 loss = 0.1205(0.3062)
2023/10/31 01:03:19 - INFO - root -   Epoch: [103/300][60/84], lr: 0.00000071 	 loss = 0.0492(0.3455)
2023/10/31 01:04:17 - INFO - root -   Epoch: [103/300][80/84], lr: 0.00000071 	 loss = 0.2284(0.3382)
2023/10/31 01:04:18 - INFO - root -   Epoch: [103/300] 	 loss = 0.3347
2023/10/31 01:04:18 - INFO - root -   train_accuracy = 0.8810
2023/10/31 01:04:48 - INFO - root -   Epoch: [104/300][0/84], lr: 0.00000071 	 loss = 0.1314(0.1314)
2023/10/31 01:05:46 - INFO - root -   Epoch: [104/300][20/84], lr: 0.00000071 	 loss = 0.4334(0.3141)
2023/10/31 01:06:56 - INFO - root -   Epoch: [104/300][40/84], lr: 0.00000071 	 loss = 0.0266(0.2884)
2023/10/31 01:08:05 - INFO - root -   Epoch: [104/300][60/84], lr: 0.00000071 	 loss = 0.0172(0.3080)
2023/10/31 01:09:02 - INFO - root -   Epoch: [104/300][80/84], lr: 0.00000071 	 loss = 0.0221(0.3062)
2023/10/31 01:09:03 - INFO - root -   Epoch: [104/300] 	 loss = 0.3066
2023/10/31 01:10:00 - INFO - root -   precision = 0.8837
2023/10/31 01:10:00 - INFO - root -   eval_loss = 0.2696
2023/10/31 01:10:00 - INFO - root -   eval_acc = 0.8837
2023/10/31 01:10:01 - INFO - root -   train_accuracy = 0.8869
2023/10/31 01:10:23 - INFO - root -   Epoch: [105/300][0/84], lr: 0.00000072 	 loss = 0.0266(0.0266)
2023/10/31 01:11:25 - INFO - root -   Epoch: [105/300][20/84], lr: 0.00000072 	 loss = 0.9329(0.2029)
2023/10/31 01:12:38 - INFO - root -   Epoch: [105/300][40/84], lr: 0.00000072 	 loss = 0.0281(0.2633)
2023/10/31 01:13:44 - INFO - root -   Epoch: [105/300][60/84], lr: 0.00000072 	 loss = 0.0804(0.3103)
2023/10/31 01:14:28 - INFO - root -   Epoch: [105/300][80/84], lr: 0.00000072 	 loss = 0.0438(0.3079)
2023/10/31 01:14:33 - INFO - root -   Epoch: [105/300] 	 loss = 0.3002
2023/10/31 01:14:33 - INFO - root -   train_accuracy = 0.8869
2023/10/31 01:15:11 - INFO - root -   Epoch: [106/300][0/84], lr: 0.00000072 	 loss = 0.0221(0.0221)
2023/10/31 01:16:16 - INFO - root -   Epoch: [106/300][20/84], lr: 0.00000072 	 loss = 0.5495(0.2548)
2023/10/31 01:17:41 - INFO - root -   Epoch: [106/300][40/84], lr: 0.00000072 	 loss = 0.1877(0.2805)
2023/10/31 01:18:28 - INFO - root -   Epoch: [106/300][60/84], lr: 0.00000072 	 loss = 0.2554(0.3236)
2023/10/31 01:19:16 - INFO - root -   Epoch: [106/300][80/84], lr: 0.00000072 	 loss = 0.0267(0.3094)
2023/10/31 01:19:18 - INFO - root -   Epoch: [106/300] 	 loss = 0.2998
2023/10/31 01:19:18 - INFO - root -   train_accuracy = 0.8988
2023/10/31 01:19:40 - INFO - root -   Epoch: [107/300][0/84], lr: 0.00000073 	 loss = 0.0162(0.0162)
2023/10/31 01:20:44 - INFO - root -   Epoch: [107/300][20/84], lr: 0.00000073 	 loss = 0.2092(0.2582)
2023/10/31 01:22:01 - INFO - root -   Epoch: [107/300][40/84], lr: 0.00000073 	 loss = 0.4206(0.3411)
2023/10/31 01:23:03 - INFO - root -   Epoch: [107/300][60/84], lr: 0.00000073 	 loss = 0.0953(0.3515)
2023/10/31 01:23:56 - INFO - root -   Epoch: [107/300][80/84], lr: 0.00000073 	 loss = 0.0670(0.3633)
2023/10/31 01:23:57 - INFO - root -   Epoch: [107/300] 	 loss = 0.3576
2023/10/31 01:23:57 - INFO - root -   train_accuracy = 0.8869
2023/10/31 01:24:19 - INFO - root -   Epoch: [108/300][0/84], lr: 0.00000074 	 loss = 0.0176(0.0176)
2023/10/31 01:25:18 - INFO - root -   Epoch: [108/300][20/84], lr: 0.00000074 	 loss = 0.3609(0.2539)
2023/10/31 01:26:28 - INFO - root -   Epoch: [108/300][40/84], lr: 0.00000074 	 loss = 0.0173(0.2294)
2023/10/31 01:27:23 - INFO - root -   Epoch: [108/300][60/84], lr: 0.00000074 	 loss = 0.1207(0.2626)
2023/10/31 01:28:24 - INFO - root -   Epoch: [108/300][80/84], lr: 0.00000074 	 loss = 0.0428(0.2862)
2023/10/31 01:28:27 - INFO - root -   Epoch: [108/300] 	 loss = 0.2793
2023/10/31 01:28:27 - INFO - root -   train_accuracy = 0.8988
2023/10/31 01:28:56 - INFO - root -   Epoch: [109/300][0/84], lr: 0.00000074 	 loss = 0.0415(0.0415)
2023/10/31 01:30:06 - INFO - root -   Epoch: [109/300][20/84], lr: 0.00000074 	 loss = 1.1248(0.2715)
2023/10/31 01:30:53 - INFO - root -   Epoch: [109/300][40/84], lr: 0.00000074 	 loss = 0.0471(0.2811)
2023/10/31 01:32:02 - INFO - root -   Epoch: [109/300][60/84], lr: 0.00000074 	 loss = 0.0724(0.2791)
2023/10/31 01:32:49 - INFO - root -   Epoch: [109/300][80/84], lr: 0.00000074 	 loss = 0.0073(0.3005)
2023/10/31 01:32:55 - INFO - root -   Epoch: [109/300] 	 loss = 0.2934
2023/10/31 01:33:52 - INFO - root -   precision = 0.8837
2023/10/31 01:33:52 - INFO - root -   eval_loss = 0.3380
2023/10/31 01:33:52 - INFO - root -   eval_acc = 0.8837
2023/10/31 01:33:53 - INFO - root -   train_accuracy = 0.8929
2023/10/31 01:34:22 - INFO - root -   Epoch: [110/300][0/84], lr: 0.00000075 	 loss = 0.0151(0.0151)
2023/10/31 01:35:20 - INFO - root -   Epoch: [110/300][20/84], lr: 0.00000075 	 loss = 0.4363(0.2418)
2023/10/31 01:36:44 - INFO - root -   Epoch: [110/300][40/84], lr: 0.00000075 	 loss = 0.0536(0.2453)
2023/10/31 01:37:40 - INFO - root -   Epoch: [110/300][60/84], lr: 0.00000075 	 loss = 0.5196(0.2821)
2023/10/31 01:38:36 - INFO - root -   Epoch: [110/300][80/84], lr: 0.00000075 	 loss = 0.3246(0.3019)
2023/10/31 01:38:38 - INFO - root -   Epoch: [110/300] 	 loss = 0.2970
2023/10/31 01:38:38 - INFO - root -   train_accuracy = 0.8869
2023/10/31 01:39:14 - INFO - root -   Epoch: [111/300][0/84], lr: 0.00000075 	 loss = 0.1239(0.1239)
2023/10/31 01:40:12 - INFO - root -   Epoch: [111/300][20/84], lr: 0.00000075 	 loss = 0.9900(0.2764)
2023/10/31 01:41:29 - INFO - root -   Epoch: [111/300][40/84], lr: 0.00000075 	 loss = 0.3330(0.3053)
2023/10/31 01:42:24 - INFO - root -   Epoch: [111/300][60/84], lr: 0.00000075 	 loss = 0.0454(0.3957)
2023/10/31 01:43:08 - INFO - root -   Epoch: [111/300][80/84], lr: 0.00000075 	 loss = 0.0423(0.3772)
2023/10/31 01:43:09 - INFO - root -   Epoch: [111/300] 	 loss = 0.3842
2023/10/31 01:43:09 - INFO - root -   train_accuracy = 0.8690
2023/10/31 01:43:31 - INFO - root -   Epoch: [112/300][0/84], lr: 0.00000076 	 loss = 0.0319(0.0319)
2023/10/31 01:44:29 - INFO - root -   Epoch: [112/300][20/84], lr: 0.00000076 	 loss = 0.2846(0.3101)
2023/10/31 01:45:36 - INFO - root -   Epoch: [112/300][40/84], lr: 0.00000076 	 loss = 0.0457(0.3176)
2023/10/31 01:46:54 - INFO - root -   Epoch: [112/300][60/84], lr: 0.00000076 	 loss = 0.0323(0.3686)
2023/10/31 01:47:50 - INFO - root -   Epoch: [112/300][80/84], lr: 0.00000076 	 loss = 0.0808(0.3383)
2023/10/31 01:47:53 - INFO - root -   Epoch: [112/300] 	 loss = 0.3374
2023/10/31 01:47:53 - INFO - root -   train_accuracy = 0.8690
2023/10/31 01:48:23 - INFO - root -   Epoch: [113/300][0/84], lr: 0.00000077 	 loss = 0.0227(0.0227)
2023/10/31 01:49:28 - INFO - root -   Epoch: [113/300][20/84], lr: 0.00000077 	 loss = 0.3511(0.2641)
2023/10/31 01:50:21 - INFO - root -   Epoch: [113/300][40/84], lr: 0.00000077 	 loss = 0.0463(0.2741)
2023/10/31 01:51:48 - INFO - root -   Epoch: [113/300][60/84], lr: 0.00000077 	 loss = 0.2041(0.3064)
2023/10/31 01:52:32 - INFO - root -   Epoch: [113/300][80/84], lr: 0.00000077 	 loss = 0.0593(0.2849)
2023/10/31 01:52:34 - INFO - root -   Epoch: [113/300] 	 loss = 0.2798
2023/10/31 01:52:34 - INFO - root -   train_accuracy = 0.9048
2023/10/31 01:53:03 - INFO - root -   Epoch: [114/300][0/84], lr: 0.00000077 	 loss = 0.0379(0.0379)
2023/10/31 01:53:55 - INFO - root -   Epoch: [114/300][20/84], lr: 0.00000077 	 loss = 0.3017(0.2361)
2023/10/31 01:54:55 - INFO - root -   Epoch: [114/300][40/84], lr: 0.00000077 	 loss = 0.0210(0.2200)
2023/10/31 01:56:07 - INFO - root -   Epoch: [114/300][60/84], lr: 0.00000077 	 loss = 0.1472(0.2297)
2023/10/31 01:57:01 - INFO - root -   Epoch: [114/300][80/84], lr: 0.00000077 	 loss = 0.0666(0.2258)
2023/10/31 01:57:03 - INFO - root -   Epoch: [114/300] 	 loss = 0.2295
2023/10/31 01:57:59 - INFO - root -   precision = 0.9070
2023/10/31 01:57:59 - INFO - root -   eval_loss = 0.2290
2023/10/31 01:57:59 - INFO - root -   eval_acc = 0.9070
2023/10/31 01:58:00 - INFO - root -   train_accuracy = 0.9048
2023/10/31 01:58:22 - INFO - root -   Epoch: [115/300][0/84], lr: 0.00000078 	 loss = 0.0817(0.0817)
2023/10/31 01:59:42 - INFO - root -   Epoch: [115/300][20/84], lr: 0.00000078 	 loss = 1.6459(0.3437)
2023/10/31 02:00:54 - INFO - root -   Epoch: [115/300][40/84], lr: 0.00000078 	 loss = 0.0621(0.2990)
2023/10/31 02:02:15 - INFO - root -   Epoch: [115/300][60/84], lr: 0.00000078 	 loss = 0.2064(0.3677)
2023/10/31 02:02:51 - INFO - root -   Epoch: [115/300][80/84], lr: 0.00000078 	 loss = 0.1296(0.3638)
2023/10/31 02:02:53 - INFO - root -   Epoch: [115/300] 	 loss = 0.3563
2023/10/31 02:02:53 - INFO - root -   train_accuracy = 0.8810
2023/10/31 02:03:14 - INFO - root -   Epoch: [116/300][0/84], lr: 0.00000078 	 loss = 0.0132(0.0132)
2023/10/31 02:04:19 - INFO - root -   Epoch: [116/300][20/84], lr: 0.00000078 	 loss = 0.5764(0.2709)
2023/10/31 02:05:31 - INFO - root -   Epoch: [116/300][40/84], lr: 0.00000078 	 loss = 0.0593(0.3482)
2023/10/31 02:06:39 - INFO - root -   Epoch: [116/300][60/84], lr: 0.00000078 	 loss = 0.0751(0.3394)
2023/10/31 02:07:30 - INFO - root -   Epoch: [116/300][80/84], lr: 0.00000078 	 loss = 0.0054(0.3263)
2023/10/31 02:07:32 - INFO - root -   Epoch: [116/300] 	 loss = 0.3171
2023/10/31 02:07:32 - INFO - root -   train_accuracy = 0.8929
2023/10/31 02:08:01 - INFO - root -   Epoch: [117/300][0/84], lr: 0.00000079 	 loss = 0.1960(0.1960)
2023/10/31 02:09:08 - INFO - root -   Epoch: [117/300][20/84], lr: 0.00000079 	 loss = 1.3520(0.3794)
2023/10/31 02:09:59 - INFO - root -   Epoch: [117/300][40/84], lr: 0.00000079 	 loss = 0.0115(0.4086)
2023/10/31 02:11:13 - INFO - root -   Epoch: [117/300][60/84], lr: 0.00000079 	 loss = 0.1783(0.3752)
2023/10/31 02:11:54 - INFO - root -   Epoch: [117/300][80/84], lr: 0.00000079 	 loss = 0.0186(0.3346)
2023/10/31 02:11:58 - INFO - root -   Epoch: [117/300] 	 loss = 0.3337
2023/10/31 02:11:58 - INFO - root -   train_accuracy = 0.8810
2023/10/31 02:12:28 - INFO - root -   Epoch: [118/300][0/84], lr: 0.00000080 	 loss = 0.0215(0.0215)
2023/10/31 02:13:25 - INFO - root -   Epoch: [118/300][20/84], lr: 0.00000080 	 loss = 0.3999(0.2220)
2023/10/31 02:14:28 - INFO - root -   Epoch: [118/300][40/84], lr: 0.00000080 	 loss = 0.0233(0.2673)
2023/10/31 02:15:49 - INFO - root -   Epoch: [118/300][60/84], lr: 0.00000080 	 loss = 0.1674(0.3002)
2023/10/31 02:16:34 - INFO - root -   Epoch: [118/300][80/84], lr: 0.00000080 	 loss = 0.0127(0.3136)
2023/10/31 02:16:37 - INFO - root -   Epoch: [118/300] 	 loss = 0.3057
2023/10/31 02:16:37 - INFO - root -   train_accuracy = 0.8869
2023/10/31 02:17:07 - INFO - root -   Epoch: [119/300][0/84], lr: 0.00000080 	 loss = 0.0169(0.0169)
2023/10/31 02:18:04 - INFO - root -   Epoch: [119/300][20/84], lr: 0.00000080 	 loss = 0.1224(0.2448)
2023/10/31 02:19:05 - INFO - root -   Epoch: [119/300][40/84], lr: 0.00000080 	 loss = 0.0206(0.2453)
2023/10/31 02:20:26 - INFO - root -   Epoch: [119/300][60/84], lr: 0.00000080 	 loss = 0.1209(0.2674)
2023/10/31 02:21:02 - INFO - root -   Epoch: [119/300][80/84], lr: 0.00000080 	 loss = 0.0828(0.2778)
2023/10/31 02:21:08 - INFO - root -   Epoch: [119/300] 	 loss = 0.2727
2023/10/31 02:22:04 - INFO - root -   precision = 0.8837
2023/10/31 02:22:04 - INFO - root -   eval_loss = 0.3435
2023/10/31 02:22:04 - INFO - root -   eval_acc = 0.8837
2023/10/31 02:22:05 - INFO - root -   train_accuracy = 0.8988
2023/10/31 02:22:43 - INFO - root -   Epoch: [120/300][0/84], lr: 0.00000081 	 loss = 0.6341(0.6341)
2023/10/31 02:23:57 - INFO - root -   Epoch: [120/300][20/84], lr: 0.00000081 	 loss = 0.9763(0.2760)
2023/10/31 02:24:47 - INFO - root -   Epoch: [120/300][40/84], lr: 0.00000081 	 loss = 0.0403(0.2969)
2023/10/31 02:26:11 - INFO - root -   Epoch: [120/300][60/84], lr: 0.00000081 	 loss = 0.2721(0.3327)
2023/10/31 02:26:52 - INFO - root -   Epoch: [120/300][80/84], lr: 0.00000081 	 loss = 0.0239(0.3121)
2023/10/31 02:26:56 - INFO - root -   Epoch: [120/300] 	 loss = 0.3133
2023/10/31 02:26:56 - INFO - root -   train_accuracy = 0.8869
2023/10/31 02:27:18 - INFO - root -   Epoch: [121/300][0/84], lr: 0.00000081 	 loss = 0.0577(0.0577)
2023/10/31 02:28:34 - INFO - root -   Epoch: [121/300][20/84], lr: 0.00000081 	 loss = 0.1300(0.2787)
2023/10/31 02:29:40 - INFO - root -   Epoch: [121/300][40/84], lr: 0.00000081 	 loss = 0.0187(0.2983)
2023/10/31 02:30:34 - INFO - root -   Epoch: [121/300][60/84], lr: 0.00000081 	 loss = 0.2236(0.3473)
2023/10/31 02:31:31 - INFO - root -   Epoch: [121/300][80/84], lr: 0.00000081 	 loss = 0.0361(0.3707)
2023/10/31 02:31:32 - INFO - root -   Epoch: [121/300] 	 loss = 0.3682
2023/10/31 02:31:32 - INFO - root -   train_accuracy = 0.8869
2023/10/31 02:32:01 - INFO - root -   Epoch: [122/300][0/84], lr: 0.00000082 	 loss = 0.0256(0.0256)
2023/10/31 02:33:08 - INFO - root -   Epoch: [122/300][20/84], lr: 0.00000082 	 loss = 0.6893(0.3764)
2023/10/31 02:34:08 - INFO - root -   Epoch: [122/300][40/84], lr: 0.00000082 	 loss = 0.0665(0.3707)
2023/10/31 02:35:31 - INFO - root -   Epoch: [122/300][60/84], lr: 0.00000082 	 loss = 0.0788(0.3893)
2023/10/31 02:36:20 - INFO - root -   Epoch: [122/300][80/84], lr: 0.00000082 	 loss = 0.0849(0.3812)
2023/10/31 02:36:21 - INFO - root -   Epoch: [122/300] 	 loss = 0.3712
2023/10/31 02:36:21 - INFO - root -   train_accuracy = 0.8810
2023/10/31 02:36:51 - INFO - root -   Epoch: [123/300][0/84], lr: 0.00000082 	 loss = 0.0541(0.0541)
2023/10/31 02:37:56 - INFO - root -   Epoch: [123/300][20/84], lr: 0.00000082 	 loss = 0.2659(0.2119)
2023/10/31 02:38:58 - INFO - root -   Epoch: [123/300][40/84], lr: 0.00000082 	 loss = 0.0230(0.2566)
2023/10/31 02:39:56 - INFO - root -   Epoch: [123/300][60/84], lr: 0.00000082 	 loss = 0.1396(0.2863)
2023/10/31 02:40:45 - INFO - root -   Epoch: [123/300][80/84], lr: 0.00000082 	 loss = 0.0082(0.2804)
2023/10/31 02:40:47 - INFO - root -   Epoch: [123/300] 	 loss = 0.2764
2023/10/31 02:40:47 - INFO - root -   train_accuracy = 0.8988
2023/10/31 02:41:16 - INFO - root -   Epoch: [124/300][0/84], lr: 0.00000083 	 loss = 0.0621(0.0621)
2023/10/31 02:42:09 - INFO - root -   Epoch: [124/300][20/84], lr: 0.00000083 	 loss = 0.5153(0.2514)
2023/10/31 02:43:11 - INFO - root -   Epoch: [124/300][40/84], lr: 0.00000083 	 loss = 0.0822(0.2824)
2023/10/31 02:44:36 - INFO - root -   Epoch: [124/300][60/84], lr: 0.00000083 	 loss = 0.0400(0.2944)
2023/10/31 02:45:22 - INFO - root -   Epoch: [124/300][80/84], lr: 0.00000083 	 loss = 0.4136(0.3025)
2023/10/31 02:45:30 - INFO - root -   Epoch: [124/300] 	 loss = 0.2942
2023/10/31 02:46:27 - INFO - root -   precision = 0.8837
2023/10/31 02:46:27 - INFO - root -   eval_loss = 0.2644
2023/10/31 02:46:27 - INFO - root -   eval_acc = 0.8837
2023/10/31 02:46:28 - INFO - root -   train_accuracy = 0.8869
2023/10/31 02:46:49 - INFO - root -   Epoch: [125/300][0/84], lr: 0.00000084 	 loss = 0.0068(0.0068)
2023/10/31 02:48:09 - INFO - root -   Epoch: [125/300][20/84], lr: 0.00000084 	 loss = 1.8235(0.3252)
2023/10/31 02:48:54 - INFO - root -   Epoch: [125/300][40/84], lr: 0.00000084 	 loss = 0.0125(0.2987)
2023/10/31 02:49:52 - INFO - root -   Epoch: [125/300][60/84], lr: 0.00000084 	 loss = 0.5512(0.3117)
2023/10/31 02:51:03 - INFO - root -   Epoch: [125/300][80/84], lr: 0.00000084 	 loss = 0.0254(0.3129)
2023/10/31 02:51:04 - INFO - root -   Epoch: [125/300] 	 loss = 0.3053
2023/10/31 02:51:04 - INFO - root -   train_accuracy = 0.8929
2023/10/31 02:51:42 - INFO - root -   Epoch: [126/300][0/84], lr: 0.00000084 	 loss = 0.0258(0.0258)
2023/10/31 02:52:41 - INFO - root -   Epoch: [126/300][20/84], lr: 0.00000084 	 loss = 0.1376(0.1816)
2023/10/31 02:53:45 - INFO - root -   Epoch: [126/300][40/84], lr: 0.00000084 	 loss = 0.1004(0.2796)
2023/10/31 02:55:00 - INFO - root -   Epoch: [126/300][60/84], lr: 0.00000084 	 loss = 0.1003(0.2931)
2023/10/31 02:55:38 - INFO - root -   Epoch: [126/300][80/84], lr: 0.00000084 	 loss = 0.0374(0.3361)
2023/10/31 02:55:39 - INFO - root -   Epoch: [126/300] 	 loss = 0.3285
2023/10/31 02:55:39 - INFO - root -   train_accuracy = 0.8869
2023/10/31 02:56:10 - INFO - root -   Epoch: [127/300][0/84], lr: 0.00000085 	 loss = 0.1086(0.1086)
2023/10/31 02:57:08 - INFO - root -   Epoch: [127/300][20/84], lr: 0.00000085 	 loss = 0.1770(0.1854)
2023/10/31 02:58:16 - INFO - root -   Epoch: [127/300][40/84], lr: 0.00000085 	 loss = 0.0382(0.2442)
2023/10/31 02:59:32 - INFO - root -   Epoch: [127/300][60/84], lr: 0.00000085 	 loss = 0.0380(0.2975)
2023/10/31 03:00:15 - INFO - root -   Epoch: [127/300][80/84], lr: 0.00000085 	 loss = 0.0777(0.2884)
2023/10/31 03:00:16 - INFO - root -   Epoch: [127/300] 	 loss = 0.2798
2023/10/31 03:00:16 - INFO - root -   train_accuracy = 0.9107
2023/10/31 03:00:38 - INFO - root -   Epoch: [128/300][0/84], lr: 0.00000085 	 loss = 0.0236(0.0236)
2023/10/31 03:01:44 - INFO - root -   Epoch: [128/300][20/84], lr: 0.00000085 	 loss = 0.5500(0.2084)
2023/10/31 03:02:48 - INFO - root -   Epoch: [128/300][40/84], lr: 0.00000085 	 loss = 0.0992(0.2269)
2023/10/31 03:03:54 - INFO - root -   Epoch: [128/300][60/84], lr: 0.00000085 	 loss = 0.0220(0.2583)
2023/10/31 03:04:40 - INFO - root -   Epoch: [128/300][80/84], lr: 0.00000085 	 loss = 0.0682(0.2707)
2023/10/31 03:04:42 - INFO - root -   Epoch: [128/300] 	 loss = 0.2657
2023/10/31 03:04:42 - INFO - root -   train_accuracy = 0.8988
2023/10/31 03:05:03 - INFO - root -   Epoch: [129/300][0/84], lr: 0.00000086 	 loss = 0.0540(0.0540)
2023/10/31 03:06:08 - INFO - root -   Epoch: [129/300][20/84], lr: 0.00000086 	 loss = 0.9177(0.2680)
2023/10/31 03:07:09 - INFO - root -   Epoch: [129/300][40/84], lr: 0.00000086 	 loss = 0.0840(0.3123)
2023/10/31 03:08:27 - INFO - root -   Epoch: [129/300][60/84], lr: 0.00000086 	 loss = 0.1778(0.3211)
2023/10/31 03:09:16 - INFO - root -   Epoch: [129/300][80/84], lr: 0.00000086 	 loss = 0.0227(0.3188)
2023/10/31 03:09:18 - INFO - root -   Epoch: [129/300] 	 loss = 0.3096
2023/10/31 03:10:14 - INFO - root -   precision = 0.9302
2023/10/31 03:10:14 - INFO - root -   eval_loss = 0.2139
2023/10/31 03:10:14 - INFO - root -   eval_acc = 0.9302
2023/10/31 03:10:15 - INFO - root -   train_accuracy = 0.8810
2023/10/31 03:10:45 - INFO - root -   Epoch: [130/300][0/84], lr: 0.00000087 	 loss = 0.0356(0.0356)
2023/10/31 03:11:49 - INFO - root -   Epoch: [130/300][20/84], lr: 0.00000087 	 loss = 0.5736(0.2982)
2023/10/31 03:13:11 - INFO - root -   Epoch: [130/300][40/84], lr: 0.00000087 	 loss = 0.0883(0.2894)
2023/10/31 03:14:14 - INFO - root -   Epoch: [130/300][60/84], lr: 0.00000087 	 loss = 0.1679(0.3068)
2023/10/31 03:15:02 - INFO - root -   Epoch: [130/300][80/84], lr: 0.00000087 	 loss = 0.0668(0.3176)
2023/10/31 03:15:03 - INFO - root -   Epoch: [130/300] 	 loss = 0.3092
2023/10/31 03:15:03 - INFO - root -   train_accuracy = 0.8810
2023/10/31 03:15:24 - INFO - root -   Epoch: [131/300][0/84], lr: 0.00000087 	 loss = 0.0271(0.0271)
2023/10/31 03:16:22 - INFO - root -   Epoch: [131/300][20/84], lr: 0.00000087 	 loss = 0.3107(0.3389)
2023/10/31 03:17:42 - INFO - root -   Epoch: [131/300][40/84], lr: 0.00000087 	 loss = 0.0536(0.3420)
2023/10/31 03:18:37 - INFO - root -   Epoch: [131/300][60/84], lr: 0.00000087 	 loss = 0.0383(0.3487)
2023/10/31 03:19:40 - INFO - root -   Epoch: [131/300][80/84], lr: 0.00000087 	 loss = 0.0203(0.3575)
2023/10/31 03:19:41 - INFO - root -   Epoch: [131/300] 	 loss = 0.3529
2023/10/31 03:19:41 - INFO - root -   train_accuracy = 0.8690
2023/10/31 03:20:03 - INFO - root -   Epoch: [132/300][0/84], lr: 0.00000088 	 loss = 0.0396(0.0396)
2023/10/31 03:21:08 - INFO - root -   Epoch: [132/300][20/84], lr: 0.00000088 	 loss = 0.3313(0.2803)
2023/10/31 03:22:19 - INFO - root -   Epoch: [132/300][40/84], lr: 0.00000088 	 loss = 0.0293(0.2754)
2023/10/31 03:23:32 - INFO - root -   Epoch: [132/300][60/84], lr: 0.00000088 	 loss = 0.0306(0.2728)
2023/10/31 03:24:14 - INFO - root -   Epoch: [132/300][80/84], lr: 0.00000088 	 loss = 0.0459(0.2752)
2023/10/31 03:24:15 - INFO - root -   Epoch: [132/300] 	 loss = 0.2672
2023/10/31 03:24:15 - INFO - root -   train_accuracy = 0.9167
2023/10/31 03:24:53 - INFO - root -   Epoch: [133/300][0/84], lr: 0.00000088 	 loss = 0.1138(0.1138)
2023/10/31 03:25:58 - INFO - root -   Epoch: [133/300][20/84], lr: 0.00000088 	 loss = 0.9467(0.3443)
2023/10/31 03:27:02 - INFO - root -   Epoch: [133/300][40/84], lr: 0.00000088 	 loss = 0.1680(0.3074)
2023/10/31 03:28:17 - INFO - root -   Epoch: [133/300][60/84], lr: 0.00000088 	 loss = 0.1916(0.3313)
2023/10/31 03:29:02 - INFO - root -   Epoch: [133/300][80/84], lr: 0.00000088 	 loss = 0.3237(0.3217)
2023/10/31 03:29:03 - INFO - root -   Epoch: [133/300] 	 loss = 0.3135
2023/10/31 03:29:03 - INFO - root -   train_accuracy = 0.8750
2023/10/31 03:29:33 - INFO - root -   Epoch: [134/300][0/84], lr: 0.00000089 	 loss = 0.0860(0.0860)
2023/10/31 03:30:31 - INFO - root -   Epoch: [134/300][20/84], lr: 0.00000089 	 loss = 0.3305(0.2536)
2023/10/31 03:31:54 - INFO - root -   Epoch: [134/300][40/84], lr: 0.00000089 	 loss = 0.0902(0.3021)
2023/10/31 03:32:44 - INFO - root -   Epoch: [134/300][60/84], lr: 0.00000089 	 loss = 0.0599(0.2940)
2023/10/31 03:33:44 - INFO - root -   Epoch: [134/300][80/84], lr: 0.00000089 	 loss = 0.0667(0.2909)
2023/10/31 03:33:45 - INFO - root -   Epoch: [134/300] 	 loss = 0.2853
2023/10/31 03:34:41 - INFO - root -   precision = 0.9535
2023/10/31 03:34:41 - INFO - root -   eval_loss = 0.2182
2023/10/31 03:34:41 - INFO - root -   eval_acc = 0.9535
2023/10/31 03:34:42 - INFO - root -   train_accuracy = 0.9048
2023/10/31 03:35:12 - INFO - root -   Epoch: [135/300][0/84], lr: 0.00000090 	 loss = 0.0162(0.0162)
2023/10/31 03:36:12 - INFO - root -   Epoch: [135/300][20/84], lr: 0.00000090 	 loss = 1.6628(0.2642)
2023/10/31 03:37:34 - INFO - root -   Epoch: [135/300][40/84], lr: 0.00000090 	 loss = 0.3853(0.2675)
2023/10/31 03:38:33 - INFO - root -   Epoch: [135/300][60/84], lr: 0.00000090 	 loss = 0.0785(0.3056)
2023/10/31 03:39:21 - INFO - root -   Epoch: [135/300][80/84], lr: 0.00000090 	 loss = 0.0119(0.3072)
2023/10/31 03:39:24 - INFO - root -   Epoch: [135/300] 	 loss = 0.3018
2023/10/31 03:39:24 - INFO - root -   train_accuracy = 0.9167
2023/10/31 03:39:46 - INFO - root -   Epoch: [136/300][0/84], lr: 0.00000090 	 loss = 0.0188(0.0188)
2023/10/31 03:40:46 - INFO - root -   Epoch: [136/300][20/84], lr: 0.00000090 	 loss = 0.2911(0.2581)
2023/10/31 03:42:01 - INFO - root -   Epoch: [136/300][40/84], lr: 0.00000090 	 loss = 0.0697(0.2546)
2023/10/31 03:43:09 - INFO - root -   Epoch: [136/300][60/84], lr: 0.00000090 	 loss = 0.0241(0.2925)
2023/10/31 03:43:56 - INFO - root -   Epoch: [136/300][80/84], lr: 0.00000090 	 loss = 0.0570(0.2905)
2023/10/31 03:44:01 - INFO - root -   Epoch: [136/300] 	 loss = 0.2853
2023/10/31 03:44:01 - INFO - root -   train_accuracy = 0.9048
2023/10/31 03:44:23 - INFO - root -   Epoch: [137/300][0/84], lr: 0.00000091 	 loss = 0.0122(0.0122)
2023/10/31 03:45:46 - INFO - root -   Epoch: [137/300][20/84], lr: 0.00000091 	 loss = 0.2550(0.2779)
2023/10/31 03:46:36 - INFO - root -   Epoch: [137/300][40/84], lr: 0.00000091 	 loss = 0.0100(0.3043)
2023/10/31 03:48:00 - INFO - root -   Epoch: [137/300][60/84], lr: 0.00000091 	 loss = 0.0277(0.3130)
2023/10/31 03:48:38 - INFO - root -   Epoch: [137/300][80/84], lr: 0.00000091 	 loss = 0.0121(0.3105)
2023/10/31 03:48:39 - INFO - root -   Epoch: [137/300] 	 loss = 0.3013
2023/10/31 03:48:39 - INFO - root -   train_accuracy = 0.8929
2023/10/31 03:49:10 - INFO - root -   Epoch: [138/300][0/84], lr: 0.00000091 	 loss = 0.1734(0.1734)
2023/10/31 03:50:15 - INFO - root -   Epoch: [138/300][20/84], lr: 0.00000091 	 loss = 0.7751(0.4404)
2023/10/31 03:51:38 - INFO - root -   Epoch: [138/300][40/84], lr: 0.00000091 	 loss = 0.1249(0.3918)
2023/10/31 03:52:40 - INFO - root -   Epoch: [138/300][60/84], lr: 0.00000091 	 loss = 0.0685(0.3654)
2023/10/31 03:53:28 - INFO - root -   Epoch: [138/300][80/84], lr: 0.00000091 	 loss = 0.0305(0.3562)
2023/10/31 03:53:30 - INFO - root -   Epoch: [138/300] 	 loss = 0.3479
2023/10/31 03:53:30 - INFO - root -   train_accuracy = 0.8631
2023/10/31 03:53:51 - INFO - root -   Epoch: [139/300][0/84], lr: 0.00000092 	 loss = 0.0505(0.0505)
2023/10/31 03:55:04 - INFO - root -   Epoch: [139/300][20/84], lr: 0.00000092 	 loss = 0.5964(0.3061)
2023/10/31 03:56:24 - INFO - root -   Epoch: [139/300][40/84], lr: 0.00000092 	 loss = 0.1398(0.2731)
2023/10/31 03:57:13 - INFO - root -   Epoch: [139/300][60/84], lr: 0.00000092 	 loss = 0.0339(0.2276)
2023/10/31 03:58:09 - INFO - root -   Epoch: [139/300][80/84], lr: 0.00000092 	 loss = 0.0533(0.2679)
2023/10/31 03:58:11 - INFO - root -   Epoch: [139/300] 	 loss = 0.2597
2023/10/31 03:59:07 - INFO - root -   precision = 0.9302
2023/10/31 03:59:07 - INFO - root -   eval_loss = 0.2069
2023/10/31 03:59:07 - INFO - root -   eval_acc = 0.9302
2023/10/31 03:59:08 - INFO - root -   train_accuracy = 0.9048
2023/10/31 03:59:30 - INFO - root -   Epoch: [140/300][0/84], lr: 0.00000093 	 loss = 0.0299(0.0299)
2023/10/31 04:00:34 - INFO - root -   Epoch: [140/300][20/84], lr: 0.00000093 	 loss = 0.1953(0.1807)
2023/10/31 04:01:47 - INFO - root -   Epoch: [140/300][40/84], lr: 0.00000093 	 loss = 0.0121(0.2305)
2023/10/31 04:02:50 - INFO - root -   Epoch: [140/300][60/84], lr: 0.00000093 	 loss = 0.0128(0.2674)
2023/10/31 04:03:48 - INFO - root -   Epoch: [140/300][80/84], lr: 0.00000093 	 loss = 0.0065(0.2668)
2023/10/31 04:03:50 - INFO - root -   Epoch: [140/300] 	 loss = 0.2633
2023/10/31 04:03:50 - INFO - root -   train_accuracy = 0.9048
2023/10/31 04:04:12 - INFO - root -   Epoch: [141/300][0/84], lr: 0.00000093 	 loss = 0.0396(0.0396)
2023/10/31 04:05:25 - INFO - root -   Epoch: [141/300][20/84], lr: 0.00000093 	 loss = 0.1368(0.2117)
2023/10/31 04:06:25 - INFO - root -   Epoch: [141/300][40/84], lr: 0.00000093 	 loss = 0.0124(0.2346)
2023/10/31 04:07:33 - INFO - root -   Epoch: [141/300][60/84], lr: 0.00000093 	 loss = 0.0448(0.2585)
2023/10/31 04:08:24 - INFO - root -   Epoch: [141/300][80/84], lr: 0.00000093 	 loss = 0.0227(0.2374)
2023/10/31 04:08:28 - INFO - root -   Epoch: [141/300] 	 loss = 0.2368
2023/10/31 04:08:28 - INFO - root -   train_accuracy = 0.9226
2023/10/31 04:08:50 - INFO - root -   Epoch: [142/300][0/84], lr: 0.00000094 	 loss = 0.0049(0.0049)
2023/10/31 04:09:59 - INFO - root -   Epoch: [142/300][20/84], lr: 0.00000094 	 loss = 0.4251(0.2273)
2023/10/31 04:11:10 - INFO - root -   Epoch: [142/300][40/84], lr: 0.00000094 	 loss = 0.0315(0.2319)
2023/10/31 04:12:23 - INFO - root -   Epoch: [142/300][60/84], lr: 0.00000094 	 loss = 0.1256(0.3225)
2023/10/31 04:13:05 - INFO - root -   Epoch: [142/300][80/84], lr: 0.00000094 	 loss = 0.0078(0.3331)
2023/10/31 04:13:07 - INFO - root -   Epoch: [142/300] 	 loss = 0.3274
2023/10/31 04:13:07 - INFO - root -   train_accuracy = 0.8869
2023/10/31 04:13:28 - INFO - root -   Epoch: [143/300][0/84], lr: 0.00000094 	 loss = 0.0087(0.0087)
2023/10/31 04:14:35 - INFO - root -   Epoch: [143/300][20/84], lr: 0.00000094 	 loss = 0.7879(0.3204)
2023/10/31 04:15:51 - INFO - root -   Epoch: [143/300][40/84], lr: 0.00000094 	 loss = 0.0942(0.3001)
2023/10/31 04:16:59 - INFO - root -   Epoch: [143/300][60/84], lr: 0.00000094 	 loss = 0.2123(0.3329)
2023/10/31 04:17:42 - INFO - root -   Epoch: [143/300][80/84], lr: 0.00000094 	 loss = 0.0573(0.3461)
2023/10/31 04:17:47 - INFO - root -   Epoch: [143/300] 	 loss = 0.3370
2023/10/31 04:17:47 - INFO - root -   train_accuracy = 0.8750
2023/10/31 04:18:17 - INFO - root -   Epoch: [144/300][0/84], lr: 0.00000095 	 loss = 0.0467(0.0467)
2023/10/31 04:19:21 - INFO - root -   Epoch: [144/300][20/84], lr: 0.00000095 	 loss = 0.1145(0.2386)
2023/10/31 04:20:34 - INFO - root -   Epoch: [144/300][40/84], lr: 0.00000095 	 loss = 0.0378(0.3035)
2023/10/31 04:21:37 - INFO - root -   Epoch: [144/300][60/84], lr: 0.00000095 	 loss = 0.0279(0.3321)
2023/10/31 04:22:22 - INFO - root -   Epoch: [144/300][80/84], lr: 0.00000095 	 loss = 0.1723(0.3288)
2023/10/31 04:22:24 - INFO - root -   Epoch: [144/300] 	 loss = 0.3222
2023/10/31 04:23:20 - INFO - root -   precision = 0.8837
2023/10/31 04:23:20 - INFO - root -   eval_loss = 0.2789
2023/10/31 04:23:20 - INFO - root -   eval_acc = 0.8837
2023/10/31 04:23:21 - INFO - root -   train_accuracy = 0.8631
2023/10/31 04:23:50 - INFO - root -   Epoch: [145/300][0/84], lr: 0.00000095 	 loss = 0.2456(0.2456)
2023/10/31 04:24:55 - INFO - root -   Epoch: [145/300][20/84], lr: 0.00000095 	 loss = 1.3176(0.3776)
2023/10/31 04:25:52 - INFO - root -   Epoch: [145/300][40/84], lr: 0.00000095 	 loss = 0.0617(0.3307)
2023/10/31 04:26:57 - INFO - root -   Epoch: [145/300][60/84], lr: 0.00000095 	 loss = 0.0382(0.3490)
2023/10/31 04:27:51 - INFO - root -   Epoch: [145/300][80/84], lr: 0.00000095 	 loss = 0.1258(0.3512)
2023/10/31 04:27:56 - INFO - root -   Epoch: [145/300] 	 loss = 0.3452
2023/10/31 04:27:56 - INFO - root -   train_accuracy = 0.8750
2023/10/31 04:28:18 - INFO - root -   Epoch: [146/300][0/84], lr: 0.00000096 	 loss = 0.0805(0.0805)
2023/10/31 04:29:30 - INFO - root -   Epoch: [146/300][20/84], lr: 0.00000096 	 loss = 0.4818(0.1972)
2023/10/31 04:30:24 - INFO - root -   Epoch: [146/300][40/84], lr: 0.00000096 	 loss = 0.0492(0.2410)
2023/10/31 04:31:55 - INFO - root -   Epoch: [146/300][60/84], lr: 0.00000096 	 loss = 0.1115(0.2916)
2023/10/31 04:32:31 - INFO - root -   Epoch: [146/300][80/84], lr: 0.00000096 	 loss = 0.0326(0.3063)
2023/10/31 04:32:34 - INFO - root -   Epoch: [146/300] 	 loss = 0.2980
2023/10/31 04:32:34 - INFO - root -   train_accuracy = 0.8631
2023/10/31 04:32:57 - INFO - root -   Epoch: [147/300][0/84], lr: 0.00000097 	 loss = 0.0086(0.0086)
2023/10/31 04:34:03 - INFO - root -   Epoch: [147/300][20/84], lr: 0.00000097 	 loss = 0.2487(0.2309)
2023/10/31 04:34:55 - INFO - root -   Epoch: [147/300][40/84], lr: 0.00000097 	 loss = 0.0976(0.2625)
2023/10/31 04:36:16 - INFO - root -   Epoch: [147/300][60/84], lr: 0.00000097 	 loss = 0.0374(0.2636)
2023/10/31 04:37:05 - INFO - root -   Epoch: [147/300][80/84], lr: 0.00000097 	 loss = 0.0200(0.2775)
2023/10/31 04:37:07 - INFO - root -   Epoch: [147/300] 	 loss = 0.2714
2023/10/31 04:37:07 - INFO - root -   train_accuracy = 0.9107
2023/10/31 04:37:28 - INFO - root -   Epoch: [148/300][0/84], lr: 0.00000097 	 loss = 0.0068(0.0068)
2023/10/31 04:38:35 - INFO - root -   Epoch: [148/300][20/84], lr: 0.00000097 	 loss = 1.0834(0.2501)
2023/10/31 04:39:48 - INFO - root -   Epoch: [148/300][40/84], lr: 0.00000097 	 loss = 0.0506(0.2886)
2023/10/31 04:40:56 - INFO - root -   Epoch: [148/300][60/84], lr: 0.00000097 	 loss = 0.2191(0.3289)
2023/10/31 04:41:52 - INFO - root -   Epoch: [148/300][80/84], lr: 0.00000097 	 loss = 0.0857(0.3247)
2023/10/31 04:41:53 - INFO - root -   Epoch: [148/300] 	 loss = 0.3161
2023/10/31 04:41:53 - INFO - root -   train_accuracy = 0.8869
2023/10/31 04:42:22 - INFO - root -   Epoch: [149/300][0/84], lr: 0.00000098 	 loss = 0.0191(0.0191)
2023/10/31 04:43:15 - INFO - root -   Epoch: [149/300][20/84], lr: 0.00000098 	 loss = 0.2282(0.2590)
2023/10/31 04:44:37 - INFO - root -   Epoch: [149/300][40/84], lr: 0.00000098 	 loss = 0.0404(0.2926)
2023/10/31 04:45:34 - INFO - root -   Epoch: [149/300][60/84], lr: 0.00000098 	 loss = 0.0105(0.3395)
2023/10/31 04:46:32 - INFO - root -   Epoch: [149/300][80/84], lr: 0.00000098 	 loss = 0.0201(0.3276)
2023/10/31 04:46:33 - INFO - root -   Epoch: [149/300] 	 loss = 0.3187
2023/10/31 04:47:30 - INFO - root -   precision = 0.8837
2023/10/31 04:47:30 - INFO - root -   eval_loss = 0.2658
2023/10/31 04:47:30 - INFO - root -   eval_acc = 0.8837
2023/10/31 04:47:31 - INFO - root -   train_accuracy = 0.8869
2023/10/31 04:47:52 - INFO - root -   Epoch: [150/300][0/84], lr: 0.00000098 	 loss = 0.0123(0.0123)
2023/10/31 04:49:07 - INFO - root -   Epoch: [150/300][20/84], lr: 0.00000098 	 loss = 0.2143(0.2394)
2023/10/31 04:50:15 - INFO - root -   Epoch: [150/300][40/84], lr: 0.00000098 	 loss = 0.0959(0.2696)
2023/10/31 04:51:19 - INFO - root -   Epoch: [150/300][60/84], lr: 0.00000098 	 loss = 0.0154(0.2865)
2023/10/31 04:52:13 - INFO - root -   Epoch: [150/300][80/84], lr: 0.00000098 	 loss = 0.0089(0.2956)
2023/10/31 04:52:14 - INFO - root -   Epoch: [150/300] 	 loss = 0.2878
2023/10/31 04:52:14 - INFO - root -   train_accuracy = 0.8869
2023/10/31 04:52:36 - INFO - root -   Epoch: [151/300][0/84], lr: 0.00000099 	 loss = 0.0065(0.0065)
2023/10/31 04:53:41 - INFO - root -   Epoch: [151/300][20/84], lr: 0.00000099 	 loss = 0.4588(0.3073)
2023/10/31 04:55:07 - INFO - root -   Epoch: [151/300][40/84], lr: 0.00000099 	 loss = 0.1213(0.2745)
2023/10/31 04:56:04 - INFO - root -   Epoch: [151/300][60/84], lr: 0.00000099 	 loss = 0.0156(0.2905)
2023/10/31 04:57:02 - INFO - root -   Epoch: [151/300][80/84], lr: 0.00000099 	 loss = 0.0177(0.2952)
2023/10/31 04:57:04 - INFO - root -   Epoch: [151/300] 	 loss = 0.2890
2023/10/31 04:57:04 - INFO - root -   train_accuracy = 0.9048
2023/10/31 04:57:33 - INFO - root -   Epoch: [152/300][0/84], lr: 0.00000100 	 loss = 0.0430(0.0430)
2023/10/31 04:58:34 - INFO - root -   Epoch: [152/300][20/84], lr: 0.00000100 	 loss = 0.3653(0.2952)
2023/10/31 04:59:50 - INFO - root -   Epoch: [152/300][40/84], lr: 0.00000100 	 loss = 0.1037(0.2972)
2023/10/31 05:00:52 - INFO - root -   Epoch: [152/300][60/84], lr: 0.00000100 	 loss = 0.0820(0.3078)
2023/10/31 05:01:44 - INFO - root -   Epoch: [152/300][80/84], lr: 0.00000100 	 loss = 0.0021(0.3055)
2023/10/31 05:01:45 - INFO - root -   Epoch: [152/300] 	 loss = 0.2965
2023/10/31 05:01:45 - INFO - root -   train_accuracy = 0.9167
2023/10/31 05:02:14 - INFO - root -   Epoch: [153/300][0/84], lr: 0.00000100 	 loss = 0.1306(0.1306)
2023/10/31 05:03:15 - INFO - root -   Epoch: [153/300][20/84], lr: 0.00000100 	 loss = 0.3844(0.3218)
2023/10/31 05:04:16 - INFO - root -   Epoch: [153/300][40/84], lr: 0.00000100 	 loss = 0.0270(0.3127)
2023/10/31 05:05:35 - INFO - root -   Epoch: [153/300][60/84], lr: 0.00000100 	 loss = 0.0935(0.3231)
2023/10/31 05:06:21 - INFO - root -   Epoch: [153/300][80/84], lr: 0.00000100 	 loss = 0.3907(0.3364)
2023/10/31 05:06:24 - INFO - root -   Epoch: [153/300] 	 loss = 0.3274
2023/10/31 05:06:24 - INFO - root -   train_accuracy = 0.8810
2023/10/31 05:06:46 - INFO - root -   Epoch: [154/300][0/84], lr: 0.00000101 	 loss = 0.0194(0.0194)
2023/10/31 05:07:59 - INFO - root -   Epoch: [154/300][20/84], lr: 0.00000101 	 loss = 1.1658(0.3418)
2023/10/31 05:09:03 - INFO - root -   Epoch: [154/300][40/84], lr: 0.00000101 	 loss = 0.0220(0.3127)
2023/10/31 05:10:10 - INFO - root -   Epoch: [154/300][60/84], lr: 0.00000101 	 loss = 0.1328(0.3286)
2023/10/31 05:10:53 - INFO - root -   Epoch: [154/300][80/84], lr: 0.00000101 	 loss = 0.0073(0.3024)
2023/10/31 05:10:55 - INFO - root -   Epoch: [154/300] 	 loss = 0.2921
2023/10/31 05:11:51 - INFO - root -   precision = 0.8837
2023/10/31 05:11:51 - INFO - root -   eval_loss = 0.2823
2023/10/31 05:11:51 - INFO - root -   eval_acc = 0.8837
2023/10/31 05:11:52 - INFO - root -   train_accuracy = 0.8929
2023/10/31 05:12:29 - INFO - root -   Epoch: [155/300][0/84], lr: 0.00000101 	 loss = 0.0607(0.0607)
2023/10/31 05:13:20 - INFO - root -   Epoch: [155/300][20/84], lr: 0.00000101 	 loss = 0.3815(0.3189)
2023/10/31 05:14:33 - INFO - root -   Epoch: [155/300][40/84], lr: 0.00000101 	 loss = 0.0315(0.3216)
2023/10/31 05:15:35 - INFO - root -   Epoch: [155/300][60/84], lr: 0.00000101 	 loss = 0.1446(0.3008)
2023/10/31 05:16:32 - INFO - root -   Epoch: [155/300][80/84], lr: 0.00000101 	 loss = 0.0632(0.3301)
2023/10/31 05:16:33 - INFO - root -   Epoch: [155/300] 	 loss = 0.3245
2023/10/31 05:16:33 - INFO - root -   train_accuracy = 0.8869
2023/10/31 05:17:03 - INFO - root -   Epoch: [156/300][0/84], lr: 0.00000102 	 loss = 0.0449(0.0449)
2023/10/31 05:18:23 - INFO - root -   Epoch: [156/300][20/84], lr: 0.00000102 	 loss = 0.2666(0.2036)
2023/10/31 05:19:39 - INFO - root -   Epoch: [156/300][40/84], lr: 0.00000102 	 loss = 0.0357(0.2830)
2023/10/31 05:20:27 - INFO - root -   Epoch: [156/300][60/84], lr: 0.00000102 	 loss = 0.0509(0.3395)
2023/10/31 05:21:23 - INFO - root -   Epoch: [156/300][80/84], lr: 0.00000102 	 loss = 0.1019(0.3372)
2023/10/31 05:21:24 - INFO - root -   Epoch: [156/300] 	 loss = 0.3269
2023/10/31 05:21:24 - INFO - root -   train_accuracy = 0.9107
2023/10/31 05:21:46 - INFO - root -   Epoch: [157/300][0/84], lr: 0.00000103 	 loss = 0.0136(0.0136)
2023/10/31 05:22:57 - INFO - root -   Epoch: [157/300][20/84], lr: 0.00000103 	 loss = 1.1828(0.3167)
2023/10/31 05:24:07 - INFO - root -   Epoch: [157/300][40/84], lr: 0.00000103 	 loss = 0.1494(0.3107)
2023/10/31 05:25:13 - INFO - root -   Epoch: [157/300][60/84], lr: 0.00000103 	 loss = 0.0592(0.3353)
2023/10/31 05:26:04 - INFO - root -   Epoch: [157/300][80/84], lr: 0.00000103 	 loss = 0.2647(0.3410)
2023/10/31 05:26:06 - INFO - root -   Epoch: [157/300] 	 loss = 0.3381
2023/10/31 05:26:06 - INFO - root -   train_accuracy = 0.8750
2023/10/31 05:26:28 - INFO - root -   Epoch: [158/300][0/84], lr: 0.00000103 	 loss = 0.0317(0.0317)
2023/10/31 05:27:44 - INFO - root -   Epoch: [158/300][20/84], lr: 0.00000103 	 loss = 0.4942(0.2624)
2023/10/31 05:28:35 - INFO - root -   Epoch: [158/300][40/84], lr: 0.00000103 	 loss = 0.0994(0.2762)
2023/10/31 05:29:41 - INFO - root -   Epoch: [158/300][60/84], lr: 0.00000103 	 loss = 0.0568(0.2833)
2023/10/31 05:30:32 - INFO - root -   Epoch: [158/300][80/84], lr: 0.00000103 	 loss = 0.4919(0.2808)
2023/10/31 05:30:35 - INFO - root -   Epoch: [158/300] 	 loss = 0.2745
2023/10/31 05:30:35 - INFO - root -   train_accuracy = 0.9048
2023/10/31 05:30:56 - INFO - root -   Epoch: [159/300][0/84], lr: 0.00000104 	 loss = 0.0384(0.0384)
2023/10/31 05:32:04 - INFO - root -   Epoch: [159/300][20/84], lr: 0.00000104 	 loss = 1.0725(0.3446)
2023/10/31 05:33:13 - INFO - root -   Epoch: [159/300][40/84], lr: 0.00000104 	 loss = 0.0704(0.2872)
2023/10/31 05:34:30 - INFO - root -   Epoch: [159/300][60/84], lr: 0.00000104 	 loss = 0.1062(0.2577)
2023/10/31 05:35:20 - INFO - root -   Epoch: [159/300][80/84], lr: 0.00000104 	 loss = 0.0863(0.2783)
2023/10/31 05:35:23 - INFO - root -   Epoch: [159/300] 	 loss = 0.2699
2023/10/31 05:36:20 - INFO - root -   precision = 0.9302
2023/10/31 05:36:20 - INFO - root -   eval_loss = 0.2107
2023/10/31 05:36:20 - INFO - root -   eval_acc = 0.9302
2023/10/31 05:36:21 - INFO - root -   train_accuracy = 0.9107
2023/10/31 05:36:42 - INFO - root -   Epoch: [160/300][0/84], lr: 0.00000104 	 loss = 0.0361(0.0361)
2023/10/31 05:37:49 - INFO - root -   Epoch: [160/300][20/84], lr: 0.00000104 	 loss = 0.2183(0.2241)
2023/10/31 05:38:58 - INFO - root -   Epoch: [160/300][40/84], lr: 0.00000104 	 loss = 0.0184(0.2692)
2023/10/31 05:40:17 - INFO - root -   Epoch: [160/300][60/84], lr: 0.00000104 	 loss = 0.2051(0.2773)
2023/10/31 05:40:54 - INFO - root -   Epoch: [160/300][80/84], lr: 0.00000104 	 loss = 0.0105(0.2940)
2023/10/31 05:40:59 - INFO - root -   Epoch: [160/300] 	 loss = 0.2857
2023/10/31 05:40:59 - INFO - root -   train_accuracy = 0.9048
2023/10/31 05:41:21 - INFO - root -   Epoch: [161/300][0/84], lr: 0.00000105 	 loss = 0.0171(0.0171)
2023/10/31 05:42:28 - INFO - root -   Epoch: [161/300][20/84], lr: 0.00000105 	 loss = 0.2687(0.2836)
2023/10/31 05:43:55 - INFO - root -   Epoch: [161/300][40/84], lr: 0.00000105 	 loss = 0.0269(0.2325)
2023/10/31 05:44:53 - INFO - root -   Epoch: [161/300][60/84], lr: 0.00000105 	 loss = 0.0446(0.2484)
2023/10/31 05:45:40 - INFO - root -   Epoch: [161/300][80/84], lr: 0.00000105 	 loss = 0.0863(0.2315)
2023/10/31 05:45:41 - INFO - root -   Epoch: [161/300] 	 loss = 0.2292
2023/10/31 05:45:41 - INFO - root -   train_accuracy = 0.9405
2023/10/31 05:46:02 - INFO - root -   Epoch: [162/300][0/84], lr: 0.00000105 	 loss = 0.0269(0.0269)
2023/10/31 05:47:03 - INFO - root -   Epoch: [162/300][20/84], lr: 0.00000105 	 loss = 0.0719(0.1519)
2023/10/31 05:48:22 - INFO - root -   Epoch: [162/300][40/84], lr: 0.00000105 	 loss = 0.0764(0.3174)
2023/10/31 05:49:36 - INFO - root -   Epoch: [162/300][60/84], lr: 0.00000105 	 loss = 0.3832(0.3108)
2023/10/31 05:50:28 - INFO - root -   Epoch: [162/300][80/84], lr: 0.00000105 	 loss = 0.1572(0.3426)
2023/10/31 05:50:30 - INFO - root -   Epoch: [162/300] 	 loss = 0.3311
2023/10/31 05:50:30 - INFO - root -   train_accuracy = 0.8869
2023/10/31 05:51:01 - INFO - root -   Epoch: [163/300][0/84], lr: 0.00000106 	 loss = 0.1503(0.1503)
2023/10/31 05:52:03 - INFO - root -   Epoch: [163/300][20/84], lr: 0.00000106 	 loss = 0.1878(0.2022)
2023/10/31 05:53:13 - INFO - root -   Epoch: [163/300][40/84], lr: 0.00000106 	 loss = 0.0309(0.2549)
2023/10/31 05:54:03 - INFO - root -   Epoch: [163/300][60/84], lr: 0.00000106 	 loss = 0.1803(0.3207)
2023/10/31 05:55:02 - INFO - root -   Epoch: [163/300][80/84], lr: 0.00000106 	 loss = 0.0775(0.3118)
2023/10/31 05:55:06 - INFO - root -   Epoch: [163/300] 	 loss = 0.3049
2023/10/31 05:55:06 - INFO - root -   train_accuracy = 0.8929
2023/10/31 05:55:27 - INFO - root -   Epoch: [164/300][0/84], lr: 0.00000107 	 loss = 0.0077(0.0077)
2023/10/31 05:56:35 - INFO - root -   Epoch: [164/300][20/84], lr: 0.00000107 	 loss = 0.4101(0.2883)
2023/10/31 05:57:35 - INFO - root -   Epoch: [164/300][40/84], lr: 0.00000107 	 loss = 0.0585(0.2765)
2023/10/31 05:58:52 - INFO - root -   Epoch: [164/300][60/84], lr: 0.00000107 	 loss = 0.0622(0.2794)
2023/10/31 05:59:45 - INFO - root -   Epoch: [164/300][80/84], lr: 0.00000107 	 loss = 0.0361(0.3116)
2023/10/31 05:59:47 - INFO - root -   Epoch: [164/300] 	 loss = 0.3061
2023/10/31 06:00:44 - INFO - root -   precision = 0.8837
2023/10/31 06:00:44 - INFO - root -   eval_loss = 0.2512
2023/10/31 06:00:44 - INFO - root -   eval_acc = 0.8837
2023/10/31 06:00:45 - INFO - root -   train_accuracy = 0.8869
2023/10/31 06:01:14 - INFO - root -   Epoch: [165/300][0/84], lr: 0.00000107 	 loss = 0.0382(0.0382)
2023/10/31 06:02:21 - INFO - root -   Epoch: [165/300][20/84], lr: 0.00000107 	 loss = 0.3171(0.2686)
2023/10/31 06:03:34 - INFO - root -   Epoch: [165/300][40/84], lr: 0.00000107 	 loss = 0.0482(0.2890)
2023/10/31 06:04:24 - INFO - root -   Epoch: [165/300][60/84], lr: 0.00000107 	 loss = 0.1227(0.3219)
2023/10/31 06:05:23 - INFO - root -   Epoch: [165/300][80/84], lr: 0.00000107 	 loss = 0.0116(0.3105)
2023/10/31 06:05:24 - INFO - root -   Epoch: [165/300] 	 loss = 0.3036
2023/10/31 06:05:24 - INFO - root -   train_accuracy = 0.8988
2023/10/31 06:05:54 - INFO - root -   Epoch: [166/300][0/84], lr: 0.00000108 	 loss = 0.0637(0.0637)
2023/10/31 06:06:54 - INFO - root -   Epoch: [166/300][20/84], lr: 0.00000108 	 loss = 0.4148(0.3287)
2023/10/31 06:08:08 - INFO - root -   Epoch: [166/300][40/84], lr: 0.00000108 	 loss = 0.1337(0.2893)
2023/10/31 06:09:13 - INFO - root -   Epoch: [166/300][60/84], lr: 0.00000108 	 loss = 0.0268(0.2795)
2023/10/31 06:10:10 - INFO - root -   Epoch: [166/300][80/84], lr: 0.00000108 	 loss = 0.0331(0.2792)
2023/10/31 06:10:11 - INFO - root -   Epoch: [166/300] 	 loss = 0.2749
2023/10/31 06:10:11 - INFO - root -   train_accuracy = 0.8869
2023/10/31 06:10:33 - INFO - root -   Epoch: [167/300][0/84], lr: 0.00000108 	 loss = 0.0168(0.0168)
2023/10/31 06:11:41 - INFO - root -   Epoch: [167/300][20/84], lr: 0.00000108 	 loss = 1.4267(0.2115)
2023/10/31 06:12:46 - INFO - root -   Epoch: [167/300][40/84], lr: 0.00000108 	 loss = 0.0167(0.2266)
2023/10/31 06:14:04 - INFO - root -   Epoch: [167/300][60/84], lr: 0.00000108 	 loss = 0.0162(0.2705)
2023/10/31 06:14:54 - INFO - root -   Epoch: [167/300][80/84], lr: 0.00000108 	 loss = 0.0434(0.2782)
2023/10/31 06:14:58 - INFO - root -   Epoch: [167/300] 	 loss = 0.2725
2023/10/31 06:14:58 - INFO - root -   train_accuracy = 0.8988
2023/10/31 06:15:37 - INFO - root -   Epoch: [168/300][0/84], lr: 0.00000109 	 loss = 0.1861(0.1861)
2023/10/31 06:16:34 - INFO - root -   Epoch: [168/300][20/84], lr: 0.00000109 	 loss = 0.2522(0.2003)
2023/10/31 06:17:57 - INFO - root -   Epoch: [168/300][40/84], lr: 0.00000109 	 loss = 0.3023(0.2469)
2023/10/31 06:18:38 - INFO - root -   Epoch: [168/300][60/84], lr: 0.00000109 	 loss = 0.1531(0.2908)
2023/10/31 06:19:39 - INFO - root -   Epoch: [168/300][80/84], lr: 0.00000109 	 loss = 0.0867(0.2905)
2023/10/31 06:19:41 - INFO - root -   Epoch: [168/300] 	 loss = 0.2834
2023/10/31 06:19:41 - INFO - root -   train_accuracy = 0.8929
2023/10/31 06:20:18 - INFO - root -   Epoch: [169/300][0/84], lr: 0.00000110 	 loss = 0.0498(0.0498)
2023/10/31 06:21:21 - INFO - root -   Epoch: [169/300][20/84], lr: 0.00000110 	 loss = 0.2604(0.2305)
2023/10/31 06:22:27 - INFO - root -   Epoch: [169/300][40/84], lr: 0.00000110 	 loss = 0.0113(0.2740)
2023/10/31 06:23:35 - INFO - root -   Epoch: [169/300][60/84], lr: 0.00000110 	 loss = 0.0372(0.3157)
2023/10/31 06:24:23 - INFO - root -   Epoch: [169/300][80/84], lr: 0.00000110 	 loss = 0.1200(0.3193)
2023/10/31 06:24:28 - INFO - root -   Epoch: [169/300] 	 loss = 0.3104
2023/10/31 06:25:24 - INFO - root -   precision = 0.8837
2023/10/31 06:25:24 - INFO - root -   eval_loss = 0.2683
2023/10/31 06:25:24 - INFO - root -   eval_acc = 0.8837
2023/10/31 06:25:25 - INFO - root -   train_accuracy = 0.9048
2023/10/31 06:25:56 - INFO - root -   Epoch: [170/300][0/84], lr: 0.00000110 	 loss = 0.0886(0.0886)
2023/10/31 06:27:10 - INFO - root -   Epoch: [170/300][20/84], lr: 0.00000110 	 loss = 0.3829(0.2669)
2023/10/31 06:28:13 - INFO - root -   Epoch: [170/300][40/84], lr: 0.00000110 	 loss = 0.0812(0.2868)
2023/10/31 06:29:31 - INFO - root -   Epoch: [170/300][60/84], lr: 0.00000110 	 loss = 0.2518(0.2885)
2023/10/31 06:30:09 - INFO - root -   Epoch: [170/300][80/84], lr: 0.00000110 	 loss = 0.0152(0.3112)
2023/10/31 06:30:12 - INFO - root -   Epoch: [170/300] 	 loss = 0.3025
2023/10/31 06:30:12 - INFO - root -   train_accuracy = 0.9048
2023/10/31 06:30:41 - INFO - root -   Epoch: [171/300][0/84], lr: 0.00000111 	 loss = 0.0160(0.0160)
2023/10/31 06:31:47 - INFO - root -   Epoch: [171/300][20/84], lr: 0.00000111 	 loss = 1.0117(0.3155)
2023/10/31 06:32:46 - INFO - root -   Epoch: [171/300][40/84], lr: 0.00000111 	 loss = 0.0929(0.3086)
2023/10/31 06:34:03 - INFO - root -   Epoch: [171/300][60/84], lr: 0.00000111 	 loss = 0.0620(0.3263)
2023/10/31 06:34:40 - INFO - root -   Epoch: [171/300][80/84], lr: 0.00000111 	 loss = 0.0032(0.3465)
2023/10/31 06:34:45 - INFO - root -   Epoch: [171/300] 	 loss = 0.3372
2023/10/31 06:34:45 - INFO - root -   train_accuracy = 0.8810
2023/10/31 06:35:07 - INFO - root -   Epoch: [172/300][0/84], lr: 0.00000111 	 loss = 0.0054(0.0054)
2023/10/31 06:36:12 - INFO - root -   Epoch: [172/300][20/84], lr: 0.00000111 	 loss = 0.1553(0.1750)
2023/10/31 06:37:19 - INFO - root -   Epoch: [172/300][40/84], lr: 0.00000111 	 loss = 0.0178(0.2254)
2023/10/31 06:38:11 - INFO - root -   Epoch: [172/300][60/84], lr: 0.00000111 	 loss = 0.0557(0.2630)
2023/10/31 06:39:20 - INFO - root -   Epoch: [172/300][80/84], lr: 0.00000111 	 loss = 0.0090(0.2562)
2023/10/31 06:39:21 - INFO - root -   Epoch: [172/300] 	 loss = 0.2494
2023/10/31 06:39:21 - INFO - root -   train_accuracy = 0.9048
2023/10/31 06:39:51 - INFO - root -   Epoch: [173/300][0/84], lr: 0.00000112 	 loss = 0.1698(0.1698)
2023/10/31 06:40:48 - INFO - root -   Epoch: [173/300][20/84], lr: 0.00000112 	 loss = 1.8593(0.3612)
2023/10/31 06:42:04 - INFO - root -   Epoch: [173/300][40/84], lr: 0.00000112 	 loss = 0.0163(0.3504)
2023/10/31 06:43:29 - INFO - root -   Epoch: [173/300][60/84], lr: 0.00000112 	 loss = 0.0707(0.3715)
2023/10/31 06:44:08 - INFO - root -   Epoch: [173/300][80/84], lr: 0.00000112 	 loss = 0.1650(0.3681)
2023/10/31 06:44:11 - INFO - root -   Epoch: [173/300] 	 loss = 0.3581
2023/10/31 06:44:11 - INFO - root -   train_accuracy = 0.8810
2023/10/31 06:44:33 - INFO - root -   Epoch: [174/300][0/84], lr: 0.00000113 	 loss = 0.2435(0.2435)
2023/10/31 06:45:38 - INFO - root -   Epoch: [174/300][20/84], lr: 0.00000113 	 loss = 0.1436(0.2881)
2023/10/31 06:47:15 - INFO - root -   Epoch: [174/300][40/84], lr: 0.00000113 	 loss = 0.0374(0.2332)
2023/10/31 06:48:16 - INFO - root -   Epoch: [174/300][60/84], lr: 0.00000113 	 loss = 0.0994(0.2768)
2023/10/31 06:49:03 - INFO - root -   Epoch: [174/300][80/84], lr: 0.00000113 	 loss = 0.0056(0.3020)
2023/10/31 06:49:04 - INFO - root -   Epoch: [174/300] 	 loss = 0.2971
2023/10/31 06:50:00 - INFO - root -   precision = 0.9302
2023/10/31 06:50:00 - INFO - root -   eval_loss = 0.2300
2023/10/31 06:50:00 - INFO - root -   eval_acc = 0.9302
2023/10/31 06:50:01 - INFO - root -   train_accuracy = 0.9048
2023/10/31 06:50:23 - INFO - root -   Epoch: [175/300][0/84], lr: 0.00000113 	 loss = 0.0216(0.0216)
2023/10/31 06:51:31 - INFO - root -   Epoch: [175/300][20/84], lr: 0.00000113 	 loss = 0.1983(0.2639)
2023/10/31 06:52:33 - INFO - root -   Epoch: [175/300][40/84], lr: 0.00000113 	 loss = 0.0262(0.2701)
2023/10/31 06:53:28 - INFO - root -   Epoch: [175/300][60/84], lr: 0.00000113 	 loss = 0.0470(0.2961)
2023/10/31 06:54:31 - INFO - root -   Epoch: [175/300][80/84], lr: 0.00000113 	 loss = 0.0205(0.2774)
2023/10/31 06:54:32 - INFO - root -   Epoch: [175/300] 	 loss = 0.2726
2023/10/31 06:54:32 - INFO - root -   train_accuracy = 0.8988
2023/10/31 06:55:02 - INFO - root -   Epoch: [176/300][0/84], lr: 0.00000114 	 loss = 0.0258(0.0258)
2023/10/31 06:56:00 - INFO - root -   Epoch: [176/300][20/84], lr: 0.00000114 	 loss = 1.0754(0.3063)
2023/10/31 06:57:26 - INFO - root -   Epoch: [176/300][40/84], lr: 0.00000114 	 loss = 0.5412(0.2797)
2023/10/31 06:58:15 - INFO - root -   Epoch: [176/300][60/84], lr: 0.00000114 	 loss = 0.0746(0.2727)
2023/10/31 06:59:04 - INFO - root -   Epoch: [176/300][80/84], lr: 0.00000114 	 loss = 0.0243(0.2766)
2023/10/31 06:59:05 - INFO - root -   Epoch: [176/300] 	 loss = 0.2724
2023/10/31 06:59:05 - INFO - root -   train_accuracy = 0.8929
2023/10/31 06:59:27 - INFO - root -   Epoch: [177/300][0/84], lr: 0.00000114 	 loss = 0.0095(0.0095)
2023/10/31 07:00:26 - INFO - root -   Epoch: [177/300][20/84], lr: 0.00000114 	 loss = 0.5379(0.2474)
2023/10/31 07:01:53 - INFO - root -   Epoch: [177/300][40/84], lr: 0.00000114 	 loss = 0.0232(0.2299)
2023/10/31 07:02:48 - INFO - root -   Epoch: [177/300][60/84], lr: 0.00000114 	 loss = 0.0938(0.2301)
2023/10/31 07:03:51 - INFO - root -   Epoch: [177/300][80/84], lr: 0.00000114 	 loss = 1.3783(0.2550)
2023/10/31 07:03:52 - INFO - root -   Epoch: [177/300] 	 loss = 0.2487
2023/10/31 07:03:52 - INFO - root -   train_accuracy = 0.9107
2023/10/31 07:04:22 - INFO - root -   Epoch: [178/300][0/84], lr: 0.00000115 	 loss = 0.0650(0.0650)
2023/10/31 07:05:14 - INFO - root -   Epoch: [178/300][20/84], lr: 0.00000115 	 loss = 0.2643(0.2889)
2023/10/31 07:06:19 - INFO - root -   Epoch: [178/300][40/84], lr: 0.00000115 	 loss = 0.0206(0.2259)
2023/10/31 07:07:21 - INFO - root -   Epoch: [178/300][60/84], lr: 0.00000115 	 loss = 0.0448(0.2389)
2023/10/31 07:08:20 - INFO - root -   Epoch: [178/300][80/84], lr: 0.00000115 	 loss = 0.0058(0.2619)
2023/10/31 07:08:22 - INFO - root -   Epoch: [178/300] 	 loss = 0.2549
2023/10/31 07:08:22 - INFO - root -   train_accuracy = 0.9167
2023/10/31 07:08:59 - INFO - root -   Epoch: [179/300][0/84], lr: 0.00000115 	 loss = 0.0649(0.0649)
2023/10/31 07:09:57 - INFO - root -   Epoch: [179/300][20/84], lr: 0.00000115 	 loss = 0.4160(0.2998)
2023/10/31 07:11:11 - INFO - root -   Epoch: [179/300][40/84], lr: 0.00000115 	 loss = 0.1416(0.3074)
2023/10/31 07:12:13 - INFO - root -   Epoch: [179/300][60/84], lr: 0.00000115 	 loss = 0.1901(0.3349)
2023/10/31 07:12:54 - INFO - root -   Epoch: [179/300][80/84], lr: 0.00000115 	 loss = 0.0067(0.3114)
2023/10/31 07:12:56 - INFO - root -   Epoch: [179/300] 	 loss = 0.3025
2023/10/31 07:13:54 - INFO - root -   precision = 0.9070
2023/10/31 07:13:54 - INFO - root -   eval_loss = 0.2035
2023/10/31 07:13:54 - INFO - root -   eval_acc = 0.9070
2023/10/31 07:13:55 - INFO - root -   train_accuracy = 0.9107
2023/10/31 07:14:17 - INFO - root -   Epoch: [180/300][0/84], lr: 0.00000116 	 loss = 0.0142(0.0142)
2023/10/31 07:15:39 - INFO - root -   Epoch: [180/300][20/84], lr: 0.00000116 	 loss = 1.3302(0.3294)
2023/10/31 07:17:01 - INFO - root -   Epoch: [180/300][40/84], lr: 0.00000116 	 loss = 0.3789(0.3339)
2023/10/31 07:18:10 - INFO - root -   Epoch: [180/300][60/84], lr: 0.00000116 	 loss = 0.1443(0.3268)
2023/10/31 07:18:51 - INFO - root -   Epoch: [180/300][80/84], lr: 0.00000116 	 loss = 0.0244(0.3219)
2023/10/31 07:18:52 - INFO - root -   Epoch: [180/300] 	 loss = 0.3135
2023/10/31 07:18:52 - INFO - root -   train_accuracy = 0.8869
2023/10/31 07:19:29 - INFO - root -   Epoch: [181/300][0/84], lr: 0.00000117 	 loss = 0.0629(0.0629)
2023/10/31 07:20:27 - INFO - root -   Epoch: [181/300][20/84], lr: 0.00000117 	 loss = 0.1001(0.1605)
2023/10/31 07:21:37 - INFO - root -   Epoch: [181/300][40/84], lr: 0.00000117 	 loss = 0.0468(0.3070)
2023/10/31 07:22:40 - INFO - root -   Epoch: [181/300][60/84], lr: 0.00000117 	 loss = 0.0534(0.3330)
2023/10/31 07:23:27 - INFO - root -   Epoch: [181/300][80/84], lr: 0.00000117 	 loss = 0.0088(0.3074)
2023/10/31 07:23:30 - INFO - root -   Epoch: [181/300] 	 loss = 0.3040
2023/10/31 07:23:30 - INFO - root -   train_accuracy = 0.8929
2023/10/31 07:23:51 - INFO - root -   Epoch: [182/300][0/84], lr: 0.00000117 	 loss = 0.0115(0.0115)
2023/10/31 07:25:04 - INFO - root -   Epoch: [182/300][20/84], lr: 0.00000117 	 loss = 0.6306(0.3216)
2023/10/31 07:26:01 - INFO - root -   Epoch: [182/300][40/84], lr: 0.00000117 	 loss = 0.0271(0.3245)
2023/10/31 07:27:00 - INFO - root -   Epoch: [182/300][60/84], lr: 0.00000117 	 loss = 0.0749(0.3184)
2023/10/31 07:28:01 - INFO - root -   Epoch: [182/300][80/84], lr: 0.00000117 	 loss = 0.0141(0.3136)
2023/10/31 07:28:03 - INFO - root -   Epoch: [182/300] 	 loss = 0.3074
2023/10/31 07:28:03 - INFO - root -   train_accuracy = 0.8810
2023/10/31 07:28:25 - INFO - root -   Epoch: [183/300][0/84], lr: 0.00000118 	 loss = 0.0644(0.0644)
2023/10/31 07:29:24 - INFO - root -   Epoch: [183/300][20/84], lr: 0.00000118 	 loss = 1.1363(0.3655)
2023/10/31 07:30:43 - INFO - root -   Epoch: [183/300][40/84], lr: 0.00000118 	 loss = 0.1588(0.3617)
2023/10/31 07:31:58 - INFO - root -   Epoch: [183/300][60/84], lr: 0.00000118 	 loss = 0.0600(0.3877)
2023/10/31 07:32:48 - INFO - root -   Epoch: [183/300][80/84], lr: 0.00000118 	 loss = 0.5529(0.3707)
2023/10/31 07:32:49 - INFO - root -   Epoch: [183/300] 	 loss = 0.3656
2023/10/31 07:32:49 - INFO - root -   train_accuracy = 0.8690
2023/10/31 07:33:11 - INFO - root -   Epoch: [184/300][0/84], lr: 0.00000118 	 loss = 0.0708(0.0708)
2023/10/31 07:34:23 - INFO - root -   Epoch: [184/300][20/84], lr: 0.00000118 	 loss = 3.1566(0.4434)
2023/10/31 07:35:28 - INFO - root -   Epoch: [184/300][40/84], lr: 0.00000118 	 loss = 0.0815(0.3551)
2023/10/31 07:36:44 - INFO - root -   Epoch: [184/300][60/84], lr: 0.00000118 	 loss = 0.0925(0.3614)
2023/10/31 07:37:34 - INFO - root -   Epoch: [184/300][80/84], lr: 0.00000118 	 loss = 0.0141(0.3533)
2023/10/31 07:37:37 - INFO - root -   Epoch: [184/300] 	 loss = 0.3449
2023/10/31 07:38:35 - INFO - root -   precision = 0.8837
2023/10/31 07:38:35 - INFO - root -   eval_loss = 0.2831
2023/10/31 07:38:35 - INFO - root -   eval_acc = 0.8837
2023/10/31 07:38:36 - INFO - root -   train_accuracy = 0.8869
2023/10/31 07:39:07 - INFO - root -   Epoch: [185/300][0/84], lr: 0.00000119 	 loss = 0.2436(0.2436)
2023/10/31 07:40:17 - INFO - root -   Epoch: [185/300][20/84], lr: 0.00000119 	 loss = 0.3830(0.2967)
2023/10/31 07:41:08 - INFO - root -   Epoch: [185/300][40/84], lr: 0.00000119 	 loss = 0.0497(0.2767)
2023/10/31 07:42:27 - INFO - root -   Epoch: [185/300][60/84], lr: 0.00000119 	 loss = 0.0266(0.2906)
2023/10/31 07:43:14 - INFO - root -   Epoch: [185/300][80/84], lr: 0.00000119 	 loss = 0.0246(0.3122)
2023/10/31 07:43:18 - INFO - root -   Epoch: [185/300] 	 loss = 0.3054
2023/10/31 07:43:18 - INFO - root -   train_accuracy = 0.8869
2023/10/31 07:43:47 - INFO - root -   Epoch: [186/300][0/84], lr: 0.00000120 	 loss = 0.0867(0.0867)
2023/10/31 07:44:54 - INFO - root -   Epoch: [186/300][20/84], lr: 0.00000120 	 loss = 1.1596(0.3816)
2023/10/31 07:46:02 - INFO - root -   Epoch: [186/300][40/84], lr: 0.00000120 	 loss = 0.1436(0.3509)
2023/10/31 07:47:11 - INFO - root -   Epoch: [186/300][60/84], lr: 0.00000120 	 loss = 0.0324(0.3640)
2023/10/31 07:47:49 - INFO - root -   Epoch: [186/300][80/84], lr: 0.00000120 	 loss = 0.2278(0.3534)
2023/10/31 07:47:50 - INFO - root -   Epoch: [186/300] 	 loss = 0.3446
2023/10/31 07:47:50 - INFO - root -   train_accuracy = 0.8750
2023/10/31 07:48:20 - INFO - root -   Epoch: [187/300][0/84], lr: 0.00000120 	 loss = 0.0680(0.0680)
2023/10/31 07:49:24 - INFO - root -   Epoch: [187/300][20/84], lr: 0.00000120 	 loss = 0.3546(0.2486)
2023/10/31 07:50:20 - INFO - root -   Epoch: [187/300][40/84], lr: 0.00000120 	 loss = 0.1009(0.2894)
2023/10/31 07:51:37 - INFO - root -   Epoch: [187/300][60/84], lr: 0.00000120 	 loss = 0.0329(0.3217)
2023/10/31 07:52:29 - INFO - root -   Epoch: [187/300][80/84], lr: 0.00000120 	 loss = 0.1160(0.3316)
2023/10/31 07:52:31 - INFO - root -   Epoch: [187/300] 	 loss = 0.3268
2023/10/31 07:52:31 - INFO - root -   train_accuracy = 0.9048
2023/10/31 07:52:52 - INFO - root -   Epoch: [188/300][0/84], lr: 0.00000121 	 loss = 0.1091(0.1091)
2023/10/31 07:53:57 - INFO - root -   Epoch: [188/300][20/84], lr: 0.00000121 	 loss = 1.6470(0.3463)
2023/10/31 07:55:02 - INFO - root -   Epoch: [188/300][40/84], lr: 0.00000121 	 loss = 0.0401(0.3020)
2023/10/31 07:56:04 - INFO - root -   Epoch: [188/300][60/84], lr: 0.00000121 	 loss = 0.0427(0.3145)
2023/10/31 07:56:54 - INFO - root -   Epoch: [188/300][80/84], lr: 0.00000121 	 loss = 0.0792(0.3076)
2023/10/31 07:56:55 - INFO - root -   Epoch: [188/300] 	 loss = 0.3015
2023/10/31 07:56:55 - INFO - root -   train_accuracy = 0.8869
2023/10/31 07:57:24 - INFO - root -   Epoch: [189/300][0/84], lr: 0.00000121 	 loss = 0.2148(0.2148)
2023/10/31 07:58:23 - INFO - root -   Epoch: [189/300][20/84], lr: 0.00000121 	 loss = 0.0491(0.2157)
2023/10/31 07:59:23 - INFO - root -   Epoch: [189/300][40/84], lr: 0.00000121 	 loss = 0.0484(0.2194)
2023/10/31 08:00:23 - INFO - root -   Epoch: [189/300][60/84], lr: 0.00000121 	 loss = 0.0684(0.2211)
2023/10/31 08:01:22 - INFO - root -   Epoch: [189/300][80/84], lr: 0.00000121 	 loss = 0.0748(0.2341)
2023/10/31 08:01:23 - INFO - root -   Epoch: [189/300] 	 loss = 0.2293
2023/10/31 08:02:20 - INFO - root -   precision = 0.9070
2023/10/31 08:02:20 - INFO - root -   eval_loss = 0.2788
2023/10/31 08:02:20 - INFO - root -   eval_acc = 0.9070
2023/10/31 08:02:21 - INFO - root -   train_accuracy = 0.9226
2023/10/31 08:02:43 - INFO - root -   Epoch: [190/300][0/84], lr: 0.00000122 	 loss = 0.0201(0.0201)
2023/10/31 08:04:00 - INFO - root -   Epoch: [190/300][20/84], lr: 0.00000122 	 loss = 0.9968(0.3947)
2023/10/31 08:05:04 - INFO - root -   Epoch: [190/300][40/84], lr: 0.00000122 	 loss = 0.0452(0.3853)
2023/10/31 08:06:11 - INFO - root -   Epoch: [190/300][60/84], lr: 0.00000122 	 loss = 0.0768(0.3806)
2023/10/31 08:06:55 - INFO - root -   Epoch: [190/300][80/84], lr: 0.00000122 	 loss = 0.0049(0.3489)
2023/10/31 08:06:58 - INFO - root -   Epoch: [190/300] 	 loss = 0.3409
2023/10/31 08:06:58 - INFO - root -   train_accuracy = 0.8750
2023/10/31 08:07:29 - INFO - root -   Epoch: [191/300][0/84], lr: 0.00000123 	 loss = 0.0362(0.0362)
2023/10/31 08:08:32 - INFO - root -   Epoch: [191/300][20/84], lr: 0.00000123 	 loss = 0.3776(0.2493)
2023/10/31 08:09:45 - INFO - root -   Epoch: [191/300][40/84], lr: 0.00000123 	 loss = 0.0136(0.2181)
2023/10/31 08:10:49 - INFO - root -   Epoch: [191/300][60/84], lr: 0.00000123 	 loss = 0.0146(0.2801)
2023/10/31 08:11:34 - INFO - root -   Epoch: [191/300][80/84], lr: 0.00000123 	 loss = 0.0108(0.2974)
2023/10/31 08:11:36 - INFO - root -   Epoch: [191/300] 	 loss = 0.2877
2023/10/31 08:11:36 - INFO - root -   train_accuracy = 0.8988
2023/10/31 08:11:57 - INFO - root -   Epoch: [192/300][0/84], lr: 0.00000123 	 loss = 0.0027(0.0027)
2023/10/31 08:13:06 - INFO - root -   Epoch: [192/300][20/84], lr: 0.00000123 	 loss = 0.2521(0.2179)
2023/10/31 08:14:18 - INFO - root -   Epoch: [192/300][40/84], lr: 0.00000123 	 loss = 0.0197(0.2889)
2023/10/31 08:15:44 - INFO - root -   Epoch: [192/300][60/84], lr: 0.00000123 	 loss = 0.1475(0.3009)
2023/10/31 08:16:26 - INFO - root -   Epoch: [192/300][80/84], lr: 0.00000123 	 loss = 0.1218(0.3200)
2023/10/31 08:16:27 - INFO - root -   Epoch: [192/300] 	 loss = 0.3129
2023/10/31 08:16:27 - INFO - root -   train_accuracy = 0.8988
2023/10/31 08:16:49 - INFO - root -   Epoch: [193/300][0/84], lr: 0.00000124 	 loss = 0.0305(0.0305)
2023/10/31 08:18:01 - INFO - root -   Epoch: [193/300][20/84], lr: 0.00000124 	 loss = 0.2852(0.2280)
2023/10/31 08:19:13 - INFO - root -   Epoch: [193/300][40/84], lr: 0.00000124 	 loss = 0.0234(0.2251)
2023/10/31 08:20:10 - INFO - root -   Epoch: [193/300][60/84], lr: 0.00000124 	 loss = 0.0266(0.2390)
2023/10/31 08:21:09 - INFO - root -   Epoch: [193/300][80/84], lr: 0.00000124 	 loss = 0.0083(0.2499)
2023/10/31 08:21:10 - INFO - root -   Epoch: [193/300] 	 loss = 0.2431
2023/10/31 08:21:10 - INFO - root -   train_accuracy = 0.9286
2023/10/31 08:21:32 - INFO - root -   Epoch: [194/300][0/84], lr: 0.00000124 	 loss = 0.0159(0.0159)
2023/10/31 08:22:57 - INFO - root -   Epoch: [194/300][20/84], lr: 0.00000124 	 loss = 1.7937(0.2870)
2023/10/31 08:23:55 - INFO - root -   Epoch: [194/300][40/84], lr: 0.00000124 	 loss = 0.0083(0.2977)
2023/10/31 08:24:58 - INFO - root -   Epoch: [194/300][60/84], lr: 0.00000124 	 loss = 0.1409(0.3280)
2023/10/31 08:25:50 - INFO - root -   Epoch: [194/300][80/84], lr: 0.00000124 	 loss = 0.0110(0.3332)
2023/10/31 08:25:51 - INFO - root -   Epoch: [194/300] 	 loss = 0.3268
2023/10/31 08:26:48 - INFO - root -   precision = 0.9070
2023/10/31 08:26:48 - INFO - root -   eval_loss = 0.2443
2023/10/31 08:26:48 - INFO - root -   eval_acc = 0.9070
2023/10/31 08:26:49 - INFO - root -   train_accuracy = 0.9048
2023/10/31 08:27:19 - INFO - root -   Epoch: [195/300][0/84], lr: 0.00000125 	 loss = 0.1091(0.1091)
2023/10/31 08:28:26 - INFO - root -   Epoch: [195/300][20/84], lr: 0.00000125 	 loss = 0.6754(0.2375)
2023/10/31 08:29:42 - INFO - root -   Epoch: [195/300][40/84], lr: 0.00000125 	 loss = 0.0219(0.2616)
2023/10/31 08:30:39 - INFO - root -   Epoch: [195/300][60/84], lr: 0.00000125 	 loss = 0.0109(0.2515)
2023/10/31 08:31:32 - INFO - root -   Epoch: [195/300][80/84], lr: 0.00000125 	 loss = 0.0149(0.2830)
2023/10/31 08:31:35 - INFO - root -   Epoch: [195/300] 	 loss = 0.2810
2023/10/31 08:31:35 - INFO - root -   train_accuracy = 0.8929
2023/10/31 08:31:57 - INFO - root -   Epoch: [196/300][0/84], lr: 0.00000126 	 loss = 0.0147(0.0147)
2023/10/31 08:33:04 - INFO - root -   Epoch: [196/300][20/84], lr: 0.00000126 	 loss = 1.0014(0.2475)
2023/10/31 08:34:06 - INFO - root -   Epoch: [196/300][40/84], lr: 0.00000126 	 loss = 0.0132(0.3497)
2023/10/31 08:35:03 - INFO - root -   Epoch: [196/300][60/84], lr: 0.00000126 	 loss = 0.0541(0.3115)
2023/10/31 08:36:00 - INFO - root -   Epoch: [196/300][80/84], lr: 0.00000126 	 loss = 0.0971(0.3063)
2023/10/31 08:36:06 - INFO - root -   Epoch: [196/300] 	 loss = 0.2999
2023/10/31 08:36:06 - INFO - root -   train_accuracy = 0.8869
2023/10/31 08:36:43 - INFO - root -   Epoch: [197/300][0/84], lr: 0.00000126 	 loss = 0.1612(0.1612)
2023/10/31 08:37:43 - INFO - root -   Epoch: [197/300][20/84], lr: 0.00000126 	 loss = 0.1919(0.3679)
2023/10/31 08:38:59 - INFO - root -   Epoch: [197/300][40/84], lr: 0.00000126 	 loss = 0.0192(0.3638)
2023/10/31 08:39:50 - INFO - root -   Epoch: [197/300][60/84], lr: 0.00000126 	 loss = 0.0423(0.3530)
2023/10/31 08:40:46 - INFO - root -   Epoch: [197/300][80/84], lr: 0.00000126 	 loss = 0.1493(0.3346)
2023/10/31 08:40:49 - INFO - root -   Epoch: [197/300] 	 loss = 0.3326
2023/10/31 08:40:49 - INFO - root -   train_accuracy = 0.8750
2023/10/31 08:41:11 - INFO - root -   Epoch: [198/300][0/84], lr: 0.00000127 	 loss = 0.0109(0.0109)
2023/10/31 08:42:33 - INFO - root -   Epoch: [198/300][20/84], lr: 0.00000127 	 loss = 1.9392(0.3131)
2023/10/31 08:43:32 - INFO - root -   Epoch: [198/300][40/84], lr: 0.00000127 	 loss = 0.0371(0.2599)
2023/10/31 08:44:44 - INFO - root -   Epoch: [198/300][60/84], lr: 0.00000127 	 loss = 0.0070(0.2703)
2023/10/31 08:45:22 - INFO - root -   Epoch: [198/300][80/84], lr: 0.00000127 	 loss = 0.0070(0.2850)
2023/10/31 08:45:25 - INFO - root -   Epoch: [198/300] 	 loss = 0.2791
2023/10/31 08:45:25 - INFO - root -   train_accuracy = 0.9107
2023/10/31 08:45:48 - INFO - root -   Epoch: [199/300][0/84], lr: 0.00000127 	 loss = 0.0423(0.0423)
2023/10/31 08:46:59 - INFO - root -   Epoch: [199/300][20/84], lr: 0.00000127 	 loss = 0.0884(0.1908)
2023/10/31 08:47:49 - INFO - root -   Epoch: [199/300][40/84], lr: 0.00000127 	 loss = 0.0610(0.1908)
2023/10/31 08:49:18 - INFO - root -   Epoch: [199/300][60/84], lr: 0.00000127 	 loss = 0.0900(0.2345)
2023/10/31 08:50:03 - INFO - root -   Epoch: [199/300][80/84], lr: 0.00000127 	 loss = 0.0451(0.2489)
2023/10/31 08:50:08 - INFO - root -   Epoch: [199/300] 	 loss = 0.2432
2023/10/31 08:51:04 - INFO - root -   precision = 0.9302
2023/10/31 08:51:04 - INFO - root -   eval_loss = 0.1954
2023/10/31 08:51:04 - INFO - root -   eval_acc = 0.9302
2023/10/31 08:51:05 - INFO - root -   train_accuracy = 0.9048
2023/10/31 08:51:42 - INFO - root -   Epoch: [200/300][0/84], lr: 0.00000128 	 loss = 0.1146(0.1146)
2023/10/31 08:52:47 - INFO - root -   Epoch: [200/300][20/84], lr: 0.00000128 	 loss = 0.2019(0.2140)
2023/10/31 08:54:04 - INFO - root -   Epoch: [200/300][40/84], lr: 0.00000128 	 loss = 0.0650(0.2304)
2023/10/31 08:55:09 - INFO - root -   Epoch: [200/300][60/84], lr: 0.00000128 	 loss = 0.0516(0.2493)
2023/10/31 08:55:53 - INFO - root -   Epoch: [200/300][80/84], lr: 0.00000128 	 loss = 0.0239(0.2545)
2023/10/31 08:55:59 - INFO - root -   Epoch: [200/300] 	 loss = 0.2474
2023/10/31 08:55:59 - INFO - root -   train_accuracy = 0.9048
2023/10/31 08:56:21 - INFO - root -   Epoch: [201/300][0/84], lr: 0.00000128 	 loss = 0.0139(0.0139)
2023/10/31 08:57:29 - INFO - root -   Epoch: [201/300][20/84], lr: 0.00000128 	 loss = 1.6109(0.2490)
2023/10/31 08:58:38 - INFO - root -   Epoch: [201/300][40/84], lr: 0.00000128 	 loss = 0.2134(0.2842)
2023/10/31 08:59:55 - INFO - root -   Epoch: [201/300][60/84], lr: 0.00000128 	 loss = 0.0291(0.2686)
2023/10/31 09:00:36 - INFO - root -   Epoch: [201/300][80/84], lr: 0.00000128 	 loss = 0.0038(0.2818)
2023/10/31 09:00:38 - INFO - root -   Epoch: [201/300] 	 loss = 0.2734
2023/10/31 09:00:38 - INFO - root -   train_accuracy = 0.9048
2023/10/31 09:00:59 - INFO - root -   Epoch: [202/300][0/84], lr: 0.00000129 	 loss = 0.0022(0.0022)
2023/10/31 09:01:58 - INFO - root -   Epoch: [202/300][20/84], lr: 0.00000129 	 loss = 0.4261(0.2504)
2023/10/31 09:03:02 - INFO - root -   Epoch: [202/300][40/84], lr: 0.00000129 	 loss = 0.0267(0.2953)
2023/10/31 09:04:29 - INFO - root -   Epoch: [202/300][60/84], lr: 0.00000129 	 loss = 0.0405(0.3309)
2023/10/31 09:05:14 - INFO - root -   Epoch: [202/300][80/84], lr: 0.00000129 	 loss = 0.1248(0.3000)
2023/10/31 09:05:19 - INFO - root -   Epoch: [202/300] 	 loss = 0.2952
2023/10/31 09:05:19 - INFO - root -   train_accuracy = 0.8988
2023/10/31 09:05:48 - INFO - root -   Epoch: [203/300][0/84], lr: 0.00000130 	 loss = 0.0660(0.0660)
2023/10/31 09:06:46 - INFO - root -   Epoch: [203/300][20/84], lr: 0.00000130 	 loss = 0.0970(0.1566)
2023/10/31 09:08:04 - INFO - root -   Epoch: [203/300][40/84], lr: 0.00000130 	 loss = 0.1385(0.1826)
2023/10/31 09:09:00 - INFO - root -   Epoch: [203/300][60/84], lr: 0.00000130 	 loss = 0.2650(0.2046)
2023/10/31 09:09:54 - INFO - root -   Epoch: [203/300][80/84], lr: 0.00000130 	 loss = 0.1745(0.2376)
2023/10/31 09:09:55 - INFO - root -   Epoch: [203/300] 	 loss = 0.2299
2023/10/31 09:09:55 - INFO - root -   train_accuracy = 0.9167
2023/10/31 09:10:17 - INFO - root -   Epoch: [204/300][0/84], lr: 0.00000130 	 loss = 0.0031(0.0031)
2023/10/31 09:11:31 - INFO - root -   Epoch: [204/300][20/84], lr: 0.00000130 	 loss = 0.7087(0.2768)
2023/10/31 09:12:48 - INFO - root -   Epoch: [204/300][40/84], lr: 0.00000130 	 loss = 0.0086(0.3063)
2023/10/31 09:13:54 - INFO - root -   Epoch: [204/300][60/84], lr: 0.00000130 	 loss = 0.0283(0.2975)
2023/10/31 09:14:47 - INFO - root -   Epoch: [204/300][80/84], lr: 0.00000130 	 loss = 0.1165(0.3417)
2023/10/31 09:14:50 - INFO - root -   Epoch: [204/300] 	 loss = 0.3337
2023/10/31 09:15:46 - INFO - root -   precision = 0.8837
2023/10/31 09:15:46 - INFO - root -   eval_loss = 0.3532
2023/10/31 09:15:46 - INFO - root -   eval_acc = 0.8837
2023/10/31 09:15:47 - INFO - root -   train_accuracy = 0.8869
2023/10/31 09:16:09 - INFO - root -   Epoch: [205/300][0/84], lr: 0.00000131 	 loss = 0.0070(0.0070)
2023/10/31 09:17:17 - INFO - root -   Epoch: [205/300][20/84], lr: 0.00000131 	 loss = 0.0475(0.2757)
2023/10/31 09:18:14 - INFO - root -   Epoch: [205/300][40/84], lr: 0.00000131 	 loss = 0.0271(0.2870)
2023/10/31 09:19:32 - INFO - root -   Epoch: [205/300][60/84], lr: 0.00000131 	 loss = 0.0195(0.3057)
2023/10/31 09:20:20 - INFO - root -   Epoch: [205/300][80/84], lr: 0.00000131 	 loss = 0.0297(0.2713)
2023/10/31 09:20:24 - INFO - root -   Epoch: [205/300] 	 loss = 0.2667
2023/10/31 09:20:24 - INFO - root -   train_accuracy = 0.8869
2023/10/31 09:20:46 - INFO - root -   Epoch: [206/300][0/84], lr: 0.00000131 	 loss = 0.0077(0.0077)
2023/10/31 09:21:52 - INFO - root -   Epoch: [206/300][20/84], lr: 0.00000131 	 loss = 0.2134(0.2717)
2023/10/31 09:23:02 - INFO - root -   Epoch: [206/300][40/84], lr: 0.00000131 	 loss = 0.0576(0.2678)
2023/10/31 09:24:10 - INFO - root -   Epoch: [206/300][60/84], lr: 0.00000131 	 loss = 0.2668(0.3049)
2023/10/31 09:25:00 - INFO - root -   Epoch: [206/300][80/84], lr: 0.00000131 	 loss = 0.1300(0.2980)
2023/10/31 09:25:02 - INFO - root -   Epoch: [206/300] 	 loss = 0.2886
2023/10/31 09:25:02 - INFO - root -   train_accuracy = 0.8929
2023/10/31 09:25:24 - INFO - root -   Epoch: [207/300][0/84], lr: 0.00000132 	 loss = 0.0492(0.0492)
2023/10/31 09:26:31 - INFO - root -   Epoch: [207/300][20/84], lr: 0.00000132 	 loss = 1.5390(0.3432)
2023/10/31 09:27:40 - INFO - root -   Epoch: [207/300][40/84], lr: 0.00000132 	 loss = 0.0833(0.3046)
2023/10/31 09:28:42 - INFO - root -   Epoch: [207/300][60/84], lr: 0.00000132 	 loss = 0.1087(0.2725)
2023/10/31 09:29:32 - INFO - root -   Epoch: [207/300][80/84], lr: 0.00000132 	 loss = 0.0120(0.2458)
2023/10/31 09:29:39 - INFO - root -   Epoch: [207/300] 	 loss = 0.2488
2023/10/31 09:29:39 - INFO - root -   train_accuracy = 0.9048
2023/10/31 09:30:01 - INFO - root -   Epoch: [208/300][0/84], lr: 0.00000133 	 loss = 0.0291(0.0291)
2023/10/31 09:31:22 - INFO - root -   Epoch: [208/300][20/84], lr: 0.00000133 	 loss = 0.0775(0.2120)
2023/10/31 09:32:22 - INFO - root -   Epoch: [208/300][40/84], lr: 0.00000133 	 loss = 0.0928(0.2382)
2023/10/31 09:33:44 - INFO - root -   Epoch: [208/300][60/84], lr: 0.00000133 	 loss = 0.2500(0.2558)
2023/10/31 09:34:31 - INFO - root -   Epoch: [208/300][80/84], lr: 0.00000133 	 loss = 0.0079(0.2943)
2023/10/31 09:34:33 - INFO - root -   Epoch: [208/300] 	 loss = 0.2853
2023/10/31 09:34:33 - INFO - root -   train_accuracy = 0.9167
2023/10/31 09:35:04 - INFO - root -   Epoch: [209/300][0/84], lr: 0.00000133 	 loss = 0.3638(0.3638)
2023/10/31 09:36:13 - INFO - root -   Epoch: [209/300][20/84], lr: 0.00000133 	 loss = 0.2460(0.2134)
2023/10/31 09:37:12 - INFO - root -   Epoch: [209/300][40/84], lr: 0.00000133 	 loss = 0.0118(0.2613)
2023/10/31 09:38:10 - INFO - root -   Epoch: [209/300][60/84], lr: 0.00000133 	 loss = 0.0292(0.2739)
2023/10/31 09:39:02 - INFO - root -   Epoch: [209/300][80/84], lr: 0.00000133 	 loss = 0.1131(0.3046)
2023/10/31 09:39:04 - INFO - root -   Epoch: [209/300] 	 loss = 0.2969
2023/10/31 09:40:00 - INFO - root -   precision = 0.9070
2023/10/31 09:40:00 - INFO - root -   eval_loss = 0.2476
2023/10/31 09:40:00 - INFO - root -   eval_acc = 0.9070
2023/10/31 09:40:01 - INFO - root -   train_accuracy = 0.8810
2023/10/31 09:40:30 - INFO - root -   Epoch: [210/300][0/84], lr: 0.00000134 	 loss = 0.0497(0.0497)
2023/10/31 09:41:37 - INFO - root -   Epoch: [210/300][20/84], lr: 0.00000134 	 loss = 0.2780(0.2517)
2023/10/31 09:42:36 - INFO - root -   Epoch: [210/300][40/84], lr: 0.00000134 	 loss = 0.0179(0.2329)
2023/10/31 09:43:57 - INFO - root -   Epoch: [210/300][60/84], lr: 0.00000134 	 loss = 0.2776(0.2834)
2023/10/31 09:44:39 - INFO - root -   Epoch: [210/300][80/84], lr: 0.00000134 	 loss = 0.0495(0.2776)
2023/10/31 09:44:42 - INFO - root -   Epoch: [210/300] 	 loss = 0.2688
2023/10/31 09:44:42 - INFO - root -   train_accuracy = 0.8988
2023/10/31 09:45:12 - INFO - root -   Epoch: [211/300][0/84], lr: 0.00000134 	 loss = 0.0391(0.0391)
2023/10/31 09:46:11 - INFO - root -   Epoch: [211/300][20/84], lr: 0.00000134 	 loss = 1.1193(0.2313)
2023/10/31 09:47:32 - INFO - root -   Epoch: [211/300][40/84], lr: 0.00000134 	 loss = 0.1459(0.2671)
2023/10/31 09:48:14 - INFO - root -   Epoch: [211/300][60/84], lr: 0.00000134 	 loss = 0.3076(0.2916)
2023/10/31 09:49:13 - INFO - root -   Epoch: [211/300][80/84], lr: 0.00000134 	 loss = 0.0983(0.2916)
2023/10/31 09:49:14 - INFO - root -   Epoch: [211/300] 	 loss = 0.2835
2023/10/31 09:49:14 - INFO - root -   train_accuracy = 0.8988
2023/10/31 09:49:59 - INFO - root -   Epoch: [212/300][0/84], lr: 0.00000135 	 loss = 0.1558(0.1558)
2023/10/31 09:50:48 - INFO - root -   Epoch: [212/300][20/84], lr: 0.00000135 	 loss = 0.0760(0.2067)
2023/10/31 09:51:45 - INFO - root -   Epoch: [212/300][40/84], lr: 0.00000135 	 loss = 0.1948(0.2372)
2023/10/31 09:53:07 - INFO - root -   Epoch: [212/300][60/84], lr: 0.00000135 	 loss = 0.0225(0.2368)
2023/10/31 09:53:42 - INFO - root -   Epoch: [212/300][80/84], lr: 0.00000135 	 loss = 0.0013(0.2295)
2023/10/31 09:53:46 - INFO - root -   Epoch: [212/300] 	 loss = 0.2297
2023/10/31 09:53:46 - INFO - root -   train_accuracy = 0.9048
2023/10/31 09:54:16 - INFO - root -   Epoch: [213/300][0/84], lr: 0.00000136 	 loss = 0.4201(0.4201)
2023/10/31 09:55:13 - INFO - root -   Epoch: [213/300][20/84], lr: 0.00000136 	 loss = 0.1829(0.2497)
2023/10/31 09:56:30 - INFO - root -   Epoch: [213/300][40/84], lr: 0.00000136 	 loss = 0.1198(0.2747)
2023/10/31 09:57:29 - INFO - root -   Epoch: [213/300][60/84], lr: 0.00000136 	 loss = 0.0113(0.2851)
2023/10/31 09:58:30 - INFO - root -   Epoch: [213/300][80/84], lr: 0.00000136 	 loss = 0.0186(0.2813)
2023/10/31 09:58:31 - INFO - root -   Epoch: [213/300] 	 loss = 0.2772
2023/10/31 09:58:31 - INFO - root -   train_accuracy = 0.8929
2023/10/31 09:58:53 - INFO - root -   Epoch: [214/300][0/84], lr: 0.00000136 	 loss = 0.0030(0.0030)
2023/10/31 09:59:58 - INFO - root -   Epoch: [214/300][20/84], lr: 0.00000136 	 loss = 0.0458(0.1990)
2023/10/31 10:00:58 - INFO - root -   Epoch: [214/300][40/84], lr: 0.00000136 	 loss = 0.0148(0.1597)
2023/10/31 10:02:33 - INFO - root -   Epoch: [214/300][60/84], lr: 0.00000136 	 loss = 0.0367(0.2530)
2023/10/31 10:03:09 - INFO - root -   Epoch: [214/300][80/84], lr: 0.00000136 	 loss = 0.0146(0.2496)
2023/10/31 10:03:12 - INFO - root -   Epoch: [214/300] 	 loss = 0.2528
2023/10/31 10:04:08 - INFO - root -   precision = 0.9070
2023/10/31 10:04:08 - INFO - root -   eval_loss = 0.2389
2023/10/31 10:04:08 - INFO - root -   eval_acc = 0.9070
2023/10/31 10:04:09 - INFO - root -   train_accuracy = 0.9048
2023/10/31 10:04:39 - INFO - root -   Epoch: [215/300][0/84], lr: 0.00000137 	 loss = 0.0697(0.0697)
2023/10/31 10:05:37 - INFO - root -   Epoch: [215/300][20/84], lr: 0.00000137 	 loss = 1.1241(0.1475)
2023/10/31 10:06:45 - INFO - root -   Epoch: [215/300][40/84], lr: 0.00000137 	 loss = 0.0307(0.2012)
2023/10/31 10:07:41 - INFO - root -   Epoch: [215/300][60/84], lr: 0.00000137 	 loss = 0.0211(0.2459)
2023/10/31 10:08:35 - INFO - root -   Epoch: [215/300][80/84], lr: 0.00000137 	 loss = 0.0495(0.2259)
2023/10/31 10:08:37 - INFO - root -   Epoch: [215/300] 	 loss = 0.2192
2023/10/31 10:08:37 - INFO - root -   train_accuracy = 0.9405
2023/10/31 10:09:06 - INFO - root -   Epoch: [216/300][0/84], lr: 0.00000137 	 loss = 0.0067(0.0067)
2023/10/31 10:10:11 - INFO - root -   Epoch: [216/300][20/84], lr: 0.00000137 	 loss = 1.4299(0.3997)
2023/10/31 10:11:19 - INFO - root -   Epoch: [216/300][40/84], lr: 0.00000137 	 loss = 0.0282(0.3489)
2023/10/31 10:12:41 - INFO - root -   Epoch: [216/300][60/84], lr: 0.00000137 	 loss = 0.0128(0.3157)
2023/10/31 10:13:20 - INFO - root -   Epoch: [216/300][80/84], lr: 0.00000137 	 loss = 0.1034(0.3257)
2023/10/31 10:13:21 - INFO - root -   Epoch: [216/300] 	 loss = 0.3161
2023/10/31 10:13:21 - INFO - root -   train_accuracy = 0.8988
2023/10/31 10:13:50 - INFO - root -   Epoch: [217/300][0/84], lr: 0.00000138 	 loss = 0.0206(0.0206)
2023/10/31 10:14:51 - INFO - root -   Epoch: [217/300][20/84], lr: 0.00000138 	 loss = 0.0392(0.1722)
2023/10/31 10:16:03 - INFO - root -   Epoch: [217/300][40/84], lr: 0.00000138 	 loss = 0.0407(0.2516)
2023/10/31 10:17:01 - INFO - root -   Epoch: [217/300][60/84], lr: 0.00000138 	 loss = 0.0204(0.2683)
2023/10/31 10:17:52 - INFO - root -   Epoch: [217/300][80/84], lr: 0.00000138 	 loss = 0.0999(0.2660)
2023/10/31 10:17:53 - INFO - root -   Epoch: [217/300] 	 loss = 0.2603
2023/10/31 10:17:53 - INFO - root -   train_accuracy = 0.9286
2023/10/31 10:18:16 - INFO - root -   Epoch: [218/300][0/84], lr: 0.00000138 	 loss = 0.0393(0.0393)
2023/10/31 10:19:13 - INFO - root -   Epoch: [218/300][20/84], lr: 0.00000138 	 loss = 0.3192(0.2197)
2023/10/31 10:20:15 - INFO - root -   Epoch: [218/300][40/84], lr: 0.00000138 	 loss = 0.0408(0.2816)
2023/10/31 10:21:27 - INFO - root -   Epoch: [218/300][60/84], lr: 0.00000138 	 loss = 0.2908(0.3174)
2023/10/31 10:22:14 - INFO - root -   Epoch: [218/300][80/84], lr: 0.00000138 	 loss = 0.0059(0.2971)
2023/10/31 10:22:16 - INFO - root -   Epoch: [218/300] 	 loss = 0.2900
2023/10/31 10:22:16 - INFO - root -   train_accuracy = 0.8810
2023/10/31 10:22:45 - INFO - root -   Epoch: [219/300][0/84], lr: 0.00000139 	 loss = 0.0940(0.0940)
2023/10/31 10:24:01 - INFO - root -   Epoch: [219/300][20/84], lr: 0.00000139 	 loss = 1.7507(0.4094)
2023/10/31 10:24:56 - INFO - root -   Epoch: [219/300][40/84], lr: 0.00000139 	 loss = 0.0762(0.3283)
2023/10/31 10:25:56 - INFO - root -   Epoch: [219/300][60/84], lr: 0.00000139 	 loss = 0.1440(0.3336)
2023/10/31 10:26:50 - INFO - root -   Epoch: [219/300][80/84], lr: 0.00000139 	 loss = 0.1036(0.2906)
2023/10/31 10:26:54 - INFO - root -   Epoch: [219/300] 	 loss = 0.2860
2023/10/31 10:27:50 - INFO - root -   precision = 0.9070
2023/10/31 10:27:50 - INFO - root -   eval_loss = 0.2380
2023/10/31 10:27:50 - INFO - root -   eval_acc = 0.9070
2023/10/31 10:27:51 - INFO - root -   train_accuracy = 0.9167
2023/10/31 10:28:13 - INFO - root -   Epoch: [220/300][0/84], lr: 0.00000140 	 loss = 0.0071(0.0071)
2023/10/31 10:29:21 - INFO - root -   Epoch: [220/300][20/84], lr: 0.00000140 	 loss = 0.2527(0.1839)
2023/10/31 10:30:12 - INFO - root -   Epoch: [220/300][40/84], lr: 0.00000140 	 loss = 0.0695(0.1796)
2023/10/31 10:31:28 - INFO - root -   Epoch: [220/300][60/84], lr: 0.00000140 	 loss = 0.1062(0.1798)
2023/10/31 10:32:15 - INFO - root -   Epoch: [220/300][80/84], lr: 0.00000140 	 loss = 0.0015(0.1980)
2023/10/31 10:32:17 - INFO - root -   Epoch: [220/300] 	 loss = 0.1927
2023/10/31 10:32:17 - INFO - root -   train_accuracy = 0.9345
2023/10/31 10:32:47 - INFO - root -   Epoch: [221/300][0/84], lr: 0.00000140 	 loss = 0.0088(0.0088)
2023/10/31 10:33:37 - INFO - root -   Epoch: [221/300][20/84], lr: 0.00000140 	 loss = 0.2873(0.2705)
2023/10/31 10:35:03 - INFO - root -   Epoch: [221/300][40/84], lr: 0.00000140 	 loss = 0.0467(0.2924)
2023/10/31 10:36:02 - INFO - root -   Epoch: [221/300][60/84], lr: 0.00000140 	 loss = 0.0599(0.2741)
2023/10/31 10:36:43 - INFO - root -   Epoch: [221/300][80/84], lr: 0.00000140 	 loss = 0.0039(0.2808)
2023/10/31 10:36:45 - INFO - root -   Epoch: [221/300] 	 loss = 0.2722
2023/10/31 10:36:45 - INFO - root -   train_accuracy = 0.9167
2023/10/31 10:37:15 - INFO - root -   Epoch: [222/300][0/84], lr: 0.00000141 	 loss = 0.0260(0.0260)
2023/10/31 10:38:28 - INFO - root -   Epoch: [222/300][20/84], lr: 0.00000141 	 loss = 0.3455(0.2142)
2023/10/31 10:39:38 - INFO - root -   Epoch: [222/300][40/84], lr: 0.00000141 	 loss = 0.0633(0.2595)
2023/10/31 10:40:40 - INFO - root -   Epoch: [222/300][60/84], lr: 0.00000141 	 loss = 0.0210(0.2430)
2023/10/31 10:41:18 - INFO - root -   Epoch: [222/300][80/84], lr: 0.00000141 	 loss = 0.0240(0.2302)
2023/10/31 10:41:21 - INFO - root -   Epoch: [222/300] 	 loss = 0.2249
2023/10/31 10:41:21 - INFO - root -   train_accuracy = 0.9286
2023/10/31 10:41:50 - INFO - root -   Epoch: [223/300][0/84], lr: 0.00000141 	 loss = 0.0623(0.0623)
2023/10/31 10:42:57 - INFO - root -   Epoch: [223/300][20/84], lr: 0.00000141 	 loss = 0.3990(0.1363)
2023/10/31 10:44:00 - INFO - root -   Epoch: [223/300][40/84], lr: 0.00000141 	 loss = 0.0395(0.1668)
2023/10/31 10:45:09 - INFO - root -   Epoch: [223/300][60/84], lr: 0.00000141 	 loss = 0.0472(0.1677)
2023/10/31 10:45:59 - INFO - root -   Epoch: [223/300][80/84], lr: 0.00000141 	 loss = 0.0588(0.1601)
2023/10/31 10:46:01 - INFO - root -   Epoch: [223/300] 	 loss = 0.1555
2023/10/31 10:46:01 - INFO - root -   train_accuracy = 0.9464
2023/10/31 10:46:31 - INFO - root -   Epoch: [224/300][0/84], lr: 0.00000142 	 loss = 0.0174(0.0174)
2023/10/31 10:47:38 - INFO - root -   Epoch: [224/300][20/84], lr: 0.00000142 	 loss = 1.4834(0.2372)
2023/10/31 10:48:43 - INFO - root -   Epoch: [224/300][40/84], lr: 0.00000142 	 loss = 0.0078(0.2363)
2023/10/31 10:49:43 - INFO - root -   Epoch: [224/300][60/84], lr: 0.00000142 	 loss = 0.0065(0.2611)
2023/10/31 10:50:37 - INFO - root -   Epoch: [224/300][80/84], lr: 0.00000142 	 loss = 0.0016(0.2536)
2023/10/31 10:50:39 - INFO - root -   Epoch: [224/300] 	 loss = 0.2462
2023/10/31 10:51:38 - INFO - root -   precision = 0.8837
2023/10/31 10:51:38 - INFO - root -   eval_loss = 0.3355
2023/10/31 10:51:38 - INFO - root -   eval_acc = 0.8837
2023/10/31 10:51:39 - INFO - root -   train_accuracy = 0.8988
2023/10/31 10:52:09 - INFO - root -   Epoch: [225/300][0/84], lr: 0.00000143 	 loss = 0.1876(0.1876)
2023/10/31 10:53:07 - INFO - root -   Epoch: [225/300][20/84], lr: 0.00000143 	 loss = 0.2308(0.3038)
2023/10/31 10:54:13 - INFO - root -   Epoch: [225/300][40/84], lr: 0.00000143 	 loss = 0.0517(0.2731)
2023/10/31 10:55:25 - INFO - root -   Epoch: [225/300][60/84], lr: 0.00000143 	 loss = 0.1095(0.2997)
2023/10/31 10:56:16 - INFO - root -   Epoch: [225/300][80/84], lr: 0.00000143 	 loss = 0.0196(0.2855)
2023/10/31 10:56:17 - INFO - root -   Epoch: [225/300] 	 loss = 0.2859
2023/10/31 10:56:17 - INFO - root -   train_accuracy = 0.8869
2023/10/31 10:56:39 - INFO - root -   Epoch: [226/300][0/84], lr: 0.00000143 	 loss = 0.0059(0.0059)
2023/10/31 10:57:40 - INFO - root -   Epoch: [226/300][20/84], lr: 0.00000143 	 loss = 0.9557(0.1878)
2023/10/31 10:58:55 - INFO - root -   Epoch: [226/300][40/84], lr: 0.00000143 	 loss = 0.0718(0.2064)
2023/10/31 11:00:00 - INFO - root -   Epoch: [226/300][60/84], lr: 0.00000143 	 loss = 0.0079(0.1748)
2023/10/31 11:00:49 - INFO - root -   Epoch: [226/300][80/84], lr: 0.00000143 	 loss = 0.0001(0.1931)
2023/10/31 11:00:51 - INFO - root -   Epoch: [226/300] 	 loss = 0.1938
2023/10/31 11:00:51 - INFO - root -   train_accuracy = 0.9226
2023/10/31 11:01:12 - INFO - root -   Epoch: [227/300][0/84], lr: 0.00000144 	 loss = 0.0074(0.0074)
2023/10/31 11:02:46 - INFO - root -   Epoch: [227/300][20/84], lr: 0.00000144 	 loss = 1.3903(0.3463)
2023/10/31 11:03:35 - INFO - root -   Epoch: [227/300][40/84], lr: 0.00000144 	 loss = 0.2566(0.2516)
2023/10/31 11:04:47 - INFO - root -   Epoch: [227/300][60/84], lr: 0.00000144 	 loss = 0.0767(0.2555)
2023/10/31 11:05:23 - INFO - root -   Epoch: [227/300][80/84], lr: 0.00000144 	 loss = 0.0019(0.2633)
2023/10/31 11:05:27 - INFO - root -   Epoch: [227/300] 	 loss = 0.2554
2023/10/31 11:05:27 - INFO - root -   train_accuracy = 0.9048
2023/10/31 11:05:48 - INFO - root -   Epoch: [228/300][0/84], lr: 0.00000144 	 loss = 0.0090(0.0090)
2023/10/31 11:06:56 - INFO - root -   Epoch: [228/300][20/84], lr: 0.00000144 	 loss = 0.0824(0.1538)
2023/10/31 11:08:07 - INFO - root -   Epoch: [228/300][40/84], lr: 0.00000144 	 loss = 0.0565(0.1607)
2023/10/31 11:09:22 - INFO - root -   Epoch: [228/300][60/84], lr: 0.00000144 	 loss = 0.1202(0.2050)
2023/10/31 11:10:09 - INFO - root -   Epoch: [228/300][80/84], lr: 0.00000144 	 loss = 0.0014(0.1932)
2023/10/31 11:10:10 - INFO - root -   Epoch: [228/300] 	 loss = 0.1867
2023/10/31 11:10:10 - INFO - root -   train_accuracy = 0.9524
2023/10/31 11:10:39 - INFO - root -   Epoch: [229/300][0/84], lr: 0.00000145 	 loss = 0.0408(0.0408)
2023/10/31 11:11:45 - INFO - root -   Epoch: [229/300][20/84], lr: 0.00000145 	 loss = 0.0300(0.3741)
2023/10/31 11:12:50 - INFO - root -   Epoch: [229/300][40/84], lr: 0.00000145 	 loss = 0.0060(0.2815)
2023/10/31 11:13:49 - INFO - root -   Epoch: [229/300][60/84], lr: 0.00000145 	 loss = 0.1077(0.2610)
2023/10/31 11:14:45 - INFO - root -   Epoch: [229/300][80/84], lr: 0.00000145 	 loss = 0.0004(0.2451)
2023/10/31 11:14:46 - INFO - root -   Epoch: [229/300] 	 loss = 0.2377
2023/10/31 11:15:42 - INFO - root -   precision = 0.9070
2023/10/31 11:15:42 - INFO - root -   eval_loss = 0.2897
2023/10/31 11:15:42 - INFO - root -   eval_acc = 0.9070
2023/10/31 11:15:43 - INFO - root -   train_accuracy = 0.9345
2023/10/31 11:16:04 - INFO - root -   Epoch: [230/300][0/84], lr: 0.00000146 	 loss = 0.0011(0.0011)
2023/10/31 11:17:19 - INFO - root -   Epoch: [230/300][20/84], lr: 0.00000146 	 loss = 1.3261(0.1752)
2023/10/31 11:18:20 - INFO - root -   Epoch: [230/300][40/84], lr: 0.00000146 	 loss = 0.0628(0.2041)
2023/10/31 11:19:27 - INFO - root -   Epoch: [230/300][60/84], lr: 0.00000146 	 loss = 0.0088(0.2358)
2023/10/31 11:20:23 - INFO - root -   Epoch: [230/300][80/84], lr: 0.00000146 	 loss = 0.0032(0.2197)
2023/10/31 11:20:26 - INFO - root -   Epoch: [230/300] 	 loss = 0.2136
2023/10/31 11:20:26 - INFO - root -   train_accuracy = 0.9286
2023/10/31 11:20:48 - INFO - root -   Epoch: [231/300][0/84], lr: 0.00000146 	 loss = 0.0261(0.0261)
2023/10/31 11:21:57 - INFO - root -   Epoch: [231/300][20/84], lr: 0.00000146 	 loss = 2.1267(0.2309)
2023/10/31 11:23:23 - INFO - root -   Epoch: [231/300][40/84], lr: 0.00000146 	 loss = 0.0432(0.1764)
2023/10/31 11:24:21 - INFO - root -   Epoch: [231/300][60/84], lr: 0.00000146 	 loss = 0.0032(0.2330)
2023/10/31 11:25:07 - INFO - root -   Epoch: [231/300][80/84], lr: 0.00000146 	 loss = 0.0018(0.2533)
2023/10/31 11:25:09 - INFO - root -   Epoch: [231/300] 	 loss = 0.2525
2023/10/31 11:25:09 - INFO - root -   train_accuracy = 0.9167
2023/10/31 11:25:31 - INFO - root -   Epoch: [232/300][0/84], lr: 0.00000147 	 loss = 0.0210(0.0210)
2023/10/31 11:26:39 - INFO - root -   Epoch: [232/300][20/84], lr: 0.00000147 	 loss = 0.5614(0.3770)
2023/10/31 11:28:02 - INFO - root -   Epoch: [232/300][40/84], lr: 0.00000147 	 loss = 0.1164(0.3033)
2023/10/31 11:28:49 - INFO - root -   Epoch: [232/300][60/84], lr: 0.00000147 	 loss = 0.0438(0.2771)
2023/10/31 11:29:41 - INFO - root -   Epoch: [232/300][80/84], lr: 0.00000147 	 loss = 0.0053(0.2812)
2023/10/31 11:29:42 - INFO - root -   Epoch: [232/300] 	 loss = 0.2764
2023/10/31 11:29:42 - INFO - root -   train_accuracy = 0.8988
2023/10/31 11:30:20 - INFO - root -   Epoch: [233/300][0/84], lr: 0.00000147 	 loss = 0.1269(0.1269)
2023/10/31 11:31:20 - INFO - root -   Epoch: [233/300][20/84], lr: 0.00000147 	 loss = 0.0659(0.2496)
2023/10/31 11:32:27 - INFO - root -   Epoch: [233/300][40/84], lr: 0.00000147 	 loss = 0.0913(0.2062)
2023/10/31 11:33:09 - INFO - root -   Epoch: [233/300][60/84], lr: 0.00000147 	 loss = 0.0173(0.1977)
2023/10/31 11:34:17 - INFO - root -   Epoch: [233/300][80/84], lr: 0.00000147 	 loss = 0.0901(0.2254)
2023/10/31 11:34:18 - INFO - root -   Epoch: [233/300] 	 loss = 0.2205
2023/10/31 11:34:18 - INFO - root -   train_accuracy = 0.9286
2023/10/31 11:34:40 - INFO - root -   Epoch: [234/300][0/84], lr: 0.00000148 	 loss = 0.0124(0.0124)
2023/10/31 11:35:45 - INFO - root -   Epoch: [234/300][20/84], lr: 0.00000148 	 loss = 0.0702(0.3885)
2023/10/31 11:37:08 - INFO - root -   Epoch: [234/300][40/84], lr: 0.00000148 	 loss = 0.1694(0.2828)
2023/10/31 11:38:06 - INFO - root -   Epoch: [234/300][60/84], lr: 0.00000148 	 loss = 0.0130(0.2557)
2023/10/31 11:38:49 - INFO - root -   Epoch: [234/300][80/84], lr: 0.00000148 	 loss = 0.0048(0.2587)
2023/10/31 11:38:51 - INFO - root -   Epoch: [234/300] 	 loss = 0.2515
2023/10/31 11:39:47 - INFO - root -   precision = 0.9070
2023/10/31 11:39:47 - INFO - root -   eval_loss = 0.1999
2023/10/31 11:39:47 - INFO - root -   eval_acc = 0.9070
2023/10/31 11:39:48 - INFO - root -   train_accuracy = 0.9107
2023/10/31 11:40:10 - INFO - root -   Epoch: [235/300][0/84], lr: 0.00000148 	 loss = 0.0768(0.0768)
2023/10/31 11:41:16 - INFO - root -   Epoch: [235/300][20/84], lr: 0.00000148 	 loss = 0.1165(0.1100)
2023/10/31 11:42:32 - INFO - root -   Epoch: [235/300][40/84], lr: 0.00000148 	 loss = 0.0681(0.1415)
2023/10/31 11:43:45 - INFO - root -   Epoch: [235/300][60/84], lr: 0.00000148 	 loss = 0.0021(0.1785)
2023/10/31 11:44:29 - INFO - root -   Epoch: [235/300][80/84], lr: 0.00000148 	 loss = 0.0040(0.1979)
2023/10/31 11:44:31 - INFO - root -   Epoch: [235/300] 	 loss = 0.1942
2023/10/31 11:44:31 - INFO - root -   train_accuracy = 0.9583
2023/10/31 11:45:12 - INFO - root -   Epoch: [236/300][0/84], lr: 0.00000149 	 loss = 0.1884(0.1884)
2023/10/31 11:46:02 - INFO - root -   Epoch: [236/300][20/84], lr: 0.00000149 	 loss = 0.0568(0.2401)
2023/10/31 11:47:10 - INFO - root -   Epoch: [236/300][40/84], lr: 0.00000149 	 loss = 0.0196(0.3081)
2023/10/31 11:48:24 - INFO - root -   Epoch: [236/300][60/84], lr: 0.00000149 	 loss = 0.0264(0.3460)
2023/10/31 11:49:11 - INFO - root -   Epoch: [236/300][80/84], lr: 0.00000149 	 loss = 0.1040(0.3290)
2023/10/31 11:49:13 - INFO - root -   Epoch: [236/300] 	 loss = 0.3193
2023/10/31 11:49:13 - INFO - root -   train_accuracy = 0.8988
2023/10/31 11:49:43 - INFO - root -   Epoch: [237/300][0/84], lr: 0.00000150 	 loss = 0.0915(0.0915)
2023/10/31 11:50:42 - INFO - root -   Epoch: [237/300][20/84], lr: 0.00000150 	 loss = 0.0864(0.2310)
2023/10/31 11:52:06 - INFO - root -   Epoch: [237/300][40/84], lr: 0.00000150 	 loss = 0.2342(0.2788)
2023/10/31 11:53:03 - INFO - root -   Epoch: [237/300][60/84], lr: 0.00000150 	 loss = 0.0193(0.3087)
2023/10/31 11:53:54 - INFO - root -   Epoch: [237/300][80/84], lr: 0.00000150 	 loss = 0.0016(0.2828)
2023/10/31 11:53:55 - INFO - root -   Epoch: [237/300] 	 loss = 0.2750
2023/10/31 11:53:55 - INFO - root -   train_accuracy = 0.8988
2023/10/31 11:54:17 - INFO - root -   Epoch: [238/300][0/84], lr: 0.00000150 	 loss = 0.0073(0.0073)
2023/10/31 11:55:30 - INFO - root -   Epoch: [238/300][20/84], lr: 0.00000150 	 loss = 0.0562(0.2823)
2023/10/31 11:56:35 - INFO - root -   Epoch: [238/300][40/84], lr: 0.00000150 	 loss = 0.0495(0.2743)
2023/10/31 11:57:49 - INFO - root -   Epoch: [238/300][60/84], lr: 0.00000150 	 loss = 0.0711(0.2246)
2023/10/31 11:58:30 - INFO - root -   Epoch: [238/300][80/84], lr: 0.00000150 	 loss = 0.0038(0.2284)
2023/10/31 11:58:36 - INFO - root -   Epoch: [238/300] 	 loss = 0.2214
2023/10/31 11:58:36 - INFO - root -   train_accuracy = 0.9226
2023/10/31 11:59:05 - INFO - root -   Epoch: [239/300][0/84], lr: 0.00000151 	 loss = 0.0322(0.0322)
2023/10/31 11:59:58 - INFO - root -   Epoch: [239/300][20/84], lr: 0.00000151 	 loss = 0.0991(0.1786)
2023/10/31 12:01:21 - INFO - root -   Epoch: [239/300][40/84], lr: 0.00000151 	 loss = 0.0459(0.2046)
2023/10/31 12:02:34 - INFO - root -   Epoch: [239/300][60/84], lr: 0.00000151 	 loss = 0.1123(0.1985)
2023/10/31 12:03:06 - INFO - root -   Epoch: [239/300][80/84], lr: 0.00000151 	 loss = 0.0324(0.2423)
2023/10/31 12:03:11 - INFO - root -   Epoch: [239/300] 	 loss = 0.2391
2023/10/31 12:04:08 - INFO - root -   precision = 0.9302
2023/10/31 12:04:08 - INFO - root -   eval_loss = 0.2274
2023/10/31 12:04:08 - INFO - root -   eval_acc = 0.9302
2023/10/31 12:04:09 - INFO - root -   train_accuracy = 0.9167
2023/10/31 12:04:46 - INFO - root -   Epoch: [240/300][0/84], lr: 0.00000151 	 loss = 0.0111(0.0111)
2023/10/31 12:05:43 - INFO - root -   Epoch: [240/300][20/84], lr: 0.00000151 	 loss = 0.0331(0.3431)
2023/10/31 12:07:09 - INFO - root -   Epoch: [240/300][40/84], lr: 0.00000151 	 loss = 0.0517(0.3156)
2023/10/31 12:08:04 - INFO - root -   Epoch: [240/300][60/84], lr: 0.00000151 	 loss = 0.0534(0.2586)
2023/10/31 12:08:44 - INFO - root -   Epoch: [240/300][80/84], lr: 0.00000151 	 loss = 0.0023(0.2530)
2023/10/31 12:08:46 - INFO - root -   Epoch: [240/300] 	 loss = 0.2446
2023/10/31 12:08:46 - INFO - root -   train_accuracy = 0.9286
2023/10/31 12:09:07 - INFO - root -   Epoch: [241/300][0/84], lr: 0.00000152 	 loss = 0.0158(0.0158)
2023/10/31 12:10:05 - INFO - root -   Epoch: [241/300][20/84], lr: 0.00000152 	 loss = 0.0579(0.2464)
2023/10/31 12:11:27 - INFO - root -   Epoch: [241/300][40/84], lr: 0.00000152 	 loss = 0.0354(0.2130)
2023/10/31 12:12:18 - INFO - root -   Epoch: [241/300][60/84], lr: 0.00000152 	 loss = 0.0365(0.2314)
2023/10/31 12:13:21 - INFO - root -   Epoch: [241/300][80/84], lr: 0.00000152 	 loss = 0.0011(0.2393)
2023/10/31 12:13:22 - INFO - root -   Epoch: [241/300] 	 loss = 0.2377
2023/10/31 12:13:22 - INFO - root -   train_accuracy = 0.9107
2023/10/31 12:13:45 - INFO - root -   Epoch: [242/300][0/84], lr: 0.00000153 	 loss = 0.0102(0.0102)
2023/10/31 12:14:51 - INFO - root -   Epoch: [242/300][20/84], lr: 0.00000153 	 loss = 0.0238(0.2390)
2023/10/31 12:16:05 - INFO - root -   Epoch: [242/300][40/84], lr: 0.00000153 	 loss = 0.0341(0.1923)
2023/10/31 12:17:01 - INFO - root -   Epoch: [242/300][60/84], lr: 0.00000153 	 loss = 0.0123(0.1952)
2023/10/31 12:17:51 - INFO - root -   Epoch: [242/300][80/84], lr: 0.00000153 	 loss = 0.1458(0.2027)
2023/10/31 12:17:52 - INFO - root -   Epoch: [242/300] 	 loss = 0.2049
2023/10/31 12:17:52 - INFO - root -   train_accuracy = 0.9286
2023/10/31 12:18:15 - INFO - root -   Epoch: [243/300][0/84], lr: 0.00000153 	 loss = 0.0062(0.0062)
2023/10/31 12:19:11 - INFO - root -   Epoch: [243/300][20/84], lr: 0.00000153 	 loss = 1.0184(0.2263)
2023/10/31 12:20:28 - INFO - root -   Epoch: [243/300][40/84], lr: 0.00000153 	 loss = 0.0381(0.2181)
2023/10/31 12:21:29 - INFO - root -   Epoch: [243/300][60/84], lr: 0.00000153 	 loss = 0.0080(0.2144)
2023/10/31 12:22:26 - INFO - root -   Epoch: [243/300][80/84], lr: 0.00000153 	 loss = 0.0326(0.1861)
2023/10/31 12:22:27 - INFO - root -   Epoch: [243/300] 	 loss = 0.1822
2023/10/31 12:22:27 - INFO - root -   train_accuracy = 0.9464
2023/10/31 12:22:49 - INFO - root -   Epoch: [244/300][0/84], lr: 0.00000154 	 loss = 0.0132(0.0132)
2023/10/31 12:23:53 - INFO - root -   Epoch: [244/300][20/84], lr: 0.00000154 	 loss = 0.0086(0.2208)
2023/10/31 12:24:59 - INFO - root -   Epoch: [244/300][40/84], lr: 0.00000154 	 loss = 0.0084(0.2989)
2023/10/31 12:26:02 - INFO - root -   Epoch: [244/300][60/84], lr: 0.00000154 	 loss = 0.0098(0.2461)
2023/10/31 12:26:59 - INFO - root -   Epoch: [244/300][80/84], lr: 0.00000154 	 loss = 0.0015(0.2587)
2023/10/31 12:27:00 - INFO - root -   Epoch: [244/300] 	 loss = 0.2583
2023/10/31 12:27:56 - INFO - root -   precision = 0.9302
2023/10/31 12:27:56 - INFO - root -   eval_loss = 0.1834
2023/10/31 12:27:56 - INFO - root -   eval_acc = 0.9302
2023/10/31 12:27:57 - INFO - root -   train_accuracy = 0.9167
2023/10/31 12:28:26 - INFO - root -   Epoch: [245/300][0/84], lr: 0.00000154 	 loss = 0.1084(0.1084)
2023/10/31 12:29:25 - INFO - root -   Epoch: [245/300][20/84], lr: 0.00000154 	 loss = 0.1761(0.3274)
2023/10/31 12:30:46 - INFO - root -   Epoch: [245/300][40/84], lr: 0.00000154 	 loss = 0.0607(0.2611)
2023/10/31 12:31:43 - INFO - root -   Epoch: [245/300][60/84], lr: 0.00000154 	 loss = 0.0525(0.2392)
2023/10/31 12:32:31 - INFO - root -   Epoch: [245/300][80/84], lr: 0.00000154 	 loss = 0.0128(0.2558)
2023/10/31 12:32:32 - INFO - root -   Epoch: [245/300] 	 loss = 0.2486
2023/10/31 12:32:32 - INFO - root -   train_accuracy = 0.9048
2023/10/31 12:33:02 - INFO - root -   Epoch: [246/300][0/84], lr: 0.00000155 	 loss = 0.0699(0.0699)
2023/10/31 12:34:09 - INFO - root -   Epoch: [246/300][20/84], lr: 0.00000155 	 loss = 1.8971(0.3607)
2023/10/31 12:35:06 - INFO - root -   Epoch: [246/300][40/84], lr: 0.00000155 	 loss = 0.0349(0.2950)
2023/10/31 12:36:19 - INFO - root -   Epoch: [246/300][60/84], lr: 0.00000155 	 loss = 0.1378(0.2811)
2023/10/31 12:37:06 - INFO - root -   Epoch: [246/300][80/84], lr: 0.00000155 	 loss = 0.1665(0.2668)
2023/10/31 12:37:11 - INFO - root -   Epoch: [246/300] 	 loss = 0.2591
2023/10/31 12:37:11 - INFO - root -   train_accuracy = 0.9167
2023/10/31 12:37:40 - INFO - root -   Epoch: [247/300][0/84], lr: 0.00000156 	 loss = 0.1597(0.1597)
2023/10/31 12:38:41 - INFO - root -   Epoch: [247/300][20/84], lr: 0.00000156 	 loss = 0.0728(0.2395)
2023/10/31 12:40:01 - INFO - root -   Epoch: [247/300][40/84], lr: 0.00000156 	 loss = 0.0642(0.2444)
2023/10/31 12:40:56 - INFO - root -   Epoch: [247/300][60/84], lr: 0.00000156 	 loss = 0.0079(0.2445)
2023/10/31 12:41:45 - INFO - root -   Epoch: [247/300][80/84], lr: 0.00000156 	 loss = 0.0649(0.2172)
2023/10/31 12:41:47 - INFO - root -   Epoch: [247/300] 	 loss = 0.2100
2023/10/31 12:41:47 - INFO - root -   train_accuracy = 0.9345
2023/10/31 12:42:09 - INFO - root -   Epoch: [248/300][0/84], lr: 0.00000156 	 loss = 0.0058(0.0058)
2023/10/31 12:43:24 - INFO - root -   Epoch: [248/300][20/84], lr: 0.00000156 	 loss = 1.0731(0.3395)
2023/10/31 12:44:47 - INFO - root -   Epoch: [248/300][40/84], lr: 0.00000156 	 loss = 0.1235(0.3272)
2023/10/31 12:45:42 - INFO - root -   Epoch: [248/300][60/84], lr: 0.00000156 	 loss = 0.0626(0.2949)
2023/10/31 12:46:40 - INFO - root -   Epoch: [248/300][80/84], lr: 0.00000156 	 loss = 0.0021(0.2845)
2023/10/31 12:46:41 - INFO - root -   Epoch: [248/300] 	 loss = 0.2765
2023/10/31 12:46:41 - INFO - root -   train_accuracy = 0.9048
2023/10/31 12:47:03 - INFO - root -   Epoch: [249/300][0/84], lr: 0.00000157 	 loss = 0.0011(0.0011)
2023/10/31 12:48:08 - INFO - root -   Epoch: [249/300][20/84], lr: 0.00000157 	 loss = 0.8929(0.2706)
2023/10/31 12:49:27 - INFO - root -   Epoch: [249/300][40/84], lr: 0.00000157 	 loss = 0.0576(0.2101)
2023/10/31 12:50:17 - INFO - root -   Epoch: [249/300][60/84], lr: 0.00000157 	 loss = 0.0075(0.1856)
2023/10/31 12:51:07 - INFO - root -   Epoch: [249/300][80/84], lr: 0.00000157 	 loss = 0.0258(0.1795)
2023/10/31 12:51:09 - INFO - root -   Epoch: [249/300] 	 loss = 0.1742
2023/10/31 12:52:05 - INFO - root -   precision = 0.8837
2023/10/31 12:52:05 - INFO - root -   eval_loss = 0.2736
2023/10/31 12:52:05 - INFO - root -   eval_acc = 0.8837
2023/10/31 12:52:06 - INFO - root -   train_accuracy = 0.9524
2023/10/31 12:52:35 - INFO - root -   Epoch: [250/300][0/84], lr: 0.00000157 	 loss = 0.1794(0.1794)
2023/10/31 12:53:40 - INFO - root -   Epoch: [250/300][20/84], lr: 0.00000157 	 loss = 1.5360(0.2058)
2023/10/31 12:54:42 - INFO - root -   Epoch: [250/300][40/84], lr: 0.00000157 	 loss = 0.0370(0.2364)
2023/10/31 12:55:50 - INFO - root -   Epoch: [250/300][60/84], lr: 0.00000157 	 loss = 0.0136(0.2392)
2023/10/31 12:56:43 - INFO - root -   Epoch: [250/300][80/84], lr: 0.00000157 	 loss = 0.0899(0.2155)
2023/10/31 12:56:45 - INFO - root -   Epoch: [250/300] 	 loss = 0.2107
2023/10/31 12:56:45 - INFO - root -   train_accuracy = 0.9286
2023/10/31 12:57:31 - INFO - root -   Epoch: [251/300][0/84], lr: 0.00000158 	 loss = 0.0534(0.0534)
2023/10/31 12:58:35 - INFO - root -   Epoch: [251/300][20/84], lr: 0.00000158 	 loss = 0.1620(0.1931)
2023/10/31 12:59:44 - INFO - root -   Epoch: [251/300][40/84], lr: 0.00000158 	 loss = 0.0094(0.2188)
2023/10/31 13:00:41 - INFO - root -   Epoch: [251/300][60/84], lr: 0.00000158 	 loss = 0.1328(0.2740)
2023/10/31 13:01:35 - INFO - root -   Epoch: [251/300][80/84], lr: 0.00000158 	 loss = 0.2797(0.2974)
2023/10/31 13:01:36 - INFO - root -   Epoch: [251/300] 	 loss = 0.2876
2023/10/31 13:01:36 - INFO - root -   train_accuracy = 0.8988
2023/10/31 13:02:06 - INFO - root -   Epoch: [252/300][0/84], lr: 0.00000159 	 loss = 0.0949(0.0949)
2023/10/31 13:03:03 - INFO - root -   Epoch: [252/300][20/84], lr: 0.00000159 	 loss = 0.0219(0.1463)
2023/10/31 13:04:06 - INFO - root -   Epoch: [252/300][40/84], lr: 0.00000159 	 loss = 0.0030(0.1600)
2023/10/31 13:05:13 - INFO - root -   Epoch: [252/300][60/84], lr: 0.00000159 	 loss = 0.0065(0.1766)
2023/10/31 13:06:07 - INFO - root -   Epoch: [252/300][80/84], lr: 0.00000159 	 loss = 0.0015(0.2046)
2023/10/31 13:06:08 - INFO - root -   Epoch: [252/300] 	 loss = 0.1976
2023/10/31 13:06:08 - INFO - root -   train_accuracy = 0.9167
2023/10/31 13:06:30 - INFO - root -   Epoch: [253/300][0/84], lr: 0.00000159 	 loss = 0.0135(0.0135)
2023/10/31 13:07:36 - INFO - root -   Epoch: [253/300][20/84], lr: 0.00000159 	 loss = 1.4112(0.2940)
2023/10/31 13:08:26 - INFO - root -   Epoch: [253/300][40/84], lr: 0.00000159 	 loss = 0.0405(0.2499)
2023/10/31 13:09:43 - INFO - root -   Epoch: [253/300][60/84], lr: 0.00000159 	 loss = 0.1937(0.2435)
2023/10/31 13:10:30 - INFO - root -   Epoch: [253/300][80/84], lr: 0.00000159 	 loss = 0.0257(0.2259)
2023/10/31 13:10:32 - INFO - root -   Epoch: [253/300] 	 loss = 0.2205
2023/10/31 13:10:32 - INFO - root -   train_accuracy = 0.9226
2023/10/31 13:11:01 - INFO - root -   Epoch: [254/300][0/84], lr: 0.00000160 	 loss = 0.1045(0.1045)
2023/10/31 13:12:00 - INFO - root -   Epoch: [254/300][20/84], lr: 0.00000160 	 loss = 0.4646(0.1977)
2023/10/31 13:13:17 - INFO - root -   Epoch: [254/300][40/84], lr: 0.00000160 	 loss = 0.1070(0.2938)
2023/10/31 13:13:59 - INFO - root -   Epoch: [254/300][60/84], lr: 0.00000160 	 loss = 0.0244(0.2898)
2023/10/31 13:15:00 - INFO - root -   Epoch: [254/300][80/84], lr: 0.00000160 	 loss = 0.0416(0.2640)
2023/10/31 13:15:02 - INFO - root -   Epoch: [254/300] 	 loss = 0.2565
2023/10/31 13:15:58 - INFO - root -   precision = 0.9302
2023/10/31 13:15:58 - INFO - root -   eval_loss = 0.2207
2023/10/31 13:15:58 - INFO - root -   eval_acc = 0.9302
2023/10/31 13:15:59 - INFO - root -   train_accuracy = 0.9226
2023/10/31 13:16:28 - INFO - root -   Epoch: [255/300][0/84], lr: 0.00000160 	 loss = 0.0467(0.0467)
2023/10/31 13:17:20 - INFO - root -   Epoch: [255/300][20/84], lr: 0.00000160 	 loss = 0.2171(0.2275)
2023/10/31 13:18:45 - INFO - root -   Epoch: [255/300][40/84], lr: 0.00000160 	 loss = 0.0403(0.2330)
2023/10/31 13:19:50 - INFO - root -   Epoch: [255/300][60/84], lr: 0.00000160 	 loss = 0.0032(0.2135)
2023/10/31 13:20:46 - INFO - root -   Epoch: [255/300][80/84], lr: 0.00000160 	 loss = 0.0070(0.1984)
2023/10/31 13:20:47 - INFO - root -   Epoch: [255/300] 	 loss = 0.1977
2023/10/31 13:20:47 - INFO - root -   train_accuracy = 0.9405
2023/10/31 13:21:16 - INFO - root -   Epoch: [256/300][0/84], lr: 0.00000161 	 loss = 0.0165(0.0165)
2023/10/31 13:22:21 - INFO - root -   Epoch: [256/300][20/84], lr: 0.00000161 	 loss = 0.0220(0.1781)
2023/10/31 13:23:23 - INFO - root -   Epoch: [256/300][40/84], lr: 0.00000161 	 loss = 0.0091(0.2453)
2023/10/31 13:24:38 - INFO - root -   Epoch: [256/300][60/84], lr: 0.00000161 	 loss = 0.0545(0.2410)
2023/10/31 13:25:23 - INFO - root -   Epoch: [256/300][80/84], lr: 0.00000161 	 loss = 0.0508(0.2151)
2023/10/31 13:25:27 - INFO - root -   Epoch: [256/300] 	 loss = 0.2095
2023/10/31 13:25:27 - INFO - root -   train_accuracy = 0.9286
2023/10/31 13:25:59 - INFO - root -   Epoch: [257/300][0/84], lr: 0.00000161 	 loss = 0.0195(0.0195)
2023/10/31 13:26:47 - INFO - root -   Epoch: [257/300][20/84], lr: 0.00000161 	 loss = 1.1808(0.3004)
2023/10/31 13:27:59 - INFO - root -   Epoch: [257/300][40/84], lr: 0.00000161 	 loss = 0.3408(0.2958)
2023/10/31 13:29:16 - INFO - root -   Epoch: [257/300][60/84], lr: 0.00000161 	 loss = 0.1102(0.2722)
2023/10/31 13:30:01 - INFO - root -   Epoch: [257/300][80/84], lr: 0.00000161 	 loss = 0.0002(0.2440)
2023/10/31 13:30:07 - INFO - root -   Epoch: [257/300] 	 loss = 0.2382
2023/10/31 13:30:07 - INFO - root -   train_accuracy = 0.9107
2023/10/31 13:30:28 - INFO - root -   Epoch: [258/300][0/84], lr: 0.00000162 	 loss = 0.0002(0.0002)
2023/10/31 13:31:37 - INFO - root -   Epoch: [258/300][20/84], lr: 0.00000162 	 loss = 0.0119(0.2268)
2023/10/31 13:32:52 - INFO - root -   Epoch: [258/300][40/84], lr: 0.00000162 	 loss = 0.2191(0.3034)
2023/10/31 13:34:21 - INFO - root -   Epoch: [258/300][60/84], lr: 0.00000162 	 loss = 0.1784(0.3168)
2023/10/31 13:34:56 - INFO - root -   Epoch: [258/300][80/84], lr: 0.00000162 	 loss = 0.0031(0.3015)
2023/10/31 13:34:57 - INFO - root -   Epoch: [258/300] 	 loss = 0.2946
2023/10/31 13:34:57 - INFO - root -   train_accuracy = 0.9107
2023/10/31 13:35:19 - INFO - root -   Epoch: [259/300][0/84], lr: 0.00000163 	 loss = 0.0013(0.0013)
2023/10/31 13:36:28 - INFO - root -   Epoch: [259/300][20/84], lr: 0.00000163 	 loss = 0.3845(0.1725)
2023/10/31 13:37:35 - INFO - root -   Epoch: [259/300][40/84], lr: 0.00000163 	 loss = 0.1577(0.2170)
2023/10/31 13:38:55 - INFO - root -   Epoch: [259/300][60/84], lr: 0.00000163 	 loss = 0.0553(0.2090)
2023/10/31 13:39:38 - INFO - root -   Epoch: [259/300][80/84], lr: 0.00000163 	 loss = 0.0043(0.2462)
2023/10/31 13:39:39 - INFO - root -   Epoch: [259/300] 	 loss = 0.2438
2023/10/31 13:40:36 - INFO - root -   precision = 0.9070
2023/10/31 13:40:36 - INFO - root -   eval_loss = 0.2552
2023/10/31 13:40:36 - INFO - root -   eval_acc = 0.9070
2023/10/31 13:40:37 - INFO - root -   train_accuracy = 0.9167
2023/10/31 13:41:06 - INFO - root -   Epoch: [260/300][0/84], lr: 0.00000163 	 loss = 0.1298(0.1298)
2023/10/31 13:42:11 - INFO - root -   Epoch: [260/300][20/84], lr: 0.00000163 	 loss = 0.0194(0.1792)
2023/10/31 13:43:26 - INFO - root -   Epoch: [260/300][40/84], lr: 0.00000163 	 loss = 0.0804(0.2738)
2023/10/31 13:44:28 - INFO - root -   Epoch: [260/300][60/84], lr: 0.00000163 	 loss = 0.0674(0.2557)
2023/10/31 13:45:17 - INFO - root -   Epoch: [260/300][80/84], lr: 0.00000163 	 loss = 0.0495(0.3037)
2023/10/31 13:45:21 - INFO - root -   Epoch: [260/300] 	 loss = 0.2956
2023/10/31 13:45:21 - INFO - root -   train_accuracy = 0.8988
2023/10/31 13:45:59 - INFO - root -   Epoch: [261/300][0/84], lr: 0.00000164 	 loss = 0.0714(0.0714)
2023/10/31 13:46:57 - INFO - root -   Epoch: [261/300][20/84], lr: 0.00000164 	 loss = 0.4156(0.2383)
2023/10/31 13:48:03 - INFO - root -   Epoch: [261/300][40/84], lr: 0.00000164 	 loss = 0.0084(0.2112)
2023/10/31 13:49:07 - INFO - root -   Epoch: [261/300][60/84], lr: 0.00000164 	 loss = 0.0077(0.2276)
2023/10/31 13:49:49 - INFO - root -   Epoch: [261/300][80/84], lr: 0.00000164 	 loss = 0.0489(0.2127)
2023/10/31 13:49:52 - INFO - root -   Epoch: [261/300] 	 loss = 0.2079
2023/10/31 13:49:52 - INFO - root -   train_accuracy = 0.9405
2023/10/31 13:50:30 - INFO - root -   Epoch: [262/300][0/84], lr: 0.00000164 	 loss = 0.0468(0.0468)
2023/10/31 13:51:29 - INFO - root -   Epoch: [262/300][20/84], lr: 0.00000164 	 loss = 0.0208(0.2000)
2023/10/31 13:52:39 - INFO - root -   Epoch: [262/300][40/84], lr: 0.00000164 	 loss = 0.0147(0.1811)
2023/10/31 13:53:38 - INFO - root -   Epoch: [262/300][60/84], lr: 0.00000164 	 loss = 0.1655(0.2108)
2023/10/31 13:54:33 - INFO - root -   Epoch: [262/300][80/84], lr: 0.00000164 	 loss = 0.1556(0.2335)
2023/10/31 13:54:35 - INFO - root -   Epoch: [262/300] 	 loss = 0.2252
2023/10/31 13:54:35 - INFO - root -   train_accuracy = 0.9226
2023/10/31 13:54:57 - INFO - root -   Epoch: [263/300][0/84], lr: 0.00000165 	 loss = 0.0003(0.0003)
2023/10/31 13:56:02 - INFO - root -   Epoch: [263/300][20/84], lr: 0.00000165 	 loss = 0.3076(0.3070)
2023/10/31 13:56:54 - INFO - root -   Epoch: [263/300][40/84], lr: 0.00000165 	 loss = 0.0400(0.3247)
2023/10/31 13:58:07 - INFO - root -   Epoch: [263/300][60/84], lr: 0.00000165 	 loss = 0.0089(0.3481)
2023/10/31 13:58:50 - INFO - root -   Epoch: [263/300][80/84], lr: 0.00000165 	 loss = 0.0262(0.3256)
2023/10/31 13:58:56 - INFO - root -   Epoch: [263/300] 	 loss = 0.3196
2023/10/31 13:58:56 - INFO - root -   train_accuracy = 0.8988
2023/10/31 13:59:26 - INFO - root -   Epoch: [264/300][0/84], lr: 0.00000166 	 loss = 0.0468(0.0468)
2023/10/31 14:00:31 - INFO - root -   Epoch: [264/300][20/84], lr: 0.00000166 	 loss = 0.0575(0.1708)
2023/10/31 14:01:41 - INFO - root -   Epoch: [264/300][40/84], lr: 0.00000166 	 loss = 0.0235(0.2081)
2023/10/31 14:02:29 - INFO - root -   Epoch: [264/300][60/84], lr: 0.00000166 	 loss = 0.0059(0.1833)
2023/10/31 14:03:27 - INFO - root -   Epoch: [264/300][80/84], lr: 0.00000166 	 loss = 0.0023(0.1902)
2023/10/31 14:03:30 - INFO - root -   Epoch: [264/300] 	 loss = 0.1854
2023/10/31 14:04:26 - INFO - root -   precision = 0.9302
2023/10/31 14:04:26 - INFO - root -   eval_loss = 0.2290
2023/10/31 14:04:26 - INFO - root -   eval_acc = 0.9302
2023/10/31 14:04:27 - INFO - root -   train_accuracy = 0.9286
2023/10/31 14:04:48 - INFO - root -   Epoch: [265/300][0/84], lr: 0.00000166 	 loss = 0.0028(0.0028)
2023/10/31 14:06:09 - INFO - root -   Epoch: [265/300][20/84], lr: 0.00000166 	 loss = 0.5730(0.1951)
2023/10/31 14:07:16 - INFO - root -   Epoch: [265/300][40/84], lr: 0.00000166 	 loss = 0.0220(0.2335)
2023/10/31 14:08:21 - INFO - root -   Epoch: [265/300][60/84], lr: 0.00000166 	 loss = 0.0859(0.2170)
2023/10/31 14:09:12 - INFO - root -   Epoch: [265/300][80/84], lr: 0.00000166 	 loss = 0.0246(0.2007)
2023/10/31 14:09:13 - INFO - root -   Epoch: [265/300] 	 loss = 0.1987
2023/10/31 14:09:13 - INFO - root -   train_accuracy = 0.9286
2023/10/31 14:09:43 - INFO - root -   Epoch: [266/300][0/84], lr: 0.00000167 	 loss = 0.0471(0.0471)
2023/10/31 14:10:50 - INFO - root -   Epoch: [266/300][20/84], lr: 0.00000167 	 loss = 0.0980(0.2018)
2023/10/31 14:11:59 - INFO - root -   Epoch: [266/300][40/84], lr: 0.00000167 	 loss = 0.0182(0.2168)
2023/10/31 14:13:00 - INFO - root -   Epoch: [266/300][60/84], lr: 0.00000167 	 loss = 0.0013(0.2533)
2023/10/31 14:13:51 - INFO - root -   Epoch: [266/300][80/84], lr: 0.00000167 	 loss = 0.0001(0.2360)
2023/10/31 14:13:54 - INFO - root -   Epoch: [266/300] 	 loss = 0.2287
2023/10/31 14:13:54 - INFO - root -   train_accuracy = 0.9286
2023/10/31 14:14:16 - INFO - root -   Epoch: [267/300][0/84], lr: 0.00000167 	 loss = 0.0061(0.0061)
2023/10/31 14:15:29 - INFO - root -   Epoch: [267/300][20/84], lr: 0.00000167 	 loss = 0.0418(0.2976)
2023/10/31 14:16:33 - INFO - root -   Epoch: [267/300][40/84], lr: 0.00000167 	 loss = 0.1703(0.2643)
2023/10/31 14:17:56 - INFO - root -   Epoch: [267/300][60/84], lr: 0.00000167 	 loss = 0.2339(0.2483)
2023/10/31 14:18:33 - INFO - root -   Epoch: [267/300][80/84], lr: 0.00000167 	 loss = 0.0016(0.2717)
2023/10/31 14:18:34 - INFO - root -   Epoch: [267/300] 	 loss = 0.2639
2023/10/31 14:18:34 - INFO - root -   train_accuracy = 0.9048
2023/10/31 14:18:55 - INFO - root -   Epoch: [268/300][0/84], lr: 0.00000168 	 loss = 0.0057(0.0057)
2023/10/31 14:20:08 - INFO - root -   Epoch: [268/300][20/84], lr: 0.00000168 	 loss = 0.2494(0.3303)
2023/10/31 14:21:31 - INFO - root -   Epoch: [268/300][40/84], lr: 0.00000168 	 loss = 0.0982(0.2459)
2023/10/31 14:22:20 - INFO - root -   Epoch: [268/300][60/84], lr: 0.00000168 	 loss = 0.0296(0.2378)
2023/10/31 14:23:13 - INFO - root -   Epoch: [268/300][80/84], lr: 0.00000168 	 loss = 0.0054(0.2230)
2023/10/31 14:23:14 - INFO - root -   Epoch: [268/300] 	 loss = 0.2162
2023/10/31 14:23:14 - INFO - root -   train_accuracy = 0.9405
2023/10/31 14:23:55 - INFO - root -   Epoch: [269/300][0/84], lr: 0.00000169 	 loss = 0.0638(0.0638)
2023/10/31 14:24:53 - INFO - root -   Epoch: [269/300][20/84], lr: 0.00000169 	 loss = 0.0478(0.2039)
2023/10/31 14:26:06 - INFO - root -   Epoch: [269/300][40/84], lr: 0.00000169 	 loss = 0.1528(0.2193)
2023/10/31 14:26:57 - INFO - root -   Epoch: [269/300][60/84], lr: 0.00000169 	 loss = 0.0464(0.2041)
2023/10/31 14:27:55 - INFO - root -   Epoch: [269/300][80/84], lr: 0.00000169 	 loss = 0.1423(0.2018)
2023/10/31 14:27:56 - INFO - root -   Epoch: [269/300] 	 loss = 0.1977
2023/10/31 14:28:52 - INFO - root -   precision = 0.9070
2023/10/31 14:28:52 - INFO - root -   eval_loss = 0.2817
2023/10/31 14:28:52 - INFO - root -   eval_acc = 0.9070
2023/10/31 14:28:53 - INFO - root -   train_accuracy = 0.9345
2023/10/31 14:29:15 - INFO - root -   Epoch: [270/300][0/84], lr: 0.00000169 	 loss = 0.0019(0.0019)
2023/10/31 14:30:21 - INFO - root -   Epoch: [270/300][20/84], lr: 0.00000169 	 loss = 0.0238(0.2689)
2023/10/31 14:31:28 - INFO - root -   Epoch: [270/300][40/84], lr: 0.00000169 	 loss = 0.0589(0.1776)
2023/10/31 14:32:50 - INFO - root -   Epoch: [270/300][60/84], lr: 0.00000169 	 loss = 0.0148(0.2241)
2023/10/31 14:33:31 - INFO - root -   Epoch: [270/300][80/84], lr: 0.00000169 	 loss = 0.0031(0.2783)
2023/10/31 14:33:32 - INFO - root -   Epoch: [270/300] 	 loss = 0.2769
2023/10/31 14:33:32 - INFO - root -   train_accuracy = 0.9048
2023/10/31 14:34:10 - INFO - root -   Epoch: [271/300][0/84], lr: 0.00000170 	 loss = 0.0371(0.0371)
2023/10/31 14:35:09 - INFO - root -   Epoch: [271/300][20/84], lr: 0.00000170 	 loss = 0.0515(0.2020)
2023/10/31 14:36:21 - INFO - root -   Epoch: [271/300][40/84], lr: 0.00000170 	 loss = 0.0306(0.1724)
2023/10/31 14:37:07 - INFO - root -   Epoch: [271/300][60/84], lr: 0.00000170 	 loss = 0.0307(0.1574)
2023/10/31 14:38:08 - INFO - root -   Epoch: [271/300][80/84], lr: 0.00000170 	 loss = 0.0844(0.2076)
2023/10/31 14:38:10 - INFO - root -   Epoch: [271/300] 	 loss = 0.2043
2023/10/31 14:38:10 - INFO - root -   train_accuracy = 0.9226
2023/10/31 14:38:31 - INFO - root -   Epoch: [272/300][0/84], lr: 0.00000170 	 loss = 0.0215(0.0215)
2023/10/31 14:39:36 - INFO - root -   Epoch: [272/300][20/84], lr: 0.00000170 	 loss = 0.1980(0.0975)
2023/10/31 14:40:31 - INFO - root -   Epoch: [272/300][40/84], lr: 0.00000170 	 loss = 0.0359(0.1215)
2023/10/31 14:42:04 - INFO - root -   Epoch: [272/300][60/84], lr: 0.00000170 	 loss = 0.3300(0.2386)
2023/10/31 14:42:48 - INFO - root -   Epoch: [272/300][80/84], lr: 0.00000170 	 loss = 0.0056(0.2544)
2023/10/31 14:42:53 - INFO - root -   Epoch: [272/300] 	 loss = 0.2476
2023/10/31 14:42:53 - INFO - root -   train_accuracy = 0.9167
2023/10/31 14:43:15 - INFO - root -   Epoch: [273/300][0/84], lr: 0.00000171 	 loss = 0.0078(0.0078)
2023/10/31 14:44:34 - INFO - root -   Epoch: [273/300][20/84], lr: 0.00000171 	 loss = 1.2552(0.2463)
2023/10/31 14:45:18 - INFO - root -   Epoch: [273/300][40/84], lr: 0.00000171 	 loss = 0.1973(0.2494)
2023/10/31 14:46:24 - INFO - root -   Epoch: [273/300][60/84], lr: 0.00000171 	 loss = 0.0097(0.2336)
2023/10/31 14:47:30 - INFO - root -   Epoch: [273/300][80/84], lr: 0.00000171 	 loss = 0.2468(0.2532)
2023/10/31 14:47:31 - INFO - root -   Epoch: [273/300] 	 loss = 0.2521
2023/10/31 14:47:31 - INFO - root -   train_accuracy = 0.9107
2023/10/31 14:47:53 - INFO - root -   Epoch: [274/300][0/84], lr: 0.00000171 	 loss = 0.0122(0.0122)
2023/10/31 14:49:00 - INFO - root -   Epoch: [274/300][20/84], lr: 0.00000171 	 loss = 0.0155(0.2249)
2023/10/31 14:50:04 - INFO - root -   Epoch: [274/300][40/84], lr: 0.00000171 	 loss = 0.0036(0.2481)
2023/10/31 14:51:06 - INFO - root -   Epoch: [274/300][60/84], lr: 0.00000171 	 loss = 0.1891(0.2219)
2023/10/31 14:52:06 - INFO - root -   Epoch: [274/300][80/84], lr: 0.00000171 	 loss = 0.0339(0.2101)
2023/10/31 14:52:10 - INFO - root -   Epoch: [274/300] 	 loss = 0.2063
2023/10/31 14:53:06 - INFO - root -   precision = 0.9302
2023/10/31 14:53:06 - INFO - root -   eval_loss = 0.2122
2023/10/31 14:53:06 - INFO - root -   eval_acc = 0.9302
2023/10/31 14:53:07 - INFO - root -   train_accuracy = 0.9345
2023/10/31 14:53:37 - INFO - root -   Epoch: [275/300][0/84], lr: 0.00000172 	 loss = 0.0319(0.0319)
2023/10/31 14:54:51 - INFO - root -   Epoch: [275/300][20/84], lr: 0.00000172 	 loss = 1.1268(0.1958)
2023/10/31 14:55:50 - INFO - root -   Epoch: [275/300][40/84], lr: 0.00000172 	 loss = 0.0110(0.2388)
2023/10/31 14:56:56 - INFO - root -   Epoch: [275/300][60/84], lr: 0.00000172 	 loss = 0.0748(0.2529)
2023/10/31 14:57:52 - INFO - root -   Epoch: [275/300][80/84], lr: 0.00000172 	 loss = 0.0011(0.2476)
2023/10/31 14:57:55 - INFO - root -   Epoch: [275/300] 	 loss = 0.2456
2023/10/31 14:57:55 - INFO - root -   train_accuracy = 0.9286
2023/10/31 14:58:18 - INFO - root -   Epoch: [276/300][0/84], lr: 0.00000173 	 loss = 0.2322(0.2322)
2023/10/31 14:59:29 - INFO - root -   Epoch: [276/300][20/84], lr: 0.00000173 	 loss = 0.2245(0.1702)
2023/10/31 15:00:47 - INFO - root -   Epoch: [276/300][40/84], lr: 0.00000173 	 loss = 0.0194(0.1787)
2023/10/31 15:01:43 - INFO - root -   Epoch: [276/300][60/84], lr: 0.00000173 	 loss = 0.0503(0.1414)
2023/10/31 15:02:41 - INFO - root -   Epoch: [276/300][80/84], lr: 0.00000173 	 loss = 0.0001(0.1388)
2023/10/31 15:02:42 - INFO - root -   Epoch: [276/300] 	 loss = 0.1346
2023/10/31 15:02:42 - INFO - root -   train_accuracy = 0.9643
2023/10/31 15:03:04 - INFO - root -   Epoch: [277/300][0/84], lr: 0.00000173 	 loss = 0.0004(0.0004)
2023/10/31 15:04:19 - INFO - root -   Epoch: [277/300][20/84], lr: 0.00000173 	 loss = 0.0219(0.2224)
2023/10/31 15:05:22 - INFO - root -   Epoch: [277/300][40/84], lr: 0.00000173 	 loss = 0.0006(0.2692)
2023/10/31 15:06:32 - INFO - root -   Epoch: [277/300][60/84], lr: 0.00000173 	 loss = 0.2282(0.3904)
2023/10/31 15:07:21 - INFO - root -   Epoch: [277/300][80/84], lr: 0.00000173 	 loss = 0.0780(0.3906)
2023/10/31 15:07:22 - INFO - root -   Epoch: [277/300] 	 loss = 0.3776
2023/10/31 15:07:22 - INFO - root -   train_accuracy = 0.9107
2023/10/31 15:07:44 - INFO - root -   Epoch: [278/300][0/84], lr: 0.00000174 	 loss = 0.0049(0.0049)
2023/10/31 15:08:54 - INFO - root -   Epoch: [278/300][20/84], lr: 0.00000174 	 loss = 0.5051(0.3056)
2023/10/31 15:10:08 - INFO - root -   Epoch: [278/300][40/84], lr: 0.00000174 	 loss = 0.1286(0.3103)
2023/10/31 15:11:29 - INFO - root -   Epoch: [278/300][60/84], lr: 0.00000174 	 loss = 0.0849(0.3520)
2023/10/31 15:12:12 - INFO - root -   Epoch: [278/300][80/84], lr: 0.00000174 	 loss = 0.1254(0.3310)
2023/10/31 15:12:14 - INFO - root -   Epoch: [278/300] 	 loss = 0.3239
2023/10/31 15:12:14 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:12:46 - INFO - root -   Epoch: [279/300][0/84], lr: 0.00000174 	 loss = 0.1517(0.1517)
2023/10/31 15:13:51 - INFO - root -   Epoch: [279/300][20/84], lr: 0.00000174 	 loss = 0.3528(0.2718)
2023/10/31 15:14:56 - INFO - root -   Epoch: [279/300][40/84], lr: 0.00000174 	 loss = 0.0107(0.2861)
2023/10/31 15:16:01 - INFO - root -   Epoch: [279/300][60/84], lr: 0.00000174 	 loss = 0.0346(0.3049)
2023/10/31 15:17:10 - INFO - root -   Epoch: [279/300][80/84], lr: 0.00000174 	 loss = 0.0884(0.3132)
2023/10/31 15:17:11 - INFO - root -   Epoch: [279/300] 	 loss = 0.3087
2023/10/31 15:18:08 - INFO - root -   precision = 0.8837
2023/10/31 15:18:08 - INFO - root -   eval_loss = 0.3586
2023/10/31 15:18:08 - INFO - root -   eval_acc = 0.8837
2023/10/31 15:18:09 - INFO - root -   train_accuracy = 0.8988
2023/10/31 15:18:40 - INFO - root -   Epoch: [280/300][0/84], lr: 0.00000175 	 loss = 0.1131(0.1131)
2023/10/31 15:19:43 - INFO - root -   Epoch: [280/300][20/84], lr: 0.00000175 	 loss = 0.1515(0.2770)
2023/10/31 15:20:50 - INFO - root -   Epoch: [280/300][40/84], lr: 0.00000175 	 loss = 0.0502(0.2507)
2023/10/31 15:22:06 - INFO - root -   Epoch: [280/300][60/84], lr: 0.00000175 	 loss = 0.0579(0.3142)
2023/10/31 15:22:50 - INFO - root -   Epoch: [280/300][80/84], lr: 0.00000175 	 loss = 0.0116(0.2907)
2023/10/31 15:22:54 - INFO - root -   Epoch: [280/300] 	 loss = 0.2820
2023/10/31 15:22:54 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:23:39 - INFO - root -   Epoch: [281/300][0/84], lr: 0.00000176 	 loss = 0.2038(0.2038)
2023/10/31 15:24:35 - INFO - root -   Epoch: [281/300][20/84], lr: 0.00000176 	 loss = 0.3632(0.2870)
2023/10/31 15:25:34 - INFO - root -   Epoch: [281/300][40/84], lr: 0.00000176 	 loss = 0.0051(0.2545)
2023/10/31 15:26:57 - INFO - root -   Epoch: [281/300][60/84], lr: 0.00000176 	 loss = 0.0810(0.2611)
2023/10/31 15:27:35 - INFO - root -   Epoch: [281/300][80/84], lr: 0.00000176 	 loss = 0.0527(0.3076)
2023/10/31 15:27:40 - INFO - root -   Epoch: [281/300] 	 loss = 0.3012
2023/10/31 15:27:40 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:28:09 - INFO - root -   Epoch: [282/300][0/84], lr: 0.00000176 	 loss = 0.0256(0.0256)
2023/10/31 15:29:17 - INFO - root -   Epoch: [282/300][20/84], lr: 0.00000176 	 loss = 0.3931(0.3212)
2023/10/31 15:30:17 - INFO - root -   Epoch: [282/300][40/84], lr: 0.00000176 	 loss = 0.0207(0.3083)
2023/10/31 15:31:36 - INFO - root -   Epoch: [282/300][60/84], lr: 0.00000176 	 loss = 0.0960(0.2941)
2023/10/31 15:32:28 - INFO - root -   Epoch: [282/300][80/84], lr: 0.00000176 	 loss = 0.0588(0.3082)
2023/10/31 15:32:31 - INFO - root -   Epoch: [282/300] 	 loss = 0.3014
2023/10/31 15:32:31 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:32:52 - INFO - root -   Epoch: [283/300][0/84], lr: 0.00000177 	 loss = 0.0044(0.0044)
2023/10/31 15:33:57 - INFO - root -   Epoch: [283/300][20/84], lr: 0.00000177 	 loss = 0.1477(0.2029)
2023/10/31 15:34:59 - INFO - root -   Epoch: [283/300][40/84], lr: 0.00000177 	 loss = 0.0562(0.1655)
2023/10/31 15:36:21 - INFO - root -   Epoch: [283/300][60/84], lr: 0.00000177 	 loss = 0.2284(0.2464)
2023/10/31 15:37:03 - INFO - root -   Epoch: [283/300][80/84], lr: 0.00000177 	 loss = 0.0402(0.2216)
2023/10/31 15:37:10 - INFO - root -   Epoch: [283/300] 	 loss = 0.2201
2023/10/31 15:37:10 - INFO - root -   train_accuracy = 0.9345
2023/10/31 15:37:39 - INFO - root -   Epoch: [284/300][0/84], lr: 0.00000177 	 loss = 0.0497(0.0497)
2023/10/31 15:38:45 - INFO - root -   Epoch: [284/300][20/84], lr: 0.00000177 	 loss = 0.0508(0.2021)
2023/10/31 15:39:29 - INFO - root -   Epoch: [284/300][40/84], lr: 0.00000177 	 loss = 0.0482(0.1790)
2023/10/31 15:40:43 - INFO - root -   Epoch: [284/300][60/84], lr: 0.00000177 	 loss = 0.1062(0.2396)
2023/10/31 15:41:31 - INFO - root -   Epoch: [284/300][80/84], lr: 0.00000177 	 loss = 0.8403(0.2615)
2023/10/31 15:41:34 - INFO - root -   Epoch: [284/300] 	 loss = 0.2564
2023/10/31 15:42:31 - INFO - root -   precision = 0.8837
2023/10/31 15:42:31 - INFO - root -   eval_loss = 0.2890
2023/10/31 15:42:31 - INFO - root -   eval_acc = 0.8837
2023/10/31 15:42:32 - INFO - root -   train_accuracy = 0.9167
2023/10/31 15:43:02 - INFO - root -   Epoch: [285/300][0/84], lr: 0.00000178 	 loss = 0.1906(0.1906)
2023/10/31 15:44:18 - INFO - root -   Epoch: [285/300][20/84], lr: 0.00000178 	 loss = 1.3237(0.3159)
2023/10/31 15:45:05 - INFO - root -   Epoch: [285/300][40/84], lr: 0.00000178 	 loss = 0.6048(0.2803)
2023/10/31 15:46:14 - INFO - root -   Epoch: [285/300][60/84], lr: 0.00000178 	 loss = 0.0075(0.3225)
2023/10/31 15:47:04 - INFO - root -   Epoch: [285/300][80/84], lr: 0.00000178 	 loss = 0.2678(0.3023)
2023/10/31 15:47:06 - INFO - root -   Epoch: [285/300] 	 loss = 0.2976
2023/10/31 15:47:06 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:47:37 - INFO - root -   Epoch: [286/300][0/84], lr: 0.00000179 	 loss = 0.1739(0.1739)
2023/10/31 15:48:42 - INFO - root -   Epoch: [286/300][20/84], lr: 0.00000179 	 loss = 0.2812(0.2106)
2023/10/31 15:50:06 - INFO - root -   Epoch: [286/300][40/84], lr: 0.00000179 	 loss = 0.0953(0.2753)
2023/10/31 15:51:05 - INFO - root -   Epoch: [286/300][60/84], lr: 0.00000179 	 loss = 0.0462(0.2815)
2023/10/31 15:51:49 - INFO - root -   Epoch: [286/300][80/84], lr: 0.00000179 	 loss = 0.1144(0.2792)
2023/10/31 15:51:51 - INFO - root -   Epoch: [286/300] 	 loss = 0.2730
2023/10/31 15:51:51 - INFO - root -   train_accuracy = 0.8929
2023/10/31 15:52:12 - INFO - root -   Epoch: [287/300][0/84], lr: 0.00000179 	 loss = 0.0138(0.0138)
2023/10/31 15:53:31 - INFO - root -   Epoch: [287/300][20/84], lr: 0.00000179 	 loss = 1.3018(0.2377)
2023/10/31 15:54:27 - INFO - root -   Epoch: [287/300][40/84], lr: 0.00000179 	 loss = 0.1148(0.2192)
2023/10/31 15:55:31 - INFO - root -   Epoch: [287/300][60/84], lr: 0.00000179 	 loss = 0.0437(0.2677)
2023/10/31 15:56:15 - INFO - root -   Epoch: [287/300][80/84], lr: 0.00000179 	 loss = 0.0009(0.2498)
2023/10/31 15:56:18 - INFO - root -   Epoch: [287/300] 	 loss = 0.2425
2023/10/31 15:56:18 - INFO - root -   train_accuracy = 0.9107
2023/10/31 15:56:48 - INFO - root -   Epoch: [288/300][0/84], lr: 0.00000180 	 loss = 0.0412(0.0412)
2023/10/31 15:57:53 - INFO - root -   Epoch: [288/300][20/84], lr: 0.00000180 	 loss = 0.1630(0.2168)
2023/10/31 15:59:00 - INFO - root -   Epoch: [288/300][40/84], lr: 0.00000180 	 loss = 0.0178(0.2344)
2023/10/31 16:00:00 - INFO - root -   Epoch: [288/300][60/84], lr: 0.00000180 	 loss = 0.0367(0.2685)
2023/10/31 16:00:54 - INFO - root -   Epoch: [288/300][80/84], lr: 0.00000180 	 loss = 0.0083(0.2637)
2023/10/31 16:00:55 - INFO - root -   Epoch: [288/300] 	 loss = 0.2571
2023/10/31 16:00:55 - INFO - root -   train_accuracy = 0.9048
2023/10/31 16:01:26 - INFO - root -   Epoch: [289/300][0/84], lr: 0.00000180 	 loss = 0.0604(0.0604)
2023/10/31 16:02:38 - INFO - root -   Epoch: [289/300][20/84], lr: 0.00000180 	 loss = 1.5845(0.2817)
2023/10/31 16:03:37 - INFO - root -   Epoch: [289/300][40/84], lr: 0.00000180 	 loss = 0.0042(0.2615)
2023/10/31 16:04:40 - INFO - root -   Epoch: [289/300][60/84], lr: 0.00000180 	 loss = 0.0474(0.2748)
2023/10/31 16:05:20 - INFO - root -   Epoch: [289/300][80/84], lr: 0.00000180 	 loss = 0.0005(0.2642)
2023/10/31 16:05:27 - INFO - root -   Epoch: [289/300] 	 loss = 0.2554
2023/10/31 16:06:24 - INFO - root -   precision = 0.8837
2023/10/31 16:06:24 - INFO - root -   eval_loss = 0.2917
2023/10/31 16:06:24 - INFO - root -   eval_acc = 0.8837
2023/10/31 16:06:25 - INFO - root -   train_accuracy = 0.9167
2023/10/31 16:06:47 - INFO - root -   Epoch: [290/300][0/84], lr: 0.00000181 	 loss = 0.0014(0.0014)
2023/10/31 16:08:01 - INFO - root -   Epoch: [290/300][20/84], lr: 0.00000181 	 loss = 0.1079(0.2369)
2023/10/31 16:09:09 - INFO - root -   Epoch: [290/300][40/84], lr: 0.00000181 	 loss = 0.0658(0.2483)
2023/10/31 16:10:18 - INFO - root -   Epoch: [290/300][60/84], lr: 0.00000181 	 loss = 0.0063(0.2423)
2023/10/31 16:11:17 - INFO - root -   Epoch: [290/300][80/84], lr: 0.00000181 	 loss = 0.0538(0.2550)
2023/10/31 16:11:19 - INFO - root -   Epoch: [290/300] 	 loss = 0.2492
2023/10/31 16:11:19 - INFO - root -   train_accuracy = 0.9048
2023/10/31 16:11:58 - INFO - root -   Epoch: [291/300][0/84], lr: 0.00000181 	 loss = 0.4127(0.4127)
2023/10/31 16:13:01 - INFO - root -   Epoch: [291/300][20/84], lr: 0.00000181 	 loss = 0.2595(0.2687)
2023/10/31 16:14:13 - INFO - root -   Epoch: [291/300][40/84], lr: 0.00000181 	 loss = 0.0144(0.2287)
2023/10/31 16:14:55 - INFO - root -   Epoch: [291/300][60/84], lr: 0.00000181 	 loss = 0.0166(0.2431)
2023/10/31 16:16:00 - INFO - root -   Epoch: [291/300][80/84], lr: 0.00000181 	 loss = 0.1110(0.2503)
2023/10/31 16:16:01 - INFO - root -   Epoch: [291/300] 	 loss = 0.2431
2023/10/31 16:16:01 - INFO - root -   train_accuracy = 0.9048
2023/10/31 16:16:22 - INFO - root -   Epoch: [292/300][0/84], lr: 0.00000182 	 loss = 0.0108(0.0108)
2023/10/31 16:17:36 - INFO - root -   Epoch: [292/300][20/84], lr: 0.00000182 	 loss = 1.7221(0.2645)
2023/10/31 16:18:39 - INFO - root -   Epoch: [292/300][40/84], lr: 0.00000182 	 loss = 0.0620(0.3038)
2023/10/31 16:20:03 - INFO - root -   Epoch: [292/300][60/84], lr: 0.00000182 	 loss = 0.0519(0.2989)
2023/10/31 16:20:37 - INFO - root -   Epoch: [292/300][80/84], lr: 0.00000182 	 loss = 0.0009(0.2930)
2023/10/31 16:20:39 - INFO - root -   Epoch: [292/300] 	 loss = 0.2846
2023/10/31 16:20:39 - INFO - root -   train_accuracy = 0.9048
2023/10/31 16:21:00 - INFO - root -   Epoch: [293/300][0/84], lr: 0.00000183 	 loss = 0.0024(0.0024)
2023/10/31 16:22:22 - INFO - root -   Epoch: [293/300][20/84], lr: 0.00000183 	 loss = 0.0162(0.1958)
2023/10/31 16:23:25 - INFO - root -   Epoch: [293/300][40/84], lr: 0.00000183 	 loss = 0.0373(0.1718)
2023/10/31 16:24:32 - INFO - root -   Epoch: [293/300][60/84], lr: 0.00000183 	 loss = 0.0434(0.2035)
2023/10/31 16:25:20 - INFO - root -   Epoch: [293/300][80/84], lr: 0.00000183 	 loss = 0.0091(0.2242)
2023/10/31 16:25:27 - INFO - root -   Epoch: [293/300] 	 loss = 0.2173
2023/10/31 16:25:27 - INFO - root -   train_accuracy = 0.9286
2023/10/31 16:25:48 - INFO - root -   Epoch: [294/300][0/84], lr: 0.00000183 	 loss = 0.0007(0.0007)
2023/10/31 16:27:03 - INFO - root -   Epoch: [294/300][20/84], lr: 0.00000183 	 loss = 1.9595(0.4565)
2023/10/31 16:28:15 - INFO - root -   Epoch: [294/300][40/84], lr: 0.00000183 	 loss = 0.0069(0.3685)
2023/10/31 16:29:23 - INFO - root -   Epoch: [294/300][60/84], lr: 0.00000183 	 loss = 0.0441(0.3448)
2023/10/31 16:30:07 - INFO - root -   Epoch: [294/300][80/84], lr: 0.00000183 	 loss = 0.0054(0.2898)
2023/10/31 16:30:08 - INFO - root -   Epoch: [294/300] 	 loss = 0.2802
2023/10/31 16:31:05 - INFO - root -   precision = 0.9070
2023/10/31 16:31:05 - INFO - root -   eval_loss = 0.2625
2023/10/31 16:31:05 - INFO - root -   eval_acc = 0.9070
2023/10/31 16:31:06 - INFO - root -   train_accuracy = 0.9167
2023/10/31 16:31:44 - INFO - root -   Epoch: [295/300][0/84], lr: 0.00000184 	 loss = 0.0717(0.0717)
2023/10/31 16:32:41 - INFO - root -   Epoch: [295/300][20/84], lr: 0.00000184 	 loss = 0.9905(0.3118)
2023/10/31 16:33:50 - INFO - root -   Epoch: [295/300][40/84], lr: 0.00000184 	 loss = 0.0126(0.2082)
2023/10/31 16:34:53 - INFO - root -   Epoch: [295/300][60/84], lr: 0.00000184 	 loss = 0.0229(0.1928)
2023/10/31 16:35:38 - INFO - root -   Epoch: [295/300][80/84], lr: 0.00000184 	 loss = 0.0115(0.2208)
2023/10/31 16:35:39 - INFO - root -   Epoch: [295/300] 	 loss = 0.2153
2023/10/31 16:35:39 - INFO - root -   train_accuracy = 0.9226
2023/10/31 16:36:01 - INFO - root -   Epoch: [296/300][0/84], lr: 0.00000184 	 loss = 0.0047(0.0047)
2023/10/31 16:37:15 - INFO - root -   Epoch: [296/300][20/84], lr: 0.00000184 	 loss = 0.0205(0.2746)
2023/10/31 16:38:35 - INFO - root -   Epoch: [296/300][40/84], lr: 0.00000184 	 loss = 0.0633(0.3079)
2023/10/31 16:39:25 - INFO - root -   Epoch: [296/300][60/84], lr: 0.00000184 	 loss = 0.2878(0.3269)
2023/10/31 16:40:17 - INFO - root -   Epoch: [296/300][80/84], lr: 0.00000184 	 loss = 0.1540(0.3541)
2023/10/31 16:40:18 - INFO - root -   Epoch: [296/300] 	 loss = 0.3429
2023/10/31 16:40:18 - INFO - root -   train_accuracy = 0.8869
2023/10/31 16:40:40 - INFO - root -   Epoch: [297/300][0/84], lr: 0.00000185 	 loss = 0.0153(0.0153)
2023/10/31 16:42:01 - INFO - root -   Epoch: [297/300][20/84], lr: 0.00000185 	 loss = 0.1043(0.2524)
2023/10/31 16:43:00 - INFO - root -   Epoch: [297/300][40/84], lr: 0.00000185 	 loss = 0.0747(0.2576)
2023/10/31 16:44:05 - INFO - root -   Epoch: [297/300][60/84], lr: 0.00000185 	 loss = 0.0223(0.2573)
2023/10/31 16:45:01 - INFO - root -   Epoch: [297/300][80/84], lr: 0.00000185 	 loss = 0.0036(0.2467)
2023/10/31 16:45:03 - INFO - root -   Epoch: [297/300] 	 loss = 0.2392
2023/10/31 16:45:03 - INFO - root -   train_accuracy = 0.9286
2023/10/31 16:45:25 - INFO - root -   Epoch: [298/300][0/84], lr: 0.00000186 	 loss = 0.0264(0.0264)
2023/10/31 16:46:30 - INFO - root -   Epoch: [298/300][20/84], lr: 0.00000186 	 loss = 0.1046(0.2849)
2023/10/31 16:47:34 - INFO - root -   Epoch: [298/300][40/84], lr: 0.00000186 	 loss = 0.0412(0.2619)
2023/10/31 16:48:35 - INFO - root -   Epoch: [298/300][60/84], lr: 0.00000186 	 loss = 0.0120(0.2390)
2023/10/31 16:49:26 - INFO - root -   Epoch: [298/300][80/84], lr: 0.00000186 	 loss = 0.0108(0.2213)
2023/10/31 16:49:32 - INFO - root -   Epoch: [298/300] 	 loss = 0.2154
2023/10/31 16:49:32 - INFO - root -   train_accuracy = 0.9286
2023/10/31 16:49:54 - INFO - root -   Epoch: [299/300][0/84], lr: 0.00000186 	 loss = 0.0160(0.0160)
2023/10/31 16:51:08 - INFO - root -   Epoch: [299/300][20/84], lr: 0.00000186 	 loss = 0.0692(0.1033)
2023/10/31 16:52:15 - INFO - root -   Epoch: [299/300][40/84], lr: 0.00000186 	 loss = 0.0117(0.1377)
2023/10/31 16:53:25 - INFO - root -   Epoch: [299/300][60/84], lr: 0.00000186 	 loss = 0.0420(0.2338)
2023/10/31 16:54:15 - INFO - root -   Epoch: [299/300][80/84], lr: 0.00000186 	 loss = 0.0952(0.2464)
2023/10/31 16:54:17 - INFO - root -   Epoch: [299/300] 	 loss = 0.2394
2023/10/31 16:55:13 - INFO - root -   precision = 0.8837
2023/10/31 16:55:13 - INFO - root -   eval_loss = 0.2940
2023/10/31 16:55:13 - INFO - root -   eval_acc = 0.8837
2023/10/31 16:55:14 - INFO - root -   train_accuracy = 0.9226
