2023/11/21 15:36:58 - INFO - root -   Num train examples = 400
2023/11/21 15:36:58 - INFO - root -   Num val examples = 101
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/21 15:36:58 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/21 15:36:59 - INFO - root -   backend = nccl
2023/11/21 15:36:59 - INFO - root -   batch_size = 2
2023/11/21 15:36:59 - INFO - root -   dropout = 0.5
2023/11/21 15:36:59 - INFO - root -   epochs = 300
2023/11/21 15:36:59 - INFO - root -   eval_freq = 5
2023/11/21 15:36:59 - INFO - root -   focal_loss = False
2023/11/21 15:36:59 - INFO - root -   input_size = 128
2023/11/21 15:36:59 - INFO - root -   is_pretrained = False
2023/11/21 15:36:59 - INFO - root -   label_smooth = False
2023/11/21 15:36:59 - INFO - root -   local_rank = -1
2023/11/21 15:36:59 - INFO - root -   lr = 1e-05
2023/11/21 15:36:59 - INFO - root -   lr_decay_rate = 0.1
2023/11/21 15:36:59 - INFO - root -   lr_steps = [50, 100]
2023/11/21 15:36:59 - INFO - root -   lr_type = cosine
2023/11/21 15:36:59 - INFO - root -   model_depth = 34
2023/11/21 15:36:59 - INFO - root -   model_name = resnet50
2023/11/21 15:36:59 - INFO - root -   momentum = 0.9
2023/11/21 15:36:59 - INFO - root -   num_classes = 3
2023/11/21 15:36:59 - INFO - root -   output = ./ucsf_roi_who_outputs
2023/11/21 15:36:59 - INFO - root -   print_freq = 20
2023/11/21 15:36:59 - INFO - root -   resume = 
2023/11/21 15:36:59 - INFO - root -   start_epoch = 0
2023/11/21 15:36:59 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/ucsf_train_patients.txt
2023/11/21 15:36:59 - INFO - root -   tune_from = 
2023/11/21 15:36:59 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/ucsf_test_patients.txt
2023/11/21 15:36:59 - INFO - root -   warmup_epoch = 20
2023/11/21 15:36:59 - INFO - root -   warmup_multiplier = 100
2023/11/21 15:36:59 - INFO - root -   weight_decay = 0.0005
2023/11/21 15:36:59 - INFO - root -   workers = 8
2023/11/21 15:37:05 - INFO - root -   Epoch: [0/300][0/200], lr: 0.00000010 	 loss = 1.3815(1.3815)
2023/11/21 15:37:13 - INFO - root -   Epoch: [0/300][20/200], lr: 0.00000010 	 loss = 1.4503(1.1422)
2023/11/21 15:37:36 - INFO - root -   Epoch: [0/300][40/200], lr: 0.00000010 	 loss = 0.6673(1.1409)
2023/11/21 15:37:58 - INFO - root -   Epoch: [0/300][60/200], lr: 0.00000010 	 loss = 0.6140(1.1128)
2023/11/21 15:38:09 - INFO - root -   Epoch: [0/300][80/200], lr: 0.00000010 	 loss = 1.2540(1.0737)
2023/11/21 15:38:31 - INFO - root -   Epoch: [0/300][100/200], lr: 0.00000010 	 loss = 0.8082(1.0512)
2023/11/21 15:38:45 - INFO - root -   Epoch: [0/300][120/200], lr: 0.00000010 	 loss = 0.6622(1.0437)
2023/11/21 15:38:58 - INFO - root -   Epoch: [0/300][140/200], lr: 0.00000010 	 loss = 0.2270(1.0180)
2023/11/21 15:39:11 - INFO - root -   Epoch: [0/300][160/200], lr: 0.00000010 	 loss = 0.9123(0.9994)
2023/11/21 15:39:26 - INFO - root -   Epoch: [0/300][180/200], lr: 0.00000010 	 loss = 0.6099(0.9767)
2023/11/21 15:39:44 - INFO - root -   Epoch: [0/300] 	 loss = 0.9535
2023/11/21 15:39:44 - INFO - root -   train_accuracy = 0.5625
2023/11/21 15:39:45 - INFO - root -   Epoch: [1/300][0/200], lr: 0.00000010 	 loss = 1.1691(1.1691)
2023/11/21 15:40:10 - INFO - root -   Epoch: [1/300][20/200], lr: 0.00000010 	 loss = 1.3906(0.8316)
2023/11/21 15:40:24 - INFO - root -   Epoch: [1/300][40/200], lr: 0.00000010 	 loss = 1.0887(0.7780)
2023/11/21 15:40:40 - INFO - root -   Epoch: [1/300][60/200], lr: 0.00000010 	 loss = 0.2193(0.7073)
2023/11/21 15:40:54 - INFO - root -   Epoch: [1/300][80/200], lr: 0.00000010 	 loss = 1.4815(0.6938)
2023/11/21 15:41:08 - INFO - root -   Epoch: [1/300][100/200], lr: 0.00000010 	 loss = 0.3919(0.6896)
2023/11/21 15:41:19 - INFO - root -   Epoch: [1/300][120/200], lr: 0.00000010 	 loss = 0.1491(0.6999)
2023/11/21 15:41:38 - INFO - root -   Epoch: [1/300][140/200], lr: 0.00000010 	 loss = 0.2030(0.6764)
2023/11/21 15:41:52 - INFO - root -   Epoch: [1/300][160/200], lr: 0.00000010 	 loss = 0.9227(0.6934)
2023/11/21 15:42:10 - INFO - root -   Epoch: [1/300][180/200], lr: 0.00000010 	 loss = 0.2908(0.7050)
2023/11/21 15:42:27 - INFO - root -   Epoch: [1/300] 	 loss = 0.7090
2023/11/21 15:42:27 - INFO - root -   train_accuracy = 0.7800
2023/11/21 15:42:34 - INFO - root -   Epoch: [2/300][0/200], lr: 0.00000010 	 loss = 1.4031(1.4031)
2023/11/21 15:42:45 - INFO - root -   Epoch: [2/300][20/200], lr: 0.00000010 	 loss = 1.4029(0.9006)
2023/11/21 15:43:02 - INFO - root -   Epoch: [2/300][40/200], lr: 0.00000010 	 loss = 1.0734(0.8305)
2023/11/21 15:43:14 - INFO - root -   Epoch: [2/300][60/200], lr: 0.00000010 	 loss = 0.4402(0.7490)
2023/11/21 15:43:33 - INFO - root -   Epoch: [2/300][80/200], lr: 0.00000010 	 loss = 1.6971(0.7217)
2023/11/21 15:43:47 - INFO - root -   Epoch: [2/300][100/200], lr: 0.00000010 	 loss = 0.1424(0.7339)
2023/11/21 15:44:06 - INFO - root -   Epoch: [2/300][120/200], lr: 0.00000010 	 loss = 0.1929(0.7244)
2023/11/21 15:44:18 - INFO - root -   Epoch: [2/300][140/200], lr: 0.00000010 	 loss = 0.1067(0.7019)
2023/11/21 15:44:36 - INFO - root -   Epoch: [2/300][160/200], lr: 0.00000010 	 loss = 1.9488(0.7198)
2023/11/21 15:44:50 - INFO - root -   Epoch: [2/300][180/200], lr: 0.00000010 	 loss = 0.0893(0.7161)
2023/11/21 15:45:03 - INFO - root -   Epoch: [2/300] 	 loss = 0.7099
2023/11/21 15:45:03 - INFO - root -   train_accuracy = 0.7975
2023/11/21 15:45:05 - INFO - root -   Epoch: [3/300][0/200], lr: 0.00000011 	 loss = 1.3861(1.3861)
2023/11/21 15:45:31 - INFO - root -   Epoch: [3/300][20/200], lr: 0.00000011 	 loss = 1.2656(0.8225)
2023/11/21 15:45:39 - INFO - root -   Epoch: [3/300][40/200], lr: 0.00000011 	 loss = 1.7117(0.7678)
2023/11/21 15:45:54 - INFO - root -   Epoch: [3/300][60/200], lr: 0.00000011 	 loss = 0.1219(0.7111)
2023/11/21 15:46:10 - INFO - root -   Epoch: [3/300][80/200], lr: 0.00000011 	 loss = 2.0290(0.7019)
2023/11/21 15:46:25 - INFO - root -   Epoch: [3/300][100/200], lr: 0.00000011 	 loss = 0.5542(0.7075)
2023/11/21 15:46:46 - INFO - root -   Epoch: [3/300][120/200], lr: 0.00000011 	 loss = 0.3466(0.7259)
2023/11/21 15:46:57 - INFO - root -   Epoch: [3/300][140/200], lr: 0.00000011 	 loss = 0.1689(0.6891)
2023/11/21 15:47:10 - INFO - root -   Epoch: [3/300][160/200], lr: 0.00000011 	 loss = 0.8648(0.7023)
2023/11/21 15:47:25 - INFO - root -   Epoch: [3/300][180/200], lr: 0.00000011 	 loss = 0.1223(0.6988)
2023/11/21 15:47:38 - INFO - root -   Epoch: [3/300] 	 loss = 0.6909
2023/11/21 15:47:38 - INFO - root -   train_accuracy = 0.7975
2023/11/21 15:47:40 - INFO - root -   Epoch: [4/300][0/200], lr: 0.00000011 	 loss = 1.4616(1.4616)
2023/11/21 15:48:07 - INFO - root -   Epoch: [4/300][20/200], lr: 0.00000011 	 loss = 1.2520(0.7954)
2023/11/21 15:48:23 - INFO - root -   Epoch: [4/300][40/200], lr: 0.00000011 	 loss = 0.9053(0.7042)
2023/11/21 15:48:34 - INFO - root -   Epoch: [4/300][60/200], lr: 0.00000011 	 loss = 0.1836(0.6432)
2023/11/21 15:48:51 - INFO - root -   Epoch: [4/300][80/200], lr: 0.00000011 	 loss = 1.1563(0.6301)
2023/11/21 15:49:12 - INFO - root -   Epoch: [4/300][100/200], lr: 0.00000011 	 loss = 0.1772(0.6516)
2023/11/21 15:49:26 - INFO - root -   Epoch: [4/300][120/200], lr: 0.00000011 	 loss = 0.3266(0.6572)
2023/11/21 15:49:38 - INFO - root -   Epoch: [4/300][140/200], lr: 0.00000011 	 loss = 0.1864(0.6416)
2023/11/21 15:49:55 - INFO - root -   Epoch: [4/300][160/200], lr: 0.00000011 	 loss = 1.4255(0.6544)
2023/11/21 15:50:08 - INFO - root -   Epoch: [4/300][180/200], lr: 0.00000011 	 loss = 0.0648(0.6644)
2023/11/21 15:50:17 - INFO - root -   Epoch: [4/300] 	 loss = 0.6560
2023/11/21 15:52:23 - INFO - root -   precision = 0.8020
2023/11/21 15:52:23 - INFO - root -   eval_loss = 0.5934
2023/11/21 15:52:23 - INFO - root -   eval_acc = 0.8020
2023/11/21 15:52:24 - INFO - root -   train_accuracy = 0.7975
2023/11/21 15:52:25 - INFO - root -   Epoch: [5/300][0/200], lr: 0.00000011 	 loss = 1.3484(1.3484)
2023/11/21 15:52:49 - INFO - root -   Epoch: [5/300][20/200], lr: 0.00000011 	 loss = 1.2923(0.7874)
2023/11/21 15:53:05 - INFO - root -   Epoch: [5/300][40/200], lr: 0.00000011 	 loss = 1.3547(0.7439)
2023/11/21 15:53:18 - INFO - root -   Epoch: [5/300][60/200], lr: 0.00000011 	 loss = 0.4331(0.6777)
2023/11/21 15:53:32 - INFO - root -   Epoch: [5/300][80/200], lr: 0.00000011 	 loss = 2.0051(0.6908)
2023/11/21 15:53:47 - INFO - root -   Epoch: [5/300][100/200], lr: 0.00000011 	 loss = 0.4191(0.7013)
2023/11/21 15:54:04 - INFO - root -   Epoch: [5/300][120/200], lr: 0.00000011 	 loss = 0.2947(0.6907)
2023/11/21 15:54:15 - INFO - root -   Epoch: [5/300][140/200], lr: 0.00000011 	 loss = 0.1669(0.6820)
2023/11/21 15:54:31 - INFO - root -   Epoch: [5/300][160/200], lr: 0.00000011 	 loss = 0.9458(0.6754)
2023/11/21 15:54:48 - INFO - root -   Epoch: [5/300][180/200], lr: 0.00000011 	 loss = 0.1731(0.6795)
2023/11/21 15:55:04 - INFO - root -   Epoch: [5/300] 	 loss = 0.6709
2023/11/21 15:55:04 - INFO - root -   train_accuracy = 0.8025
2023/11/21 15:55:05 - INFO - root -   Epoch: [6/300][0/200], lr: 0.00000011 	 loss = 1.2163(1.2163)
2023/11/21 15:55:26 - INFO - root -   Epoch: [6/300][20/200], lr: 0.00000011 	 loss = 1.2073(0.7506)
2023/11/21 15:55:39 - INFO - root -   Epoch: [6/300][40/200], lr: 0.00000011 	 loss = 0.8082(0.6257)
2023/11/21 15:55:53 - INFO - root -   Epoch: [6/300][60/200], lr: 0.00000011 	 loss = 0.2850(0.5977)
2023/11/21 15:56:13 - INFO - root -   Epoch: [6/300][80/200], lr: 0.00000011 	 loss = 2.0233(0.6317)
2023/11/21 15:56:31 - INFO - root -   Epoch: [6/300][100/200], lr: 0.00000011 	 loss = 0.1397(0.6575)
2023/11/21 15:56:45 - INFO - root -   Epoch: [6/300][120/200], lr: 0.00000011 	 loss = 0.1434(0.6538)
2023/11/21 15:56:58 - INFO - root -   Epoch: [6/300][140/200], lr: 0.00000011 	 loss = 0.1410(0.6351)
2023/11/21 15:57:19 - INFO - root -   Epoch: [6/300][160/200], lr: 0.00000011 	 loss = 1.4319(0.6349)
2023/11/21 15:57:33 - INFO - root -   Epoch: [6/300][180/200], lr: 0.00000011 	 loss = 0.1631(0.6357)
2023/11/21 15:57:41 - INFO - root -   Epoch: [6/300] 	 loss = 0.6357
2023/11/21 15:57:41 - INFO - root -   train_accuracy = 0.7950
2023/11/21 15:57:48 - INFO - root -   Epoch: [7/300][0/200], lr: 0.00000012 	 loss = 1.3617(1.3617)
2023/11/21 15:57:59 - INFO - root -   Epoch: [7/300][20/200], lr: 0.00000012 	 loss = 0.9659(0.7337)
2023/11/21 15:58:18 - INFO - root -   Epoch: [7/300][40/200], lr: 0.00000012 	 loss = 1.4199(0.7096)
2023/11/21 15:58:28 - INFO - root -   Epoch: [7/300][60/200], lr: 0.00000012 	 loss = 0.3414(0.6459)
2023/11/21 15:58:41 - INFO - root -   Epoch: [7/300][80/200], lr: 0.00000012 	 loss = 1.2507(0.6408)
2023/11/21 15:59:00 - INFO - root -   Epoch: [7/300][100/200], lr: 0.00000012 	 loss = 0.1911(0.6741)
2023/11/21 15:59:15 - INFO - root -   Epoch: [7/300][120/200], lr: 0.00000012 	 loss = 0.1831(0.6904)
2023/11/21 15:59:29 - INFO - root -   Epoch: [7/300][140/200], lr: 0.00000012 	 loss = 0.3199(0.6612)
2023/11/21 15:59:52 - INFO - root -   Epoch: [7/300][160/200], lr: 0.00000012 	 loss = 0.7303(0.6690)
2023/11/21 16:00:04 - INFO - root -   Epoch: [7/300][180/200], lr: 0.00000012 	 loss = 0.2081(0.6695)
2023/11/21 16:00:13 - INFO - root -   Epoch: [7/300] 	 loss = 0.6657
2023/11/21 16:00:13 - INFO - root -   train_accuracy = 0.7900
2023/11/21 16:00:14 - INFO - root -   Epoch: [8/300][0/200], lr: 0.00000012 	 loss = 0.9760(0.9760)
2023/11/21 16:00:44 - INFO - root -   Epoch: [8/300][20/200], lr: 0.00000012 	 loss = 1.5383(0.7241)
2023/11/21 16:00:57 - INFO - root -   Epoch: [8/300][40/200], lr: 0.00000012 	 loss = 0.6813(0.6271)
2023/11/21 16:01:15 - INFO - root -   Epoch: [8/300][60/200], lr: 0.00000012 	 loss = 0.3257(0.5610)
2023/11/21 16:01:29 - INFO - root -   Epoch: [8/300][80/200], lr: 0.00000012 	 loss = 1.4462(0.5900)
2023/11/21 16:01:47 - INFO - root -   Epoch: [8/300][100/200], lr: 0.00000012 	 loss = 0.2486(0.6025)
2023/11/21 16:02:00 - INFO - root -   Epoch: [8/300][120/200], lr: 0.00000012 	 loss = 0.4067(0.6095)
2023/11/21 16:02:17 - INFO - root -   Epoch: [8/300][140/200], lr: 0.00000012 	 loss = 0.1275(0.5956)
2023/11/21 16:02:31 - INFO - root -   Epoch: [8/300][160/200], lr: 0.00000012 	 loss = 1.2294(0.5933)
2023/11/21 16:02:41 - INFO - root -   Epoch: [8/300][180/200], lr: 0.00000012 	 loss = 0.0799(0.5977)
2023/11/21 16:02:54 - INFO - root -   Epoch: [8/300] 	 loss = 0.5929
2023/11/21 16:02:54 - INFO - root -   train_accuracy = 0.7975
2023/11/21 16:03:06 - INFO - root -   Epoch: [9/300][0/200], lr: 0.00000012 	 loss = 1.6598(1.6598)
2023/11/21 16:03:17 - INFO - root -   Epoch: [9/300][20/200], lr: 0.00000012 	 loss = 1.3411(0.7448)
2023/11/21 16:03:35 - INFO - root -   Epoch: [9/300][40/200], lr: 0.00000012 	 loss = 1.3458(0.6507)
2023/11/21 16:03:57 - INFO - root -   Epoch: [9/300][60/200], lr: 0.00000012 	 loss = 0.2381(0.6098)
2023/11/21 16:04:07 - INFO - root -   Epoch: [9/300][80/200], lr: 0.00000012 	 loss = 1.2210(0.6176)
2023/11/21 16:04:22 - INFO - root -   Epoch: [9/300][100/200], lr: 0.00000012 	 loss = 0.2227(0.6335)
2023/11/21 16:04:40 - INFO - root -   Epoch: [9/300][120/200], lr: 0.00000012 	 loss = 0.2483(0.6237)
2023/11/21 16:04:59 - INFO - root -   Epoch: [9/300][140/200], lr: 0.00000012 	 loss = 0.0704(0.5974)
2023/11/21 16:05:07 - INFO - root -   Epoch: [9/300][160/200], lr: 0.00000012 	 loss = 0.8543(0.6062)
2023/11/21 16:05:23 - INFO - root -   Epoch: [9/300][180/200], lr: 0.00000012 	 loss = 0.1678(0.5965)
2023/11/21 16:05:44 - INFO - root -   Epoch: [9/300] 	 loss = 0.6085
2023/11/21 16:07:45 - INFO - root -   precision = 0.8020
2023/11/21 16:07:45 - INFO - root -   eval_loss = 0.5013
2023/11/21 16:07:45 - INFO - root -   eval_acc = 0.8020
2023/11/21 16:07:46 - INFO - root -   train_accuracy = 0.7925
2023/11/21 16:07:54 - INFO - root -   Epoch: [10/300][0/200], lr: 0.00000012 	 loss = 1.3551(1.3551)
2023/11/21 16:08:07 - INFO - root -   Epoch: [10/300][20/200], lr: 0.00000012 	 loss = 1.1782(0.6255)
2023/11/21 16:08:25 - INFO - root -   Epoch: [10/300][40/200], lr: 0.00000012 	 loss = 0.4990(0.5919)
2023/11/21 16:08:46 - INFO - root -   Epoch: [10/300][60/200], lr: 0.00000012 	 loss = 0.4011(0.6068)
2023/11/21 16:08:59 - INFO - root -   Epoch: [10/300][80/200], lr: 0.00000012 	 loss = 1.1427(0.6054)
2023/11/21 16:09:17 - INFO - root -   Epoch: [10/300][100/200], lr: 0.00000012 	 loss = 0.2250(0.6105)
2023/11/21 16:09:32 - INFO - root -   Epoch: [10/300][120/200], lr: 0.00000012 	 loss = 0.3519(0.6082)
2023/11/21 16:09:42 - INFO - root -   Epoch: [10/300][140/200], lr: 0.00000012 	 loss = 0.2862(0.5971)
2023/11/21 16:09:52 - INFO - root -   Epoch: [10/300][160/200], lr: 0.00000012 	 loss = 1.4104(0.6052)
2023/11/21 16:10:16 - INFO - root -   Epoch: [10/300][180/200], lr: 0.00000012 	 loss = 0.3181(0.6067)
2023/11/21 16:10:32 - INFO - root -   Epoch: [10/300] 	 loss = 0.5954
2023/11/21 16:10:32 - INFO - root -   train_accuracy = 0.7975
2023/11/21 16:10:39 - INFO - root -   Epoch: [11/300][0/200], lr: 0.00000013 	 loss = 1.6083(1.6083)
2023/11/21 16:10:52 - INFO - root -   Epoch: [11/300][20/200], lr: 0.00000013 	 loss = 2.1155(0.8440)
2023/11/21 16:11:17 - INFO - root -   Epoch: [11/300][40/200], lr: 0.00000013 	 loss = 0.9134(0.6790)
2023/11/21 16:11:36 - INFO - root -   Epoch: [11/300][60/200], lr: 0.00000013 	 loss = 0.5101(0.6211)
2023/11/21 16:11:56 - INFO - root -   Epoch: [11/300][80/200], lr: 0.00000013 	 loss = 2.4096(0.6156)
2023/11/21 16:12:09 - INFO - root -   Epoch: [11/300][100/200], lr: 0.00000013 	 loss = 0.1414(0.6533)
2023/11/21 16:12:23 - INFO - root -   Epoch: [11/300][120/200], lr: 0.00000013 	 loss = 0.1728(0.6582)
2023/11/21 16:12:31 - INFO - root -   Epoch: [11/300][140/200], lr: 0.00000013 	 loss = 0.1177(0.6271)
2023/11/21 16:12:46 - INFO - root -   Epoch: [11/300][160/200], lr: 0.00000013 	 loss = 0.3639(0.6309)
2023/11/21 16:12:59 - INFO - root -   Epoch: [11/300][180/200], lr: 0.00000013 	 loss = 0.1576(0.6345)
2023/11/21 16:13:07 - INFO - root -   Epoch: [11/300] 	 loss = 0.6225
2023/11/21 16:13:07 - INFO - root -   train_accuracy = 0.8125
2023/11/21 16:13:18 - INFO - root -   Epoch: [12/300][0/200], lr: 0.00000013 	 loss = 0.9805(0.9805)
2023/11/21 16:13:34 - INFO - root -   Epoch: [12/300][20/200], lr: 0.00000013 	 loss = 1.6487(0.7803)
2023/11/21 16:13:47 - INFO - root -   Epoch: [12/300][40/200], lr: 0.00000013 	 loss = 1.7525(0.6267)
2023/11/21 16:14:06 - INFO - root -   Epoch: [12/300][60/200], lr: 0.00000013 	 loss = 0.3942(0.6086)
2023/11/21 16:14:27 - INFO - root -   Epoch: [12/300][80/200], lr: 0.00000013 	 loss = 1.5649(0.5965)
2023/11/21 16:14:41 - INFO - root -   Epoch: [12/300][100/200], lr: 0.00000013 	 loss = 0.1071(0.6224)
2023/11/21 16:15:01 - INFO - root -   Epoch: [12/300][120/200], lr: 0.00000013 	 loss = 0.1003(0.6407)
2023/11/21 16:15:17 - INFO - root -   Epoch: [12/300][140/200], lr: 0.00000013 	 loss = 0.3574(0.6054)
2023/11/21 16:15:28 - INFO - root -   Epoch: [12/300][160/200], lr: 0.00000013 	 loss = 0.5483(0.6175)
2023/11/21 16:15:36 - INFO - root -   Epoch: [12/300][180/200], lr: 0.00000013 	 loss = 0.0870(0.6186)
2023/11/21 16:15:47 - INFO - root -   Epoch: [12/300] 	 loss = 0.6225
2023/11/21 16:15:47 - INFO - root -   train_accuracy = 0.8000
2023/11/21 16:15:48 - INFO - root -   Epoch: [13/300][0/200], lr: 0.00000013 	 loss = 1.3079(1.3079)
2023/11/21 16:16:11 - INFO - root -   Epoch: [13/300][20/200], lr: 0.00000013 	 loss = 0.8382(0.6949)
2023/11/21 16:16:29 - INFO - root -   Epoch: [13/300][40/200], lr: 0.00000013 	 loss = 1.1848(0.6137)
2023/11/21 16:16:37 - INFO - root -   Epoch: [13/300][60/200], lr: 0.00000013 	 loss = 0.6641(0.5472)
2023/11/21 16:16:53 - INFO - root -   Epoch: [13/300][80/200], lr: 0.00000013 	 loss = 0.8281(0.5334)
2023/11/21 16:17:06 - INFO - root -   Epoch: [13/300][100/200], lr: 0.00000013 	 loss = 0.1645(0.5614)
2023/11/21 16:17:20 - INFO - root -   Epoch: [13/300][120/200], lr: 0.00000013 	 loss = 0.1446(0.5838)
2023/11/21 16:17:41 - INFO - root -   Epoch: [13/300][140/200], lr: 0.00000013 	 loss = 0.1714(0.5748)
2023/11/21 16:17:57 - INFO - root -   Epoch: [13/300][160/200], lr: 0.00000013 	 loss = 1.6637(0.5981)
2023/11/21 16:18:13 - INFO - root -   Epoch: [13/300][180/200], lr: 0.00000013 	 loss = 0.5034(0.6005)
2023/11/21 16:18:30 - INFO - root -   Epoch: [13/300] 	 loss = 0.6038
2023/11/21 16:18:30 - INFO - root -   train_accuracy = 0.8025
2023/11/21 16:18:31 - INFO - root -   Epoch: [14/300][0/200], lr: 0.00000013 	 loss = 1.1396(1.1396)
2023/11/21 16:18:48 - INFO - root -   Epoch: [14/300][20/200], lr: 0.00000013 	 loss = 1.3161(0.5912)
2023/11/21 16:19:06 - INFO - root -   Epoch: [14/300][40/200], lr: 0.00000013 	 loss = 1.3344(0.5625)
2023/11/21 16:19:16 - INFO - root -   Epoch: [14/300][60/200], lr: 0.00000013 	 loss = 0.8215(0.5291)
2023/11/21 16:19:32 - INFO - root -   Epoch: [14/300][80/200], lr: 0.00000013 	 loss = 2.0430(0.5494)
2023/11/21 16:19:49 - INFO - root -   Epoch: [14/300][100/200], lr: 0.00000013 	 loss = 0.1725(0.5787)
2023/11/21 16:20:09 - INFO - root -   Epoch: [14/300][120/200], lr: 0.00000013 	 loss = 0.2040(0.5809)
2023/11/21 16:20:24 - INFO - root -   Epoch: [14/300][140/200], lr: 0.00000013 	 loss = 0.0512(0.5596)
2023/11/21 16:20:40 - INFO - root -   Epoch: [14/300][160/200], lr: 0.00000013 	 loss = 1.7240(0.5715)
2023/11/21 16:20:48 - INFO - root -   Epoch: [14/300][180/200], lr: 0.00000013 	 loss = 0.4554(0.5641)
2023/11/21 16:21:01 - INFO - root -   Epoch: [14/300] 	 loss = 0.5623
2023/11/21 16:23:03 - INFO - root -   precision = 0.8119
2023/11/21 16:23:03 - INFO - root -   eval_loss = 0.4750
2023/11/21 16:23:03 - INFO - root -   eval_acc = 0.8119
2023/11/21 16:23:04 - INFO - root -   train_accuracy = 0.7975
2023/11/21 16:23:17 - INFO - root -   Epoch: [15/300][0/200], lr: 0.00000014 	 loss = 1.7037(1.7037)
2023/11/21 16:23:25 - INFO - root -   Epoch: [15/300][20/200], lr: 0.00000014 	 loss = 1.5037(0.7144)
2023/11/21 16:23:47 - INFO - root -   Epoch: [15/300][40/200], lr: 0.00000014 	 loss = 2.0061(0.5938)
2023/11/21 16:23:57 - INFO - root -   Epoch: [15/300][60/200], lr: 0.00000014 	 loss = 0.4498(0.5279)
2023/11/21 16:24:07 - INFO - root -   Epoch: [15/300][80/200], lr: 0.00000014 	 loss = 1.8070(0.5371)
2023/11/21 16:24:23 - INFO - root -   Epoch: [15/300][100/200], lr: 0.00000014 	 loss = 0.0802(0.5413)
2023/11/21 16:24:49 - INFO - root -   Epoch: [15/300][120/200], lr: 0.00000014 	 loss = 0.0792(0.5527)
2023/11/21 16:25:05 - INFO - root -   Epoch: [15/300][140/200], lr: 0.00000014 	 loss = 0.2185(0.5418)
2023/11/21 16:25:16 - INFO - root -   Epoch: [15/300][160/200], lr: 0.00000014 	 loss = 1.3410(0.5551)
2023/11/21 16:25:30 - INFO - root -   Epoch: [15/300][180/200], lr: 0.00000014 	 loss = 0.2808(0.5616)
2023/11/21 16:25:40 - INFO - root -   Epoch: [15/300] 	 loss = 0.5612
2023/11/21 16:25:40 - INFO - root -   train_accuracy = 0.8275
2023/11/21 16:25:41 - INFO - root -   Epoch: [16/300][0/200], lr: 0.00000014 	 loss = 1.7336(1.7336)
2023/11/21 16:26:00 - INFO - root -   Epoch: [16/300][20/200], lr: 0.00000014 	 loss = 1.4884(0.7407)
2023/11/21 16:26:16 - INFO - root -   Epoch: [16/300][40/200], lr: 0.00000014 	 loss = 0.6402(0.6196)
2023/11/21 16:26:31 - INFO - root -   Epoch: [16/300][60/200], lr: 0.00000014 	 loss = 0.1799(0.5845)
2023/11/21 16:26:41 - INFO - root -   Epoch: [16/300][80/200], lr: 0.00000014 	 loss = 0.8000(0.5769)
2023/11/21 16:27:00 - INFO - root -   Epoch: [16/300][100/200], lr: 0.00000014 	 loss = 0.1944(0.6168)
2023/11/21 16:27:23 - INFO - root -   Epoch: [16/300][120/200], lr: 0.00000014 	 loss = 0.1364(0.6208)
2023/11/21 16:27:35 - INFO - root -   Epoch: [16/300][140/200], lr: 0.00000014 	 loss = 0.3046(0.6068)
2023/11/21 16:27:49 - INFO - root -   Epoch: [16/300][160/200], lr: 0.00000014 	 loss = 1.0751(0.6156)
2023/11/21 16:27:59 - INFO - root -   Epoch: [16/300][180/200], lr: 0.00000014 	 loss = 0.1030(0.6010)
2023/11/21 16:28:09 - INFO - root -   Epoch: [16/300] 	 loss = 0.5875
2023/11/21 16:28:09 - INFO - root -   train_accuracy = 0.7975
2023/11/21 16:28:26 - INFO - root -   Epoch: [17/300][0/200], lr: 0.00000014 	 loss = 1.3141(1.3141)
2023/11/21 16:28:45 - INFO - root -   Epoch: [17/300][20/200], lr: 0.00000014 	 loss = 2.3531(0.7758)
2023/11/21 16:29:01 - INFO - root -   Epoch: [17/300][40/200], lr: 0.00000014 	 loss = 1.1496(0.6462)
2023/11/21 16:29:20 - INFO - root -   Epoch: [17/300][60/200], lr: 0.00000014 	 loss = 0.7497(0.6189)
2023/11/21 16:29:31 - INFO - root -   Epoch: [17/300][80/200], lr: 0.00000014 	 loss = 1.1618(0.6003)
2023/11/21 16:29:42 - INFO - root -   Epoch: [17/300][100/200], lr: 0.00000014 	 loss = 0.0653(0.5904)
2023/11/21 16:30:01 - INFO - root -   Epoch: [17/300][120/200], lr: 0.00000014 	 loss = 0.0286(0.5835)
2023/11/21 16:30:09 - INFO - root -   Epoch: [17/300][140/200], lr: 0.00000014 	 loss = 0.0952(0.5653)
2023/11/21 16:30:27 - INFO - root -   Epoch: [17/300][160/200], lr: 0.00000014 	 loss = 0.4334(0.5794)
2023/11/21 16:30:37 - INFO - root -   Epoch: [17/300][180/200], lr: 0.00000014 	 loss = 0.3010(0.5776)
2023/11/21 16:30:48 - INFO - root -   Epoch: [17/300] 	 loss = 0.5640
2023/11/21 16:30:48 - INFO - root -   train_accuracy = 0.8250
2023/11/21 16:30:55 - INFO - root -   Epoch: [18/300][0/200], lr: 0.00000014 	 loss = 2.1840(2.1840)
2023/11/21 16:31:07 - INFO - root -   Epoch: [18/300][20/200], lr: 0.00000014 	 loss = 1.7497(0.6053)
2023/11/21 16:31:26 - INFO - root -   Epoch: [18/300][40/200], lr: 0.00000014 	 loss = 0.6074(0.5920)
2023/11/21 16:31:44 - INFO - root -   Epoch: [18/300][60/200], lr: 0.00000014 	 loss = 0.5214(0.5309)
2023/11/21 16:31:52 - INFO - root -   Epoch: [18/300][80/200], lr: 0.00000014 	 loss = 1.0131(0.5280)
2023/11/21 16:32:08 - INFO - root -   Epoch: [18/300][100/200], lr: 0.00000014 	 loss = 0.2470(0.5249)
2023/11/21 16:32:26 - INFO - root -   Epoch: [18/300][120/200], lr: 0.00000014 	 loss = 0.0806(0.5240)
2023/11/21 16:32:47 - INFO - root -   Epoch: [18/300][140/200], lr: 0.00000014 	 loss = 0.4543(0.5150)
2023/11/21 16:33:02 - INFO - root -   Epoch: [18/300][160/200], lr: 0.00000014 	 loss = 0.5305(0.5348)
2023/11/21 16:33:22 - INFO - root -   Epoch: [18/300][180/200], lr: 0.00000014 	 loss = 0.1700(0.5507)
2023/11/21 16:33:35 - INFO - root -   Epoch: [18/300] 	 loss = 0.5581
2023/11/21 16:33:35 - INFO - root -   train_accuracy = 0.8075
2023/11/21 16:33:39 - INFO - root -   Epoch: [19/300][0/200], lr: 0.00000015 	 loss = 0.9874(0.9874)
2023/11/21 16:33:50 - INFO - root -   Epoch: [19/300][20/200], lr: 0.00000015 	 loss = 0.7571(0.6213)
2023/11/21 16:34:06 - INFO - root -   Epoch: [19/300][40/200], lr: 0.00000015 	 loss = 0.5960(0.5809)
2023/11/21 16:34:14 - INFO - root -   Epoch: [19/300][60/200], lr: 0.00000015 	 loss = 0.6063(0.5426)
2023/11/21 16:34:38 - INFO - root -   Epoch: [19/300][80/200], lr: 0.00000015 	 loss = 1.8932(0.5552)
2023/11/21 16:34:53 - INFO - root -   Epoch: [19/300][100/200], lr: 0.00000015 	 loss = 0.1026(0.5631)
2023/11/21 16:35:12 - INFO - root -   Epoch: [19/300][120/200], lr: 0.00000015 	 loss = 0.1030(0.5749)
2023/11/21 16:35:27 - INFO - root -   Epoch: [19/300][140/200], lr: 0.00000015 	 loss = 0.1855(0.5522)
2023/11/21 16:35:44 - INFO - root -   Epoch: [19/300][160/200], lr: 0.00000015 	 loss = 1.1439(0.5733)
2023/11/21 16:36:00 - INFO - root -   Epoch: [19/300][180/200], lr: 0.00000015 	 loss = 0.1322(0.5708)
2023/11/21 16:36:10 - INFO - root -   Epoch: [19/300] 	 loss = 0.5792
2023/11/21 16:38:10 - INFO - root -   precision = 0.8317
2023/11/21 16:38:10 - INFO - root -   eval_loss = 0.4512
2023/11/21 16:38:10 - INFO - root -   eval_acc = 0.8317
2023/11/21 16:38:11 - INFO - root -   train_accuracy = 0.8025
2023/11/21 16:38:13 - INFO - root -   Epoch: [20/300][0/200], lr: 0.00000015 	 loss = 1.4256(1.4256)
2023/11/21 16:38:37 - INFO - root -   Epoch: [20/300][20/200], lr: 0.00000015 	 loss = 1.1888(0.5986)
2023/11/21 16:38:51 - INFO - root -   Epoch: [20/300][40/200], lr: 0.00000015 	 loss = 1.0493(0.5466)
2023/11/21 16:39:00 - INFO - root -   Epoch: [20/300][60/200], lr: 0.00000015 	 loss = 0.1487(0.5024)
2023/11/21 16:39:14 - INFO - root -   Epoch: [20/300][80/200], lr: 0.00000015 	 loss = 1.9890(0.5390)
2023/11/21 16:39:29 - INFO - root -   Epoch: [20/300][100/200], lr: 0.00000015 	 loss = 0.1151(0.5581)
2023/11/21 16:39:48 - INFO - root -   Epoch: [20/300][120/200], lr: 0.00000015 	 loss = 0.1629(0.5610)
2023/11/21 16:40:03 - INFO - root -   Epoch: [20/300][140/200], lr: 0.00000015 	 loss = 0.4596(0.5409)
2023/11/21 16:40:20 - INFO - root -   Epoch: [20/300][160/200], lr: 0.00000015 	 loss = 1.1942(0.5581)
2023/11/21 16:40:34 - INFO - root -   Epoch: [20/300][180/200], lr: 0.00000015 	 loss = 0.0534(0.5656)
2023/11/21 16:40:42 - INFO - root -   Epoch: [20/300] 	 loss = 0.5651
2023/11/21 16:40:42 - INFO - root -   train_accuracy = 0.8150
2023/11/21 16:40:49 - INFO - root -   Epoch: [21/300][0/200], lr: 0.00000015 	 loss = 1.5261(1.5261)
2023/11/21 16:41:13 - INFO - root -   Epoch: [21/300][20/200], lr: 0.00000015 	 loss = 0.9457(0.6602)
2023/11/21 16:41:29 - INFO - root -   Epoch: [21/300][40/200], lr: 0.00000015 	 loss = 1.1548(0.5814)
2023/11/21 16:41:43 - INFO - root -   Epoch: [21/300][60/200], lr: 0.00000015 	 loss = 0.3388(0.5503)
2023/11/21 16:41:53 - INFO - root -   Epoch: [21/300][80/200], lr: 0.00000015 	 loss = 1.1859(0.5579)
2023/11/21 16:42:11 - INFO - root -   Epoch: [21/300][100/200], lr: 0.00000015 	 loss = 0.0619(0.5585)
2023/11/21 16:42:28 - INFO - root -   Epoch: [21/300][120/200], lr: 0.00000015 	 loss = 0.0733(0.5723)
2023/11/21 16:42:44 - INFO - root -   Epoch: [21/300][140/200], lr: 0.00000015 	 loss = 0.1346(0.5520)
2023/11/21 16:42:57 - INFO - root -   Epoch: [21/300][160/200], lr: 0.00000015 	 loss = 0.4571(0.5599)
2023/11/21 16:43:14 - INFO - root -   Epoch: [21/300][180/200], lr: 0.00000015 	 loss = 0.5281(0.5743)
2023/11/21 16:43:27 - INFO - root -   Epoch: [21/300] 	 loss = 0.5804
2023/11/21 16:43:27 - INFO - root -   train_accuracy = 0.8050
2023/11/21 16:43:28 - INFO - root -   Epoch: [22/300][0/200], lr: 0.00000015 	 loss = 0.8465(0.8465)
2023/11/21 16:43:46 - INFO - root -   Epoch: [22/300][20/200], lr: 0.00000015 	 loss = 1.8370(0.7007)
2023/11/21 16:44:00 - INFO - root -   Epoch: [22/300][40/200], lr: 0.00000015 	 loss = 0.4967(0.6278)
2023/11/21 16:44:13 - INFO - root -   Epoch: [22/300][60/200], lr: 0.00000015 	 loss = 0.4244(0.5342)
2023/11/21 16:44:23 - INFO - root -   Epoch: [22/300][80/200], lr: 0.00000015 	 loss = 1.5043(0.5304)
2023/11/21 16:44:43 - INFO - root -   Epoch: [22/300][100/200], lr: 0.00000015 	 loss = 0.0627(0.5253)
2023/11/21 16:44:54 - INFO - root -   Epoch: [22/300][120/200], lr: 0.00000015 	 loss = 0.1278(0.5307)
2023/11/21 16:45:15 - INFO - root -   Epoch: [22/300][140/200], lr: 0.00000015 	 loss = 0.1552(0.5270)
2023/11/21 16:45:29 - INFO - root -   Epoch: [22/300][160/200], lr: 0.00000015 	 loss = 0.3997(0.5496)
2023/11/21 16:45:37 - INFO - root -   Epoch: [22/300][180/200], lr: 0.00000015 	 loss = 0.1215(0.5414)
2023/11/21 16:45:45 - INFO - root -   Epoch: [22/300] 	 loss = 0.5352
2023/11/21 16:45:45 - INFO - root -   train_accuracy = 0.8250
2023/11/21 16:45:51 - INFO - root -   Epoch: [23/300][0/200], lr: 0.00000016 	 loss = 2.3793(2.3793)
2023/11/21 16:46:03 - INFO - root -   Epoch: [23/300][20/200], lr: 0.00000016 	 loss = 2.6938(0.7875)
2023/11/21 16:46:13 - INFO - root -   Epoch: [23/300][40/200], lr: 0.00000016 	 loss = 0.6398(0.6750)
2023/11/21 16:46:24 - INFO - root -   Epoch: [23/300][60/200], lr: 0.00000016 	 loss = 0.5784(0.5801)
2023/11/21 16:46:40 - INFO - root -   Epoch: [23/300][80/200], lr: 0.00000016 	 loss = 1.2337(0.5714)
2023/11/21 16:46:48 - INFO - root -   Epoch: [23/300][100/200], lr: 0.00000016 	 loss = 0.0682(0.5599)
2023/11/21 16:47:05 - INFO - root -   Epoch: [23/300][120/200], lr: 0.00000016 	 loss = 0.3912(0.5646)
2023/11/21 16:47:21 - INFO - root -   Epoch: [23/300][140/200], lr: 0.00000016 	 loss = 0.2364(0.5475)
2023/11/21 16:47:34 - INFO - root -   Epoch: [23/300][160/200], lr: 0.00000016 	 loss = 1.8531(0.5703)
2023/11/21 16:47:45 - INFO - root -   Epoch: [23/300][180/200], lr: 0.00000016 	 loss = 0.1095(0.5718)
2023/11/21 16:47:55 - INFO - root -   Epoch: [23/300] 	 loss = 0.5886
2023/11/21 16:47:55 - INFO - root -   train_accuracy = 0.8175
2023/11/21 16:48:04 - INFO - root -   Epoch: [24/300][0/200], lr: 0.00000016 	 loss = 1.1331(1.1331)
2023/11/21 16:48:14 - INFO - root -   Epoch: [24/300][20/200], lr: 0.00000016 	 loss = 1.5720(0.5569)
2023/11/21 16:48:32 - INFO - root -   Epoch: [24/300][40/200], lr: 0.00000016 	 loss = 0.5108(0.5295)
2023/11/21 16:48:45 - INFO - root -   Epoch: [24/300][60/200], lr: 0.00000016 	 loss = 0.2785(0.4973)
2023/11/21 16:49:03 - INFO - root -   Epoch: [24/300][80/200], lr: 0.00000016 	 loss = 0.8772(0.5049)
2023/11/21 16:49:15 - INFO - root -   Epoch: [24/300][100/200], lr: 0.00000016 	 loss = 0.0418(0.4880)
2023/11/21 16:49:31 - INFO - root -   Epoch: [24/300][120/200], lr: 0.00000016 	 loss = 0.2640(0.4937)
2023/11/21 16:49:51 - INFO - root -   Epoch: [24/300][140/200], lr: 0.00000016 	 loss = 0.3677(0.4985)
2023/11/21 16:50:06 - INFO - root -   Epoch: [24/300][160/200], lr: 0.00000016 	 loss = 1.9941(0.5266)
2023/11/21 16:50:19 - INFO - root -   Epoch: [24/300][180/200], lr: 0.00000016 	 loss = 0.2201(0.5314)
2023/11/21 16:50:27 - INFO - root -   Epoch: [24/300] 	 loss = 0.5207
2023/11/21 16:52:28 - INFO - root -   precision = 0.8317
2023/11/21 16:52:28 - INFO - root -   eval_loss = 0.4351
2023/11/21 16:52:28 - INFO - root -   eval_acc = 0.8317
2023/11/21 16:52:29 - INFO - root -   train_accuracy = 0.8275
2023/11/21 16:52:31 - INFO - root -   Epoch: [25/300][0/200], lr: 0.00000016 	 loss = 1.7200(1.7200)
2023/11/21 16:52:52 - INFO - root -   Epoch: [25/300][20/200], lr: 0.00000016 	 loss = 1.3873(0.6120)
2023/11/21 16:53:00 - INFO - root -   Epoch: [25/300][40/200], lr: 0.00000016 	 loss = 0.9433(0.5591)
2023/11/21 16:53:13 - INFO - root -   Epoch: [25/300][60/200], lr: 0.00000016 	 loss = 0.3527(0.4867)
2023/11/21 16:53:25 - INFO - root -   Epoch: [25/300][80/200], lr: 0.00000016 	 loss = 1.2348(0.4897)
2023/11/21 16:53:46 - INFO - root -   Epoch: [25/300][100/200], lr: 0.00000016 	 loss = 0.0489(0.5210)
2023/11/21 16:54:01 - INFO - root -   Epoch: [25/300][120/200], lr: 0.00000016 	 loss = 0.3478(0.5433)
2023/11/21 16:54:17 - INFO - root -   Epoch: [25/300][140/200], lr: 0.00000016 	 loss = 0.4302(0.5194)
2023/11/21 16:54:25 - INFO - root -   Epoch: [25/300][160/200], lr: 0.00000016 	 loss = 0.5957(0.5431)
2023/11/21 16:54:44 - INFO - root -   Epoch: [25/300][180/200], lr: 0.00000016 	 loss = 0.2263(0.5407)
2023/11/21 16:54:54 - INFO - root -   Epoch: [25/300] 	 loss = 0.5404
2023/11/21 16:54:54 - INFO - root -   train_accuracy = 0.8125
2023/11/21 16:55:03 - INFO - root -   Epoch: [26/300][0/200], lr: 0.00000016 	 loss = 0.9930(0.9930)
2023/11/21 16:55:18 - INFO - root -   Epoch: [26/300][20/200], lr: 0.00000016 	 loss = 1.5547(0.6576)
2023/11/21 16:55:31 - INFO - root -   Epoch: [26/300][40/200], lr: 0.00000016 	 loss = 0.4386(0.5276)
2023/11/21 16:55:45 - INFO - root -   Epoch: [26/300][60/200], lr: 0.00000016 	 loss = 0.0538(0.4944)
2023/11/21 16:56:02 - INFO - root -   Epoch: [26/300][80/200], lr: 0.00000016 	 loss = 1.1844(0.5123)
2023/11/21 16:56:17 - INFO - root -   Epoch: [26/300][100/200], lr: 0.00000016 	 loss = 0.0323(0.5209)
2023/11/21 16:56:40 - INFO - root -   Epoch: [26/300][120/200], lr: 0.00000016 	 loss = 0.1178(0.5401)
2023/11/21 16:56:53 - INFO - root -   Epoch: [26/300][140/200], lr: 0.00000016 	 loss = 0.0756(0.5414)
2023/11/21 16:57:06 - INFO - root -   Epoch: [26/300][160/200], lr: 0.00000016 	 loss = 0.9785(0.5428)
2023/11/21 16:57:13 - INFO - root -   Epoch: [26/300][180/200], lr: 0.00000016 	 loss = 0.3048(0.5488)
2023/11/21 16:57:23 - INFO - root -   Epoch: [26/300] 	 loss = 0.5479
2023/11/21 16:57:23 - INFO - root -   train_accuracy = 0.8150
2023/11/21 16:57:24 - INFO - root -   Epoch: [27/300][0/200], lr: 0.00000017 	 loss = 1.8623(1.8623)
2023/11/21 16:57:50 - INFO - root -   Epoch: [27/300][20/200], lr: 0.00000017 	 loss = 0.6804(0.6363)
2023/11/21 16:58:04 - INFO - root -   Epoch: [27/300][40/200], lr: 0.00000017 	 loss = 0.3744(0.5169)
2023/11/21 16:58:21 - INFO - root -   Epoch: [27/300][60/200], lr: 0.00000017 	 loss = 0.3082(0.4760)
2023/11/21 16:58:37 - INFO - root -   Epoch: [27/300][80/200], lr: 0.00000017 	 loss = 1.4107(0.4988)
2023/11/21 16:58:54 - INFO - root -   Epoch: [27/300][100/200], lr: 0.00000017 	 loss = 0.1506(0.5343)
2023/11/21 16:59:09 - INFO - root -   Epoch: [27/300][120/200], lr: 0.00000017 	 loss = 0.1462(0.5408)
2023/11/21 16:59:22 - INFO - root -   Epoch: [27/300][140/200], lr: 0.00000017 	 loss = 0.0998(0.5429)
2023/11/21 16:59:32 - INFO - root -   Epoch: [27/300][160/200], lr: 0.00000017 	 loss = 1.1934(0.5507)
2023/11/21 16:59:43 - INFO - root -   Epoch: [27/300][180/200], lr: 0.00000017 	 loss = 0.2173(0.5611)
2023/11/21 16:59:53 - INFO - root -   Epoch: [27/300] 	 loss = 0.5541
2023/11/21 16:59:53 - INFO - root -   train_accuracy = 0.8075
2023/11/21 17:00:03 - INFO - root -   Epoch: [28/300][0/200], lr: 0.00000017 	 loss = 1.2253(1.2253)
2023/11/21 17:00:16 - INFO - root -   Epoch: [28/300][20/200], lr: 0.00000017 	 loss = 1.4146(0.6319)
2023/11/21 17:00:30 - INFO - root -   Epoch: [28/300][40/200], lr: 0.00000017 	 loss = 1.7906(0.5611)
2023/11/21 17:00:53 - INFO - root -   Epoch: [28/300][60/200], lr: 0.00000017 	 loss = 0.2397(0.5203)
2023/11/21 17:01:10 - INFO - root -   Epoch: [28/300][80/200], lr: 0.00000017 	 loss = 1.7497(0.5510)
2023/11/21 17:01:18 - INFO - root -   Epoch: [28/300][100/200], lr: 0.00000017 	 loss = 0.1405(0.5733)
2023/11/21 17:01:35 - INFO - root -   Epoch: [28/300][120/200], lr: 0.00000017 	 loss = 0.2714(0.5809)
2023/11/21 17:01:45 - INFO - root -   Epoch: [28/300][140/200], lr: 0.00000017 	 loss = 0.0915(0.5590)
2023/11/21 17:02:00 - INFO - root -   Epoch: [28/300][160/200], lr: 0.00000017 	 loss = 0.8667(0.5786)
2023/11/21 17:02:15 - INFO - root -   Epoch: [28/300][180/200], lr: 0.00000017 	 loss = 0.1543(0.5711)
2023/11/21 17:02:23 - INFO - root -   Epoch: [28/300] 	 loss = 0.5685
2023/11/21 17:02:23 - INFO - root -   train_accuracy = 0.8150
2023/11/21 17:02:31 - INFO - root -   Epoch: [29/300][0/200], lr: 0.00000017 	 loss = 2.0965(2.0965)
2023/11/21 17:02:50 - INFO - root -   Epoch: [29/300][20/200], lr: 0.00000017 	 loss = 1.3878(0.7599)
2023/11/21 17:03:06 - INFO - root -   Epoch: [29/300][40/200], lr: 0.00000017 	 loss = 0.9846(0.6422)
2023/11/21 17:03:19 - INFO - root -   Epoch: [29/300][60/200], lr: 0.00000017 	 loss = 0.3208(0.5672)
2023/11/21 17:03:33 - INFO - root -   Epoch: [29/300][80/200], lr: 0.00000017 	 loss = 1.1159(0.5782)
2023/11/21 17:03:51 - INFO - root -   Epoch: [29/300][100/200], lr: 0.00000017 	 loss = 0.1030(0.5644)
2023/11/21 17:04:04 - INFO - root -   Epoch: [29/300][120/200], lr: 0.00000017 	 loss = 0.4617(0.5688)
2023/11/21 17:04:20 - INFO - root -   Epoch: [29/300][140/200], lr: 0.00000017 	 loss = 0.5845(0.5640)
2023/11/21 17:04:31 - INFO - root -   Epoch: [29/300][160/200], lr: 0.00000017 	 loss = 0.6109(0.5603)
2023/11/21 17:04:52 - INFO - root -   Epoch: [29/300][180/200], lr: 0.00000017 	 loss = 0.1227(0.5649)
2023/11/21 17:05:00 - INFO - root -   Epoch: [29/300] 	 loss = 0.5634
2023/11/21 17:07:00 - INFO - root -   precision = 0.8317
2023/11/21 17:07:00 - INFO - root -   eval_loss = 0.4474
2023/11/21 17:07:00 - INFO - root -   eval_acc = 0.8317
2023/11/21 17:07:01 - INFO - root -   train_accuracy = 0.8100
2023/11/21 17:07:15 - INFO - root -   Epoch: [30/300][0/200], lr: 0.00000017 	 loss = 0.9555(0.9555)
2023/11/21 17:07:29 - INFO - root -   Epoch: [30/300][20/200], lr: 0.00000017 	 loss = 1.7197(0.6563)
2023/11/21 17:07:49 - INFO - root -   Epoch: [30/300][40/200], lr: 0.00000017 	 loss = 0.4427(0.5576)
2023/11/21 17:08:00 - INFO - root -   Epoch: [30/300][60/200], lr: 0.00000017 	 loss = 0.1982(0.4971)
2023/11/21 17:08:09 - INFO - root -   Epoch: [30/300][80/200], lr: 0.00000017 	 loss = 1.5007(0.5136)
2023/11/21 17:08:17 - INFO - root -   Epoch: [30/300][100/200], lr: 0.00000017 	 loss = 0.3342(0.5019)
2023/11/21 17:08:38 - INFO - root -   Epoch: [30/300][120/200], lr: 0.00000017 	 loss = 0.6309(0.5175)
2023/11/21 17:08:54 - INFO - root -   Epoch: [30/300][140/200], lr: 0.00000017 	 loss = 0.1525(0.5044)
2023/11/21 17:09:04 - INFO - root -   Epoch: [30/300][160/200], lr: 0.00000017 	 loss = 0.9658(0.5189)
2023/11/21 17:09:22 - INFO - root -   Epoch: [30/300][180/200], lr: 0.00000017 	 loss = 0.1515(0.5206)
2023/11/21 17:09:31 - INFO - root -   Epoch: [30/300] 	 loss = 0.5191
2023/11/21 17:09:31 - INFO - root -   train_accuracy = 0.8300
2023/11/21 17:09:32 - INFO - root -   Epoch: [31/300][0/200], lr: 0.00000018 	 loss = 0.7388(0.7388)
2023/11/21 17:09:48 - INFO - root -   Epoch: [31/300][20/200], lr: 0.00000018 	 loss = 1.4588(0.5312)
2023/11/21 17:10:00 - INFO - root -   Epoch: [31/300][40/200], lr: 0.00000018 	 loss = 1.7700(0.4618)
2023/11/21 17:10:17 - INFO - root -   Epoch: [31/300][60/200], lr: 0.00000018 	 loss = 0.0909(0.4445)
2023/11/21 17:10:32 - INFO - root -   Epoch: [31/300][80/200], lr: 0.00000018 	 loss = 1.2552(0.4667)
2023/11/21 17:10:56 - INFO - root -   Epoch: [31/300][100/200], lr: 0.00000018 	 loss = 0.3048(0.4989)
2023/11/21 17:11:06 - INFO - root -   Epoch: [31/300][120/200], lr: 0.00000018 	 loss = 0.1505(0.5115)
2023/11/21 17:11:18 - INFO - root -   Epoch: [31/300][140/200], lr: 0.00000018 	 loss = 0.0784(0.4913)
2023/11/21 17:11:31 - INFO - root -   Epoch: [31/300][160/200], lr: 0.00000018 	 loss = 0.5092(0.5151)
2023/11/21 17:11:46 - INFO - root -   Epoch: [31/300][180/200], lr: 0.00000018 	 loss = 0.0859(0.5185)
2023/11/21 17:11:57 - INFO - root -   Epoch: [31/300] 	 loss = 0.5219
2023/11/21 17:11:57 - INFO - root -   train_accuracy = 0.8350
2023/11/21 17:12:04 - INFO - root -   Epoch: [32/300][0/200], lr: 0.00000018 	 loss = 1.4272(1.4272)
2023/11/21 17:12:16 - INFO - root -   Epoch: [32/300][20/200], lr: 0.00000018 	 loss = 2.0412(0.6681)
2023/11/21 17:12:31 - INFO - root -   Epoch: [32/300][40/200], lr: 0.00000018 	 loss = 0.9088(0.6209)
2023/11/21 17:12:54 - INFO - root -   Epoch: [32/300][60/200], lr: 0.00000018 	 loss = 0.2144(0.5443)
2023/11/21 17:13:03 - INFO - root -   Epoch: [32/300][80/200], lr: 0.00000018 	 loss = 1.2770(0.5527)
2023/11/21 17:13:23 - INFO - root -   Epoch: [32/300][100/200], lr: 0.00000018 	 loss = 0.0899(0.5671)
2023/11/21 17:13:45 - INFO - root -   Epoch: [32/300][120/200], lr: 0.00000018 	 loss = 0.1508(0.5714)
2023/11/21 17:13:57 - INFO - root -   Epoch: [32/300][140/200], lr: 0.00000018 	 loss = 0.2932(0.5548)
2023/11/21 17:14:19 - INFO - root -   Epoch: [32/300][160/200], lr: 0.00000018 	 loss = 1.4441(0.5521)
2023/11/21 17:14:34 - INFO - root -   Epoch: [32/300][180/200], lr: 0.00000018 	 loss = 0.3419(0.5623)
2023/11/21 17:14:47 - INFO - root -   Epoch: [32/300] 	 loss = 0.5584
2023/11/21 17:14:47 - INFO - root -   train_accuracy = 0.8300
2023/11/21 17:14:58 - INFO - root -   Epoch: [33/300][0/200], lr: 0.00000018 	 loss = 2.8774(2.8774)
2023/11/21 17:15:10 - INFO - root -   Epoch: [33/300][20/200], lr: 0.00000018 	 loss = 1.5151(0.6958)
2023/11/21 17:15:32 - INFO - root -   Epoch: [33/300][40/200], lr: 0.00000018 	 loss = 1.2285(0.6392)
2023/11/21 17:15:44 - INFO - root -   Epoch: [33/300][60/200], lr: 0.00000018 	 loss = 0.7607(0.5492)
2023/11/21 17:16:00 - INFO - root -   Epoch: [33/300][80/200], lr: 0.00000018 	 loss = 1.9501(0.5494)
2023/11/21 17:16:19 - INFO - root -   Epoch: [33/300][100/200], lr: 0.00000018 	 loss = 0.0618(0.5728)
2023/11/21 17:16:36 - INFO - root -   Epoch: [33/300][120/200], lr: 0.00000018 	 loss = 0.0949(0.5810)
2023/11/21 17:16:59 - INFO - root -   Epoch: [33/300][140/200], lr: 0.00000018 	 loss = 0.2361(0.5699)
2023/11/21 17:17:14 - INFO - root -   Epoch: [33/300][160/200], lr: 0.00000018 	 loss = 0.4449(0.5669)
2023/11/21 17:17:32 - INFO - root -   Epoch: [33/300][180/200], lr: 0.00000018 	 loss = 0.0900(0.5696)
2023/11/21 17:17:48 - INFO - root -   Epoch: [33/300] 	 loss = 0.5735
2023/11/21 17:17:48 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:18:02 - INFO - root -   Epoch: [34/300][0/200], lr: 0.00000018 	 loss = 1.8000(1.8000)
2023/11/21 17:18:10 - INFO - root -   Epoch: [34/300][20/200], lr: 0.00000018 	 loss = 1.5271(0.7032)
2023/11/21 17:18:22 - INFO - root -   Epoch: [34/300][40/200], lr: 0.00000018 	 loss = 0.4151(0.5804)
2023/11/21 17:18:32 - INFO - root -   Epoch: [34/300][60/200], lr: 0.00000018 	 loss = 0.5500(0.5356)
2023/11/21 17:18:52 - INFO - root -   Epoch: [34/300][80/200], lr: 0.00000018 	 loss = 0.9039(0.5553)
2023/11/21 17:19:06 - INFO - root -   Epoch: [34/300][100/200], lr: 0.00000018 	 loss = 0.2528(0.5297)
2023/11/21 17:19:22 - INFO - root -   Epoch: [34/300][120/200], lr: 0.00000018 	 loss = 0.3688(0.5160)
2023/11/21 17:19:42 - INFO - root -   Epoch: [34/300][140/200], lr: 0.00000018 	 loss = 0.6378(0.5079)
2023/11/21 17:20:00 - INFO - root -   Epoch: [34/300][160/200], lr: 0.00000018 	 loss = 1.2214(0.5254)
2023/11/21 17:20:13 - INFO - root -   Epoch: [34/300][180/200], lr: 0.00000018 	 loss = 0.0573(0.5260)
2023/11/21 17:20:26 - INFO - root -   Epoch: [34/300] 	 loss = 0.5164
2023/11/21 17:22:28 - INFO - root -   precision = 0.8515
2023/11/21 17:22:28 - INFO - root -   eval_loss = 0.4253
2023/11/21 17:22:28 - INFO - root -   eval_acc = 0.8515
2023/11/21 17:22:29 - INFO - root -   train_accuracy = 0.8300
2023/11/21 17:22:37 - INFO - root -   Epoch: [35/300][0/200], lr: 0.00000019 	 loss = 0.5550(0.5550)
2023/11/21 17:22:54 - INFO - root -   Epoch: [35/300][20/200], lr: 0.00000019 	 loss = 2.0267(0.6052)
2023/11/21 17:23:06 - INFO - root -   Epoch: [35/300][40/200], lr: 0.00000019 	 loss = 0.5869(0.5184)
2023/11/21 17:23:22 - INFO - root -   Epoch: [35/300][60/200], lr: 0.00000019 	 loss = 0.2051(0.4629)
2023/11/21 17:23:38 - INFO - root -   Epoch: [35/300][80/200], lr: 0.00000019 	 loss = 1.6110(0.5014)
2023/11/21 17:23:55 - INFO - root -   Epoch: [35/300][100/200], lr: 0.00000019 	 loss = 0.0555(0.5101)
2023/11/21 17:24:08 - INFO - root -   Epoch: [35/300][120/200], lr: 0.00000019 	 loss = 0.0642(0.5178)
2023/11/21 17:24:28 - INFO - root -   Epoch: [35/300][140/200], lr: 0.00000019 	 loss = 0.1688(0.5098)
2023/11/21 17:24:49 - INFO - root -   Epoch: [35/300][160/200], lr: 0.00000019 	 loss = 0.2248(0.5179)
2023/11/21 17:24:58 - INFO - root -   Epoch: [35/300][180/200], lr: 0.00000019 	 loss = 0.2351(0.5171)
2023/11/21 17:25:06 - INFO - root -   Epoch: [35/300] 	 loss = 0.5168
2023/11/21 17:25:06 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:25:12 - INFO - root -   Epoch: [36/300][0/200], lr: 0.00000019 	 loss = 1.9753(1.9753)
2023/11/21 17:25:22 - INFO - root -   Epoch: [36/300][20/200], lr: 0.00000019 	 loss = 1.9384(0.5989)
2023/11/21 17:25:38 - INFO - root -   Epoch: [36/300][40/200], lr: 0.00000019 	 loss = 1.1073(0.5406)
2023/11/21 17:25:51 - INFO - root -   Epoch: [36/300][60/200], lr: 0.00000019 	 loss = 0.3682(0.5228)
2023/11/21 17:26:06 - INFO - root -   Epoch: [36/300][80/200], lr: 0.00000019 	 loss = 1.5052(0.5271)
2023/11/21 17:26:21 - INFO - root -   Epoch: [36/300][100/200], lr: 0.00000019 	 loss = 0.0530(0.5156)
2023/11/21 17:26:29 - INFO - root -   Epoch: [36/300][120/200], lr: 0.00000019 	 loss = 0.0875(0.5123)
2023/11/21 17:26:39 - INFO - root -   Epoch: [36/300][140/200], lr: 0.00000019 	 loss = 0.1310(0.5003)
2023/11/21 17:27:07 - INFO - root -   Epoch: [36/300][160/200], lr: 0.00000019 	 loss = 1.9049(0.5176)
2023/11/21 17:27:15 - INFO - root -   Epoch: [36/300][180/200], lr: 0.00000019 	 loss = 0.2439(0.5061)
2023/11/21 17:27:28 - INFO - root -   Epoch: [36/300] 	 loss = 0.5109
2023/11/21 17:27:28 - INFO - root -   train_accuracy = 0.8450
2023/11/21 17:27:30 - INFO - root -   Epoch: [37/300][0/200], lr: 0.00000019 	 loss = 1.6035(1.6035)
2023/11/21 17:27:53 - INFO - root -   Epoch: [37/300][20/200], lr: 0.00000019 	 loss = 1.2256(0.6922)
2023/11/21 17:28:09 - INFO - root -   Epoch: [37/300][40/200], lr: 0.00000019 	 loss = 0.8345(0.5744)
2023/11/21 17:28:26 - INFO - root -   Epoch: [37/300][60/200], lr: 0.00000019 	 loss = 0.6390(0.5237)
2023/11/21 17:28:41 - INFO - root -   Epoch: [37/300][80/200], lr: 0.00000019 	 loss = 0.6479(0.5199)
2023/11/21 17:28:50 - INFO - root -   Epoch: [37/300][100/200], lr: 0.00000019 	 loss = 0.0836(0.5304)
2023/11/21 17:29:08 - INFO - root -   Epoch: [37/300][120/200], lr: 0.00000019 	 loss = 0.2823(0.5348)
2023/11/21 17:29:21 - INFO - root -   Epoch: [37/300][140/200], lr: 0.00000019 	 loss = 0.1613(0.5360)
2023/11/21 17:29:39 - INFO - root -   Epoch: [37/300][160/200], lr: 0.00000019 	 loss = 0.9333(0.5488)
2023/11/21 17:29:52 - INFO - root -   Epoch: [37/300][180/200], lr: 0.00000019 	 loss = 0.3109(0.5403)
2023/11/21 17:30:07 - INFO - root -   Epoch: [37/300] 	 loss = 0.5368
2023/11/21 17:30:07 - INFO - root -   train_accuracy = 0.8250
2023/11/21 17:30:09 - INFO - root -   Epoch: [38/300][0/200], lr: 0.00000019 	 loss = 2.0472(2.0472)
2023/11/21 17:30:31 - INFO - root -   Epoch: [38/300][20/200], lr: 0.00000019 	 loss = 1.7824(0.6446)
2023/11/21 17:30:42 - INFO - root -   Epoch: [38/300][40/200], lr: 0.00000019 	 loss = 0.4439(0.5412)
2023/11/21 17:30:53 - INFO - root -   Epoch: [38/300][60/200], lr: 0.00000019 	 loss = 0.6639(0.5201)
2023/11/21 17:31:05 - INFO - root -   Epoch: [38/300][80/200], lr: 0.00000019 	 loss = 0.8610(0.5065)
2023/11/21 17:31:26 - INFO - root -   Epoch: [38/300][100/200], lr: 0.00000019 	 loss = 0.0880(0.5032)
2023/11/21 17:31:37 - INFO - root -   Epoch: [38/300][120/200], lr: 0.00000019 	 loss = 0.1631(0.5277)
2023/11/21 17:31:47 - INFO - root -   Epoch: [38/300][140/200], lr: 0.00000019 	 loss = 0.4290(0.5226)
2023/11/21 17:32:05 - INFO - root -   Epoch: [38/300][160/200], lr: 0.00000019 	 loss = 0.8074(0.5350)
2023/11/21 17:32:23 - INFO - root -   Epoch: [38/300][180/200], lr: 0.00000019 	 loss = 0.4062(0.5288)
2023/11/21 17:32:31 - INFO - root -   Epoch: [38/300] 	 loss = 0.5301
2023/11/21 17:32:31 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:32:32 - INFO - root -   Epoch: [39/300][0/200], lr: 0.00000020 	 loss = 1.2987(1.2987)
2023/11/21 17:32:49 - INFO - root -   Epoch: [39/300][20/200], lr: 0.00000020 	 loss = 1.4832(0.6027)
2023/11/21 17:33:04 - INFO - root -   Epoch: [39/300][40/200], lr: 0.00000020 	 loss = 1.0309(0.5700)
2023/11/21 17:33:20 - INFO - root -   Epoch: [39/300][60/200], lr: 0.00000020 	 loss = 0.1839(0.5485)
2023/11/21 17:33:36 - INFO - root -   Epoch: [39/300][80/200], lr: 0.00000020 	 loss = 0.5854(0.5217)
2023/11/21 17:33:52 - INFO - root -   Epoch: [39/300][100/200], lr: 0.00000020 	 loss = 0.3056(0.5175)
2023/11/21 17:34:06 - INFO - root -   Epoch: [39/300][120/200], lr: 0.00000020 	 loss = 0.4773(0.5132)
2023/11/21 17:34:22 - INFO - root -   Epoch: [39/300][140/200], lr: 0.00000020 	 loss = 0.0929(0.4969)
2023/11/21 17:34:40 - INFO - root -   Epoch: [39/300][160/200], lr: 0.00000020 	 loss = 0.6954(0.4933)
2023/11/21 17:34:56 - INFO - root -   Epoch: [39/300][180/200], lr: 0.00000020 	 loss = 0.1472(0.5140)
2023/11/21 17:35:03 - INFO - root -   Epoch: [39/300] 	 loss = 0.5124
2023/11/21 17:37:04 - INFO - root -   precision = 0.8515
2023/11/21 17:37:04 - INFO - root -   eval_loss = 0.4177
2023/11/21 17:37:04 - INFO - root -   eval_acc = 0.8515
2023/11/21 17:37:05 - INFO - root -   train_accuracy = 0.8375
2023/11/21 17:37:15 - INFO - root -   Epoch: [40/300][0/200], lr: 0.00000020 	 loss = 1.5892(1.5892)
2023/11/21 17:37:28 - INFO - root -   Epoch: [40/300][20/200], lr: 0.00000020 	 loss = 1.1776(0.5667)
2023/11/21 17:37:47 - INFO - root -   Epoch: [40/300][40/200], lr: 0.00000020 	 loss = 0.9941(0.5407)
2023/11/21 17:38:03 - INFO - root -   Epoch: [40/300][60/200], lr: 0.00000020 	 loss = 0.2845(0.5213)
2023/11/21 17:38:12 - INFO - root -   Epoch: [40/300][80/200], lr: 0.00000020 	 loss = 0.8770(0.5120)
2023/11/21 17:38:33 - INFO - root -   Epoch: [40/300][100/200], lr: 0.00000020 	 loss = 0.0730(0.5146)
2023/11/21 17:38:52 - INFO - root -   Epoch: [40/300][120/200], lr: 0.00000020 	 loss = 0.3357(0.5253)
2023/11/21 17:39:06 - INFO - root -   Epoch: [40/300][140/200], lr: 0.00000020 	 loss = 0.0648(0.5305)
2023/11/21 17:39:31 - INFO - root -   Epoch: [40/300][160/200], lr: 0.00000020 	 loss = 0.8326(0.5481)
2023/11/21 17:39:50 - INFO - root -   Epoch: [40/300][180/200], lr: 0.00000020 	 loss = 0.1256(0.5375)
2023/11/21 17:40:00 - INFO - root -   Epoch: [40/300] 	 loss = 0.5343
2023/11/21 17:40:00 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:40:01 - INFO - root -   Epoch: [41/300][0/200], lr: 0.00000020 	 loss = 0.9625(0.9625)
2023/11/21 17:40:23 - INFO - root -   Epoch: [41/300][20/200], lr: 0.00000020 	 loss = 1.8285(0.6331)
2023/11/21 17:40:32 - INFO - root -   Epoch: [41/300][40/200], lr: 0.00000020 	 loss = 0.6540(0.5479)
2023/11/21 17:40:41 - INFO - root -   Epoch: [41/300][60/200], lr: 0.00000020 	 loss = 0.4208(0.5241)
2023/11/21 17:40:57 - INFO - root -   Epoch: [41/300][80/200], lr: 0.00000020 	 loss = 1.3060(0.5088)
2023/11/21 17:41:09 - INFO - root -   Epoch: [41/300][100/200], lr: 0.00000020 	 loss = 0.0973(0.5341)
2023/11/21 17:41:23 - INFO - root -   Epoch: [41/300][120/200], lr: 0.00000020 	 loss = 0.2913(0.5392)
2023/11/21 17:41:38 - INFO - root -   Epoch: [41/300][140/200], lr: 0.00000020 	 loss = 0.2784(0.5327)
2023/11/21 17:41:54 - INFO - root -   Epoch: [41/300][160/200], lr: 0.00000020 	 loss = 1.1668(0.5420)
2023/11/21 17:42:13 - INFO - root -   Epoch: [41/300][180/200], lr: 0.00000020 	 loss = 0.3222(0.5237)
2023/11/21 17:42:23 - INFO - root -   Epoch: [41/300] 	 loss = 0.5139
2023/11/21 17:42:23 - INFO - root -   train_accuracy = 0.8275
2023/11/21 17:42:35 - INFO - root -   Epoch: [42/300][0/200], lr: 0.00000020 	 loss = 1.2110(1.2110)
2023/11/21 17:42:50 - INFO - root -   Epoch: [42/300][20/200], lr: 0.00000020 	 loss = 1.9424(0.6203)
2023/11/21 17:43:09 - INFO - root -   Epoch: [42/300][40/200], lr: 0.00000020 	 loss = 1.1575(0.5800)
2023/11/21 17:43:30 - INFO - root -   Epoch: [42/300][60/200], lr: 0.00000020 	 loss = 0.6138(0.5604)
2023/11/21 17:43:45 - INFO - root -   Epoch: [42/300][80/200], lr: 0.00000020 	 loss = 1.6500(0.5561)
2023/11/21 17:43:59 - INFO - root -   Epoch: [42/300][100/200], lr: 0.00000020 	 loss = 0.1928(0.5553)
2023/11/21 17:44:13 - INFO - root -   Epoch: [42/300][120/200], lr: 0.00000020 	 loss = 0.2342(0.5401)
2023/11/21 17:44:29 - INFO - root -   Epoch: [42/300][140/200], lr: 0.00000020 	 loss = 0.0469(0.5401)
2023/11/21 17:44:37 - INFO - root -   Epoch: [42/300][160/200], lr: 0.00000020 	 loss = 1.1794(0.5465)
2023/11/21 17:44:46 - INFO - root -   Epoch: [42/300][180/200], lr: 0.00000020 	 loss = 0.1750(0.5517)
2023/11/21 17:44:57 - INFO - root -   Epoch: [42/300] 	 loss = 0.5501
2023/11/21 17:44:57 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:44:58 - INFO - root -   Epoch: [43/300][0/200], lr: 0.00000021 	 loss = 1.1213(1.1213)
2023/11/21 17:45:15 - INFO - root -   Epoch: [43/300][20/200], lr: 0.00000021 	 loss = 1.5472(0.5296)
2023/11/21 17:45:26 - INFO - root -   Epoch: [43/300][40/200], lr: 0.00000021 	 loss = 0.6852(0.4747)
2023/11/21 17:45:44 - INFO - root -   Epoch: [43/300][60/200], lr: 0.00000021 	 loss = 0.0931(0.4793)
2023/11/21 17:45:56 - INFO - root -   Epoch: [43/300][80/200], lr: 0.00000021 	 loss = 0.8340(0.4720)
2023/11/21 17:46:15 - INFO - root -   Epoch: [43/300][100/200], lr: 0.00000021 	 loss = 0.0763(0.4781)
2023/11/21 17:46:23 - INFO - root -   Epoch: [43/300][120/200], lr: 0.00000021 	 loss = 0.2279(0.4821)
2023/11/21 17:46:35 - INFO - root -   Epoch: [43/300][140/200], lr: 0.00000021 	 loss = 0.2729(0.4746)
2023/11/21 17:46:49 - INFO - root -   Epoch: [43/300][160/200], lr: 0.00000021 	 loss = 1.0508(0.4904)
2023/11/21 17:47:03 - INFO - root -   Epoch: [43/300][180/200], lr: 0.00000021 	 loss = 0.1576(0.4852)
2023/11/21 17:47:15 - INFO - root -   Epoch: [43/300] 	 loss = 0.4854
2023/11/21 17:47:15 - INFO - root -   train_accuracy = 0.8300
2023/11/21 17:47:16 - INFO - root -   Epoch: [44/300][0/200], lr: 0.00000021 	 loss = 0.9029(0.9029)
2023/11/21 17:47:30 - INFO - root -   Epoch: [44/300][20/200], lr: 0.00000021 	 loss = 1.0648(0.4845)
2023/11/21 17:47:55 - INFO - root -   Epoch: [44/300][40/200], lr: 0.00000021 	 loss = 0.5995(0.4040)
2023/11/21 17:48:10 - INFO - root -   Epoch: [44/300][60/200], lr: 0.00000021 	 loss = 0.4819(0.4155)
2023/11/21 17:48:24 - INFO - root -   Epoch: [44/300][80/200], lr: 0.00000021 	 loss = 2.7180(0.4633)
2023/11/21 17:48:36 - INFO - root -   Epoch: [44/300][100/200], lr: 0.00000021 	 loss = 0.0800(0.4818)
2023/11/21 17:48:50 - INFO - root -   Epoch: [44/300][120/200], lr: 0.00000021 	 loss = 0.1723(0.4858)
2023/11/21 17:49:04 - INFO - root -   Epoch: [44/300][140/200], lr: 0.00000021 	 loss = 0.0961(0.4878)
2023/11/21 17:49:17 - INFO - root -   Epoch: [44/300][160/200], lr: 0.00000021 	 loss = 1.6036(0.5015)
2023/11/21 17:49:28 - INFO - root -   Epoch: [44/300][180/200], lr: 0.00000021 	 loss = 0.1202(0.5012)
2023/11/21 17:49:40 - INFO - root -   Epoch: [44/300] 	 loss = 0.5012
2023/11/21 17:51:41 - INFO - root -   precision = 0.8515
2023/11/21 17:51:41 - INFO - root -   eval_loss = 0.4218
2023/11/21 17:51:41 - INFO - root -   eval_acc = 0.8515
2023/11/21 17:51:42 - INFO - root -   train_accuracy = 0.8150
2023/11/21 17:51:43 - INFO - root -   Epoch: [45/300][0/200], lr: 0.00000021 	 loss = 1.4962(1.4962)
2023/11/21 17:52:07 - INFO - root -   Epoch: [45/300][20/200], lr: 0.00000021 	 loss = 1.3913(0.5061)
2023/11/21 17:52:27 - INFO - root -   Epoch: [45/300][40/200], lr: 0.00000021 	 loss = 1.1070(0.5294)
2023/11/21 17:52:39 - INFO - root -   Epoch: [45/300][60/200], lr: 0.00000021 	 loss = 0.2545(0.4817)
2023/11/21 17:52:55 - INFO - root -   Epoch: [45/300][80/200], lr: 0.00000021 	 loss = 1.6360(0.4833)
2023/11/21 17:53:09 - INFO - root -   Epoch: [45/300][100/200], lr: 0.00000021 	 loss = 0.0778(0.4806)
2023/11/21 17:53:28 - INFO - root -   Epoch: [45/300][120/200], lr: 0.00000021 	 loss = 0.1158(0.4878)
2023/11/21 17:53:36 - INFO - root -   Epoch: [45/300][140/200], lr: 0.00000021 	 loss = 0.2496(0.4911)
2023/11/21 17:53:49 - INFO - root -   Epoch: [45/300][160/200], lr: 0.00000021 	 loss = 1.5501(0.5065)
2023/11/21 17:54:09 - INFO - root -   Epoch: [45/300][180/200], lr: 0.00000021 	 loss = 0.2113(0.5040)
2023/11/21 17:54:16 - INFO - root -   Epoch: [45/300] 	 loss = 0.4990
2023/11/21 17:54:16 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:54:24 - INFO - root -   Epoch: [46/300][0/200], lr: 0.00000021 	 loss = 1.8479(1.8479)
2023/11/21 17:54:38 - INFO - root -   Epoch: [46/300][20/200], lr: 0.00000021 	 loss = 1.4304(0.6547)
2023/11/21 17:54:51 - INFO - root -   Epoch: [46/300][40/200], lr: 0.00000021 	 loss = 0.6513(0.5463)
2023/11/21 17:55:09 - INFO - root -   Epoch: [46/300][60/200], lr: 0.00000021 	 loss = 0.5108(0.4940)
2023/11/21 17:55:23 - INFO - root -   Epoch: [46/300][80/200], lr: 0.00000021 	 loss = 0.7563(0.4746)
2023/11/21 17:55:41 - INFO - root -   Epoch: [46/300][100/200], lr: 0.00000021 	 loss = 0.0630(0.4810)
2023/11/21 17:55:52 - INFO - root -   Epoch: [46/300][120/200], lr: 0.00000021 	 loss = 0.0558(0.4869)
2023/11/21 17:56:06 - INFO - root -   Epoch: [46/300][140/200], lr: 0.00000021 	 loss = 0.1865(0.4758)
2023/11/21 17:56:19 - INFO - root -   Epoch: [46/300][160/200], lr: 0.00000021 	 loss = 1.2762(0.4895)
2023/11/21 17:56:32 - INFO - root -   Epoch: [46/300][180/200], lr: 0.00000021 	 loss = 0.4706(0.4962)
2023/11/21 17:56:42 - INFO - root -   Epoch: [46/300] 	 loss = 0.5113
2023/11/21 17:56:42 - INFO - root -   train_accuracy = 0.8075
2023/11/21 17:56:43 - INFO - root -   Epoch: [47/300][0/200], lr: 0.00000022 	 loss = 1.3721(1.3721)
2023/11/21 17:57:10 - INFO - root -   Epoch: [47/300][20/200], lr: 0.00000022 	 loss = 1.3484(0.6005)
2023/11/21 17:57:30 - INFO - root -   Epoch: [47/300][40/200], lr: 0.00000022 	 loss = 1.0715(0.5514)
2023/11/21 17:57:44 - INFO - root -   Epoch: [47/300][60/200], lr: 0.00000022 	 loss = 0.8685(0.5183)
2023/11/21 17:58:01 - INFO - root -   Epoch: [47/300][80/200], lr: 0.00000022 	 loss = 1.8689(0.5090)
2023/11/21 17:58:09 - INFO - root -   Epoch: [47/300][100/200], lr: 0.00000022 	 loss = 0.1711(0.5352)
2023/11/21 17:58:26 - INFO - root -   Epoch: [47/300][120/200], lr: 0.00000022 	 loss = 0.1013(0.5288)
2023/11/21 17:58:46 - INFO - root -   Epoch: [47/300][140/200], lr: 0.00000022 	 loss = 0.1210(0.5186)
2023/11/21 17:58:58 - INFO - root -   Epoch: [47/300][160/200], lr: 0.00000022 	 loss = 0.6301(0.5182)
2023/11/21 17:59:13 - INFO - root -   Epoch: [47/300][180/200], lr: 0.00000022 	 loss = 0.4627(0.5280)
2023/11/21 17:59:27 - INFO - root -   Epoch: [47/300] 	 loss = 0.5283
2023/11/21 17:59:27 - INFO - root -   train_accuracy = 0.8200
2023/11/21 17:59:36 - INFO - root -   Epoch: [48/300][0/200], lr: 0.00000022 	 loss = 1.5433(1.5433)
2023/11/21 17:59:52 - INFO - root -   Epoch: [48/300][20/200], lr: 0.00000022 	 loss = 0.9795(0.4610)
2023/11/21 18:00:02 - INFO - root -   Epoch: [48/300][40/200], lr: 0.00000022 	 loss = 0.5785(0.4613)
2023/11/21 18:00:19 - INFO - root -   Epoch: [48/300][60/200], lr: 0.00000022 	 loss = 1.1492(0.4233)
2023/11/21 18:00:26 - INFO - root -   Epoch: [48/300][80/200], lr: 0.00000022 	 loss = 0.5574(0.4473)
2023/11/21 18:00:40 - INFO - root -   Epoch: [48/300][100/200], lr: 0.00000022 	 loss = 0.0400(0.4699)
2023/11/21 18:00:50 - INFO - root -   Epoch: [48/300][120/200], lr: 0.00000022 	 loss = 0.3670(0.4849)
2023/11/21 18:01:13 - INFO - root -   Epoch: [48/300][140/200], lr: 0.00000022 	 loss = 0.3633(0.4931)
2023/11/21 18:01:31 - INFO - root -   Epoch: [48/300][160/200], lr: 0.00000022 	 loss = 1.1042(0.5259)
2023/11/21 18:01:51 - INFO - root -   Epoch: [48/300][180/200], lr: 0.00000022 	 loss = 0.4798(0.5377)
2023/11/21 18:02:01 - INFO - root -   Epoch: [48/300] 	 loss = 0.5362
2023/11/21 18:02:01 - INFO - root -   train_accuracy = 0.8225
2023/11/21 18:02:02 - INFO - root -   Epoch: [49/300][0/200], lr: 0.00000022 	 loss = 2.0832(2.0832)
2023/11/21 18:02:20 - INFO - root -   Epoch: [49/300][20/200], lr: 0.00000022 	 loss = 1.2432(0.6424)
2023/11/21 18:02:35 - INFO - root -   Epoch: [49/300][40/200], lr: 0.00000022 	 loss = 0.7107(0.5185)
2023/11/21 18:02:46 - INFO - root -   Epoch: [49/300][60/200], lr: 0.00000022 	 loss = 0.5480(0.4971)
2023/11/21 18:03:00 - INFO - root -   Epoch: [49/300][80/200], lr: 0.00000022 	 loss = 0.7727(0.5117)
2023/11/21 18:03:19 - INFO - root -   Epoch: [49/300][100/200], lr: 0.00000022 	 loss = 0.0504(0.5098)
2023/11/21 18:03:32 - INFO - root -   Epoch: [49/300][120/200], lr: 0.00000022 	 loss = 0.1612(0.5074)
2023/11/21 18:03:45 - INFO - root -   Epoch: [49/300][140/200], lr: 0.00000022 	 loss = 0.2189(0.4842)
2023/11/21 18:03:58 - INFO - root -   Epoch: [49/300][160/200], lr: 0.00000022 	 loss = 0.7767(0.4844)
2023/11/21 18:04:12 - INFO - root -   Epoch: [49/300][180/200], lr: 0.00000022 	 loss = 0.0606(0.4930)
2023/11/21 18:04:20 - INFO - root -   Epoch: [49/300] 	 loss = 0.4951
2023/11/21 18:06:22 - INFO - root -   precision = 0.8614
2023/11/21 18:06:22 - INFO - root -   eval_loss = 0.4120
2023/11/21 18:06:22 - INFO - root -   eval_acc = 0.8614
2023/11/21 18:06:23 - INFO - root -   train_accuracy = 0.8050
2023/11/21 18:06:34 - INFO - root -   Epoch: [50/300][0/200], lr: 0.00000022 	 loss = 1.9560(1.9560)
2023/11/21 18:06:50 - INFO - root -   Epoch: [50/300][20/200], lr: 0.00000022 	 loss = 0.7479(0.6582)
2023/11/21 18:07:11 - INFO - root -   Epoch: [50/300][40/200], lr: 0.00000022 	 loss = 1.5080(0.6064)
2023/11/21 18:07:25 - INFO - root -   Epoch: [50/300][60/200], lr: 0.00000022 	 loss = 0.1847(0.5455)
2023/11/21 18:07:33 - INFO - root -   Epoch: [50/300][80/200], lr: 0.00000022 	 loss = 1.9208(0.5295)
2023/11/21 18:07:55 - INFO - root -   Epoch: [50/300][100/200], lr: 0.00000022 	 loss = 0.1738(0.5200)
2023/11/21 18:08:21 - INFO - root -   Epoch: [50/300][120/200], lr: 0.00000022 	 loss = 0.4775(0.5422)
2023/11/21 18:08:38 - INFO - root -   Epoch: [50/300][140/200], lr: 0.00000022 	 loss = 0.3213(0.5280)
2023/11/21 18:08:49 - INFO - root -   Epoch: [50/300][160/200], lr: 0.00000022 	 loss = 0.9615(0.5313)
2023/11/21 18:09:09 - INFO - root -   Epoch: [50/300][180/200], lr: 0.00000022 	 loss = 0.2738(0.5225)
2023/11/21 18:09:17 - INFO - root -   Epoch: [50/300] 	 loss = 0.5319
2023/11/21 18:09:17 - INFO - root -   train_accuracy = 0.8250
2023/11/21 18:09:18 - INFO - root -   Epoch: [51/300][0/200], lr: 0.00000023 	 loss = 0.7630(0.7630)
2023/11/21 18:09:40 - INFO - root -   Epoch: [51/300][20/200], lr: 0.00000023 	 loss = 1.1497(0.5414)
2023/11/21 18:09:53 - INFO - root -   Epoch: [51/300][40/200], lr: 0.00000023 	 loss = 0.8866(0.5000)
2023/11/21 18:10:08 - INFO - root -   Epoch: [51/300][60/200], lr: 0.00000023 	 loss = 0.4489(0.4658)
2023/11/21 18:10:18 - INFO - root -   Epoch: [51/300][80/200], lr: 0.00000023 	 loss = 0.8225(0.4633)
2023/11/21 18:10:33 - INFO - root -   Epoch: [51/300][100/200], lr: 0.00000023 	 loss = 0.1007(0.4930)
2023/11/21 18:10:48 - INFO - root -   Epoch: [51/300][120/200], lr: 0.00000023 	 loss = 0.3377(0.5012)
2023/11/21 18:11:06 - INFO - root -   Epoch: [51/300][140/200], lr: 0.00000023 	 loss = 0.3798(0.4828)
2023/11/21 18:11:27 - INFO - root -   Epoch: [51/300][160/200], lr: 0.00000023 	 loss = 1.0406(0.4962)
2023/11/21 18:11:37 - INFO - root -   Epoch: [51/300][180/200], lr: 0.00000023 	 loss = 0.2317(0.5087)
2023/11/21 18:11:48 - INFO - root -   Epoch: [51/300] 	 loss = 0.5106
2023/11/21 18:11:48 - INFO - root -   train_accuracy = 0.8275
2023/11/21 18:11:49 - INFO - root -   Epoch: [52/300][0/200], lr: 0.00000023 	 loss = 0.9282(0.9282)
2023/11/21 18:12:11 - INFO - root -   Epoch: [52/300][20/200], lr: 0.00000023 	 loss = 2.5931(0.8616)
2023/11/21 18:12:24 - INFO - root -   Epoch: [52/300][40/200], lr: 0.00000023 	 loss = 1.2193(0.6313)
2023/11/21 18:12:42 - INFO - root -   Epoch: [52/300][60/200], lr: 0.00000023 	 loss = 0.4318(0.5903)
2023/11/21 18:12:55 - INFO - root -   Epoch: [52/300][80/200], lr: 0.00000023 	 loss = 0.7484(0.5730)
2023/11/21 18:13:03 - INFO - root -   Epoch: [52/300][100/200], lr: 0.00000023 	 loss = 0.0808(0.5541)
2023/11/21 18:13:18 - INFO - root -   Epoch: [52/300][120/200], lr: 0.00000023 	 loss = 0.1019(0.5279)
2023/11/21 18:13:26 - INFO - root -   Epoch: [52/300][140/200], lr: 0.00000023 	 loss = 0.2129(0.5319)
2023/11/21 18:13:46 - INFO - root -   Epoch: [52/300][160/200], lr: 0.00000023 	 loss = 1.0940(0.5426)
2023/11/21 18:14:02 - INFO - root -   Epoch: [52/300][180/200], lr: 0.00000023 	 loss = 1.1029(0.5375)
2023/11/21 18:14:20 - INFO - root -   Epoch: [52/300] 	 loss = 0.5344
2023/11/21 18:14:20 - INFO - root -   train_accuracy = 0.8275
2023/11/21 18:14:27 - INFO - root -   Epoch: [53/300][0/200], lr: 0.00000023 	 loss = 2.0478(2.0478)
2023/11/21 18:14:50 - INFO - root -   Epoch: [53/300][20/200], lr: 0.00000023 	 loss = 1.3850(0.7360)
2023/11/21 18:15:09 - INFO - root -   Epoch: [53/300][40/200], lr: 0.00000023 	 loss = 1.8856(0.6137)
2023/11/21 18:15:25 - INFO - root -   Epoch: [53/300][60/200], lr: 0.00000023 	 loss = 0.8506(0.5426)
2023/11/21 18:15:47 - INFO - root -   Epoch: [53/300][80/200], lr: 0.00000023 	 loss = 1.6163(0.5288)
2023/11/21 18:15:55 - INFO - root -   Epoch: [53/300][100/200], lr: 0.00000023 	 loss = 0.0400(0.5269)
2023/11/21 18:16:11 - INFO - root -   Epoch: [53/300][120/200], lr: 0.00000023 	 loss = 0.9674(0.5273)
2023/11/21 18:16:29 - INFO - root -   Epoch: [53/300][140/200], lr: 0.00000023 	 loss = 0.3150(0.5227)
2023/11/21 18:16:44 - INFO - root -   Epoch: [53/300][160/200], lr: 0.00000023 	 loss = 0.4183(0.5310)
2023/11/21 18:17:05 - INFO - root -   Epoch: [53/300][180/200], lr: 0.00000023 	 loss = 0.2134(0.5376)
2023/11/21 18:17:12 - INFO - root -   Epoch: [53/300] 	 loss = 0.5280
2023/11/21 18:17:12 - INFO - root -   train_accuracy = 0.8200
2023/11/21 18:17:20 - INFO - root -   Epoch: [54/300][0/200], lr: 0.00000023 	 loss = 1.3966(1.3966)
2023/11/21 18:17:42 - INFO - root -   Epoch: [54/300][20/200], lr: 0.00000023 	 loss = 2.1117(0.5382)
2023/11/21 18:17:54 - INFO - root -   Epoch: [54/300][40/200], lr: 0.00000023 	 loss = 1.2122(0.5110)
2023/11/21 18:18:08 - INFO - root -   Epoch: [54/300][60/200], lr: 0.00000023 	 loss = 0.2769(0.4935)
2023/11/21 18:18:22 - INFO - root -   Epoch: [54/300][80/200], lr: 0.00000023 	 loss = 1.5984(0.5207)
2023/11/21 18:18:35 - INFO - root -   Epoch: [54/300][100/200], lr: 0.00000023 	 loss = 0.0709(0.5153)
2023/11/21 18:18:48 - INFO - root -   Epoch: [54/300][120/200], lr: 0.00000023 	 loss = 0.3692(0.5049)
2023/11/21 18:18:59 - INFO - root -   Epoch: [54/300][140/200], lr: 0.00000023 	 loss = 0.4581(0.4974)
2023/11/21 18:19:19 - INFO - root -   Epoch: [54/300][160/200], lr: 0.00000023 	 loss = 1.2011(0.5192)
2023/11/21 18:19:43 - INFO - root -   Epoch: [54/300][180/200], lr: 0.00000023 	 loss = 0.2028(0.5213)
2023/11/21 18:19:53 - INFO - root -   Epoch: [54/300] 	 loss = 0.5226
2023/11/21 18:21:55 - INFO - root -   precision = 0.8614
2023/11/21 18:21:55 - INFO - root -   eval_loss = 0.4213
2023/11/21 18:21:55 - INFO - root -   eval_acc = 0.8614
2023/11/21 18:21:56 - INFO - root -   train_accuracy = 0.8325
2023/11/21 18:22:08 - INFO - root -   Epoch: [55/300][0/200], lr: 0.00000024 	 loss = 0.4695(0.4695)
2023/11/21 18:22:24 - INFO - root -   Epoch: [55/300][20/200], lr: 0.00000024 	 loss = 1.6537(0.4627)
2023/11/21 18:22:44 - INFO - root -   Epoch: [55/300][40/200], lr: 0.00000024 	 loss = 1.4082(0.4582)
2023/11/21 18:22:52 - INFO - root -   Epoch: [55/300][60/200], lr: 0.00000024 	 loss = 0.1887(0.4682)
2023/11/21 18:23:05 - INFO - root -   Epoch: [55/300][80/200], lr: 0.00000024 	 loss = 1.1747(0.4679)
2023/11/21 18:23:24 - INFO - root -   Epoch: [55/300][100/200], lr: 0.00000024 	 loss = 0.1384(0.4744)
2023/11/21 18:23:33 - INFO - root -   Epoch: [55/300][120/200], lr: 0.00000024 	 loss = 0.1690(0.4938)
2023/11/21 18:23:44 - INFO - root -   Epoch: [55/300][140/200], lr: 0.00000024 	 loss = 0.2015(0.4844)
2023/11/21 18:24:06 - INFO - root -   Epoch: [55/300][160/200], lr: 0.00000024 	 loss = 1.2919(0.5042)
2023/11/21 18:24:18 - INFO - root -   Epoch: [55/300][180/200], lr: 0.00000024 	 loss = 0.1823(0.5010)
2023/11/21 18:24:31 - INFO - root -   Epoch: [55/300] 	 loss = 0.4967
2023/11/21 18:24:31 - INFO - root -   train_accuracy = 0.8350
2023/11/21 18:24:32 - INFO - root -   Epoch: [56/300][0/200], lr: 0.00000024 	 loss = 1.5801(1.5801)
2023/11/21 18:24:58 - INFO - root -   Epoch: [56/300][20/200], lr: 0.00000024 	 loss = 1.3084(0.5225)
2023/11/21 18:25:16 - INFO - root -   Epoch: [56/300][40/200], lr: 0.00000024 	 loss = 0.6626(0.4956)
2023/11/21 18:25:29 - INFO - root -   Epoch: [56/300][60/200], lr: 0.00000024 	 loss = 0.3988(0.4617)
2023/11/21 18:25:43 - INFO - root -   Epoch: [56/300][80/200], lr: 0.00000024 	 loss = 0.5865(0.4479)
2023/11/21 18:26:04 - INFO - root -   Epoch: [56/300][100/200], lr: 0.00000024 	 loss = 0.1128(0.4597)
2023/11/21 18:26:22 - INFO - root -   Epoch: [56/300][120/200], lr: 0.00000024 	 loss = 0.0224(0.4664)
2023/11/21 18:26:35 - INFO - root -   Epoch: [56/300][140/200], lr: 0.00000024 	 loss = 0.1511(0.4738)
2023/11/21 18:26:50 - INFO - root -   Epoch: [56/300][160/200], lr: 0.00000024 	 loss = 0.5036(0.4883)
2023/11/21 18:27:05 - INFO - root -   Epoch: [56/300][180/200], lr: 0.00000024 	 loss = 0.1537(0.4952)
2023/11/21 18:27:18 - INFO - root -   Epoch: [56/300] 	 loss = 0.4991
2023/11/21 18:27:18 - INFO - root -   train_accuracy = 0.8375
2023/11/21 18:27:27 - INFO - root -   Epoch: [57/300][0/200], lr: 0.00000024 	 loss = 1.8691(1.8691)
2023/11/21 18:27:40 - INFO - root -   Epoch: [57/300][20/200], lr: 0.00000024 	 loss = 1.5282(0.7686)
2023/11/21 18:27:56 - INFO - root -   Epoch: [57/300][40/200], lr: 0.00000024 	 loss = 0.4840(0.6114)
2023/11/21 18:28:12 - INFO - root -   Epoch: [57/300][60/200], lr: 0.00000024 	 loss = 0.1824(0.5407)
2023/11/21 18:28:28 - INFO - root -   Epoch: [57/300][80/200], lr: 0.00000024 	 loss = 1.1164(0.5129)
2023/11/21 18:28:43 - INFO - root -   Epoch: [57/300][100/200], lr: 0.00000024 	 loss = 0.6025(0.5166)
2023/11/21 18:28:51 - INFO - root -   Epoch: [57/300][120/200], lr: 0.00000024 	 loss = 0.2443(0.5095)
2023/11/21 18:29:05 - INFO - root -   Epoch: [57/300][140/200], lr: 0.00000024 	 loss = 0.0759(0.5089)
2023/11/21 18:29:22 - INFO - root -   Epoch: [57/300][160/200], lr: 0.00000024 	 loss = 0.4020(0.5229)
2023/11/21 18:29:39 - INFO - root -   Epoch: [57/300][180/200], lr: 0.00000024 	 loss = 0.2829(0.5317)
2023/11/21 18:29:49 - INFO - root -   Epoch: [57/300] 	 loss = 0.5387
2023/11/21 18:29:49 - INFO - root -   train_accuracy = 0.8325
2023/11/21 18:30:01 - INFO - root -   Epoch: [58/300][0/200], lr: 0.00000024 	 loss = 1.5038(1.5038)
2023/11/21 18:30:18 - INFO - root -   Epoch: [58/300][20/200], lr: 0.00000024 	 loss = 0.8274(0.5335)
2023/11/21 18:30:27 - INFO - root -   Epoch: [58/300][40/200], lr: 0.00000024 	 loss = 1.4277(0.5009)
2023/11/21 18:30:40 - INFO - root -   Epoch: [58/300][60/200], lr: 0.00000024 	 loss = 0.8145(0.4548)
2023/11/21 18:30:59 - INFO - root -   Epoch: [58/300][80/200], lr: 0.00000024 	 loss = 1.7628(0.4653)
2023/11/21 18:31:07 - INFO - root -   Epoch: [58/300][100/200], lr: 0.00000024 	 loss = 0.1096(0.4578)
2023/11/21 18:31:21 - INFO - root -   Epoch: [58/300][120/200], lr: 0.00000024 	 loss = 0.1065(0.4576)
2023/11/21 18:31:42 - INFO - root -   Epoch: [58/300][140/200], lr: 0.00000024 	 loss = 0.2330(0.4542)
2023/11/21 18:32:05 - INFO - root -   Epoch: [58/300][160/200], lr: 0.00000024 	 loss = 0.4526(0.4590)
2023/11/21 18:32:18 - INFO - root -   Epoch: [58/300][180/200], lr: 0.00000024 	 loss = 0.0934(0.4655)
2023/11/21 18:32:30 - INFO - root -   Epoch: [58/300] 	 loss = 0.4695
2023/11/21 18:32:30 - INFO - root -   train_accuracy = 0.8400
2023/11/21 18:32:40 - INFO - root -   Epoch: [59/300][0/200], lr: 0.00000025 	 loss = 1.8049(1.8049)
2023/11/21 18:33:01 - INFO - root -   Epoch: [59/300][20/200], lr: 0.00000025 	 loss = 1.3409(0.6724)
2023/11/21 18:33:12 - INFO - root -   Epoch: [59/300][40/200], lr: 0.00000025 	 loss = 0.9235(0.5440)
2023/11/21 18:33:37 - INFO - root -   Epoch: [59/300][60/200], lr: 0.00000025 	 loss = 1.0862(0.4972)
2023/11/21 18:33:49 - INFO - root -   Epoch: [59/300][80/200], lr: 0.00000025 	 loss = 1.2754(0.4913)
2023/11/21 18:34:04 - INFO - root -   Epoch: [59/300][100/200], lr: 0.00000025 	 loss = 0.1236(0.4792)
2023/11/21 18:34:18 - INFO - root -   Epoch: [59/300][120/200], lr: 0.00000025 	 loss = 0.1398(0.4588)
2023/11/21 18:34:39 - INFO - root -   Epoch: [59/300][140/200], lr: 0.00000025 	 loss = 0.1684(0.4602)
2023/11/21 18:34:48 - INFO - root -   Epoch: [59/300][160/200], lr: 0.00000025 	 loss = 0.3015(0.4829)
2023/11/21 18:35:03 - INFO - root -   Epoch: [59/300][180/200], lr: 0.00000025 	 loss = 0.3878(0.4904)
2023/11/21 18:35:15 - INFO - root -   Epoch: [59/300] 	 loss = 0.4874
2023/11/21 18:37:16 - INFO - root -   precision = 0.8614
2023/11/21 18:37:16 - INFO - root -   eval_loss = 0.4079
2023/11/21 18:37:16 - INFO - root -   eval_acc = 0.8614
2023/11/21 18:37:17 - INFO - root -   train_accuracy = 0.8400
2023/11/21 18:37:18 - INFO - root -   Epoch: [60/300][0/200], lr: 0.00000025 	 loss = 1.3390(1.3390)
2023/11/21 18:37:44 - INFO - root -   Epoch: [60/300][20/200], lr: 0.00000025 	 loss = 2.5610(0.5818)
2023/11/21 18:37:52 - INFO - root -   Epoch: [60/300][40/200], lr: 0.00000025 	 loss = 0.2672(0.4863)
2023/11/21 18:38:11 - INFO - root -   Epoch: [60/300][60/200], lr: 0.00000025 	 loss = 0.2350(0.4465)
2023/11/21 18:38:27 - INFO - root -   Epoch: [60/300][80/200], lr: 0.00000025 	 loss = 1.4561(0.4424)
2023/11/21 18:38:41 - INFO - root -   Epoch: [60/300][100/200], lr: 0.00000025 	 loss = 0.0880(0.4780)
2023/11/21 18:38:56 - INFO - root -   Epoch: [60/300][120/200], lr: 0.00000025 	 loss = 0.1889(0.4939)
2023/11/21 18:39:11 - INFO - root -   Epoch: [60/300][140/200], lr: 0.00000025 	 loss = 0.1999(0.4961)
2023/11/21 18:39:29 - INFO - root -   Epoch: [60/300][160/200], lr: 0.00000025 	 loss = 0.9618(0.4965)
2023/11/21 18:39:43 - INFO - root -   Epoch: [60/300][180/200], lr: 0.00000025 	 loss = 0.2283(0.5060)
2023/11/21 18:39:58 - INFO - root -   Epoch: [60/300] 	 loss = 0.5066
2023/11/21 18:39:58 - INFO - root -   train_accuracy = 0.8375
2023/11/21 18:40:13 - INFO - root -   Epoch: [61/300][0/200], lr: 0.00000025 	 loss = 1.8162(1.8162)
2023/11/21 18:40:28 - INFO - root -   Epoch: [61/300][20/200], lr: 0.00000025 	 loss = 0.9034(0.7112)
2023/11/21 18:40:44 - INFO - root -   Epoch: [61/300][40/200], lr: 0.00000025 	 loss = 0.9528(0.5522)
2023/11/21 18:40:59 - INFO - root -   Epoch: [61/300][60/200], lr: 0.00000025 	 loss = 0.6519(0.5235)
2023/11/21 18:41:12 - INFO - root -   Epoch: [61/300][80/200], lr: 0.00000025 	 loss = 1.4150(0.5004)
2023/11/21 18:41:32 - INFO - root -   Epoch: [61/300][100/200], lr: 0.00000025 	 loss = 0.0801(0.5073)
2023/11/21 18:41:44 - INFO - root -   Epoch: [61/300][120/200], lr: 0.00000025 	 loss = 0.3627(0.5082)
2023/11/21 18:41:57 - INFO - root -   Epoch: [61/300][140/200], lr: 0.00000025 	 loss = 0.0577(0.4838)
2023/11/21 18:42:15 - INFO - root -   Epoch: [61/300][160/200], lr: 0.00000025 	 loss = 0.7119(0.5249)
2023/11/21 18:42:29 - INFO - root -   Epoch: [61/300][180/200], lr: 0.00000025 	 loss = 0.1934(0.5094)
2023/11/21 18:42:37 - INFO - root -   Epoch: [61/300] 	 loss = 0.5110
2023/11/21 18:42:37 - INFO - root -   train_accuracy = 0.8375
2023/11/21 18:42:39 - INFO - root -   Epoch: [62/300][0/200], lr: 0.00000025 	 loss = 1.8488(1.8488)
2023/11/21 18:42:59 - INFO - root -   Epoch: [62/300][20/200], lr: 0.00000025 	 loss = 1.1339(0.5117)
2023/11/21 18:43:21 - INFO - root -   Epoch: [62/300][40/200], lr: 0.00000025 	 loss = 1.5026(0.5169)
2023/11/21 18:43:29 - INFO - root -   Epoch: [62/300][60/200], lr: 0.00000025 	 loss = 0.2927(0.4918)
2023/11/21 18:43:42 - INFO - root -   Epoch: [62/300][80/200], lr: 0.00000025 	 loss = 1.8870(0.5204)
2023/11/21 18:43:54 - INFO - root -   Epoch: [62/300][100/200], lr: 0.00000025 	 loss = 0.0579(0.5181)
2023/11/21 18:44:08 - INFO - root -   Epoch: [62/300][120/200], lr: 0.00000025 	 loss = 0.2176(0.4977)
2023/11/21 18:44:30 - INFO - root -   Epoch: [62/300][140/200], lr: 0.00000025 	 loss = 0.1354(0.4984)
2023/11/21 18:44:52 - INFO - root -   Epoch: [62/300][160/200], lr: 0.00000025 	 loss = 0.2944(0.5129)
2023/11/21 18:45:02 - INFO - root -   Epoch: [62/300][180/200], lr: 0.00000025 	 loss = 0.0629(0.5071)
2023/11/21 18:45:10 - INFO - root -   Epoch: [62/300] 	 loss = 0.5088
2023/11/21 18:45:10 - INFO - root -   train_accuracy = 0.8200
2023/11/21 18:45:11 - INFO - root -   Epoch: [63/300][0/200], lr: 0.00000026 	 loss = 2.7758(2.7758)
2023/11/21 18:45:33 - INFO - root -   Epoch: [63/300][20/200], lr: 0.00000026 	 loss = 1.4957(0.7344)
2023/11/21 18:45:44 - INFO - root -   Epoch: [63/300][40/200], lr: 0.00000026 	 loss = 0.9502(0.5742)
2023/11/21 18:46:09 - INFO - root -   Epoch: [63/300][60/200], lr: 0.00000026 	 loss = 0.2070(0.5234)
2023/11/21 18:46:24 - INFO - root -   Epoch: [63/300][80/200], lr: 0.00000026 	 loss = 1.6656(0.5020)
2023/11/21 18:46:32 - INFO - root -   Epoch: [63/300][100/200], lr: 0.00000026 	 loss = 0.1340(0.5079)
2023/11/21 18:46:45 - INFO - root -   Epoch: [63/300][120/200], lr: 0.00000026 	 loss = 0.0923(0.5043)
2023/11/21 18:46:58 - INFO - root -   Epoch: [63/300][140/200], lr: 0.00000026 	 loss = 0.1018(0.4799)
2023/11/21 18:47:21 - INFO - root -   Epoch: [63/300][160/200], lr: 0.00000026 	 loss = 0.5010(0.4876)
2023/11/21 18:47:36 - INFO - root -   Epoch: [63/300][180/200], lr: 0.00000026 	 loss = 0.1261(0.4953)
2023/11/21 18:47:46 - INFO - root -   Epoch: [63/300] 	 loss = 0.5049
2023/11/21 18:47:46 - INFO - root -   train_accuracy = 0.8250
2023/11/21 18:47:48 - INFO - root -   Epoch: [64/300][0/200], lr: 0.00000026 	 loss = 1.1698(1.1698)
2023/11/21 18:48:09 - INFO - root -   Epoch: [64/300][20/200], lr: 0.00000026 	 loss = 1.5787(0.6531)
2023/11/21 18:48:29 - INFO - root -   Epoch: [64/300][40/200], lr: 0.00000026 	 loss = 0.6113(0.5991)
2023/11/21 18:48:44 - INFO - root -   Epoch: [64/300][60/200], lr: 0.00000026 	 loss = 0.5829(0.5199)
2023/11/21 18:48:59 - INFO - root -   Epoch: [64/300][80/200], lr: 0.00000026 	 loss = 2.2740(0.5324)
2023/11/21 18:49:13 - INFO - root -   Epoch: [64/300][100/200], lr: 0.00000026 	 loss = 0.0364(0.5156)
2023/11/21 18:49:25 - INFO - root -   Epoch: [64/300][120/200], lr: 0.00000026 	 loss = 0.1181(0.5295)
2023/11/21 18:49:43 - INFO - root -   Epoch: [64/300][140/200], lr: 0.00000026 	 loss = 0.3819(0.5314)
2023/11/21 18:50:01 - INFO - root -   Epoch: [64/300][160/200], lr: 0.00000026 	 loss = 1.2216(0.5298)
2023/11/21 18:50:20 - INFO - root -   Epoch: [64/300][180/200], lr: 0.00000026 	 loss = 0.1637(0.5279)
2023/11/21 18:50:33 - INFO - root -   Epoch: [64/300] 	 loss = 0.5333
2023/11/21 18:52:33 - INFO - root -   precision = 0.8515
2023/11/21 18:52:33 - INFO - root -   eval_loss = 0.4204
2023/11/21 18:52:33 - INFO - root -   eval_acc = 0.8515
2023/11/21 18:52:34 - INFO - root -   train_accuracy = 0.8200
2023/11/21 18:52:35 - INFO - root -   Epoch: [65/300][0/200], lr: 0.00000026 	 loss = 2.3657(2.3657)
2023/11/21 18:52:55 - INFO - root -   Epoch: [65/300][20/200], lr: 0.00000026 	 loss = 1.7095(0.6588)
2023/11/21 18:53:12 - INFO - root -   Epoch: [65/300][40/200], lr: 0.00000026 	 loss = 0.6111(0.5388)
2023/11/21 18:53:29 - INFO - root -   Epoch: [65/300][60/200], lr: 0.00000026 	 loss = 0.1642(0.4933)
2023/11/21 18:53:48 - INFO - root -   Epoch: [65/300][80/200], lr: 0.00000026 	 loss = 1.4985(0.4862)
2023/11/21 18:54:00 - INFO - root -   Epoch: [65/300][100/200], lr: 0.00000026 	 loss = 0.0975(0.4999)
2023/11/21 18:54:14 - INFO - root -   Epoch: [65/300][120/200], lr: 0.00000026 	 loss = 0.3292(0.4940)
2023/11/21 18:54:28 - INFO - root -   Epoch: [65/300][140/200], lr: 0.00000026 	 loss = 0.0698(0.4882)
2023/11/21 18:54:36 - INFO - root -   Epoch: [65/300][160/200], lr: 0.00000026 	 loss = 0.7948(0.4997)
2023/11/21 18:54:54 - INFO - root -   Epoch: [65/300][180/200], lr: 0.00000026 	 loss = 0.3467(0.4971)
2023/11/21 18:55:08 - INFO - root -   Epoch: [65/300] 	 loss = 0.5092
2023/11/21 18:55:08 - INFO - root -   train_accuracy = 0.8425
2023/11/21 18:55:19 - INFO - root -   Epoch: [66/300][0/200], lr: 0.00000026 	 loss = 1.2557(1.2557)
2023/11/21 18:55:27 - INFO - root -   Epoch: [66/300][20/200], lr: 0.00000026 	 loss = 1.5971(0.6082)
2023/11/21 18:55:37 - INFO - root -   Epoch: [66/300][40/200], lr: 0.00000026 	 loss = 1.3092(0.5337)
2023/11/21 18:55:55 - INFO - root -   Epoch: [66/300][60/200], lr: 0.00000026 	 loss = 0.2422(0.4841)
2023/11/21 18:56:09 - INFO - root -   Epoch: [66/300][80/200], lr: 0.00000026 	 loss = 0.5451(0.4903)
2023/11/21 18:56:18 - INFO - root -   Epoch: [66/300][100/200], lr: 0.00000026 	 loss = 0.1370(0.4929)
2023/11/21 18:56:33 - INFO - root -   Epoch: [66/300][120/200], lr: 0.00000026 	 loss = 0.0562(0.4848)
2023/11/21 18:56:46 - INFO - root -   Epoch: [66/300][140/200], lr: 0.00000026 	 loss = 0.2444(0.4735)
2023/11/21 18:57:07 - INFO - root -   Epoch: [66/300][160/200], lr: 0.00000026 	 loss = 0.4410(0.4941)
2023/11/21 18:57:16 - INFO - root -   Epoch: [66/300][180/200], lr: 0.00000026 	 loss = 0.2598(0.4886)
2023/11/21 18:57:36 - INFO - root -   Epoch: [66/300] 	 loss = 0.4962
2023/11/21 18:57:36 - INFO - root -   train_accuracy = 0.8375
2023/11/21 18:57:44 - INFO - root -   Epoch: [67/300][0/200], lr: 0.00000027 	 loss = 1.3577(1.3577)
2023/11/21 18:57:58 - INFO - root -   Epoch: [67/300][20/200], lr: 0.00000027 	 loss = 1.6721(0.6233)
2023/11/21 18:58:08 - INFO - root -   Epoch: [67/300][40/200], lr: 0.00000027 	 loss = 0.5348(0.5625)
2023/11/21 18:58:20 - INFO - root -   Epoch: [67/300][60/200], lr: 0.00000027 	 loss = 0.5170(0.4971)
2023/11/21 18:58:45 - INFO - root -   Epoch: [67/300][80/200], lr: 0.00000027 	 loss = 0.6404(0.4633)
2023/11/21 18:58:58 - INFO - root -   Epoch: [67/300][100/200], lr: 0.00000027 	 loss = 0.2319(0.4661)
2023/11/21 18:59:10 - INFO - root -   Epoch: [67/300][120/200], lr: 0.00000027 	 loss = 0.2448(0.4696)
2023/11/21 18:59:27 - INFO - root -   Epoch: [67/300][140/200], lr: 0.00000027 	 loss = 0.2992(0.4554)
2023/11/21 18:59:43 - INFO - root -   Epoch: [67/300][160/200], lr: 0.00000027 	 loss = 0.3902(0.4579)
2023/11/21 18:59:51 - INFO - root -   Epoch: [67/300][180/200], lr: 0.00000027 	 loss = 0.3511(0.4538)
2023/11/21 19:00:03 - INFO - root -   Epoch: [67/300] 	 loss = 0.4584
2023/11/21 19:00:03 - INFO - root -   train_accuracy = 0.8425
2023/11/21 19:00:05 - INFO - root -   Epoch: [68/300][0/200], lr: 0.00000027 	 loss = 1.5477(1.5477)
2023/11/21 19:00:30 - INFO - root -   Epoch: [68/300][20/200], lr: 0.00000027 	 loss = 0.7410(0.4635)
2023/11/21 19:00:49 - INFO - root -   Epoch: [68/300][40/200], lr: 0.00000027 	 loss = 0.8680(0.4859)
2023/11/21 19:01:02 - INFO - root -   Epoch: [68/300][60/200], lr: 0.00000027 	 loss = 0.2517(0.4289)
2023/11/21 19:01:27 - INFO - root -   Epoch: [68/300][80/200], lr: 0.00000027 	 loss = 1.7255(0.4471)
2023/11/21 19:01:35 - INFO - root -   Epoch: [68/300][100/200], lr: 0.00000027 	 loss = 0.0418(0.4449)
2023/11/21 19:01:53 - INFO - root -   Epoch: [68/300][120/200], lr: 0.00000027 	 loss = 0.1339(0.4368)
2023/11/21 19:02:03 - INFO - root -   Epoch: [68/300][140/200], lr: 0.00000027 	 loss = 0.0830(0.4484)
2023/11/21 19:02:22 - INFO - root -   Epoch: [68/300][160/200], lr: 0.00000027 	 loss = 0.8655(0.4538)
2023/11/21 19:02:38 - INFO - root -   Epoch: [68/300][180/200], lr: 0.00000027 	 loss = 0.1749(0.4476)
2023/11/21 19:02:47 - INFO - root -   Epoch: [68/300] 	 loss = 0.4552
2023/11/21 19:02:47 - INFO - root -   train_accuracy = 0.8425
2023/11/21 19:02:55 - INFO - root -   Epoch: [69/300][0/200], lr: 0.00000027 	 loss = 1.3657(1.3657)
2023/11/21 19:03:06 - INFO - root -   Epoch: [69/300][20/200], lr: 0.00000027 	 loss = 1.6189(0.6131)
2023/11/21 19:03:24 - INFO - root -   Epoch: [69/300][40/200], lr: 0.00000027 	 loss = 0.5784(0.4926)
2023/11/21 19:03:37 - INFO - root -   Epoch: [69/300][60/200], lr: 0.00000027 	 loss = 0.5358(0.4522)
2023/11/21 19:03:47 - INFO - root -   Epoch: [69/300][80/200], lr: 0.00000027 	 loss = 1.1785(0.4650)
2023/11/21 19:04:14 - INFO - root -   Epoch: [69/300][100/200], lr: 0.00000027 	 loss = 0.0431(0.4975)
2023/11/21 19:04:22 - INFO - root -   Epoch: [69/300][120/200], lr: 0.00000027 	 loss = 0.3114(0.4896)
2023/11/21 19:04:37 - INFO - root -   Epoch: [69/300][140/200], lr: 0.00000027 	 loss = 0.2655(0.4919)
2023/11/21 19:05:01 - INFO - root -   Epoch: [69/300][160/200], lr: 0.00000027 	 loss = 0.4211(0.5076)
2023/11/21 19:05:16 - INFO - root -   Epoch: [69/300][180/200], lr: 0.00000027 	 loss = 0.1534(0.5074)
2023/11/21 19:05:33 - INFO - root -   Epoch: [69/300] 	 loss = 0.5128
2023/11/21 19:07:35 - INFO - root -   precision = 0.8119
2023/11/21 19:07:35 - INFO - root -   eval_loss = 0.4288
2023/11/21 19:07:35 - INFO - root -   eval_acc = 0.8119
2023/11/21 19:07:36 - INFO - root -   train_accuracy = 0.8225
2023/11/21 19:07:37 - INFO - root -   Epoch: [70/300][0/200], lr: 0.00000027 	 loss = 0.9767(0.9767)
2023/11/21 19:07:56 - INFO - root -   Epoch: [70/300][20/200], lr: 0.00000027 	 loss = 1.4883(0.5004)
2023/11/21 19:08:04 - INFO - root -   Epoch: [70/300][40/200], lr: 0.00000027 	 loss = 0.2611(0.4568)
2023/11/21 19:08:19 - INFO - root -   Epoch: [70/300][60/200], lr: 0.00000027 	 loss = 0.5097(0.4383)
2023/11/21 19:08:34 - INFO - root -   Epoch: [70/300][80/200], lr: 0.00000027 	 loss = 0.6111(0.4680)
2023/11/21 19:08:47 - INFO - root -   Epoch: [70/300][100/200], lr: 0.00000027 	 loss = 0.0593(0.4657)
2023/11/21 19:08:59 - INFO - root -   Epoch: [70/300][120/200], lr: 0.00000027 	 loss = 0.1660(0.4844)
2023/11/21 19:09:15 - INFO - root -   Epoch: [70/300][140/200], lr: 0.00000027 	 loss = 0.1848(0.4720)
2023/11/21 19:09:28 - INFO - root -   Epoch: [70/300][160/200], lr: 0.00000027 	 loss = 1.9050(0.4907)
2023/11/21 19:09:41 - INFO - root -   Epoch: [70/300][180/200], lr: 0.00000027 	 loss = 0.2737(0.4681)
2023/11/21 19:09:49 - INFO - root -   Epoch: [70/300] 	 loss = 0.4730
2023/11/21 19:09:49 - INFO - root -   train_accuracy = 0.8400
2023/11/21 19:09:56 - INFO - root -   Epoch: [71/300][0/200], lr: 0.00000028 	 loss = 1.9471(1.9471)
2023/11/21 19:10:16 - INFO - root -   Epoch: [71/300][20/200], lr: 0.00000028 	 loss = 1.2423(0.5683)
2023/11/21 19:10:30 - INFO - root -   Epoch: [71/300][40/200], lr: 0.00000028 	 loss = 0.7646(0.5037)
2023/11/21 19:10:43 - INFO - root -   Epoch: [71/300][60/200], lr: 0.00000028 	 loss = 0.0972(0.4755)
2023/11/21 19:11:02 - INFO - root -   Epoch: [71/300][80/200], lr: 0.00000028 	 loss = 0.6586(0.4638)
2023/11/21 19:11:15 - INFO - root -   Epoch: [71/300][100/200], lr: 0.00000028 	 loss = 0.0743(0.4804)
2023/11/21 19:11:30 - INFO - root -   Epoch: [71/300][120/200], lr: 0.00000028 	 loss = 0.3830(0.4754)
2023/11/21 19:11:45 - INFO - root -   Epoch: [71/300][140/200], lr: 0.00000028 	 loss = 0.0792(0.4536)
2023/11/21 19:11:55 - INFO - root -   Epoch: [71/300][160/200], lr: 0.00000028 	 loss = 0.3566(0.4626)
2023/11/21 19:12:09 - INFO - root -   Epoch: [71/300][180/200], lr: 0.00000028 	 loss = 0.2481(0.4831)
2023/11/21 19:12:17 - INFO - root -   Epoch: [71/300] 	 loss = 0.4792
2023/11/21 19:12:17 - INFO - root -   train_accuracy = 0.8250
2023/11/21 19:12:19 - INFO - root -   Epoch: [72/300][0/200], lr: 0.00000028 	 loss = 1.1113(1.1113)
2023/11/21 19:12:34 - INFO - root -   Epoch: [72/300][20/200], lr: 0.00000028 	 loss = 1.5958(0.5595)
2023/11/21 19:12:50 - INFO - root -   Epoch: [72/300][40/200], lr: 0.00000028 	 loss = 0.5736(0.4641)
2023/11/21 19:13:01 - INFO - root -   Epoch: [72/300][60/200], lr: 0.00000028 	 loss = 0.0511(0.4479)
2023/11/21 19:13:11 - INFO - root -   Epoch: [72/300][80/200], lr: 0.00000028 	 loss = 1.1285(0.4564)
2023/11/21 19:13:22 - INFO - root -   Epoch: [72/300][100/200], lr: 0.00000028 	 loss = 0.2277(0.4524)
2023/11/21 19:13:40 - INFO - root -   Epoch: [72/300][120/200], lr: 0.00000028 	 loss = 0.2130(0.4470)
2023/11/21 19:13:50 - INFO - root -   Epoch: [72/300][140/200], lr: 0.00000028 	 loss = 0.0848(0.4454)
2023/11/21 19:14:03 - INFO - root -   Epoch: [72/300][160/200], lr: 0.00000028 	 loss = 1.3407(0.4627)
2023/11/21 19:14:13 - INFO - root -   Epoch: [72/300][180/200], lr: 0.00000028 	 loss = 0.1580(0.4601)
2023/11/21 19:14:27 - INFO - root -   Epoch: [72/300] 	 loss = 0.4661
2023/11/21 19:14:27 - INFO - root -   train_accuracy = 0.8425
2023/11/21 19:14:41 - INFO - root -   Epoch: [73/300][0/200], lr: 0.00000028 	 loss = 1.4846(1.4846)
2023/11/21 19:14:51 - INFO - root -   Epoch: [73/300][20/200], lr: 0.00000028 	 loss = 1.4033(0.6081)
2023/11/21 19:15:09 - INFO - root -   Epoch: [73/300][40/200], lr: 0.00000028 	 loss = 0.9219(0.5277)
2023/11/21 19:15:23 - INFO - root -   Epoch: [73/300][60/200], lr: 0.00000028 	 loss = 0.4346(0.4647)
2023/11/21 19:15:38 - INFO - root -   Epoch: [73/300][80/200], lr: 0.00000028 	 loss = 0.8573(0.4656)
2023/11/21 19:15:46 - INFO - root -   Epoch: [73/300][100/200], lr: 0.00000028 	 loss = 0.1437(0.4719)
2023/11/21 19:16:04 - INFO - root -   Epoch: [73/300][120/200], lr: 0.00000028 	 loss = 0.3601(0.4593)
2023/11/21 19:16:21 - INFO - root -   Epoch: [73/300][140/200], lr: 0.00000028 	 loss = 0.3530(0.4534)
2023/11/21 19:16:40 - INFO - root -   Epoch: [73/300][160/200], lr: 0.00000028 	 loss = 1.2581(0.4730)
2023/11/21 19:16:51 - INFO - root -   Epoch: [73/300][180/200], lr: 0.00000028 	 loss = 0.0679(0.4666)
2023/11/21 19:17:05 - INFO - root -   Epoch: [73/300] 	 loss = 0.4779
2023/11/21 19:17:05 - INFO - root -   train_accuracy = 0.8450
2023/11/21 19:17:11 - INFO - root -   Epoch: [74/300][0/200], lr: 0.00000028 	 loss = 1.4433(1.4433)
2023/11/21 19:17:22 - INFO - root -   Epoch: [74/300][20/200], lr: 0.00000028 	 loss = 1.6796(0.5524)
2023/11/21 19:17:38 - INFO - root -   Epoch: [74/300][40/200], lr: 0.00000028 	 loss = 1.0101(0.5881)
2023/11/21 19:17:56 - INFO - root -   Epoch: [74/300][60/200], lr: 0.00000028 	 loss = 0.0936(0.5317)
2023/11/21 19:18:10 - INFO - root -   Epoch: [74/300][80/200], lr: 0.00000028 	 loss = 1.1743(0.5251)
2023/11/21 19:18:26 - INFO - root -   Epoch: [74/300][100/200], lr: 0.00000028 	 loss = 0.0500(0.5276)
2023/11/21 19:18:40 - INFO - root -   Epoch: [74/300][120/200], lr: 0.00000028 	 loss = 0.0904(0.5135)
2023/11/21 19:18:57 - INFO - root -   Epoch: [74/300][140/200], lr: 0.00000028 	 loss = 0.1864(0.5022)
2023/11/21 19:19:24 - INFO - root -   Epoch: [74/300][160/200], lr: 0.00000028 	 loss = 1.2412(0.5020)
2023/11/21 19:19:35 - INFO - root -   Epoch: [74/300][180/200], lr: 0.00000028 	 loss = 0.1955(0.5034)
2023/11/21 19:19:42 - INFO - root -   Epoch: [74/300] 	 loss = 0.5044
2023/11/21 19:21:43 - INFO - root -   precision = 0.8515
2023/11/21 19:21:43 - INFO - root -   eval_loss = 0.4272
2023/11/21 19:21:43 - INFO - root -   eval_acc = 0.8515
2023/11/21 19:21:44 - INFO - root -   train_accuracy = 0.8200
2023/11/21 19:21:58 - INFO - root -   Epoch: [75/300][0/200], lr: 0.00000029 	 loss = 1.7501(1.7501)
2023/11/21 19:22:06 - INFO - root -   Epoch: [75/300][20/200], lr: 0.00000029 	 loss = 1.3387(0.6300)
2023/11/21 19:22:19 - INFO - root -   Epoch: [75/300][40/200], lr: 0.00000029 	 loss = 0.3709(0.4995)
2023/11/21 19:22:36 - INFO - root -   Epoch: [75/300][60/200], lr: 0.00000029 	 loss = 0.0855(0.4855)
2023/11/21 19:22:44 - INFO - root -   Epoch: [75/300][80/200], lr: 0.00000029 	 loss = 1.0990(0.4759)
2023/11/21 19:23:00 - INFO - root -   Epoch: [75/300][100/200], lr: 0.00000029 	 loss = 0.1771(0.4455)
2023/11/21 19:23:19 - INFO - root -   Epoch: [75/300][120/200], lr: 0.00000029 	 loss = 0.2256(0.4467)
2023/11/21 19:23:35 - INFO - root -   Epoch: [75/300][140/200], lr: 0.00000029 	 loss = 0.9488(0.4548)
2023/11/21 19:23:53 - INFO - root -   Epoch: [75/300][160/200], lr: 0.00000029 	 loss = 0.6443(0.4673)
2023/11/21 19:24:14 - INFO - root -   Epoch: [75/300][180/200], lr: 0.00000029 	 loss = 0.1377(0.4799)
2023/11/21 19:24:27 - INFO - root -   Epoch: [75/300] 	 loss = 0.4859
2023/11/21 19:24:27 - INFO - root -   train_accuracy = 0.8300
2023/11/21 19:24:28 - INFO - root -   Epoch: [76/300][0/200], lr: 0.00000029 	 loss = 1.6061(1.6061)
2023/11/21 19:24:48 - INFO - root -   Epoch: [76/300][20/200], lr: 0.00000029 	 loss = 0.8641(0.5118)
2023/11/21 19:25:07 - INFO - root -   Epoch: [76/300][40/200], lr: 0.00000029 	 loss = 0.5229(0.4521)
2023/11/21 19:25:24 - INFO - root -   Epoch: [76/300][60/200], lr: 0.00000029 	 loss = 0.2215(0.4349)
2023/11/21 19:25:34 - INFO - root -   Epoch: [76/300][80/200], lr: 0.00000029 	 loss = 1.2758(0.4542)
2023/11/21 19:25:51 - INFO - root -   Epoch: [76/300][100/200], lr: 0.00000029 	 loss = 0.0094(0.4578)
2023/11/21 19:26:08 - INFO - root -   Epoch: [76/300][120/200], lr: 0.00000029 	 loss = 0.1626(0.4686)
2023/11/21 19:26:24 - INFO - root -   Epoch: [76/300][140/200], lr: 0.00000029 	 loss = 0.0471(0.4690)
2023/11/21 19:26:37 - INFO - root -   Epoch: [76/300][160/200], lr: 0.00000029 	 loss = 0.5257(0.4876)
2023/11/21 19:26:51 - INFO - root -   Epoch: [76/300][180/200], lr: 0.00000029 	 loss = 0.0248(0.4864)
2023/11/21 19:27:03 - INFO - root -   Epoch: [76/300] 	 loss = 0.4819
2023/11/21 19:27:03 - INFO - root -   train_accuracy = 0.8275
2023/11/21 19:27:05 - INFO - root -   Epoch: [77/300][0/200], lr: 0.00000029 	 loss = 1.0454(1.0454)
2023/11/21 19:27:23 - INFO - root -   Epoch: [77/300][20/200], lr: 0.00000029 	 loss = 0.9261(0.4545)
2023/11/21 19:27:44 - INFO - root -   Epoch: [77/300][40/200], lr: 0.00000029 	 loss = 0.6615(0.4387)
2023/11/21 19:27:59 - INFO - root -   Epoch: [77/300][60/200], lr: 0.00000029 	 loss = 1.1790(0.4229)
2023/11/21 19:28:12 - INFO - root -   Epoch: [77/300][80/200], lr: 0.00000029 	 loss = 0.7761(0.4216)
2023/11/21 19:28:31 - INFO - root -   Epoch: [77/300][100/200], lr: 0.00000029 	 loss = 0.1752(0.4213)
2023/11/21 19:28:41 - INFO - root -   Epoch: [77/300][120/200], lr: 0.00000029 	 loss = 0.1956(0.4357)
2023/11/21 19:28:55 - INFO - root -   Epoch: [77/300][140/200], lr: 0.00000029 	 loss = 0.2377(0.4355)
2023/11/21 19:29:03 - INFO - root -   Epoch: [77/300][160/200], lr: 0.00000029 	 loss = 0.7940(0.4490)
2023/11/21 19:29:16 - INFO - root -   Epoch: [77/300][180/200], lr: 0.00000029 	 loss = 0.3760(0.4609)
2023/11/21 19:29:34 - INFO - root -   Epoch: [77/300] 	 loss = 0.4653
2023/11/21 19:29:34 - INFO - root -   train_accuracy = 0.8150
2023/11/21 19:29:35 - INFO - root -   Epoch: [78/300][0/200], lr: 0.00000029 	 loss = 0.9333(0.9333)
2023/11/21 19:29:58 - INFO - root -   Epoch: [78/300][20/200], lr: 0.00000029 	 loss = 0.8500(0.4933)
2023/11/21 19:30:12 - INFO - root -   Epoch: [78/300][40/200], lr: 0.00000029 	 loss = 0.6294(0.4537)
2023/11/21 19:30:27 - INFO - root -   Epoch: [78/300][60/200], lr: 0.00000029 	 loss = 0.1521(0.3975)
2023/11/21 19:30:36 - INFO - root -   Epoch: [78/300][80/200], lr: 0.00000029 	 loss = 0.6795(0.4200)
2023/11/21 19:30:53 - INFO - root -   Epoch: [78/300][100/200], lr: 0.00000029 	 loss = 0.2320(0.4070)
2023/11/21 19:31:08 - INFO - root -   Epoch: [78/300][120/200], lr: 0.00000029 	 loss = 0.0290(0.4295)
2023/11/21 19:31:21 - INFO - root -   Epoch: [78/300][140/200], lr: 0.00000029 	 loss = 0.0319(0.4277)
2023/11/21 19:31:38 - INFO - root -   Epoch: [78/300][160/200], lr: 0.00000029 	 loss = 0.8342(0.4503)
2023/11/21 19:31:51 - INFO - root -   Epoch: [78/300][180/200], lr: 0.00000029 	 loss = 0.3603(0.4460)
2023/11/21 19:32:05 - INFO - root -   Epoch: [78/300] 	 loss = 0.4535
2023/11/21 19:32:05 - INFO - root -   train_accuracy = 0.8525
2023/11/21 19:32:15 - INFO - root -   Epoch: [79/300][0/200], lr: 0.00000030 	 loss = 3.0599(3.0599)
2023/11/21 19:32:28 - INFO - root -   Epoch: [79/300][20/200], lr: 0.00000030 	 loss = 1.9273(0.7546)
2023/11/21 19:32:46 - INFO - root -   Epoch: [79/300][40/200], lr: 0.00000030 	 loss = 1.4829(0.5849)
2023/11/21 19:33:00 - INFO - root -   Epoch: [79/300][60/200], lr: 0.00000030 	 loss = 0.3319(0.5415)
2023/11/21 19:33:15 - INFO - root -   Epoch: [79/300][80/200], lr: 0.00000030 	 loss = 0.8252(0.5313)
2023/11/21 19:33:30 - INFO - root -   Epoch: [79/300][100/200], lr: 0.00000030 	 loss = 0.2141(0.5128)
2023/11/21 19:33:49 - INFO - root -   Epoch: [79/300][120/200], lr: 0.00000030 	 loss = 0.2437(0.5088)
2023/11/21 19:33:59 - INFO - root -   Epoch: [79/300][140/200], lr: 0.00000030 	 loss = 0.1914(0.4834)
2023/11/21 19:34:12 - INFO - root -   Epoch: [79/300][160/200], lr: 0.00000030 	 loss = 0.3726(0.4882)
2023/11/21 19:34:20 - INFO - root -   Epoch: [79/300][180/200], lr: 0.00000030 	 loss = 0.0578(0.4822)
2023/11/21 19:34:34 - INFO - root -   Epoch: [79/300] 	 loss = 0.4740
2023/11/21 19:36:36 - INFO - root -   precision = 0.8218
2023/11/21 19:36:36 - INFO - root -   eval_loss = 0.4304
2023/11/21 19:36:36 - INFO - root -   eval_acc = 0.8218
2023/11/21 19:36:37 - INFO - root -   train_accuracy = 0.8450
2023/11/21 19:36:50 - INFO - root -   Epoch: [80/300][0/200], lr: 0.00000030 	 loss = 2.4962(2.4962)
2023/11/21 19:36:58 - INFO - root -   Epoch: [80/300][20/200], lr: 0.00000030 	 loss = 1.0879(0.6248)
2023/11/21 19:37:13 - INFO - root -   Epoch: [80/300][40/200], lr: 0.00000030 	 loss = 0.4489(0.5355)
2023/11/21 19:37:34 - INFO - root -   Epoch: [80/300][60/200], lr: 0.00000030 	 loss = 0.6642(0.4870)
2023/11/21 19:37:49 - INFO - root -   Epoch: [80/300][80/200], lr: 0.00000030 	 loss = 2.5586(0.4938)
2023/11/21 19:38:02 - INFO - root -   Epoch: [80/300][100/200], lr: 0.00000030 	 loss = 0.0205(0.5026)
2023/11/21 19:38:16 - INFO - root -   Epoch: [80/300][120/200], lr: 0.00000030 	 loss = 0.1612(0.4930)
2023/11/21 19:38:30 - INFO - root -   Epoch: [80/300][140/200], lr: 0.00000030 	 loss = 0.2125(0.4754)
2023/11/21 19:38:44 - INFO - root -   Epoch: [80/300][160/200], lr: 0.00000030 	 loss = 1.7026(0.4900)
2023/11/21 19:38:58 - INFO - root -   Epoch: [80/300][180/200], lr: 0.00000030 	 loss = 0.0421(0.4881)
2023/11/21 19:39:07 - INFO - root -   Epoch: [80/300] 	 loss = 0.4819
2023/11/21 19:39:07 - INFO - root -   train_accuracy = 0.8375
2023/11/21 19:39:16 - INFO - root -   Epoch: [81/300][0/200], lr: 0.00000030 	 loss = 1.8474(1.8474)
2023/11/21 19:39:43 - INFO - root -   Epoch: [81/300][20/200], lr: 0.00000030 	 loss = 1.8535(0.7018)
2023/11/21 19:39:55 - INFO - root -   Epoch: [81/300][40/200], lr: 0.00000030 	 loss = 0.6769(0.5286)
2023/11/21 19:40:08 - INFO - root -   Epoch: [81/300][60/200], lr: 0.00000030 	 loss = 0.9564(0.4687)
2023/11/21 19:40:27 - INFO - root -   Epoch: [81/300][80/200], lr: 0.00000030 	 loss = 0.5625(0.4574)
2023/11/21 19:40:51 - INFO - root -   Epoch: [81/300][100/200], lr: 0.00000030 	 loss = 0.1402(0.4601)
2023/11/21 19:41:01 - INFO - root -   Epoch: [81/300][120/200], lr: 0.00000030 	 loss = 0.3184(0.4674)
2023/11/21 19:41:17 - INFO - root -   Epoch: [81/300][140/200], lr: 0.00000030 	 loss = 0.1332(0.4474)
2023/11/21 19:41:32 - INFO - root -   Epoch: [81/300][160/200], lr: 0.00000030 	 loss = 1.3579(0.4756)
2023/11/21 19:41:47 - INFO - root -   Epoch: [81/300][180/200], lr: 0.00000030 	 loss = 0.2360(0.4802)
2023/11/21 19:41:55 - INFO - root -   Epoch: [81/300] 	 loss = 0.4853
2023/11/21 19:41:55 - INFO - root -   train_accuracy = 0.8250
2023/11/21 19:42:02 - INFO - root -   Epoch: [82/300][0/200], lr: 0.00000030 	 loss = 1.2084(1.2084)
2023/11/21 19:42:21 - INFO - root -   Epoch: [82/300][20/200], lr: 0.00000030 	 loss = 1.1650(0.5502)
2023/11/21 19:42:33 - INFO - root -   Epoch: [82/300][40/200], lr: 0.00000030 	 loss = 1.0727(0.4388)
2023/11/21 19:42:45 - INFO - root -   Epoch: [82/300][60/200], lr: 0.00000030 	 loss = 0.0283(0.3926)
2023/11/21 19:42:59 - INFO - root -   Epoch: [82/300][80/200], lr: 0.00000030 	 loss = 1.1733(0.4141)
2023/11/21 19:43:13 - INFO - root -   Epoch: [82/300][100/200], lr: 0.00000030 	 loss = 0.0914(0.4247)
2023/11/21 19:43:25 - INFO - root -   Epoch: [82/300][120/200], lr: 0.00000030 	 loss = 0.1689(0.4320)
2023/11/21 19:43:37 - INFO - root -   Epoch: [82/300][140/200], lr: 0.00000030 	 loss = 0.1795(0.4490)
2023/11/21 19:43:54 - INFO - root -   Epoch: [82/300][160/200], lr: 0.00000030 	 loss = 0.7692(0.4670)
2023/11/21 19:44:13 - INFO - root -   Epoch: [82/300][180/200], lr: 0.00000030 	 loss = 0.0991(0.4788)
2023/11/21 19:44:22 - INFO - root -   Epoch: [82/300] 	 loss = 0.4796
2023/11/21 19:44:22 - INFO - root -   train_accuracy = 0.8200
2023/11/21 19:44:34 - INFO - root -   Epoch: [83/300][0/200], lr: 0.00000031 	 loss = 1.4766(1.4766)
2023/11/21 19:44:43 - INFO - root -   Epoch: [83/300][20/200], lr: 0.00000031 	 loss = 1.3731(0.6104)
2023/11/21 19:44:54 - INFO - root -   Epoch: [83/300][40/200], lr: 0.00000031 	 loss = 0.4609(0.4580)
2023/11/21 19:45:06 - INFO - root -   Epoch: [83/300][60/200], lr: 0.00000031 	 loss = 0.5477(0.4045)
2023/11/21 19:45:23 - INFO - root -   Epoch: [83/300][80/200], lr: 0.00000031 	 loss = 0.9881(0.4101)
2023/11/21 19:45:33 - INFO - root -   Epoch: [83/300][100/200], lr: 0.00000031 	 loss = 0.0124(0.4327)
2023/11/21 19:45:48 - INFO - root -   Epoch: [83/300][120/200], lr: 0.00000031 	 loss = 0.1946(0.4451)
2023/11/21 19:46:03 - INFO - root -   Epoch: [83/300][140/200], lr: 0.00000031 	 loss = 0.0597(0.4386)
2023/11/21 19:46:18 - INFO - root -   Epoch: [83/300][160/200], lr: 0.00000031 	 loss = 1.4323(0.4605)
2023/11/21 19:46:28 - INFO - root -   Epoch: [83/300][180/200], lr: 0.00000031 	 loss = 0.0701(0.4516)
2023/11/21 19:46:38 - INFO - root -   Epoch: [83/300] 	 loss = 0.4550
2023/11/21 19:46:38 - INFO - root -   train_accuracy = 0.8350
2023/11/21 19:46:39 - INFO - root -   Epoch: [84/300][0/200], lr: 0.00000031 	 loss = 2.6360(2.6360)
2023/11/21 19:47:00 - INFO - root -   Epoch: [84/300][20/200], lr: 0.00000031 	 loss = 1.8799(0.5441)
2023/11/21 19:47:20 - INFO - root -   Epoch: [84/300][40/200], lr: 0.00000031 	 loss = 0.8463(0.4985)
2023/11/21 19:47:41 - INFO - root -   Epoch: [84/300][60/200], lr: 0.00000031 	 loss = 0.2927(0.4274)
2023/11/21 19:48:06 - INFO - root -   Epoch: [84/300][80/200], lr: 0.00000031 	 loss = 0.7521(0.4299)
2023/11/21 19:48:24 - INFO - root -   Epoch: [84/300][100/200], lr: 0.00000031 	 loss = 0.0799(0.4216)
2023/11/21 19:48:40 - INFO - root -   Epoch: [84/300][120/200], lr: 0.00000031 	 loss = 0.2982(0.4152)
2023/11/21 19:48:59 - INFO - root -   Epoch: [84/300][140/200], lr: 0.00000031 	 loss = 0.1355(0.4188)
2023/11/21 19:49:12 - INFO - root -   Epoch: [84/300][160/200], lr: 0.00000031 	 loss = 0.4602(0.4333)
2023/11/21 19:49:31 - INFO - root -   Epoch: [84/300][180/200], lr: 0.00000031 	 loss = 0.4573(0.4556)
2023/11/21 19:49:38 - INFO - root -   Epoch: [84/300] 	 loss = 0.4616
2023/11/21 19:51:40 - INFO - root -   precision = 0.8218
2023/11/21 19:51:40 - INFO - root -   eval_loss = 0.4452
2023/11/21 19:51:40 - INFO - root -   eval_acc = 0.8218
2023/11/21 19:51:41 - INFO - root -   train_accuracy = 0.8425
2023/11/21 19:51:49 - INFO - root -   Epoch: [85/300][0/200], lr: 0.00000031 	 loss = 1.5276(1.5276)
2023/11/21 19:52:11 - INFO - root -   Epoch: [85/300][20/200], lr: 0.00000031 	 loss = 1.5869(0.6220)
2023/11/21 19:52:25 - INFO - root -   Epoch: [85/300][40/200], lr: 0.00000031 	 loss = 1.0239(0.5486)
2023/11/21 19:52:40 - INFO - root -   Epoch: [85/300][60/200], lr: 0.00000031 	 loss = 0.2638(0.4817)
2023/11/21 19:52:55 - INFO - root -   Epoch: [85/300][80/200], lr: 0.00000031 	 loss = 1.6787(0.4917)
2023/11/21 19:53:05 - INFO - root -   Epoch: [85/300][100/200], lr: 0.00000031 	 loss = 0.0960(0.4793)
2023/11/21 19:53:25 - INFO - root -   Epoch: [85/300][120/200], lr: 0.00000031 	 loss = 0.0818(0.4719)
2023/11/21 19:53:40 - INFO - root -   Epoch: [85/300][140/200], lr: 0.00000031 	 loss = 0.3413(0.4643)
2023/11/21 19:53:54 - INFO - root -   Epoch: [85/300][160/200], lr: 0.00000031 	 loss = 0.6962(0.4659)
2023/11/21 19:54:03 - INFO - root -   Epoch: [85/300][180/200], lr: 0.00000031 	 loss = 0.1304(0.4708)
2023/11/21 19:54:20 - INFO - root -   Epoch: [85/300] 	 loss = 0.4682
2023/11/21 19:54:20 - INFO - root -   train_accuracy = 0.8325
2023/11/21 19:54:30 - INFO - root -   Epoch: [86/300][0/200], lr: 0.00000031 	 loss = 1.3916(1.3916)
2023/11/21 19:54:58 - INFO - root -   Epoch: [86/300][20/200], lr: 0.00000031 	 loss = 1.7712(0.6704)
2023/11/21 19:55:09 - INFO - root -   Epoch: [86/300][40/200], lr: 0.00000031 	 loss = 1.1689(0.5878)
2023/11/21 19:55:22 - INFO - root -   Epoch: [86/300][60/200], lr: 0.00000031 	 loss = 0.4451(0.4901)
2023/11/21 19:55:38 - INFO - root -   Epoch: [86/300][80/200], lr: 0.00000031 	 loss = 0.8734(0.4777)
2023/11/21 19:55:49 - INFO - root -   Epoch: [86/300][100/200], lr: 0.00000031 	 loss = 0.0797(0.4754)
2023/11/21 19:56:06 - INFO - root -   Epoch: [86/300][120/200], lr: 0.00000031 	 loss = 0.5010(0.4887)
2023/11/21 19:56:25 - INFO - root -   Epoch: [86/300][140/200], lr: 0.00000031 	 loss = 0.1732(0.4937)
2023/11/21 19:56:46 - INFO - root -   Epoch: [86/300][160/200], lr: 0.00000031 	 loss = 0.5854(0.5002)
2023/11/21 19:56:58 - INFO - root -   Epoch: [86/300][180/200], lr: 0.00000031 	 loss = 0.1075(0.4839)
2023/11/21 19:57:08 - INFO - root -   Epoch: [86/300] 	 loss = 0.5026
2023/11/21 19:57:08 - INFO - root -   train_accuracy = 0.8225
2023/11/21 19:57:17 - INFO - root -   Epoch: [87/300][0/200], lr: 0.00000032 	 loss = 1.9415(1.9415)
2023/11/21 19:57:30 - INFO - root -   Epoch: [87/300][20/200], lr: 0.00000032 	 loss = 0.9775(0.6232)
2023/11/21 19:57:44 - INFO - root -   Epoch: [87/300][40/200], lr: 0.00000032 	 loss = 0.4639(0.5206)
2023/11/21 19:58:06 - INFO - root -   Epoch: [87/300][60/200], lr: 0.00000032 	 loss = 0.2264(0.4537)
2023/11/21 19:58:17 - INFO - root -   Epoch: [87/300][80/200], lr: 0.00000032 	 loss = 1.4911(0.4522)
2023/11/21 19:58:36 - INFO - root -   Epoch: [87/300][100/200], lr: 0.00000032 	 loss = 0.1354(0.4363)
2023/11/21 19:58:52 - INFO - root -   Epoch: [87/300][120/200], lr: 0.00000032 	 loss = 0.4715(0.4303)
2023/11/21 19:59:10 - INFO - root -   Epoch: [87/300][140/200], lr: 0.00000032 	 loss = 0.5870(0.4171)
2023/11/21 19:59:30 - INFO - root -   Epoch: [87/300][160/200], lr: 0.00000032 	 loss = 0.7533(0.4353)
2023/11/21 19:59:45 - INFO - root -   Epoch: [87/300][180/200], lr: 0.00000032 	 loss = 0.1780(0.4360)
2023/11/21 19:59:57 - INFO - root -   Epoch: [87/300] 	 loss = 0.4534
2023/11/21 19:59:57 - INFO - root -   train_accuracy = 0.8375
2023/11/21 19:59:58 - INFO - root -   Epoch: [88/300][0/200], lr: 0.00000032 	 loss = 1.1242(1.1242)
2023/11/21 20:00:18 - INFO - root -   Epoch: [88/300][20/200], lr: 0.00000032 	 loss = 1.2241(0.5434)
2023/11/21 20:00:32 - INFO - root -   Epoch: [88/300][40/200], lr: 0.00000032 	 loss = 1.1347(0.5050)
2023/11/21 20:00:47 - INFO - root -   Epoch: [88/300][60/200], lr: 0.00000032 	 loss = 0.2214(0.4732)
2023/11/21 20:01:02 - INFO - root -   Epoch: [88/300][80/200], lr: 0.00000032 	 loss = 1.5114(0.4962)
2023/11/21 20:01:16 - INFO - root -   Epoch: [88/300][100/200], lr: 0.00000032 	 loss = 0.0124(0.4714)
2023/11/21 20:01:32 - INFO - root -   Epoch: [88/300][120/200], lr: 0.00000032 	 loss = 0.1719(0.4910)
2023/11/21 20:01:45 - INFO - root -   Epoch: [88/300][140/200], lr: 0.00000032 	 loss = 0.0742(0.4793)
2023/11/21 20:02:07 - INFO - root -   Epoch: [88/300][160/200], lr: 0.00000032 	 loss = 1.5441(0.4928)
2023/11/21 20:02:25 - INFO - root -   Epoch: [88/300][180/200], lr: 0.00000032 	 loss = 0.1179(0.4899)
2023/11/21 20:02:35 - INFO - root -   Epoch: [88/300] 	 loss = 0.4817
2023/11/21 20:02:35 - INFO - root -   train_accuracy = 0.8375
2023/11/21 20:02:41 - INFO - root -   Epoch: [89/300][0/200], lr: 0.00000032 	 loss = 2.0396(2.0396)
2023/11/21 20:02:59 - INFO - root -   Epoch: [89/300][20/200], lr: 0.00000032 	 loss = 0.7356(0.6004)
2023/11/21 20:03:18 - INFO - root -   Epoch: [89/300][40/200], lr: 0.00000032 	 loss = 0.5562(0.5157)
2023/11/21 20:03:28 - INFO - root -   Epoch: [89/300][60/200], lr: 0.00000032 	 loss = 0.1530(0.5014)
2023/11/21 20:03:43 - INFO - root -   Epoch: [89/300][80/200], lr: 0.00000032 	 loss = 0.4081(0.4958)
2023/11/21 20:03:54 - INFO - root -   Epoch: [89/300][100/200], lr: 0.00000032 	 loss = 0.0177(0.4718)
2023/11/21 20:04:15 - INFO - root -   Epoch: [89/300][120/200], lr: 0.00000032 	 loss = 0.0513(0.4736)
2023/11/21 20:04:23 - INFO - root -   Epoch: [89/300][140/200], lr: 0.00000032 	 loss = 0.1094(0.4790)
2023/11/21 20:04:42 - INFO - root -   Epoch: [89/300][160/200], lr: 0.00000032 	 loss = 0.7012(0.5011)
2023/11/21 20:04:58 - INFO - root -   Epoch: [89/300][180/200], lr: 0.00000032 	 loss = 0.3966(0.5071)
2023/11/21 20:05:06 - INFO - root -   Epoch: [89/300] 	 loss = 0.5078
2023/11/21 20:07:07 - INFO - root -   precision = 0.8119
2023/11/21 20:07:07 - INFO - root -   eval_loss = 0.4436
2023/11/21 20:07:07 - INFO - root -   eval_acc = 0.8119
2023/11/21 20:07:08 - INFO - root -   train_accuracy = 0.8175
2023/11/21 20:07:13 - INFO - root -   Epoch: [90/300][0/200], lr: 0.00000032 	 loss = 1.0778(1.0778)
2023/11/21 20:07:29 - INFO - root -   Epoch: [90/300][20/200], lr: 0.00000032 	 loss = 2.0931(0.5580)
2023/11/21 20:07:38 - INFO - root -   Epoch: [90/300][40/200], lr: 0.00000032 	 loss = 0.3609(0.5024)
2023/11/21 20:07:59 - INFO - root -   Epoch: [90/300][60/200], lr: 0.00000032 	 loss = 0.4890(0.4585)
2023/11/21 20:08:16 - INFO - root -   Epoch: [90/300][80/200], lr: 0.00000032 	 loss = 0.9636(0.4763)
2023/11/21 20:08:36 - INFO - root -   Epoch: [90/300][100/200], lr: 0.00000032 	 loss = 0.0498(0.4619)
2023/11/21 20:08:49 - INFO - root -   Epoch: [90/300][120/200], lr: 0.00000032 	 loss = 0.1111(0.4763)
2023/11/21 20:09:03 - INFO - root -   Epoch: [90/300][140/200], lr: 0.00000032 	 loss = 0.0630(0.4707)
2023/11/21 20:09:18 - INFO - root -   Epoch: [90/300][160/200], lr: 0.00000032 	 loss = 1.0127(0.4808)
2023/11/21 20:09:33 - INFO - root -   Epoch: [90/300][180/200], lr: 0.00000032 	 loss = 0.1411(0.4875)
2023/11/21 20:09:49 - INFO - root -   Epoch: [90/300] 	 loss = 0.4936
2023/11/21 20:09:49 - INFO - root -   train_accuracy = 0.8325
2023/11/21 20:09:50 - INFO - root -   Epoch: [91/300][0/200], lr: 0.00000033 	 loss = 1.6057(1.6057)
2023/11/21 20:10:07 - INFO - root -   Epoch: [91/300][20/200], lr: 0.00000033 	 loss = 1.6745(0.4636)
2023/11/21 20:10:26 - INFO - root -   Epoch: [91/300][40/200], lr: 0.00000033 	 loss = 0.5233(0.4276)
2023/11/21 20:10:35 - INFO - root -   Epoch: [91/300][60/200], lr: 0.00000033 	 loss = 0.3377(0.4366)
2023/11/21 20:10:56 - INFO - root -   Epoch: [91/300][80/200], lr: 0.00000033 	 loss = 1.4379(0.4205)
2023/11/21 20:11:04 - INFO - root -   Epoch: [91/300][100/200], lr: 0.00000033 	 loss = 0.0174(0.4176)
2023/11/21 20:11:12 - INFO - root -   Epoch: [91/300][120/200], lr: 0.00000033 	 loss = 0.4072(0.4165)
2023/11/21 20:11:31 - INFO - root -   Epoch: [91/300][140/200], lr: 0.00000033 	 loss = 0.0350(0.4073)
2023/11/21 20:11:38 - INFO - root -   Epoch: [91/300][160/200], lr: 0.00000033 	 loss = 0.3524(0.4113)
2023/11/21 20:11:49 - INFO - root -   Epoch: [91/300][180/200], lr: 0.00000033 	 loss = 0.1021(0.4184)
2023/11/21 20:12:02 - INFO - root -   Epoch: [91/300] 	 loss = 0.4211
2023/11/21 20:12:02 - INFO - root -   train_accuracy = 0.8375
2023/11/21 20:12:03 - INFO - root -   Epoch: [92/300][0/200], lr: 0.00000033 	 loss = 1.2428(1.2428)
2023/11/21 20:12:32 - INFO - root -   Epoch: [92/300][20/200], lr: 0.00000033 	 loss = 1.8676(0.6153)
2023/11/21 20:12:50 - INFO - root -   Epoch: [92/300][40/200], lr: 0.00000033 	 loss = 0.4382(0.5775)
2023/11/21 20:13:09 - INFO - root -   Epoch: [92/300][60/200], lr: 0.00000033 	 loss = 0.2045(0.5145)
2023/11/21 20:13:25 - INFO - root -   Epoch: [92/300][80/200], lr: 0.00000033 	 loss = 1.4654(0.5039)
2023/11/21 20:13:41 - INFO - root -   Epoch: [92/300][100/200], lr: 0.00000033 	 loss = 0.2080(0.4848)
2023/11/21 20:13:55 - INFO - root -   Epoch: [92/300][120/200], lr: 0.00000033 	 loss = 0.2583(0.4597)
2023/11/21 20:14:13 - INFO - root -   Epoch: [92/300][140/200], lr: 0.00000033 	 loss = 0.0586(0.4569)
2023/11/21 20:14:27 - INFO - root -   Epoch: [92/300][160/200], lr: 0.00000033 	 loss = 0.5802(0.4600)
2023/11/21 20:14:41 - INFO - root -   Epoch: [92/300][180/200], lr: 0.00000033 	 loss = 0.2603(0.4555)
2023/11/21 20:14:53 - INFO - root -   Epoch: [92/300] 	 loss = 0.4624
2023/11/21 20:14:53 - INFO - root -   train_accuracy = 0.8425
2023/11/21 20:14:55 - INFO - root -   Epoch: [93/300][0/200], lr: 0.00000033 	 loss = 2.0784(2.0784)
2023/11/21 20:15:17 - INFO - root -   Epoch: [93/300][20/200], lr: 0.00000033 	 loss = 0.5128(0.4657)
2023/11/21 20:15:28 - INFO - root -   Epoch: [93/300][40/200], lr: 0.00000033 	 loss = 0.4046(0.4253)
2023/11/21 20:15:38 - INFO - root -   Epoch: [93/300][60/200], lr: 0.00000033 	 loss = 0.5021(0.4112)
2023/11/21 20:15:52 - INFO - root -   Epoch: [93/300][80/200], lr: 0.00000033 	 loss = 0.6234(0.3989)
2023/11/21 20:16:07 - INFO - root -   Epoch: [93/300][100/200], lr: 0.00000033 	 loss = 0.0298(0.4324)
2023/11/21 20:16:17 - INFO - root -   Epoch: [93/300][120/200], lr: 0.00000033 	 loss = 0.0995(0.4371)
2023/11/21 20:16:29 - INFO - root -   Epoch: [93/300][140/200], lr: 0.00000033 	 loss = 0.0781(0.4305)
2023/11/21 20:16:48 - INFO - root -   Epoch: [93/300][160/200], lr: 0.00000033 	 loss = 0.3592(0.4466)
2023/11/21 20:17:03 - INFO - root -   Epoch: [93/300][180/200], lr: 0.00000033 	 loss = 0.1139(0.4479)
2023/11/21 20:17:13 - INFO - root -   Epoch: [93/300] 	 loss = 0.4553
2023/11/21 20:17:13 - INFO - root -   train_accuracy = 0.8425
2023/11/21 20:17:14 - INFO - root -   Epoch: [94/300][0/200], lr: 0.00000033 	 loss = 1.1097(1.1097)
2023/11/21 20:17:33 - INFO - root -   Epoch: [94/300][20/200], lr: 0.00000033 	 loss = 1.7276(0.4907)
2023/11/21 20:17:43 - INFO - root -   Epoch: [94/300][40/200], lr: 0.00000033 	 loss = 0.5912(0.4343)
2023/11/21 20:18:03 - INFO - root -   Epoch: [94/300][60/200], lr: 0.00000033 	 loss = 0.3276(0.3937)
2023/11/21 20:18:13 - INFO - root -   Epoch: [94/300][80/200], lr: 0.00000033 	 loss = 0.9096(0.4020)
2023/11/21 20:18:23 - INFO - root -   Epoch: [94/300][100/200], lr: 0.00000033 	 loss = 0.0206(0.3920)
2023/11/21 20:18:38 - INFO - root -   Epoch: [94/300][120/200], lr: 0.00000033 	 loss = 0.5792(0.4074)
2023/11/21 20:18:56 - INFO - root -   Epoch: [94/300][140/200], lr: 0.00000033 	 loss = 0.0748(0.4060)
2023/11/21 20:19:15 - INFO - root -   Epoch: [94/300][160/200], lr: 0.00000033 	 loss = 1.0684(0.4187)
2023/11/21 20:19:28 - INFO - root -   Epoch: [94/300][180/200], lr: 0.00000033 	 loss = 0.0985(0.4246)
2023/11/21 20:19:43 - INFO - root -   Epoch: [94/300] 	 loss = 0.4404
2023/11/21 20:21:45 - INFO - root -   precision = 0.8515
2023/11/21 20:21:45 - INFO - root -   eval_loss = 0.4408
2023/11/21 20:21:45 - INFO - root -   eval_acc = 0.8515
2023/11/21 20:21:46 - INFO - root -   train_accuracy = 0.8425
2023/11/21 20:21:53 - INFO - root -   Epoch: [95/300][0/200], lr: 0.00000034 	 loss = 0.9208(0.9208)
2023/11/21 20:22:14 - INFO - root -   Epoch: [95/300][20/200], lr: 0.00000034 	 loss = 0.9271(0.4513)
2023/11/21 20:22:30 - INFO - root -   Epoch: [95/300][40/200], lr: 0.00000034 	 loss = 1.2454(0.4301)
2023/11/21 20:22:46 - INFO - root -   Epoch: [95/300][60/200], lr: 0.00000034 	 loss = 0.4891(0.4180)
2023/11/21 20:23:00 - INFO - root -   Epoch: [95/300][80/200], lr: 0.00000034 	 loss = 0.8161(0.4330)
2023/11/21 20:23:11 - INFO - root -   Epoch: [95/300][100/200], lr: 0.00000034 	 loss = 0.0215(0.4242)
2023/11/21 20:23:34 - INFO - root -   Epoch: [95/300][120/200], lr: 0.00000034 	 loss = 0.1092(0.4167)
2023/11/21 20:23:52 - INFO - root -   Epoch: [95/300][140/200], lr: 0.00000034 	 loss = 0.0817(0.4099)
2023/11/21 20:24:06 - INFO - root -   Epoch: [95/300][160/200], lr: 0.00000034 	 loss = 0.4083(0.4270)
2023/11/21 20:24:22 - INFO - root -   Epoch: [95/300][180/200], lr: 0.00000034 	 loss = 0.2702(0.4464)
2023/11/21 20:24:31 - INFO - root -   Epoch: [95/300] 	 loss = 0.4441
2023/11/21 20:24:31 - INFO - root -   train_accuracy = 0.8375
2023/11/21 20:24:33 - INFO - root -   Epoch: [96/300][0/200], lr: 0.00000034 	 loss = 1.0786(1.0786)
2023/11/21 20:24:55 - INFO - root -   Epoch: [96/300][20/200], lr: 0.00000034 	 loss = 1.2080(0.4899)
2023/11/21 20:25:12 - INFO - root -   Epoch: [96/300][40/200], lr: 0.00000034 	 loss = 1.9841(0.4763)
2023/11/21 20:25:21 - INFO - root -   Epoch: [96/300][60/200], lr: 0.00000034 	 loss = 0.1231(0.4352)
2023/11/21 20:25:45 - INFO - root -   Epoch: [96/300][80/200], lr: 0.00000034 	 loss = 1.3638(0.4240)
2023/11/21 20:26:01 - INFO - root -   Epoch: [96/300][100/200], lr: 0.00000034 	 loss = 0.0295(0.4860)
2023/11/21 20:26:15 - INFO - root -   Epoch: [96/300][120/200], lr: 0.00000034 	 loss = 0.3559(0.4899)
2023/11/21 20:26:29 - INFO - root -   Epoch: [96/300][140/200], lr: 0.00000034 	 loss = 0.0638(0.4799)
2023/11/21 20:26:45 - INFO - root -   Epoch: [96/300][160/200], lr: 0.00000034 	 loss = 1.0818(0.4843)
2023/11/21 20:26:54 - INFO - root -   Epoch: [96/300][180/200], lr: 0.00000034 	 loss = 0.1394(0.4829)
2023/11/21 20:27:10 - INFO - root -   Epoch: [96/300] 	 loss = 0.4826
2023/11/21 20:27:10 - INFO - root -   train_accuracy = 0.8250
2023/11/21 20:27:12 - INFO - root -   Epoch: [97/300][0/200], lr: 0.00000034 	 loss = 1.1073(1.1073)
2023/11/21 20:27:35 - INFO - root -   Epoch: [97/300][20/200], lr: 0.00000034 	 loss = 1.1715(0.4778)
2023/11/21 20:27:55 - INFO - root -   Epoch: [97/300][40/200], lr: 0.00000034 	 loss = 0.3317(0.4128)
2023/11/21 20:28:13 - INFO - root -   Epoch: [97/300][60/200], lr: 0.00000034 	 loss = 0.0979(0.3925)
2023/11/21 20:28:21 - INFO - root -   Epoch: [97/300][80/200], lr: 0.00000034 	 loss = 1.2134(0.4201)
2023/11/21 20:28:39 - INFO - root -   Epoch: [97/300][100/200], lr: 0.00000034 	 loss = 0.0978(0.4172)
2023/11/21 20:28:55 - INFO - root -   Epoch: [97/300][120/200], lr: 0.00000034 	 loss = 0.0782(0.4117)
2023/11/21 20:29:19 - INFO - root -   Epoch: [97/300][140/200], lr: 0.00000034 	 loss = 0.0836(0.4239)
2023/11/21 20:29:32 - INFO - root -   Epoch: [97/300][160/200], lr: 0.00000034 	 loss = 1.6107(0.4349)
2023/11/21 20:29:52 - INFO - root -   Epoch: [97/300][180/200], lr: 0.00000034 	 loss = 0.0573(0.4457)
2023/11/21 20:30:04 - INFO - root -   Epoch: [97/300] 	 loss = 0.4536
2023/11/21 20:30:04 - INFO - root -   train_accuracy = 0.8325
2023/11/21 20:30:12 - INFO - root -   Epoch: [98/300][0/200], lr: 0.00000034 	 loss = 1.6253(1.6253)
2023/11/21 20:30:25 - INFO - root -   Epoch: [98/300][20/200], lr: 0.00000034 	 loss = 1.0507(0.4657)
2023/11/21 20:30:33 - INFO - root -   Epoch: [98/300][40/200], lr: 0.00000034 	 loss = 1.5728(0.4692)
2023/11/21 20:30:50 - INFO - root -   Epoch: [98/300][60/200], lr: 0.00000034 	 loss = 0.0797(0.4414)
2023/11/21 20:31:09 - INFO - root -   Epoch: [98/300][80/200], lr: 0.00000034 	 loss = 1.5657(0.4751)
2023/11/21 20:31:25 - INFO - root -   Epoch: [98/300][100/200], lr: 0.00000034 	 loss = 0.0769(0.4558)
2023/11/21 20:31:42 - INFO - root -   Epoch: [98/300][120/200], lr: 0.00000034 	 loss = 0.1845(0.4644)
2023/11/21 20:32:04 - INFO - root -   Epoch: [98/300][140/200], lr: 0.00000034 	 loss = 0.0116(0.4539)
2023/11/21 20:32:17 - INFO - root -   Epoch: [98/300][160/200], lr: 0.00000034 	 loss = 0.4735(0.4616)
2023/11/21 20:32:33 - INFO - root -   Epoch: [98/300][180/200], lr: 0.00000034 	 loss = 0.0936(0.4582)
2023/11/21 20:32:42 - INFO - root -   Epoch: [98/300] 	 loss = 0.4631
2023/11/21 20:32:42 - INFO - root -   train_accuracy = 0.8450
2023/11/21 20:32:49 - INFO - root -   Epoch: [99/300][0/200], lr: 0.00000035 	 loss = 1.0282(1.0282)
2023/11/21 20:33:06 - INFO - root -   Epoch: [99/300][20/200], lr: 0.00000035 	 loss = 1.0567(0.5299)
2023/11/21 20:33:14 - INFO - root -   Epoch: [99/300][40/200], lr: 0.00000035 	 loss = 0.7203(0.4475)
2023/11/21 20:33:27 - INFO - root -   Epoch: [99/300][60/200], lr: 0.00000035 	 loss = 0.1293(0.4498)
2023/11/21 20:33:45 - INFO - root -   Epoch: [99/300][80/200], lr: 0.00000035 	 loss = 0.6823(0.4355)
2023/11/21 20:34:00 - INFO - root -   Epoch: [99/300][100/200], lr: 0.00000035 	 loss = 0.0663(0.4511)
2023/11/21 20:34:16 - INFO - root -   Epoch: [99/300][120/200], lr: 0.00000035 	 loss = 0.7929(0.4548)
2023/11/21 20:34:36 - INFO - root -   Epoch: [99/300][140/200], lr: 0.00000035 	 loss = 0.0852(0.4567)
2023/11/21 20:34:48 - INFO - root -   Epoch: [99/300][160/200], lr: 0.00000035 	 loss = 0.2994(0.4404)
2023/11/21 20:34:56 - INFO - root -   Epoch: [99/300][180/200], lr: 0.00000035 	 loss = 0.0430(0.4308)
2023/11/21 20:35:06 - INFO - root -   Epoch: [99/300] 	 loss = 0.4345
2023/11/21 20:37:07 - INFO - root -   precision = 0.8317
2023/11/21 20:37:07 - INFO - root -   eval_loss = 0.4219
2023/11/21 20:37:07 - INFO - root -   eval_acc = 0.8317
2023/11/21 20:37:08 - INFO - root -   train_accuracy = 0.8575
2023/11/21 20:37:17 - INFO - root -   Epoch: [100/300][0/200], lr: 0.00000035 	 loss = 1.2805(1.2805)
2023/11/21 20:37:29 - INFO - root -   Epoch: [100/300][20/200], lr: 0.00000035 	 loss = 0.8515(0.4473)
2023/11/21 20:37:45 - INFO - root -   Epoch: [100/300][40/200], lr: 0.00000035 	 loss = 0.3371(0.4319)
2023/11/21 20:37:54 - INFO - root -   Epoch: [100/300][60/200], lr: 0.00000035 	 loss = 0.1069(0.3913)
2023/11/21 20:38:09 - INFO - root -   Epoch: [100/300][80/200], lr: 0.00000035 	 loss = 0.6361(0.4064)
2023/11/21 20:38:29 - INFO - root -   Epoch: [100/300][100/200], lr: 0.00000035 	 loss = 0.0456(0.4204)
2023/11/21 20:38:40 - INFO - root -   Epoch: [100/300][120/200], lr: 0.00000035 	 loss = 0.3153(0.4243)
2023/11/21 20:38:50 - INFO - root -   Epoch: [100/300][140/200], lr: 0.00000035 	 loss = 0.0444(0.4274)
2023/11/21 20:39:09 - INFO - root -   Epoch: [100/300][160/200], lr: 0.00000035 	 loss = 0.4674(0.4298)
2023/11/21 20:39:17 - INFO - root -   Epoch: [100/300][180/200], lr: 0.00000035 	 loss = 0.1465(0.4344)
2023/11/21 20:39:36 - INFO - root -   Epoch: [100/300] 	 loss = 0.4372
2023/11/21 20:39:36 - INFO - root -   train_accuracy = 0.8375
2023/11/21 20:39:44 - INFO - root -   Epoch: [101/300][0/200], lr: 0.00000035 	 loss = 2.4561(2.4561)
2023/11/21 20:39:59 - INFO - root -   Epoch: [101/300][20/200], lr: 0.00000035 	 loss = 1.2378(0.6062)
2023/11/21 20:40:17 - INFO - root -   Epoch: [101/300][40/200], lr: 0.00000035 	 loss = 0.4901(0.4923)
2023/11/21 20:40:35 - INFO - root -   Epoch: [101/300][60/200], lr: 0.00000035 	 loss = 0.0893(0.4339)
2023/11/21 20:40:48 - INFO - root -   Epoch: [101/300][80/200], lr: 0.00000035 	 loss = 1.3805(0.4398)
2023/11/21 20:41:10 - INFO - root -   Epoch: [101/300][100/200], lr: 0.00000035 	 loss = 0.3372(0.4343)
2023/11/21 20:41:23 - INFO - root -   Epoch: [101/300][120/200], lr: 0.00000035 	 loss = 0.2878(0.4466)
2023/11/21 20:41:38 - INFO - root -   Epoch: [101/300][140/200], lr: 0.00000035 	 loss = 0.0704(0.4567)
2023/11/21 20:41:50 - INFO - root -   Epoch: [101/300][160/200], lr: 0.00000035 	 loss = 0.6011(0.4599)
2023/11/21 20:42:08 - INFO - root -   Epoch: [101/300][180/200], lr: 0.00000035 	 loss = 0.2507(0.4692)
2023/11/21 20:42:17 - INFO - root -   Epoch: [101/300] 	 loss = 0.4609
2023/11/21 20:42:17 - INFO - root -   train_accuracy = 0.8550
2023/11/21 20:42:18 - INFO - root -   Epoch: [102/300][0/200], lr: 0.00000035 	 loss = 2.0809(2.0809)
2023/11/21 20:42:46 - INFO - root -   Epoch: [102/300][20/200], lr: 0.00000035 	 loss = 1.5610(0.6099)
2023/11/21 20:42:59 - INFO - root -   Epoch: [102/300][40/200], lr: 0.00000035 	 loss = 1.2222(0.5225)
2023/11/21 20:43:09 - INFO - root -   Epoch: [102/300][60/200], lr: 0.00000035 	 loss = 0.3261(0.4962)
2023/11/21 20:43:20 - INFO - root -   Epoch: [102/300][80/200], lr: 0.00000035 	 loss = 1.0323(0.4742)
2023/11/21 20:43:34 - INFO - root -   Epoch: [102/300][100/200], lr: 0.00000035 	 loss = 0.1264(0.4678)
2023/11/21 20:43:47 - INFO - root -   Epoch: [102/300][120/200], lr: 0.00000035 	 loss = 0.7340(0.4752)
2023/11/21 20:43:58 - INFO - root -   Epoch: [102/300][140/200], lr: 0.00000035 	 loss = 0.2207(0.4626)
2023/11/21 20:44:12 - INFO - root -   Epoch: [102/300][160/200], lr: 0.00000035 	 loss = 0.4731(0.4645)
2023/11/21 20:44:32 - INFO - root -   Epoch: [102/300][180/200], lr: 0.00000035 	 loss = 0.2524(0.4646)
2023/11/21 20:44:44 - INFO - root -   Epoch: [102/300] 	 loss = 0.4712
2023/11/21 20:44:44 - INFO - root -   train_accuracy = 0.8275
2023/11/21 20:44:51 - INFO - root -   Epoch: [103/300][0/200], lr: 0.00000035 	 loss = 2.1010(2.1010)
2023/11/21 20:45:02 - INFO - root -   Epoch: [103/300][20/200], lr: 0.00000035 	 loss = 1.0161(0.5594)
2023/11/21 20:45:24 - INFO - root -   Epoch: [103/300][40/200], lr: 0.00000035 	 loss = 0.4769(0.5572)
2023/11/21 20:45:39 - INFO - root -   Epoch: [103/300][60/200], lr: 0.00000035 	 loss = 0.5680(0.4733)
2023/11/21 20:45:56 - INFO - root -   Epoch: [103/300][80/200], lr: 0.00000035 	 loss = 0.3985(0.4516)
2023/11/21 20:46:13 - INFO - root -   Epoch: [103/300][100/200], lr: 0.00000035 	 loss = 0.0401(0.4809)
2023/11/21 20:46:29 - INFO - root -   Epoch: [103/300][120/200], lr: 0.00000035 	 loss = 0.3102(0.4691)
2023/11/21 20:46:43 - INFO - root -   Epoch: [103/300][140/200], lr: 0.00000035 	 loss = 0.0604(0.4618)
2023/11/21 20:46:58 - INFO - root -   Epoch: [103/300][160/200], lr: 0.00000035 	 loss = 0.8626(0.4652)
2023/11/21 20:47:11 - INFO - root -   Epoch: [103/300][180/200], lr: 0.00000035 	 loss = 0.4490(0.4716)
2023/11/21 20:47:24 - INFO - root -   Epoch: [103/300] 	 loss = 0.4632
2023/11/21 20:47:24 - INFO - root -   train_accuracy = 0.8350
2023/11/21 20:47:37 - INFO - root -   Epoch: [104/300][0/200], lr: 0.00000036 	 loss = 2.1891(2.1891)
2023/11/21 20:47:50 - INFO - root -   Epoch: [104/300][20/200], lr: 0.00000036 	 loss = 2.3367(0.7175)
2023/11/21 20:48:00 - INFO - root -   Epoch: [104/300][40/200], lr: 0.00000036 	 loss = 0.4963(0.5322)
2023/11/21 20:48:20 - INFO - root -   Epoch: [104/300][60/200], lr: 0.00000036 	 loss = 0.4241(0.4857)
2023/11/21 20:48:41 - INFO - root -   Epoch: [104/300][80/200], lr: 0.00000036 	 loss = 1.3494(0.4934)
2023/11/21 20:48:54 - INFO - root -   Epoch: [104/300][100/200], lr: 0.00000036 	 loss = 0.0879(0.4849)
2023/11/21 20:49:05 - INFO - root -   Epoch: [104/300][120/200], lr: 0.00000036 	 loss = 0.5636(0.4601)
2023/11/21 20:49:20 - INFO - root -   Epoch: [104/300][140/200], lr: 0.00000036 	 loss = 0.1291(0.4557)
2023/11/21 20:49:33 - INFO - root -   Epoch: [104/300][160/200], lr: 0.00000036 	 loss = 0.7843(0.4709)
2023/11/21 20:49:43 - INFO - root -   Epoch: [104/300][180/200], lr: 0.00000036 	 loss = 0.1499(0.4609)
2023/11/21 20:49:53 - INFO - root -   Epoch: [104/300] 	 loss = 0.4671
2023/11/21 20:51:54 - INFO - root -   precision = 0.8218
2023/11/21 20:51:54 - INFO - root -   eval_loss = 0.4316
2023/11/21 20:51:54 - INFO - root -   eval_acc = 0.8218
2023/11/21 20:51:55 - INFO - root -   train_accuracy = 0.8375
2023/11/21 20:52:04 - INFO - root -   Epoch: [105/300][0/200], lr: 0.00000036 	 loss = 0.8323(0.8323)
2023/11/21 20:52:25 - INFO - root -   Epoch: [105/300][20/200], lr: 0.00000036 	 loss = 2.5711(0.6135)
2023/11/21 20:52:44 - INFO - root -   Epoch: [105/300][40/200], lr: 0.00000036 	 loss = 1.0600(0.4864)
2023/11/21 20:52:55 - INFO - root -   Epoch: [105/300][60/200], lr: 0.00000036 	 loss = 0.6724(0.4564)
2023/11/21 20:53:11 - INFO - root -   Epoch: [105/300][80/200], lr: 0.00000036 	 loss = 0.7321(0.4536)
2023/11/21 20:53:27 - INFO - root -   Epoch: [105/300][100/200], lr: 0.00000036 	 loss = 0.0324(0.4465)
2023/11/21 20:53:41 - INFO - root -   Epoch: [105/300][120/200], lr: 0.00000036 	 loss = 0.2399(0.4374)
2023/11/21 20:54:02 - INFO - root -   Epoch: [105/300][140/200], lr: 0.00000036 	 loss = 0.1037(0.4280)
2023/11/21 20:54:11 - INFO - root -   Epoch: [105/300][160/200], lr: 0.00000036 	 loss = 0.9360(0.4347)
2023/11/21 20:54:27 - INFO - root -   Epoch: [105/300][180/200], lr: 0.00000036 	 loss = 0.0270(0.4350)
2023/11/21 20:54:46 - INFO - root -   Epoch: [105/300] 	 loss = 0.4396
2023/11/21 20:54:46 - INFO - root -   train_accuracy = 0.8425
2023/11/21 20:54:48 - INFO - root -   Epoch: [106/300][0/200], lr: 0.00000036 	 loss = 1.1646(1.1646)
2023/11/21 20:55:01 - INFO - root -   Epoch: [106/300][20/200], lr: 0.00000036 	 loss = 1.1127(0.5242)
2023/11/21 20:55:21 - INFO - root -   Epoch: [106/300][40/200], lr: 0.00000036 	 loss = 1.8521(0.5238)
2023/11/21 20:55:29 - INFO - root -   Epoch: [106/300][60/200], lr: 0.00000036 	 loss = 0.1448(0.4539)
2023/11/21 20:55:43 - INFO - root -   Epoch: [106/300][80/200], lr: 0.00000036 	 loss = 0.3924(0.4423)
2023/11/21 20:56:00 - INFO - root -   Epoch: [106/300][100/200], lr: 0.00000036 	 loss = 0.0797(0.4263)
2023/11/21 20:56:13 - INFO - root -   Epoch: [106/300][120/200], lr: 0.00000036 	 loss = 0.2269(0.4393)
2023/11/21 20:56:21 - INFO - root -   Epoch: [106/300][140/200], lr: 0.00000036 	 loss = 0.0847(0.4277)
2023/11/21 20:56:33 - INFO - root -   Epoch: [106/300][160/200], lr: 0.00000036 	 loss = 0.8208(0.4376)
2023/11/21 20:56:57 - INFO - root -   Epoch: [106/300][180/200], lr: 0.00000036 	 loss = 0.2995(0.4525)
2023/11/21 20:57:04 - INFO - root -   Epoch: [106/300] 	 loss = 0.4590
2023/11/21 20:57:04 - INFO - root -   train_accuracy = 0.8400
2023/11/21 20:57:12 - INFO - root -   Epoch: [107/300][0/200], lr: 0.00000036 	 loss = 1.7805(1.7805)
2023/11/21 20:57:26 - INFO - root -   Epoch: [107/300][20/200], lr: 0.00000036 	 loss = 0.4564(0.4389)
2023/11/21 20:57:34 - INFO - root -   Epoch: [107/300][40/200], lr: 0.00000036 	 loss = 0.5246(0.3431)
2023/11/21 20:57:45 - INFO - root -   Epoch: [107/300][60/200], lr: 0.00000036 	 loss = 0.0671(0.3289)
2023/11/21 20:58:03 - INFO - root -   Epoch: [107/300][80/200], lr: 0.00000036 	 loss = 1.3133(0.3555)
2023/11/21 20:58:17 - INFO - root -   Epoch: [107/300][100/200], lr: 0.00000036 	 loss = 0.1330(0.3478)
2023/11/21 20:58:35 - INFO - root -   Epoch: [107/300][120/200], lr: 0.00000036 	 loss = 0.0717(0.3585)
2023/11/21 20:58:43 - INFO - root -   Epoch: [107/300][140/200], lr: 0.00000036 	 loss = 0.0284(0.3635)
2023/11/21 20:58:56 - INFO - root -   Epoch: [107/300][160/200], lr: 0.00000036 	 loss = 0.4195(0.3846)
2023/11/21 20:59:11 - INFO - root -   Epoch: [107/300][180/200], lr: 0.00000036 	 loss = 0.0259(0.3979)
2023/11/21 20:59:22 - INFO - root -   Epoch: [107/300] 	 loss = 0.4194
2023/11/21 20:59:22 - INFO - root -   train_accuracy = 0.8425
2023/11/21 20:59:31 - INFO - root -   Epoch: [108/300][0/200], lr: 0.00000037 	 loss = 2.2547(2.2547)
2023/11/21 20:59:42 - INFO - root -   Epoch: [108/300][20/200], lr: 0.00000037 	 loss = 0.8201(0.4728)
2023/11/21 20:59:56 - INFO - root -   Epoch: [108/300][40/200], lr: 0.00000037 	 loss = 1.3300(0.4256)
2023/11/21 21:00:10 - INFO - root -   Epoch: [108/300][60/200], lr: 0.00000037 	 loss = 0.1112(0.3981)
2023/11/21 21:00:22 - INFO - root -   Epoch: [108/300][80/200], lr: 0.00000037 	 loss = 1.0554(0.3741)
2023/11/21 21:00:38 - INFO - root -   Epoch: [108/300][100/200], lr: 0.00000037 	 loss = 0.0943(0.3847)
2023/11/21 21:00:59 - INFO - root -   Epoch: [108/300][120/200], lr: 0.00000037 	 loss = 0.1521(0.3847)
2023/11/21 21:01:12 - INFO - root -   Epoch: [108/300][140/200], lr: 0.00000037 	 loss = 0.0952(0.3930)
2023/11/21 21:01:33 - INFO - root -   Epoch: [108/300][160/200], lr: 0.00000037 	 loss = 1.0459(0.4029)
2023/11/21 21:01:43 - INFO - root -   Epoch: [108/300][180/200], lr: 0.00000037 	 loss = 0.2459(0.4065)
2023/11/21 21:01:55 - INFO - root -   Epoch: [108/300] 	 loss = 0.4241
2023/11/21 21:01:55 - INFO - root -   train_accuracy = 0.8450
2023/11/21 21:01:56 - INFO - root -   Epoch: [109/300][0/200], lr: 0.00000037 	 loss = 1.2522(1.2522)
2023/11/21 21:02:15 - INFO - root -   Epoch: [109/300][20/200], lr: 0.00000037 	 loss = 1.3839(0.4949)
2023/11/21 21:02:36 - INFO - root -   Epoch: [109/300][40/200], lr: 0.00000037 	 loss = 0.6088(0.3987)
2023/11/21 21:03:00 - INFO - root -   Epoch: [109/300][60/200], lr: 0.00000037 	 loss = 0.0426(0.4384)
2023/11/21 21:03:15 - INFO - root -   Epoch: [109/300][80/200], lr: 0.00000037 	 loss = 1.7168(0.4471)
2023/11/21 21:03:34 - INFO - root -   Epoch: [109/300][100/200], lr: 0.00000037 	 loss = 0.0980(0.4459)
2023/11/21 21:03:52 - INFO - root -   Epoch: [109/300][120/200], lr: 0.00000037 	 loss = 0.0470(0.4479)
2023/11/21 21:04:06 - INFO - root -   Epoch: [109/300][140/200], lr: 0.00000037 	 loss = 0.0121(0.4437)
2023/11/21 21:04:26 - INFO - root -   Epoch: [109/300][160/200], lr: 0.00000037 	 loss = 0.7952(0.4557)
2023/11/21 21:04:36 - INFO - root -   Epoch: [109/300][180/200], lr: 0.00000037 	 loss = 0.3361(0.4557)
2023/11/21 21:04:46 - INFO - root -   Epoch: [109/300] 	 loss = 0.4645
2023/11/21 21:06:47 - INFO - root -   precision = 0.8119
2023/11/21 21:06:47 - INFO - root -   eval_loss = 0.4378
2023/11/21 21:06:47 - INFO - root -   eval_acc = 0.8119
2023/11/21 21:06:48 - INFO - root -   train_accuracy = 0.8400
2023/11/21 21:06:57 - INFO - root -   Epoch: [110/300][0/200], lr: 0.00000037 	 loss = 1.4233(1.4233)
2023/11/21 21:07:14 - INFO - root -   Epoch: [110/300][20/200], lr: 0.00000037 	 loss = 2.2132(0.4829)
2023/11/21 21:07:35 - INFO - root -   Epoch: [110/300][40/200], lr: 0.00000037 	 loss = 0.5640(0.4696)
2023/11/21 21:07:52 - INFO - root -   Epoch: [110/300][60/200], lr: 0.00000037 	 loss = 1.0898(0.4718)
2023/11/21 21:08:11 - INFO - root -   Epoch: [110/300][80/200], lr: 0.00000037 	 loss = 1.2674(0.4594)
2023/11/21 21:08:22 - INFO - root -   Epoch: [110/300][100/200], lr: 0.00000037 	 loss = 0.0379(0.4390)
2023/11/21 21:08:37 - INFO - root -   Epoch: [110/300][120/200], lr: 0.00000037 	 loss = 0.1652(0.4272)
2023/11/21 21:08:50 - INFO - root -   Epoch: [110/300][140/200], lr: 0.00000037 	 loss = 0.0417(0.4193)
2023/11/21 21:08:59 - INFO - root -   Epoch: [110/300][160/200], lr: 0.00000037 	 loss = 0.2939(0.4218)
2023/11/21 21:09:25 - INFO - root -   Epoch: [110/300][180/200], lr: 0.00000037 	 loss = 0.2568(0.4214)
2023/11/21 21:09:33 - INFO - root -   Epoch: [110/300] 	 loss = 0.4330
2023/11/21 21:09:33 - INFO - root -   train_accuracy = 0.8475
2023/11/21 21:09:34 - INFO - root -   Epoch: [111/300][0/200], lr: 0.00000037 	 loss = 1.2325(1.2325)
2023/11/21 21:10:01 - INFO - root -   Epoch: [111/300][20/200], lr: 0.00000037 	 loss = 2.3472(0.4680)
2023/11/21 21:10:18 - INFO - root -   Epoch: [111/300][40/200], lr: 0.00000037 	 loss = 0.8918(0.4470)
2023/11/21 21:10:27 - INFO - root -   Epoch: [111/300][60/200], lr: 0.00000037 	 loss = 0.1079(0.4136)
2023/11/21 21:10:43 - INFO - root -   Epoch: [111/300][80/200], lr: 0.00000037 	 loss = 0.4060(0.3933)
2023/11/21 21:11:03 - INFO - root -   Epoch: [111/300][100/200], lr: 0.00000037 	 loss = 0.0453(0.3962)
2023/11/21 21:11:21 - INFO - root -   Epoch: [111/300][120/200], lr: 0.00000037 	 loss = 0.1042(0.4081)
2023/11/21 21:11:36 - INFO - root -   Epoch: [111/300][140/200], lr: 0.00000037 	 loss = 0.0145(0.4103)
2023/11/21 21:11:54 - INFO - root -   Epoch: [111/300][160/200], lr: 0.00000037 	 loss = 1.1094(0.4252)
2023/11/21 21:12:10 - INFO - root -   Epoch: [111/300][180/200], lr: 0.00000037 	 loss = 0.3500(0.4258)
2023/11/21 21:12:18 - INFO - root -   Epoch: [111/300] 	 loss = 0.4239
2023/11/21 21:12:18 - INFO - root -   train_accuracy = 0.8475
2023/11/21 21:12:19 - INFO - root -   Epoch: [112/300][0/200], lr: 0.00000038 	 loss = 1.1917(1.1917)
2023/11/21 21:12:52 - INFO - root -   Epoch: [112/300][20/200], lr: 0.00000038 	 loss = 0.5479(0.4503)
2023/11/21 21:13:05 - INFO - root -   Epoch: [112/300][40/200], lr: 0.00000038 	 loss = 0.3598(0.4090)
2023/11/21 21:13:18 - INFO - root -   Epoch: [112/300][60/200], lr: 0.00000038 	 loss = 0.1326(0.3949)
2023/11/21 21:13:38 - INFO - root -   Epoch: [112/300][80/200], lr: 0.00000038 	 loss = 1.0923(0.4105)
2023/11/21 21:13:55 - INFO - root -   Epoch: [112/300][100/200], lr: 0.00000038 	 loss = 0.0755(0.4090)
2023/11/21 21:14:08 - INFO - root -   Epoch: [112/300][120/200], lr: 0.00000038 	 loss = 0.5132(0.4046)
2023/11/21 21:14:24 - INFO - root -   Epoch: [112/300][140/200], lr: 0.00000038 	 loss = 0.0726(0.3980)
2023/11/21 21:14:45 - INFO - root -   Epoch: [112/300][160/200], lr: 0.00000038 	 loss = 0.6524(0.4263)
2023/11/21 21:14:58 - INFO - root -   Epoch: [112/300][180/200], lr: 0.00000038 	 loss = 0.1126(0.4216)
2023/11/21 21:15:09 - INFO - root -   Epoch: [112/300] 	 loss = 0.4291
2023/11/21 21:15:09 - INFO - root -   train_accuracy = 0.8575
2023/11/21 21:15:10 - INFO - root -   Epoch: [113/300][0/200], lr: 0.00000038 	 loss = 1.8684(1.8684)
2023/11/21 21:15:34 - INFO - root -   Epoch: [113/300][20/200], lr: 0.00000038 	 loss = 0.9691(0.3715)
2023/11/21 21:15:55 - INFO - root -   Epoch: [113/300][40/200], lr: 0.00000038 	 loss = 1.1981(0.4779)
2023/11/21 21:16:13 - INFO - root -   Epoch: [113/300][60/200], lr: 0.00000038 	 loss = 0.0468(0.4238)
2023/11/21 21:16:25 - INFO - root -   Epoch: [113/300][80/200], lr: 0.00000038 	 loss = 0.4062(0.4057)
2023/11/21 21:16:34 - INFO - root -   Epoch: [113/300][100/200], lr: 0.00000038 	 loss = 0.1321(0.3993)
2023/11/21 21:17:03 - INFO - root -   Epoch: [113/300][120/200], lr: 0.00000038 	 loss = 0.8881(0.4432)
2023/11/21 21:17:12 - INFO - root -   Epoch: [113/300][140/200], lr: 0.00000038 	 loss = 0.1652(0.4321)
2023/11/21 21:17:24 - INFO - root -   Epoch: [113/300][160/200], lr: 0.00000038 	 loss = 0.4385(0.4378)
2023/11/21 21:17:50 - INFO - root -   Epoch: [113/300][180/200], lr: 0.00000038 	 loss = 0.0226(0.4454)
2023/11/21 21:18:00 - INFO - root -   Epoch: [113/300] 	 loss = 0.4379
2023/11/21 21:18:00 - INFO - root -   train_accuracy = 0.8350
2023/11/21 21:18:01 - INFO - root -   Epoch: [114/300][0/200], lr: 0.00000038 	 loss = 0.6430(0.6430)
2023/11/21 21:18:25 - INFO - root -   Epoch: [114/300][20/200], lr: 0.00000038 	 loss = 0.7916(0.5175)
2023/11/21 21:18:41 - INFO - root -   Epoch: [114/300][40/200], lr: 0.00000038 	 loss = 1.8029(0.5079)
2023/11/21 21:18:49 - INFO - root -   Epoch: [114/300][60/200], lr: 0.00000038 	 loss = 0.1971(0.4301)
2023/11/21 21:19:05 - INFO - root -   Epoch: [114/300][80/200], lr: 0.00000038 	 loss = 0.8235(0.4027)
2023/11/21 21:19:15 - INFO - root -   Epoch: [114/300][100/200], lr: 0.00000038 	 loss = 0.0301(0.4118)
2023/11/21 21:19:26 - INFO - root -   Epoch: [114/300][120/200], lr: 0.00000038 	 loss = 0.5931(0.4124)
2023/11/21 21:19:41 - INFO - root -   Epoch: [114/300][140/200], lr: 0.00000038 	 loss = 0.0637(0.4080)
2023/11/21 21:20:00 - INFO - root -   Epoch: [114/300][160/200], lr: 0.00000038 	 loss = 1.1899(0.4193)
2023/11/21 21:20:08 - INFO - root -   Epoch: [114/300][180/200], lr: 0.00000038 	 loss = 0.1587(0.4332)
2023/11/21 21:20:24 - INFO - root -   Epoch: [114/300] 	 loss = 0.4390
2023/11/21 21:22:26 - INFO - root -   precision = 0.8218
2023/11/21 21:22:26 - INFO - root -   eval_loss = 0.4379
2023/11/21 21:22:26 - INFO - root -   eval_acc = 0.8218
2023/11/21 21:22:27 - INFO - root -   train_accuracy = 0.8375
2023/11/21 21:22:28 - INFO - root -   Epoch: [115/300][0/200], lr: 0.00000038 	 loss = 0.8101(0.8101)
2023/11/21 21:22:47 - INFO - root -   Epoch: [115/300][20/200], lr: 0.00000038 	 loss = 0.8004(0.5137)
2023/11/21 21:22:59 - INFO - root -   Epoch: [115/300][40/200], lr: 0.00000038 	 loss = 0.4322(0.4148)
2023/11/21 21:23:15 - INFO - root -   Epoch: [115/300][60/200], lr: 0.00000038 	 loss = 0.5335(0.3979)
2023/11/21 21:23:38 - INFO - root -   Epoch: [115/300][80/200], lr: 0.00000038 	 loss = 1.1288(0.4276)
2023/11/21 21:23:54 - INFO - root -   Epoch: [115/300][100/200], lr: 0.00000038 	 loss = 0.0118(0.4563)
2023/11/21 21:24:14 - INFO - root -   Epoch: [115/300][120/200], lr: 0.00000038 	 loss = 0.5830(0.4508)
2023/11/21 21:24:33 - INFO - root -   Epoch: [115/300][140/200], lr: 0.00000038 	 loss = 0.0999(0.4481)
2023/11/21 21:24:55 - INFO - root -   Epoch: [115/300][160/200], lr: 0.00000038 	 loss = 0.3826(0.4565)
2023/11/21 21:25:05 - INFO - root -   Epoch: [115/300][180/200], lr: 0.00000038 	 loss = 0.1776(0.4656)
2023/11/21 21:25:18 - INFO - root -   Epoch: [115/300] 	 loss = 0.4730
2023/11/21 21:25:18 - INFO - root -   train_accuracy = 0.8250
2023/11/21 21:25:24 - INFO - root -   Epoch: [116/300][0/200], lr: 0.00000039 	 loss = 1.2777(1.2777)
2023/11/21 21:25:39 - INFO - root -   Epoch: [116/300][20/200], lr: 0.00000039 	 loss = 1.7030(0.3673)
2023/11/21 21:25:52 - INFO - root -   Epoch: [116/300][40/200], lr: 0.00000039 	 loss = 1.3600(0.3872)
2023/11/21 21:26:07 - INFO - root -   Epoch: [116/300][60/200], lr: 0.00000039 	 loss = 0.1940(0.3855)
2023/11/21 21:26:25 - INFO - root -   Epoch: [116/300][80/200], lr: 0.00000039 	 loss = 0.9559(0.3728)
2023/11/21 21:26:33 - INFO - root -   Epoch: [116/300][100/200], lr: 0.00000039 	 loss = 0.3221(0.3678)
2023/11/21 21:26:44 - INFO - root -   Epoch: [116/300][120/200], lr: 0.00000039 	 loss = 0.3090(0.3771)
2023/11/21 21:27:03 - INFO - root -   Epoch: [116/300][140/200], lr: 0.00000039 	 loss = 0.1942(0.3823)
2023/11/21 21:27:23 - INFO - root -   Epoch: [116/300][160/200], lr: 0.00000039 	 loss = 0.8705(0.4039)
2023/11/21 21:27:31 - INFO - root -   Epoch: [116/300][180/200], lr: 0.00000039 	 loss = 0.0921(0.4073)
2023/11/21 21:27:45 - INFO - root -   Epoch: [116/300] 	 loss = 0.4147
2023/11/21 21:27:45 - INFO - root -   train_accuracy = 0.8625
2023/11/21 21:27:52 - INFO - root -   Epoch: [117/300][0/200], lr: 0.00000039 	 loss = 2.3339(2.3339)
2023/11/21 21:28:07 - INFO - root -   Epoch: [117/300][20/200], lr: 0.00000039 	 loss = 2.0967(0.4811)
2023/11/21 21:28:18 - INFO - root -   Epoch: [117/300][40/200], lr: 0.00000039 	 loss = 0.1937(0.4252)
2023/11/21 21:28:30 - INFO - root -   Epoch: [117/300][60/200], lr: 0.00000039 	 loss = 0.1509(0.4055)
2023/11/21 21:28:39 - INFO - root -   Epoch: [117/300][80/200], lr: 0.00000039 	 loss = 0.3966(0.4081)
2023/11/21 21:29:02 - INFO - root -   Epoch: [117/300][100/200], lr: 0.00000039 	 loss = 0.0050(0.4126)
2023/11/21 21:29:15 - INFO - root -   Epoch: [117/300][120/200], lr: 0.00000039 	 loss = 0.3237(0.3984)
2023/11/21 21:29:28 - INFO - root -   Epoch: [117/300][140/200], lr: 0.00000039 	 loss = 0.1024(0.4090)
2023/11/21 21:29:44 - INFO - root -   Epoch: [117/300][160/200], lr: 0.00000039 	 loss = 1.0938(0.4439)
2023/11/21 21:29:52 - INFO - root -   Epoch: [117/300][180/200], lr: 0.00000039 	 loss = 0.0462(0.4370)
2023/11/21 21:30:06 - INFO - root -   Epoch: [117/300] 	 loss = 0.4580
2023/11/21 21:30:06 - INFO - root -   train_accuracy = 0.8400
2023/11/21 21:30:07 - INFO - root -   Epoch: [118/300][0/200], lr: 0.00000039 	 loss = 2.1848(2.1848)
2023/11/21 21:30:29 - INFO - root -   Epoch: [118/300][20/200], lr: 0.00000039 	 loss = 0.9879(0.4922)
2023/11/21 21:30:50 - INFO - root -   Epoch: [118/300][40/200], lr: 0.00000039 	 loss = 1.4987(0.4632)
2023/11/21 21:30:58 - INFO - root -   Epoch: [118/300][60/200], lr: 0.00000039 	 loss = 0.1879(0.4335)
2023/11/21 21:31:12 - INFO - root -   Epoch: [118/300][80/200], lr: 0.00000039 	 loss = 0.8028(0.4462)
2023/11/21 21:31:23 - INFO - root -   Epoch: [118/300][100/200], lr: 0.00000039 	 loss = 0.0278(0.4248)
2023/11/21 21:31:38 - INFO - root -   Epoch: [118/300][120/200], lr: 0.00000039 	 loss = 0.4025(0.4375)
2023/11/21 21:31:48 - INFO - root -   Epoch: [118/300][140/200], lr: 0.00000039 	 loss = 0.1017(0.4305)
2023/11/21 21:32:07 - INFO - root -   Epoch: [118/300][160/200], lr: 0.00000039 	 loss = 1.2863(0.4305)
2023/11/21 21:32:18 - INFO - root -   Epoch: [118/300][180/200], lr: 0.00000039 	 loss = 0.0388(0.4396)
2023/11/21 21:32:31 - INFO - root -   Epoch: [118/300] 	 loss = 0.4430
2023/11/21 21:32:31 - INFO - root -   train_accuracy = 0.8350
2023/11/21 21:32:32 - INFO - root -   Epoch: [119/300][0/200], lr: 0.00000039 	 loss = 1.3654(1.3654)
2023/11/21 21:32:50 - INFO - root -   Epoch: [119/300][20/200], lr: 0.00000039 	 loss = 1.9160(0.5273)
2023/11/21 21:33:09 - INFO - root -   Epoch: [119/300][40/200], lr: 0.00000039 	 loss = 0.5657(0.4303)
2023/11/21 21:33:17 - INFO - root -   Epoch: [119/300][60/200], lr: 0.00000039 	 loss = 0.1967(0.3897)
2023/11/21 21:33:32 - INFO - root -   Epoch: [119/300][80/200], lr: 0.00000039 	 loss = 0.8566(0.4010)
2023/11/21 21:33:49 - INFO - root -   Epoch: [119/300][100/200], lr: 0.00000039 	 loss = 0.0728(0.3896)
2023/11/21 21:34:04 - INFO - root -   Epoch: [119/300][120/200], lr: 0.00000039 	 loss = 0.2750(0.3872)
2023/11/21 21:34:12 - INFO - root -   Epoch: [119/300][140/200], lr: 0.00000039 	 loss = 0.1180(0.3788)
2023/11/21 21:34:30 - INFO - root -   Epoch: [119/300][160/200], lr: 0.00000039 	 loss = 0.6099(0.4077)
2023/11/21 21:34:48 - INFO - root -   Epoch: [119/300][180/200], lr: 0.00000039 	 loss = 0.2896(0.4081)
2023/11/21 21:34:58 - INFO - root -   Epoch: [119/300] 	 loss = 0.4090
2023/11/21 21:37:00 - INFO - root -   precision = 0.8614
2023/11/21 21:37:00 - INFO - root -   eval_loss = 0.4193
2023/11/21 21:37:00 - INFO - root -   eval_acc = 0.8614
2023/11/21 21:37:01 - INFO - root -   train_accuracy = 0.8575
2023/11/21 21:37:03 - INFO - root -   Epoch: [120/300][0/200], lr: 0.00000040 	 loss = 2.1099(2.1099)
2023/11/21 21:37:21 - INFO - root -   Epoch: [120/300][20/200], lr: 0.00000040 	 loss = 0.9966(0.4029)
2023/11/21 21:37:46 - INFO - root -   Epoch: [120/300][40/200], lr: 0.00000040 	 loss = 0.5295(0.4453)
2023/11/21 21:38:04 - INFO - root -   Epoch: [120/300][60/200], lr: 0.00000040 	 loss = 0.3467(0.4420)
2023/11/21 21:38:17 - INFO - root -   Epoch: [120/300][80/200], lr: 0.00000040 	 loss = 0.9150(0.4324)
2023/11/21 21:38:36 - INFO - root -   Epoch: [120/300][100/200], lr: 0.00000040 	 loss = 0.0257(0.4022)
2023/11/21 21:38:56 - INFO - root -   Epoch: [120/300][120/200], lr: 0.00000040 	 loss = 0.4068(0.3978)
2023/11/21 21:39:09 - INFO - root -   Epoch: [120/300][140/200], lr: 0.00000040 	 loss = 0.1251(0.3849)
2023/11/21 21:39:25 - INFO - root -   Epoch: [120/300][160/200], lr: 0.00000040 	 loss = 0.4084(0.3975)
2023/11/21 21:39:35 - INFO - root -   Epoch: [120/300][180/200], lr: 0.00000040 	 loss = 0.3038(0.4018)
2023/11/21 21:39:42 - INFO - root -   Epoch: [120/300] 	 loss = 0.4051
2023/11/21 21:39:42 - INFO - root -   train_accuracy = 0.8525
2023/11/21 21:39:52 - INFO - root -   Epoch: [121/300][0/200], lr: 0.00000040 	 loss = 1.3367(1.3367)
2023/11/21 21:40:07 - INFO - root -   Epoch: [121/300][20/200], lr: 0.00000040 	 loss = 1.8840(0.4344)
2023/11/21 21:40:28 - INFO - root -   Epoch: [121/300][40/200], lr: 0.00000040 	 loss = 0.4689(0.4819)
2023/11/21 21:40:44 - INFO - root -   Epoch: [121/300][60/200], lr: 0.00000040 	 loss = 0.1354(0.4187)
2023/11/21 21:40:58 - INFO - root -   Epoch: [121/300][80/200], lr: 0.00000040 	 loss = 0.6965(0.4026)
2023/11/21 21:41:11 - INFO - root -   Epoch: [121/300][100/200], lr: 0.00000040 	 loss = 0.0710(0.3997)
2023/11/21 21:41:29 - INFO - root -   Epoch: [121/300][120/200], lr: 0.00000040 	 loss = 0.1092(0.3854)
2023/11/21 21:41:37 - INFO - root -   Epoch: [121/300][140/200], lr: 0.00000040 	 loss = 0.2348(0.3887)
2023/11/21 21:41:55 - INFO - root -   Epoch: [121/300][160/200], lr: 0.00000040 	 loss = 0.9984(0.3961)
2023/11/21 21:42:13 - INFO - root -   Epoch: [121/300][180/200], lr: 0.00000040 	 loss = 0.1339(0.4181)
2023/11/21 21:42:23 - INFO - root -   Epoch: [121/300] 	 loss = 0.4359
2023/11/21 21:42:23 - INFO - root -   train_accuracy = 0.8475
2023/11/21 21:42:36 - INFO - root -   Epoch: [122/300][0/200], lr: 0.00000040 	 loss = 2.7927(2.7927)
2023/11/21 21:42:50 - INFO - root -   Epoch: [122/300][20/200], lr: 0.00000040 	 loss = 0.9704(0.5862)
2023/11/21 21:43:07 - INFO - root -   Epoch: [122/300][40/200], lr: 0.00000040 	 loss = 1.0185(0.4811)
2023/11/21 21:43:20 - INFO - root -   Epoch: [122/300][60/200], lr: 0.00000040 	 loss = 0.5616(0.4166)
2023/11/21 21:43:35 - INFO - root -   Epoch: [122/300][80/200], lr: 0.00000040 	 loss = 1.4837(0.4136)
2023/11/21 21:43:52 - INFO - root -   Epoch: [122/300][100/200], lr: 0.00000040 	 loss = 0.0214(0.4186)
2023/11/21 21:44:03 - INFO - root -   Epoch: [122/300][120/200], lr: 0.00000040 	 loss = 0.5655(0.4283)
2023/11/21 21:44:23 - INFO - root -   Epoch: [122/300][140/200], lr: 0.00000040 	 loss = 0.1912(0.4141)
2023/11/21 21:44:40 - INFO - root -   Epoch: [122/300][160/200], lr: 0.00000040 	 loss = 0.6094(0.4165)
2023/11/21 21:44:54 - INFO - root -   Epoch: [122/300][180/200], lr: 0.00000040 	 loss = 0.0200(0.4150)
2023/11/21 21:45:04 - INFO - root -   Epoch: [122/300] 	 loss = 0.4288
2023/11/21 21:45:04 - INFO - root -   train_accuracy = 0.8400
2023/11/21 21:45:06 - INFO - root -   Epoch: [123/300][0/200], lr: 0.00000040 	 loss = 1.5964(1.5964)
2023/11/21 21:45:20 - INFO - root -   Epoch: [123/300][20/200], lr: 0.00000040 	 loss = 0.5445(0.3451)
2023/11/21 21:45:39 - INFO - root -   Epoch: [123/300][40/200], lr: 0.00000040 	 loss = 0.5462(0.3149)
2023/11/21 21:45:50 - INFO - root -   Epoch: [123/300][60/200], lr: 0.00000040 	 loss = 0.2011(0.3329)
2023/11/21 21:46:07 - INFO - root -   Epoch: [123/300][80/200], lr: 0.00000040 	 loss = 1.1045(0.3773)
2023/11/21 21:46:19 - INFO - root -   Epoch: [123/300][100/200], lr: 0.00000040 	 loss = 0.2705(0.3958)
2023/11/21 21:46:35 - INFO - root -   Epoch: [123/300][120/200], lr: 0.00000040 	 loss = 0.2984(0.3945)
2023/11/21 21:46:49 - INFO - root -   Epoch: [123/300][140/200], lr: 0.00000040 	 loss = 0.0474(0.3895)
2023/11/21 21:47:12 - INFO - root -   Epoch: [123/300][160/200], lr: 0.00000040 	 loss = 1.3470(0.4125)
2023/11/21 21:47:28 - INFO - root -   Epoch: [123/300][180/200], lr: 0.00000040 	 loss = 0.0299(0.4163)
2023/11/21 21:47:36 - INFO - root -   Epoch: [123/300] 	 loss = 0.4234
2023/11/21 21:47:36 - INFO - root -   train_accuracy = 0.8425
2023/11/21 21:47:43 - INFO - root -   Epoch: [124/300][0/200], lr: 0.00000041 	 loss = 0.5951(0.5951)
2023/11/21 21:47:57 - INFO - root -   Epoch: [124/300][20/200], lr: 0.00000041 	 loss = 0.8706(0.3052)
2023/11/21 21:48:15 - INFO - root -   Epoch: [124/300][40/200], lr: 0.00000041 	 loss = 0.4094(0.2700)
2023/11/21 21:48:30 - INFO - root -   Epoch: [124/300][60/200], lr: 0.00000041 	 loss = 0.3352(0.2879)
2023/11/21 21:48:44 - INFO - root -   Epoch: [124/300][80/200], lr: 0.00000041 	 loss = 0.3713(0.2945)
2023/11/21 21:49:03 - INFO - root -   Epoch: [124/300][100/200], lr: 0.00000041 	 loss = 0.0110(0.3018)
2023/11/21 21:49:15 - INFO - root -   Epoch: [124/300][120/200], lr: 0.00000041 	 loss = 0.0868(0.3094)
2023/11/21 21:49:31 - INFO - root -   Epoch: [124/300][140/200], lr: 0.00000041 	 loss = 0.0485(0.3040)
2023/11/21 21:49:46 - INFO - root -   Epoch: [124/300][160/200], lr: 0.00000041 	 loss = 1.9698(0.3282)
2023/11/21 21:49:59 - INFO - root -   Epoch: [124/300][180/200], lr: 0.00000041 	 loss = 0.0303(0.3177)
2023/11/21 21:50:07 - INFO - root -   Epoch: [124/300] 	 loss = 0.3202
2023/11/21 21:52:09 - INFO - root -   precision = 0.8515
2023/11/21 21:52:09 - INFO - root -   eval_loss = 0.4304
2023/11/21 21:52:09 - INFO - root -   eval_acc = 0.8515
2023/11/21 21:52:10 - INFO - root -   train_accuracy = 0.8775
2023/11/21 21:52:19 - INFO - root -   Epoch: [125/300][0/200], lr: 0.00000041 	 loss = 2.1713(2.1713)
2023/11/21 21:52:33 - INFO - root -   Epoch: [125/300][20/200], lr: 0.00000041 	 loss = 0.5962(0.4492)
2023/11/21 21:52:52 - INFO - root -   Epoch: [125/300][40/200], lr: 0.00000041 	 loss = 1.3027(0.3757)
2023/11/21 21:53:06 - INFO - root -   Epoch: [125/300][60/200], lr: 0.00000041 	 loss = 0.0398(0.3298)
2023/11/21 21:53:21 - INFO - root -   Epoch: [125/300][80/200], lr: 0.00000041 	 loss = 1.8562(0.3458)
2023/11/21 21:53:32 - INFO - root -   Epoch: [125/300][100/200], lr: 0.00000041 	 loss = 0.0442(0.3524)
2023/11/21 21:53:50 - INFO - root -   Epoch: [125/300][120/200], lr: 0.00000041 	 loss = 0.1107(0.3539)
2023/11/21 21:54:03 - INFO - root -   Epoch: [125/300][140/200], lr: 0.00000041 	 loss = 0.2149(0.3631)
2023/11/21 21:54:11 - INFO - root -   Epoch: [125/300][160/200], lr: 0.00000041 	 loss = 0.5310(0.4042)
2023/11/21 21:54:27 - INFO - root -   Epoch: [125/300][180/200], lr: 0.00000041 	 loss = 0.2306(0.4049)
2023/11/21 21:54:37 - INFO - root -   Epoch: [125/300] 	 loss = 0.4162
2023/11/21 21:54:37 - INFO - root -   train_accuracy = 0.8625
2023/11/21 21:54:46 - INFO - root -   Epoch: [126/300][0/200], lr: 0.00000041 	 loss = 0.7438(0.7438)
2023/11/21 21:55:00 - INFO - root -   Epoch: [126/300][20/200], lr: 0.00000041 	 loss = 2.4874(0.4073)
2023/11/21 21:55:16 - INFO - root -   Epoch: [126/300][40/200], lr: 0.00000041 	 loss = 0.8494(0.4553)
2023/11/21 21:55:32 - INFO - root -   Epoch: [126/300][60/200], lr: 0.00000041 	 loss = 0.2068(0.4047)
2023/11/21 21:55:40 - INFO - root -   Epoch: [126/300][80/200], lr: 0.00000041 	 loss = 1.4294(0.4111)
2023/11/21 21:55:59 - INFO - root -   Epoch: [126/300][100/200], lr: 0.00000041 	 loss = 0.0866(0.4152)
2023/11/21 21:56:06 - INFO - root -   Epoch: [126/300][120/200], lr: 0.00000041 	 loss = 0.2668(0.4188)
2023/11/21 21:56:23 - INFO - root -   Epoch: [126/300][140/200], lr: 0.00000041 	 loss = 0.2344(0.4229)
2023/11/21 21:56:38 - INFO - root -   Epoch: [126/300][160/200], lr: 0.00000041 	 loss = 1.8763(0.4387)
2023/11/21 21:56:58 - INFO - root -   Epoch: [126/300][180/200], lr: 0.00000041 	 loss = 0.0219(0.4290)
2023/11/21 21:57:05 - INFO - root -   Epoch: [126/300] 	 loss = 0.4307
2023/11/21 21:57:05 - INFO - root -   train_accuracy = 0.8400
2023/11/21 21:57:12 - INFO - root -   Epoch: [127/300][0/200], lr: 0.00000041 	 loss = 1.8356(1.8356)
2023/11/21 21:57:28 - INFO - root -   Epoch: [127/300][20/200], lr: 0.00000041 	 loss = 2.4369(0.6053)
2023/11/21 21:57:44 - INFO - root -   Epoch: [127/300][40/200], lr: 0.00000041 	 loss = 0.5743(0.4837)
2023/11/21 21:58:01 - INFO - root -   Epoch: [127/300][60/200], lr: 0.00000041 	 loss = 0.4369(0.4617)
2023/11/21 21:58:15 - INFO - root -   Epoch: [127/300][80/200], lr: 0.00000041 	 loss = 1.2748(0.4523)
2023/11/21 21:58:29 - INFO - root -   Epoch: [127/300][100/200], lr: 0.00000041 	 loss = 0.0536(0.4267)
2023/11/21 21:58:46 - INFO - root -   Epoch: [127/300][120/200], lr: 0.00000041 	 loss = 0.3534(0.4105)
2023/11/21 21:59:01 - INFO - root -   Epoch: [127/300][140/200], lr: 0.00000041 	 loss = 0.2243(0.3865)
2023/11/21 21:59:08 - INFO - root -   Epoch: [127/300][160/200], lr: 0.00000041 	 loss = 0.5264(0.3899)
2023/11/21 21:59:20 - INFO - root -   Epoch: [127/300][180/200], lr: 0.00000041 	 loss = 0.0349(0.3793)
2023/11/21 21:59:29 - INFO - root -   Epoch: [127/300] 	 loss = 0.3926
2023/11/21 21:59:29 - INFO - root -   train_accuracy = 0.8600
2023/11/21 21:59:30 - INFO - root -   Epoch: [128/300][0/200], lr: 0.00000042 	 loss = 0.8730(0.8730)
2023/11/21 21:59:45 - INFO - root -   Epoch: [128/300][20/200], lr: 0.00000042 	 loss = 0.8790(0.3520)
2023/11/21 22:00:04 - INFO - root -   Epoch: [128/300][40/200], lr: 0.00000042 	 loss = 0.2848(0.3002)
2023/11/21 22:00:12 - INFO - root -   Epoch: [128/300][60/200], lr: 0.00000042 	 loss = 0.3080(0.2982)
2023/11/21 22:00:36 - INFO - root -   Epoch: [128/300][80/200], lr: 0.00000042 	 loss = 0.4147(0.3118)
2023/11/21 22:00:53 - INFO - root -   Epoch: [128/300][100/200], lr: 0.00000042 	 loss = 0.0927(0.3300)
2023/11/21 22:01:01 - INFO - root -   Epoch: [128/300][120/200], lr: 0.00000042 	 loss = 0.0889(0.3264)
2023/11/21 22:01:13 - INFO - root -   Epoch: [128/300][140/200], lr: 0.00000042 	 loss = 0.2528(0.3274)
2023/11/21 22:01:32 - INFO - root -   Epoch: [128/300][160/200], lr: 0.00000042 	 loss = 1.0826(0.3377)
2023/11/21 22:01:52 - INFO - root -   Epoch: [128/300][180/200], lr: 0.00000042 	 loss = 0.0332(0.3548)
2023/11/21 22:02:01 - INFO - root -   Epoch: [128/300] 	 loss = 0.3552
2023/11/21 22:02:01 - INFO - root -   train_accuracy = 0.8700
2023/11/21 22:02:02 - INFO - root -   Epoch: [129/300][0/200], lr: 0.00000042 	 loss = 1.2112(1.2112)
2023/11/21 22:02:21 - INFO - root -   Epoch: [129/300][20/200], lr: 0.00000042 	 loss = 0.6567(0.4237)
2023/11/21 22:02:34 - INFO - root -   Epoch: [129/300][40/200], lr: 0.00000042 	 loss = 0.4576(0.3955)
2023/11/21 22:02:45 - INFO - root -   Epoch: [129/300][60/200], lr: 0.00000042 	 loss = 0.0442(0.3742)
2023/11/21 22:02:57 - INFO - root -   Epoch: [129/300][80/200], lr: 0.00000042 	 loss = 0.7022(0.3865)
2023/11/21 22:03:09 - INFO - root -   Epoch: [129/300][100/200], lr: 0.00000042 	 loss = 0.1847(0.3675)
2023/11/21 22:03:21 - INFO - root -   Epoch: [129/300][120/200], lr: 0.00000042 	 loss = 0.2528(0.3627)
2023/11/21 22:03:38 - INFO - root -   Epoch: [129/300][140/200], lr: 0.00000042 	 loss = 0.0599(0.3746)
2023/11/21 22:03:45 - INFO - root -   Epoch: [129/300][160/200], lr: 0.00000042 	 loss = 0.2803(0.3783)
2023/11/21 22:03:57 - INFO - root -   Epoch: [129/300][180/200], lr: 0.00000042 	 loss = 0.0565(0.3807)
2023/11/21 22:04:07 - INFO - root -   Epoch: [129/300] 	 loss = 0.3976
2023/11/21 22:06:07 - INFO - root -   precision = 0.8416
2023/11/21 22:06:07 - INFO - root -   eval_loss = 0.4262
2023/11/21 22:06:07 - INFO - root -   eval_acc = 0.8416
2023/11/21 22:06:08 - INFO - root -   train_accuracy = 0.8600
2023/11/21 22:06:09 - INFO - root -   Epoch: [130/300][0/200], lr: 0.00000042 	 loss = 1.6320(1.6320)
2023/11/21 22:06:23 - INFO - root -   Epoch: [130/300][20/200], lr: 0.00000042 	 loss = 2.2281(0.6005)
2023/11/21 22:06:41 - INFO - root -   Epoch: [130/300][40/200], lr: 0.00000042 	 loss = 0.4696(0.4711)
2023/11/21 22:06:49 - INFO - root -   Epoch: [130/300][60/200], lr: 0.00000042 	 loss = 0.1336(0.4415)
2023/11/21 22:07:04 - INFO - root -   Epoch: [130/300][80/200], lr: 0.00000042 	 loss = 1.0371(0.4180)
2023/11/21 22:07:21 - INFO - root -   Epoch: [130/300][100/200], lr: 0.00000042 	 loss = 0.0110(0.4056)
2023/11/21 22:07:39 - INFO - root -   Epoch: [130/300][120/200], lr: 0.00000042 	 loss = 0.1331(0.3985)
2023/11/21 22:07:53 - INFO - root -   Epoch: [130/300][140/200], lr: 0.00000042 	 loss = 0.0351(0.3996)
2023/11/21 22:08:06 - INFO - root -   Epoch: [130/300][160/200], lr: 0.00000042 	 loss = 0.4813(0.4011)
2023/11/21 22:08:16 - INFO - root -   Epoch: [130/300][180/200], lr: 0.00000042 	 loss = 0.1474(0.4105)
2023/11/21 22:08:31 - INFO - root -   Epoch: [130/300] 	 loss = 0.4021
2023/11/21 22:08:31 - INFO - root -   train_accuracy = 0.8600
2023/11/21 22:08:33 - INFO - root -   Epoch: [131/300][0/200], lr: 0.00000042 	 loss = 0.9616(0.9616)
2023/11/21 22:08:48 - INFO - root -   Epoch: [131/300][20/200], lr: 0.00000042 	 loss = 0.9487(0.4665)
2023/11/21 22:09:09 - INFO - root -   Epoch: [131/300][40/200], lr: 0.00000042 	 loss = 1.7413(0.4401)
2023/11/21 22:09:23 - INFO - root -   Epoch: [131/300][60/200], lr: 0.00000042 	 loss = 0.1163(0.3887)
2023/11/21 22:09:42 - INFO - root -   Epoch: [131/300][80/200], lr: 0.00000042 	 loss = 1.0091(0.3630)
2023/11/21 22:10:00 - INFO - root -   Epoch: [131/300][100/200], lr: 0.00000042 	 loss = 0.0904(0.3609)
2023/11/21 22:10:08 - INFO - root -   Epoch: [131/300][120/200], lr: 0.00000042 	 loss = 1.2774(0.3649)
2023/11/21 22:10:32 - INFO - root -   Epoch: [131/300][140/200], lr: 0.00000042 	 loss = 0.0790(0.3629)
2023/11/21 22:10:42 - INFO - root -   Epoch: [131/300][160/200], lr: 0.00000042 	 loss = 0.7066(0.3684)
2023/11/21 22:10:57 - INFO - root -   Epoch: [131/300][180/200], lr: 0.00000042 	 loss = 0.0497(0.3764)
2023/11/21 22:11:10 - INFO - root -   Epoch: [131/300] 	 loss = 0.3863
2023/11/21 22:11:10 - INFO - root -   train_accuracy = 0.8500
2023/11/21 22:11:20 - INFO - root -   Epoch: [132/300][0/200], lr: 0.00000043 	 loss = 0.7210(0.7210)
2023/11/21 22:11:36 - INFO - root -   Epoch: [132/300][20/200], lr: 0.00000043 	 loss = 0.7801(0.4852)
2023/11/21 22:11:56 - INFO - root -   Epoch: [132/300][40/200], lr: 0.00000043 	 loss = 0.2722(0.4446)
2023/11/21 22:12:11 - INFO - root -   Epoch: [132/300][60/200], lr: 0.00000043 	 loss = 0.0344(0.3818)
2023/11/21 22:12:20 - INFO - root -   Epoch: [132/300][80/200], lr: 0.00000043 	 loss = 0.5027(0.3583)
2023/11/21 22:12:33 - INFO - root -   Epoch: [132/300][100/200], lr: 0.00000043 	 loss = 0.3547(0.3681)
2023/11/21 22:12:42 - INFO - root -   Epoch: [132/300][120/200], lr: 0.00000043 	 loss = 1.0500(0.3558)
2023/11/21 22:13:07 - INFO - root -   Epoch: [132/300][140/200], lr: 0.00000043 	 loss = 0.0354(0.3559)
2023/11/21 22:13:20 - INFO - root -   Epoch: [132/300][160/200], lr: 0.00000043 	 loss = 0.2681(0.3662)
2023/11/21 22:13:34 - INFO - root -   Epoch: [132/300][180/200], lr: 0.00000043 	 loss = 0.0649(0.3617)
2023/11/21 22:13:45 - INFO - root -   Epoch: [132/300] 	 loss = 0.3871
2023/11/21 22:13:45 - INFO - root -   train_accuracy = 0.8450
2023/11/21 22:13:46 - INFO - root -   Epoch: [133/300][0/200], lr: 0.00000043 	 loss = 0.8034(0.8034)
2023/11/21 22:14:10 - INFO - root -   Epoch: [133/300][20/200], lr: 0.00000043 	 loss = 0.3970(0.4019)
2023/11/21 22:14:27 - INFO - root -   Epoch: [133/300][40/200], lr: 0.00000043 	 loss = 0.9497(0.4597)
2023/11/21 22:14:36 - INFO - root -   Epoch: [133/300][60/200], lr: 0.00000043 	 loss = 0.2560(0.4314)
2023/11/21 22:14:49 - INFO - root -   Epoch: [133/300][80/200], lr: 0.00000043 	 loss = 0.1680(0.4173)
2023/11/21 22:14:58 - INFO - root -   Epoch: [133/300][100/200], lr: 0.00000043 	 loss = 0.4708(0.4132)
2023/11/21 22:15:16 - INFO - root -   Epoch: [133/300][120/200], lr: 0.00000043 	 loss = 0.2149(0.4092)
2023/11/21 22:15:30 - INFO - root -   Epoch: [133/300][140/200], lr: 0.00000043 	 loss = 0.1756(0.4023)
2023/11/21 22:15:40 - INFO - root -   Epoch: [133/300][160/200], lr: 0.00000043 	 loss = 1.6411(0.4084)
2023/11/21 22:15:57 - INFO - root -   Epoch: [133/300][180/200], lr: 0.00000043 	 loss = 0.0516(0.4005)
2023/11/21 22:16:11 - INFO - root -   Epoch: [133/300] 	 loss = 0.4128
2023/11/21 22:16:11 - INFO - root -   train_accuracy = 0.8400
2023/11/21 22:16:13 - INFO - root -   Epoch: [134/300][0/200], lr: 0.00000043 	 loss = 0.7923(0.7923)
2023/11/21 22:16:32 - INFO - root -   Epoch: [134/300][20/200], lr: 0.00000043 	 loss = 1.1444(0.5111)
2023/11/21 22:16:46 - INFO - root -   Epoch: [134/300][40/200], lr: 0.00000043 	 loss = 0.5111(0.4494)
2023/11/21 22:17:06 - INFO - root -   Epoch: [134/300][60/200], lr: 0.00000043 	 loss = 0.0452(0.3975)
2023/11/21 22:17:21 - INFO - root -   Epoch: [134/300][80/200], lr: 0.00000043 	 loss = 1.2179(0.3800)
2023/11/21 22:17:32 - INFO - root -   Epoch: [134/300][100/200], lr: 0.00000043 	 loss = 0.0774(0.3729)
2023/11/21 22:17:57 - INFO - root -   Epoch: [134/300][120/200], lr: 0.00000043 	 loss = 0.8996(0.3860)
2023/11/21 22:18:10 - INFO - root -   Epoch: [134/300][140/200], lr: 0.00000043 	 loss = 0.2567(0.3707)
2023/11/21 22:18:30 - INFO - root -   Epoch: [134/300][160/200], lr: 0.00000043 	 loss = 1.3727(0.3792)
2023/11/21 22:18:39 - INFO - root -   Epoch: [134/300][180/200], lr: 0.00000043 	 loss = 0.0626(0.3859)
2023/11/21 22:18:54 - INFO - root -   Epoch: [134/300] 	 loss = 0.3917
2023/11/21 22:20:55 - INFO - root -   precision = 0.8515
2023/11/21 22:20:55 - INFO - root -   eval_loss = 0.3998
2023/11/21 22:20:55 - INFO - root -   eval_acc = 0.8515
2023/11/21 22:20:56 - INFO - root -   train_accuracy = 0.8400
2023/11/21 22:20:57 - INFO - root -   Epoch: [135/300][0/200], lr: 0.00000043 	 loss = 0.7745(0.7745)
2023/11/21 22:21:22 - INFO - root -   Epoch: [135/300][20/200], lr: 0.00000043 	 loss = 1.7057(0.5802)
2023/11/21 22:21:34 - INFO - root -   Epoch: [135/300][40/200], lr: 0.00000043 	 loss = 1.2991(0.4960)
2023/11/21 22:21:47 - INFO - root -   Epoch: [135/300][60/200], lr: 0.00000043 	 loss = 0.2074(0.4171)
2023/11/21 22:22:07 - INFO - root -   Epoch: [135/300][80/200], lr: 0.00000043 	 loss = 1.1886(0.4180)
2023/11/21 22:22:21 - INFO - root -   Epoch: [135/300][100/200], lr: 0.00000043 	 loss = 0.0146(0.4044)
2023/11/21 22:22:37 - INFO - root -   Epoch: [135/300][120/200], lr: 0.00000043 	 loss = 1.5539(0.4165)
2023/11/21 22:22:53 - INFO - root -   Epoch: [135/300][140/200], lr: 0.00000043 	 loss = 0.1210(0.4054)
2023/11/21 22:23:15 - INFO - root -   Epoch: [135/300][160/200], lr: 0.00000043 	 loss = 1.0637(0.4109)
2023/11/21 22:23:35 - INFO - root -   Epoch: [135/300][180/200], lr: 0.00000043 	 loss = 0.0718(0.4260)
2023/11/21 22:23:43 - INFO - root -   Epoch: [135/300] 	 loss = 0.4333
2023/11/21 22:23:43 - INFO - root -   train_accuracy = 0.8350
2023/11/21 22:23:54 - INFO - root -   Epoch: [136/300][0/200], lr: 0.00000044 	 loss = 0.7998(0.7998)
2023/11/21 22:24:07 - INFO - root -   Epoch: [136/300][20/200], lr: 0.00000044 	 loss = 0.4036(0.3513)
2023/11/21 22:24:25 - INFO - root -   Epoch: [136/300][40/200], lr: 0.00000044 	 loss = 1.2121(0.4106)
2023/11/21 22:24:40 - INFO - root -   Epoch: [136/300][60/200], lr: 0.00000044 	 loss = 0.3193(0.3696)
2023/11/21 22:24:51 - INFO - root -   Epoch: [136/300][80/200], lr: 0.00000044 	 loss = 0.5027(0.3859)
2023/11/21 22:25:02 - INFO - root -   Epoch: [136/300][100/200], lr: 0.00000044 	 loss = 0.0259(0.3766)
2023/11/21 22:25:26 - INFO - root -   Epoch: [136/300][120/200], lr: 0.00000044 	 loss = 0.6130(0.3741)
2023/11/21 22:25:45 - INFO - root -   Epoch: [136/300][140/200], lr: 0.00000044 	 loss = 0.0041(0.3675)
2023/11/21 22:26:05 - INFO - root -   Epoch: [136/300][160/200], lr: 0.00000044 	 loss = 0.5084(0.4012)
2023/11/21 22:26:22 - INFO - root -   Epoch: [136/300][180/200], lr: 0.00000044 	 loss = 0.0280(0.4058)
2023/11/21 22:26:35 - INFO - root -   Epoch: [136/300] 	 loss = 0.4067
2023/11/21 22:26:35 - INFO - root -   train_accuracy = 0.8375
2023/11/21 22:26:36 - INFO - root -   Epoch: [137/300][0/200], lr: 0.00000044 	 loss = 0.6248(0.6248)
2023/11/21 22:27:01 - INFO - root -   Epoch: [137/300][20/200], lr: 0.00000044 	 loss = 1.1747(0.3887)
2023/11/21 22:27:15 - INFO - root -   Epoch: [137/300][40/200], lr: 0.00000044 	 loss = 0.7528(0.3935)
2023/11/21 22:27:25 - INFO - root -   Epoch: [137/300][60/200], lr: 0.00000044 	 loss = 0.0579(0.3680)
2023/11/21 22:27:45 - INFO - root -   Epoch: [137/300][80/200], lr: 0.00000044 	 loss = 1.3531(0.3768)
2023/11/21 22:27:53 - INFO - root -   Epoch: [137/300][100/200], lr: 0.00000044 	 loss = 0.0069(0.3504)
2023/11/21 22:28:11 - INFO - root -   Epoch: [137/300][120/200], lr: 0.00000044 	 loss = 0.9331(0.3503)
2023/11/21 22:28:20 - INFO - root -   Epoch: [137/300][140/200], lr: 0.00000044 	 loss = 0.0225(0.3275)
2023/11/21 22:28:39 - INFO - root -   Epoch: [137/300][160/200], lr: 0.00000044 	 loss = 0.3307(0.3497)
2023/11/21 22:28:54 - INFO - root -   Epoch: [137/300][180/200], lr: 0.00000044 	 loss = 0.2106(0.3627)
2023/11/21 22:29:03 - INFO - root -   Epoch: [137/300] 	 loss = 0.3510
2023/11/21 22:29:03 - INFO - root -   train_accuracy = 0.8800
2023/11/21 22:29:04 - INFO - root -   Epoch: [138/300][0/200], lr: 0.00000044 	 loss = 0.8485(0.8485)
2023/11/21 22:29:19 - INFO - root -   Epoch: [138/300][20/200], lr: 0.00000044 	 loss = 0.2559(0.3592)
2023/11/21 22:29:33 - INFO - root -   Epoch: [138/300][40/200], lr: 0.00000044 	 loss = 0.3933(0.4314)
2023/11/21 22:29:52 - INFO - root -   Epoch: [138/300][60/200], lr: 0.00000044 	 loss = 0.0285(0.3752)
2023/11/21 22:30:02 - INFO - root -   Epoch: [138/300][80/200], lr: 0.00000044 	 loss = 0.3187(0.3663)
2023/11/21 22:30:18 - INFO - root -   Epoch: [138/300][100/200], lr: 0.00000044 	 loss = 0.0114(0.3630)
2023/11/21 22:30:32 - INFO - root -   Epoch: [138/300][120/200], lr: 0.00000044 	 loss = 0.1025(0.3487)
2023/11/21 22:30:45 - INFO - root -   Epoch: [138/300][140/200], lr: 0.00000044 	 loss = 0.0546(0.3455)
2023/11/21 22:30:59 - INFO - root -   Epoch: [138/300][160/200], lr: 0.00000044 	 loss = 0.4535(0.3549)
2023/11/21 22:31:11 - INFO - root -   Epoch: [138/300][180/200], lr: 0.00000044 	 loss = 0.1624(0.3509)
2023/11/21 22:31:19 - INFO - root -   Epoch: [138/300] 	 loss = 0.3526
2023/11/21 22:31:19 - INFO - root -   train_accuracy = 0.8725
2023/11/21 22:31:20 - INFO - root -   Epoch: [139/300][0/200], lr: 0.00000044 	 loss = 1.3359(1.3359)
2023/11/21 22:31:44 - INFO - root -   Epoch: [139/300][20/200], lr: 0.00000044 	 loss = 0.3439(0.2973)
2023/11/21 22:31:58 - INFO - root -   Epoch: [139/300][40/200], lr: 0.00000044 	 loss = 0.3783(0.3626)
2023/11/21 22:32:08 - INFO - root -   Epoch: [139/300][60/200], lr: 0.00000044 	 loss = 0.2170(0.3192)
2023/11/21 22:32:23 - INFO - root -   Epoch: [139/300][80/200], lr: 0.00000044 	 loss = 1.3279(0.3608)
2023/11/21 22:32:36 - INFO - root -   Epoch: [139/300][100/200], lr: 0.00000044 	 loss = 0.0165(0.3570)
2023/11/21 22:32:56 - INFO - root -   Epoch: [139/300][120/200], lr: 0.00000044 	 loss = 0.7101(0.3705)
2023/11/21 22:33:13 - INFO - root -   Epoch: [139/300][140/200], lr: 0.00000044 	 loss = 0.0695(0.3537)
2023/11/21 22:33:32 - INFO - root -   Epoch: [139/300][160/200], lr: 0.00000044 	 loss = 0.4626(0.3610)
2023/11/21 22:33:50 - INFO - root -   Epoch: [139/300][180/200], lr: 0.00000044 	 loss = 0.1046(0.3640)
2023/11/21 22:34:07 - INFO - root -   Epoch: [139/300] 	 loss = 0.3668
2023/11/21 22:36:09 - INFO - root -   precision = 0.8614
2023/11/21 22:36:09 - INFO - root -   eval_loss = 0.3855
2023/11/21 22:36:09 - INFO - root -   eval_acc = 0.8614
2023/11/21 22:36:10 - INFO - root -   train_accuracy = 0.8550
2023/11/21 22:36:11 - INFO - root -   Epoch: [140/300][0/200], lr: 0.00000045 	 loss = 0.7893(0.7893)
2023/11/21 22:36:39 - INFO - root -   Epoch: [140/300][20/200], lr: 0.00000045 	 loss = 1.2623(0.5285)
2023/11/21 22:36:47 - INFO - root -   Epoch: [140/300][40/200], lr: 0.00000045 	 loss = 1.5440(0.4390)
2023/11/21 22:37:07 - INFO - root -   Epoch: [140/300][60/200], lr: 0.00000045 	 loss = 0.1065(0.4029)
2023/11/21 22:37:20 - INFO - root -   Epoch: [140/300][80/200], lr: 0.00000045 	 loss = 0.6473(0.3555)
2023/11/21 22:37:30 - INFO - root -   Epoch: [140/300][100/200], lr: 0.00000045 	 loss = 0.0414(0.3807)
2023/11/21 22:37:48 - INFO - root -   Epoch: [140/300][120/200], lr: 0.00000045 	 loss = 0.8651(0.3800)
2023/11/21 22:38:04 - INFO - root -   Epoch: [140/300][140/200], lr: 0.00000045 	 loss = 0.0036(0.3567)
2023/11/21 22:38:25 - INFO - root -   Epoch: [140/300][160/200], lr: 0.00000045 	 loss = 0.4831(0.3835)
2023/11/21 22:38:38 - INFO - root -   Epoch: [140/300][180/200], lr: 0.00000045 	 loss = 0.0144(0.3744)
2023/11/21 22:38:48 - INFO - root -   Epoch: [140/300] 	 loss = 0.3671
2023/11/21 22:38:48 - INFO - root -   train_accuracy = 0.8725
2023/11/21 22:38:50 - INFO - root -   Epoch: [141/300][0/200], lr: 0.00000045 	 loss = 1.4034(1.4034)
2023/11/21 22:39:16 - INFO - root -   Epoch: [141/300][20/200], lr: 0.00000045 	 loss = 1.8557(0.3954)
2023/11/21 22:39:33 - INFO - root -   Epoch: [141/300][40/200], lr: 0.00000045 	 loss = 1.6270(0.3733)
2023/11/21 22:39:47 - INFO - root -   Epoch: [141/300][60/200], lr: 0.00000045 	 loss = 0.5035(0.3511)
2023/11/21 22:40:04 - INFO - root -   Epoch: [141/300][80/200], lr: 0.00000045 	 loss = 0.8605(0.3812)
2023/11/21 22:40:26 - INFO - root -   Epoch: [141/300][100/200], lr: 0.00000045 	 loss = 0.4763(0.4209)
2023/11/21 22:40:43 - INFO - root -   Epoch: [141/300][120/200], lr: 0.00000045 	 loss = 1.2080(0.4470)
2023/11/21 22:40:56 - INFO - root -   Epoch: [141/300][140/200], lr: 0.00000045 	 loss = 0.0787(0.4294)
2023/11/21 22:41:04 - INFO - root -   Epoch: [141/300][160/200], lr: 0.00000045 	 loss = 0.3828(0.4223)
2023/11/21 22:41:20 - INFO - root -   Epoch: [141/300][180/200], lr: 0.00000045 	 loss = 0.0502(0.4218)
2023/11/21 22:41:29 - INFO - root -   Epoch: [141/300] 	 loss = 0.4146
2023/11/21 22:41:29 - INFO - root -   train_accuracy = 0.8550
2023/11/21 22:41:30 - INFO - root -   Epoch: [142/300][0/200], lr: 0.00000045 	 loss = 0.8774(0.8774)
2023/11/21 22:41:41 - INFO - root -   Epoch: [142/300][20/200], lr: 0.00000045 	 loss = 2.3077(0.3967)
2023/11/21 22:41:54 - INFO - root -   Epoch: [142/300][40/200], lr: 0.00000045 	 loss = 1.7833(0.4396)
2023/11/21 22:42:07 - INFO - root -   Epoch: [142/300][60/200], lr: 0.00000045 	 loss = 0.6719(0.3976)
2023/11/21 22:42:23 - INFO - root -   Epoch: [142/300][80/200], lr: 0.00000045 	 loss = 1.6486(0.3994)
2023/11/21 22:42:33 - INFO - root -   Epoch: [142/300][100/200], lr: 0.00000045 	 loss = 0.0131(0.3659)
2023/11/21 22:42:48 - INFO - root -   Epoch: [142/300][120/200], lr: 0.00000045 	 loss = 0.1982(0.3557)
2023/11/21 22:43:01 - INFO - root -   Epoch: [142/300][140/200], lr: 0.00000045 	 loss = 0.0738(0.3461)
2023/11/21 22:43:21 - INFO - root -   Epoch: [142/300][160/200], lr: 0.00000045 	 loss = 0.7484(0.3550)
2023/11/21 22:43:35 - INFO - root -   Epoch: [142/300][180/200], lr: 0.00000045 	 loss = 0.5309(0.3683)
2023/11/21 22:43:48 - INFO - root -   Epoch: [142/300] 	 loss = 0.3813
2023/11/21 22:43:48 - INFO - root -   train_accuracy = 0.8675
2023/11/21 22:43:49 - INFO - root -   Epoch: [143/300][0/200], lr: 0.00000045 	 loss = 1.3965(1.3965)
2023/11/21 22:44:09 - INFO - root -   Epoch: [143/300][20/200], lr: 0.00000045 	 loss = 0.4801(0.3976)
2023/11/21 22:44:29 - INFO - root -   Epoch: [143/300][40/200], lr: 0.00000045 	 loss = 1.9115(0.4255)
2023/11/21 22:44:51 - INFO - root -   Epoch: [143/300][60/200], lr: 0.00000045 	 loss = 0.0768(0.3906)
2023/11/21 22:44:59 - INFO - root -   Epoch: [143/300][80/200], lr: 0.00000045 	 loss = 0.9241(0.3860)
2023/11/21 22:45:18 - INFO - root -   Epoch: [143/300][100/200], lr: 0.00000045 	 loss = 0.0912(0.3912)
2023/11/21 22:45:35 - INFO - root -   Epoch: [143/300][120/200], lr: 0.00000045 	 loss = 1.1858(0.3931)
2023/11/21 22:45:48 - INFO - root -   Epoch: [143/300][140/200], lr: 0.00000045 	 loss = 0.1635(0.3759)
2023/11/21 22:46:00 - INFO - root -   Epoch: [143/300][160/200], lr: 0.00000045 	 loss = 0.2135(0.3772)
2023/11/21 22:46:16 - INFO - root -   Epoch: [143/300][180/200], lr: 0.00000045 	 loss = 0.3923(0.3816)
2023/11/21 22:46:29 - INFO - root -   Epoch: [143/300] 	 loss = 0.3826
2023/11/21 22:46:29 - INFO - root -   train_accuracy = 0.8600
2023/11/21 22:46:38 - INFO - root -   Epoch: [144/300][0/200], lr: 0.00000046 	 loss = 0.8757(0.8757)
2023/11/21 22:46:55 - INFO - root -   Epoch: [144/300][20/200], lr: 0.00000046 	 loss = 1.4747(0.4855)
2023/11/21 22:47:14 - INFO - root -   Epoch: [144/300][40/200], lr: 0.00000046 	 loss = 0.2643(0.3652)
2023/11/21 22:47:22 - INFO - root -   Epoch: [144/300][60/200], lr: 0.00000046 	 loss = 0.2440(0.3242)
2023/11/21 22:47:33 - INFO - root -   Epoch: [144/300][80/200], lr: 0.00000046 	 loss = 0.4599(0.3401)
2023/11/21 22:47:57 - INFO - root -   Epoch: [144/300][100/200], lr: 0.00000046 	 loss = 0.0056(0.3353)
2023/11/21 22:48:10 - INFO - root -   Epoch: [144/300][120/200], lr: 0.00000046 	 loss = 0.1139(0.3361)
2023/11/21 22:48:27 - INFO - root -   Epoch: [144/300][140/200], lr: 0.00000046 	 loss = 0.3945(0.3348)
2023/11/21 22:48:39 - INFO - root -   Epoch: [144/300][160/200], lr: 0.00000046 	 loss = 0.9938(0.3341)
2023/11/21 22:48:56 - INFO - root -   Epoch: [144/300][180/200], lr: 0.00000046 	 loss = 0.2535(0.3501)
2023/11/21 22:49:04 - INFO - root -   Epoch: [144/300] 	 loss = 0.3604
2023/11/21 22:51:05 - INFO - root -   precision = 0.8515
2023/11/21 22:51:05 - INFO - root -   eval_loss = 0.3599
2023/11/21 22:51:05 - INFO - root -   eval_acc = 0.8515
2023/11/21 22:51:06 - INFO - root -   train_accuracy = 0.8725
2023/11/21 22:51:07 - INFO - root -   Epoch: [145/300][0/200], lr: 0.00000046 	 loss = 1.1910(1.1910)
2023/11/21 22:51:29 - INFO - root -   Epoch: [145/300][20/200], lr: 0.00000046 	 loss = 1.1020(0.4141)
2023/11/21 22:51:48 - INFO - root -   Epoch: [145/300][40/200], lr: 0.00000046 	 loss = 0.8351(0.4329)
2023/11/21 22:52:01 - INFO - root -   Epoch: [145/300][60/200], lr: 0.00000046 	 loss = 0.3310(0.3923)
2023/11/21 22:52:14 - INFO - root -   Epoch: [145/300][80/200], lr: 0.00000046 	 loss = 0.5674(0.3618)
2023/11/21 22:52:23 - INFO - root -   Epoch: [145/300][100/200], lr: 0.00000046 	 loss = 0.0096(0.3446)
2023/11/21 22:52:32 - INFO - root -   Epoch: [145/300][120/200], lr: 0.00000046 	 loss = 1.0653(0.3557)
2023/11/21 22:52:46 - INFO - root -   Epoch: [145/300][140/200], lr: 0.00000046 	 loss = 0.0747(0.3455)
2023/11/21 22:53:00 - INFO - root -   Epoch: [145/300][160/200], lr: 0.00000046 	 loss = 1.1068(0.3557)
2023/11/21 22:53:19 - INFO - root -   Epoch: [145/300][180/200], lr: 0.00000046 	 loss = 0.3513(0.3458)
2023/11/21 22:53:26 - INFO - root -   Epoch: [145/300] 	 loss = 0.3572
2023/11/21 22:53:26 - INFO - root -   train_accuracy = 0.8625
2023/11/21 22:53:34 - INFO - root -   Epoch: [146/300][0/200], lr: 0.00000046 	 loss = 0.5411(0.5411)
2023/11/21 22:53:50 - INFO - root -   Epoch: [146/300][20/200], lr: 0.00000046 	 loss = 2.3064(0.4748)
2023/11/21 22:54:09 - INFO - root -   Epoch: [146/300][40/200], lr: 0.00000046 	 loss = 1.1519(0.5103)
2023/11/21 22:54:27 - INFO - root -   Epoch: [146/300][60/200], lr: 0.00000046 	 loss = 0.0963(0.4544)
2023/11/21 22:54:39 - INFO - root -   Epoch: [146/300][80/200], lr: 0.00000046 	 loss = 0.5841(0.4068)
2023/11/21 22:55:02 - INFO - root -   Epoch: [146/300][100/200], lr: 0.00000046 	 loss = 0.0050(0.4119)
2023/11/21 22:55:10 - INFO - root -   Epoch: [146/300][120/200], lr: 0.00000046 	 loss = 1.1321(0.4192)
2023/11/21 22:55:28 - INFO - root -   Epoch: [146/300][140/200], lr: 0.00000046 	 loss = 0.0197(0.4149)
2023/11/21 22:55:51 - INFO - root -   Epoch: [146/300][160/200], lr: 0.00000046 	 loss = 0.1885(0.4084)
2023/11/21 22:55:59 - INFO - root -   Epoch: [146/300][180/200], lr: 0.00000046 	 loss = 0.1025(0.3952)
2023/11/21 22:56:14 - INFO - root -   Epoch: [146/300] 	 loss = 0.3956
2023/11/21 22:56:14 - INFO - root -   train_accuracy = 0.8550
2023/11/21 22:56:15 - INFO - root -   Epoch: [147/300][0/200], lr: 0.00000046 	 loss = 0.4236(0.4236)
2023/11/21 22:56:37 - INFO - root -   Epoch: [147/300][20/200], lr: 0.00000046 	 loss = 2.1130(0.3881)
2023/11/21 22:56:58 - INFO - root -   Epoch: [147/300][40/200], lr: 0.00000046 	 loss = 1.1213(0.4050)
2023/11/21 22:57:12 - INFO - root -   Epoch: [147/300][60/200], lr: 0.00000046 	 loss = 0.1785(0.3722)
2023/11/21 22:57:30 - INFO - root -   Epoch: [147/300][80/200], lr: 0.00000046 	 loss = 1.4226(0.3675)
2023/11/21 22:57:48 - INFO - root -   Epoch: [147/300][100/200], lr: 0.00000046 	 loss = 0.0721(0.3678)
2023/11/21 22:58:04 - INFO - root -   Epoch: [147/300][120/200], lr: 0.00000046 	 loss = 0.8095(0.3699)
2023/11/21 22:58:18 - INFO - root -   Epoch: [147/300][140/200], lr: 0.00000046 	 loss = 0.3757(0.3421)
2023/11/21 22:58:38 - INFO - root -   Epoch: [147/300][160/200], lr: 0.00000046 	 loss = 0.6465(0.3429)
2023/11/21 22:58:46 - INFO - root -   Epoch: [147/300][180/200], lr: 0.00000046 	 loss = 0.1052(0.3358)
2023/11/21 22:58:55 - INFO - root -   Epoch: [147/300] 	 loss = 0.3349
2023/11/21 22:58:55 - INFO - root -   train_accuracy = 0.8700
2023/11/21 22:59:08 - INFO - root -   Epoch: [148/300][0/200], lr: 0.00000047 	 loss = 1.3027(1.3027)
2023/11/21 22:59:16 - INFO - root -   Epoch: [148/300][20/200], lr: 0.00000047 	 loss = 0.2576(0.3233)
2023/11/21 22:59:40 - INFO - root -   Epoch: [148/300][40/200], lr: 0.00000047 	 loss = 1.3315(0.3452)
2023/11/21 22:59:49 - INFO - root -   Epoch: [148/300][60/200], lr: 0.00000047 	 loss = 0.1860(0.3659)
2023/11/21 23:00:08 - INFO - root -   Epoch: [148/300][80/200], lr: 0.00000047 	 loss = 0.3637(0.3480)
2023/11/21 23:00:32 - INFO - root -   Epoch: [148/300][100/200], lr: 0.00000047 	 loss = 0.0466(0.3615)
2023/11/21 23:00:45 - INFO - root -   Epoch: [148/300][120/200], lr: 0.00000047 	 loss = 1.3718(0.3879)
2023/11/21 23:01:01 - INFO - root -   Epoch: [148/300][140/200], lr: 0.00000047 	 loss = 0.0039(0.3637)
2023/11/21 23:01:15 - INFO - root -   Epoch: [148/300][160/200], lr: 0.00000047 	 loss = 1.6272(0.3780)
2023/11/21 23:01:31 - INFO - root -   Epoch: [148/300][180/200], lr: 0.00000047 	 loss = 0.0929(0.3698)
2023/11/21 23:01:46 - INFO - root -   Epoch: [148/300] 	 loss = 0.3746
2023/11/21 23:01:46 - INFO - root -   train_accuracy = 0.8675
2023/11/21 23:01:53 - INFO - root -   Epoch: [149/300][0/200], lr: 0.00000047 	 loss = 1.1508(1.1508)
2023/11/21 23:02:09 - INFO - root -   Epoch: [149/300][20/200], lr: 0.00000047 	 loss = 1.3772(0.4065)
2023/11/21 23:02:22 - INFO - root -   Epoch: [149/300][40/200], lr: 0.00000047 	 loss = 1.1357(0.3447)
2023/11/21 23:02:39 - INFO - root -   Epoch: [149/300][60/200], lr: 0.00000047 	 loss = 0.0164(0.3275)
2023/11/21 23:02:54 - INFO - root -   Epoch: [149/300][80/200], lr: 0.00000047 	 loss = 0.4836(0.3265)
2023/11/21 23:03:04 - INFO - root -   Epoch: [149/300][100/200], lr: 0.00000047 	 loss = 0.0388(0.3378)
2023/11/21 23:03:26 - INFO - root -   Epoch: [149/300][120/200], lr: 0.00000047 	 loss = 1.0521(0.3387)
2023/11/21 23:03:39 - INFO - root -   Epoch: [149/300][140/200], lr: 0.00000047 	 loss = 0.0207(0.3423)
2023/11/21 23:03:54 - INFO - root -   Epoch: [149/300][160/200], lr: 0.00000047 	 loss = 0.7858(0.3672)
2023/11/21 23:04:09 - INFO - root -   Epoch: [149/300][180/200], lr: 0.00000047 	 loss = 0.0249(0.3562)
2023/11/21 23:04:19 - INFO - root -   Epoch: [149/300] 	 loss = 0.3546
2023/11/21 23:06:20 - INFO - root -   precision = 0.8416
2023/11/21 23:06:20 - INFO - root -   eval_loss = 0.4314
2023/11/21 23:06:20 - INFO - root -   eval_acc = 0.8416
2023/11/21 23:06:21 - INFO - root -   train_accuracy = 0.8725
2023/11/21 23:06:23 - INFO - root -   Epoch: [150/300][0/200], lr: 0.00000047 	 loss = 0.6839(0.6839)
2023/11/21 23:06:37 - INFO - root -   Epoch: [150/300][20/200], lr: 0.00000047 	 loss = 0.5610(0.3083)
2023/11/21 23:06:54 - INFO - root -   Epoch: [150/300][40/200], lr: 0.00000047 	 loss = 0.8817(0.3950)
2023/11/21 23:07:11 - INFO - root -   Epoch: [150/300][60/200], lr: 0.00000047 	 loss = 0.0705(0.3487)
2023/11/21 23:07:28 - INFO - root -   Epoch: [150/300][80/200], lr: 0.00000047 	 loss = 1.0903(0.3296)
2023/11/21 23:07:42 - INFO - root -   Epoch: [150/300][100/200], lr: 0.00000047 	 loss = 0.0066(0.3665)
2023/11/21 23:07:52 - INFO - root -   Epoch: [150/300][120/200], lr: 0.00000047 	 loss = 0.1354(0.3686)
2023/11/21 23:08:21 - INFO - root -   Epoch: [150/300][140/200], lr: 0.00000047 	 loss = 0.3133(0.3670)
2023/11/21 23:08:35 - INFO - root -   Epoch: [150/300][160/200], lr: 0.00000047 	 loss = 0.6690(0.3690)
2023/11/21 23:08:48 - INFO - root -   Epoch: [150/300][180/200], lr: 0.00000047 	 loss = 0.0093(0.3755)
2023/11/21 23:09:02 - INFO - root -   Epoch: [150/300] 	 loss = 0.3923
2023/11/21 23:09:02 - INFO - root -   train_accuracy = 0.8475
2023/11/21 23:09:09 - INFO - root -   Epoch: [151/300][0/200], lr: 0.00000047 	 loss = 0.3346(0.3346)
2023/11/21 23:09:29 - INFO - root -   Epoch: [151/300][20/200], lr: 0.00000047 	 loss = 0.1196(0.3576)
2023/11/21 23:09:41 - INFO - root -   Epoch: [151/300][40/200], lr: 0.00000047 	 loss = 0.8861(0.4009)
2023/11/21 23:09:52 - INFO - root -   Epoch: [151/300][60/200], lr: 0.00000047 	 loss = 0.0084(0.3576)
2023/11/21 23:10:09 - INFO - root -   Epoch: [151/300][80/200], lr: 0.00000047 	 loss = 1.0779(0.3485)
2023/11/21 23:10:32 - INFO - root -   Epoch: [151/300][100/200], lr: 0.00000047 	 loss = 0.0114(0.3583)
2023/11/21 23:10:40 - INFO - root -   Epoch: [151/300][120/200], lr: 0.00000047 	 loss = 0.5553(0.3482)
2023/11/21 23:10:52 - INFO - root -   Epoch: [151/300][140/200], lr: 0.00000047 	 loss = 0.0028(0.3333)
2023/11/21 23:11:05 - INFO - root -   Epoch: [151/300][160/200], lr: 0.00000047 	 loss = 1.4404(0.3446)
2023/11/21 23:11:17 - INFO - root -   Epoch: [151/300][180/200], lr: 0.00000047 	 loss = 0.0610(0.3456)
2023/11/21 23:11:27 - INFO - root -   Epoch: [151/300] 	 loss = 0.3405
2023/11/21 23:11:27 - INFO - root -   train_accuracy = 0.8750
2023/11/21 23:11:38 - INFO - root -   Epoch: [152/300][0/200], lr: 0.00000048 	 loss = 2.1234(2.1234)
2023/11/21 23:11:51 - INFO - root -   Epoch: [152/300][20/200], lr: 0.00000048 	 loss = 1.4215(0.6918)
2023/11/21 23:12:05 - INFO - root -   Epoch: [152/300][40/200], lr: 0.00000048 	 loss = 0.5035(0.5259)
2023/11/21 23:12:30 - INFO - root -   Epoch: [152/300][60/200], lr: 0.00000048 	 loss = 0.1004(0.4630)
2023/11/21 23:12:39 - INFO - root -   Epoch: [152/300][80/200], lr: 0.00000048 	 loss = 0.3461(0.4144)
2023/11/21 23:12:57 - INFO - root -   Epoch: [152/300][100/200], lr: 0.00000048 	 loss = 0.1161(0.4160)
2023/11/21 23:13:16 - INFO - root -   Epoch: [152/300][120/200], lr: 0.00000048 	 loss = 0.9705(0.3904)
2023/11/21 23:13:24 - INFO - root -   Epoch: [152/300][140/200], lr: 0.00000048 	 loss = 0.0280(0.3622)
2023/11/21 23:13:36 - INFO - root -   Epoch: [152/300][160/200], lr: 0.00000048 	 loss = 0.5030(0.3520)
2023/11/21 23:13:47 - INFO - root -   Epoch: [152/300][180/200], lr: 0.00000048 	 loss = 0.0440(0.3566)
2023/11/21 23:13:55 - INFO - root -   Epoch: [152/300] 	 loss = 0.3546
2023/11/21 23:13:55 - INFO - root -   train_accuracy = 0.8575
2023/11/21 23:14:03 - INFO - root -   Epoch: [153/300][0/200], lr: 0.00000048 	 loss = 1.7864(1.7864)
2023/11/21 23:14:19 - INFO - root -   Epoch: [153/300][20/200], lr: 0.00000048 	 loss = 1.9428(0.4806)
2023/11/21 23:14:35 - INFO - root -   Epoch: [153/300][40/200], lr: 0.00000048 	 loss = 0.5897(0.4139)
2023/11/21 23:14:51 - INFO - root -   Epoch: [153/300][60/200], lr: 0.00000048 	 loss = 0.5425(0.3806)
2023/11/21 23:15:03 - INFO - root -   Epoch: [153/300][80/200], lr: 0.00000048 	 loss = 0.4010(0.3466)
2023/11/21 23:15:22 - INFO - root -   Epoch: [153/300][100/200], lr: 0.00000048 	 loss = 0.0087(0.3547)
2023/11/21 23:15:38 - INFO - root -   Epoch: [153/300][120/200], lr: 0.00000048 	 loss = 1.7544(0.3616)
2023/11/21 23:15:56 - INFO - root -   Epoch: [153/300][140/200], lr: 0.00000048 	 loss = 0.3033(0.3554)
2023/11/21 23:16:17 - INFO - root -   Epoch: [153/300][160/200], lr: 0.00000048 	 loss = 2.0161(0.3629)
2023/11/21 23:16:29 - INFO - root -   Epoch: [153/300][180/200], lr: 0.00000048 	 loss = 0.0080(0.3674)
2023/11/21 23:16:37 - INFO - root -   Epoch: [153/300] 	 loss = 0.3660
2023/11/21 23:16:37 - INFO - root -   train_accuracy = 0.8550
2023/11/21 23:16:38 - INFO - root -   Epoch: [154/300][0/200], lr: 0.00000048 	 loss = 1.3054(1.3054)
2023/11/21 23:17:05 - INFO - root -   Epoch: [154/300][20/200], lr: 0.00000048 	 loss = 1.8201(0.5243)
2023/11/21 23:17:20 - INFO - root -   Epoch: [154/300][40/200], lr: 0.00000048 	 loss = 0.3789(0.4218)
2023/11/21 23:17:34 - INFO - root -   Epoch: [154/300][60/200], lr: 0.00000048 	 loss = 0.1063(0.3687)
2023/11/21 23:17:49 - INFO - root -   Epoch: [154/300][80/200], lr: 0.00000048 	 loss = 0.3035(0.3580)
2023/11/21 23:18:03 - INFO - root -   Epoch: [154/300][100/200], lr: 0.00000048 	 loss = 0.0681(0.3669)
2023/11/21 23:18:21 - INFO - root -   Epoch: [154/300][120/200], lr: 0.00000048 	 loss = 1.0116(0.3837)
2023/11/21 23:18:37 - INFO - root -   Epoch: [154/300][140/200], lr: 0.00000048 	 loss = 0.0081(0.3869)
2023/11/21 23:18:48 - INFO - root -   Epoch: [154/300][160/200], lr: 0.00000048 	 loss = 0.6310(0.3994)
2023/11/21 23:18:59 - INFO - root -   Epoch: [154/300][180/200], lr: 0.00000048 	 loss = 0.0378(0.3980)
2023/11/21 23:19:13 - INFO - root -   Epoch: [154/300] 	 loss = 0.4117
2023/11/21 23:21:14 - INFO - root -   precision = 0.8713
2023/11/21 23:21:14 - INFO - root -   eval_loss = 0.3833
2023/11/21 23:21:14 - INFO - root -   eval_acc = 0.8713
2023/11/21 23:21:15 - INFO - root -   train_accuracy = 0.8500
2023/11/21 23:21:23 - INFO - root -   Epoch: [155/300][0/200], lr: 0.00000048 	 loss = 0.5670(0.5670)
2023/11/21 23:21:33 - INFO - root -   Epoch: [155/300][20/200], lr: 0.00000048 	 loss = 0.5231(0.3162)
2023/11/21 23:21:53 - INFO - root -   Epoch: [155/300][40/200], lr: 0.00000048 	 loss = 0.2560(0.3376)
2023/11/21 23:22:03 - INFO - root -   Epoch: [155/300][60/200], lr: 0.00000048 	 loss = 0.0172(0.3003)
2023/11/21 23:22:21 - INFO - root -   Epoch: [155/300][80/200], lr: 0.00000048 	 loss = 0.5972(0.3014)
2023/11/21 23:22:35 - INFO - root -   Epoch: [155/300][100/200], lr: 0.00000048 	 loss = 0.1026(0.3388)
2023/11/21 23:22:43 - INFO - root -   Epoch: [155/300][120/200], lr: 0.00000048 	 loss = 0.8675(0.3343)
2023/11/21 23:23:00 - INFO - root -   Epoch: [155/300][140/200], lr: 0.00000048 	 loss = 0.0734(0.3534)
2023/11/21 23:23:15 - INFO - root -   Epoch: [155/300][160/200], lr: 0.00000048 	 loss = 1.5877(0.3648)
2023/11/21 23:23:26 - INFO - root -   Epoch: [155/300][180/200], lr: 0.00000048 	 loss = 0.1059(0.3632)
2023/11/21 23:23:39 - INFO - root -   Epoch: [155/300] 	 loss = 0.3613
2023/11/21 23:23:39 - INFO - root -   train_accuracy = 0.8675
2023/11/21 23:23:41 - INFO - root -   Epoch: [156/300][0/200], lr: 0.00000049 	 loss = 1.0328(1.0328)
2023/11/21 23:24:00 - INFO - root -   Epoch: [156/300][20/200], lr: 0.00000049 	 loss = 0.2761(0.4028)
2023/11/21 23:24:08 - INFO - root -   Epoch: [156/300][40/200], lr: 0.00000049 	 loss = 0.9337(0.3531)
2023/11/21 23:24:23 - INFO - root -   Epoch: [156/300][60/200], lr: 0.00000049 	 loss = 0.2465(0.3223)
2023/11/21 23:24:37 - INFO - root -   Epoch: [156/300][80/200], lr: 0.00000049 	 loss = 0.3719(0.3431)
2023/11/21 23:25:02 - INFO - root -   Epoch: [156/300][100/200], lr: 0.00000049 	 loss = 0.0163(0.3839)
2023/11/21 23:25:15 - INFO - root -   Epoch: [156/300][120/200], lr: 0.00000049 	 loss = 0.0282(0.3974)
2023/11/21 23:25:25 - INFO - root -   Epoch: [156/300][140/200], lr: 0.00000049 	 loss = 0.0948(0.3974)
2023/11/21 23:25:37 - INFO - root -   Epoch: [156/300][160/200], lr: 0.00000049 	 loss = 0.8602(0.3952)
2023/11/21 23:25:47 - INFO - root -   Epoch: [156/300][180/200], lr: 0.00000049 	 loss = 0.1845(0.3863)
2023/11/21 23:25:58 - INFO - root -   Epoch: [156/300] 	 loss = 0.3855
2023/11/21 23:25:58 - INFO - root -   train_accuracy = 0.8575
2023/11/21 23:26:00 - INFO - root -   Epoch: [157/300][0/200], lr: 0.00000049 	 loss = 0.4656(0.4656)
2023/11/21 23:26:19 - INFO - root -   Epoch: [157/300][20/200], lr: 0.00000049 	 loss = 1.6670(0.3957)
2023/11/21 23:26:38 - INFO - root -   Epoch: [157/300][40/200], lr: 0.00000049 	 loss = 0.4198(0.3803)
2023/11/21 23:26:51 - INFO - root -   Epoch: [157/300][60/200], lr: 0.00000049 	 loss = 0.1434(0.3454)
2023/11/21 23:27:02 - INFO - root -   Epoch: [157/300][80/200], lr: 0.00000049 	 loss = 0.7875(0.3295)
2023/11/21 23:27:16 - INFO - root -   Epoch: [157/300][100/200], lr: 0.00000049 	 loss = 0.0019(0.3580)
2023/11/21 23:27:39 - INFO - root -   Epoch: [157/300][120/200], lr: 0.00000049 	 loss = 0.9172(0.3656)
2023/11/21 23:27:59 - INFO - root -   Epoch: [157/300][140/200], lr: 0.00000049 	 loss = 0.1920(0.3454)
2023/11/21 23:28:16 - INFO - root -   Epoch: [157/300][160/200], lr: 0.00000049 	 loss = 0.1255(0.3425)
2023/11/21 23:28:31 - INFO - root -   Epoch: [157/300][180/200], lr: 0.00000049 	 loss = 0.0367(0.3397)
2023/11/21 23:28:43 - INFO - root -   Epoch: [157/300] 	 loss = 0.3403
2023/11/21 23:28:43 - INFO - root -   train_accuracy = 0.8650
2023/11/21 23:28:50 - INFO - root -   Epoch: [158/300][0/200], lr: 0.00000049 	 loss = 1.5405(1.5405)
2023/11/21 23:29:08 - INFO - root -   Epoch: [158/300][20/200], lr: 0.00000049 	 loss = 0.2160(0.4625)
2023/11/21 23:29:25 - INFO - root -   Epoch: [158/300][40/200], lr: 0.00000049 	 loss = 0.2852(0.4480)
2023/11/21 23:29:39 - INFO - root -   Epoch: [158/300][60/200], lr: 0.00000049 	 loss = 0.0554(0.3838)
2023/11/21 23:29:58 - INFO - root -   Epoch: [158/300][80/200], lr: 0.00000049 	 loss = 0.7494(0.3661)
2023/11/21 23:30:06 - INFO - root -   Epoch: [158/300][100/200], lr: 0.00000049 	 loss = 0.0118(0.3459)
2023/11/21 23:30:20 - INFO - root -   Epoch: [158/300][120/200], lr: 0.00000049 	 loss = 0.7652(0.3422)
2023/11/21 23:30:33 - INFO - root -   Epoch: [158/300][140/200], lr: 0.00000049 	 loss = 0.0116(0.3341)
2023/11/21 23:30:45 - INFO - root -   Epoch: [158/300][160/200], lr: 0.00000049 	 loss = 0.7564(0.3438)
2023/11/21 23:31:05 - INFO - root -   Epoch: [158/300][180/200], lr: 0.00000049 	 loss = 0.0129(0.3496)
2023/11/21 23:31:14 - INFO - root -   Epoch: [158/300] 	 loss = 0.3558
2023/11/21 23:31:14 - INFO - root -   train_accuracy = 0.8675
2023/11/21 23:31:16 - INFO - root -   Epoch: [159/300][0/200], lr: 0.00000049 	 loss = 0.3643(0.3643)
2023/11/21 23:31:38 - INFO - root -   Epoch: [159/300][20/200], lr: 0.00000049 	 loss = 0.1795(0.4754)
2023/11/21 23:31:46 - INFO - root -   Epoch: [159/300][40/200], lr: 0.00000049 	 loss = 0.7488(0.3606)
2023/11/21 23:32:05 - INFO - root -   Epoch: [159/300][60/200], lr: 0.00000049 	 loss = 0.0598(0.3341)
2023/11/21 23:32:21 - INFO - root -   Epoch: [159/300][80/200], lr: 0.00000049 	 loss = 2.0173(0.3354)
2023/11/21 23:32:36 - INFO - root -   Epoch: [159/300][100/200], lr: 0.00000049 	 loss = 0.0101(0.3321)
2023/11/21 23:32:55 - INFO - root -   Epoch: [159/300][120/200], lr: 0.00000049 	 loss = 0.9278(0.3356)
2023/11/21 23:33:15 - INFO - root -   Epoch: [159/300][140/200], lr: 0.00000049 	 loss = 0.0836(0.3278)
2023/11/21 23:33:29 - INFO - root -   Epoch: [159/300][160/200], lr: 0.00000049 	 loss = 0.6886(0.3484)
2023/11/21 23:33:42 - INFO - root -   Epoch: [159/300][180/200], lr: 0.00000049 	 loss = 0.0108(0.3516)
2023/11/21 23:33:54 - INFO - root -   Epoch: [159/300] 	 loss = 0.3612
2023/11/21 23:35:56 - INFO - root -   precision = 0.8713
2023/11/21 23:35:56 - INFO - root -   eval_loss = 0.3579
2023/11/21 23:35:56 - INFO - root -   eval_acc = 0.8713
2023/11/21 23:35:57 - INFO - root -   train_accuracy = 0.8625
2023/11/21 23:35:58 - INFO - root -   Epoch: [160/300][0/200], lr: 0.00000050 	 loss = 2.1545(2.1545)
2023/11/21 23:36:23 - INFO - root -   Epoch: [160/300][20/200], lr: 0.00000050 	 loss = 1.0608(0.4230)
2023/11/21 23:36:39 - INFO - root -   Epoch: [160/300][40/200], lr: 0.00000050 	 loss = 0.4640(0.4087)
2023/11/21 23:36:54 - INFO - root -   Epoch: [160/300][60/200], lr: 0.00000050 	 loss = 0.2025(0.3637)
2023/11/21 23:37:05 - INFO - root -   Epoch: [160/300][80/200], lr: 0.00000050 	 loss = 0.1307(0.3296)
2023/11/21 23:37:19 - INFO - root -   Epoch: [160/300][100/200], lr: 0.00000050 	 loss = 0.0028(0.3461)
2023/11/21 23:37:34 - INFO - root -   Epoch: [160/300][120/200], lr: 0.00000050 	 loss = 0.6725(0.3576)
2023/11/21 23:37:48 - INFO - root -   Epoch: [160/300][140/200], lr: 0.00000050 	 loss = 0.0072(0.3351)
2023/11/21 23:37:59 - INFO - root -   Epoch: [160/300][160/200], lr: 0.00000050 	 loss = 0.8614(0.3352)
2023/11/21 23:38:19 - INFO - root -   Epoch: [160/300][180/200], lr: 0.00000050 	 loss = 0.0948(0.3204)
2023/11/21 23:38:33 - INFO - root -   Epoch: [160/300] 	 loss = 0.3393
2023/11/21 23:38:33 - INFO - root -   train_accuracy = 0.8775
2023/11/21 23:38:34 - INFO - root -   Epoch: [161/300][0/200], lr: 0.00000050 	 loss = 0.6909(0.6909)
2023/11/21 23:38:50 - INFO - root -   Epoch: [161/300][20/200], lr: 0.00000050 	 loss = 0.7082(0.3339)
2023/11/21 23:38:58 - INFO - root -   Epoch: [161/300][40/200], lr: 0.00000050 	 loss = 0.5871(0.3434)
2023/11/21 23:39:17 - INFO - root -   Epoch: [161/300][60/200], lr: 0.00000050 	 loss = 0.0470(0.3345)
2023/11/21 23:39:35 - INFO - root -   Epoch: [161/300][80/200], lr: 0.00000050 	 loss = 1.3464(0.3131)
2023/11/21 23:39:56 - INFO - root -   Epoch: [161/300][100/200], lr: 0.00000050 	 loss = 0.1828(0.3117)
2023/11/21 23:40:04 - INFO - root -   Epoch: [161/300][120/200], lr: 0.00000050 	 loss = 0.5609(0.3197)
2023/11/21 23:40:27 - INFO - root -   Epoch: [161/300][140/200], lr: 0.00000050 	 loss = 0.0042(0.3209)
2023/11/21 23:40:38 - INFO - root -   Epoch: [161/300][160/200], lr: 0.00000050 	 loss = 0.2167(0.3372)
2023/11/21 23:40:48 - INFO - root -   Epoch: [161/300][180/200], lr: 0.00000050 	 loss = 0.1173(0.3297)
2023/11/21 23:40:57 - INFO - root -   Epoch: [161/300] 	 loss = 0.3366
2023/11/21 23:40:57 - INFO - root -   train_accuracy = 0.8675
2023/11/21 23:41:02 - INFO - root -   Epoch: [162/300][0/200], lr: 0.00000050 	 loss = 1.7533(1.7533)
2023/11/21 23:41:11 - INFO - root -   Epoch: [162/300][20/200], lr: 0.00000050 	 loss = 0.4228(0.3599)
2023/11/21 23:41:25 - INFO - root -   Epoch: [162/300][40/200], lr: 0.00000050 	 loss = 0.6984(0.3712)
2023/11/21 23:41:39 - INFO - root -   Epoch: [162/300][60/200], lr: 0.00000050 	 loss = 0.0205(0.3497)
2023/11/21 23:41:54 - INFO - root -   Epoch: [162/300][80/200], lr: 0.00000050 	 loss = 0.6234(0.3383)
2023/11/21 23:42:08 - INFO - root -   Epoch: [162/300][100/200], lr: 0.00000050 	 loss = 0.0040(0.3246)
2023/11/21 23:42:21 - INFO - root -   Epoch: [162/300][120/200], lr: 0.00000050 	 loss = 0.8231(0.3375)
2023/11/21 23:42:37 - INFO - root -   Epoch: [162/300][140/200], lr: 0.00000050 	 loss = 0.0046(0.3177)
2023/11/21 23:42:54 - INFO - root -   Epoch: [162/300][160/200], lr: 0.00000050 	 loss = 0.3944(0.3218)
2023/11/21 23:43:14 - INFO - root -   Epoch: [162/300][180/200], lr: 0.00000050 	 loss = 0.0816(0.3350)
2023/11/21 23:43:23 - INFO - root -   Epoch: [162/300] 	 loss = 0.3336
2023/11/21 23:43:23 - INFO - root -   train_accuracy = 0.8675
2023/11/21 23:43:31 - INFO - root -   Epoch: [163/300][0/200], lr: 0.00000050 	 loss = 3.5036(3.5036)
2023/11/21 23:43:43 - INFO - root -   Epoch: [163/300][20/200], lr: 0.00000050 	 loss = 0.4350(0.4795)
2023/11/21 23:43:52 - INFO - root -   Epoch: [163/300][40/200], lr: 0.00000050 	 loss = 0.5779(0.4252)
2023/11/21 23:44:10 - INFO - root -   Epoch: [163/300][60/200], lr: 0.00000050 	 loss = 0.0182(0.3937)
2023/11/21 23:44:32 - INFO - root -   Epoch: [163/300][80/200], lr: 0.00000050 	 loss = 0.3430(0.3978)
2023/11/21 23:44:44 - INFO - root -   Epoch: [163/300][100/200], lr: 0.00000050 	 loss = 0.1388(0.4236)
2023/11/21 23:44:57 - INFO - root -   Epoch: [163/300][120/200], lr: 0.00000050 	 loss = 0.8795(0.4167)
2023/11/21 23:45:09 - INFO - root -   Epoch: [163/300][140/200], lr: 0.00000050 	 loss = 0.0455(0.3939)
2023/11/21 23:45:25 - INFO - root -   Epoch: [163/300][160/200], lr: 0.00000050 	 loss = 1.3594(0.4016)
2023/11/21 23:45:42 - INFO - root -   Epoch: [163/300][180/200], lr: 0.00000050 	 loss = 0.0106(0.3941)
2023/11/21 23:45:57 - INFO - root -   Epoch: [163/300] 	 loss = 0.3937
2023/11/21 23:45:57 - INFO - root -   train_accuracy = 0.8400
2023/11/21 23:46:07 - INFO - root -   Epoch: [164/300][0/200], lr: 0.00000051 	 loss = 1.3599(1.3599)
2023/11/21 23:46:25 - INFO - root -   Epoch: [164/300][20/200], lr: 0.00000051 	 loss = 1.2644(0.3842)
2023/11/21 23:46:39 - INFO - root -   Epoch: [164/300][40/200], lr: 0.00000051 	 loss = 0.3528(0.2983)
2023/11/21 23:46:59 - INFO - root -   Epoch: [164/300][60/200], lr: 0.00000051 	 loss = 0.0808(0.3068)
2023/11/21 23:47:13 - INFO - root -   Epoch: [164/300][80/200], lr: 0.00000051 	 loss = 1.7676(0.3507)
2023/11/21 23:47:36 - INFO - root -   Epoch: [164/300][100/200], lr: 0.00000051 	 loss = 0.0007(0.3726)
2023/11/21 23:47:47 - INFO - root -   Epoch: [164/300][120/200], lr: 0.00000051 	 loss = 1.5554(0.3860)
2023/11/21 23:48:02 - INFO - root -   Epoch: [164/300][140/200], lr: 0.00000051 	 loss = 0.0804(0.3583)
2023/11/21 23:48:17 - INFO - root -   Epoch: [164/300][160/200], lr: 0.00000051 	 loss = 1.4070(0.3583)
2023/11/21 23:48:27 - INFO - root -   Epoch: [164/300][180/200], lr: 0.00000051 	 loss = 0.0045(0.3680)
2023/11/21 23:48:42 - INFO - root -   Epoch: [164/300] 	 loss = 0.3636
2023/11/21 23:50:43 - INFO - root -   precision = 0.8713
2023/11/21 23:50:43 - INFO - root -   eval_loss = 0.3570
2023/11/21 23:50:43 - INFO - root -   eval_acc = 0.8713
2023/11/21 23:50:44 - INFO - root -   train_accuracy = 0.8600
2023/11/21 23:50:45 - INFO - root -   Epoch: [165/300][0/200], lr: 0.00000051 	 loss = 0.5773(0.5773)
2023/11/21 23:51:03 - INFO - root -   Epoch: [165/300][20/200], lr: 0.00000051 	 loss = 0.6281(0.3765)
2023/11/21 23:51:27 - INFO - root -   Epoch: [165/300][40/200], lr: 0.00000051 	 loss = 0.3378(0.3940)
2023/11/21 23:51:43 - INFO - root -   Epoch: [165/300][60/200], lr: 0.00000051 	 loss = 0.2519(0.3743)
2023/11/21 23:52:00 - INFO - root -   Epoch: [165/300][80/200], lr: 0.00000051 	 loss = 0.1590(0.3553)
2023/11/21 23:52:10 - INFO - root -   Epoch: [165/300][100/200], lr: 0.00000051 	 loss = 0.0614(0.3389)
2023/11/21 23:52:34 - INFO - root -   Epoch: [165/300][120/200], lr: 0.00000051 	 loss = 1.4142(0.3363)
2023/11/21 23:52:48 - INFO - root -   Epoch: [165/300][140/200], lr: 0.00000051 	 loss = 0.0298(0.3185)
2023/11/21 23:53:02 - INFO - root -   Epoch: [165/300][160/200], lr: 0.00000051 	 loss = 1.6905(0.3334)
2023/11/21 23:53:10 - INFO - root -   Epoch: [165/300][180/200], lr: 0.00000051 	 loss = 0.0123(0.3257)
2023/11/21 23:53:26 - INFO - root -   Epoch: [165/300] 	 loss = 0.3415
2023/11/21 23:53:26 - INFO - root -   train_accuracy = 0.8775
2023/11/21 23:53:35 - INFO - root -   Epoch: [166/300][0/200], lr: 0.00000051 	 loss = 1.7570(1.7570)
2023/11/21 23:53:51 - INFO - root -   Epoch: [166/300][20/200], lr: 0.00000051 	 loss = 0.6566(0.5889)
2023/11/21 23:54:06 - INFO - root -   Epoch: [166/300][40/200], lr: 0.00000051 	 loss = 0.1319(0.4772)
2023/11/21 23:54:23 - INFO - root -   Epoch: [166/300][60/200], lr: 0.00000051 	 loss = 0.0772(0.3836)
2023/11/21 23:54:36 - INFO - root -   Epoch: [166/300][80/200], lr: 0.00000051 	 loss = 1.0939(0.3790)
2023/11/21 23:54:56 - INFO - root -   Epoch: [166/300][100/200], lr: 0.00000051 	 loss = 0.0528(0.3811)
2023/11/21 23:55:11 - INFO - root -   Epoch: [166/300][120/200], lr: 0.00000051 	 loss = 1.6610(0.3772)
2023/11/21 23:55:27 - INFO - root -   Epoch: [166/300][140/200], lr: 0.00000051 	 loss = 0.0471(0.3616)
2023/11/21 23:55:44 - INFO - root -   Epoch: [166/300][160/200], lr: 0.00000051 	 loss = 0.6102(0.3768)
2023/11/21 23:55:59 - INFO - root -   Epoch: [166/300][180/200], lr: 0.00000051 	 loss = 0.0267(0.3618)
2023/11/21 23:56:10 - INFO - root -   Epoch: [166/300] 	 loss = 0.3553
2023/11/21 23:56:10 - INFO - root -   train_accuracy = 0.8750
2023/11/21 23:56:11 - INFO - root -   Epoch: [167/300][0/200], lr: 0.00000051 	 loss = 2.0662(2.0662)
2023/11/21 23:56:30 - INFO - root -   Epoch: [167/300][20/200], lr: 0.00000051 	 loss = 0.4029(0.3942)
2023/11/21 23:56:46 - INFO - root -   Epoch: [167/300][40/200], lr: 0.00000051 	 loss = 0.1865(0.4059)
2023/11/21 23:56:55 - INFO - root -   Epoch: [167/300][60/200], lr: 0.00000051 	 loss = 0.0501(0.3553)
2023/11/21 23:57:10 - INFO - root -   Epoch: [167/300][80/200], lr: 0.00000051 	 loss = 0.6680(0.3311)
2023/11/21 23:57:26 - INFO - root -   Epoch: [167/300][100/200], lr: 0.00000051 	 loss = 0.0276(0.3686)
2023/11/21 23:57:38 - INFO - root -   Epoch: [167/300][120/200], lr: 0.00000051 	 loss = 0.7476(0.3673)
2023/11/21 23:57:55 - INFO - root -   Epoch: [167/300][140/200], lr: 0.00000051 	 loss = 0.0353(0.3612)
2023/11/21 23:58:09 - INFO - root -   Epoch: [167/300][160/200], lr: 0.00000051 	 loss = 0.9527(0.3768)
2023/11/21 23:58:22 - INFO - root -   Epoch: [167/300][180/200], lr: 0.00000051 	 loss = 0.1611(0.3601)
2023/11/21 23:58:31 - INFO - root -   Epoch: [167/300] 	 loss = 0.3613
2023/11/21 23:58:31 - INFO - root -   train_accuracy = 0.8650
2023/11/21 23:58:37 - INFO - root -   Epoch: [168/300][0/200], lr: 0.00000052 	 loss = 1.6130(1.6130)
2023/11/21 23:58:50 - INFO - root -   Epoch: [168/300][20/200], lr: 0.00000052 	 loss = 1.1865(0.4265)
2023/11/21 23:58:58 - INFO - root -   Epoch: [168/300][40/200], lr: 0.00000052 	 loss = 0.6501(0.3391)
2023/11/21 23:59:15 - INFO - root -   Epoch: [168/300][60/200], lr: 0.00000052 	 loss = 0.4147(0.3327)
2023/11/21 23:59:33 - INFO - root -   Epoch: [168/300][80/200], lr: 0.00000052 	 loss = 1.6072(0.3507)
2023/11/21 23:59:53 - INFO - root -   Epoch: [168/300][100/200], lr: 0.00000052 	 loss = 0.0681(0.3862)
2023/11/22 00:00:11 - INFO - root -   Epoch: [168/300][120/200], lr: 0.00000052 	 loss = 1.0082(0.3860)
2023/11/22 00:00:26 - INFO - root -   Epoch: [168/300][140/200], lr: 0.00000052 	 loss = 0.0051(0.3758)
2023/11/22 00:00:51 - INFO - root -   Epoch: [168/300][160/200], lr: 0.00000052 	 loss = 0.4866(0.3730)
2023/11/22 00:01:05 - INFO - root -   Epoch: [168/300][180/200], lr: 0.00000052 	 loss = 0.2698(0.3718)
2023/11/22 00:01:20 - INFO - root -   Epoch: [168/300] 	 loss = 0.3681
2023/11/22 00:01:20 - INFO - root -   train_accuracy = 0.8675
2023/11/22 00:01:21 - INFO - root -   Epoch: [169/300][0/200], lr: 0.00000052 	 loss = 1.3841(1.3841)
2023/11/22 00:01:41 - INFO - root -   Epoch: [169/300][20/200], lr: 0.00000052 	 loss = 1.6308(0.4284)
2023/11/22 00:01:58 - INFO - root -   Epoch: [169/300][40/200], lr: 0.00000052 	 loss = 0.7386(0.3756)
2023/11/22 00:02:09 - INFO - root -   Epoch: [169/300][60/200], lr: 0.00000052 	 loss = 0.0419(0.3326)
2023/11/22 00:02:23 - INFO - root -   Epoch: [169/300][80/200], lr: 0.00000052 	 loss = 0.9257(0.3094)
2023/11/22 00:02:40 - INFO - root -   Epoch: [169/300][100/200], lr: 0.00000052 	 loss = 0.0731(0.3102)
2023/11/22 00:02:53 - INFO - root -   Epoch: [169/300][120/200], lr: 0.00000052 	 loss = 0.1456(0.3319)
2023/11/22 00:03:08 - INFO - root -   Epoch: [169/300][140/200], lr: 0.00000052 	 loss = 0.0189(0.3158)
2023/11/22 00:03:20 - INFO - root -   Epoch: [169/300][160/200], lr: 0.00000052 	 loss = 0.2833(0.3356)
2023/11/22 00:03:45 - INFO - root -   Epoch: [169/300][180/200], lr: 0.00000052 	 loss = 0.0483(0.3463)
2023/11/22 00:03:53 - INFO - root -   Epoch: [169/300] 	 loss = 0.3421
2023/11/22 00:05:55 - INFO - root -   precision = 0.8614
2023/11/22 00:05:55 - INFO - root -   eval_loss = 0.3689
2023/11/22 00:05:55 - INFO - root -   eval_acc = 0.8614
2023/11/22 00:05:56 - INFO - root -   train_accuracy = 0.8825
2023/11/22 00:06:04 - INFO - root -   Epoch: [170/300][0/200], lr: 0.00000052 	 loss = 1.6693(1.6693)
2023/11/22 00:06:29 - INFO - root -   Epoch: [170/300][20/200], lr: 0.00000052 	 loss = 1.6637(0.4473)
2023/11/22 00:06:46 - INFO - root -   Epoch: [170/300][40/200], lr: 0.00000052 	 loss = 1.9539(0.4891)
2023/11/22 00:06:59 - INFO - root -   Epoch: [170/300][60/200], lr: 0.00000052 	 loss = 0.0522(0.3885)
2023/11/22 00:07:15 - INFO - root -   Epoch: [170/300][80/200], lr: 0.00000052 	 loss = 0.0704(0.3597)
2023/11/22 00:07:31 - INFO - root -   Epoch: [170/300][100/200], lr: 0.00000052 	 loss = 0.0024(0.3697)
2023/11/22 00:07:52 - INFO - root -   Epoch: [170/300][120/200], lr: 0.00000052 	 loss = 1.7071(0.3668)
2023/11/22 00:08:03 - INFO - root -   Epoch: [170/300][140/200], lr: 0.00000052 	 loss = 0.0113(0.3550)
2023/11/22 00:08:15 - INFO - root -   Epoch: [170/300][160/200], lr: 0.00000052 	 loss = 0.3334(0.3530)
2023/11/22 00:08:32 - INFO - root -   Epoch: [170/300][180/200], lr: 0.00000052 	 loss = 0.0318(0.3539)
2023/11/22 00:08:47 - INFO - root -   Epoch: [170/300] 	 loss = 0.3498
2023/11/22 00:08:47 - INFO - root -   train_accuracy = 0.8800
2023/11/22 00:08:48 - INFO - root -   Epoch: [171/300][0/200], lr: 0.00000052 	 loss = 1.8688(1.8688)
2023/11/22 00:09:01 - INFO - root -   Epoch: [171/300][20/200], lr: 0.00000052 	 loss = 0.6528(0.3731)
2023/11/22 00:09:18 - INFO - root -   Epoch: [171/300][40/200], lr: 0.00000052 	 loss = 0.9086(0.3329)
2023/11/22 00:09:27 - INFO - root -   Epoch: [171/300][60/200], lr: 0.00000052 	 loss = 0.0052(0.2797)
2023/11/22 00:09:46 - INFO - root -   Epoch: [171/300][80/200], lr: 0.00000052 	 loss = 1.1160(0.3022)
2023/11/22 00:09:58 - INFO - root -   Epoch: [171/300][100/200], lr: 0.00000052 	 loss = 0.0109(0.3244)
2023/11/22 00:10:20 - INFO - root -   Epoch: [171/300][120/200], lr: 0.00000052 	 loss = 0.4215(0.3273)
2023/11/22 00:10:31 - INFO - root -   Epoch: [171/300][140/200], lr: 0.00000052 	 loss = 0.0109(0.3166)
2023/11/22 00:10:54 - INFO - root -   Epoch: [171/300][160/200], lr: 0.00000052 	 loss = 1.0903(0.3273)
2023/11/22 00:11:13 - INFO - root -   Epoch: [171/300][180/200], lr: 0.00000052 	 loss = 0.1213(0.3320)
2023/11/22 00:11:21 - INFO - root -   Epoch: [171/300] 	 loss = 0.3283
2023/11/22 00:11:21 - INFO - root -   train_accuracy = 0.8625
2023/11/22 00:11:27 - INFO - root -   Epoch: [172/300][0/200], lr: 0.00000053 	 loss = 0.3085(0.3085)
2023/11/22 00:11:41 - INFO - root -   Epoch: [172/300][20/200], lr: 0.00000053 	 loss = 0.6941(0.3092)
2023/11/22 00:11:58 - INFO - root -   Epoch: [172/300][40/200], lr: 0.00000053 	 loss = 0.2585(0.2796)
2023/11/22 00:12:06 - INFO - root -   Epoch: [172/300][60/200], lr: 0.00000053 	 loss = 0.1147(0.2522)
2023/11/22 00:12:21 - INFO - root -   Epoch: [172/300][80/200], lr: 0.00000053 	 loss = 0.3530(0.2260)
2023/11/22 00:12:33 - INFO - root -   Epoch: [172/300][100/200], lr: 0.00000053 	 loss = 0.0272(0.2857)
2023/11/22 00:12:45 - INFO - root -   Epoch: [172/300][120/200], lr: 0.00000053 	 loss = 0.5484(0.2914)
2023/11/22 00:12:58 - INFO - root -   Epoch: [172/300][140/200], lr: 0.00000053 	 loss = 0.0105(0.2945)
2023/11/22 00:13:11 - INFO - root -   Epoch: [172/300][160/200], lr: 0.00000053 	 loss = 0.6245(0.3050)
2023/11/22 00:13:26 - INFO - root -   Epoch: [172/300][180/200], lr: 0.00000053 	 loss = 1.1947(0.3302)
2023/11/22 00:13:40 - INFO - root -   Epoch: [172/300] 	 loss = 0.3614
2023/11/22 00:13:40 - INFO - root -   train_accuracy = 0.8600
2023/11/22 00:13:41 - INFO - root -   Epoch: [173/300][0/200], lr: 0.00000053 	 loss = 0.6037(0.6037)
2023/11/22 00:14:05 - INFO - root -   Epoch: [173/300][20/200], lr: 0.00000053 	 loss = 0.5156(0.3522)
2023/11/22 00:14:23 - INFO - root -   Epoch: [173/300][40/200], lr: 0.00000053 	 loss = 0.8720(0.3955)
2023/11/22 00:14:38 - INFO - root -   Epoch: [173/300][60/200], lr: 0.00000053 	 loss = 0.1896(0.3756)
2023/11/22 00:14:50 - INFO - root -   Epoch: [173/300][80/200], lr: 0.00000053 	 loss = 0.2132(0.3691)
2023/11/22 00:15:04 - INFO - root -   Epoch: [173/300][100/200], lr: 0.00000053 	 loss = 0.0192(0.3659)
2023/11/22 00:15:21 - INFO - root -   Epoch: [173/300][120/200], lr: 0.00000053 	 loss = 0.5351(0.3506)
2023/11/22 00:15:32 - INFO - root -   Epoch: [173/300][140/200], lr: 0.00000053 	 loss = 0.0203(0.3412)
2023/11/22 00:15:47 - INFO - root -   Epoch: [173/300][160/200], lr: 0.00000053 	 loss = 1.6432(0.3423)
2023/11/22 00:16:03 - INFO - root -   Epoch: [173/300][180/200], lr: 0.00000053 	 loss = 0.0026(0.3334)
2023/11/22 00:16:13 - INFO - root -   Epoch: [173/300] 	 loss = 0.3349
2023/11/22 00:16:13 - INFO - root -   train_accuracy = 0.8650
2023/11/22 00:16:14 - INFO - root -   Epoch: [174/300][0/200], lr: 0.00000053 	 loss = 1.2866(1.2866)
2023/11/22 00:16:35 - INFO - root -   Epoch: [174/300][20/200], lr: 0.00000053 	 loss = 1.1578(0.3750)
2023/11/22 00:16:52 - INFO - root -   Epoch: [174/300][40/200], lr: 0.00000053 	 loss = 1.8705(0.4573)
2023/11/22 00:17:09 - INFO - root -   Epoch: [174/300][60/200], lr: 0.00000053 	 loss = 0.1318(0.3840)
2023/11/22 00:17:23 - INFO - root -   Epoch: [174/300][80/200], lr: 0.00000053 	 loss = 1.2254(0.3508)
2023/11/22 00:17:31 - INFO - root -   Epoch: [174/300][100/200], lr: 0.00000053 	 loss = 0.0136(0.3806)
2023/11/22 00:17:42 - INFO - root -   Epoch: [174/300][120/200], lr: 0.00000053 	 loss = 0.4586(0.3787)
2023/11/22 00:17:59 - INFO - root -   Epoch: [174/300][140/200], lr: 0.00000053 	 loss = 0.0112(0.3656)
2023/11/22 00:18:09 - INFO - root -   Epoch: [174/300][160/200], lr: 0.00000053 	 loss = 0.4513(0.3618)
2023/11/22 00:18:25 - INFO - root -   Epoch: [174/300][180/200], lr: 0.00000053 	 loss = 0.3064(0.3613)
2023/11/22 00:18:35 - INFO - root -   Epoch: [174/300] 	 loss = 0.3733
2023/11/22 00:20:37 - INFO - root -   precision = 0.8713
2023/11/22 00:20:37 - INFO - root -   eval_loss = 0.3700
2023/11/22 00:20:37 - INFO - root -   eval_acc = 0.8713
2023/11/22 00:20:38 - INFO - root -   train_accuracy = 0.8625
2023/11/22 00:20:40 - INFO - root -   Epoch: [175/300][0/200], lr: 0.00000053 	 loss = 0.9207(0.9207)
2023/11/22 00:21:05 - INFO - root -   Epoch: [175/300][20/200], lr: 0.00000053 	 loss = 1.0949(0.3610)
2023/11/22 00:21:20 - INFO - root -   Epoch: [175/300][40/200], lr: 0.00000053 	 loss = 0.4813(0.4457)
2023/11/22 00:21:30 - INFO - root -   Epoch: [175/300][60/200], lr: 0.00000053 	 loss = 0.2119(0.3822)
2023/11/22 00:21:44 - INFO - root -   Epoch: [175/300][80/200], lr: 0.00000053 	 loss = 0.4115(0.3482)
2023/11/22 00:21:56 - INFO - root -   Epoch: [175/300][100/200], lr: 0.00000053 	 loss = 0.0022(0.3605)
2023/11/22 00:22:07 - INFO - root -   Epoch: [175/300][120/200], lr: 0.00000053 	 loss = 1.6250(0.3425)
2023/11/22 00:22:24 - INFO - root -   Epoch: [175/300][140/200], lr: 0.00000053 	 loss = 0.2186(0.3333)
2023/11/22 00:22:39 - INFO - root -   Epoch: [175/300][160/200], lr: 0.00000053 	 loss = 0.8786(0.3412)
2023/11/22 00:22:51 - INFO - root -   Epoch: [175/300][180/200], lr: 0.00000053 	 loss = 0.0037(0.3331)
2023/11/22 00:22:58 - INFO - root -   Epoch: [175/300] 	 loss = 0.3366
2023/11/22 00:22:58 - INFO - root -   train_accuracy = 0.8650
2023/11/22 00:22:59 - INFO - root -   Epoch: [176/300][0/200], lr: 0.00000054 	 loss = 0.1448(0.1448)
2023/11/22 00:23:20 - INFO - root -   Epoch: [176/300][20/200], lr: 0.00000054 	 loss = 1.2571(0.2990)
2023/11/22 00:23:30 - INFO - root -   Epoch: [176/300][40/200], lr: 0.00000054 	 loss = 0.5622(0.2848)
2023/11/22 00:23:41 - INFO - root -   Epoch: [176/300][60/200], lr: 0.00000054 	 loss = 0.0042(0.2561)
2023/11/22 00:23:53 - INFO - root -   Epoch: [176/300][80/200], lr: 0.00000054 	 loss = 0.9660(0.2798)
2023/11/22 00:24:12 - INFO - root -   Epoch: [176/300][100/200], lr: 0.00000054 	 loss = 0.1610(0.2837)
2023/11/22 00:24:25 - INFO - root -   Epoch: [176/300][120/200], lr: 0.00000054 	 loss = 0.3529(0.2793)
2023/11/22 00:24:40 - INFO - root -   Epoch: [176/300][140/200], lr: 0.00000054 	 loss = 0.1347(0.2706)
2023/11/22 00:24:55 - INFO - root -   Epoch: [176/300][160/200], lr: 0.00000054 	 loss = 1.5851(0.2746)
2023/11/22 00:25:08 - INFO - root -   Epoch: [176/300][180/200], lr: 0.00000054 	 loss = 0.0765(0.2771)
2023/11/22 00:25:20 - INFO - root -   Epoch: [176/300] 	 loss = 0.2849
2023/11/22 00:25:20 - INFO - root -   train_accuracy = 0.8875
2023/11/22 00:25:28 - INFO - root -   Epoch: [177/300][0/200], lr: 0.00000054 	 loss = 1.4194(1.4194)
2023/11/22 00:25:49 - INFO - root -   Epoch: [177/300][20/200], lr: 0.00000054 	 loss = 1.9618(0.4982)
2023/11/22 00:26:01 - INFO - root -   Epoch: [177/300][40/200], lr: 0.00000054 	 loss = 1.8635(0.4132)
2023/11/22 00:26:16 - INFO - root -   Epoch: [177/300][60/200], lr: 0.00000054 	 loss = 0.1162(0.3474)
2023/11/22 00:26:27 - INFO - root -   Epoch: [177/300][80/200], lr: 0.00000054 	 loss = 2.7087(0.3511)
2023/11/22 00:26:46 - INFO - root -   Epoch: [177/300][100/200], lr: 0.00000054 	 loss = 0.0070(0.3699)
2023/11/22 00:27:04 - INFO - root -   Epoch: [177/300][120/200], lr: 0.00000054 	 loss = 1.3080(0.3769)
2023/11/22 00:27:21 - INFO - root -   Epoch: [177/300][140/200], lr: 0.00000054 	 loss = 0.0144(0.3536)
2023/11/22 00:27:35 - INFO - root -   Epoch: [177/300][160/200], lr: 0.00000054 	 loss = 0.3909(0.3628)
2023/11/22 00:27:47 - INFO - root -   Epoch: [177/300][180/200], lr: 0.00000054 	 loss = 0.4138(0.3461)
2023/11/22 00:27:59 - INFO - root -   Epoch: [177/300] 	 loss = 0.3345
2023/11/22 00:27:59 - INFO - root -   train_accuracy = 0.8850
2023/11/22 00:28:07 - INFO - root -   Epoch: [178/300][0/200], lr: 0.00000054 	 loss = 0.8411(0.8411)
2023/11/22 00:28:28 - INFO - root -   Epoch: [178/300][20/200], lr: 0.00000054 	 loss = 1.2727(0.3789)
2023/11/22 00:28:50 - INFO - root -   Epoch: [178/300][40/200], lr: 0.00000054 	 loss = 0.8552(0.3248)
2023/11/22 00:29:07 - INFO - root -   Epoch: [178/300][60/200], lr: 0.00000054 	 loss = 0.1226(0.2895)
2023/11/22 00:29:23 - INFO - root -   Epoch: [178/300][80/200], lr: 0.00000054 	 loss = 0.2321(0.3160)
2023/11/22 00:29:38 - INFO - root -   Epoch: [178/300][100/200], lr: 0.00000054 	 loss = 0.0259(0.2975)
2023/11/22 00:29:46 - INFO - root -   Epoch: [178/300][120/200], lr: 0.00000054 	 loss = 1.3598(0.2924)
2023/11/22 00:30:08 - INFO - root -   Epoch: [178/300][140/200], lr: 0.00000054 	 loss = 0.0044(0.2950)
2023/11/22 00:30:23 - INFO - root -   Epoch: [178/300][160/200], lr: 0.00000054 	 loss = 0.8049(0.3170)
2023/11/22 00:30:32 - INFO - root -   Epoch: [178/300][180/200], lr: 0.00000054 	 loss = 0.0025(0.3083)
2023/11/22 00:30:43 - INFO - root -   Epoch: [178/300] 	 loss = 0.3055
2023/11/22 00:30:43 - INFO - root -   train_accuracy = 0.8925
2023/11/22 00:30:45 - INFO - root -   Epoch: [179/300][0/200], lr: 0.00000054 	 loss = 1.0901(1.0901)
2023/11/22 00:31:07 - INFO - root -   Epoch: [179/300][20/200], lr: 0.00000054 	 loss = 1.6727(0.4595)
2023/11/22 00:31:20 - INFO - root -   Epoch: [179/300][40/200], lr: 0.00000054 	 loss = 1.0129(0.4420)
2023/11/22 00:31:33 - INFO - root -   Epoch: [179/300][60/200], lr: 0.00000054 	 loss = 0.0311(0.3889)
2023/11/22 00:31:49 - INFO - root -   Epoch: [179/300][80/200], lr: 0.00000054 	 loss = 0.6642(0.3538)
2023/11/22 00:32:03 - INFO - root -   Epoch: [179/300][100/200], lr: 0.00000054 	 loss = 0.0014(0.3656)
2023/11/22 00:32:22 - INFO - root -   Epoch: [179/300][120/200], lr: 0.00000054 	 loss = 0.8258(0.3663)
2023/11/22 00:32:38 - INFO - root -   Epoch: [179/300][140/200], lr: 0.00000054 	 loss = 0.0814(0.3594)
2023/11/22 00:32:52 - INFO - root -   Epoch: [179/300][160/200], lr: 0.00000054 	 loss = 0.4959(0.3575)
2023/11/22 00:33:07 - INFO - root -   Epoch: [179/300][180/200], lr: 0.00000054 	 loss = 0.2226(0.3590)
2023/11/22 00:33:14 - INFO - root -   Epoch: [179/300] 	 loss = 0.3549
2023/11/22 00:35:15 - INFO - root -   precision = 0.8713
2023/11/22 00:35:15 - INFO - root -   eval_loss = 0.4084
2023/11/22 00:35:15 - INFO - root -   eval_acc = 0.8713
2023/11/22 00:35:16 - INFO - root -   train_accuracy = 0.8675
2023/11/22 00:35:18 - INFO - root -   Epoch: [180/300][0/200], lr: 0.00000055 	 loss = 0.4527(0.4527)
2023/11/22 00:35:35 - INFO - root -   Epoch: [180/300][20/200], lr: 0.00000055 	 loss = 1.4482(0.4426)
2023/11/22 00:35:49 - INFO - root -   Epoch: [180/300][40/200], lr: 0.00000055 	 loss = 0.3350(0.3486)
2023/11/22 00:36:10 - INFO - root -   Epoch: [180/300][60/200], lr: 0.00000055 	 loss = 0.2944(0.3723)
2023/11/22 00:36:21 - INFO - root -   Epoch: [180/300][80/200], lr: 0.00000055 	 loss = 0.3989(0.3478)
2023/11/22 00:36:33 - INFO - root -   Epoch: [180/300][100/200], lr: 0.00000055 	 loss = 0.2098(0.3463)
2023/11/22 00:36:49 - INFO - root -   Epoch: [180/300][120/200], lr: 0.00000055 	 loss = 0.0316(0.3277)
2023/11/22 00:37:02 - INFO - root -   Epoch: [180/300][140/200], lr: 0.00000055 	 loss = 0.1984(0.3247)
2023/11/22 00:37:23 - INFO - root -   Epoch: [180/300][160/200], lr: 0.00000055 	 loss = 0.4038(0.3363)
2023/11/22 00:37:38 - INFO - root -   Epoch: [180/300][180/200], lr: 0.00000055 	 loss = 0.1646(0.3315)
2023/11/22 00:37:46 - INFO - root -   Epoch: [180/300] 	 loss = 0.3341
2023/11/22 00:37:46 - INFO - root -   train_accuracy = 0.8975
2023/11/22 00:37:47 - INFO - root -   Epoch: [181/300][0/200], lr: 0.00000055 	 loss = 0.3666(0.3666)
2023/11/22 00:38:16 - INFO - root -   Epoch: [181/300][20/200], lr: 0.00000055 	 loss = 0.3877(0.3645)
2023/11/22 00:38:31 - INFO - root -   Epoch: [181/300][40/200], lr: 0.00000055 	 loss = 0.4018(0.2895)
2023/11/22 00:38:50 - INFO - root -   Epoch: [181/300][60/200], lr: 0.00000055 	 loss = 0.5438(0.2889)
2023/11/22 00:39:06 - INFO - root -   Epoch: [181/300][80/200], lr: 0.00000055 	 loss = 1.0303(0.2575)
2023/11/22 00:39:15 - INFO - root -   Epoch: [181/300][100/200], lr: 0.00000055 	 loss = 0.0721(0.2739)
2023/11/22 00:39:25 - INFO - root -   Epoch: [181/300][120/200], lr: 0.00000055 	 loss = 2.0259(0.3002)
2023/11/22 00:39:45 - INFO - root -   Epoch: [181/300][140/200], lr: 0.00000055 	 loss = 0.0962(0.2991)
2023/11/22 00:39:57 - INFO - root -   Epoch: [181/300][160/200], lr: 0.00000055 	 loss = 0.6320(0.3171)
2023/11/22 00:40:15 - INFO - root -   Epoch: [181/300][180/200], lr: 0.00000055 	 loss = 0.0065(0.3115)
2023/11/22 00:40:32 - INFO - root -   Epoch: [181/300] 	 loss = 0.3224
2023/11/22 00:40:32 - INFO - root -   train_accuracy = 0.8825
2023/11/22 00:40:42 - INFO - root -   Epoch: [182/300][0/200], lr: 0.00000055 	 loss = 1.5766(1.5766)
2023/11/22 00:40:55 - INFO - root -   Epoch: [182/300][20/200], lr: 0.00000055 	 loss = 0.9179(0.3578)
2023/11/22 00:41:09 - INFO - root -   Epoch: [182/300][40/200], lr: 0.00000055 	 loss = 0.5642(0.3382)
2023/11/22 00:41:21 - INFO - root -   Epoch: [182/300][60/200], lr: 0.00000055 	 loss = 0.0538(0.3109)
2023/11/22 00:41:35 - INFO - root -   Epoch: [182/300][80/200], lr: 0.00000055 	 loss = 1.3818(0.2946)
2023/11/22 00:41:53 - INFO - root -   Epoch: [182/300][100/200], lr: 0.00000055 	 loss = 0.0045(0.2985)
2023/11/22 00:42:04 - INFO - root -   Epoch: [182/300][120/200], lr: 0.00000055 	 loss = 0.0731(0.2950)
2023/11/22 00:42:15 - INFO - root -   Epoch: [182/300][140/200], lr: 0.00000055 	 loss = 0.0250(0.2860)
2023/11/22 00:42:28 - INFO - root -   Epoch: [182/300][160/200], lr: 0.00000055 	 loss = 1.2640(0.2928)
2023/11/22 00:42:43 - INFO - root -   Epoch: [182/300][180/200], lr: 0.00000055 	 loss = 0.0416(0.2957)
2023/11/22 00:42:51 - INFO - root -   Epoch: [182/300] 	 loss = 0.3063
2023/11/22 00:42:51 - INFO - root -   train_accuracy = 0.8800
2023/11/22 00:42:52 - INFO - root -   Epoch: [183/300][0/200], lr: 0.00000055 	 loss = 0.5997(0.5997)
2023/11/22 00:43:17 - INFO - root -   Epoch: [183/300][20/200], lr: 0.00000055 	 loss = 1.6545(0.5220)
2023/11/22 00:43:39 - INFO - root -   Epoch: [183/300][40/200], lr: 0.00000055 	 loss = 0.7175(0.4313)
2023/11/22 00:43:55 - INFO - root -   Epoch: [183/300][60/200], lr: 0.00000055 	 loss = 0.0055(0.3974)
2023/11/22 00:44:06 - INFO - root -   Epoch: [183/300][80/200], lr: 0.00000055 	 loss = 0.0740(0.3844)
2023/11/22 00:44:22 - INFO - root -   Epoch: [183/300][100/200], lr: 0.00000055 	 loss = 0.2006(0.3749)
2023/11/22 00:44:34 - INFO - root -   Epoch: [183/300][120/200], lr: 0.00000055 	 loss = 0.6099(0.3721)
2023/11/22 00:44:45 - INFO - root -   Epoch: [183/300][140/200], lr: 0.00000055 	 loss = 0.0062(0.3511)
2023/11/22 00:44:57 - INFO - root -   Epoch: [183/300][160/200], lr: 0.00000055 	 loss = 1.0130(0.3657)
2023/11/22 00:45:14 - INFO - root -   Epoch: [183/300][180/200], lr: 0.00000055 	 loss = 0.0886(0.3474)
2023/11/22 00:45:26 - INFO - root -   Epoch: [183/300] 	 loss = 0.3544
2023/11/22 00:45:26 - INFO - root -   train_accuracy = 0.8700
2023/11/22 00:45:27 - INFO - root -   Epoch: [184/300][0/200], lr: 0.00000056 	 loss = 0.4564(0.4564)
2023/11/22 00:45:47 - INFO - root -   Epoch: [184/300][20/200], lr: 0.00000056 	 loss = 0.7358(0.3732)
2023/11/22 00:46:02 - INFO - root -   Epoch: [184/300][40/200], lr: 0.00000056 	 loss = 0.7155(0.3383)
2023/11/22 00:46:12 - INFO - root -   Epoch: [184/300][60/200], lr: 0.00000056 	 loss = 0.0238(0.3088)
2023/11/22 00:46:27 - INFO - root -   Epoch: [184/300][80/200], lr: 0.00000056 	 loss = 0.2301(0.3185)
2023/11/22 00:46:40 - INFO - root -   Epoch: [184/300][100/200], lr: 0.00000056 	 loss = 0.0079(0.3085)
2023/11/22 00:46:53 - INFO - root -   Epoch: [184/300][120/200], lr: 0.00000056 	 loss = 0.7937(0.3125)
2023/11/22 00:47:11 - INFO - root -   Epoch: [184/300][140/200], lr: 0.00000056 	 loss = 0.0140(0.3076)
2023/11/22 00:47:28 - INFO - root -   Epoch: [184/300][160/200], lr: 0.00000056 	 loss = 0.2849(0.3264)
2023/11/22 00:47:42 - INFO - root -   Epoch: [184/300][180/200], lr: 0.00000056 	 loss = 0.1144(0.3186)
2023/11/22 00:47:52 - INFO - root -   Epoch: [184/300] 	 loss = 0.3160
2023/11/22 00:49:52 - INFO - root -   precision = 0.8713
2023/11/22 00:49:52 - INFO - root -   eval_loss = 0.3621
2023/11/22 00:49:52 - INFO - root -   eval_acc = 0.8713
2023/11/22 00:49:53 - INFO - root -   train_accuracy = 0.9025
2023/11/22 00:50:00 - INFO - root -   Epoch: [185/300][0/200], lr: 0.00000056 	 loss = 0.5479(0.5479)
2023/11/22 00:50:11 - INFO - root -   Epoch: [185/300][20/200], lr: 0.00000056 	 loss = 0.4889(0.2945)
2023/11/22 00:50:23 - INFO - root -   Epoch: [185/300][40/200], lr: 0.00000056 	 loss = 0.5476(0.3335)
2023/11/22 00:50:39 - INFO - root -   Epoch: [185/300][60/200], lr: 0.00000056 	 loss = 0.4147(0.3647)
2023/11/22 00:50:52 - INFO - root -   Epoch: [185/300][80/200], lr: 0.00000056 	 loss = 0.2217(0.3391)
2023/11/22 00:51:04 - INFO - root -   Epoch: [185/300][100/200], lr: 0.00000056 	 loss = 0.1756(0.3392)
2023/11/22 00:51:22 - INFO - root -   Epoch: [185/300][120/200], lr: 0.00000056 	 loss = 0.2189(0.3418)
2023/11/22 00:51:38 - INFO - root -   Epoch: [185/300][140/200], lr: 0.00000056 	 loss = 0.0023(0.3453)
2023/11/22 00:51:54 - INFO - root -   Epoch: [185/300][160/200], lr: 0.00000056 	 loss = 0.7498(0.3584)
2023/11/22 00:52:06 - INFO - root -   Epoch: [185/300][180/200], lr: 0.00000056 	 loss = 0.0062(0.3543)
2023/11/22 00:52:15 - INFO - root -   Epoch: [185/300] 	 loss = 0.3665
2023/11/22 00:52:15 - INFO - root -   train_accuracy = 0.8650
2023/11/22 00:52:17 - INFO - root -   Epoch: [186/300][0/200], lr: 0.00000056 	 loss = 0.9538(0.9538)
2023/11/22 00:52:38 - INFO - root -   Epoch: [186/300][20/200], lr: 0.00000056 	 loss = 0.6063(0.4350)
2023/11/22 00:52:55 - INFO - root -   Epoch: [186/300][40/200], lr: 0.00000056 	 loss = 0.5159(0.3018)
2023/11/22 00:53:08 - INFO - root -   Epoch: [186/300][60/200], lr: 0.00000056 	 loss = 0.1371(0.3143)
2023/11/22 00:53:22 - INFO - root -   Epoch: [186/300][80/200], lr: 0.00000056 	 loss = 0.2181(0.3212)
2023/11/22 00:53:38 - INFO - root -   Epoch: [186/300][100/200], lr: 0.00000056 	 loss = 0.0927(0.3567)
2023/11/22 00:53:53 - INFO - root -   Epoch: [186/300][120/200], lr: 0.00000056 	 loss = 0.8975(0.3578)
2023/11/22 00:54:13 - INFO - root -   Epoch: [186/300][140/200], lr: 0.00000056 	 loss = 0.0092(0.3364)
2023/11/22 00:54:27 - INFO - root -   Epoch: [186/300][160/200], lr: 0.00000056 	 loss = 0.6444(0.3420)
2023/11/22 00:54:48 - INFO - root -   Epoch: [186/300][180/200], lr: 0.00000056 	 loss = 0.0240(0.3489)
2023/11/22 00:55:01 - INFO - root -   Epoch: [186/300] 	 loss = 0.3430
2023/11/22 00:55:01 - INFO - root -   train_accuracy = 0.8800
2023/11/22 00:55:02 - INFO - root -   Epoch: [187/300][0/200], lr: 0.00000056 	 loss = 0.4851(0.4851)
2023/11/22 00:55:24 - INFO - root -   Epoch: [187/300][20/200], lr: 0.00000056 	 loss = 0.6025(0.2910)
2023/11/22 00:55:39 - INFO - root -   Epoch: [187/300][40/200], lr: 0.00000056 	 loss = 1.5692(0.2692)
2023/11/22 00:55:57 - INFO - root -   Epoch: [187/300][60/200], lr: 0.00000056 	 loss = 0.1120(0.2598)
2023/11/22 00:56:11 - INFO - root -   Epoch: [187/300][80/200], lr: 0.00000056 	 loss = 0.1855(0.2735)
2023/11/22 00:56:34 - INFO - root -   Epoch: [187/300][100/200], lr: 0.00000056 	 loss = 0.1502(0.2934)
2023/11/22 00:56:48 - INFO - root -   Epoch: [187/300][120/200], lr: 0.00000056 	 loss = 0.1406(0.2910)
2023/11/22 00:57:07 - INFO - root -   Epoch: [187/300][140/200], lr: 0.00000056 	 loss = 0.0065(0.2834)
2023/11/22 00:57:15 - INFO - root -   Epoch: [187/300][160/200], lr: 0.00000056 	 loss = 0.1940(0.2958)
2023/11/22 00:57:38 - INFO - root -   Epoch: [187/300][180/200], lr: 0.00000056 	 loss = 0.0589(0.2999)
2023/11/22 00:57:51 - INFO - root -   Epoch: [187/300] 	 loss = 0.3039
2023/11/22 00:57:51 - INFO - root -   train_accuracy = 0.8925
2023/11/22 00:57:57 - INFO - root -   Epoch: [188/300][0/200], lr: 0.00000057 	 loss = 0.5295(0.5295)
2023/11/22 00:58:05 - INFO - root -   Epoch: [188/300][20/200], lr: 0.00000057 	 loss = 2.1319(0.3022)
2023/11/22 00:58:18 - INFO - root -   Epoch: [188/300][40/200], lr: 0.00000057 	 loss = 0.3155(0.3101)
2023/11/22 00:58:32 - INFO - root -   Epoch: [188/300][60/200], lr: 0.00000057 	 loss = 0.3725(0.3134)
2023/11/22 00:58:46 - INFO - root -   Epoch: [188/300][80/200], lr: 0.00000057 	 loss = 0.0698(0.2934)
2023/11/22 00:59:05 - INFO - root -   Epoch: [188/300][100/200], lr: 0.00000057 	 loss = 0.0829(0.2895)
2023/11/22 00:59:13 - INFO - root -   Epoch: [188/300][120/200], lr: 0.00000057 	 loss = 0.1008(0.2923)
2023/11/22 00:59:28 - INFO - root -   Epoch: [188/300][140/200], lr: 0.00000057 	 loss = 0.0383(0.2745)
2023/11/22 00:59:41 - INFO - root -   Epoch: [188/300][160/200], lr: 0.00000057 	 loss = 1.6095(0.2879)
2023/11/22 01:00:03 - INFO - root -   Epoch: [188/300][180/200], lr: 0.00000057 	 loss = 0.0042(0.3018)
2023/11/22 01:00:11 - INFO - root -   Epoch: [188/300] 	 loss = 0.2999
2023/11/22 01:00:11 - INFO - root -   train_accuracy = 0.8950
2023/11/22 01:00:12 - INFO - root -   Epoch: [189/300][0/200], lr: 0.00000057 	 loss = 0.2178(0.2178)
2023/11/22 01:00:31 - INFO - root -   Epoch: [189/300][20/200], lr: 0.00000057 	 loss = 0.1858(0.2120)
2023/11/22 01:00:44 - INFO - root -   Epoch: [189/300][40/200], lr: 0.00000057 	 loss = 0.6572(0.2749)
2023/11/22 01:00:58 - INFO - root -   Epoch: [189/300][60/200], lr: 0.00000057 	 loss = 0.0050(0.3023)
2023/11/22 01:01:11 - INFO - root -   Epoch: [189/300][80/200], lr: 0.00000057 	 loss = 1.1052(0.3087)
2023/11/22 01:01:29 - INFO - root -   Epoch: [189/300][100/200], lr: 0.00000057 	 loss = 0.1009(0.3165)
2023/11/22 01:01:37 - INFO - root -   Epoch: [189/300][120/200], lr: 0.00000057 	 loss = 0.1511(0.3019)
2023/11/22 01:01:53 - INFO - root -   Epoch: [189/300][140/200], lr: 0.00000057 	 loss = 0.0054(0.3005)
2023/11/22 01:02:07 - INFO - root -   Epoch: [189/300][160/200], lr: 0.00000057 	 loss = 1.4861(0.3045)
2023/11/22 01:02:24 - INFO - root -   Epoch: [189/300][180/200], lr: 0.00000057 	 loss = 0.0014(0.2925)
2023/11/22 01:02:31 - INFO - root -   Epoch: [189/300] 	 loss = 0.3006
2023/11/22 01:04:32 - INFO - root -   precision = 0.8416
2023/11/22 01:04:32 - INFO - root -   eval_loss = 0.3863
2023/11/22 01:04:32 - INFO - root -   eval_acc = 0.8416
2023/11/22 01:04:33 - INFO - root -   train_accuracy = 0.8950
2023/11/22 01:04:42 - INFO - root -   Epoch: [190/300][0/200], lr: 0.00000057 	 loss = 0.9068(0.9068)
2023/11/22 01:04:58 - INFO - root -   Epoch: [190/300][20/200], lr: 0.00000057 	 loss = 0.3396(0.3531)
2023/11/22 01:05:14 - INFO - root -   Epoch: [190/300][40/200], lr: 0.00000057 	 loss = 0.2461(0.3810)
2023/11/22 01:05:34 - INFO - root -   Epoch: [190/300][60/200], lr: 0.00000057 	 loss = 0.0018(0.3427)
2023/11/22 01:05:49 - INFO - root -   Epoch: [190/300][80/200], lr: 0.00000057 	 loss = 0.4335(0.3500)
2023/11/22 01:06:08 - INFO - root -   Epoch: [190/300][100/200], lr: 0.00000057 	 loss = 0.0071(0.3450)
2023/11/22 01:06:16 - INFO - root -   Epoch: [190/300][120/200], lr: 0.00000057 	 loss = 0.8508(0.3316)
2023/11/22 01:06:34 - INFO - root -   Epoch: [190/300][140/200], lr: 0.00000057 	 loss = 0.0094(0.3098)
2023/11/22 01:06:52 - INFO - root -   Epoch: [190/300][160/200], lr: 0.00000057 	 loss = 0.4748(0.3138)
2023/11/22 01:07:10 - INFO - root -   Epoch: [190/300][180/200], lr: 0.00000057 	 loss = 0.1408(0.3138)
2023/11/22 01:07:23 - INFO - root -   Epoch: [190/300] 	 loss = 0.3101
2023/11/22 01:07:23 - INFO - root -   train_accuracy = 0.9025
2023/11/22 01:07:24 - INFO - root -   Epoch: [191/300][0/200], lr: 0.00000057 	 loss = 0.4025(0.4025)
2023/11/22 01:07:46 - INFO - root -   Epoch: [191/300][20/200], lr: 0.00000057 	 loss = 0.1053(0.3400)
2023/11/22 01:08:06 - INFO - root -   Epoch: [191/300][40/200], lr: 0.00000057 	 loss = 0.3069(0.3130)
2023/11/22 01:08:24 - INFO - root -   Epoch: [191/300][60/200], lr: 0.00000057 	 loss = 0.0156(0.2851)
2023/11/22 01:08:32 - INFO - root -   Epoch: [191/300][80/200], lr: 0.00000057 	 loss = 0.6281(0.2668)
2023/11/22 01:08:49 - INFO - root -   Epoch: [191/300][100/200], lr: 0.00000057 	 loss = 0.0353(0.2542)
2023/11/22 01:09:03 - INFO - root -   Epoch: [191/300][120/200], lr: 0.00000057 	 loss = 1.1002(0.2780)
2023/11/22 01:09:19 - INFO - root -   Epoch: [191/300][140/200], lr: 0.00000057 	 loss = 0.0914(0.2733)
2023/11/22 01:09:27 - INFO - root -   Epoch: [191/300][160/200], lr: 0.00000057 	 loss = 0.3420(0.2766)
2023/11/22 01:09:47 - INFO - root -   Epoch: [191/300][180/200], lr: 0.00000057 	 loss = 0.1945(0.2842)
2023/11/22 01:09:54 - INFO - root -   Epoch: [191/300] 	 loss = 0.2927
2023/11/22 01:09:54 - INFO - root -   train_accuracy = 0.9000
2023/11/22 01:09:55 - INFO - root -   Epoch: [192/300][0/200], lr: 0.00000058 	 loss = 1.7362(1.7362)
2023/11/22 01:10:18 - INFO - root -   Epoch: [192/300][20/200], lr: 0.00000058 	 loss = 0.8931(0.3400)
2023/11/22 01:10:33 - INFO - root -   Epoch: [192/300][40/200], lr: 0.00000058 	 loss = 0.1278(0.2880)
2023/11/22 01:10:44 - INFO - root -   Epoch: [192/300][60/200], lr: 0.00000058 	 loss = 0.4108(0.2869)
2023/11/22 01:11:00 - INFO - root -   Epoch: [192/300][80/200], lr: 0.00000058 	 loss = 0.1257(0.2678)
2023/11/22 01:11:16 - INFO - root -   Epoch: [192/300][100/200], lr: 0.00000058 	 loss = 0.0020(0.2580)
2023/11/22 01:11:33 - INFO - root -   Epoch: [192/300][120/200], lr: 0.00000058 	 loss = 0.4326(0.2621)
2023/11/22 01:11:45 - INFO - root -   Epoch: [192/300][140/200], lr: 0.00000058 	 loss = 0.1866(0.2450)
2023/11/22 01:11:53 - INFO - root -   Epoch: [192/300][160/200], lr: 0.00000058 	 loss = 0.7022(0.2614)
2023/11/22 01:12:09 - INFO - root -   Epoch: [192/300][180/200], lr: 0.00000058 	 loss = 0.0548(0.2638)
2023/11/22 01:12:19 - INFO - root -   Epoch: [192/300] 	 loss = 0.2688
2023/11/22 01:12:19 - INFO - root -   train_accuracy = 0.9075
2023/11/22 01:12:28 - INFO - root -   Epoch: [193/300][0/200], lr: 0.00000058 	 loss = 1.9623(1.9623)
2023/11/22 01:12:36 - INFO - root -   Epoch: [193/300][20/200], lr: 0.00000058 	 loss = 0.4044(0.3305)
2023/11/22 01:12:53 - INFO - root -   Epoch: [193/300][40/200], lr: 0.00000058 	 loss = 1.5731(0.3188)
2023/11/22 01:13:12 - INFO - root -   Epoch: [193/300][60/200], lr: 0.00000058 	 loss = 0.1617(0.3440)
2023/11/22 01:13:26 - INFO - root -   Epoch: [193/300][80/200], lr: 0.00000058 	 loss = 1.5113(0.3408)
2023/11/22 01:13:42 - INFO - root -   Epoch: [193/300][100/200], lr: 0.00000058 	 loss = 0.0027(0.3453)
2023/11/22 01:14:04 - INFO - root -   Epoch: [193/300][120/200], lr: 0.00000058 	 loss = 0.2482(0.3347)
2023/11/22 01:14:15 - INFO - root -   Epoch: [193/300][140/200], lr: 0.00000058 	 loss = 0.1071(0.3124)
2023/11/22 01:14:32 - INFO - root -   Epoch: [193/300][160/200], lr: 0.00000058 	 loss = 1.0604(0.3214)
2023/11/22 01:14:48 - INFO - root -   Epoch: [193/300][180/200], lr: 0.00000058 	 loss = 0.4909(0.3296)
2023/11/22 01:14:59 - INFO - root -   Epoch: [193/300] 	 loss = 0.3315
2023/11/22 01:14:59 - INFO - root -   train_accuracy = 0.8625
2023/11/22 01:15:15 - INFO - root -   Epoch: [194/300][0/200], lr: 0.00000058 	 loss = 0.8130(0.8130)
2023/11/22 01:15:26 - INFO - root -   Epoch: [194/300][20/200], lr: 0.00000058 	 loss = 1.8944(0.4433)
2023/11/22 01:15:39 - INFO - root -   Epoch: [194/300][40/200], lr: 0.00000058 	 loss = 1.4746(0.3729)
2023/11/22 01:15:52 - INFO - root -   Epoch: [194/300][60/200], lr: 0.00000058 	 loss = 0.0113(0.3778)
2023/11/22 01:16:09 - INFO - root -   Epoch: [194/300][80/200], lr: 0.00000058 	 loss = 0.2366(0.3521)
2023/11/22 01:16:32 - INFO - root -   Epoch: [194/300][100/200], lr: 0.00000058 	 loss = 0.5132(0.3492)
2023/11/22 01:16:46 - INFO - root -   Epoch: [194/300][120/200], lr: 0.00000058 	 loss = 1.7709(0.3486)
2023/11/22 01:17:03 - INFO - root -   Epoch: [194/300][140/200], lr: 0.00000058 	 loss = 0.0511(0.3304)
2023/11/22 01:17:19 - INFO - root -   Epoch: [194/300][160/200], lr: 0.00000058 	 loss = 1.3008(0.3414)
2023/11/22 01:17:32 - INFO - root -   Epoch: [194/300][180/200], lr: 0.00000058 	 loss = 0.0016(0.3307)
2023/11/22 01:17:49 - INFO - root -   Epoch: [194/300] 	 loss = 0.3287
2023/11/22 01:19:49 - INFO - root -   precision = 0.8614
2023/11/22 01:19:49 - INFO - root -   eval_loss = 0.4337
2023/11/22 01:19:49 - INFO - root -   eval_acc = 0.8614
2023/11/22 01:19:50 - INFO - root -   train_accuracy = 0.8900
2023/11/22 01:19:51 - INFO - root -   Epoch: [195/300][0/200], lr: 0.00000058 	 loss = 0.3317(0.3317)
2023/11/22 01:20:22 - INFO - root -   Epoch: [195/300][20/200], lr: 0.00000058 	 loss = 0.5694(0.3592)
2023/11/22 01:20:40 - INFO - root -   Epoch: [195/300][40/200], lr: 0.00000058 	 loss = 0.7221(0.3285)
2023/11/22 01:20:50 - INFO - root -   Epoch: [195/300][60/200], lr: 0.00000058 	 loss = 0.0449(0.3041)
2023/11/22 01:21:04 - INFO - root -   Epoch: [195/300][80/200], lr: 0.00000058 	 loss = 0.0582(0.2959)
2023/11/22 01:21:23 - INFO - root -   Epoch: [195/300][100/200], lr: 0.00000058 	 loss = 0.0271(0.2974)
2023/11/22 01:21:44 - INFO - root -   Epoch: [195/300][120/200], lr: 0.00000058 	 loss = 0.2815(0.2845)
2023/11/22 01:21:52 - INFO - root -   Epoch: [195/300][140/200], lr: 0.00000058 	 loss = 0.1309(0.2766)
2023/11/22 01:22:09 - INFO - root -   Epoch: [195/300][160/200], lr: 0.00000058 	 loss = 0.4285(0.2945)
2023/11/22 01:22:26 - INFO - root -   Epoch: [195/300][180/200], lr: 0.00000058 	 loss = 0.4152(0.3056)
2023/11/22 01:22:35 - INFO - root -   Epoch: [195/300] 	 loss = 0.3069
2023/11/22 01:22:35 - INFO - root -   train_accuracy = 0.9000
2023/11/22 01:22:36 - INFO - root -   Epoch: [196/300][0/200], lr: 0.00000059 	 loss = 0.6995(0.6995)
2023/11/22 01:23:06 - INFO - root -   Epoch: [196/300][20/200], lr: 0.00000059 	 loss = 0.4333(0.2394)
2023/11/22 01:23:21 - INFO - root -   Epoch: [196/300][40/200], lr: 0.00000059 	 loss = 0.2709(0.3020)
2023/11/22 01:23:30 - INFO - root -   Epoch: [196/300][60/200], lr: 0.00000059 	 loss = 0.4535(0.2841)
2023/11/22 01:23:42 - INFO - root -   Epoch: [196/300][80/200], lr: 0.00000059 	 loss = 0.0488(0.2562)
2023/11/22 01:23:59 - INFO - root -   Epoch: [196/300][100/200], lr: 0.00000059 	 loss = 0.0738(0.2587)
2023/11/22 01:24:08 - INFO - root -   Epoch: [196/300][120/200], lr: 0.00000059 	 loss = 0.1443(0.2671)
2023/11/22 01:24:24 - INFO - root -   Epoch: [196/300][140/200], lr: 0.00000059 	 loss = 0.1263(0.2549)
2023/11/22 01:24:34 - INFO - root -   Epoch: [196/300][160/200], lr: 0.00000059 	 loss = 0.3979(0.2640)
2023/11/22 01:24:49 - INFO - root -   Epoch: [196/300][180/200], lr: 0.00000059 	 loss = 0.0300(0.2670)
2023/11/22 01:24:57 - INFO - root -   Epoch: [196/300] 	 loss = 0.2719
2023/11/22 01:24:57 - INFO - root -   train_accuracy = 0.9050
2023/11/22 01:25:10 - INFO - root -   Epoch: [197/300][0/200], lr: 0.00000059 	 loss = 1.9813(1.9813)
2023/11/22 01:25:29 - INFO - root -   Epoch: [197/300][20/200], lr: 0.00000059 	 loss = 1.0073(0.5076)
2023/11/22 01:25:48 - INFO - root -   Epoch: [197/300][40/200], lr: 0.00000059 	 loss = 2.2212(0.4504)
2023/11/22 01:26:01 - INFO - root -   Epoch: [197/300][60/200], lr: 0.00000059 	 loss = 0.0219(0.4192)
2023/11/22 01:26:15 - INFO - root -   Epoch: [197/300][80/200], lr: 0.00000059 	 loss = 1.1857(0.3949)
2023/11/22 01:26:23 - INFO - root -   Epoch: [197/300][100/200], lr: 0.00000059 	 loss = 0.0022(0.3718)
2023/11/22 01:26:33 - INFO - root -   Epoch: [197/300][120/200], lr: 0.00000059 	 loss = 1.7435(0.3598)
2023/11/22 01:26:49 - INFO - root -   Epoch: [197/300][140/200], lr: 0.00000059 	 loss = 0.1417(0.3590)
2023/11/22 01:27:03 - INFO - root -   Epoch: [197/300][160/200], lr: 0.00000059 	 loss = 0.6108(0.3552)
2023/11/22 01:27:21 - INFO - root -   Epoch: [197/300][180/200], lr: 0.00000059 	 loss = 0.0820(0.3419)
2023/11/22 01:27:28 - INFO - root -   Epoch: [197/300] 	 loss = 0.3327
2023/11/22 01:27:28 - INFO - root -   train_accuracy = 0.8800
2023/11/22 01:27:34 - INFO - root -   Epoch: [198/300][0/200], lr: 0.00000059 	 loss = 1.9508(1.9508)
2023/11/22 01:27:46 - INFO - root -   Epoch: [198/300][20/200], lr: 0.00000059 	 loss = 0.4569(0.3380)
2023/11/22 01:28:06 - INFO - root -   Epoch: [198/300][40/200], lr: 0.00000059 	 loss = 1.7356(0.2797)
2023/11/22 01:28:19 - INFO - root -   Epoch: [198/300][60/200], lr: 0.00000059 	 loss = 0.0647(0.2927)
2023/11/22 01:28:29 - INFO - root -   Epoch: [198/300][80/200], lr: 0.00000059 	 loss = 0.5245(0.2930)
2023/11/22 01:28:43 - INFO - root -   Epoch: [198/300][100/200], lr: 0.00000059 	 loss = 0.0058(0.3115)
2023/11/22 01:29:01 - INFO - root -   Epoch: [198/300][120/200], lr: 0.00000059 	 loss = 0.1230(0.3064)
2023/11/22 01:29:16 - INFO - root -   Epoch: [198/300][140/200], lr: 0.00000059 	 loss = 0.0014(0.2914)
2023/11/22 01:29:31 - INFO - root -   Epoch: [198/300][160/200], lr: 0.00000059 	 loss = 0.3808(0.2956)
2023/11/22 01:29:46 - INFO - root -   Epoch: [198/300][180/200], lr: 0.00000059 	 loss = 0.0008(0.2829)
2023/11/22 01:30:00 - INFO - root -   Epoch: [198/300] 	 loss = 0.2885
2023/11/22 01:30:00 - INFO - root -   train_accuracy = 0.8900
2023/11/22 01:30:08 - INFO - root -   Epoch: [199/300][0/200], lr: 0.00000059 	 loss = 0.2269(0.2269)
2023/11/22 01:30:17 - INFO - root -   Epoch: [199/300][20/200], lr: 0.00000059 	 loss = 0.7379(0.2058)
2023/11/22 01:30:29 - INFO - root -   Epoch: [199/300][40/200], lr: 0.00000059 	 loss = 0.4576(0.2703)
2023/11/22 01:30:40 - INFO - root -   Epoch: [199/300][60/200], lr: 0.00000059 	 loss = 0.0045(0.2741)
2023/11/22 01:30:53 - INFO - root -   Epoch: [199/300][80/200], lr: 0.00000059 	 loss = 0.3508(0.2758)
2023/11/22 01:31:18 - INFO - root -   Epoch: [199/300][100/200], lr: 0.00000059 	 loss = 0.0372(0.2731)
2023/11/22 01:31:30 - INFO - root -   Epoch: [199/300][120/200], lr: 0.00000059 	 loss = 1.5820(0.2889)
2023/11/22 01:31:46 - INFO - root -   Epoch: [199/300][140/200], lr: 0.00000059 	 loss = 0.1081(0.2783)
2023/11/22 01:32:01 - INFO - root -   Epoch: [199/300][160/200], lr: 0.00000059 	 loss = 1.6468(0.2796)
2023/11/22 01:32:17 - INFO - root -   Epoch: [199/300][180/200], lr: 0.00000059 	 loss = 0.1224(0.3032)
2023/11/22 01:32:28 - INFO - root -   Epoch: [199/300] 	 loss = 0.3044
2023/11/22 01:34:31 - INFO - root -   precision = 0.8614
2023/11/22 01:34:31 - INFO - root -   eval_loss = 0.3707
2023/11/22 01:34:31 - INFO - root -   eval_acc = 0.8614
2023/11/22 01:34:32 - INFO - root -   train_accuracy = 0.8975
2023/11/22 01:34:40 - INFO - root -   Epoch: [200/300][0/200], lr: 0.00000060 	 loss = 0.5810(0.5810)
2023/11/22 01:34:53 - INFO - root -   Epoch: [200/300][20/200], lr: 0.00000060 	 loss = 1.8889(0.3990)
2023/11/22 01:35:05 - INFO - root -   Epoch: [200/300][40/200], lr: 0.00000060 	 loss = 0.3707(0.4015)
2023/11/22 01:35:28 - INFO - root -   Epoch: [200/300][60/200], lr: 0.00000060 	 loss = 0.7851(0.3613)
2023/11/22 01:35:41 - INFO - root -   Epoch: [200/300][80/200], lr: 0.00000060 	 loss = 0.0285(0.3342)
2023/11/22 01:35:59 - INFO - root -   Epoch: [200/300][100/200], lr: 0.00000060 	 loss = 0.0012(0.3416)
2023/11/22 01:36:12 - INFO - root -   Epoch: [200/300][120/200], lr: 0.00000060 	 loss = 0.0781(0.3390)
2023/11/22 01:36:24 - INFO - root -   Epoch: [200/300][140/200], lr: 0.00000060 	 loss = 0.0008(0.3293)
2023/11/22 01:36:38 - INFO - root -   Epoch: [200/300][160/200], lr: 0.00000060 	 loss = 0.8122(0.3472)
2023/11/22 01:36:51 - INFO - root -   Epoch: [200/300][180/200], lr: 0.00000060 	 loss = 0.0009(0.3266)
2023/11/22 01:37:06 - INFO - root -   Epoch: [200/300] 	 loss = 0.3271
2023/11/22 01:37:06 - INFO - root -   train_accuracy = 0.8850
2023/11/22 01:37:15 - INFO - root -   Epoch: [201/300][0/200], lr: 0.00000060 	 loss = 0.6552(0.6552)
2023/11/22 01:37:29 - INFO - root -   Epoch: [201/300][20/200], lr: 0.00000060 	 loss = 0.9216(0.4216)
2023/11/22 01:37:49 - INFO - root -   Epoch: [201/300][40/200], lr: 0.00000060 	 loss = 0.1897(0.3722)
2023/11/22 01:38:03 - INFO - root -   Epoch: [201/300][60/200], lr: 0.00000060 	 loss = 0.3843(0.3525)
2023/11/22 01:38:11 - INFO - root -   Epoch: [201/300][80/200], lr: 0.00000060 	 loss = 0.2578(0.3206)
2023/11/22 01:38:23 - INFO - root -   Epoch: [201/300][100/200], lr: 0.00000060 	 loss = 0.0078(0.3074)
2023/11/22 01:38:36 - INFO - root -   Epoch: [201/300][120/200], lr: 0.00000060 	 loss = 1.6159(0.3214)
2023/11/22 01:38:48 - INFO - root -   Epoch: [201/300][140/200], lr: 0.00000060 	 loss = 0.0106(0.3041)
2023/11/22 01:38:59 - INFO - root -   Epoch: [201/300][160/200], lr: 0.00000060 	 loss = 1.0334(0.2960)
2023/11/22 01:39:07 - INFO - root -   Epoch: [201/300][180/200], lr: 0.00000060 	 loss = 0.0122(0.2899)
2023/11/22 01:39:17 - INFO - root -   Epoch: [201/300] 	 loss = 0.2907
2023/11/22 01:39:17 - INFO - root -   train_accuracy = 0.8950
2023/11/22 01:39:18 - INFO - root -   Epoch: [202/300][0/200], lr: 0.00000060 	 loss = 1.3949(1.3949)
2023/11/22 01:39:38 - INFO - root -   Epoch: [202/300][20/200], lr: 0.00000060 	 loss = 0.2428(0.3078)
2023/11/22 01:39:49 - INFO - root -   Epoch: [202/300][40/200], lr: 0.00000060 	 loss = 0.4180(0.2728)
2023/11/22 01:40:05 - INFO - root -   Epoch: [202/300][60/200], lr: 0.00000060 	 loss = 0.0032(0.3150)
2023/11/22 01:40:21 - INFO - root -   Epoch: [202/300][80/200], lr: 0.00000060 	 loss = 0.1527(0.2839)
2023/11/22 01:40:35 - INFO - root -   Epoch: [202/300][100/200], lr: 0.00000060 	 loss = 0.0013(0.2787)
2023/11/22 01:40:52 - INFO - root -   Epoch: [202/300][120/200], lr: 0.00000060 	 loss = 0.0754(0.2721)
2023/11/22 01:41:07 - INFO - root -   Epoch: [202/300][140/200], lr: 0.00000060 	 loss = 0.0584(0.2703)
2023/11/22 01:41:21 - INFO - root -   Epoch: [202/300][160/200], lr: 0.00000060 	 loss = 0.5200(0.2805)
2023/11/22 01:41:39 - INFO - root -   Epoch: [202/300][180/200], lr: 0.00000060 	 loss = 0.0009(0.2820)
2023/11/22 01:41:48 - INFO - root -   Epoch: [202/300] 	 loss = 0.2899
2023/11/22 01:41:48 - INFO - root -   train_accuracy = 0.8875
2023/11/22 01:41:49 - INFO - root -   Epoch: [203/300][0/200], lr: 0.00000060 	 loss = 0.3379(0.3379)
2023/11/22 01:42:11 - INFO - root -   Epoch: [203/300][20/200], lr: 0.00000060 	 loss = 0.8289(0.3463)
2023/11/22 01:42:19 - INFO - root -   Epoch: [203/300][40/200], lr: 0.00000060 	 loss = 0.2633(0.2992)
2023/11/22 01:42:27 - INFO - root -   Epoch: [203/300][60/200], lr: 0.00000060 	 loss = 0.0564(0.2856)
2023/11/22 01:42:46 - INFO - root -   Epoch: [203/300][80/200], lr: 0.00000060 	 loss = 0.2438(0.2971)
2023/11/22 01:42:58 - INFO - root -   Epoch: [203/300][100/200], lr: 0.00000060 	 loss = 0.0014(0.3069)
2023/11/22 01:43:14 - INFO - root -   Epoch: [203/300][120/200], lr: 0.00000060 	 loss = 0.6388(0.2983)
2023/11/22 01:43:28 - INFO - root -   Epoch: [203/300][140/200], lr: 0.00000060 	 loss = 0.0004(0.2788)
2023/11/22 01:43:40 - INFO - root -   Epoch: [203/300][160/200], lr: 0.00000060 	 loss = 0.3995(0.3057)
2023/11/22 01:43:56 - INFO - root -   Epoch: [203/300][180/200], lr: 0.00000060 	 loss = 0.0938(0.3086)
2023/11/22 01:44:03 - INFO - root -   Epoch: [203/300] 	 loss = 0.3052
2023/11/22 01:44:03 - INFO - root -   train_accuracy = 0.8875
2023/11/22 01:44:04 - INFO - root -   Epoch: [204/300][0/200], lr: 0.00000060 	 loss = 0.2927(0.2927)
2023/11/22 01:44:21 - INFO - root -   Epoch: [204/300][20/200], lr: 0.00000060 	 loss = 0.3318(0.3016)
2023/11/22 01:44:40 - INFO - root -   Epoch: [204/300][40/200], lr: 0.00000060 	 loss = 0.6075(0.3404)
2023/11/22 01:44:55 - INFO - root -   Epoch: [204/300][60/200], lr: 0.00000060 	 loss = 0.3422(0.3078)
2023/11/22 01:45:06 - INFO - root -   Epoch: [204/300][80/200], lr: 0.00000060 	 loss = 0.2911(0.2818)
2023/11/22 01:45:27 - INFO - root -   Epoch: [204/300][100/200], lr: 0.00000060 	 loss = 0.0019(0.3148)
2023/11/22 01:45:40 - INFO - root -   Epoch: [204/300][120/200], lr: 0.00000060 	 loss = 0.0558(0.3262)
2023/11/22 01:45:59 - INFO - root -   Epoch: [204/300][140/200], lr: 0.00000060 	 loss = 0.1000(0.3166)
2023/11/22 01:46:07 - INFO - root -   Epoch: [204/300][160/200], lr: 0.00000060 	 loss = 0.4027(0.3345)
2023/11/22 01:46:21 - INFO - root -   Epoch: [204/300][180/200], lr: 0.00000060 	 loss = 0.0728(0.3188)
2023/11/22 01:46:29 - INFO - root -   Epoch: [204/300] 	 loss = 0.3148
2023/11/22 01:48:30 - INFO - root -   precision = 0.8713
2023/11/22 01:48:30 - INFO - root -   eval_loss = 0.3485
2023/11/22 01:48:30 - INFO - root -   eval_acc = 0.8713
2023/11/22 01:48:31 - INFO - root -   train_accuracy = 0.8850
2023/11/22 01:48:37 - INFO - root -   Epoch: [205/300][0/200], lr: 0.00000061 	 loss = 0.3781(0.3781)
2023/11/22 01:48:55 - INFO - root -   Epoch: [205/300][20/200], lr: 0.00000061 	 loss = 0.5419(0.2670)
2023/11/22 01:49:12 - INFO - root -   Epoch: [205/300][40/200], lr: 0.00000061 	 loss = 0.7818(0.3223)
2023/11/22 01:49:29 - INFO - root -   Epoch: [205/300][60/200], lr: 0.00000061 	 loss = 0.0011(0.3326)
2023/11/22 01:49:45 - INFO - root -   Epoch: [205/300][80/200], lr: 0.00000061 	 loss = 0.9278(0.3084)
2023/11/22 01:50:01 - INFO - root -   Epoch: [205/300][100/200], lr: 0.00000061 	 loss = 0.0007(0.3286)
2023/11/22 01:50:27 - INFO - root -   Epoch: [205/300][120/200], lr: 0.00000061 	 loss = 1.1044(0.3339)
2023/11/22 01:50:41 - INFO - root -   Epoch: [205/300][140/200], lr: 0.00000061 	 loss = 0.2131(0.3236)
2023/11/22 01:50:54 - INFO - root -   Epoch: [205/300][160/200], lr: 0.00000061 	 loss = 0.4708(0.3168)
2023/11/22 01:51:12 - INFO - root -   Epoch: [205/300][180/200], lr: 0.00000061 	 loss = 0.0008(0.3134)
2023/11/22 01:51:24 - INFO - root -   Epoch: [205/300] 	 loss = 0.3136
2023/11/22 01:51:24 - INFO - root -   train_accuracy = 0.8925
2023/11/22 01:51:33 - INFO - root -   Epoch: [206/300][0/200], lr: 0.00000061 	 loss = 2.2618(2.2618)
2023/11/22 01:51:49 - INFO - root -   Epoch: [206/300][20/200], lr: 0.00000061 	 loss = 1.9273(0.4628)
2023/11/22 01:51:59 - INFO - root -   Epoch: [206/300][40/200], lr: 0.00000061 	 loss = 0.6128(0.3780)
2023/11/22 01:52:15 - INFO - root -   Epoch: [206/300][60/200], lr: 0.00000061 	 loss = 0.0805(0.3320)
2023/11/22 01:52:31 - INFO - root -   Epoch: [206/300][80/200], lr: 0.00000061 	 loss = 0.1483(0.3117)
2023/11/22 01:52:52 - INFO - root -   Epoch: [206/300][100/200], lr: 0.00000061 	 loss = 0.1163(0.3216)
2023/11/22 01:53:02 - INFO - root -   Epoch: [206/300][120/200], lr: 0.00000061 	 loss = 1.0604(0.3334)
2023/11/22 01:53:18 - INFO - root -   Epoch: [206/300][140/200], lr: 0.00000061 	 loss = 0.3060(0.3045)
2023/11/22 01:53:33 - INFO - root -   Epoch: [206/300][160/200], lr: 0.00000061 	 loss = 0.8451(0.3211)
2023/11/22 01:53:48 - INFO - root -   Epoch: [206/300][180/200], lr: 0.00000061 	 loss = 1.4045(0.3354)
2023/11/22 01:54:02 - INFO - root -   Epoch: [206/300] 	 loss = 0.3389
2023/11/22 01:54:02 - INFO - root -   train_accuracy = 0.8850
2023/11/22 01:54:03 - INFO - root -   Epoch: [207/300][0/200], lr: 0.00000061 	 loss = 1.4319(1.4319)
2023/11/22 01:54:20 - INFO - root -   Epoch: [207/300][20/200], lr: 0.00000061 	 loss = 0.6529(0.3552)
2023/11/22 01:54:41 - INFO - root -   Epoch: [207/300][40/200], lr: 0.00000061 	 loss = 1.0089(0.3787)
2023/11/22 01:55:00 - INFO - root -   Epoch: [207/300][60/200], lr: 0.00000061 	 loss = 0.0038(0.3196)
2023/11/22 01:55:14 - INFO - root -   Epoch: [207/300][80/200], lr: 0.00000061 	 loss = 0.2131(0.3212)
2023/11/22 01:55:32 - INFO - root -   Epoch: [207/300][100/200], lr: 0.00000061 	 loss = 0.0648(0.3371)
2023/11/22 01:55:47 - INFO - root -   Epoch: [207/300][120/200], lr: 0.00000061 	 loss = 1.2366(0.3438)
2023/11/22 01:56:04 - INFO - root -   Epoch: [207/300][140/200], lr: 0.00000061 	 loss = 0.1296(0.3477)
2023/11/22 01:56:22 - INFO - root -   Epoch: [207/300][160/200], lr: 0.00000061 	 loss = 0.2245(0.3393)
2023/11/22 01:56:40 - INFO - root -   Epoch: [207/300][180/200], lr: 0.00000061 	 loss = 0.0726(0.3334)
2023/11/22 01:56:55 - INFO - root -   Epoch: [207/300] 	 loss = 0.3261
2023/11/22 01:56:55 - INFO - root -   train_accuracy = 0.8825
2023/11/22 01:56:57 - INFO - root -   Epoch: [208/300][0/200], lr: 0.00000061 	 loss = 0.4457(0.4457)
2023/11/22 01:57:20 - INFO - root -   Epoch: [208/300][20/200], lr: 0.00000061 	 loss = 1.7643(0.3387)
2023/11/22 01:57:34 - INFO - root -   Epoch: [208/300][40/200], lr: 0.00000061 	 loss = 0.4964(0.2939)
2023/11/22 01:57:53 - INFO - root -   Epoch: [208/300][60/200], lr: 0.00000061 	 loss = 0.0828(0.2587)
2023/11/22 01:58:06 - INFO - root -   Epoch: [208/300][80/200], lr: 0.00000061 	 loss = 0.5124(0.2478)
2023/11/22 01:58:18 - INFO - root -   Epoch: [208/300][100/200], lr: 0.00000061 	 loss = 0.0955(0.2330)
2023/11/22 01:58:27 - INFO - root -   Epoch: [208/300][120/200], lr: 0.00000061 	 loss = 1.5069(0.2402)
2023/11/22 01:58:45 - INFO - root -   Epoch: [208/300][140/200], lr: 0.00000061 	 loss = 0.0010(0.2470)
2023/11/22 01:58:59 - INFO - root -   Epoch: [208/300][160/200], lr: 0.00000061 	 loss = 1.2242(0.2616)
2023/11/22 01:59:12 - INFO - root -   Epoch: [208/300][180/200], lr: 0.00000061 	 loss = 0.1755(0.2582)
2023/11/22 01:59:21 - INFO - root -   Epoch: [208/300] 	 loss = 0.2505
2023/11/22 01:59:21 - INFO - root -   train_accuracy = 0.9000
2023/11/22 01:59:28 - INFO - root -   Epoch: [209/300][0/200], lr: 0.00000062 	 loss = 0.4923(0.4923)
2023/11/22 01:59:45 - INFO - root -   Epoch: [209/300][20/200], lr: 0.00000062 	 loss = 1.1113(0.3296)
2023/11/22 02:00:00 - INFO - root -   Epoch: [209/300][40/200], lr: 0.00000062 	 loss = 0.5578(0.3326)
2023/11/22 02:00:13 - INFO - root -   Epoch: [209/300][60/200], lr: 0.00000062 	 loss = 0.0041(0.3162)
2023/11/22 02:00:25 - INFO - root -   Epoch: [209/300][80/200], lr: 0.00000062 	 loss = 0.0561(0.2968)
2023/11/22 02:00:45 - INFO - root -   Epoch: [209/300][100/200], lr: 0.00000062 	 loss = 0.0054(0.2998)
2023/11/22 02:00:56 - INFO - root -   Epoch: [209/300][120/200], lr: 0.00000062 	 loss = 0.7873(0.3355)
2023/11/22 02:01:08 - INFO - root -   Epoch: [209/300][140/200], lr: 0.00000062 	 loss = 0.0060(0.3725)
2023/11/22 02:01:29 - INFO - root -   Epoch: [209/300][160/200], lr: 0.00000062 	 loss = 0.8710(0.3638)
2023/11/22 02:01:40 - INFO - root -   Epoch: [209/300][180/200], lr: 0.00000062 	 loss = 0.0802(0.3497)
2023/11/22 02:01:53 - INFO - root -   Epoch: [209/300] 	 loss = 0.3362
2023/11/22 02:03:55 - INFO - root -   precision = 0.8812
2023/11/22 02:03:55 - INFO - root -   eval_loss = 0.3756
2023/11/22 02:03:55 - INFO - root -   eval_acc = 0.8812
2023/11/22 02:03:56 - INFO - root -   train_accuracy = 0.8925
2023/11/22 02:04:03 - INFO - root -   Epoch: [210/300][0/200], lr: 0.00000062 	 loss = 0.3228(0.3228)
2023/11/22 02:04:11 - INFO - root -   Epoch: [210/300][20/200], lr: 0.00000062 	 loss = 0.6700(0.2601)
2023/11/22 02:04:27 - INFO - root -   Epoch: [210/300][40/200], lr: 0.00000062 	 loss = 0.1545(0.2819)
2023/11/22 02:04:38 - INFO - root -   Epoch: [210/300][60/200], lr: 0.00000062 	 loss = 0.0048(0.2762)
2023/11/22 02:04:49 - INFO - root -   Epoch: [210/300][80/200], lr: 0.00000062 	 loss = 0.0780(0.2332)
2023/11/22 02:04:58 - INFO - root -   Epoch: [210/300][100/200], lr: 0.00000062 	 loss = 0.0505(0.2682)
2023/11/22 02:05:14 - INFO - root -   Epoch: [210/300][120/200], lr: 0.00000062 	 loss = 0.8148(0.2798)
2023/11/22 02:05:35 - INFO - root -   Epoch: [210/300][140/200], lr: 0.00000062 	 loss = 0.1012(0.2723)
2023/11/22 02:05:56 - INFO - root -   Epoch: [210/300][160/200], lr: 0.00000062 	 loss = 0.6309(0.2897)
2023/11/22 02:06:12 - INFO - root -   Epoch: [210/300][180/200], lr: 0.00000062 	 loss = 0.0022(0.2869)
2023/11/22 02:06:22 - INFO - root -   Epoch: [210/300] 	 loss = 0.2895
2023/11/22 02:06:22 - INFO - root -   train_accuracy = 0.9025
2023/11/22 02:06:23 - INFO - root -   Epoch: [211/300][0/200], lr: 0.00000062 	 loss = 0.3864(0.3864)
2023/11/22 02:06:39 - INFO - root -   Epoch: [211/300][20/200], lr: 0.00000062 	 loss = 0.3036(0.2377)
2023/11/22 02:07:00 - INFO - root -   Epoch: [211/300][40/200], lr: 0.00000062 	 loss = 0.4021(0.2610)
2023/11/22 02:07:24 - INFO - root -   Epoch: [211/300][60/200], lr: 0.00000062 	 loss = 0.0400(0.2784)
2023/11/22 02:07:32 - INFO - root -   Epoch: [211/300][80/200], lr: 0.00000062 	 loss = 0.8978(0.2639)
2023/11/22 02:07:48 - INFO - root -   Epoch: [211/300][100/200], lr: 0.00000062 	 loss = 0.0057(0.2596)
2023/11/22 02:08:00 - INFO - root -   Epoch: [211/300][120/200], lr: 0.00000062 	 loss = 0.5173(0.2623)
2023/11/22 02:08:18 - INFO - root -   Epoch: [211/300][140/200], lr: 0.00000062 	 loss = 0.0655(0.2616)
2023/11/22 02:08:30 - INFO - root -   Epoch: [211/300][160/200], lr: 0.00000062 	 loss = 0.8754(0.2502)
2023/11/22 02:08:43 - INFO - root -   Epoch: [211/300][180/200], lr: 0.00000062 	 loss = 0.0931(0.2467)
2023/11/22 02:09:01 - INFO - root -   Epoch: [211/300] 	 loss = 0.2731
2023/11/22 02:09:01 - INFO - root -   train_accuracy = 0.9050
2023/11/22 02:09:08 - INFO - root -   Epoch: [212/300][0/200], lr: 0.00000062 	 loss = 2.7453(2.7453)
2023/11/22 02:09:22 - INFO - root -   Epoch: [212/300][20/200], lr: 0.00000062 	 loss = 0.7307(0.4074)
2023/11/22 02:09:38 - INFO - root -   Epoch: [212/300][40/200], lr: 0.00000062 	 loss = 1.1997(0.3309)
2023/11/22 02:09:53 - INFO - root -   Epoch: [212/300][60/200], lr: 0.00000062 	 loss = 0.5695(0.3104)
2023/11/22 02:10:10 - INFO - root -   Epoch: [212/300][80/200], lr: 0.00000062 	 loss = 1.3279(0.3556)
2023/11/22 02:10:27 - INFO - root -   Epoch: [212/300][100/200], lr: 0.00000062 	 loss = 0.0007(0.3375)
2023/11/22 02:10:44 - INFO - root -   Epoch: [212/300][120/200], lr: 0.00000062 	 loss = 0.7154(0.3507)
2023/11/22 02:10:58 - INFO - root -   Epoch: [212/300][140/200], lr: 0.00000062 	 loss = 0.0639(0.3319)
2023/11/22 02:11:18 - INFO - root -   Epoch: [212/300][160/200], lr: 0.00000062 	 loss = 0.7054(0.3217)
2023/11/22 02:11:29 - INFO - root -   Epoch: [212/300][180/200], lr: 0.00000062 	 loss = 0.0044(0.3167)
2023/11/22 02:11:40 - INFO - root -   Epoch: [212/300] 	 loss = 0.3062
2023/11/22 02:11:40 - INFO - root -   train_accuracy = 0.8850
2023/11/22 02:11:54 - INFO - root -   Epoch: [213/300][0/200], lr: 0.00000063 	 loss = 1.0538(1.0538)
2023/11/22 02:12:10 - INFO - root -   Epoch: [213/300][20/200], lr: 0.00000063 	 loss = 0.6783(0.3694)
2023/11/22 02:12:29 - INFO - root -   Epoch: [213/300][40/200], lr: 0.00000063 	 loss = 1.5146(0.3528)
2023/11/22 02:12:44 - INFO - root -   Epoch: [213/300][60/200], lr: 0.00000063 	 loss = 0.1524(0.3183)
2023/11/22 02:12:58 - INFO - root -   Epoch: [213/300][80/200], lr: 0.00000063 	 loss = 0.0322(0.3389)
2023/11/22 02:13:14 - INFO - root -   Epoch: [213/300][100/200], lr: 0.00000063 	 loss = 0.1774(0.3176)
2023/11/22 02:13:28 - INFO - root -   Epoch: [213/300][120/200], lr: 0.00000063 	 loss = 0.0290(0.2935)
2023/11/22 02:13:51 - INFO - root -   Epoch: [213/300][140/200], lr: 0.00000063 	 loss = 0.0010(0.2925)
2023/11/22 02:14:01 - INFO - root -   Epoch: [213/300][160/200], lr: 0.00000063 	 loss = 1.4378(0.3139)
2023/11/22 02:14:09 - INFO - root -   Epoch: [213/300][180/200], lr: 0.00000063 	 loss = 0.0024(0.3014)
2023/11/22 02:14:17 - INFO - root -   Epoch: [213/300] 	 loss = 0.2944
2023/11/22 02:14:17 - INFO - root -   train_accuracy = 0.9025
2023/11/22 02:14:19 - INFO - root -   Epoch: [214/300][0/200], lr: 0.00000063 	 loss = 0.3853(0.3853)
2023/11/22 02:14:38 - INFO - root -   Epoch: [214/300][20/200], lr: 0.00000063 	 loss = 0.3061(0.3124)
2023/11/22 02:14:51 - INFO - root -   Epoch: [214/300][40/200], lr: 0.00000063 	 loss = 0.2668(0.3026)
2023/11/22 02:15:01 - INFO - root -   Epoch: [214/300][60/200], lr: 0.00000063 	 loss = 0.1825(0.2587)
2023/11/22 02:15:18 - INFO - root -   Epoch: [214/300][80/200], lr: 0.00000063 	 loss = 0.0987(0.2502)
2023/11/22 02:15:38 - INFO - root -   Epoch: [214/300][100/200], lr: 0.00000063 	 loss = 0.0023(0.3048)
2023/11/22 02:15:46 - INFO - root -   Epoch: [214/300][120/200], lr: 0.00000063 	 loss = 0.9226(0.2900)
2023/11/22 02:16:05 - INFO - root -   Epoch: [214/300][140/200], lr: 0.00000063 	 loss = 0.0148(0.2974)
2023/11/22 02:16:24 - INFO - root -   Epoch: [214/300][160/200], lr: 0.00000063 	 loss = 0.5685(0.2992)
2023/11/22 02:16:40 - INFO - root -   Epoch: [214/300][180/200], lr: 0.00000063 	 loss = 0.1480(0.2873)
2023/11/22 02:16:56 - INFO - root -   Epoch: [214/300] 	 loss = 0.2939
2023/11/22 02:18:57 - INFO - root -   precision = 0.8812
2023/11/22 02:18:57 - INFO - root -   eval_loss = 0.3740
2023/11/22 02:18:57 - INFO - root -   eval_acc = 0.8812
2023/11/22 02:18:58 - INFO - root -   train_accuracy = 0.8950
2023/11/22 02:18:59 - INFO - root -   Epoch: [215/300][0/200], lr: 0.00000063 	 loss = 1.1152(1.1152)
2023/11/22 02:19:20 - INFO - root -   Epoch: [215/300][20/200], lr: 0.00000063 	 loss = 0.2115(0.2984)
2023/11/22 02:19:33 - INFO - root -   Epoch: [215/300][40/200], lr: 0.00000063 	 loss = 1.4385(0.2596)
2023/11/22 02:19:51 - INFO - root -   Epoch: [215/300][60/200], lr: 0.00000063 	 loss = 0.0043(0.2738)
2023/11/22 02:20:05 - INFO - root -   Epoch: [215/300][80/200], lr: 0.00000063 	 loss = 0.3685(0.2524)
2023/11/22 02:20:25 - INFO - root -   Epoch: [215/300][100/200], lr: 0.00000063 	 loss = 0.1494(0.2563)
2023/11/22 02:20:36 - INFO - root -   Epoch: [215/300][120/200], lr: 0.00000063 	 loss = 0.0580(0.2494)
2023/11/22 02:20:54 - INFO - root -   Epoch: [215/300][140/200], lr: 0.00000063 	 loss = 0.0858(0.2441)
2023/11/22 02:21:15 - INFO - root -   Epoch: [215/300][160/200], lr: 0.00000063 	 loss = 0.8053(0.2834)
2023/11/22 02:21:39 - INFO - root -   Epoch: [215/300][180/200], lr: 0.00000063 	 loss = 0.2301(0.2740)
2023/11/22 02:21:47 - INFO - root -   Epoch: [215/300] 	 loss = 0.2723
2023/11/22 02:21:47 - INFO - root -   train_accuracy = 0.8975
2023/11/22 02:21:55 - INFO - root -   Epoch: [216/300][0/200], lr: 0.00000063 	 loss = 1.9257(1.9257)
2023/11/22 02:22:16 - INFO - root -   Epoch: [216/300][20/200], lr: 0.00000063 	 loss = 0.9845(0.3742)
2023/11/22 02:22:32 - INFO - root -   Epoch: [216/300][40/200], lr: 0.00000063 	 loss = 1.5131(0.3995)
2023/11/22 02:22:49 - INFO - root -   Epoch: [216/300][60/200], lr: 0.00000063 	 loss = 0.3623(0.3766)
2023/11/22 02:23:01 - INFO - root -   Epoch: [216/300][80/200], lr: 0.00000063 	 loss = 0.2011(0.3577)
2023/11/22 02:23:14 - INFO - root -   Epoch: [216/300][100/200], lr: 0.00000063 	 loss = 0.0003(0.3493)
2023/11/22 02:23:22 - INFO - root -   Epoch: [216/300][120/200], lr: 0.00000063 	 loss = 0.2964(0.3378)
2023/11/22 02:23:38 - INFO - root -   Epoch: [216/300][140/200], lr: 0.00000063 	 loss = 0.0013(0.3306)
2023/11/22 02:23:50 - INFO - root -   Epoch: [216/300][160/200], lr: 0.00000063 	 loss = 0.2410(0.3218)
2023/11/22 02:24:07 - INFO - root -   Epoch: [216/300][180/200], lr: 0.00000063 	 loss = 0.0469(0.3281)
2023/11/22 02:24:20 - INFO - root -   Epoch: [216/300] 	 loss = 0.3213
2023/11/22 02:24:20 - INFO - root -   train_accuracy = 0.9050
2023/11/22 02:24:22 - INFO - root -   Epoch: [217/300][0/200], lr: 0.00000064 	 loss = 0.6868(0.6868)
2023/11/22 02:24:38 - INFO - root -   Epoch: [217/300][20/200], lr: 0.00000064 	 loss = 0.2838(0.2569)
2023/11/22 02:24:53 - INFO - root -   Epoch: [217/300][40/200], lr: 0.00000064 	 loss = 0.6814(0.2324)
2023/11/22 02:25:07 - INFO - root -   Epoch: [217/300][60/200], lr: 0.00000064 	 loss = 0.0020(0.1953)
2023/11/22 02:25:21 - INFO - root -   Epoch: [217/300][80/200], lr: 0.00000064 	 loss = 0.1462(0.1942)
2023/11/22 02:25:36 - INFO - root -   Epoch: [217/300][100/200], lr: 0.00000064 	 loss = 0.0082(0.2145)
2023/11/22 02:25:54 - INFO - root -   Epoch: [217/300][120/200], lr: 0.00000064 	 loss = 0.0743(0.2266)
2023/11/22 02:26:07 - INFO - root -   Epoch: [217/300][140/200], lr: 0.00000064 	 loss = 0.0023(0.2087)
2023/11/22 02:26:19 - INFO - root -   Epoch: [217/300][160/200], lr: 0.00000064 	 loss = 0.2940(0.2219)
2023/11/22 02:26:33 - INFO - root -   Epoch: [217/300][180/200], lr: 0.00000064 	 loss = 0.0020(0.2317)
2023/11/22 02:26:45 - INFO - root -   Epoch: [217/300] 	 loss = 0.2353
2023/11/22 02:26:45 - INFO - root -   train_accuracy = 0.9175
2023/11/22 02:26:46 - INFO - root -   Epoch: [218/300][0/200], lr: 0.00000064 	 loss = 0.2017(0.2017)
2023/11/22 02:27:01 - INFO - root -   Epoch: [218/300][20/200], lr: 0.00000064 	 loss = 0.5605(0.3098)
2023/11/22 02:27:20 - INFO - root -   Epoch: [218/300][40/200], lr: 0.00000064 	 loss = 0.8395(0.3414)
2023/11/22 02:27:37 - INFO - root -   Epoch: [218/300][60/200], lr: 0.00000064 	 loss = 0.0074(0.3208)
2023/11/22 02:27:52 - INFO - root -   Epoch: [218/300][80/200], lr: 0.00000064 	 loss = 1.8699(0.3152)
2023/11/22 02:28:09 - INFO - root -   Epoch: [218/300][100/200], lr: 0.00000064 	 loss = 0.0507(0.3141)
2023/11/22 02:28:20 - INFO - root -   Epoch: [218/300][120/200], lr: 0.00000064 	 loss = 0.2854(0.3098)
2023/11/22 02:28:28 - INFO - root -   Epoch: [218/300][140/200], lr: 0.00000064 	 loss = 0.0190(0.2877)
2023/11/22 02:28:44 - INFO - root -   Epoch: [218/300][160/200], lr: 0.00000064 	 loss = 1.4812(0.3173)
2023/11/22 02:29:01 - INFO - root -   Epoch: [218/300][180/200], lr: 0.00000064 	 loss = 0.2121(0.3081)
2023/11/22 02:29:09 - INFO - root -   Epoch: [218/300] 	 loss = 0.3026
2023/11/22 02:29:09 - INFO - root -   train_accuracy = 0.9025
2023/11/22 02:29:20 - INFO - root -   Epoch: [219/300][0/200], lr: 0.00000064 	 loss = 0.6386(0.6386)
2023/11/22 02:29:30 - INFO - root -   Epoch: [219/300][20/200], lr: 0.00000064 	 loss = 0.3633(0.2663)
2023/11/22 02:29:48 - INFO - root -   Epoch: [219/300][40/200], lr: 0.00000064 	 loss = 0.5774(0.3106)
2023/11/22 02:30:08 - INFO - root -   Epoch: [219/300][60/200], lr: 0.00000064 	 loss = 0.0043(0.3229)
2023/11/22 02:30:22 - INFO - root -   Epoch: [219/300][80/200], lr: 0.00000064 	 loss = 1.0823(0.3373)
2023/11/22 02:30:41 - INFO - root -   Epoch: [219/300][100/200], lr: 0.00000064 	 loss = 0.0011(0.3515)
2023/11/22 02:30:49 - INFO - root -   Epoch: [219/300][120/200], lr: 0.00000064 	 loss = 0.7507(0.3456)
2023/11/22 02:31:08 - INFO - root -   Epoch: [219/300][140/200], lr: 0.00000064 	 loss = 0.0536(0.3295)
2023/11/22 02:31:27 - INFO - root -   Epoch: [219/300][160/200], lr: 0.00000064 	 loss = 0.3499(0.3289)
2023/11/22 02:31:43 - INFO - root -   Epoch: [219/300][180/200], lr: 0.00000064 	 loss = 0.1514(0.3300)
2023/11/22 02:31:55 - INFO - root -   Epoch: [219/300] 	 loss = 0.3355
2023/11/22 02:33:56 - INFO - root -   precision = 0.8812
2023/11/22 02:33:56 - INFO - root -   eval_loss = 0.3491
2023/11/22 02:33:56 - INFO - root -   eval_acc = 0.8812
2023/11/22 02:33:57 - INFO - root -   train_accuracy = 0.8650
2023/11/22 02:33:58 - INFO - root -   Epoch: [220/300][0/200], lr: 0.00000064 	 loss = 0.2699(0.2699)
2023/11/22 02:34:12 - INFO - root -   Epoch: [220/300][20/200], lr: 0.00000064 	 loss = 0.1641(0.2116)
2023/11/22 02:34:24 - INFO - root -   Epoch: [220/300][40/200], lr: 0.00000064 	 loss = 0.6121(0.2455)
2023/11/22 02:34:44 - INFO - root -   Epoch: [220/300][60/200], lr: 0.00000064 	 loss = 0.8015(0.2449)
2023/11/22 02:34:52 - INFO - root -   Epoch: [220/300][80/200], lr: 0.00000064 	 loss = 0.8795(0.2475)
2023/11/22 02:35:08 - INFO - root -   Epoch: [220/300][100/200], lr: 0.00000064 	 loss = 0.0022(0.2734)
2023/11/22 02:35:24 - INFO - root -   Epoch: [220/300][120/200], lr: 0.00000064 	 loss = 2.0295(0.2978)
2023/11/22 02:35:47 - INFO - root -   Epoch: [220/300][140/200], lr: 0.00000064 	 loss = 0.0345(0.3180)
2023/11/22 02:35:59 - INFO - root -   Epoch: [220/300][160/200], lr: 0.00000064 	 loss = 0.2900(0.3262)
2023/11/22 02:36:09 - INFO - root -   Epoch: [220/300][180/200], lr: 0.00000064 	 loss = 0.5242(0.3339)
2023/11/22 02:36:18 - INFO - root -   Epoch: [220/300] 	 loss = 0.3279
2023/11/22 02:36:18 - INFO - root -   train_accuracy = 0.8825
2023/11/22 02:36:19 - INFO - root -   Epoch: [221/300][0/200], lr: 0.00000065 	 loss = 0.3545(0.3545)
2023/11/22 02:36:38 - INFO - root -   Epoch: [221/300][20/200], lr: 0.00000065 	 loss = 0.3833(0.2769)
2023/11/22 02:37:00 - INFO - root -   Epoch: [221/300][40/200], lr: 0.00000065 	 loss = 0.4065(0.2963)
2023/11/22 02:37:08 - INFO - root -   Epoch: [221/300][60/200], lr: 0.00000065 	 loss = 0.0583(0.2596)
2023/11/22 02:37:20 - INFO - root -   Epoch: [221/300][80/200], lr: 0.00000065 	 loss = 0.2831(0.2746)
2023/11/22 02:37:41 - INFO - root -   Epoch: [221/300][100/200], lr: 0.00000065 	 loss = 0.0182(0.2618)
2023/11/22 02:37:49 - INFO - root -   Epoch: [221/300][120/200], lr: 0.00000065 	 loss = 0.5763(0.2788)
2023/11/22 02:38:07 - INFO - root -   Epoch: [221/300][140/200], lr: 0.00000065 	 loss = 0.0320(0.2711)
2023/11/22 02:38:21 - INFO - root -   Epoch: [221/300][160/200], lr: 0.00000065 	 loss = 0.5271(0.2757)
2023/11/22 02:38:43 - INFO - root -   Epoch: [221/300][180/200], lr: 0.00000065 	 loss = 0.0018(0.2767)
2023/11/22 02:38:55 - INFO - root -   Epoch: [221/300] 	 loss = 0.2970
2023/11/22 02:38:55 - INFO - root -   train_accuracy = 0.8925
2023/11/22 02:39:04 - INFO - root -   Epoch: [222/300][0/200], lr: 0.00000065 	 loss = 0.9915(0.9915)
2023/11/22 02:39:22 - INFO - root -   Epoch: [222/300][20/200], lr: 0.00000065 	 loss = 0.2511(0.3776)
2023/11/22 02:39:34 - INFO - root -   Epoch: [222/300][40/200], lr: 0.00000065 	 loss = 0.7692(0.3068)
2023/11/22 02:39:46 - INFO - root -   Epoch: [222/300][60/200], lr: 0.00000065 	 loss = 0.1036(0.2788)
2023/11/22 02:39:58 - INFO - root -   Epoch: [222/300][80/200], lr: 0.00000065 	 loss = 0.0464(0.2849)
2023/11/22 02:40:16 - INFO - root -   Epoch: [222/300][100/200], lr: 0.00000065 	 loss = 0.0024(0.2822)
2023/11/22 02:40:32 - INFO - root -   Epoch: [222/300][120/200], lr: 0.00000065 	 loss = 1.7117(0.3061)
2023/11/22 02:40:43 - INFO - root -   Epoch: [222/300][140/200], lr: 0.00000065 	 loss = 0.1147(0.2856)
2023/11/22 02:40:57 - INFO - root -   Epoch: [222/300][160/200], lr: 0.00000065 	 loss = 0.3869(0.2948)
2023/11/22 02:41:18 - INFO - root -   Epoch: [222/300][180/200], lr: 0.00000065 	 loss = 0.0650(0.2892)
2023/11/22 02:41:29 - INFO - root -   Epoch: [222/300] 	 loss = 0.2810
2023/11/22 02:41:29 - INFO - root -   train_accuracy = 0.8975
2023/11/22 02:41:41 - INFO - root -   Epoch: [223/300][0/200], lr: 0.00000065 	 loss = 0.7667(0.7667)
2023/11/22 02:41:51 - INFO - root -   Epoch: [223/300][20/200], lr: 0.00000065 	 loss = 0.3727(0.2996)
2023/11/22 02:42:12 - INFO - root -   Epoch: [223/300][40/200], lr: 0.00000065 	 loss = 0.2096(0.2859)
2023/11/22 02:42:24 - INFO - root -   Epoch: [223/300][60/200], lr: 0.00000065 	 loss = 0.0010(0.2897)
2023/11/22 02:42:40 - INFO - root -   Epoch: [223/300][80/200], lr: 0.00000065 	 loss = 2.2044(0.3265)
2023/11/22 02:42:54 - INFO - root -   Epoch: [223/300][100/200], lr: 0.00000065 	 loss = 0.0004(0.3224)
2023/11/22 02:43:15 - INFO - root -   Epoch: [223/300][120/200], lr: 0.00000065 	 loss = 1.0392(0.3275)
2023/11/22 02:43:23 - INFO - root -   Epoch: [223/300][140/200], lr: 0.00000065 	 loss = 0.0436(0.3050)
2023/11/22 02:43:41 - INFO - root -   Epoch: [223/300][160/200], lr: 0.00000065 	 loss = 0.3809(0.3112)
2023/11/22 02:43:49 - INFO - root -   Epoch: [223/300][180/200], lr: 0.00000065 	 loss = 0.0054(0.2930)
2023/11/22 02:44:01 - INFO - root -   Epoch: [223/300] 	 loss = 0.2987
2023/11/22 02:44:01 - INFO - root -   train_accuracy = 0.8900
2023/11/22 02:44:03 - INFO - root -   Epoch: [224/300][0/200], lr: 0.00000065 	 loss = 0.2810(0.2810)
2023/11/22 02:44:21 - INFO - root -   Epoch: [224/300][20/200], lr: 0.00000065 	 loss = 0.5382(0.4034)
2023/11/22 02:44:41 - INFO - root -   Epoch: [224/300][40/200], lr: 0.00000065 	 loss = 0.3375(0.2811)
2023/11/22 02:44:56 - INFO - root -   Epoch: [224/300][60/200], lr: 0.00000065 	 loss = 0.0908(0.2880)
2023/11/22 02:45:10 - INFO - root -   Epoch: [224/300][80/200], lr: 0.00000065 	 loss = 0.0291(0.2753)
2023/11/22 02:45:28 - INFO - root -   Epoch: [224/300][100/200], lr: 0.00000065 	 loss = 0.0004(0.2950)
2023/11/22 02:45:38 - INFO - root -   Epoch: [224/300][120/200], lr: 0.00000065 	 loss = 1.7608(0.3027)
2023/11/22 02:45:56 - INFO - root -   Epoch: [224/300][140/200], lr: 0.00000065 	 loss = 0.0060(0.2910)
2023/11/22 02:46:11 - INFO - root -   Epoch: [224/300][160/200], lr: 0.00000065 	 loss = 1.4619(0.3185)
2023/11/22 02:46:27 - INFO - root -   Epoch: [224/300][180/200], lr: 0.00000065 	 loss = 0.1781(0.3253)
2023/11/22 02:46:39 - INFO - root -   Epoch: [224/300] 	 loss = 0.3109
2023/11/22 02:48:40 - INFO - root -   precision = 0.8515
2023/11/22 02:48:40 - INFO - root -   eval_loss = 0.3855
2023/11/22 02:48:40 - INFO - root -   eval_acc = 0.8515
2023/11/22 02:48:41 - INFO - root -   train_accuracy = 0.9050
2023/11/22 02:48:48 - INFO - root -   Epoch: [225/300][0/200], lr: 0.00000066 	 loss = 0.7682(0.7682)
2023/11/22 02:49:07 - INFO - root -   Epoch: [225/300][20/200], lr: 0.00000066 	 loss = 0.1960(0.3949)
2023/11/22 02:49:19 - INFO - root -   Epoch: [225/300][40/200], lr: 0.00000066 	 loss = 0.0417(0.3957)
2023/11/22 02:49:27 - INFO - root -   Epoch: [225/300][60/200], lr: 0.00000066 	 loss = 0.0950(0.3135)
2023/11/22 02:49:39 - INFO - root -   Epoch: [225/300][80/200], lr: 0.00000066 	 loss = 0.1216(0.3128)
2023/11/22 02:49:54 - INFO - root -   Epoch: [225/300][100/200], lr: 0.00000066 	 loss = 0.1380(0.3008)
2023/11/22 02:50:15 - INFO - root -   Epoch: [225/300][120/200], lr: 0.00000066 	 loss = 0.9902(0.3076)
2023/11/22 02:50:26 - INFO - root -   Epoch: [225/300][140/200], lr: 0.00000066 	 loss = 0.0105(0.2998)
2023/11/22 02:50:42 - INFO - root -   Epoch: [225/300][160/200], lr: 0.00000066 	 loss = 0.8341(0.3183)
2023/11/22 02:50:54 - INFO - root -   Epoch: [225/300][180/200], lr: 0.00000066 	 loss = 0.2582(0.3171)
2023/11/22 02:51:05 - INFO - root -   Epoch: [225/300] 	 loss = 0.3300
2023/11/22 02:51:05 - INFO - root -   train_accuracy = 0.8825
2023/11/22 02:51:14 - INFO - root -   Epoch: [226/300][0/200], lr: 0.00000066 	 loss = 1.2865(1.2865)
2023/11/22 02:51:33 - INFO - root -   Epoch: [226/300][20/200], lr: 0.00000066 	 loss = 0.4942(0.4039)
2023/11/22 02:51:52 - INFO - root -   Epoch: [226/300][40/200], lr: 0.00000066 	 loss = 0.7323(0.3416)
2023/11/22 02:52:05 - INFO - root -   Epoch: [226/300][60/200], lr: 0.00000066 	 loss = 0.0042(0.2959)
2023/11/22 02:52:20 - INFO - root -   Epoch: [226/300][80/200], lr: 0.00000066 	 loss = 0.8220(0.2897)
2023/11/22 02:52:30 - INFO - root -   Epoch: [226/300][100/200], lr: 0.00000066 	 loss = 0.2639(0.2821)
2023/11/22 02:52:51 - INFO - root -   Epoch: [226/300][120/200], lr: 0.00000066 	 loss = 0.3386(0.2846)
2023/11/22 02:53:04 - INFO - root -   Epoch: [226/300][140/200], lr: 0.00000066 	 loss = 0.0613(0.2817)
2023/11/22 02:53:22 - INFO - root -   Epoch: [226/300][160/200], lr: 0.00000066 	 loss = 0.6306(0.2791)
2023/11/22 02:53:44 - INFO - root -   Epoch: [226/300][180/200], lr: 0.00000066 	 loss = 0.0005(0.2750)
2023/11/22 02:53:55 - INFO - root -   Epoch: [226/300] 	 loss = 0.2739
2023/11/22 02:53:55 - INFO - root -   train_accuracy = 0.9100
2023/11/22 02:54:04 - INFO - root -   Epoch: [227/300][0/200], lr: 0.00000066 	 loss = 1.2672(1.2672)
2023/11/22 02:54:17 - INFO - root -   Epoch: [227/300][20/200], lr: 0.00000066 	 loss = 1.5107(0.3412)
2023/11/22 02:54:30 - INFO - root -   Epoch: [227/300][40/200], lr: 0.00000066 	 loss = 0.0982(0.3227)
2023/11/22 02:54:47 - INFO - root -   Epoch: [227/300][60/200], lr: 0.00000066 	 loss = 0.0096(0.2890)
2023/11/22 02:55:05 - INFO - root -   Epoch: [227/300][80/200], lr: 0.00000066 	 loss = 0.7629(0.2927)
2023/11/22 02:55:22 - INFO - root -   Epoch: [227/300][100/200], lr: 0.00000066 	 loss = 0.0303(0.2951)
2023/11/22 02:55:38 - INFO - root -   Epoch: [227/300][120/200], lr: 0.00000066 	 loss = 0.0155(0.3018)
2023/11/22 02:55:53 - INFO - root -   Epoch: [227/300][140/200], lr: 0.00000066 	 loss = 0.0308(0.2836)
2023/11/22 02:56:08 - INFO - root -   Epoch: [227/300][160/200], lr: 0.00000066 	 loss = 1.1167(0.2997)
2023/11/22 02:56:29 - INFO - root -   Epoch: [227/300][180/200], lr: 0.00000066 	 loss = 0.1861(0.2932)
2023/11/22 02:56:36 - INFO - root -   Epoch: [227/300] 	 loss = 0.2941
2023/11/22 02:56:36 - INFO - root -   train_accuracy = 0.9075
2023/11/22 02:56:43 - INFO - root -   Epoch: [228/300][0/200], lr: 0.00000066 	 loss = 0.6108(0.6108)
2023/11/22 02:56:59 - INFO - root -   Epoch: [228/300][20/200], lr: 0.00000066 	 loss = 0.9595(0.3045)
2023/11/22 02:57:07 - INFO - root -   Epoch: [228/300][40/200], lr: 0.00000066 	 loss = 1.0415(0.3109)
2023/11/22 02:57:24 - INFO - root -   Epoch: [228/300][60/200], lr: 0.00000066 	 loss = 0.1855(0.2997)
2023/11/22 02:57:40 - INFO - root -   Epoch: [228/300][80/200], lr: 0.00000066 	 loss = 0.1707(0.2922)
2023/11/22 02:57:57 - INFO - root -   Epoch: [228/300][100/200], lr: 0.00000066 	 loss = 0.0041(0.2986)
2023/11/22 02:58:10 - INFO - root -   Epoch: [228/300][120/200], lr: 0.00000066 	 loss = 0.6369(0.2949)
2023/11/22 02:58:23 - INFO - root -   Epoch: [228/300][140/200], lr: 0.00000066 	 loss = 0.0384(0.2781)
2023/11/22 02:58:38 - INFO - root -   Epoch: [228/300][160/200], lr: 0.00000066 	 loss = 0.1907(0.2772)
2023/11/22 02:58:54 - INFO - root -   Epoch: [228/300][180/200], lr: 0.00000066 	 loss = 0.0612(0.2914)
2023/11/22 02:59:02 - INFO - root -   Epoch: [228/300] 	 loss = 0.2875
2023/11/22 02:59:02 - INFO - root -   train_accuracy = 0.8900
2023/11/22 02:59:04 - INFO - root -   Epoch: [229/300][0/200], lr: 0.00000067 	 loss = 1.1968(1.1968)
2023/11/22 02:59:24 - INFO - root -   Epoch: [229/300][20/200], lr: 0.00000067 	 loss = 1.0122(0.3677)
2023/11/22 02:59:41 - INFO - root -   Epoch: [229/300][40/200], lr: 0.00000067 	 loss = 0.9499(0.4287)
2023/11/22 02:59:57 - INFO - root -   Epoch: [229/300][60/200], lr: 0.00000067 	 loss = 0.0030(0.3555)
2023/11/22 03:00:13 - INFO - root -   Epoch: [229/300][80/200], lr: 0.00000067 	 loss = 0.0608(0.3669)
2023/11/22 03:00:33 - INFO - root -   Epoch: [229/300][100/200], lr: 0.00000067 	 loss = 0.1280(0.3713)
2023/11/22 03:00:46 - INFO - root -   Epoch: [229/300][120/200], lr: 0.00000067 	 loss = 0.3802(0.3412)
2023/11/22 03:01:00 - INFO - root -   Epoch: [229/300][140/200], lr: 0.00000067 	 loss = 0.0575(0.3448)
2023/11/22 03:01:17 - INFO - root -   Epoch: [229/300][160/200], lr: 0.00000067 	 loss = 1.2648(0.3435)
2023/11/22 03:01:36 - INFO - root -   Epoch: [229/300][180/200], lr: 0.00000067 	 loss = 0.1637(0.3431)
2023/11/22 03:01:47 - INFO - root -   Epoch: [229/300] 	 loss = 0.3274
2023/11/22 03:03:48 - INFO - root -   precision = 0.8812
2023/11/22 03:03:48 - INFO - root -   eval_loss = 0.3259
2023/11/22 03:03:48 - INFO - root -   eval_acc = 0.8812
2023/11/22 03:03:49 - INFO - root -   train_accuracy = 0.8900
2023/11/22 03:03:58 - INFO - root -   Epoch: [230/300][0/200], lr: 0.00000067 	 loss = 0.2867(0.2867)
2023/11/22 03:04:15 - INFO - root -   Epoch: [230/300][20/200], lr: 0.00000067 	 loss = 1.8333(0.3234)
2023/11/22 03:04:29 - INFO - root -   Epoch: [230/300][40/200], lr: 0.00000067 	 loss = 0.2988(0.2378)
2023/11/22 03:04:45 - INFO - root -   Epoch: [230/300][60/200], lr: 0.00000067 	 loss = 0.0016(0.2397)
2023/11/22 03:05:06 - INFO - root -   Epoch: [230/300][80/200], lr: 0.00000067 	 loss = 0.4857(0.2561)
2023/11/22 03:05:18 - INFO - root -   Epoch: [230/300][100/200], lr: 0.00000067 	 loss = 0.0001(0.2545)
2023/11/22 03:05:29 - INFO - root -   Epoch: [230/300][120/200], lr: 0.00000067 	 loss = 0.0368(0.2456)
2023/11/22 03:05:37 - INFO - root -   Epoch: [230/300][140/200], lr: 0.00000067 	 loss = 0.0666(0.2325)
2023/11/22 03:05:50 - INFO - root -   Epoch: [230/300][160/200], lr: 0.00000067 	 loss = 1.2766(0.2428)
2023/11/22 03:06:01 - INFO - root -   Epoch: [230/300][180/200], lr: 0.00000067 	 loss = 0.1610(0.2285)
2023/11/22 03:06:13 - INFO - root -   Epoch: [230/300] 	 loss = 0.2329
2023/11/22 03:06:13 - INFO - root -   train_accuracy = 0.9200
2023/11/22 03:06:15 - INFO - root -   Epoch: [231/300][0/200], lr: 0.00000067 	 loss = 0.0590(0.0590)
2023/11/22 03:06:39 - INFO - root -   Epoch: [231/300][20/200], lr: 0.00000067 	 loss = 0.5868(0.3179)
2023/11/22 03:06:56 - INFO - root -   Epoch: [231/300][40/200], lr: 0.00000067 	 loss = 0.8870(0.2654)
2023/11/22 03:07:11 - INFO - root -   Epoch: [231/300][60/200], lr: 0.00000067 	 loss = 0.0032(0.2669)
2023/11/22 03:07:27 - INFO - root -   Epoch: [231/300][80/200], lr: 0.00000067 	 loss = 0.1793(0.2633)
2023/11/22 03:07:44 - INFO - root -   Epoch: [231/300][100/200], lr: 0.00000067 	 loss = 0.0065(0.2529)
2023/11/22 03:08:10 - INFO - root -   Epoch: [231/300][120/200], lr: 0.00000067 	 loss = 0.1508(0.2448)
2023/11/22 03:08:23 - INFO - root -   Epoch: [231/300][140/200], lr: 0.00000067 	 loss = 0.0731(0.2312)
2023/11/22 03:08:37 - INFO - root -   Epoch: [231/300][160/200], lr: 0.00000067 	 loss = 1.4787(0.2569)
2023/11/22 03:08:51 - INFO - root -   Epoch: [231/300][180/200], lr: 0.00000067 	 loss = 0.0007(0.2554)
2023/11/22 03:09:01 - INFO - root -   Epoch: [231/300] 	 loss = 0.2558
2023/11/22 03:09:01 - INFO - root -   train_accuracy = 0.9100
2023/11/22 03:09:13 - INFO - root -   Epoch: [232/300][0/200], lr: 0.00000067 	 loss = 1.1872(1.1872)
2023/11/22 03:09:21 - INFO - root -   Epoch: [232/300][20/200], lr: 0.00000067 	 loss = 0.1946(0.2502)
2023/11/22 03:09:32 - INFO - root -   Epoch: [232/300][40/200], lr: 0.00000067 	 loss = 1.1405(0.2553)
2023/11/22 03:09:49 - INFO - root -   Epoch: [232/300][60/200], lr: 0.00000067 	 loss = 0.0020(0.2205)
2023/11/22 03:10:04 - INFO - root -   Epoch: [232/300][80/200], lr: 0.00000067 	 loss = 1.2958(0.2459)
2023/11/22 03:10:12 - INFO - root -   Epoch: [232/300][100/200], lr: 0.00000067 	 loss = 0.0090(0.2737)
2023/11/22 03:10:25 - INFO - root -   Epoch: [232/300][120/200], lr: 0.00000067 	 loss = 0.2880(0.2823)
2023/11/22 03:10:43 - INFO - root -   Epoch: [232/300][140/200], lr: 0.00000067 	 loss = 0.0043(0.2754)
2023/11/22 03:10:56 - INFO - root -   Epoch: [232/300][160/200], lr: 0.00000067 	 loss = 0.9116(0.2805)
2023/11/22 03:11:04 - INFO - root -   Epoch: [232/300][180/200], lr: 0.00000067 	 loss = 0.0028(0.2720)
2023/11/22 03:11:15 - INFO - root -   Epoch: [232/300] 	 loss = 0.2678
2023/11/22 03:11:15 - INFO - root -   train_accuracy = 0.9000
2023/11/22 03:11:22 - INFO - root -   Epoch: [233/300][0/200], lr: 0.00000068 	 loss = 0.6871(0.6871)
2023/11/22 03:11:34 - INFO - root -   Epoch: [233/300][20/200], lr: 0.00000068 	 loss = 0.3618(0.2327)
2023/11/22 03:11:58 - INFO - root -   Epoch: [233/300][40/200], lr: 0.00000068 	 loss = 0.4713(0.2950)
2023/11/22 03:12:14 - INFO - root -   Epoch: [233/300][60/200], lr: 0.00000068 	 loss = 0.0799(0.2752)
2023/11/22 03:12:40 - INFO - root -   Epoch: [233/300][80/200], lr: 0.00000068 	 loss = 1.4081(0.3101)
2023/11/22 03:12:53 - INFO - root -   Epoch: [233/300][100/200], lr: 0.00000068 	 loss = 0.1388(0.3130)
2023/11/22 03:13:06 - INFO - root -   Epoch: [233/300][120/200], lr: 0.00000068 	 loss = 0.0456(0.2942)
2023/11/22 03:13:20 - INFO - root -   Epoch: [233/300][140/200], lr: 0.00000068 	 loss = 0.3298(0.2711)
2023/11/22 03:13:39 - INFO - root -   Epoch: [233/300][160/200], lr: 0.00000068 	 loss = 1.2557(0.2856)
2023/11/22 03:13:57 - INFO - root -   Epoch: [233/300][180/200], lr: 0.00000068 	 loss = 0.0020(0.2902)
2023/11/22 03:14:04 - INFO - root -   Epoch: [233/300] 	 loss = 0.2822
2023/11/22 03:14:04 - INFO - root -   train_accuracy = 0.8950
2023/11/22 03:14:05 - INFO - root -   Epoch: [234/300][0/200], lr: 0.00000068 	 loss = 0.1054(0.1054)
2023/11/22 03:14:30 - INFO - root -   Epoch: [234/300][20/200], lr: 0.00000068 	 loss = 1.5896(0.3824)
2023/11/22 03:14:44 - INFO - root -   Epoch: [234/300][40/200], lr: 0.00000068 	 loss = 0.3252(0.3099)
2023/11/22 03:15:03 - INFO - root -   Epoch: [234/300][60/200], lr: 0.00000068 	 loss = 0.0042(0.3174)
2023/11/22 03:15:16 - INFO - root -   Epoch: [234/300][80/200], lr: 0.00000068 	 loss = 0.2863(0.2928)
2023/11/22 03:15:30 - INFO - root -   Epoch: [234/300][100/200], lr: 0.00000068 	 loss = 0.0013(0.2949)
2023/11/22 03:15:46 - INFO - root -   Epoch: [234/300][120/200], lr: 0.00000068 	 loss = 0.2225(0.2917)
2023/11/22 03:16:04 - INFO - root -   Epoch: [234/300][140/200], lr: 0.00000068 	 loss = 0.0850(0.2824)
2023/11/22 03:16:12 - INFO - root -   Epoch: [234/300][160/200], lr: 0.00000068 	 loss = 1.1664(0.2768)
2023/11/22 03:16:28 - INFO - root -   Epoch: [234/300][180/200], lr: 0.00000068 	 loss = 0.1929(0.2748)
2023/11/22 03:16:37 - INFO - root -   Epoch: [234/300] 	 loss = 0.2844
2023/11/22 03:18:39 - INFO - root -   precision = 0.8614
2023/11/22 03:18:39 - INFO - root -   eval_loss = 0.4097
2023/11/22 03:18:39 - INFO - root -   eval_acc = 0.8614
2023/11/22 03:18:40 - INFO - root -   train_accuracy = 0.8975
2023/11/22 03:18:42 - INFO - root -   Epoch: [235/300][0/200], lr: 0.00000068 	 loss = 0.3670(0.3670)
2023/11/22 03:19:04 - INFO - root -   Epoch: [235/300][20/200], lr: 0.00000068 	 loss = 0.4748(0.2148)
2023/11/22 03:19:24 - INFO - root -   Epoch: [235/300][40/200], lr: 0.00000068 	 loss = 1.5239(0.2269)
2023/11/22 03:19:35 - INFO - root -   Epoch: [235/300][60/200], lr: 0.00000068 	 loss = 0.0640(0.2001)
2023/11/22 03:19:43 - INFO - root -   Epoch: [235/300][80/200], lr: 0.00000068 	 loss = 0.1532(0.2016)
2023/11/22 03:20:03 - INFO - root -   Epoch: [235/300][100/200], lr: 0.00000068 	 loss = 0.2420(0.2375)
2023/11/22 03:20:18 - INFO - root -   Epoch: [235/300][120/200], lr: 0.00000068 	 loss = 0.0403(0.2469)
2023/11/22 03:20:34 - INFO - root -   Epoch: [235/300][140/200], lr: 0.00000068 	 loss = 0.0119(0.2357)
2023/11/22 03:20:48 - INFO - root -   Epoch: [235/300][160/200], lr: 0.00000068 	 loss = 0.3772(0.2493)
2023/11/22 03:21:00 - INFO - root -   Epoch: [235/300][180/200], lr: 0.00000068 	 loss = 0.0106(0.2441)
2023/11/22 03:21:12 - INFO - root -   Epoch: [235/300] 	 loss = 0.2502
2023/11/22 03:21:12 - INFO - root -   train_accuracy = 0.9150
2023/11/22 03:21:20 - INFO - root -   Epoch: [236/300][0/200], lr: 0.00000068 	 loss = 0.3186(0.3186)
2023/11/22 03:21:35 - INFO - root -   Epoch: [236/300][20/200], lr: 0.00000068 	 loss = 0.3785(0.4125)
2023/11/22 03:21:51 - INFO - root -   Epoch: [236/300][40/200], lr: 0.00000068 	 loss = 0.5032(0.3743)
2023/11/22 03:22:05 - INFO - root -   Epoch: [236/300][60/200], lr: 0.00000068 	 loss = 0.1549(0.3588)
2023/11/22 03:22:19 - INFO - root -   Epoch: [236/300][80/200], lr: 0.00000068 	 loss = 0.0623(0.3263)
2023/11/22 03:22:37 - INFO - root -   Epoch: [236/300][100/200], lr: 0.00000068 	 loss = 0.0084(0.3044)
2023/11/22 03:23:02 - INFO - root -   Epoch: [236/300][120/200], lr: 0.00000068 	 loss = 0.0761(0.3167)
2023/11/22 03:23:19 - INFO - root -   Epoch: [236/300][140/200], lr: 0.00000068 	 loss = 0.0714(0.3022)
2023/11/22 03:23:37 - INFO - root -   Epoch: [236/300][160/200], lr: 0.00000068 	 loss = 1.4743(0.3097)
2023/11/22 03:23:59 - INFO - root -   Epoch: [236/300][180/200], lr: 0.00000068 	 loss = 0.0278(0.3205)
2023/11/22 03:24:10 - INFO - root -   Epoch: [236/300] 	 loss = 0.3163
2023/11/22 03:24:10 - INFO - root -   train_accuracy = 0.9125
2023/11/22 03:24:12 - INFO - root -   Epoch: [237/300][0/200], lr: 0.00000069 	 loss = 0.9450(0.9450)
2023/11/22 03:24:28 - INFO - root -   Epoch: [237/300][20/200], lr: 0.00000069 	 loss = 0.7614(0.2587)
2023/11/22 03:24:45 - INFO - root -   Epoch: [237/300][40/200], lr: 0.00000069 	 loss = 0.2033(0.2493)
2023/11/22 03:24:58 - INFO - root -   Epoch: [237/300][60/200], lr: 0.00000069 	 loss = 0.0052(0.2253)
2023/11/22 03:25:10 - INFO - root -   Epoch: [237/300][80/200], lr: 0.00000069 	 loss = 0.4315(0.2378)
2023/11/22 03:25:29 - INFO - root -   Epoch: [237/300][100/200], lr: 0.00000069 	 loss = 0.0006(0.2426)
2023/11/22 03:25:40 - INFO - root -   Epoch: [237/300][120/200], lr: 0.00000069 	 loss = 0.1889(0.2309)
2023/11/22 03:26:00 - INFO - root -   Epoch: [237/300][140/200], lr: 0.00000069 	 loss = 0.0017(0.2588)
2023/11/22 03:26:16 - INFO - root -   Epoch: [237/300][160/200], lr: 0.00000069 	 loss = 0.0918(0.2796)
2023/11/22 03:26:29 - INFO - root -   Epoch: [237/300][180/200], lr: 0.00000069 	 loss = 0.2487(0.2785)
2023/11/22 03:26:36 - INFO - root -   Epoch: [237/300] 	 loss = 0.2865
2023/11/22 03:26:36 - INFO - root -   train_accuracy = 0.9000
2023/11/22 03:26:45 - INFO - root -   Epoch: [238/300][0/200], lr: 0.00000069 	 loss = 0.7706(0.7706)
2023/11/22 03:26:59 - INFO - root -   Epoch: [238/300][20/200], lr: 0.00000069 	 loss = 1.2463(0.3904)
2023/11/22 03:27:14 - INFO - root -   Epoch: [238/300][40/200], lr: 0.00000069 	 loss = 0.5097(0.3587)
2023/11/22 03:27:25 - INFO - root -   Epoch: [238/300][60/200], lr: 0.00000069 	 loss = 0.0061(0.3248)
2023/11/22 03:27:42 - INFO - root -   Epoch: [238/300][80/200], lr: 0.00000069 	 loss = 0.1148(0.3099)
2023/11/22 03:27:53 - INFO - root -   Epoch: [238/300][100/200], lr: 0.00000069 	 loss = 0.0895(0.3020)
2023/11/22 03:28:02 - INFO - root -   Epoch: [238/300][120/200], lr: 0.00000069 	 loss = 0.3417(0.2954)
2023/11/22 03:28:15 - INFO - root -   Epoch: [238/300][140/200], lr: 0.00000069 	 loss = 0.0893(0.2867)
2023/11/22 03:28:37 - INFO - root -   Epoch: [238/300][160/200], lr: 0.00000069 	 loss = 0.2086(0.2952)
2023/11/22 03:29:01 - INFO - root -   Epoch: [238/300][180/200], lr: 0.00000069 	 loss = 0.0491(0.2877)
2023/11/22 03:29:12 - INFO - root -   Epoch: [238/300] 	 loss = 0.2955
2023/11/22 03:29:12 - INFO - root -   train_accuracy = 0.8800
2023/11/22 03:29:20 - INFO - root -   Epoch: [239/300][0/200], lr: 0.00000069 	 loss = 0.3510(0.3510)
2023/11/22 03:29:34 - INFO - root -   Epoch: [239/300][20/200], lr: 0.00000069 	 loss = 1.1283(0.2951)
2023/11/22 03:29:47 - INFO - root -   Epoch: [239/300][40/200], lr: 0.00000069 	 loss = 0.7883(0.3039)
2023/11/22 03:30:00 - INFO - root -   Epoch: [239/300][60/200], lr: 0.00000069 	 loss = 0.0009(0.2671)
2023/11/22 03:30:15 - INFO - root -   Epoch: [239/300][80/200], lr: 0.00000069 	 loss = 0.2252(0.2694)
2023/11/22 03:30:30 - INFO - root -   Epoch: [239/300][100/200], lr: 0.00000069 	 loss = 0.0034(0.2546)
2023/11/22 03:30:40 - INFO - root -   Epoch: [239/300][120/200], lr: 0.00000069 	 loss = 0.0386(0.2348)
2023/11/22 03:30:52 - INFO - root -   Epoch: [239/300][140/200], lr: 0.00000069 	 loss = 0.0122(0.2167)
2023/11/22 03:31:12 - INFO - root -   Epoch: [239/300][160/200], lr: 0.00000069 	 loss = 0.3806(0.2325)
2023/11/22 03:31:24 - INFO - root -   Epoch: [239/300][180/200], lr: 0.00000069 	 loss = 0.0005(0.2378)
2023/11/22 03:31:33 - INFO - root -   Epoch: [239/300] 	 loss = 0.2404
2023/11/22 03:33:33 - INFO - root -   precision = 0.8614
2023/11/22 03:33:33 - INFO - root -   eval_loss = 0.4162
2023/11/22 03:33:33 - INFO - root -   eval_acc = 0.8614
2023/11/22 03:33:34 - INFO - root -   train_accuracy = 0.9100
2023/11/22 03:33:36 - INFO - root -   Epoch: [240/300][0/200], lr: 0.00000069 	 loss = 0.1026(0.1026)
2023/11/22 03:34:04 - INFO - root -   Epoch: [240/300][20/200], lr: 0.00000069 	 loss = 2.0303(0.3900)
2023/11/22 03:34:18 - INFO - root -   Epoch: [240/300][40/200], lr: 0.00000069 	 loss = 1.4749(0.3253)
2023/11/22 03:34:29 - INFO - root -   Epoch: [240/300][60/200], lr: 0.00000069 	 loss = 0.0031(0.3183)
2023/11/22 03:34:50 - INFO - root -   Epoch: [240/300][80/200], lr: 0.00000069 	 loss = 0.2672(0.3001)
2023/11/22 03:35:01 - INFO - root -   Epoch: [240/300][100/200], lr: 0.00000069 	 loss = 0.0007(0.2994)
2023/11/22 03:35:21 - INFO - root -   Epoch: [240/300][120/200], lr: 0.00000069 	 loss = 0.1747(0.2834)
2023/11/22 03:35:34 - INFO - root -   Epoch: [240/300][140/200], lr: 0.00000069 	 loss = 0.0698(0.2714)
2023/11/22 03:36:01 - INFO - root -   Epoch: [240/300][160/200], lr: 0.00000069 	 loss = 0.6306(0.2728)
2023/11/22 03:36:17 - INFO - root -   Epoch: [240/300][180/200], lr: 0.00000069 	 loss = 0.0005(0.2699)
2023/11/22 03:36:25 - INFO - root -   Epoch: [240/300] 	 loss = 0.2624
2023/11/22 03:36:25 - INFO - root -   train_accuracy = 0.9000
2023/11/22 03:36:26 - INFO - root -   Epoch: [241/300][0/200], lr: 0.00000070 	 loss = 0.4232(0.4232)
2023/11/22 03:36:50 - INFO - root -   Epoch: [241/300][20/200], lr: 0.00000070 	 loss = 0.3382(0.2651)
2023/11/22 03:37:03 - INFO - root -   Epoch: [241/300][40/200], lr: 0.00000070 	 loss = 0.2321(0.2701)
2023/11/22 03:37:17 - INFO - root -   Epoch: [241/300][60/200], lr: 0.00000070 	 loss = 0.0012(0.2497)
2023/11/22 03:37:35 - INFO - root -   Epoch: [241/300][80/200], lr: 0.00000070 	 loss = 0.1748(0.2761)
2023/11/22 03:37:45 - INFO - root -   Epoch: [241/300][100/200], lr: 0.00000070 	 loss = 0.0006(0.2719)
2023/11/22 03:38:01 - INFO - root -   Epoch: [241/300][120/200], lr: 0.00000070 	 loss = 0.1232(0.2666)
2023/11/22 03:38:18 - INFO - root -   Epoch: [241/300][140/200], lr: 0.00000070 	 loss = 0.2689(0.2765)
2023/11/22 03:38:31 - INFO - root -   Epoch: [241/300][160/200], lr: 0.00000070 	 loss = 1.2180(0.2883)
2023/11/22 03:38:48 - INFO - root -   Epoch: [241/300][180/200], lr: 0.00000070 	 loss = 0.2699(0.2968)
2023/11/22 03:39:01 - INFO - root -   Epoch: [241/300] 	 loss = 0.2900
2023/11/22 03:39:01 - INFO - root -   train_accuracy = 0.8975
2023/11/22 03:39:14 - INFO - root -   Epoch: [242/300][0/200], lr: 0.00000070 	 loss = 1.4258(1.4258)
2023/11/22 03:39:27 - INFO - root -   Epoch: [242/300][20/200], lr: 0.00000070 	 loss = 1.7690(0.4012)
2023/11/22 03:39:40 - INFO - root -   Epoch: [242/300][40/200], lr: 0.00000070 	 loss = 0.4540(0.3657)
2023/11/22 03:40:03 - INFO - root -   Epoch: [242/300][60/200], lr: 0.00000070 	 loss = 0.0497(0.3212)
2023/11/22 03:40:18 - INFO - root -   Epoch: [242/300][80/200], lr: 0.00000070 	 loss = 0.5707(0.3281)
2023/11/22 03:40:31 - INFO - root -   Epoch: [242/300][100/200], lr: 0.00000070 	 loss = 0.0004(0.3143)
2023/11/22 03:40:47 - INFO - root -   Epoch: [242/300][120/200], lr: 0.00000070 	 loss = 0.4223(0.3184)
2023/11/22 03:41:06 - INFO - root -   Epoch: [242/300][140/200], lr: 0.00000070 	 loss = 0.0042(0.3003)
2023/11/22 03:41:29 - INFO - root -   Epoch: [242/300][160/200], lr: 0.00000070 	 loss = 0.3523(0.3251)
2023/11/22 03:41:48 - INFO - root -   Epoch: [242/300][180/200], lr: 0.00000070 	 loss = 0.0175(0.3213)
2023/11/22 03:42:01 - INFO - root -   Epoch: [242/300] 	 loss = 0.3311
2023/11/22 03:42:01 - INFO - root -   train_accuracy = 0.8925
2023/11/22 03:42:03 - INFO - root -   Epoch: [243/300][0/200], lr: 0.00000070 	 loss = 1.2555(1.2555)
2023/11/22 03:42:27 - INFO - root -   Epoch: [243/300][20/200], lr: 0.00000070 	 loss = 0.3675(0.2025)
2023/11/22 03:42:46 - INFO - root -   Epoch: [243/300][40/200], lr: 0.00000070 	 loss = 0.3989(0.2487)
2023/11/22 03:42:56 - INFO - root -   Epoch: [243/300][60/200], lr: 0.00000070 	 loss = 0.0730(0.2488)
2023/11/22 03:43:08 - INFO - root -   Epoch: [243/300][80/200], lr: 0.00000070 	 loss = 2.0628(0.2842)
2023/11/22 03:43:25 - INFO - root -   Epoch: [243/300][100/200], lr: 0.00000070 	 loss = 0.0062(0.2700)
2023/11/22 03:43:50 - INFO - root -   Epoch: [243/300][120/200], lr: 0.00000070 	 loss = 0.0410(0.2562)
2023/11/22 03:44:09 - INFO - root -   Epoch: [243/300][140/200], lr: 0.00000070 	 loss = 0.3717(0.2357)
2023/11/22 03:44:29 - INFO - root -   Epoch: [243/300][160/200], lr: 0.00000070 	 loss = 0.3420(0.2473)
2023/11/22 03:44:37 - INFO - root -   Epoch: [243/300][180/200], lr: 0.00000070 	 loss = 0.0853(0.2565)
2023/11/22 03:44:49 - INFO - root -   Epoch: [243/300] 	 loss = 0.2819
2023/11/22 03:44:49 - INFO - root -   train_accuracy = 0.9000
2023/11/22 03:44:57 - INFO - root -   Epoch: [244/300][0/200], lr: 0.00000070 	 loss = 1.5026(1.5026)
2023/11/22 03:45:15 - INFO - root -   Epoch: [244/300][20/200], lr: 0.00000070 	 loss = 0.3561(0.2341)
2023/11/22 03:45:33 - INFO - root -   Epoch: [244/300][40/200], lr: 0.00000070 	 loss = 0.3873(0.2508)
2023/11/22 03:45:54 - INFO - root -   Epoch: [244/300][60/200], lr: 0.00000070 	 loss = 0.0520(0.2572)
2023/11/22 03:46:07 - INFO - root -   Epoch: [244/300][80/200], lr: 0.00000070 	 loss = 0.1935(0.2734)
2023/11/22 03:46:34 - INFO - root -   Epoch: [244/300][100/200], lr: 0.00000070 	 loss = 0.0137(0.2927)
2023/11/22 03:46:50 - INFO - root -   Epoch: [244/300][120/200], lr: 0.00000070 	 loss = 1.3087(0.2957)
2023/11/22 03:47:04 - INFO - root -   Epoch: [244/300][140/200], lr: 0.00000070 	 loss = 0.2622(0.2734)
2023/11/22 03:47:18 - INFO - root -   Epoch: [244/300][160/200], lr: 0.00000070 	 loss = 0.7137(0.3050)
2023/11/22 03:47:34 - INFO - root -   Epoch: [244/300][180/200], lr: 0.00000070 	 loss = 0.0039(0.2865)
2023/11/22 03:47:43 - INFO - root -   Epoch: [244/300] 	 loss = 0.2854
2023/11/22 03:49:44 - INFO - root -   precision = 0.8614
2023/11/22 03:49:44 - INFO - root -   eval_loss = 0.3991
2023/11/22 03:49:44 - INFO - root -   eval_acc = 0.8614
2023/11/22 03:49:45 - INFO - root -   train_accuracy = 0.9050
2023/11/22 03:49:46 - INFO - root -   Epoch: [245/300][0/200], lr: 0.00000071 	 loss = 0.2762(0.2762)
2023/11/22 03:50:02 - INFO - root -   Epoch: [245/300][20/200], lr: 0.00000071 	 loss = 0.2189(0.1540)
2023/11/22 03:50:16 - INFO - root -   Epoch: [245/300][40/200], lr: 0.00000071 	 loss = 0.6408(0.1864)
2023/11/22 03:50:37 - INFO - root -   Epoch: [245/300][60/200], lr: 0.00000071 	 loss = 0.3726(0.2457)
2023/11/22 03:50:46 - INFO - root -   Epoch: [245/300][80/200], lr: 0.00000071 	 loss = 0.2301(0.2413)
2023/11/22 03:50:56 - INFO - root -   Epoch: [245/300][100/200], lr: 0.00000071 	 loss = 0.0152(0.2311)
2023/11/22 03:51:08 - INFO - root -   Epoch: [245/300][120/200], lr: 0.00000071 	 loss = 0.1848(0.2134)
2023/11/22 03:51:29 - INFO - root -   Epoch: [245/300][140/200], lr: 0.00000071 	 loss = 0.1323(0.2114)
2023/11/22 03:51:45 - INFO - root -   Epoch: [245/300][160/200], lr: 0.00000071 	 loss = 0.4655(0.2341)
2023/11/22 03:52:00 - INFO - root -   Epoch: [245/300][180/200], lr: 0.00000071 	 loss = 0.0069(0.2295)
2023/11/22 03:52:08 - INFO - root -   Epoch: [245/300] 	 loss = 0.2331
2023/11/22 03:52:08 - INFO - root -   train_accuracy = 0.9300
2023/11/22 03:52:17 - INFO - root -   Epoch: [246/300][0/200], lr: 0.00000071 	 loss = 1.7981(1.7981)
2023/11/22 03:52:38 - INFO - root -   Epoch: [246/300][20/200], lr: 0.00000071 	 loss = 1.3266(0.4270)
2023/11/22 03:52:47 - INFO - root -   Epoch: [246/300][40/200], lr: 0.00000071 	 loss = 0.6043(0.3860)
2023/11/22 03:52:56 - INFO - root -   Epoch: [246/300][60/200], lr: 0.00000071 	 loss = 0.1455(0.3313)
2023/11/22 03:53:16 - INFO - root -   Epoch: [246/300][80/200], lr: 0.00000071 	 loss = 0.4851(0.2952)
2023/11/22 03:53:28 - INFO - root -   Epoch: [246/300][100/200], lr: 0.00000071 	 loss = 0.1654(0.2977)
2023/11/22 03:53:42 - INFO - root -   Epoch: [246/300][120/200], lr: 0.00000071 	 loss = 0.8928(0.3172)
2023/11/22 03:53:50 - INFO - root -   Epoch: [246/300][140/200], lr: 0.00000071 	 loss = 0.0443(0.3035)
2023/11/22 03:54:02 - INFO - root -   Epoch: [246/300][160/200], lr: 0.00000071 	 loss = 0.3019(0.2947)
2023/11/22 03:54:15 - INFO - root -   Epoch: [246/300][180/200], lr: 0.00000071 	 loss = 0.3307(0.2836)
2023/11/22 03:54:23 - INFO - root -   Epoch: [246/300] 	 loss = 0.2726
2023/11/22 03:54:23 - INFO - root -   train_accuracy = 0.8975
2023/11/22 03:54:24 - INFO - root -   Epoch: [247/300][0/200], lr: 0.00000071 	 loss = 1.0462(1.0462)
2023/11/22 03:54:47 - INFO - root -   Epoch: [247/300][20/200], lr: 0.00000071 	 loss = 1.6639(0.2921)
2023/11/22 03:54:55 - INFO - root -   Epoch: [247/300][40/200], lr: 0.00000071 	 loss = 0.6301(0.2289)
2023/11/22 03:55:13 - INFO - root -   Epoch: [247/300][60/200], lr: 0.00000071 	 loss = 0.0304(0.2342)
2023/11/22 03:55:29 - INFO - root -   Epoch: [247/300][80/200], lr: 0.00000071 	 loss = 1.6972(0.2445)
2023/11/22 03:55:45 - INFO - root -   Epoch: [247/300][100/200], lr: 0.00000071 	 loss = 0.2084(0.2580)
2023/11/22 03:55:57 - INFO - root -   Epoch: [247/300][120/200], lr: 0.00000071 	 loss = 0.0063(0.2379)
2023/11/22 03:56:05 - INFO - root -   Epoch: [247/300][140/200], lr: 0.00000071 	 loss = 0.1501(0.2421)
2023/11/22 03:56:30 - INFO - root -   Epoch: [247/300][160/200], lr: 0.00000071 	 loss = 1.1474(0.2453)
2023/11/22 03:56:46 - INFO - root -   Epoch: [247/300][180/200], lr: 0.00000071 	 loss = 0.0009(0.2402)
2023/11/22 03:56:53 - INFO - root -   Epoch: [247/300] 	 loss = 0.2386
2023/11/22 03:56:53 - INFO - root -   train_accuracy = 0.9325
2023/11/22 03:57:00 - INFO - root -   Epoch: [248/300][0/200], lr: 0.00000071 	 loss = 0.1447(0.1447)
2023/11/22 03:57:12 - INFO - root -   Epoch: [248/300][20/200], lr: 0.00000071 	 loss = 0.7007(0.2089)
2023/11/22 03:57:29 - INFO - root -   Epoch: [248/300][40/200], lr: 0.00000071 	 loss = 1.2239(0.2688)
2023/11/22 03:57:38 - INFO - root -   Epoch: [248/300][60/200], lr: 0.00000071 	 loss = 0.0162(0.2662)
2023/11/22 03:57:56 - INFO - root -   Epoch: [248/300][80/200], lr: 0.00000071 	 loss = 0.0722(0.2693)
2023/11/22 03:58:10 - INFO - root -   Epoch: [248/300][100/200], lr: 0.00000071 	 loss = 0.0019(0.2587)
2023/11/22 03:58:24 - INFO - root -   Epoch: [248/300][120/200], lr: 0.00000071 	 loss = 0.1832(0.2576)
2023/11/22 03:58:35 - INFO - root -   Epoch: [248/300][140/200], lr: 0.00000071 	 loss = 0.0117(0.2344)
2023/11/22 03:58:56 - INFO - root -   Epoch: [248/300][160/200], lr: 0.00000071 	 loss = 0.1779(0.2450)
2023/11/22 03:59:08 - INFO - root -   Epoch: [248/300][180/200], lr: 0.00000071 	 loss = 0.1112(0.2361)
2023/11/22 03:59:16 - INFO - root -   Epoch: [248/300] 	 loss = 0.2438
2023/11/22 03:59:16 - INFO - root -   train_accuracy = 0.9200
2023/11/22 03:59:27 - INFO - root -   Epoch: [249/300][0/200], lr: 0.00000072 	 loss = 1.9251(1.9251)
2023/11/22 03:59:35 - INFO - root -   Epoch: [249/300][20/200], lr: 0.00000072 	 loss = 0.6410(0.3054)
2023/11/22 03:59:48 - INFO - root -   Epoch: [249/300][40/200], lr: 0.00000072 	 loss = 0.3282(0.3232)
2023/11/22 04:00:06 - INFO - root -   Epoch: [249/300][60/200], lr: 0.00000072 	 loss = 0.1990(0.3281)
2023/11/22 04:00:21 - INFO - root -   Epoch: [249/300][80/200], lr: 0.00000072 	 loss = 0.7536(0.3480)
2023/11/22 04:00:39 - INFO - root -   Epoch: [249/300][100/200], lr: 0.00000072 	 loss = 0.0206(0.3420)
2023/11/22 04:00:53 - INFO - root -   Epoch: [249/300][120/200], lr: 0.00000072 	 loss = 1.6860(0.3430)
2023/11/22 04:01:10 - INFO - root -   Epoch: [249/300][140/200], lr: 0.00000072 	 loss = 0.0294(0.3267)
2023/11/22 04:01:31 - INFO - root -   Epoch: [249/300][160/200], lr: 0.00000072 	 loss = 0.3083(0.3301)
2023/11/22 04:01:43 - INFO - root -   Epoch: [249/300][180/200], lr: 0.00000072 	 loss = 0.0020(0.3424)
2023/11/22 04:01:56 - INFO - root -   Epoch: [249/300] 	 loss = 0.3299
2023/11/22 04:03:57 - INFO - root -   precision = 0.8515
2023/11/22 04:03:57 - INFO - root -   eval_loss = 0.3868
2023/11/22 04:03:57 - INFO - root -   eval_acc = 0.8515
2023/11/22 04:03:58 - INFO - root -   train_accuracy = 0.8825
2023/11/22 04:03:59 - INFO - root -   Epoch: [250/300][0/200], lr: 0.00000072 	 loss = 1.1539(1.1539)
2023/11/22 04:04:16 - INFO - root -   Epoch: [250/300][20/200], lr: 0.00000072 	 loss = 0.1437(0.2402)
2023/11/22 04:04:31 - INFO - root -   Epoch: [250/300][40/200], lr: 0.00000072 	 loss = 1.5672(0.2277)
2023/11/22 04:04:42 - INFO - root -   Epoch: [250/300][60/200], lr: 0.00000072 	 loss = 0.0019(0.2372)
2023/11/22 04:05:01 - INFO - root -   Epoch: [250/300][80/200], lr: 0.00000072 	 loss = 1.2136(0.2733)
2023/11/22 04:05:18 - INFO - root -   Epoch: [250/300][100/200], lr: 0.00000072 	 loss = 0.0010(0.2571)
2023/11/22 04:05:27 - INFO - root -   Epoch: [250/300][120/200], lr: 0.00000072 	 loss = 0.6324(0.2524)
2023/11/22 04:05:44 - INFO - root -   Epoch: [250/300][140/200], lr: 0.00000072 	 loss = 0.1244(0.2284)
2023/11/22 04:05:59 - INFO - root -   Epoch: [250/300][160/200], lr: 0.00000072 	 loss = 0.1943(0.2350)
2023/11/22 04:06:22 - INFO - root -   Epoch: [250/300][180/200], lr: 0.00000072 	 loss = 0.0443(0.2505)
2023/11/22 04:06:30 - INFO - root -   Epoch: [250/300] 	 loss = 0.2494
2023/11/22 04:06:30 - INFO - root -   train_accuracy = 0.9125
2023/11/22 04:06:31 - INFO - root -   Epoch: [251/300][0/200], lr: 0.00000072 	 loss = 0.1951(0.1951)
2023/11/22 04:06:48 - INFO - root -   Epoch: [251/300][20/200], lr: 0.00000072 	 loss = 0.7712(0.3603)
2023/11/22 04:06:56 - INFO - root -   Epoch: [251/300][40/200], lr: 0.00000072 	 loss = 0.1748(0.2349)
2023/11/22 04:07:07 - INFO - root -   Epoch: [251/300][60/200], lr: 0.00000072 	 loss = 0.0149(0.2069)
2023/11/22 04:07:21 - INFO - root -   Epoch: [251/300][80/200], lr: 0.00000072 	 loss = 1.0154(0.2415)
2023/11/22 04:07:33 - INFO - root -   Epoch: [251/300][100/200], lr: 0.00000072 	 loss = 0.0028(0.2273)
2023/11/22 04:07:44 - INFO - root -   Epoch: [251/300][120/200], lr: 0.00000072 	 loss = 0.4587(0.2410)
2023/11/22 04:08:02 - INFO - root -   Epoch: [251/300][140/200], lr: 0.00000072 	 loss = 0.0046(0.2304)
2023/11/22 04:08:21 - INFO - root -   Epoch: [251/300][160/200], lr: 0.00000072 	 loss = 1.1585(0.2723)
2023/11/22 04:08:33 - INFO - root -   Epoch: [251/300][180/200], lr: 0.00000072 	 loss = 0.1051(0.2641)
2023/11/22 04:08:43 - INFO - root -   Epoch: [251/300] 	 loss = 0.2726
2023/11/22 04:08:43 - INFO - root -   train_accuracy = 0.9200
2023/11/22 04:08:51 - INFO - root -   Epoch: [252/300][0/200], lr: 0.00000072 	 loss = 0.2954(0.2954)
2023/11/22 04:09:08 - INFO - root -   Epoch: [252/300][20/200], lr: 0.00000072 	 loss = 1.3325(0.4104)
2023/11/22 04:09:19 - INFO - root -   Epoch: [252/300][40/200], lr: 0.00000072 	 loss = 0.1351(0.2794)
2023/11/22 04:09:37 - INFO - root -   Epoch: [252/300][60/200], lr: 0.00000072 	 loss = 0.1424(0.2841)
2023/11/22 04:09:58 - INFO - root -   Epoch: [252/300][80/200], lr: 0.00000072 	 loss = 2.4843(0.3258)
2023/11/22 04:10:05 - INFO - root -   Epoch: [252/300][100/200], lr: 0.00000072 	 loss = 0.0008(0.3062)
2023/11/22 04:10:19 - INFO - root -   Epoch: [252/300][120/200], lr: 0.00000072 	 loss = 0.0409(0.2872)
2023/11/22 04:10:28 - INFO - root -   Epoch: [252/300][140/200], lr: 0.00000072 	 loss = 0.0160(0.2582)
2023/11/22 04:10:48 - INFO - root -   Epoch: [252/300][160/200], lr: 0.00000072 	 loss = 1.3436(0.2670)
2023/11/22 04:11:06 - INFO - root -   Epoch: [252/300][180/200], lr: 0.00000072 	 loss = 0.1265(0.2850)
2023/11/22 04:11:13 - INFO - root -   Epoch: [252/300] 	 loss = 0.2811
2023/11/22 04:11:13 - INFO - root -   train_accuracy = 0.9050
2023/11/22 04:11:24 - INFO - root -   Epoch: [253/300][0/200], lr: 0.00000073 	 loss = 1.6178(1.6178)
2023/11/22 04:11:35 - INFO - root -   Epoch: [253/300][20/200], lr: 0.00000073 	 loss = 0.4188(0.3973)
2023/11/22 04:11:46 - INFO - root -   Epoch: [253/300][40/200], lr: 0.00000073 	 loss = 1.5313(0.4105)
2023/11/22 04:12:02 - INFO - root -   Epoch: [253/300][60/200], lr: 0.00000073 	 loss = 0.0014(0.3662)
2023/11/22 04:12:13 - INFO - root -   Epoch: [253/300][80/200], lr: 0.00000073 	 loss = 0.8066(0.3398)
2023/11/22 04:12:24 - INFO - root -   Epoch: [253/300][100/200], lr: 0.00000073 	 loss = 0.0061(0.3156)
2023/11/22 04:12:40 - INFO - root -   Epoch: [253/300][120/200], lr: 0.00000073 	 loss = 0.2891(0.3267)
2023/11/22 04:12:57 - INFO - root -   Epoch: [253/300][140/200], lr: 0.00000073 	 loss = 0.0256(0.2982)
2023/11/22 04:13:16 - INFO - root -   Epoch: [253/300][160/200], lr: 0.00000073 	 loss = 0.8971(0.3254)
2023/11/22 04:13:29 - INFO - root -   Epoch: [253/300][180/200], lr: 0.00000073 	 loss = 0.0049(0.3133)
2023/11/22 04:13:37 - INFO - root -   Epoch: [253/300] 	 loss = 0.2978
2023/11/22 04:13:37 - INFO - root -   train_accuracy = 0.9075
2023/11/22 04:13:38 - INFO - root -   Epoch: [254/300][0/200], lr: 0.00000073 	 loss = 0.1797(0.1797)
2023/11/22 04:14:04 - INFO - root -   Epoch: [254/300][20/200], lr: 0.00000073 	 loss = 1.0580(0.4123)
2023/11/22 04:14:12 - INFO - root -   Epoch: [254/300][40/200], lr: 0.00000073 	 loss = 0.4928(0.4303)
2023/11/22 04:14:26 - INFO - root -   Epoch: [254/300][60/200], lr: 0.00000073 	 loss = 0.3036(0.3763)
2023/11/22 04:14:46 - INFO - root -   Epoch: [254/300][80/200], lr: 0.00000073 	 loss = 0.1312(0.3473)
2023/11/22 04:15:01 - INFO - root -   Epoch: [254/300][100/200], lr: 0.00000073 	 loss = 0.0025(0.3178)
2023/11/22 04:15:17 - INFO - root -   Epoch: [254/300][120/200], lr: 0.00000073 	 loss = 0.6534(0.3270)
2023/11/22 04:15:29 - INFO - root -   Epoch: [254/300][140/200], lr: 0.00000073 	 loss = 0.0290(0.3092)
2023/11/22 04:15:48 - INFO - root -   Epoch: [254/300][160/200], lr: 0.00000073 	 loss = 0.6145(0.3046)
2023/11/22 04:16:07 - INFO - root -   Epoch: [254/300][180/200], lr: 0.00000073 	 loss = 0.0558(0.3029)
2023/11/22 04:16:14 - INFO - root -   Epoch: [254/300] 	 loss = 0.3118
2023/11/22 04:18:17 - INFO - root -   precision = 0.8416
2023/11/22 04:18:17 - INFO - root -   eval_loss = 0.4553
2023/11/22 04:18:17 - INFO - root -   eval_acc = 0.8416
2023/11/22 04:18:18 - INFO - root -   train_accuracy = 0.8925
2023/11/22 04:18:26 - INFO - root -   Epoch: [255/300][0/200], lr: 0.00000073 	 loss = 0.7573(0.7573)
2023/11/22 04:18:37 - INFO - root -   Epoch: [255/300][20/200], lr: 0.00000073 	 loss = 0.2104(0.2962)
2023/11/22 04:18:56 - INFO - root -   Epoch: [255/300][40/200], lr: 0.00000073 	 loss = 0.4909(0.2706)
2023/11/22 04:19:06 - INFO - root -   Epoch: [255/300][60/200], lr: 0.00000073 	 loss = 0.0821(0.2714)
2023/11/22 04:19:20 - INFO - root -   Epoch: [255/300][80/200], lr: 0.00000073 	 loss = 0.1108(0.2510)
2023/11/22 04:19:33 - INFO - root -   Epoch: [255/300][100/200], lr: 0.00000073 	 loss = 0.0187(0.2659)
2023/11/22 04:19:41 - INFO - root -   Epoch: [255/300][120/200], lr: 0.00000073 	 loss = 0.2690(0.2571)
2023/11/22 04:19:55 - INFO - root -   Epoch: [255/300][140/200], lr: 0.00000073 	 loss = 0.0011(0.2424)
2023/11/22 04:20:10 - INFO - root -   Epoch: [255/300][160/200], lr: 0.00000073 	 loss = 1.6851(0.2687)
2023/11/22 04:20:19 - INFO - root -   Epoch: [255/300][180/200], lr: 0.00000073 	 loss = 0.9344(0.2685)
2023/11/22 04:20:27 - INFO - root -   Epoch: [255/300] 	 loss = 0.2610
2023/11/22 04:20:27 - INFO - root -   train_accuracy = 0.9175
2023/11/22 04:20:28 - INFO - root -   Epoch: [256/300][0/200], lr: 0.00000073 	 loss = 0.2328(0.2328)
2023/11/22 04:20:43 - INFO - root -   Epoch: [256/300][20/200], lr: 0.00000073 	 loss = 0.4003(0.2651)
2023/11/22 04:20:59 - INFO - root -   Epoch: [256/300][40/200], lr: 0.00000073 	 loss = 0.3887(0.2814)
2023/11/22 04:21:19 - INFO - root -   Epoch: [256/300][60/200], lr: 0.00000073 	 loss = 0.3123(0.2713)
2023/11/22 04:21:37 - INFO - root -   Epoch: [256/300][80/200], lr: 0.00000073 	 loss = 1.0104(0.3059)
2023/11/22 04:21:51 - INFO - root -   Epoch: [256/300][100/200], lr: 0.00000073 	 loss = 0.0012(0.3170)
2023/11/22 04:22:09 - INFO - root -   Epoch: [256/300][120/200], lr: 0.00000073 	 loss = 0.8087(0.3341)
2023/11/22 04:22:26 - INFO - root -   Epoch: [256/300][140/200], lr: 0.00000073 	 loss = 0.0097(0.3300)
2023/11/22 04:22:39 - INFO - root -   Epoch: [256/300][160/200], lr: 0.00000073 	 loss = 0.9578(0.3265)
2023/11/22 04:22:51 - INFO - root -   Epoch: [256/300][180/200], lr: 0.00000073 	 loss = 0.0010(0.3188)
2023/11/22 04:23:00 - INFO - root -   Epoch: [256/300] 	 loss = 0.3182
2023/11/22 04:23:00 - INFO - root -   train_accuracy = 0.8900
2023/11/22 04:23:01 - INFO - root -   Epoch: [257/300][0/200], lr: 0.00000074 	 loss = 0.4275(0.4275)
2023/11/22 04:23:15 - INFO - root -   Epoch: [257/300][20/200], lr: 0.00000074 	 loss = 1.7075(0.3320)
2023/11/22 04:23:28 - INFO - root -   Epoch: [257/300][40/200], lr: 0.00000074 	 loss = 1.5796(0.3763)
2023/11/22 04:23:41 - INFO - root -   Epoch: [257/300][60/200], lr: 0.00000074 	 loss = 0.0012(0.3532)
2023/11/22 04:24:00 - INFO - root -   Epoch: [257/300][80/200], lr: 0.00000074 	 loss = 0.9260(0.3453)
2023/11/22 04:24:15 - INFO - root -   Epoch: [257/300][100/200], lr: 0.00000074 	 loss = 0.0340(0.3196)
2023/11/22 04:24:28 - INFO - root -   Epoch: [257/300][120/200], lr: 0.00000074 	 loss = 0.3930(0.3126)
2023/11/22 04:24:36 - INFO - root -   Epoch: [257/300][140/200], lr: 0.00000074 	 loss = 0.3829(0.3067)
2023/11/22 04:24:47 - INFO - root -   Epoch: [257/300][160/200], lr: 0.00000074 	 loss = 1.0288(0.3094)
2023/11/22 04:25:13 - INFO - root -   Epoch: [257/300][180/200], lr: 0.00000074 	 loss = 0.1606(0.3098)
2023/11/22 04:25:20 - INFO - root -   Epoch: [257/300] 	 loss = 0.3019
2023/11/22 04:25:20 - INFO - root -   train_accuracy = 0.9050
2023/11/22 04:25:27 - INFO - root -   Epoch: [258/300][0/200], lr: 0.00000074 	 loss = 0.6021(0.6021)
2023/11/22 04:25:46 - INFO - root -   Epoch: [258/300][20/200], lr: 0.00000074 	 loss = 1.0376(0.3113)
2023/11/22 04:26:03 - INFO - root -   Epoch: [258/300][40/200], lr: 0.00000074 	 loss = 0.0770(0.2355)
2023/11/22 04:26:17 - INFO - root -   Epoch: [258/300][60/200], lr: 0.00000074 	 loss = 0.0665(0.2539)
2023/11/22 04:26:28 - INFO - root -   Epoch: [258/300][80/200], lr: 0.00000074 	 loss = 0.0439(0.2530)
2023/11/22 04:26:44 - INFO - root -   Epoch: [258/300][100/200], lr: 0.00000074 	 loss = 0.0008(0.2560)
2023/11/22 04:26:58 - INFO - root -   Epoch: [258/300][120/200], lr: 0.00000074 	 loss = 0.4341(0.2522)
2023/11/22 04:27:09 - INFO - root -   Epoch: [258/300][140/200], lr: 0.00000074 	 loss = 0.0024(0.2442)
2023/11/22 04:27:37 - INFO - root -   Epoch: [258/300][160/200], lr: 0.00000074 	 loss = 1.2020(0.2626)
2023/11/22 04:27:45 - INFO - root -   Epoch: [258/300][180/200], lr: 0.00000074 	 loss = 0.1654(0.2777)
2023/11/22 04:27:58 - INFO - root -   Epoch: [258/300] 	 loss = 0.2721
2023/11/22 04:27:58 - INFO - root -   train_accuracy = 0.8950
2023/11/22 04:27:59 - INFO - root -   Epoch: [259/300][0/200], lr: 0.00000074 	 loss = 0.2930(0.2930)
2023/11/22 04:28:15 - INFO - root -   Epoch: [259/300][20/200], lr: 0.00000074 	 loss = 0.3128(0.1819)
2023/11/22 04:28:31 - INFO - root -   Epoch: [259/300][40/200], lr: 0.00000074 	 loss = 0.5550(0.2646)
2023/11/22 04:28:44 - INFO - root -   Epoch: [259/300][60/200], lr: 0.00000074 	 loss = 0.2173(0.2634)
2023/11/22 04:28:55 - INFO - root -   Epoch: [259/300][80/200], lr: 0.00000074 	 loss = 1.1774(0.2946)
2023/11/22 04:29:10 - INFO - root -   Epoch: [259/300][100/200], lr: 0.00000074 	 loss = 0.0377(0.2807)
2023/11/22 04:29:24 - INFO - root -   Epoch: [259/300][120/200], lr: 0.00000074 	 loss = 0.1867(0.2899)
2023/11/22 04:29:36 - INFO - root -   Epoch: [259/300][140/200], lr: 0.00000074 	 loss = 0.0083(0.2743)
2023/11/22 04:29:58 - INFO - root -   Epoch: [259/300][160/200], lr: 0.00000074 	 loss = 1.0406(0.2682)
2023/11/22 04:30:11 - INFO - root -   Epoch: [259/300][180/200], lr: 0.00000074 	 loss = 0.0004(0.2593)
2023/11/22 04:30:21 - INFO - root -   Epoch: [259/300] 	 loss = 0.2598
2023/11/22 04:32:21 - INFO - root -   precision = 0.8515
2023/11/22 04:32:21 - INFO - root -   eval_loss = 0.4239
2023/11/22 04:32:21 - INFO - root -   eval_acc = 0.8515
2023/11/22 04:32:22 - INFO - root -   train_accuracy = 0.9150
2023/11/22 04:32:23 - INFO - root -   Epoch: [260/300][0/200], lr: 0.00000074 	 loss = 0.8037(0.8037)
2023/11/22 04:32:45 - INFO - root -   Epoch: [260/300][20/200], lr: 0.00000074 	 loss = 0.5879(0.3272)
2023/11/22 04:32:57 - INFO - root -   Epoch: [260/300][40/200], lr: 0.00000074 	 loss = 0.1170(0.2478)
2023/11/22 04:33:15 - INFO - root -   Epoch: [260/300][60/200], lr: 0.00000074 	 loss = 0.1147(0.2750)
2023/11/22 04:33:26 - INFO - root -   Epoch: [260/300][80/200], lr: 0.00000074 	 loss = 0.0076(0.2668)
2023/11/22 04:33:44 - INFO - root -   Epoch: [260/300][100/200], lr: 0.00000074 	 loss = 0.0699(0.2775)
2023/11/22 04:33:57 - INFO - root -   Epoch: [260/300][120/200], lr: 0.00000074 	 loss = 0.5719(0.2896)
2023/11/22 04:34:12 - INFO - root -   Epoch: [260/300][140/200], lr: 0.00000074 	 loss = 0.0061(0.2692)
2023/11/22 04:34:33 - INFO - root -   Epoch: [260/300][160/200], lr: 0.00000074 	 loss = 0.1631(0.2705)
2023/11/22 04:34:45 - INFO - root -   Epoch: [260/300][180/200], lr: 0.00000074 	 loss = 0.0006(0.2542)
2023/11/22 04:34:53 - INFO - root -   Epoch: [260/300] 	 loss = 0.2581
2023/11/22 04:34:53 - INFO - root -   train_accuracy = 0.9050
2023/11/22 04:34:54 - INFO - root -   Epoch: [261/300][0/200], lr: 0.00000075 	 loss = 0.2499(0.2499)
2023/11/22 04:35:16 - INFO - root -   Epoch: [261/300][20/200], lr: 0.00000075 	 loss = 0.7943(0.1899)
2023/11/22 04:35:26 - INFO - root -   Epoch: [261/300][40/200], lr: 0.00000075 	 loss = 0.0669(0.2084)
2023/11/22 04:35:39 - INFO - root -   Epoch: [261/300][60/200], lr: 0.00000075 	 loss = 0.0266(0.2015)
2023/11/22 04:35:53 - INFO - root -   Epoch: [261/300][80/200], lr: 0.00000075 	 loss = 0.0304(0.2290)
2023/11/22 04:36:02 - INFO - root -   Epoch: [261/300][100/200], lr: 0.00000075 	 loss = 0.0005(0.2319)
2023/11/22 04:36:17 - INFO - root -   Epoch: [261/300][120/200], lr: 0.00000075 	 loss = 0.1133(0.2402)
2023/11/22 04:36:29 - INFO - root -   Epoch: [261/300][140/200], lr: 0.00000075 	 loss = 0.1690(0.2468)
2023/11/22 04:36:50 - INFO - root -   Epoch: [261/300][160/200], lr: 0.00000075 	 loss = 0.1137(0.2751)
2023/11/22 04:37:02 - INFO - root -   Epoch: [261/300][180/200], lr: 0.00000075 	 loss = 0.0019(0.2852)
2023/11/22 04:37:14 - INFO - root -   Epoch: [261/300] 	 loss = 0.2909
2023/11/22 04:37:14 - INFO - root -   train_accuracy = 0.9050
2023/11/22 04:37:15 - INFO - root -   Epoch: [262/300][0/200], lr: 0.00000075 	 loss = 0.5019(0.5019)
2023/11/22 04:37:31 - INFO - root -   Epoch: [262/300][20/200], lr: 0.00000075 	 loss = 0.7774(0.2386)
2023/11/22 04:37:43 - INFO - root -   Epoch: [262/300][40/200], lr: 0.00000075 	 loss = 1.9337(0.2291)
2023/11/22 04:37:58 - INFO - root -   Epoch: [262/300][60/200], lr: 0.00000075 	 loss = 0.0630(0.2066)
2023/11/22 04:38:14 - INFO - root -   Epoch: [262/300][80/200], lr: 0.00000075 	 loss = 2.5027(0.2884)
2023/11/22 04:38:30 - INFO - root -   Epoch: [262/300][100/200], lr: 0.00000075 	 loss = 0.3533(0.2617)
2023/11/22 04:38:38 - INFO - root -   Epoch: [262/300][120/200], lr: 0.00000075 	 loss = 1.4225(0.2760)
2023/11/22 04:39:00 - INFO - root -   Epoch: [262/300][140/200], lr: 0.00000075 	 loss = 0.0907(0.2728)
2023/11/22 04:39:17 - INFO - root -   Epoch: [262/300][160/200], lr: 0.00000075 	 loss = 0.6821(0.2774)
2023/11/22 04:39:33 - INFO - root -   Epoch: [262/300][180/200], lr: 0.00000075 	 loss = 0.0013(0.2821)
2023/11/22 04:39:43 - INFO - root -   Epoch: [262/300] 	 loss = 0.2699
2023/11/22 04:39:43 - INFO - root -   train_accuracy = 0.9000
2023/11/22 04:39:51 - INFO - root -   Epoch: [263/300][0/200], lr: 0.00000075 	 loss = 0.3001(0.3001)
2023/11/22 04:40:05 - INFO - root -   Epoch: [263/300][20/200], lr: 0.00000075 	 loss = 0.6956(0.3187)
2023/11/22 04:40:15 - INFO - root -   Epoch: [263/300][40/200], lr: 0.00000075 	 loss = 0.5949(0.2513)
2023/11/22 04:40:27 - INFO - root -   Epoch: [263/300][60/200], lr: 0.00000075 	 loss = 0.0010(0.2494)
2023/11/22 04:40:50 - INFO - root -   Epoch: [263/300][80/200], lr: 0.00000075 	 loss = 0.0465(0.2530)
2023/11/22 04:41:02 - INFO - root -   Epoch: [263/300][100/200], lr: 0.00000075 	 loss = 0.0642(0.2894)
2023/11/22 04:41:13 - INFO - root -   Epoch: [263/300][120/200], lr: 0.00000075 	 loss = 1.0079(0.2885)
2023/11/22 04:41:33 - INFO - root -   Epoch: [263/300][140/200], lr: 0.00000075 	 loss = 0.0153(0.2693)
2023/11/22 04:41:47 - INFO - root -   Epoch: [263/300][160/200], lr: 0.00000075 	 loss = 0.0438(0.2935)
2023/11/22 04:42:03 - INFO - root -   Epoch: [263/300][180/200], lr: 0.00000075 	 loss = 0.2080(0.3021)
2023/11/22 04:42:13 - INFO - root -   Epoch: [263/300] 	 loss = 0.2998
2023/11/22 04:42:13 - INFO - root -   train_accuracy = 0.8900
2023/11/22 04:42:14 - INFO - root -   Epoch: [264/300][0/200], lr: 0.00000075 	 loss = 0.1681(0.1681)
2023/11/22 04:42:34 - INFO - root -   Epoch: [264/300][20/200], lr: 0.00000075 	 loss = 0.6673(0.2569)
2023/11/22 04:42:48 - INFO - root -   Epoch: [264/300][40/200], lr: 0.00000075 	 loss = 1.3979(0.3109)
2023/11/22 04:43:01 - INFO - root -   Epoch: [264/300][60/200], lr: 0.00000075 	 loss = 0.0009(0.2754)
2023/11/22 04:43:11 - INFO - root -   Epoch: [264/300][80/200], lr: 0.00000075 	 loss = 0.1310(0.2643)
2023/11/22 04:43:32 - INFO - root -   Epoch: [264/300][100/200], lr: 0.00000075 	 loss = 0.0776(0.2549)
2023/11/22 04:43:46 - INFO - root -   Epoch: [264/300][120/200], lr: 0.00000075 	 loss = 0.1105(0.2432)
2023/11/22 04:44:06 - INFO - root -   Epoch: [264/300][140/200], lr: 0.00000075 	 loss = 0.1779(0.2414)
2023/11/22 04:44:17 - INFO - root -   Epoch: [264/300][160/200], lr: 0.00000075 	 loss = 0.1401(0.2497)
2023/11/22 04:44:30 - INFO - root -   Epoch: [264/300][180/200], lr: 0.00000075 	 loss = 0.1174(0.2399)
2023/11/22 04:44:43 - INFO - root -   Epoch: [264/300] 	 loss = 0.2393
2023/11/22 04:46:44 - INFO - root -   precision = 0.8812
2023/11/22 04:46:44 - INFO - root -   eval_loss = 0.4361
2023/11/22 04:46:44 - INFO - root -   eval_acc = 0.8812
2023/11/22 04:46:45 - INFO - root -   train_accuracy = 0.9150
2023/11/22 04:46:55 - INFO - root -   Epoch: [265/300][0/200], lr: 0.00000076 	 loss = 1.3128(1.3128)
2023/11/22 04:47:09 - INFO - root -   Epoch: [265/300][20/200], lr: 0.00000076 	 loss = 0.8895(0.4327)
2023/11/22 04:47:21 - INFO - root -   Epoch: [265/300][40/200], lr: 0.00000076 	 loss = 0.7073(0.3119)
2023/11/22 04:47:36 - INFO - root -   Epoch: [265/300][60/200], lr: 0.00000076 	 loss = 0.0400(0.2676)
2023/11/22 04:47:47 - INFO - root -   Epoch: [265/300][80/200], lr: 0.00000076 	 loss = 0.0049(0.2464)
2023/11/22 04:48:01 - INFO - root -   Epoch: [265/300][100/200], lr: 0.00000076 	 loss = 0.0003(0.2164)
2023/11/22 04:48:18 - INFO - root -   Epoch: [265/300][120/200], lr: 0.00000076 	 loss = 0.0095(0.2425)
2023/11/22 04:48:39 - INFO - root -   Epoch: [265/300][140/200], lr: 0.00000076 	 loss = 0.0006(0.2214)
2023/11/22 04:48:50 - INFO - root -   Epoch: [265/300][160/200], lr: 0.00000076 	 loss = 1.6039(0.2315)
2023/11/22 04:49:05 - INFO - root -   Epoch: [265/300][180/200], lr: 0.00000076 	 loss = 0.0027(0.2292)
2023/11/22 04:49:18 - INFO - root -   Epoch: [265/300] 	 loss = 0.2171
2023/11/22 04:49:18 - INFO - root -   train_accuracy = 0.9300
2023/11/22 04:49:19 - INFO - root -   Epoch: [266/300][0/200], lr: 0.00000076 	 loss = 0.6981(0.6981)
2023/11/22 04:49:40 - INFO - root -   Epoch: [266/300][20/200], lr: 0.00000076 	 loss = 1.2525(0.2769)
2023/11/22 04:50:00 - INFO - root -   Epoch: [266/300][40/200], lr: 0.00000076 	 loss = 1.7540(0.3479)
2023/11/22 04:50:16 - INFO - root -   Epoch: [266/300][60/200], lr: 0.00000076 	 loss = 0.1970(0.3401)
2023/11/22 04:50:32 - INFO - root -   Epoch: [266/300][80/200], lr: 0.00000076 	 loss = 0.0634(0.3540)
2023/11/22 04:50:46 - INFO - root -   Epoch: [266/300][100/200], lr: 0.00000076 	 loss = 0.0003(0.3294)
2023/11/22 04:51:02 - INFO - root -   Epoch: [266/300][120/200], lr: 0.00000076 	 loss = 0.0600(0.3381)
2023/11/22 04:51:24 - INFO - root -   Epoch: [266/300][140/200], lr: 0.00000076 	 loss = 0.0112(0.3232)
2023/11/22 04:51:43 - INFO - root -   Epoch: [266/300][160/200], lr: 0.00000076 	 loss = 1.1090(0.3427)
2023/11/22 04:51:56 - INFO - root -   Epoch: [266/300][180/200], lr: 0.00000076 	 loss = 0.4230(0.3457)
2023/11/22 04:52:07 - INFO - root -   Epoch: [266/300] 	 loss = 0.3487
2023/11/22 04:52:07 - INFO - root -   train_accuracy = 0.8825
2023/11/22 04:52:15 - INFO - root -   Epoch: [267/300][0/200], lr: 0.00000076 	 loss = 0.1793(0.1793)
2023/11/22 04:52:23 - INFO - root -   Epoch: [267/300][20/200], lr: 0.00000076 	 loss = 1.5124(0.2444)
2023/11/22 04:52:37 - INFO - root -   Epoch: [267/300][40/200], lr: 0.00000076 	 loss = 0.5155(0.2616)
2023/11/22 04:52:52 - INFO - root -   Epoch: [267/300][60/200], lr: 0.00000076 	 loss = 0.6389(0.2498)
2023/11/22 04:53:10 - INFO - root -   Epoch: [267/300][80/200], lr: 0.00000076 	 loss = 0.3881(0.2498)
2023/11/22 04:53:27 - INFO - root -   Epoch: [267/300][100/200], lr: 0.00000076 	 loss = 0.3107(0.2575)
2023/11/22 04:53:45 - INFO - root -   Epoch: [267/300][120/200], lr: 0.00000076 	 loss = 0.1784(0.2641)
2023/11/22 04:54:04 - INFO - root -   Epoch: [267/300][140/200], lr: 0.00000076 	 loss = 0.0213(0.2429)
2023/11/22 04:54:17 - INFO - root -   Epoch: [267/300][160/200], lr: 0.00000076 	 loss = 0.3001(0.2626)
2023/11/22 04:54:34 - INFO - root -   Epoch: [267/300][180/200], lr: 0.00000076 	 loss = 0.0001(0.2698)
2023/11/22 04:54:43 - INFO - root -   Epoch: [267/300] 	 loss = 0.2670
2023/11/22 04:54:43 - INFO - root -   train_accuracy = 0.9075
2023/11/22 04:54:51 - INFO - root -   Epoch: [268/300][0/200], lr: 0.00000076 	 loss = 0.6750(0.6750)
2023/11/22 04:55:08 - INFO - root -   Epoch: [268/300][20/200], lr: 0.00000076 	 loss = 0.9625(0.4500)
2023/11/22 04:55:16 - INFO - root -   Epoch: [268/300][40/200], lr: 0.00000076 	 loss = 1.8863(0.3633)
2023/11/22 04:55:39 - INFO - root -   Epoch: [268/300][60/200], lr: 0.00000076 	 loss = 0.0136(0.3149)
2023/11/22 04:55:55 - INFO - root -   Epoch: [268/300][80/200], lr: 0.00000076 	 loss = 0.1295(0.2792)
2023/11/22 04:56:11 - INFO - root -   Epoch: [268/300][100/200], lr: 0.00000076 	 loss = 0.0004(0.3060)
2023/11/22 04:56:23 - INFO - root -   Epoch: [268/300][120/200], lr: 0.00000076 	 loss = 2.1293(0.2990)
2023/11/22 04:56:39 - INFO - root -   Epoch: [268/300][140/200], lr: 0.00000076 	 loss = 0.3146(0.2798)
2023/11/22 04:56:56 - INFO - root -   Epoch: [268/300][160/200], lr: 0.00000076 	 loss = 0.1208(0.2901)
2023/11/22 04:57:08 - INFO - root -   Epoch: [268/300][180/200], lr: 0.00000076 	 loss = 0.0006(0.2929)
2023/11/22 04:57:19 - INFO - root -   Epoch: [268/300] 	 loss = 0.3029
2023/11/22 04:57:19 - INFO - root -   train_accuracy = 0.8750
2023/11/22 04:57:25 - INFO - root -   Epoch: [269/300][0/200], lr: 0.00000077 	 loss = 0.3500(0.3500)
2023/11/22 04:57:43 - INFO - root -   Epoch: [269/300][20/200], lr: 0.00000077 	 loss = 1.2835(0.2996)
2023/11/22 04:57:59 - INFO - root -   Epoch: [269/300][40/200], lr: 0.00000077 	 loss = 0.7033(0.2624)
2023/11/22 04:58:11 - INFO - root -   Epoch: [269/300][60/200], lr: 0.00000077 	 loss = 0.0003(0.2929)
2023/11/22 04:58:30 - INFO - root -   Epoch: [269/300][80/200], lr: 0.00000077 	 loss = 1.1694(0.2791)
2023/11/22 04:58:41 - INFO - root -   Epoch: [269/300][100/200], lr: 0.00000077 	 loss = 0.0006(0.2567)
2023/11/22 04:58:55 - INFO - root -   Epoch: [269/300][120/200], lr: 0.00000077 	 loss = 1.9321(0.2750)
2023/11/22 04:59:08 - INFO - root -   Epoch: [269/300][140/200], lr: 0.00000077 	 loss = 0.1003(0.2526)
2023/11/22 04:59:18 - INFO - root -   Epoch: [269/300][160/200], lr: 0.00000077 	 loss = 0.4660(0.2710)
2023/11/22 04:59:36 - INFO - root -   Epoch: [269/300][180/200], lr: 0.00000077 	 loss = 0.0436(0.2764)
2023/11/22 04:59:49 - INFO - root -   Epoch: [269/300] 	 loss = 0.2772
2023/11/22 05:01:50 - INFO - root -   precision = 0.8713
2023/11/22 05:01:50 - INFO - root -   eval_loss = 0.3977
2023/11/22 05:01:50 - INFO - root -   eval_acc = 0.8713
2023/11/22 05:01:51 - INFO - root -   train_accuracy = 0.9075
2023/11/22 05:02:00 - INFO - root -   Epoch: [270/300][0/200], lr: 0.00000077 	 loss = 1.1673(1.1673)
2023/11/22 05:02:14 - INFO - root -   Epoch: [270/300][20/200], lr: 0.00000077 	 loss = 0.8778(0.3500)
2023/11/22 05:02:34 - INFO - root -   Epoch: [270/300][40/200], lr: 0.00000077 	 loss = 0.3756(0.2834)
2023/11/22 05:02:42 - INFO - root -   Epoch: [270/300][60/200], lr: 0.00000077 	 loss = 0.7534(0.3041)
2023/11/22 05:02:56 - INFO - root -   Epoch: [270/300][80/200], lr: 0.00000077 	 loss = 0.1533(0.2750)
2023/11/22 05:03:13 - INFO - root -   Epoch: [270/300][100/200], lr: 0.00000077 	 loss = 0.0743(0.3024)
2023/11/22 05:03:25 - INFO - root -   Epoch: [270/300][120/200], lr: 0.00000077 	 loss = 0.0688(0.2853)
2023/11/22 05:03:42 - INFO - root -   Epoch: [270/300][140/200], lr: 0.00000077 	 loss = 0.0059(0.2862)
2023/11/22 05:03:56 - INFO - root -   Epoch: [270/300][160/200], lr: 0.00000077 	 loss = 0.3591(0.2879)
2023/11/22 05:04:12 - INFO - root -   Epoch: [270/300][180/200], lr: 0.00000077 	 loss = 0.0170(0.2860)
2023/11/22 05:04:23 - INFO - root -   Epoch: [270/300] 	 loss = 0.3058
2023/11/22 05:04:23 - INFO - root -   train_accuracy = 0.8950
2023/11/22 05:04:35 - INFO - root -   Epoch: [271/300][0/200], lr: 0.00000077 	 loss = 0.3763(0.3763)
2023/11/22 05:04:47 - INFO - root -   Epoch: [271/300][20/200], lr: 0.00000077 	 loss = 1.2761(0.3939)
2023/11/22 05:05:07 - INFO - root -   Epoch: [271/300][40/200], lr: 0.00000077 	 loss = 0.2533(0.3052)
2023/11/22 05:05:28 - INFO - root -   Epoch: [271/300][60/200], lr: 0.00000077 	 loss = 0.0765(0.3027)
2023/11/22 05:05:46 - INFO - root -   Epoch: [271/300][80/200], lr: 0.00000077 	 loss = 0.0653(0.2897)
2023/11/22 05:05:57 - INFO - root -   Epoch: [271/300][100/200], lr: 0.00000077 	 loss = 0.0006(0.2909)
2023/11/22 05:06:13 - INFO - root -   Epoch: [271/300][120/200], lr: 0.00000077 	 loss = 0.7713(0.2907)
2023/11/22 05:06:25 - INFO - root -   Epoch: [271/300][140/200], lr: 0.00000077 	 loss = 0.0009(0.2833)
2023/11/22 05:06:38 - INFO - root -   Epoch: [271/300][160/200], lr: 0.00000077 	 loss = 1.0940(0.3023)
2023/11/22 05:06:54 - INFO - root -   Epoch: [271/300][180/200], lr: 0.00000077 	 loss = 0.0048(0.2895)
2023/11/22 05:07:13 - INFO - root -   Epoch: [271/300] 	 loss = 0.3092
2023/11/22 05:07:13 - INFO - root -   train_accuracy = 0.8875
2023/11/22 05:07:28 - INFO - root -   Epoch: [272/300][0/200], lr: 0.00000077 	 loss = 0.1703(0.1703)
2023/11/22 05:07:48 - INFO - root -   Epoch: [272/300][20/200], lr: 0.00000077 	 loss = 0.7514(0.3661)
2023/11/22 05:07:55 - INFO - root -   Epoch: [272/300][40/200], lr: 0.00000077 	 loss = 1.1131(0.3106)
2023/11/22 05:08:13 - INFO - root -   Epoch: [272/300][60/200], lr: 0.00000077 	 loss = 0.0918(0.3009)
2023/11/22 05:08:22 - INFO - root -   Epoch: [272/300][80/200], lr: 0.00000077 	 loss = 0.0388(0.2570)
2023/11/22 05:08:31 - INFO - root -   Epoch: [272/300][100/200], lr: 0.00000077 	 loss = 0.0005(0.2572)
2023/11/22 05:08:46 - INFO - root -   Epoch: [272/300][120/200], lr: 0.00000077 	 loss = 2.8026(0.2710)
2023/11/22 05:08:57 - INFO - root -   Epoch: [272/300][140/200], lr: 0.00000077 	 loss = 0.0035(0.2677)
2023/11/22 05:09:13 - INFO - root -   Epoch: [272/300][160/200], lr: 0.00000077 	 loss = 0.7707(0.2643)
2023/11/22 05:09:30 - INFO - root -   Epoch: [272/300][180/200], lr: 0.00000077 	 loss = 0.1097(0.2722)
2023/11/22 05:09:41 - INFO - root -   Epoch: [272/300] 	 loss = 0.2632
2023/11/22 05:09:41 - INFO - root -   train_accuracy = 0.9075
2023/11/22 05:09:49 - INFO - root -   Epoch: [273/300][0/200], lr: 0.00000078 	 loss = 0.4258(0.4258)
2023/11/22 05:10:02 - INFO - root -   Epoch: [273/300][20/200], lr: 0.00000078 	 loss = 0.3508(0.3009)
2023/11/22 05:10:18 - INFO - root -   Epoch: [273/300][40/200], lr: 0.00000078 	 loss = 1.6181(0.2953)
2023/11/22 05:10:32 - INFO - root -   Epoch: [273/300][60/200], lr: 0.00000078 	 loss = 0.5686(0.3057)
2023/11/22 05:10:44 - INFO - root -   Epoch: [273/300][80/200], lr: 0.00000078 	 loss = 0.0735(0.3123)
2023/11/22 05:11:04 - INFO - root -   Epoch: [273/300][100/200], lr: 0.00000078 	 loss = 0.1853(0.3208)
2023/11/22 05:11:18 - INFO - root -   Epoch: [273/300][120/200], lr: 0.00000078 	 loss = 0.0182(0.3177)
2023/11/22 05:11:37 - INFO - root -   Epoch: [273/300][140/200], lr: 0.00000078 	 loss = 0.3230(0.3350)
2023/11/22 05:11:48 - INFO - root -   Epoch: [273/300][160/200], lr: 0.00000078 	 loss = 1.3365(0.3375)
2023/11/22 05:12:00 - INFO - root -   Epoch: [273/300][180/200], lr: 0.00000078 	 loss = 0.0029(0.3251)
2023/11/22 05:12:14 - INFO - root -   Epoch: [273/300] 	 loss = 0.3464
2023/11/22 05:12:14 - INFO - root -   train_accuracy = 0.8775
2023/11/22 05:12:29 - INFO - root -   Epoch: [274/300][0/200], lr: 0.00000078 	 loss = 0.3356(0.3356)
2023/11/22 05:12:37 - INFO - root -   Epoch: [274/300][20/200], lr: 0.00000078 	 loss = 0.8559(0.3061)
2023/11/22 05:12:52 - INFO - root -   Epoch: [274/300][40/200], lr: 0.00000078 	 loss = 0.1763(0.3399)
2023/11/22 05:13:11 - INFO - root -   Epoch: [274/300][60/200], lr: 0.00000078 	 loss = 0.1503(0.3036)
2023/11/22 05:13:28 - INFO - root -   Epoch: [274/300][80/200], lr: 0.00000078 	 loss = 0.0982(0.2946)
2023/11/22 05:13:45 - INFO - root -   Epoch: [274/300][100/200], lr: 0.00000078 	 loss = 0.1210(0.2921)
2023/11/22 05:13:57 - INFO - root -   Epoch: [274/300][120/200], lr: 0.00000078 	 loss = 0.1397(0.3015)
2023/11/22 05:14:13 - INFO - root -   Epoch: [274/300][140/200], lr: 0.00000078 	 loss = 0.0099(0.2926)
2023/11/22 05:14:24 - INFO - root -   Epoch: [274/300][160/200], lr: 0.00000078 	 loss = 0.1618(0.2786)
2023/11/22 05:14:38 - INFO - root -   Epoch: [274/300][180/200], lr: 0.00000078 	 loss = 0.1334(0.2658)
2023/11/22 05:14:47 - INFO - root -   Epoch: [274/300] 	 loss = 0.2626
2023/11/22 05:16:49 - INFO - root -   precision = 0.9010
2023/11/22 05:16:49 - INFO - root -   eval_loss = 0.3946
2023/11/22 05:16:49 - INFO - root -   eval_acc = 0.9010
2023/11/22 05:16:50 - INFO - root -   train_accuracy = 0.9075
2023/11/22 05:16:51 - INFO - root -   Epoch: [275/300][0/200], lr: 0.00000078 	 loss = 0.1944(0.1944)
2023/11/22 05:17:11 - INFO - root -   Epoch: [275/300][20/200], lr: 0.00000078 	 loss = 1.0262(0.2405)
2023/11/22 05:17:31 - INFO - root -   Epoch: [275/300][40/200], lr: 0.00000078 	 loss = 0.1999(0.2620)
2023/11/22 05:17:40 - INFO - root -   Epoch: [275/300][60/200], lr: 0.00000078 	 loss = 0.0096(0.2195)
2023/11/22 05:18:02 - INFO - root -   Epoch: [275/300][80/200], lr: 0.00000078 	 loss = 0.6686(0.2166)
2023/11/22 05:18:14 - INFO - root -   Epoch: [275/300][100/200], lr: 0.00000078 	 loss = 0.0137(0.2390)
2023/11/22 05:18:31 - INFO - root -   Epoch: [275/300][120/200], lr: 0.00000078 	 loss = 1.5427(0.2403)
2023/11/22 05:18:47 - INFO - root -   Epoch: [275/300][140/200], lr: 0.00000078 	 loss = 0.1119(0.2531)
2023/11/22 05:19:00 - INFO - root -   Epoch: [275/300][160/200], lr: 0.00000078 	 loss = 1.0967(0.2614)
2023/11/22 05:19:20 - INFO - root -   Epoch: [275/300][180/200], lr: 0.00000078 	 loss = 0.0508(0.2719)
2023/11/22 05:19:34 - INFO - root -   Epoch: [275/300] 	 loss = 0.2777
2023/11/22 05:19:34 - INFO - root -   train_accuracy = 0.9100
2023/11/22 05:19:47 - INFO - root -   Epoch: [276/300][0/200], lr: 0.00000078 	 loss = 1.3503(1.3503)
2023/11/22 05:20:02 - INFO - root -   Epoch: [276/300][20/200], lr: 0.00000078 	 loss = 2.0181(0.4107)
2023/11/22 05:20:13 - INFO - root -   Epoch: [276/300][40/200], lr: 0.00000078 	 loss = 0.5355(0.3537)
2023/11/22 05:20:33 - INFO - root -   Epoch: [276/300][60/200], lr: 0.00000078 	 loss = 0.7559(0.2944)
2023/11/22 05:20:46 - INFO - root -   Epoch: [276/300][80/200], lr: 0.00000078 	 loss = 0.5068(0.2602)
2023/11/22 05:21:07 - INFO - root -   Epoch: [276/300][100/200], lr: 0.00000078 	 loss = 0.0195(0.2515)
2023/11/22 05:21:27 - INFO - root -   Epoch: [276/300][120/200], lr: 0.00000078 	 loss = 0.3618(0.2639)
2023/11/22 05:21:39 - INFO - root -   Epoch: [276/300][140/200], lr: 0.00000078 	 loss = 0.0024(0.2524)
2023/11/22 05:21:52 - INFO - root -   Epoch: [276/300][160/200], lr: 0.00000078 	 loss = 0.3569(0.2396)
2023/11/22 05:22:02 - INFO - root -   Epoch: [276/300][180/200], lr: 0.00000078 	 loss = 0.0003(0.2436)
2023/11/22 05:22:12 - INFO - root -   Epoch: [276/300] 	 loss = 0.2439
2023/11/22 05:22:12 - INFO - root -   train_accuracy = 0.9175
2023/11/22 05:22:20 - INFO - root -   Epoch: [277/300][0/200], lr: 0.00000079 	 loss = 0.5284(0.5284)
2023/11/22 05:22:36 - INFO - root -   Epoch: [277/300][20/200], lr: 0.00000079 	 loss = 1.1344(0.2424)
2023/11/22 05:22:49 - INFO - root -   Epoch: [277/300][40/200], lr: 0.00000079 	 loss = 1.1268(0.1970)
2023/11/22 05:23:02 - INFO - root -   Epoch: [277/300][60/200], lr: 0.00000079 	 loss = 0.0018(0.2109)
2023/11/22 05:23:12 - INFO - root -   Epoch: [277/300][80/200], lr: 0.00000079 	 loss = 0.1747(0.2177)
2023/11/22 05:23:39 - INFO - root -   Epoch: [277/300][100/200], lr: 0.00000079 	 loss = 0.1165(0.2375)
2023/11/22 05:23:57 - INFO - root -   Epoch: [277/300][120/200], lr: 0.00000079 	 loss = 2.0727(0.2865)
2023/11/22 05:24:09 - INFO - root -   Epoch: [277/300][140/200], lr: 0.00000079 	 loss = 0.1603(0.2666)
2023/11/22 05:24:22 - INFO - root -   Epoch: [277/300][160/200], lr: 0.00000079 	 loss = 1.3620(0.2739)
2023/11/22 05:24:33 - INFO - root -   Epoch: [277/300][180/200], lr: 0.00000079 	 loss = 0.0431(0.2642)
2023/11/22 05:24:44 - INFO - root -   Epoch: [277/300] 	 loss = 0.2540
2023/11/22 05:24:44 - INFO - root -   train_accuracy = 0.9075
2023/11/22 05:24:45 - INFO - root -   Epoch: [278/300][0/200], lr: 0.00000079 	 loss = 0.1347(0.1347)
2023/11/22 05:25:02 - INFO - root -   Epoch: [278/300][20/200], lr: 0.00000079 	 loss = 0.5425(0.1442)
2023/11/22 05:25:11 - INFO - root -   Epoch: [278/300][40/200], lr: 0.00000079 	 loss = 0.4662(0.1505)
2023/11/22 05:25:22 - INFO - root -   Epoch: [278/300][60/200], lr: 0.00000079 	 loss = 0.0013(0.1760)
2023/11/22 05:25:34 - INFO - root -   Epoch: [278/300][80/200], lr: 0.00000079 	 loss = 0.0104(0.1780)
2023/11/22 05:25:50 - INFO - root -   Epoch: [278/300][100/200], lr: 0.00000079 	 loss = 0.1104(0.1848)
2023/11/22 05:26:11 - INFO - root -   Epoch: [278/300][120/200], lr: 0.00000079 	 loss = 0.1052(0.2094)
2023/11/22 05:26:29 - INFO - root -   Epoch: [278/300][140/200], lr: 0.00000079 	 loss = 0.0092(0.2082)
2023/11/22 05:26:52 - INFO - root -   Epoch: [278/300][160/200], lr: 0.00000079 	 loss = 0.1504(0.2351)
2023/11/22 05:27:09 - INFO - root -   Epoch: [278/300][180/200], lr: 0.00000079 	 loss = 0.0937(0.2496)
2023/11/22 05:27:19 - INFO - root -   Epoch: [278/300] 	 loss = 0.2559
2023/11/22 05:27:19 - INFO - root -   train_accuracy = 0.9025
2023/11/22 05:27:26 - INFO - root -   Epoch: [279/300][0/200], lr: 0.00000079 	 loss = 0.0956(0.0956)
2023/11/22 05:27:37 - INFO - root -   Epoch: [279/300][20/200], lr: 0.00000079 	 loss = 0.1800(0.2315)
2023/11/22 05:27:53 - INFO - root -   Epoch: [279/300][40/200], lr: 0.00000079 	 loss = 0.4443(0.2181)
2023/11/22 05:28:13 - INFO - root -   Epoch: [279/300][60/200], lr: 0.00000079 	 loss = 0.1923(0.2500)
2023/11/22 05:28:26 - INFO - root -   Epoch: [279/300][80/200], lr: 0.00000079 	 loss = 1.0503(0.3170)
2023/11/22 05:28:36 - INFO - root -   Epoch: [279/300][100/200], lr: 0.00000079 	 loss = 0.0006(0.2856)
2023/11/22 05:28:52 - INFO - root -   Epoch: [279/300][120/200], lr: 0.00000079 	 loss = 0.2459(0.2754)
2023/11/22 05:29:09 - INFO - root -   Epoch: [279/300][140/200], lr: 0.00000079 	 loss = 0.0491(0.2642)
2023/11/22 05:29:23 - INFO - root -   Epoch: [279/300][160/200], lr: 0.00000079 	 loss = 0.6483(0.2711)
2023/11/22 05:29:36 - INFO - root -   Epoch: [279/300][180/200], lr: 0.00000079 	 loss = 0.0125(0.2655)
2023/11/22 05:29:45 - INFO - root -   Epoch: [279/300] 	 loss = 0.2700
2023/11/22 05:31:46 - INFO - root -   precision = 0.8614
2023/11/22 05:31:46 - INFO - root -   eval_loss = 0.4632
2023/11/22 05:31:46 - INFO - root -   eval_acc = 0.8614
2023/11/22 05:31:48 - INFO - root -   train_accuracy = 0.8975
2023/11/22 05:31:49 - INFO - root -   Epoch: [280/300][0/200], lr: 0.00000079 	 loss = 0.8833(0.8833)
2023/11/22 05:32:07 - INFO - root -   Epoch: [280/300][20/200], lr: 0.00000079 	 loss = 0.3061(0.2064)
2023/11/22 05:32:23 - INFO - root -   Epoch: [280/300][40/200], lr: 0.00000079 	 loss = 1.3517(0.1973)
2023/11/22 05:32:34 - INFO - root -   Epoch: [280/300][60/200], lr: 0.00000079 	 loss = 0.1288(0.1722)
2023/11/22 05:32:48 - INFO - root -   Epoch: [280/300][80/200], lr: 0.00000079 	 loss = 0.0148(0.2004)
2023/11/22 05:33:05 - INFO - root -   Epoch: [280/300][100/200], lr: 0.00000079 	 loss = 0.0057(0.2227)
2023/11/22 05:33:18 - INFO - root -   Epoch: [280/300][120/200], lr: 0.00000079 	 loss = 0.2825(0.2155)
2023/11/22 05:33:34 - INFO - root -   Epoch: [280/300][140/200], lr: 0.00000079 	 loss = 0.0069(0.2285)
2023/11/22 05:33:47 - INFO - root -   Epoch: [280/300][160/200], lr: 0.00000079 	 loss = 1.2971(0.2624)
2023/11/22 05:34:00 - INFO - root -   Epoch: [280/300][180/200], lr: 0.00000079 	 loss = 0.1462(0.2455)
2023/11/22 05:34:10 - INFO - root -   Epoch: [280/300] 	 loss = 0.2434
2023/11/22 05:34:10 - INFO - root -   train_accuracy = 0.9175
2023/11/22 05:34:11 - INFO - root -   Epoch: [281/300][0/200], lr: 0.00000080 	 loss = 0.1888(0.1888)
2023/11/22 05:34:29 - INFO - root -   Epoch: [281/300][20/200], lr: 0.00000080 	 loss = 0.5241(0.2076)
2023/11/22 05:34:40 - INFO - root -   Epoch: [281/300][40/200], lr: 0.00000080 	 loss = 0.4460(0.2095)
2023/11/22 05:34:51 - INFO - root -   Epoch: [281/300][60/200], lr: 0.00000080 	 loss = 0.0023(0.1848)
2023/11/22 05:35:03 - INFO - root -   Epoch: [281/300][80/200], lr: 0.00000080 	 loss = 0.1999(0.1871)
2023/11/22 05:35:17 - INFO - root -   Epoch: [281/300][100/200], lr: 0.00000080 	 loss = 0.1048(0.1813)
2023/11/22 05:35:32 - INFO - root -   Epoch: [281/300][120/200], lr: 0.00000080 	 loss = 0.3203(0.2076)
2023/11/22 05:35:41 - INFO - root -   Epoch: [281/300][140/200], lr: 0.00000080 	 loss = 0.0016(0.1870)
2023/11/22 05:36:02 - INFO - root -   Epoch: [281/300][160/200], lr: 0.00000080 	 loss = 0.7147(0.2082)
2023/11/22 05:36:21 - INFO - root -   Epoch: [281/300][180/200], lr: 0.00000080 	 loss = 0.0019(0.2018)
2023/11/22 05:36:31 - INFO - root -   Epoch: [281/300] 	 loss = 0.2057
2023/11/22 05:36:31 - INFO - root -   train_accuracy = 0.9375
2023/11/22 05:36:38 - INFO - root -   Epoch: [282/300][0/200], lr: 0.00000080 	 loss = 0.0366(0.0366)
2023/11/22 05:37:02 - INFO - root -   Epoch: [282/300][20/200], lr: 0.00000080 	 loss = 1.3067(0.3234)
2023/11/22 05:37:11 - INFO - root -   Epoch: [282/300][40/200], lr: 0.00000080 	 loss = 0.1674(0.3099)
2023/11/22 05:37:28 - INFO - root -   Epoch: [282/300][60/200], lr: 0.00000080 	 loss = 0.0624(0.2924)
2023/11/22 05:37:39 - INFO - root -   Epoch: [282/300][80/200], lr: 0.00000080 	 loss = 0.1064(0.2643)
2023/11/22 05:37:52 - INFO - root -   Epoch: [282/300][100/200], lr: 0.00000080 	 loss = 0.0001(0.2648)
2023/11/22 05:38:06 - INFO - root -   Epoch: [282/300][120/200], lr: 0.00000080 	 loss = 0.0195(0.2391)
2023/11/22 05:38:21 - INFO - root -   Epoch: [282/300][140/200], lr: 0.00000080 	 loss = 0.1073(0.2268)
2023/11/22 05:38:34 - INFO - root -   Epoch: [282/300][160/200], lr: 0.00000080 	 loss = 0.1315(0.2227)
2023/11/22 05:38:53 - INFO - root -   Epoch: [282/300][180/200], lr: 0.00000080 	 loss = 0.1190(0.2313)
2023/11/22 05:39:00 - INFO - root -   Epoch: [282/300] 	 loss = 0.2380
2023/11/22 05:39:00 - INFO - root -   train_accuracy = 0.9175
2023/11/22 05:39:08 - INFO - root -   Epoch: [283/300][0/200], lr: 0.00000080 	 loss = 1.3810(1.3810)
2023/11/22 05:39:23 - INFO - root -   Epoch: [283/300][20/200], lr: 0.00000080 	 loss = 0.9718(0.3204)
2023/11/22 05:39:41 - INFO - root -   Epoch: [283/300][40/200], lr: 0.00000080 	 loss = 1.1340(0.3400)
2023/11/22 05:40:00 - INFO - root -   Epoch: [283/300][60/200], lr: 0.00000080 	 loss = 0.1038(0.3185)
2023/11/22 05:40:22 - INFO - root -   Epoch: [283/300][80/200], lr: 0.00000080 	 loss = 0.1645(0.2573)
2023/11/22 05:40:30 - INFO - root -   Epoch: [283/300][100/200], lr: 0.00000080 	 loss = 0.0933(0.2737)
2023/11/22 05:40:50 - INFO - root -   Epoch: [283/300][120/200], lr: 0.00000080 	 loss = 0.1605(0.2820)
2023/11/22 05:41:15 - INFO - root -   Epoch: [283/300][140/200], lr: 0.00000080 	 loss = 0.0084(0.2648)
2023/11/22 05:41:27 - INFO - root -   Epoch: [283/300][160/200], lr: 0.00000080 	 loss = 0.2105(0.2772)
2023/11/22 05:41:40 - INFO - root -   Epoch: [283/300][180/200], lr: 0.00000080 	 loss = 0.1423(0.2633)
2023/11/22 05:41:54 - INFO - root -   Epoch: [283/300] 	 loss = 0.2645
2023/11/22 05:41:54 - INFO - root -   train_accuracy = 0.9050
2023/11/22 05:42:02 - INFO - root -   Epoch: [284/300][0/200], lr: 0.00000080 	 loss = 1.8483(1.8483)
2023/11/22 05:42:17 - INFO - root -   Epoch: [284/300][20/200], lr: 0.00000080 	 loss = 0.1381(0.3030)
2023/11/22 05:42:31 - INFO - root -   Epoch: [284/300][40/200], lr: 0.00000080 	 loss = 0.1992(0.2142)
2023/11/22 05:42:39 - INFO - root -   Epoch: [284/300][60/200], lr: 0.00000080 	 loss = 0.0003(0.1995)
2023/11/22 05:42:57 - INFO - root -   Epoch: [284/300][80/200], lr: 0.00000080 	 loss = 0.1424(0.2228)
2023/11/22 05:43:19 - INFO - root -   Epoch: [284/300][100/200], lr: 0.00000080 	 loss = 0.0359(0.1976)
2023/11/22 05:43:33 - INFO - root -   Epoch: [284/300][120/200], lr: 0.00000080 	 loss = 0.0738(0.1837)
2023/11/22 05:43:49 - INFO - root -   Epoch: [284/300][140/200], lr: 0.00000080 	 loss = 0.0182(0.1975)
2023/11/22 05:44:02 - INFO - root -   Epoch: [284/300][160/200], lr: 0.00000080 	 loss = 0.1956(0.1915)
2023/11/22 05:44:20 - INFO - root -   Epoch: [284/300][180/200], lr: 0.00000080 	 loss = 0.0337(0.1795)
2023/11/22 05:44:34 - INFO - root -   Epoch: [284/300] 	 loss = 0.1819
2023/11/22 05:46:35 - INFO - root -   precision = 0.8614
2023/11/22 05:46:35 - INFO - root -   eval_loss = 0.4751
2023/11/22 05:46:35 - INFO - root -   eval_acc = 0.8614
2023/11/22 05:46:36 - INFO - root -   train_accuracy = 0.9450
2023/11/22 05:46:47 - INFO - root -   Epoch: [285/300][0/200], lr: 0.00000081 	 loss = 1.5982(1.5982)
2023/11/22 05:46:58 - INFO - root -   Epoch: [285/300][20/200], lr: 0.00000081 	 loss = 0.3478(0.3763)
2023/11/22 05:47:21 - INFO - root -   Epoch: [285/300][40/200], lr: 0.00000081 	 loss = 0.5241(0.2564)
2023/11/22 05:47:38 - INFO - root -   Epoch: [285/300][60/200], lr: 0.00000081 	 loss = 0.0752(0.2395)
2023/11/22 05:47:50 - INFO - root -   Epoch: [285/300][80/200], lr: 0.00000081 	 loss = 0.1454(0.2175)
2023/11/22 05:48:06 - INFO - root -   Epoch: [285/300][100/200], lr: 0.00000081 	 loss = 0.0677(0.2143)
2023/11/22 05:48:24 - INFO - root -   Epoch: [285/300][120/200], lr: 0.00000081 	 loss = 0.1259(0.2428)
2023/11/22 05:48:34 - INFO - root -   Epoch: [285/300][140/200], lr: 0.00000081 	 loss = 0.0633(0.2421)
2023/11/22 05:48:50 - INFO - root -   Epoch: [285/300][160/200], lr: 0.00000081 	 loss = 2.4764(0.2551)
2023/11/22 05:49:04 - INFO - root -   Epoch: [285/300][180/200], lr: 0.00000081 	 loss = 0.1079(0.2543)
2023/11/22 05:49:14 - INFO - root -   Epoch: [285/300] 	 loss = 0.2500
2023/11/22 05:49:14 - INFO - root -   train_accuracy = 0.9225
2023/11/22 05:49:15 - INFO - root -   Epoch: [286/300][0/200], lr: 0.00000081 	 loss = 0.5540(0.5540)
2023/11/22 05:49:34 - INFO - root -   Epoch: [286/300][20/200], lr: 0.00000081 	 loss = 0.2759(0.2671)
2023/11/22 05:49:44 - INFO - root -   Epoch: [286/300][40/200], lr: 0.00000081 	 loss = 0.1388(0.1821)
2023/11/22 05:49:57 - INFO - root -   Epoch: [286/300][60/200], lr: 0.00000081 	 loss = 0.0018(0.1847)
2023/11/22 05:50:14 - INFO - root -   Epoch: [286/300][80/200], lr: 0.00000081 	 loss = 0.0904(0.1916)
2023/11/22 05:50:31 - INFO - root -   Epoch: [286/300][100/200], lr: 0.00000081 	 loss = 0.0007(0.2129)
2023/11/22 05:50:45 - INFO - root -   Epoch: [286/300][120/200], lr: 0.00000081 	 loss = 2.4691(0.2619)
2023/11/22 05:50:57 - INFO - root -   Epoch: [286/300][140/200], lr: 0.00000081 	 loss = 0.1025(0.2725)
2023/11/22 05:51:18 - INFO - root -   Epoch: [286/300][160/200], lr: 0.00000081 	 loss = 2.1617(0.2950)
2023/11/22 05:51:35 - INFO - root -   Epoch: [286/300][180/200], lr: 0.00000081 	 loss = 0.0906(0.2773)
2023/11/22 05:51:45 - INFO - root -   Epoch: [286/300] 	 loss = 0.2957
2023/11/22 05:51:45 - INFO - root -   train_accuracy = 0.9100
2023/11/22 05:51:46 - INFO - root -   Epoch: [287/300][0/200], lr: 0.00000081 	 loss = 0.7557(0.7557)
2023/11/22 05:52:00 - INFO - root -   Epoch: [287/300][20/200], lr: 0.00000081 	 loss = 0.0747(0.1879)
2023/11/22 05:52:08 - INFO - root -   Epoch: [287/300][40/200], lr: 0.00000081 	 loss = 0.0110(0.1651)
2023/11/22 05:52:25 - INFO - root -   Epoch: [287/300][60/200], lr: 0.00000081 	 loss = 0.0148(0.1389)
2023/11/22 05:52:34 - INFO - root -   Epoch: [287/300][80/200], lr: 0.00000081 	 loss = 0.0266(0.1910)
2023/11/22 05:52:47 - INFO - root -   Epoch: [287/300][100/200], lr: 0.00000081 	 loss = 0.1094(0.1732)
2023/11/22 05:53:06 - INFO - root -   Epoch: [287/300][120/200], lr: 0.00000081 	 loss = 0.6583(0.1982)
2023/11/22 05:53:19 - INFO - root -   Epoch: [287/300][140/200], lr: 0.00000081 	 loss = 0.0016(0.1848)
2023/11/22 05:53:34 - INFO - root -   Epoch: [287/300][160/200], lr: 0.00000081 	 loss = 0.4991(0.2142)
2023/11/22 05:53:48 - INFO - root -   Epoch: [287/300][180/200], lr: 0.00000081 	 loss = 0.0028(0.2291)
2023/11/22 05:54:02 - INFO - root -   Epoch: [287/300] 	 loss = 0.2351
2023/11/22 05:54:02 - INFO - root -   train_accuracy = 0.9300
2023/11/22 05:54:03 - INFO - root -   Epoch: [288/300][0/200], lr: 0.00000081 	 loss = 0.7805(0.7805)
2023/11/22 05:54:22 - INFO - root -   Epoch: [288/300][20/200], lr: 0.00000081 	 loss = 0.3657(0.3162)
2023/11/22 05:54:36 - INFO - root -   Epoch: [288/300][40/200], lr: 0.00000081 	 loss = 0.3854(0.2482)
2023/11/22 05:54:52 - INFO - root -   Epoch: [288/300][60/200], lr: 0.00000081 	 loss = 0.0566(0.2067)
2023/11/22 05:55:00 - INFO - root -   Epoch: [288/300][80/200], lr: 0.00000081 	 loss = 0.7749(0.2324)
2023/11/22 05:55:18 - INFO - root -   Epoch: [288/300][100/200], lr: 0.00000081 	 loss = 0.0039(0.2442)
2023/11/22 05:55:27 - INFO - root -   Epoch: [288/300][120/200], lr: 0.00000081 	 loss = 0.8065(0.2347)
2023/11/22 05:55:48 - INFO - root -   Epoch: [288/300][140/200], lr: 0.00000081 	 loss = 0.0024(0.2227)
2023/11/22 05:56:06 - INFO - root -   Epoch: [288/300][160/200], lr: 0.00000081 	 loss = 1.5823(0.2501)
2023/11/22 05:56:23 - INFO - root -   Epoch: [288/300][180/200], lr: 0.00000081 	 loss = 0.1962(0.2385)
2023/11/22 05:56:31 - INFO - root -   Epoch: [288/300] 	 loss = 0.2571
2023/11/22 05:56:31 - INFO - root -   train_accuracy = 0.9125
2023/11/22 05:56:33 - INFO - root -   Epoch: [289/300][0/200], lr: 0.00000082 	 loss = 0.1366(0.1366)
2023/11/22 05:56:49 - INFO - root -   Epoch: [289/300][20/200], lr: 0.00000082 	 loss = 0.5580(0.2438)
2023/11/22 05:57:04 - INFO - root -   Epoch: [289/300][40/200], lr: 0.00000082 	 loss = 1.1190(0.2909)
2023/11/22 05:57:18 - INFO - root -   Epoch: [289/300][60/200], lr: 0.00000082 	 loss = 0.0019(0.2625)
2023/11/22 05:57:35 - INFO - root -   Epoch: [289/300][80/200], lr: 0.00000082 	 loss = 0.9946(0.2737)
2023/11/22 05:57:49 - INFO - root -   Epoch: [289/300][100/200], lr: 0.00000082 	 loss = 0.2050(0.2783)
2023/11/22 05:58:09 - INFO - root -   Epoch: [289/300][120/200], lr: 0.00000082 	 loss = 0.3576(0.2805)
2023/11/22 05:58:17 - INFO - root -   Epoch: [289/300][140/200], lr: 0.00000082 	 loss = 0.0039(0.2725)
2023/11/22 05:58:36 - INFO - root -   Epoch: [289/300][160/200], lr: 0.00000082 	 loss = 0.1010(0.2863)
2023/11/22 05:58:45 - INFO - root -   Epoch: [289/300][180/200], lr: 0.00000082 	 loss = 0.0928(0.2742)
2023/11/22 05:58:52 - INFO - root -   Epoch: [289/300] 	 loss = 0.2702
2023/11/22 06:00:53 - INFO - root -   precision = 0.8515
2023/11/22 06:00:53 - INFO - root -   eval_loss = 0.4102
2023/11/22 06:00:53 - INFO - root -   eval_acc = 0.8515
2023/11/22 06:00:54 - INFO - root -   train_accuracy = 0.9050
2023/11/22 06:01:02 - INFO - root -   Epoch: [290/300][0/200], lr: 0.00000082 	 loss = 0.1389(0.1389)
2023/11/22 06:01:18 - INFO - root -   Epoch: [290/300][20/200], lr: 0.00000082 	 loss = 1.5718(0.3439)
2023/11/22 06:01:34 - INFO - root -   Epoch: [290/300][40/200], lr: 0.00000082 	 loss = 0.0264(0.2637)
2023/11/22 06:01:47 - INFO - root -   Epoch: [290/300][60/200], lr: 0.00000082 	 loss = 0.0007(0.2323)
2023/11/22 06:02:06 - INFO - root -   Epoch: [290/300][80/200], lr: 0.00000082 	 loss = 1.2193(0.2273)
2023/11/22 06:02:19 - INFO - root -   Epoch: [290/300][100/200], lr: 0.00000082 	 loss = 0.1157(0.2486)
2023/11/22 06:02:34 - INFO - root -   Epoch: [290/300][120/200], lr: 0.00000082 	 loss = 0.3411(0.2668)
2023/11/22 06:02:52 - INFO - root -   Epoch: [290/300][140/200], lr: 0.00000082 	 loss = 0.0013(0.2517)
2023/11/22 06:03:00 - INFO - root -   Epoch: [290/300][160/200], lr: 0.00000082 	 loss = 0.1230(0.2512)
2023/11/22 06:03:14 - INFO - root -   Epoch: [290/300][180/200], lr: 0.00000082 	 loss = 0.2269(0.2404)
2023/11/22 06:03:23 - INFO - root -   Epoch: [290/300] 	 loss = 0.2466
2023/11/22 06:03:23 - INFO - root -   train_accuracy = 0.9150
2023/11/22 06:03:24 - INFO - root -   Epoch: [291/300][0/200], lr: 0.00000082 	 loss = 0.5094(0.5094)
2023/11/22 06:03:39 - INFO - root -   Epoch: [291/300][20/200], lr: 0.00000082 	 loss = 0.7115(0.2446)
2023/11/22 06:03:49 - INFO - root -   Epoch: [291/300][40/200], lr: 0.00000082 	 loss = 1.2869(0.2198)
2023/11/22 06:04:13 - INFO - root -   Epoch: [291/300][60/200], lr: 0.00000082 	 loss = 0.0049(0.2097)
2023/11/22 06:04:27 - INFO - root -   Epoch: [291/300][80/200], lr: 0.00000082 	 loss = 0.1419(0.2718)
2023/11/22 06:04:45 - INFO - root -   Epoch: [291/300][100/200], lr: 0.00000082 	 loss = 0.0004(0.2592)
2023/11/22 06:05:00 - INFO - root -   Epoch: [291/300][120/200], lr: 0.00000082 	 loss = 0.1826(0.2583)
2023/11/22 06:05:14 - INFO - root -   Epoch: [291/300][140/200], lr: 0.00000082 	 loss = 0.0018(0.2482)
2023/11/22 06:05:30 - INFO - root -   Epoch: [291/300][160/200], lr: 0.00000082 	 loss = 1.2963(0.2554)
2023/11/22 06:05:40 - INFO - root -   Epoch: [291/300][180/200], lr: 0.00000082 	 loss = 0.2402(0.2530)
2023/11/22 06:05:58 - INFO - root -   Epoch: [291/300] 	 loss = 0.2529
2023/11/22 06:05:58 - INFO - root -   train_accuracy = 0.9225
2023/11/22 06:06:07 - INFO - root -   Epoch: [292/300][0/200], lr: 0.00000082 	 loss = 0.3069(0.3069)
2023/11/22 06:06:23 - INFO - root -   Epoch: [292/300][20/200], lr: 0.00000082 	 loss = 0.2160(0.3737)
2023/11/22 06:06:40 - INFO - root -   Epoch: [292/300][40/200], lr: 0.00000082 	 loss = 0.2472(0.2810)
2023/11/22 06:06:59 - INFO - root -   Epoch: [292/300][60/200], lr: 0.00000082 	 loss = 0.0446(0.2393)
2023/11/22 06:07:07 - INFO - root -   Epoch: [292/300][80/200], lr: 0.00000082 	 loss = 0.0622(0.1998)
2023/11/22 06:07:26 - INFO - root -   Epoch: [292/300][100/200], lr: 0.00000082 	 loss = 0.0761(0.1927)
2023/11/22 06:07:37 - INFO - root -   Epoch: [292/300][120/200], lr: 0.00000082 	 loss = 0.1874(0.1851)
2023/11/22 06:07:47 - INFO - root -   Epoch: [292/300][140/200], lr: 0.00000082 	 loss = 0.0243(0.1682)
2023/11/22 06:08:05 - INFO - root -   Epoch: [292/300][160/200], lr: 0.00000082 	 loss = 0.5581(0.1800)
2023/11/22 06:08:25 - INFO - root -   Epoch: [292/300][180/200], lr: 0.00000082 	 loss = 0.0045(0.1787)
2023/11/22 06:08:33 - INFO - root -   Epoch: [292/300] 	 loss = 0.1990
2023/11/22 06:08:33 - INFO - root -   train_accuracy = 0.9400
2023/11/22 06:08:34 - INFO - root -   Epoch: [293/300][0/200], lr: 0.00000083 	 loss = 0.2163(0.2163)
2023/11/22 06:08:49 - INFO - root -   Epoch: [293/300][20/200], lr: 0.00000083 	 loss = 0.1484(0.1794)
2023/11/22 06:09:03 - INFO - root -   Epoch: [293/300][40/200], lr: 0.00000083 	 loss = 0.3178(0.2219)
2023/11/22 06:09:16 - INFO - root -   Epoch: [293/300][60/200], lr: 0.00000083 	 loss = 0.1726(0.2067)
2023/11/22 06:09:34 - INFO - root -   Epoch: [293/300][80/200], lr: 0.00000083 	 loss = 1.4565(0.2214)
2023/11/22 06:09:50 - INFO - root -   Epoch: [293/300][100/200], lr: 0.00000083 	 loss = 0.1658(0.2590)
2023/11/22 06:10:08 - INFO - root -   Epoch: [293/300][120/200], lr: 0.00000083 	 loss = 0.1810(0.2498)
2023/11/22 06:10:27 - INFO - root -   Epoch: [293/300][140/200], lr: 0.00000083 	 loss = 0.0899(0.2399)
2023/11/22 06:10:39 - INFO - root -   Epoch: [293/300][160/200], lr: 0.00000083 	 loss = 0.2001(0.2431)
2023/11/22 06:10:54 - INFO - root -   Epoch: [293/300][180/200], lr: 0.00000083 	 loss = 0.0575(0.2455)
2023/11/22 06:11:06 - INFO - root -   Epoch: [293/300] 	 loss = 0.2448
2023/11/22 06:11:06 - INFO - root -   train_accuracy = 0.9100
2023/11/22 06:11:14 - INFO - root -   Epoch: [294/300][0/200], lr: 0.00000083 	 loss = 0.5512(0.5512)
2023/11/22 06:11:38 - INFO - root -   Epoch: [294/300][20/200], lr: 0.00000083 	 loss = 1.2816(0.4601)
2023/11/22 06:11:47 - INFO - root -   Epoch: [294/300][40/200], lr: 0.00000083 	 loss = 0.0763(0.3644)
2023/11/22 06:11:59 - INFO - root -   Epoch: [294/300][60/200], lr: 0.00000083 	 loss = 0.0629(0.3043)
2023/11/22 06:12:13 - INFO - root -   Epoch: [294/300][80/200], lr: 0.00000083 	 loss = 0.0204(0.2613)
2023/11/22 06:12:24 - INFO - root -   Epoch: [294/300][100/200], lr: 0.00000083 	 loss = 0.0011(0.2672)
2023/11/22 06:12:36 - INFO - root -   Epoch: [294/300][120/200], lr: 0.00000083 	 loss = 0.0997(0.2597)
2023/11/22 06:12:48 - INFO - root -   Epoch: [294/300][140/200], lr: 0.00000083 	 loss = 0.1071(0.2478)
2023/11/22 06:12:59 - INFO - root -   Epoch: [294/300][160/200], lr: 0.00000083 	 loss = 1.0791(0.2641)
2023/11/22 06:13:13 - INFO - root -   Epoch: [294/300][180/200], lr: 0.00000083 	 loss = 0.0005(0.2488)
2023/11/22 06:13:28 - INFO - root -   Epoch: [294/300] 	 loss = 0.2567
2023/11/22 06:15:32 - INFO - root -   precision = 0.8515
2023/11/22 06:15:32 - INFO - root -   eval_loss = 0.5063
2023/11/22 06:15:32 - INFO - root -   eval_acc = 0.8515
2023/11/22 06:15:33 - INFO - root -   train_accuracy = 0.9125
2023/11/22 06:15:34 - INFO - root -   Epoch: [295/300][0/200], lr: 0.00000083 	 loss = 0.6075(0.6075)
2023/11/22 06:15:53 - INFO - root -   Epoch: [295/300][20/200], lr: 0.00000083 	 loss = 0.3832(0.3406)
2023/11/22 06:16:11 - INFO - root -   Epoch: [295/300][40/200], lr: 0.00000083 	 loss = 0.4053(0.2771)
2023/11/22 06:16:29 - INFO - root -   Epoch: [295/300][60/200], lr: 0.00000083 	 loss = 0.0125(0.2568)
2023/11/22 06:16:47 - INFO - root -   Epoch: [295/300][80/200], lr: 0.00000083 	 loss = 0.1086(0.2577)
2023/11/22 06:17:02 - INFO - root -   Epoch: [295/300][100/200], lr: 0.00000083 	 loss = 0.0554(0.2316)
2023/11/22 06:17:25 - INFO - root -   Epoch: [295/300][120/200], lr: 0.00000083 	 loss = 0.1076(0.2069)
2023/11/22 06:17:36 - INFO - root -   Epoch: [295/300][140/200], lr: 0.00000083 	 loss = 0.1779(0.1993)
2023/11/22 06:17:51 - INFO - root -   Epoch: [295/300][160/200], lr: 0.00000083 	 loss = 1.5446(0.2084)
2023/11/22 06:18:07 - INFO - root -   Epoch: [295/300][180/200], lr: 0.00000083 	 loss = 0.0002(0.2313)
2023/11/22 06:18:19 - INFO - root -   Epoch: [295/300] 	 loss = 0.2247
2023/11/22 06:18:19 - INFO - root -   train_accuracy = 0.9400
2023/11/22 06:18:20 - INFO - root -   Epoch: [296/300][0/200], lr: 0.00000083 	 loss = 0.0867(0.0867)
2023/11/22 06:18:44 - INFO - root -   Epoch: [296/300][20/200], lr: 0.00000083 	 loss = 1.5678(0.2256)
2023/11/22 06:18:54 - INFO - root -   Epoch: [296/300][40/200], lr: 0.00000083 	 loss = 0.0386(0.2565)
2023/11/22 06:19:02 - INFO - root -   Epoch: [296/300][60/200], lr: 0.00000083 	 loss = 0.0543(0.2394)
2023/11/22 06:19:12 - INFO - root -   Epoch: [296/300][80/200], lr: 0.00000083 	 loss = 0.0039(0.2725)
2023/11/22 06:19:24 - INFO - root -   Epoch: [296/300][100/200], lr: 0.00000083 	 loss = 0.0005(0.2762)
2023/11/22 06:19:44 - INFO - root -   Epoch: [296/300][120/200], lr: 0.00000083 	 loss = 0.1456(0.2641)
2023/11/22 06:19:56 - INFO - root -   Epoch: [296/300][140/200], lr: 0.00000083 	 loss = 0.0120(0.2573)
2023/11/22 06:20:12 - INFO - root -   Epoch: [296/300][160/200], lr: 0.00000083 	 loss = 0.2173(0.2555)
2023/11/22 06:20:32 - INFO - root -   Epoch: [296/300][180/200], lr: 0.00000083 	 loss = 0.0827(0.2503)
2023/11/22 06:20:40 - INFO - root -   Epoch: [296/300] 	 loss = 0.2429
2023/11/22 06:20:40 - INFO - root -   train_accuracy = 0.9250
2023/11/22 06:20:47 - INFO - root -   Epoch: [297/300][0/200], lr: 0.00000084 	 loss = 1.1159(1.1159)
2023/11/22 06:21:03 - INFO - root -   Epoch: [297/300][20/200], lr: 0.00000084 	 loss = 1.2585(0.3452)
2023/11/22 06:21:18 - INFO - root -   Epoch: [297/300][40/200], lr: 0.00000084 	 loss = 0.0636(0.2963)
2023/11/22 06:21:36 - INFO - root -   Epoch: [297/300][60/200], lr: 0.00000084 	 loss = 0.7916(0.2580)
2023/11/22 06:21:48 - INFO - root -   Epoch: [297/300][80/200], lr: 0.00000084 	 loss = 0.0999(0.2360)
2023/11/22 06:21:58 - INFO - root -   Epoch: [297/300][100/200], lr: 0.00000084 	 loss = 0.0020(0.2733)
2023/11/22 06:22:16 - INFO - root -   Epoch: [297/300][120/200], lr: 0.00000084 	 loss = 2.3421(0.2865)
2023/11/22 06:22:30 - INFO - root -   Epoch: [297/300][140/200], lr: 0.00000084 	 loss = 0.0227(0.2797)
2023/11/22 06:22:45 - INFO - root -   Epoch: [297/300][160/200], lr: 0.00000084 	 loss = 0.3521(0.2922)
2023/11/22 06:22:58 - INFO - root -   Epoch: [297/300][180/200], lr: 0.00000084 	 loss = 0.0261(0.2848)
2023/11/22 06:23:09 - INFO - root -   Epoch: [297/300] 	 loss = 0.2755
2023/11/22 06:23:09 - INFO - root -   train_accuracy = 0.9000
2023/11/22 06:23:10 - INFO - root -   Epoch: [298/300][0/200], lr: 0.00000084 	 loss = 0.4386(0.4386)
2023/11/22 06:23:31 - INFO - root -   Epoch: [298/300][20/200], lr: 0.00000084 	 loss = 0.2220(0.2352)
2023/11/22 06:23:44 - INFO - root -   Epoch: [298/300][40/200], lr: 0.00000084 	 loss = 0.4151(0.2666)
2023/11/22 06:24:00 - INFO - root -   Epoch: [298/300][60/200], lr: 0.00000084 	 loss = 0.0025(0.2295)
2023/11/22 06:24:16 - INFO - root -   Epoch: [298/300][80/200], lr: 0.00000084 	 loss = 2.3107(0.2560)
2023/11/22 06:24:32 - INFO - root -   Epoch: [298/300][100/200], lr: 0.00000084 	 loss = 0.0999(0.2460)
2023/11/22 06:24:44 - INFO - root -   Epoch: [298/300][120/200], lr: 0.00000084 	 loss = 0.5997(0.2457)
2023/11/22 06:24:56 - INFO - root -   Epoch: [298/300][140/200], lr: 0.00000084 	 loss = 0.0507(0.2349)
2023/11/22 06:25:15 - INFO - root -   Epoch: [298/300][160/200], lr: 0.00000084 	 loss = 1.4290(0.2418)
2023/11/22 06:25:31 - INFO - root -   Epoch: [298/300][180/200], lr: 0.00000084 	 loss = 0.7522(0.2401)
2023/11/22 06:25:39 - INFO - root -   Epoch: [298/300] 	 loss = 0.2800
2023/11/22 06:25:39 - INFO - root -   train_accuracy = 0.9050
2023/11/22 06:25:39 - INFO - root -   Epoch: [299/300][0/200], lr: 0.00000084 	 loss = 0.1526(0.1526)
2023/11/22 06:26:07 - INFO - root -   Epoch: [299/300][20/200], lr: 0.00000084 	 loss = 1.3749(0.2944)
2023/11/22 06:26:32 - INFO - root -   Epoch: [299/300][40/200], lr: 0.00000084 	 loss = 0.7393(0.3042)
2023/11/22 06:26:42 - INFO - root -   Epoch: [299/300][60/200], lr: 0.00000084 	 loss = 0.0917(0.2634)
2023/11/22 06:27:12 - INFO - root -   Epoch: [299/300][80/200], lr: 0.00000084 	 loss = 1.3204(0.2587)
2023/11/22 06:27:22 - INFO - root -   Epoch: [299/300][100/200], lr: 0.00000084 	 loss = 0.0002(0.2547)
2023/11/22 06:27:38 - INFO - root -   Epoch: [299/300][120/200], lr: 0.00000084 	 loss = 0.0819(0.2459)
2023/11/22 06:27:58 - INFO - root -   Epoch: [299/300][140/200], lr: 0.00000084 	 loss = 0.0801(0.2392)
2023/11/22 06:28:13 - INFO - root -   Epoch: [299/300][160/200], lr: 0.00000084 	 loss = 1.0070(0.2451)
2023/11/22 06:28:21 - INFO - root -   Epoch: [299/300][180/200], lr: 0.00000084 	 loss = 0.0059(0.2243)
2023/11/22 06:28:28 - INFO - root -   Epoch: [299/300] 	 loss = 0.2234
2023/11/22 06:30:29 - INFO - root -   precision = 0.8515
2023/11/22 06:30:29 - INFO - root -   eval_loss = 0.4635
2023/11/22 06:30:29 - INFO - root -   eval_acc = 0.8515
2023/11/22 06:30:30 - INFO - root -   train_accuracy = 0.9300
