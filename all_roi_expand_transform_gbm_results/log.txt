2023/11/08 17:23:28 - INFO - root -   Num train examples = 682
2023/11/08 17:23:28 - INFO - root -   Num val examples = 171
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 17:23:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 17:23:28 - INFO - root -   backend = nccl
2023/11/08 17:23:28 - INFO - root -   batch_size = 4
2023/11/08 17:23:28 - INFO - root -   dropout = 0.5
2023/11/08 17:23:28 - INFO - root -   epochs = 300
2023/11/08 17:23:28 - INFO - root -   eval_freq = 5
2023/11/08 17:23:28 - INFO - root -   focal_loss = False
2023/11/08 17:23:28 - INFO - root -   input_size = 224
2023/11/08 17:23:28 - INFO - root -   is_pretrained = False
2023/11/08 17:23:28 - INFO - root -   label_smooth = False
2023/11/08 17:23:28 - INFO - root -   local_rank = -1
2023/11/08 17:23:28 - INFO - root -   lr = 1e-05
2023/11/08 17:23:28 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 17:23:28 - INFO - root -   lr_steps = [50, 100]
2023/11/08 17:23:28 - INFO - root -   lr_type = cosine
2023/11/08 17:23:28 - INFO - root -   model_depth = 34
2023/11/08 17:23:28 - INFO - root -   model_name = resnet50
2023/11/08 17:23:28 - INFO - root -   momentum = 0.9
2023/11/08 17:23:28 - INFO - root -   num_classes = 2
2023/11/08 17:23:28 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 17:23:28 - INFO - root -   print_freq = 20
2023/11/08 17:23:28 - INFO - root -   resume = 
2023/11/08 17:23:28 - INFO - root -   start_epoch = 0
2023/11/08 17:23:28 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 17:23:28 - INFO - root -   tune_from = 
2023/11/08 17:23:28 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 17:23:28 - INFO - root -   warmup_epoch = 20
2023/11/08 17:23:28 - INFO - root -   warmup_multiplier = 100
2023/11/08 17:23:28 - INFO - root -   weight_decay = 0.0005
2023/11/08 17:23:28 - INFO - root -   workers = 8
2023/11/08 17:46:18 - INFO - root -   Num train examples = 682
2023/11/08 17:46:18 - INFO - root -   Num val examples = 171
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 17:46:18 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 17:46:18 - INFO - root -   backend = nccl
2023/11/08 17:46:18 - INFO - root -   batch_size = 2
2023/11/08 17:46:18 - INFO - root -   dropout = 0.5
2023/11/08 17:46:18 - INFO - root -   epochs = 300
2023/11/08 17:46:18 - INFO - root -   eval_freq = 5
2023/11/08 17:46:18 - INFO - root -   focal_loss = False
2023/11/08 17:46:18 - INFO - root -   input_size = 224
2023/11/08 17:46:18 - INFO - root -   is_pretrained = False
2023/11/08 17:46:18 - INFO - root -   label_smooth = False
2023/11/08 17:46:18 - INFO - root -   local_rank = -1
2023/11/08 17:46:18 - INFO - root -   lr = 1e-05
2023/11/08 17:46:18 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 17:46:18 - INFO - root -   lr_steps = [50, 100]
2023/11/08 17:46:18 - INFO - root -   lr_type = cosine
2023/11/08 17:46:18 - INFO - root -   model_depth = 34
2023/11/08 17:46:18 - INFO - root -   model_name = resnet50
2023/11/08 17:46:18 - INFO - root -   momentum = 0.9
2023/11/08 17:46:18 - INFO - root -   num_classes = 2
2023/11/08 17:46:18 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 17:46:18 - INFO - root -   print_freq = 20
2023/11/08 17:46:18 - INFO - root -   resume = 
2023/11/08 17:46:18 - INFO - root -   start_epoch = 0
2023/11/08 17:46:18 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 17:46:18 - INFO - root -   tune_from = 
2023/11/08 17:46:18 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 17:46:18 - INFO - root -   warmup_epoch = 20
2023/11/08 17:46:18 - INFO - root -   warmup_multiplier = 100
2023/11/08 17:46:18 - INFO - root -   weight_decay = 0.0005
2023/11/08 17:46:18 - INFO - root -   workers = 8
2023/11/08 17:46:46 - INFO - root -   Epoch: [0/300][0/341], lr: 0.00000010 	 loss = 0.9502(0.9502)
2023/11/08 17:47:24 - INFO - root -   Num train examples = 682
2023/11/08 17:47:24 - INFO - root -   Num val examples = 171
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:24 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 17:47:25 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 17:47:25 - INFO - root -   backend = nccl
2023/11/08 17:47:25 - INFO - root -   batch_size = 4
2023/11/08 17:47:25 - INFO - root -   dropout = 0.5
2023/11/08 17:47:25 - INFO - root -   epochs = 300
2023/11/08 17:47:25 - INFO - root -   eval_freq = 5
2023/11/08 17:47:25 - INFO - root -   focal_loss = False
2023/11/08 17:47:25 - INFO - root -   input_size = 224
2023/11/08 17:47:25 - INFO - root -   is_pretrained = False
2023/11/08 17:47:25 - INFO - root -   label_smooth = False
2023/11/08 17:47:25 - INFO - root -   local_rank = -1
2023/11/08 17:47:25 - INFO - root -   lr = 1e-05
2023/11/08 17:47:25 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 17:47:25 - INFO - root -   lr_steps = [50, 100]
2023/11/08 17:47:25 - INFO - root -   lr_type = cosine
2023/11/08 17:47:25 - INFO - root -   model_depth = 34
2023/11/08 17:47:25 - INFO - root -   model_name = resnet50
2023/11/08 17:47:25 - INFO - root -   momentum = 0.9
2023/11/08 17:47:25 - INFO - root -   num_classes = 2
2023/11/08 17:47:25 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 17:47:25 - INFO - root -   print_freq = 20
2023/11/08 17:47:25 - INFO - root -   resume = 
2023/11/08 17:47:25 - INFO - root -   start_epoch = 0
2023/11/08 17:47:25 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 17:47:25 - INFO - root -   tune_from = 
2023/11/08 17:47:25 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 17:47:25 - INFO - root -   warmup_epoch = 20
2023/11/08 17:47:25 - INFO - root -   warmup_multiplier = 100
2023/11/08 17:47:25 - INFO - root -   weight_decay = 0.0005
2023/11/08 17:47:25 - INFO - root -   workers = 8
2023/11/08 17:48:10 - INFO - root -   Epoch: [0/300][0/170], lr: 0.00000010 	 loss = 0.7000(0.7000)
2023/11/08 17:50:17 - INFO - root -   Epoch: [0/300][20/170], lr: 0.00000010 	 loss = 0.7900(0.8018)
2023/11/08 17:52:33 - INFO - root -   Epoch: [0/300][40/170], lr: 0.00000010 	 loss = 0.6318(0.8473)
2023/11/08 17:54:55 - INFO - root -   Epoch: [0/300][60/170], lr: 0.00000010 	 loss = 0.5772(0.8134)
2023/11/08 17:57:22 - INFO - root -   Epoch: [0/300][80/170], lr: 0.00000010 	 loss = 0.7874(0.8283)
2023/11/08 17:59:26 - INFO - root -   Epoch: [0/300][100/170], lr: 0.00000010 	 loss = 0.6588(0.8158)
2023/11/08 18:01:58 - INFO - root -   Epoch: [0/300][120/170], lr: 0.00000010 	 loss = 0.4727(0.8209)
2023/11/08 18:04:07 - INFO - root -   Epoch: [0/300][140/170], lr: 0.00000010 	 loss = 0.7241(0.8089)
2023/11/08 18:06:02 - INFO - root -   Epoch: [0/300][160/170], lr: 0.00000010 	 loss = 0.3734(0.8060)
2023/11/08 18:06:41 - INFO - root -   Epoch: [0/300] 	 loss = 0.7962
2023/11/08 18:07:40 - INFO - root -   Epoch: [1/300][0/170], lr: 0.00000010 	 loss = 0.6552(0.6552)
2023/11/08 18:09:44 - INFO - root -   Epoch: [1/300][20/170], lr: 0.00000010 	 loss = 0.5228(0.7300)
2023/11/08 18:12:13 - INFO - root -   Epoch: [1/300][40/170], lr: 0.00000010 	 loss = 1.0634(0.7127)
2023/11/08 18:14:58 - INFO - root -   Epoch: [1/300][60/170], lr: 0.00000010 	 loss = 1.0810(0.7214)
2023/11/08 18:16:33 - INFO - root -   Epoch: [1/300][80/170], lr: 0.00000010 	 loss = 0.7138(0.7184)
2023/11/08 18:19:23 - INFO - root -   Epoch: [1/300][100/170], lr: 0.00000010 	 loss = 0.8765(0.7382)
2023/11/08 18:21:07 - INFO - root -   Epoch: [1/300][120/170], lr: 0.00000010 	 loss = 0.6783(0.7445)
2023/11/08 18:23:49 - INFO - root -   Epoch: [1/300][140/170], lr: 0.00000010 	 loss = 0.8009(0.7523)
2023/11/08 18:25:42 - INFO - root -   Epoch: [1/300][160/170], lr: 0.00000010 	 loss = 0.6277(0.7613)
2023/11/08 18:26:10 - INFO - root -   Epoch: [1/300] 	 loss = 0.7664
2023/11/08 18:27:10 - INFO - root -   Epoch: [2/300][0/170], lr: 0.00000011 	 loss = 1.1382(1.1382)
2023/11/08 18:29:01 - INFO - root -   Epoch: [2/300][20/170], lr: 0.00000011 	 loss = 0.6412(0.7658)
2023/11/08 18:31:19 - INFO - root -   Epoch: [2/300][40/170], lr: 0.00000011 	 loss = 0.7022(0.7826)
2023/11/08 18:33:47 - INFO - root -   Epoch: [2/300][60/170], lr: 0.00000011 	 loss = 0.8441(0.7949)
2023/11/08 18:35:49 - INFO - root -   Epoch: [2/300][80/170], lr: 0.00000011 	 loss = 0.4088(0.7935)
2023/11/08 18:38:20 - INFO - root -   Epoch: [2/300][100/170], lr: 0.00000011 	 loss = 1.0327(0.7858)
2023/11/08 18:40:22 - INFO - root -   Epoch: [2/300][120/170], lr: 0.00000011 	 loss = 0.4717(0.7796)
2023/11/08 18:42:54 - INFO - root -   Epoch: [2/300][140/170], lr: 0.00000011 	 loss = 0.4390(0.7789)
2023/11/08 18:44:53 - INFO - root -   Epoch: [2/300][160/170], lr: 0.00000011 	 loss = 0.8445(0.7791)
2023/11/08 18:45:17 - INFO - root -   Epoch: [2/300] 	 loss = 0.7838
2023/11/08 18:46:26 - INFO - root -   Epoch: [3/300][0/170], lr: 0.00000011 	 loss = 0.6927(0.6927)
2023/11/08 18:48:16 - INFO - root -   Epoch: [3/300][20/170], lr: 0.00000011 	 loss = 0.9523(0.8410)
2023/11/08 18:51:14 - INFO - root -   Epoch: [3/300][40/170], lr: 0.00000011 	 loss = 0.7659(0.8107)
2023/11/08 18:52:49 - INFO - root -   Epoch: [3/300][60/170], lr: 0.00000011 	 loss = 0.9052(0.7851)
2023/11/08 18:55:03 - INFO - root -   Epoch: [3/300][80/170], lr: 0.00000011 	 loss = 0.5492(0.7621)
2023/11/08 18:57:02 - INFO - root -   Epoch: [3/300][100/170], lr: 0.00000011 	 loss = 1.1757(0.7839)
2023/11/08 18:59:40 - INFO - root -   Epoch: [3/300][120/170], lr: 0.00000011 	 loss = 1.0252(0.7921)
2023/11/08 19:01:39 - INFO - root -   Epoch: [3/300][140/170], lr: 0.00000011 	 loss = 0.5607(0.7883)
2023/11/08 19:03:54 - INFO - root -   Epoch: [3/300][160/170], lr: 0.00000011 	 loss = 0.7836(0.7896)
2023/11/08 19:04:08 - INFO - root -   Epoch: [3/300] 	 loss = 0.7946
2023/11/08 19:05:07 - INFO - root -   Epoch: [4/300][0/170], lr: 0.00000011 	 loss = 0.6504(0.6504)
2023/11/08 19:07:15 - INFO - root -   Epoch: [4/300][20/170], lr: 0.00000011 	 loss = 1.0017(0.8055)
2023/11/08 19:09:42 - INFO - root -   Epoch: [4/300][40/170], lr: 0.00000011 	 loss = 0.6292(0.7716)
2023/11/08 19:11:54 - INFO - root -   Epoch: [4/300][60/170], lr: 0.00000011 	 loss = 0.9736(0.7879)
2023/11/08 19:14:50 - INFO - root -   Epoch: [4/300][80/170], lr: 0.00000011 	 loss = 0.6776(0.7653)
2023/11/08 19:16:34 - INFO - root -   Epoch: [4/300][100/170], lr: 0.00000011 	 loss = 0.5431(0.7773)
2023/11/08 19:19:20 - INFO - root -   Epoch: [4/300][120/170], lr: 0.00000011 	 loss = 0.5679(0.7742)
2023/11/08 19:21:09 - INFO - root -   Epoch: [4/300][140/170], lr: 0.00000011 	 loss = 0.4965(0.7666)
2023/11/08 19:23:12 - INFO - root -   Epoch: [4/300][160/170], lr: 0.00000011 	 loss = 0.8045(0.7630)
2023/11/08 19:23:19 - INFO - root -   Epoch: [4/300] 	 loss = 0.7631
2023/11/08 19:27:31 - INFO - root -   precision = 0.6199
2023/11/08 19:27:31 - INFO - root -   eval_loss = 0.6609
2023/11/08 19:27:31 - INFO - root -   eval_acc = 0.6199
2023/11/08 19:27:32 - INFO - root -   train_accuracy = 0.5206
2023/11/08 19:28:23 - INFO - root -   Epoch: [5/300][0/170], lr: 0.00000011 	 loss = 0.4434(0.4434)
2023/11/08 19:30:43 - INFO - root -   Epoch: [5/300][20/170], lr: 0.00000011 	 loss = 0.5787(0.7643)
2023/11/08 19:32:48 - INFO - root -   Epoch: [5/300][40/170], lr: 0.00000011 	 loss = 0.9141(0.7771)
2023/11/08 19:35:17 - INFO - root -   Epoch: [5/300][60/170], lr: 0.00000011 	 loss = 1.0165(0.7401)
2023/11/08 19:37:16 - INFO - root -   Epoch: [5/300][80/170], lr: 0.00000011 	 loss = 0.4171(0.7197)
2023/11/08 19:39:38 - INFO - root -   Epoch: [5/300][100/170], lr: 0.00000011 	 loss = 0.5952(0.7186)
2023/11/08 19:41:10 - INFO - root -   Epoch: [5/300][120/170], lr: 0.00000011 	 loss = 0.5665(0.7299)
2023/11/08 19:43:45 - INFO - root -   Epoch: [5/300][140/170], lr: 0.00000011 	 loss = 0.6807(0.7308)
2023/11/08 19:45:41 - INFO - root -   Epoch: [5/300][160/170], lr: 0.00000011 	 loss = 0.8224(0.7368)
2023/11/08 19:46:01 - INFO - root -   Epoch: [5/300] 	 loss = 0.7375
2023/11/08 19:46:53 - INFO - root -   Epoch: [6/300][0/170], lr: 0.00000012 	 loss = 0.8605(0.8605)
2023/11/08 19:48:42 - INFO - root -   Epoch: [6/300][20/170], lr: 0.00000012 	 loss = 0.8813(0.7820)
2023/11/08 19:51:14 - INFO - root -   Epoch: [6/300][40/170], lr: 0.00000012 	 loss = 0.7612(0.8288)
2023/11/08 19:53:27 - INFO - root -   Epoch: [6/300][60/170], lr: 0.00000012 	 loss = 0.8307(0.7920)
2023/11/08 19:56:03 - INFO - root -   Epoch: [6/300][80/170], lr: 0.00000012 	 loss = 0.3863(0.7574)
2023/11/08 19:57:58 - INFO - root -   Epoch: [6/300][100/170], lr: 0.00000012 	 loss = 0.6910(0.7598)
2023/11/08 20:00:27 - INFO - root -   Epoch: [6/300][120/170], lr: 0.00000012 	 loss = 0.4749(0.7668)
2023/11/08 20:02:14 - INFO - root -   Epoch: [6/300][140/170], lr: 0.00000012 	 loss = 0.7947(0.7525)
2023/11/08 20:04:37 - INFO - root -   Epoch: [6/300][160/170], lr: 0.00000012 	 loss = 0.6219(0.7535)
2023/11/08 20:04:47 - INFO - root -   Epoch: [6/300] 	 loss = 0.7464
2023/11/08 20:05:38 - INFO - root -   Epoch: [7/300][0/170], lr: 0.00000012 	 loss = 0.8286(0.8286)
2023/11/08 20:07:46 - INFO - root -   Epoch: [7/300][20/170], lr: 0.00000012 	 loss = 0.4048(0.7491)
2023/11/08 20:08:05 - INFO - root -   Num train examples = 682
2023/11/08 20:08:05 - INFO - root -   Num val examples = 171
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 20:08:05 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 20:08:05 - INFO - root -   backend = nccl
2023/11/08 20:08:05 - INFO - root -   batch_size = 4
2023/11/08 20:08:05 - INFO - root -   dropout = 0.5
2023/11/08 20:08:05 - INFO - root -   epochs = 300
2023/11/08 20:08:05 - INFO - root -   eval_freq = 5
2023/11/08 20:08:05 - INFO - root -   focal_loss = False
2023/11/08 20:08:05 - INFO - root -   input_size = 224
2023/11/08 20:08:05 - INFO - root -   is_pretrained = False
2023/11/08 20:08:05 - INFO - root -   label_smooth = False
2023/11/08 20:08:05 - INFO - root -   local_rank = -1
2023/11/08 20:08:05 - INFO - root -   lr = 1e-05
2023/11/08 20:08:05 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 20:08:05 - INFO - root -   lr_steps = [50, 100]
2023/11/08 20:08:05 - INFO - root -   lr_type = cosine
2023/11/08 20:08:05 - INFO - root -   model_depth = 34
2023/11/08 20:08:05 - INFO - root -   model_name = resnet50
2023/11/08 20:08:05 - INFO - root -   momentum = 0.9
2023/11/08 20:08:05 - INFO - root -   num_classes = 2
2023/11/08 20:08:05 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 20:08:05 - INFO - root -   print_freq = 20
2023/11/08 20:08:05 - INFO - root -   resume = 
2023/11/08 20:08:05 - INFO - root -   start_epoch = 0
2023/11/08 20:08:05 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 20:08:05 - INFO - root -   tune_from = 
2023/11/08 20:08:05 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 20:08:05 - INFO - root -   warmup_epoch = 20
2023/11/08 20:08:05 - INFO - root -   warmup_multiplier = 100
2023/11/08 20:08:05 - INFO - root -   weight_decay = 0.0005
2023/11/08 20:08:05 - INFO - root -   workers = 8
2023/11/08 20:13:54 - INFO - root -   Num train examples = 682
2023/11/08 20:13:54 - INFO - root -   Num val examples = 171
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 20:13:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 20:13:55 - INFO - root -   backend = nccl
2023/11/08 20:13:55 - INFO - root -   batch_size = 4
2023/11/08 20:13:55 - INFO - root -   dropout = 0.5
2023/11/08 20:13:55 - INFO - root -   epochs = 300
2023/11/08 20:13:55 - INFO - root -   eval_freq = 5
2023/11/08 20:13:55 - INFO - root -   focal_loss = False
2023/11/08 20:13:55 - INFO - root -   input_size = 224
2023/11/08 20:13:55 - INFO - root -   is_pretrained = False
2023/11/08 20:13:55 - INFO - root -   label_smooth = False
2023/11/08 20:13:55 - INFO - root -   local_rank = -1
2023/11/08 20:13:55 - INFO - root -   lr = 1e-05
2023/11/08 20:13:55 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 20:13:55 - INFO - root -   lr_steps = [50, 100]
2023/11/08 20:13:55 - INFO - root -   lr_type = cosine
2023/11/08 20:13:55 - INFO - root -   model_depth = 34
2023/11/08 20:13:55 - INFO - root -   model_name = resnet50
2023/11/08 20:13:55 - INFO - root -   momentum = 0.9
2023/11/08 20:13:55 - INFO - root -   num_classes = 2
2023/11/08 20:13:55 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 20:13:55 - INFO - root -   print_freq = 20
2023/11/08 20:13:55 - INFO - root -   resume = 
2023/11/08 20:13:55 - INFO - root -   start_epoch = 0
2023/11/08 20:13:55 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 20:13:55 - INFO - root -   tune_from = 
2023/11/08 20:13:55 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 20:13:55 - INFO - root -   warmup_epoch = 20
2023/11/08 20:13:55 - INFO - root -   warmup_multiplier = 100
2023/11/08 20:13:55 - INFO - root -   weight_decay = 0.0005
2023/11/08 20:13:55 - INFO - root -   workers = 8
2023/11/08 20:14:40 - INFO - root -   Epoch: [0/300][0/170], lr: 0.00000010 	 loss = 0.6847(0.6847)
2023/11/08 20:17:09 - INFO - root -   Epoch: [0/300][20/170], lr: 0.00000010 	 loss = 0.7769(0.8270)
2023/11/08 20:19:06 - INFO - root -   Epoch: [0/300][40/170], lr: 0.00000010 	 loss = 0.4418(0.8173)
2023/11/08 20:22:05 - INFO - root -   Epoch: [0/300][60/170], lr: 0.00000010 	 loss = 0.5983(0.8061)
2023/11/08 20:23:44 - INFO - root -   Epoch: [0/300][80/170], lr: 0.00000010 	 loss = 1.0215(0.7961)
2023/11/08 20:24:02 - INFO - root -   Num train examples = 682
2023/11/08 20:24:02 - INFO - root -   Num val examples = 171
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 20:24:02 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 20:24:03 - INFO - root -   backend = nccl
2023/11/08 20:24:03 - INFO - root -   batch_size = 4
2023/11/08 20:24:03 - INFO - root -   dropout = 0.5
2023/11/08 20:24:03 - INFO - root -   epochs = 300
2023/11/08 20:24:03 - INFO - root -   eval_freq = 5
2023/11/08 20:24:03 - INFO - root -   focal_loss = False
2023/11/08 20:24:03 - INFO - root -   input_size = 224
2023/11/08 20:24:03 - INFO - root -   is_pretrained = False
2023/11/08 20:24:03 - INFO - root -   label_smooth = False
2023/11/08 20:24:03 - INFO - root -   local_rank = -1
2023/11/08 20:24:03 - INFO - root -   lr = 1e-05
2023/11/08 20:24:03 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 20:24:03 - INFO - root -   lr_steps = [50, 100]
2023/11/08 20:24:03 - INFO - root -   lr_type = cosine
2023/11/08 20:24:03 - INFO - root -   model_depth = 34
2023/11/08 20:24:03 - INFO - root -   model_name = resnet50
2023/11/08 20:24:03 - INFO - root -   momentum = 0.9
2023/11/08 20:24:03 - INFO - root -   num_classes = 2
2023/11/08 20:24:03 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 20:24:03 - INFO - root -   print_freq = 20
2023/11/08 20:24:03 - INFO - root -   resume = 
2023/11/08 20:24:03 - INFO - root -   start_epoch = 0
2023/11/08 20:24:03 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 20:24:03 - INFO - root -   tune_from = 
2023/11/08 20:24:03 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 20:24:03 - INFO - root -   warmup_epoch = 20
2023/11/08 20:24:03 - INFO - root -   warmup_multiplier = 100
2023/11/08 20:24:03 - INFO - root -   weight_decay = 0.0005
2023/11/08 20:24:03 - INFO - root -   workers = 8
2023/11/08 20:27:07 - INFO - root -   Num train examples = 682
2023/11/08 20:27:07 - INFO - root -   Num val examples = 171
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 20:27:07 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 20:27:08 - INFO - root -   backend = nccl
2023/11/08 20:27:08 - INFO - root -   batch_size = 4
2023/11/08 20:27:08 - INFO - root -   dropout = 0.5
2023/11/08 20:27:08 - INFO - root -   epochs = 300
2023/11/08 20:27:08 - INFO - root -   eval_freq = 5
2023/11/08 20:27:08 - INFO - root -   focal_loss = False
2023/11/08 20:27:08 - INFO - root -   input_size = 224
2023/11/08 20:27:08 - INFO - root -   is_pretrained = False
2023/11/08 20:27:08 - INFO - root -   label_smooth = False
2023/11/08 20:27:08 - INFO - root -   local_rank = -1
2023/11/08 20:27:08 - INFO - root -   lr = 1e-05
2023/11/08 20:27:08 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 20:27:08 - INFO - root -   lr_steps = [50, 100]
2023/11/08 20:27:08 - INFO - root -   lr_type = cosine
2023/11/08 20:27:08 - INFO - root -   model_depth = 34
2023/11/08 20:27:08 - INFO - root -   model_name = resnet50
2023/11/08 20:27:08 - INFO - root -   momentum = 0.9
2023/11/08 20:27:08 - INFO - root -   num_classes = 2
2023/11/08 20:27:08 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 20:27:08 - INFO - root -   print_freq = 20
2023/11/08 20:27:08 - INFO - root -   resume = 
2023/11/08 20:27:08 - INFO - root -   start_epoch = 0
2023/11/08 20:27:08 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 20:27:08 - INFO - root -   tune_from = 
2023/11/08 20:27:08 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 20:27:08 - INFO - root -   warmup_epoch = 20
2023/11/08 20:27:08 - INFO - root -   warmup_multiplier = 100
2023/11/08 20:27:08 - INFO - root -   weight_decay = 0.0005
2023/11/08 20:27:08 - INFO - root -   workers = 8
2023/11/08 20:28:28 - INFO - root -   Num train examples = 682
2023/11/08 20:28:28 - INFO - root -   Num val examples = 171
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/08 20:28:28 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/08 20:28:28 - INFO - root -   backend = nccl
2023/11/08 20:28:28 - INFO - root -   batch_size = 2
2023/11/08 20:28:28 - INFO - root -   dropout = 0.5
2023/11/08 20:28:28 - INFO - root -   epochs = 300
2023/11/08 20:28:28 - INFO - root -   eval_freq = 5
2023/11/08 20:28:28 - INFO - root -   focal_loss = False
2023/11/08 20:28:28 - INFO - root -   input_size = 224
2023/11/08 20:28:28 - INFO - root -   is_pretrained = False
2023/11/08 20:28:28 - INFO - root -   label_smooth = False
2023/11/08 20:28:28 - INFO - root -   local_rank = -1
2023/11/08 20:28:28 - INFO - root -   lr = 1e-05
2023/11/08 20:28:28 - INFO - root -   lr_decay_rate = 0.1
2023/11/08 20:28:28 - INFO - root -   lr_steps = [50, 100]
2023/11/08 20:28:28 - INFO - root -   lr_type = cosine
2023/11/08 20:28:28 - INFO - root -   model_depth = 34
2023/11/08 20:28:28 - INFO - root -   model_name = resnet50
2023/11/08 20:28:28 - INFO - root -   momentum = 0.9
2023/11/08 20:28:28 - INFO - root -   num_classes = 2
2023/11/08 20:28:28 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/08 20:28:28 - INFO - root -   print_freq = 20
2023/11/08 20:28:28 - INFO - root -   resume = 
2023/11/08 20:28:28 - INFO - root -   start_epoch = 0
2023/11/08 20:28:28 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/train_patients.txt
2023/11/08 20:28:28 - INFO - root -   tune_from = 
2023/11/08 20:28:28 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/test_patients.txt
2023/11/08 20:28:28 - INFO - root -   warmup_epoch = 20
2023/11/08 20:28:28 - INFO - root -   warmup_multiplier = 100
2023/11/08 20:28:28 - INFO - root -   weight_decay = 0.0005
2023/11/08 20:28:28 - INFO - root -   workers = 8
2023/11/08 20:28:52 - INFO - root -   Epoch: [0/300][0/341], lr: 0.00000010 	 loss = 1.4295(1.4295)
2023/11/08 20:29:55 - INFO - root -   Epoch: [0/300][20/341], lr: 0.00000010 	 loss = 0.6370(0.8485)
2023/11/08 20:31:03 - INFO - root -   Epoch: [0/300][40/341], lr: 0.00000010 	 loss = 0.4682(0.8088)
2023/11/08 20:32:19 - INFO - root -   Epoch: [0/300][60/341], lr: 0.00000010 	 loss = 0.9047(0.8235)
2023/11/08 20:33:26 - INFO - root -   Epoch: [0/300][80/341], lr: 0.00000010 	 loss = 1.1295(0.8388)
2023/11/08 20:34:45 - INFO - root -   Epoch: [0/300][100/341], lr: 0.00000010 	 loss = 0.3323(0.8483)
2023/11/08 20:35:39 - INFO - root -   Epoch: [0/300][120/341], lr: 0.00000010 	 loss = 0.4142(0.8280)
2023/11/08 20:36:54 - INFO - root -   Epoch: [0/300][140/341], lr: 0.00000010 	 loss = 1.0345(0.8282)
2023/11/08 20:37:44 - INFO - root -   Epoch: [0/300][160/341], lr: 0.00000010 	 loss = 0.2588(0.8388)
2023/11/08 20:39:04 - INFO - root -   Epoch: [0/300][180/341], lr: 0.00000010 	 loss = 0.4539(0.8330)
2023/11/08 20:39:56 - INFO - root -   Epoch: [0/300][200/341], lr: 0.00000010 	 loss = 0.5694(0.8252)
2023/11/08 20:41:07 - INFO - root -   Epoch: [0/300][220/341], lr: 0.00000010 	 loss = 0.4876(0.8180)
2023/11/08 20:42:10 - INFO - root -   Epoch: [0/300][240/341], lr: 0.00000010 	 loss = 1.1196(0.8241)
2023/11/08 20:43:25 - INFO - root -   Epoch: [0/300][260/341], lr: 0.00000010 	 loss = 0.4052(0.8172)
2023/11/08 20:44:13 - INFO - root -   Epoch: [0/300][280/341], lr: 0.00000010 	 loss = 0.8650(0.8115)
2023/11/08 20:45:38 - INFO - root -   Epoch: [0/300][300/341], lr: 0.00000010 	 loss = 1.0149(0.8028)
2023/11/08 20:46:31 - INFO - root -   Epoch: [0/300][320/341], lr: 0.00000010 	 loss = 0.3263(0.7903)
2023/11/08 20:47:22 - INFO - root -   Epoch: [0/300][340/341], lr: 0.00000010 	 loss = 0.5962(0.7930)
2023/11/08 20:47:22 - INFO - root -   Epoch: [0/300] 	 loss = 0.7930
2023/11/08 20:47:22 - INFO - root -   train_accuracy = 0.5044
2023/11/08 20:47:44 - INFO - root -   Epoch: [1/300][0/341], lr: 0.00000010 	 loss = 0.4376(0.4376)
2023/11/08 20:49:01 - INFO - root -   Epoch: [1/300][20/341], lr: 0.00000010 	 loss = 0.9195(0.6907)
2023/11/08 20:49:58 - INFO - root -   Epoch: [1/300][40/341], lr: 0.00000010 	 loss = 0.8480(0.7621)
2023/11/08 20:51:22 - INFO - root -   Epoch: [1/300][60/341], lr: 0.00000010 	 loss = 0.7983(0.7512)
2023/11/08 20:52:28 - INFO - root -   Epoch: [1/300][80/341], lr: 0.00000010 	 loss = 0.5195(0.7575)
2023/11/08 20:53:38 - INFO - root -   Epoch: [1/300][100/341], lr: 0.00000010 	 loss = 0.5424(0.7673)
2023/11/08 20:54:27 - INFO - root -   Epoch: [1/300][120/341], lr: 0.00000010 	 loss = 0.4593(0.7477)
2023/11/08 20:55:45 - INFO - root -   Epoch: [1/300][140/341], lr: 0.00000010 	 loss = 0.5999(0.7574)
2023/11/08 20:56:44 - INFO - root -   Epoch: [1/300][160/341], lr: 0.00000010 	 loss = 0.6657(0.7466)
2023/11/08 20:58:00 - INFO - root -   Epoch: [1/300][180/341], lr: 0.00000010 	 loss = 0.4165(0.7341)
2023/11/08 20:58:59 - INFO - root -   Epoch: [1/300][200/341], lr: 0.00000010 	 loss = 0.6519(0.7376)
2023/11/08 21:00:27 - INFO - root -   Epoch: [1/300][220/341], lr: 0.00000010 	 loss = 0.8548(0.7529)
2023/11/08 21:01:22 - INFO - root -   Epoch: [1/300][240/341], lr: 0.00000010 	 loss = 0.5345(0.7435)
2023/11/08 21:02:40 - INFO - root -   Epoch: [1/300][260/341], lr: 0.00000010 	 loss = 0.7746(0.7424)
2023/11/08 21:03:28 - INFO - root -   Epoch: [1/300][280/341], lr: 0.00000010 	 loss = 0.6037(0.7465)
2023/11/08 21:04:42 - INFO - root -   Epoch: [1/300][300/341], lr: 0.00000010 	 loss = 0.6104(0.7475)
2023/11/08 21:05:41 - INFO - root -   Epoch: [1/300][320/341], lr: 0.00000010 	 loss = 0.2264(0.7471)
2023/11/08 21:06:30 - INFO - root -   Epoch: [1/300][340/341], lr: 0.00000010 	 loss = 1.0087(0.7474)
2023/11/08 21:06:30 - INFO - root -   Epoch: [1/300] 	 loss = 0.7474
2023/11/08 21:06:30 - INFO - root -   train_accuracy = 0.5337
2023/11/08 21:07:00 - INFO - root -   Epoch: [2/300][0/341], lr: 0.00000010 	 loss = 0.4184(0.4184)
2023/11/08 21:08:01 - INFO - root -   Epoch: [2/300][20/341], lr: 0.00000010 	 loss = 0.5907(0.8064)
2023/11/08 21:09:09 - INFO - root -   Epoch: [2/300][40/341], lr: 0.00000010 	 loss = 0.3606(0.8254)
2023/11/08 21:10:18 - INFO - root -   Epoch: [2/300][60/341], lr: 0.00000010 	 loss = 1.0914(0.8070)
2023/11/08 21:11:27 - INFO - root -   Epoch: [2/300][80/341], lr: 0.00000010 	 loss = 0.8478(0.8042)
2023/11/08 21:12:19 - INFO - root -   Epoch: [2/300][100/341], lr: 0.00000010 	 loss = 1.3945(0.7909)
2023/11/08 21:13:33 - INFO - root -   Epoch: [2/300][120/341], lr: 0.00000010 	 loss = 0.5695(0.7775)
2023/11/08 21:14:25 - INFO - root -   Epoch: [2/300][140/341], lr: 0.00000010 	 loss = 0.5057(0.7826)
2023/11/08 21:15:53 - INFO - root -   Epoch: [2/300][160/341], lr: 0.00000010 	 loss = 0.4823(0.7681)
2023/11/08 21:16:36 - INFO - root -   Epoch: [2/300][180/341], lr: 0.00000010 	 loss = 0.6373(0.7768)
2023/11/08 21:17:50 - INFO - root -   Epoch: [2/300][200/341], lr: 0.00000010 	 loss = 0.6008(0.7643)
2023/11/08 21:18:49 - INFO - root -   Epoch: [2/300][220/341], lr: 0.00000010 	 loss = 0.5159(0.7712)
2023/11/08 21:20:28 - INFO - root -   Epoch: [2/300][240/341], lr: 0.00000010 	 loss = 0.4457(0.7730)
2023/11/08 21:21:17 - INFO - root -   Epoch: [2/300][260/341], lr: 0.00000010 	 loss = 0.4842(0.7742)
2023/11/08 21:22:18 - INFO - root -   Epoch: [2/300][280/341], lr: 0.00000010 	 loss = 0.3018(0.7765)
2023/11/08 21:23:28 - INFO - root -   Epoch: [2/300][300/341], lr: 0.00000010 	 loss = 0.4867(0.7725)
2023/11/08 21:24:37 - INFO - root -   Epoch: [2/300][320/341], lr: 0.00000010 	 loss = 0.4134(0.7710)
2023/11/08 21:25:18 - INFO - root -   Epoch: [2/300][340/341], lr: 0.00000010 	 loss = 0.5988(0.7730)
2023/11/08 21:25:18 - INFO - root -   Epoch: [2/300] 	 loss = 0.7730
2023/11/08 21:25:18 - INFO - root -   train_accuracy = 0.5015
2023/11/08 21:25:51 - INFO - root -   Epoch: [3/300][0/341], lr: 0.00000010 	 loss = 0.8498(0.8498)
2023/11/08 21:26:54 - INFO - root -   Epoch: [3/300][20/341], lr: 0.00000010 	 loss = 0.7114(0.7717)
2023/11/08 21:28:05 - INFO - root -   Epoch: [3/300][40/341], lr: 0.00000010 	 loss = 1.2535(0.7488)
2023/11/08 21:29:10 - INFO - root -   Epoch: [3/300][60/341], lr: 0.00000010 	 loss = 0.5711(0.7630)
2023/11/08 21:30:37 - INFO - root -   Epoch: [3/300][80/341], lr: 0.00000010 	 loss = 0.2813(0.7228)
2023/11/08 21:31:29 - INFO - root -   Epoch: [3/300][100/341], lr: 0.00000010 	 loss = 0.8229(0.7276)
2023/11/08 21:32:39 - INFO - root -   Epoch: [3/300][120/341], lr: 0.00000010 	 loss = 0.5465(0.7197)
2023/11/08 21:33:45 - INFO - root -   Epoch: [3/300][140/341], lr: 0.00000010 	 loss = 0.2842(0.7329)
2023/11/08 21:34:53 - INFO - root -   Epoch: [3/300][160/341], lr: 0.00000010 	 loss = 0.8093(0.7437)
2023/11/08 21:35:52 - INFO - root -   Epoch: [3/300][180/341], lr: 0.00000010 	 loss = 0.6518(0.7325)
2023/11/08 21:37:12 - INFO - root -   Epoch: [3/300][200/341], lr: 0.00000010 	 loss = 0.1907(0.7353)
2023/11/08 21:37:57 - INFO - root -   Epoch: [3/300][220/341], lr: 0.00000010 	 loss = 0.6177(0.7351)
2023/11/08 21:39:14 - INFO - root -   Epoch: [3/300][240/341], lr: 0.00000010 	 loss = 0.7475(0.7468)
2023/11/08 21:40:09 - INFO - root -   Epoch: [3/300][260/341], lr: 0.00000010 	 loss = 0.5788(0.7434)
2023/11/08 21:41:36 - INFO - root -   Epoch: [3/300][280/341], lr: 0.00000010 	 loss = 0.9242(0.7447)
2023/11/08 21:42:27 - INFO - root -   Epoch: [3/300][300/341], lr: 0.00000010 	 loss = 0.6531(0.7494)
2023/11/08 21:43:49 - INFO - root -   Epoch: [3/300][320/341], lr: 0.00000010 	 loss = 1.0754(0.7555)
2023/11/08 21:44:30 - INFO - root -   Epoch: [3/300][340/341], lr: 0.00000010 	 loss = 1.0430(0.7585)
2023/11/08 21:44:30 - INFO - root -   Epoch: [3/300] 	 loss = 0.7585
2023/11/08 21:44:30 - INFO - root -   train_accuracy = 0.5220
2023/11/08 21:45:01 - INFO - root -   Epoch: [4/300][0/341], lr: 0.00000011 	 loss = 0.6658(0.6658)
2023/11/08 21:46:15 - INFO - root -   Epoch: [4/300][20/341], lr: 0.00000011 	 loss = 0.5053(0.8833)
2023/11/08 21:47:22 - INFO - root -   Epoch: [4/300][40/341], lr: 0.00000011 	 loss = 0.6694(0.8262)
2023/11/08 21:48:20 - INFO - root -   Epoch: [4/300][60/341], lr: 0.00000011 	 loss = 0.7925(0.7974)
2023/11/08 21:49:29 - INFO - root -   Epoch: [4/300][80/341], lr: 0.00000011 	 loss = 0.7249(0.7790)
2023/11/08 21:50:34 - INFO - root -   Epoch: [4/300][100/341], lr: 0.00000011 	 loss = 0.5868(0.7656)
2023/11/08 21:51:50 - INFO - root -   Epoch: [4/300][120/341], lr: 0.00000011 	 loss = 1.0431(0.7509)
2023/11/08 21:52:47 - INFO - root -   Epoch: [4/300][140/341], lr: 0.00000011 	 loss = 1.0550(0.7763)
2023/11/08 21:54:05 - INFO - root -   Epoch: [4/300][160/341], lr: 0.00000011 	 loss = 0.8033(0.7616)
2023/11/08 21:54:56 - INFO - root -   Epoch: [4/300][180/341], lr: 0.00000011 	 loss = 1.4195(0.7584)
2023/11/08 21:56:25 - INFO - root -   Epoch: [4/300][200/341], lr: 0.00000011 	 loss = 0.5293(0.7507)
2023/11/08 21:57:04 - INFO - root -   Epoch: [4/300][220/341], lr: 0.00000011 	 loss = 0.4888(0.7560)
2023/11/08 21:58:27 - INFO - root -   Epoch: [4/300][240/341], lr: 0.00000011 	 loss = 0.6682(0.7593)
2023/11/08 21:59:32 - INFO - root -   Epoch: [4/300][260/341], lr: 0.00000011 	 loss = 0.5565(0.7585)
2023/11/08 22:00:35 - INFO - root -   Epoch: [4/300][280/341], lr: 0.00000011 	 loss = 0.8325(0.7565)
2023/11/08 22:01:50 - INFO - root -   Epoch: [4/300][300/341], lr: 0.00000011 	 loss = 1.2603(0.7552)
2023/11/08 22:02:48 - INFO - root -   Epoch: [4/300][320/341], lr: 0.00000011 	 loss = 0.9262(0.7541)
2023/11/08 22:03:32 - INFO - root -   Epoch: [4/300][340/341], lr: 0.00000011 	 loss = 0.9779(0.7579)
2023/11/08 22:03:32 - INFO - root -   Epoch: [4/300] 	 loss = 0.7579
2023/11/08 22:07:17 - INFO - root -   precision = 0.5673
2023/11/08 22:07:17 - INFO - root -   eval_loss = 0.6693
2023/11/08 22:07:17 - INFO - root -   eval_acc = 0.5673
2023/11/08 22:07:18 - INFO - root -   train_accuracy = 0.5235
2023/11/08 22:07:40 - INFO - root -   Epoch: [5/300][0/341], lr: 0.00000011 	 loss = 0.8593(0.8593)
2023/11/08 22:09:00 - INFO - root -   Epoch: [5/300][20/341], lr: 0.00000011 	 loss = 0.5676(0.6398)
2023/11/08 22:10:10 - INFO - root -   Epoch: [5/300][40/341], lr: 0.00000011 	 loss = 0.8066(0.6753)
2023/11/08 22:11:23 - INFO - root -   Epoch: [5/300][60/341], lr: 0.00000011 	 loss = 0.7975(0.6809)
2023/11/08 22:12:20 - INFO - root -   Epoch: [5/300][80/341], lr: 0.00000011 	 loss = 0.7167(0.7061)
2023/11/08 22:13:27 - INFO - root -   Epoch: [5/300][100/341], lr: 0.00000011 	 loss = 0.6243(0.7040)
2023/11/08 22:14:22 - INFO - root -   Epoch: [5/300][120/341], lr: 0.00000011 	 loss = 0.4926(0.7142)
2023/11/08 22:15:53 - INFO - root -   Epoch: [5/300][140/341], lr: 0.00000011 	 loss = 0.6705(0.7261)
2023/11/08 22:16:53 - INFO - root -   Epoch: [5/300][160/341], lr: 0.00000011 	 loss = 0.5367(0.7273)
2023/11/08 22:18:12 - INFO - root -   Epoch: [5/300][180/341], lr: 0.00000011 	 loss = 0.4555(0.7238)
2023/11/08 22:18:56 - INFO - root -   Epoch: [5/300][200/341], lr: 0.00000011 	 loss = 0.4495(0.7316)
2023/11/08 22:20:18 - INFO - root -   Epoch: [5/300][220/341], lr: 0.00000011 	 loss = 0.7768(0.7298)
2023/11/08 22:21:07 - INFO - root -   Epoch: [5/300][240/341], lr: 0.00000011 	 loss = 0.5676(0.7268)
2023/11/08 22:22:33 - INFO - root -   Epoch: [5/300][260/341], lr: 0.00000011 	 loss = 0.9027(0.7270)
2023/11/08 22:23:29 - INFO - root -   Epoch: [5/300][280/341], lr: 0.00000011 	 loss = 0.3645(0.7333)
2023/11/08 22:24:52 - INFO - root -   Epoch: [5/300][300/341], lr: 0.00000011 	 loss = 1.1562(0.7387)
2023/11/08 22:25:44 - INFO - root -   Epoch: [5/300][320/341], lr: 0.00000011 	 loss = 1.2352(0.7456)
2023/11/08 22:26:26 - INFO - root -   Epoch: [5/300][340/341], lr: 0.00000011 	 loss = 0.7702(0.7464)
2023/11/08 22:26:26 - INFO - root -   Epoch: [5/300] 	 loss = 0.7464
2023/11/08 22:26:26 - INFO - root -   train_accuracy = 0.5220
2023/11/08 22:26:57 - INFO - root -   Epoch: [6/300][0/341], lr: 0.00000011 	 loss = 0.6008(0.6008)
2023/11/08 22:27:58 - INFO - root -   Epoch: [6/300][20/341], lr: 0.00000011 	 loss = 1.5653(0.8831)
2023/11/08 22:29:20 - INFO - root -   Epoch: [6/300][40/341], lr: 0.00000011 	 loss = 0.6080(0.8038)
2023/11/08 22:30:12 - INFO - root -   Epoch: [6/300][60/341], lr: 0.00000011 	 loss = 0.8488(0.7652)
2023/11/08 22:31:29 - INFO - root -   Epoch: [6/300][80/341], lr: 0.00000011 	 loss = 0.7812(0.8018)
2023/11/08 22:32:30 - INFO - root -   Epoch: [6/300][100/341], lr: 0.00000011 	 loss = 0.7855(0.7735)
2023/11/08 22:33:51 - INFO - root -   Epoch: [6/300][120/341], lr: 0.00000011 	 loss = 0.5257(0.7611)
2023/11/08 22:34:37 - INFO - root -   Epoch: [6/300][140/341], lr: 0.00000011 	 loss = 1.1428(0.7667)
2023/11/08 22:35:57 - INFO - root -   Epoch: [6/300][160/341], lr: 0.00000011 	 loss = 0.3667(0.7693)
2023/11/08 22:36:58 - INFO - root -   Epoch: [6/300][180/341], lr: 0.00000011 	 loss = 1.1245(0.7747)
2023/11/08 22:38:13 - INFO - root -   Epoch: [6/300][200/341], lr: 0.00000011 	 loss = 0.4513(0.7764)
2023/11/08 22:39:13 - INFO - root -   Epoch: [6/300][220/341], lr: 0.00000011 	 loss = 0.5234(0.7699)
2023/11/08 22:40:36 - INFO - root -   Epoch: [6/300][240/341], lr: 0.00000011 	 loss = 0.8161(0.7612)
2023/11/08 22:41:28 - INFO - root -   Epoch: [6/300][260/341], lr: 0.00000011 	 loss = 0.5752(0.7627)
2023/11/08 22:42:32 - INFO - root -   Epoch: [6/300][280/341], lr: 0.00000011 	 loss = 0.8442(0.7648)
2023/11/08 22:43:41 - INFO - root -   Epoch: [6/300][300/341], lr: 0.00000011 	 loss = 0.5979(0.7600)
2023/11/08 22:44:37 - INFO - root -   Epoch: [6/300][320/341], lr: 0.00000011 	 loss = 0.7150(0.7663)
2023/11/08 22:45:33 - INFO - root -   Epoch: [6/300][340/341], lr: 0.00000011 	 loss = 0.7544(0.7685)
2023/11/08 22:45:33 - INFO - root -   Epoch: [6/300] 	 loss = 0.7685
2023/11/08 22:45:33 - INFO - root -   train_accuracy = 0.5367
2023/11/08 22:46:11 - INFO - root -   Epoch: [7/300][0/341], lr: 0.00000011 	 loss = 0.2570(0.2570)
2023/11/08 22:47:04 - INFO - root -   Epoch: [7/300][20/341], lr: 0.00000011 	 loss = 0.8170(0.6044)
2023/11/08 22:48:18 - INFO - root -   Epoch: [7/300][40/341], lr: 0.00000011 	 loss = 1.4932(0.7673)
2023/11/08 22:49:30 - INFO - root -   Epoch: [7/300][60/341], lr: 0.00000011 	 loss = 1.0955(0.7699)
2023/11/08 22:50:29 - INFO - root -   Epoch: [7/300][80/341], lr: 0.00000011 	 loss = 1.1379(0.7831)
2023/11/08 22:51:44 - INFO - root -   Epoch: [7/300][100/341], lr: 0.00000011 	 loss = 0.7672(0.7861)
2023/11/08 22:52:45 - INFO - root -   Epoch: [7/300][120/341], lr: 0.00000011 	 loss = 0.7016(0.7773)
2023/11/08 22:54:04 - INFO - root -   Epoch: [7/300][140/341], lr: 0.00000011 	 loss = 0.3320(0.7788)
2023/11/08 22:54:58 - INFO - root -   Epoch: [7/300][160/341], lr: 0.00000011 	 loss = 0.9855(0.7709)
2023/11/08 22:56:11 - INFO - root -   Epoch: [7/300][180/341], lr: 0.00000011 	 loss = 0.5827(0.7629)
2023/11/08 22:57:43 - INFO - root -   Epoch: [7/300][200/341], lr: 0.00000011 	 loss = 0.5111(0.7632)
2023/11/08 22:58:45 - INFO - root -   Epoch: [7/300][220/341], lr: 0.00000011 	 loss = 0.3633(0.7623)
2023/11/08 22:59:47 - INFO - root -   Epoch: [7/300][240/341], lr: 0.00000011 	 loss = 0.7947(0.7583)
2023/11/08 23:01:05 - INFO - root -   Epoch: [7/300][260/341], lr: 0.00000011 	 loss = 0.3010(0.7589)
2023/11/08 23:02:29 - INFO - root -   Epoch: [7/300][280/341], lr: 0.00000011 	 loss = 0.6770(0.7596)
2023/11/08 23:03:25 - INFO - root -   Epoch: [7/300][300/341], lr: 0.00000011 	 loss = 0.6521(0.7590)
2023/11/08 23:04:27 - INFO - root -   Epoch: [7/300][320/341], lr: 0.00000011 	 loss = 0.3940(0.7619)
2023/11/08 23:05:14 - INFO - root -   Epoch: [7/300][340/341], lr: 0.00000011 	 loss = 0.4663(0.7673)
2023/11/08 23:05:15 - INFO - root -   Epoch: [7/300] 	 loss = 0.7673
2023/11/08 23:05:15 - INFO - root -   train_accuracy = 0.5425
2023/11/08 23:05:45 - INFO - root -   Epoch: [8/300][0/341], lr: 0.00000011 	 loss = 0.9427(0.9427)
2023/11/08 23:06:57 - INFO - root -   Epoch: [8/300][20/341], lr: 0.00000011 	 loss = 0.5621(0.6429)
2023/11/08 23:07:50 - INFO - root -   Epoch: [8/300][40/341], lr: 0.00000011 	 loss = 0.3280(0.7163)
2023/11/08 23:08:51 - INFO - root -   Epoch: [8/300][60/341], lr: 0.00000011 	 loss = 0.3395(0.7057)
2023/11/08 23:10:21 - INFO - root -   Epoch: [8/300][80/341], lr: 0.00000011 	 loss = 0.9861(0.7199)
2023/11/08 23:11:05 - INFO - root -   Epoch: [8/300][100/341], lr: 0.00000011 	 loss = 0.7995(0.7477)
2023/11/08 23:12:20 - INFO - root -   Epoch: [8/300][120/341], lr: 0.00000011 	 loss = 0.4937(0.7412)
2023/11/08 23:13:22 - INFO - root -   Epoch: [8/300][140/341], lr: 0.00000011 	 loss = 0.4101(0.7334)
2023/11/08 23:14:31 - INFO - root -   Epoch: [8/300][160/341], lr: 0.00000011 	 loss = 0.2539(0.7196)
2023/11/08 23:15:49 - INFO - root -   Epoch: [8/300][180/341], lr: 0.00000011 	 loss = 1.0524(0.7225)
2023/11/08 23:16:44 - INFO - root -   Epoch: [8/300][200/341], lr: 0.00000011 	 loss = 0.2654(0.7200)
2023/11/08 23:18:02 - INFO - root -   Epoch: [8/300][220/341], lr: 0.00000011 	 loss = 0.6790(0.7189)
2023/11/08 23:19:05 - INFO - root -   Epoch: [8/300][240/341], lr: 0.00000011 	 loss = 0.8978(0.7224)
2023/11/08 23:20:15 - INFO - root -   Epoch: [8/300][260/341], lr: 0.00000011 	 loss = 0.8495(0.7278)
2023/11/08 23:21:20 - INFO - root -   Epoch: [8/300][280/341], lr: 0.00000011 	 loss = 0.5584(0.7300)
2023/11/08 23:22:34 - INFO - root -   Epoch: [8/300][300/341], lr: 0.00000011 	 loss = 0.4244(0.7240)
2023/11/08 23:23:16 - INFO - root -   Epoch: [8/300][320/341], lr: 0.00000011 	 loss = 0.3384(0.7220)
2023/11/08 23:24:15 - INFO - root -   Epoch: [8/300][340/341], lr: 0.00000011 	 loss = 1.2300(0.7212)
2023/11/08 23:24:15 - INFO - root -   Epoch: [8/300] 	 loss = 0.7212
2023/11/08 23:24:15 - INFO - root -   train_accuracy = 0.5469
2023/11/08 23:24:37 - INFO - root -   Epoch: [9/300][0/341], lr: 0.00000011 	 loss = 0.8960(0.8960)
2023/11/08 23:25:37 - INFO - root -   Epoch: [9/300][20/341], lr: 0.00000011 	 loss = 0.7888(0.6662)
2023/11/08 23:26:38 - INFO - root -   Epoch: [9/300][40/341], lr: 0.00000011 	 loss = 0.8120(0.6944)
2023/11/08 23:27:48 - INFO - root -   Epoch: [9/300][60/341], lr: 0.00000011 	 loss = 0.7113(0.6979)
2023/11/08 23:29:00 - INFO - root -   Epoch: [9/300][80/341], lr: 0.00000011 	 loss = 1.2165(0.7149)
2023/11/08 23:29:54 - INFO - root -   Epoch: [9/300][100/341], lr: 0.00000011 	 loss = 0.6180(0.7075)
2023/11/08 23:31:25 - INFO - root -   Epoch: [9/300][120/341], lr: 0.00000011 	 loss = 0.5309(0.7040)
2023/11/08 23:32:23 - INFO - root -   Epoch: [9/300][140/341], lr: 0.00000011 	 loss = 1.0373(0.7189)
2023/11/08 23:33:33 - INFO - root -   Epoch: [9/300][160/341], lr: 0.00000011 	 loss = 0.6682(0.7082)
2023/11/08 23:34:24 - INFO - root -   Epoch: [9/300][180/341], lr: 0.00000011 	 loss = 0.6403(0.7217)
2023/11/08 23:35:52 - INFO - root -   Epoch: [9/300][200/341], lr: 0.00000011 	 loss = 0.2751(0.7267)
2023/11/08 23:36:40 - INFO - root -   Epoch: [9/300][220/341], lr: 0.00000011 	 loss = 0.3592(0.7298)
2023/11/08 23:37:50 - INFO - root -   Epoch: [9/300][240/341], lr: 0.00000011 	 loss = 0.5982(0.7308)
2023/11/08 23:38:41 - INFO - root -   Epoch: [9/300][260/341], lr: 0.00000011 	 loss = 1.1533(0.7282)
2023/11/08 23:40:12 - INFO - root -   Epoch: [9/300][280/341], lr: 0.00000011 	 loss = 1.0060(0.7298)
2023/11/08 23:41:08 - INFO - root -   Epoch: [9/300][300/341], lr: 0.00000011 	 loss = 0.2005(0.7241)
2023/11/08 23:42:16 - INFO - root -   Epoch: [9/300][320/341], lr: 0.00000011 	 loss = 0.4936(0.7263)
2023/11/08 23:42:53 - INFO - root -   Epoch: [9/300][340/341], lr: 0.00000011 	 loss = 1.3235(0.7334)
2023/11/08 23:42:53 - INFO - root -   Epoch: [9/300] 	 loss = 0.7334
2023/11/08 23:46:38 - INFO - root -   precision = 0.6550
2023/11/08 23:46:38 - INFO - root -   eval_loss = 0.6473
2023/11/08 23:46:38 - INFO - root -   eval_acc = 0.6550
2023/11/08 23:46:39 - INFO - root -   train_accuracy = 0.5455
2023/11/08 23:47:01 - INFO - root -   Epoch: [10/300][0/341], lr: 0.00000011 	 loss = 1.4401(1.4401)
2023/11/08 23:48:13 - INFO - root -   Epoch: [10/300][20/341], lr: 0.00000011 	 loss = 0.5328(0.7358)
2023/11/08 23:49:12 - INFO - root -   Epoch: [10/300][40/341], lr: 0.00000011 	 loss = 1.0219(0.7202)
2023/11/08 23:50:31 - INFO - root -   Epoch: [10/300][60/341], lr: 0.00000011 	 loss = 0.2625(0.7152)
2023/11/08 23:51:19 - INFO - root -   Epoch: [10/300][80/341], lr: 0.00000011 	 loss = 0.8497(0.7150)
2023/11/08 23:52:22 - INFO - root -   Epoch: [10/300][100/341], lr: 0.00000011 	 loss = 1.2008(0.7184)
2023/11/08 23:53:35 - INFO - root -   Epoch: [10/300][120/341], lr: 0.00000011 	 loss = 0.8504(0.7173)
2023/11/08 23:54:39 - INFO - root -   Epoch: [10/300][140/341], lr: 0.00000011 	 loss = 0.7236(0.7206)
2023/11/08 23:55:45 - INFO - root -   Epoch: [10/300][160/341], lr: 0.00000011 	 loss = 0.5125(0.7128)
2023/11/08 23:57:01 - INFO - root -   Epoch: [10/300][180/341], lr: 0.00000011 	 loss = 0.7417(0.7084)
2023/11/08 23:57:53 - INFO - root -   Epoch: [10/300][200/341], lr: 0.00000011 	 loss = 0.5848(0.7041)
2023/11/08 23:59:12 - INFO - root -   Epoch: [10/300][220/341], lr: 0.00000011 	 loss = 0.5015(0.7160)
2023/11/09 00:00:10 - INFO - root -   Epoch: [10/300][240/341], lr: 0.00000011 	 loss = 0.8935(0.7223)
2023/11/09 00:01:25 - INFO - root -   Epoch: [10/300][260/341], lr: 0.00000011 	 loss = 0.4459(0.7209)
2023/11/09 00:02:22 - INFO - root -   Epoch: [10/300][280/341], lr: 0.00000011 	 loss = 0.5370(0.7255)
2023/11/09 00:03:45 - INFO - root -   Epoch: [10/300][300/341], lr: 0.00000011 	 loss = 0.6204(0.7295)
2023/11/09 00:05:06 - INFO - root -   Epoch: [10/300][320/341], lr: 0.00000011 	 loss = 0.2783(0.7315)
2023/11/09 00:05:32 - INFO - root -   Epoch: [10/300][340/341], lr: 0.00000011 	 loss = 1.2998(0.7382)
2023/11/09 00:05:32 - INFO - root -   Epoch: [10/300] 	 loss = 0.7382
2023/11/09 00:05:32 - INFO - root -   train_accuracy = 0.5484
2023/11/09 00:06:02 - INFO - root -   Epoch: [11/300][0/341], lr: 0.00000012 	 loss = 0.7224(0.7224)
2023/11/09 00:07:17 - INFO - root -   Epoch: [11/300][20/341], lr: 0.00000012 	 loss = 0.6966(0.7265)
2023/11/09 00:08:20 - INFO - root -   Epoch: [11/300][40/341], lr: 0.00000012 	 loss = 0.5379(0.7715)
2023/11/09 00:09:22 - INFO - root -   Epoch: [11/300][60/341], lr: 0.00000012 	 loss = 0.5173(0.7565)
2023/11/09 00:10:29 - INFO - root -   Epoch: [11/300][80/341], lr: 0.00000012 	 loss = 0.4228(0.7777)
2023/11/09 00:11:48 - INFO - root -   Epoch: [11/300][100/341], lr: 0.00000012 	 loss = 0.7566(0.7743)
2023/11/09 00:12:50 - INFO - root -   Epoch: [11/300][120/341], lr: 0.00000012 	 loss = 0.8831(0.7631)
2023/11/09 00:13:52 - INFO - root -   Epoch: [11/300][140/341], lr: 0.00000012 	 loss = 0.3291(0.7712)
2023/11/09 00:14:50 - INFO - root -   Epoch: [11/300][160/341], lr: 0.00000012 	 loss = 0.4919(0.7568)
2023/11/09 00:16:06 - INFO - root -   Epoch: [11/300][180/341], lr: 0.00000012 	 loss = 0.5876(0.7533)
2023/11/09 00:17:08 - INFO - root -   Epoch: [11/300][200/341], lr: 0.00000012 	 loss = 0.6497(0.7501)
2023/11/09 00:18:08 - INFO - root -   Epoch: [11/300][220/341], lr: 0.00000012 	 loss = 0.5031(0.7455)
2023/11/09 00:19:32 - INFO - root -   Epoch: [11/300][240/341], lr: 0.00000012 	 loss = 0.4773(0.7386)
2023/11/09 00:20:20 - INFO - root -   Epoch: [11/300][260/341], lr: 0.00000012 	 loss = 0.1910(0.7425)
2023/11/09 00:21:36 - INFO - root -   Epoch: [11/300][280/341], lr: 0.00000012 	 loss = 1.1206(0.7435)
2023/11/09 00:22:52 - INFO - root -   Epoch: [11/300][300/341], lr: 0.00000012 	 loss = 0.7245(0.7440)
2023/11/09 00:24:09 - INFO - root -   Epoch: [11/300][320/341], lr: 0.00000012 	 loss = 0.6400(0.7439)
2023/11/09 00:24:47 - INFO - root -   Epoch: [11/300][340/341], lr: 0.00000012 	 loss = 0.5789(0.7442)
2023/11/09 00:24:47 - INFO - root -   Epoch: [11/300] 	 loss = 0.7442
2023/11/09 00:24:47 - INFO - root -   train_accuracy = 0.5279
2023/11/09 00:25:09 - INFO - root -   Epoch: [12/300][0/341], lr: 0.00000012 	 loss = 0.2458(0.2458)
2023/11/09 00:26:24 - INFO - root -   Epoch: [12/300][20/341], lr: 0.00000012 	 loss = 0.4188(0.6760)
2023/11/09 00:27:24 - INFO - root -   Epoch: [12/300][40/341], lr: 0.00000012 	 loss = 0.3771(0.6758)
2023/11/09 00:28:35 - INFO - root -   Epoch: [12/300][60/341], lr: 0.00000012 	 loss = 1.1999(0.6876)
2023/11/09 00:29:33 - INFO - root -   Epoch: [12/300][80/341], lr: 0.00000012 	 loss = 1.4994(0.7252)
2023/11/09 00:30:52 - INFO - root -   Epoch: [12/300][100/341], lr: 0.00000012 	 loss = 0.7332(0.7179)
2023/11/09 00:31:44 - INFO - root -   Epoch: [12/300][120/341], lr: 0.00000012 	 loss = 0.2516(0.7204)
2023/11/09 00:33:01 - INFO - root -   Epoch: [12/300][140/341], lr: 0.00000012 	 loss = 1.4576(0.7352)
2023/11/09 00:34:23 - INFO - root -   Epoch: [12/300][160/341], lr: 0.00000012 	 loss = 0.7045(0.7356)
2023/11/09 00:35:32 - INFO - root -   Epoch: [12/300][180/341], lr: 0.00000012 	 loss = 0.5512(0.7398)
2023/11/09 00:36:36 - INFO - root -   Epoch: [12/300][200/341], lr: 0.00000012 	 loss = 0.6877(0.7406)
2023/11/09 00:37:34 - INFO - root -   Epoch: [12/300][220/341], lr: 0.00000012 	 loss = 0.4019(0.7311)
2023/11/09 00:38:56 - INFO - root -   Epoch: [12/300][240/341], lr: 0.00000012 	 loss = 1.0863(0.7344)
2023/11/09 00:39:51 - INFO - root -   Epoch: [12/300][260/341], lr: 0.00000012 	 loss = 0.9202(0.7248)
2023/11/09 00:40:58 - INFO - root -   Epoch: [12/300][280/341], lr: 0.00000012 	 loss = 0.5854(0.7297)
2023/11/09 00:42:07 - INFO - root -   Epoch: [12/300][300/341], lr: 0.00000012 	 loss = 0.8703(0.7333)
2023/11/09 00:43:33 - INFO - root -   Epoch: [12/300][320/341], lr: 0.00000012 	 loss = 0.8479(0.7323)
2023/11/09 00:44:08 - INFO - root -   Epoch: [12/300][340/341], lr: 0.00000012 	 loss = 0.8104(0.7293)
2023/11/09 00:44:09 - INFO - root -   Epoch: [12/300] 	 loss = 0.7293
2023/11/09 00:44:09 - INFO - root -   train_accuracy = 0.5499
2023/11/09 00:44:40 - INFO - root -   Epoch: [13/300][0/341], lr: 0.00000012 	 loss = 0.6209(0.6209)
2023/11/09 00:45:47 - INFO - root -   Epoch: [13/300][20/341], lr: 0.00000012 	 loss = 0.6314(0.6659)
2023/11/09 00:46:57 - INFO - root -   Epoch: [13/300][40/341], lr: 0.00000012 	 loss = 0.8482(0.7033)
2023/11/09 00:48:08 - INFO - root -   Epoch: [13/300][60/341], lr: 0.00000012 	 loss = 0.6943(0.7605)
2023/11/09 00:49:15 - INFO - root -   Epoch: [13/300][80/341], lr: 0.00000012 	 loss = 0.8984(0.7566)
2023/11/09 00:50:17 - INFO - root -   Epoch: [13/300][100/341], lr: 0.00000012 	 loss = 0.4330(0.7473)
2023/11/09 00:51:11 - INFO - root -   Epoch: [13/300][120/341], lr: 0.00000012 	 loss = 0.3782(0.7368)
2023/11/09 00:52:39 - INFO - root -   Epoch: [13/300][140/341], lr: 0.00000012 	 loss = 0.8904(0.7253)
2023/11/09 00:53:43 - INFO - root -   Epoch: [13/300][160/341], lr: 0.00000012 	 loss = 0.3250(0.7053)
2023/11/09 00:54:47 - INFO - root -   Epoch: [13/300][180/341], lr: 0.00000012 	 loss = 0.4824(0.7045)
2023/11/09 00:55:52 - INFO - root -   Epoch: [13/300][200/341], lr: 0.00000012 	 loss = 0.6053(0.7029)
2023/11/09 00:57:05 - INFO - root -   Epoch: [13/300][220/341], lr: 0.00000012 	 loss = 0.2268(0.6965)
2023/11/09 00:58:19 - INFO - root -   Epoch: [13/300][240/341], lr: 0.00000012 	 loss = 0.4209(0.6940)
2023/11/09 00:59:27 - INFO - root -   Epoch: [13/300][260/341], lr: 0.00000012 	 loss = 0.3539(0.7046)
2023/11/09 01:00:33 - INFO - root -   Epoch: [13/300][280/341], lr: 0.00000012 	 loss = 0.3039(0.7165)
2023/11/09 01:01:33 - INFO - root -   Epoch: [13/300][300/341], lr: 0.00000012 	 loss = 0.8059(0.7176)
2023/11/09 01:02:59 - INFO - root -   Epoch: [13/300][320/341], lr: 0.00000012 	 loss = 1.0579(0.7134)
2023/11/09 01:03:29 - INFO - root -   Epoch: [13/300][340/341], lr: 0.00000012 	 loss = 1.2652(0.7175)
2023/11/09 01:03:29 - INFO - root -   Epoch: [13/300] 	 loss = 0.7175
2023/11/09 01:03:29 - INFO - root -   train_accuracy = 0.5572
2023/11/09 01:03:52 - INFO - root -   Epoch: [14/300][0/341], lr: 0.00000012 	 loss = 1.0755(1.0755)
2023/11/09 01:05:08 - INFO - root -   Epoch: [14/300][20/341], lr: 0.00000012 	 loss = 0.5944(0.7022)
2023/11/09 01:06:07 - INFO - root -   Epoch: [14/300][40/341], lr: 0.00000012 	 loss = 0.6491(0.6701)
2023/11/09 01:07:26 - INFO - root -   Epoch: [14/300][60/341], lr: 0.00000012 	 loss = 0.4972(0.7272)
2023/11/09 01:08:26 - INFO - root -   Epoch: [14/300][80/341], lr: 0.00000012 	 loss = 0.7373(0.7148)
2023/11/09 01:09:31 - INFO - root -   Epoch: [14/300][100/341], lr: 0.00000012 	 loss = 0.5212(0.7089)
2023/11/09 01:10:35 - INFO - root -   Epoch: [14/300][120/341], lr: 0.00000012 	 loss = 0.6634(0.7100)
2023/11/09 01:11:46 - INFO - root -   Epoch: [14/300][140/341], lr: 0.00000012 	 loss = 0.8806(0.7338)
2023/11/09 01:12:35 - INFO - root -   Epoch: [14/300][160/341], lr: 0.00000012 	 loss = 0.6200(0.7287)
2023/11/09 01:14:00 - INFO - root -   Epoch: [14/300][180/341], lr: 0.00000012 	 loss = 0.3820(0.7248)
2023/11/09 01:14:58 - INFO - root -   Epoch: [14/300][200/341], lr: 0.00000012 	 loss = 0.3461(0.7247)
2023/11/09 01:16:24 - INFO - root -   Epoch: [14/300][220/341], lr: 0.00000012 	 loss = 0.2911(0.7259)
2023/11/09 01:17:29 - INFO - root -   Epoch: [14/300][240/341], lr: 0.00000012 	 loss = 0.3054(0.7160)
2023/11/09 01:18:21 - INFO - root -   Epoch: [14/300][260/341], lr: 0.00000012 	 loss = 0.3570(0.7119)
2023/11/09 01:19:38 - INFO - root -   Epoch: [14/300][280/341], lr: 0.00000012 	 loss = 0.9042(0.7129)
2023/11/09 01:20:28 - INFO - root -   Epoch: [14/300][300/341], lr: 0.00000012 	 loss = 0.1080(0.7149)
2023/11/09 01:21:40 - INFO - root -   Epoch: [14/300][320/341], lr: 0.00000012 	 loss = 0.2180(0.7157)
2023/11/09 01:22:29 - INFO - root -   Epoch: [14/300][340/341], lr: 0.00000012 	 loss = 1.1776(0.7229)
2023/11/09 01:22:29 - INFO - root -   Epoch: [14/300] 	 loss = 0.7229
2023/11/09 01:26:13 - INFO - root -   precision = 0.7135
2023/11/09 01:26:13 - INFO - root -   eval_loss = 0.6239
2023/11/09 01:26:13 - INFO - root -   eval_acc = 0.7135
2023/11/09 01:26:14 - INFO - root -   train_accuracy = 0.5572
2023/11/09 01:26:36 - INFO - root -   Epoch: [15/300][0/341], lr: 0.00000012 	 loss = 0.9754(0.9754)
2023/11/09 01:27:35 - INFO - root -   Epoch: [15/300][20/341], lr: 0.00000012 	 loss = 0.4089(0.7724)
2023/11/09 01:28:48 - INFO - root -   Epoch: [15/300][40/341], lr: 0.00000012 	 loss = 0.2100(0.7566)
2023/11/09 01:30:14 - INFO - root -   Epoch: [15/300][60/341], lr: 0.00000012 	 loss = 0.4188(0.7195)
2023/11/09 01:31:12 - INFO - root -   Epoch: [15/300][80/341], lr: 0.00000012 	 loss = 0.7050(0.7194)
2023/11/09 01:32:16 - INFO - root -   Epoch: [15/300][100/341], lr: 0.00000012 	 loss = 0.4896(0.7146)
2023/11/09 01:33:40 - INFO - root -   Epoch: [15/300][120/341], lr: 0.00000012 	 loss = 0.7004(0.7209)
2023/11/09 01:34:35 - INFO - root -   Epoch: [15/300][140/341], lr: 0.00000012 	 loss = 0.4093(0.7036)
2023/11/09 01:35:52 - INFO - root -   Epoch: [15/300][160/341], lr: 0.00000012 	 loss = 0.2229(0.7055)
2023/11/09 01:36:42 - INFO - root -   Epoch: [15/300][180/341], lr: 0.00000012 	 loss = 0.6945(0.7047)
2023/11/09 01:38:13 - INFO - root -   Epoch: [15/300][200/341], lr: 0.00000012 	 loss = 0.6189(0.7083)
2023/11/09 01:39:10 - INFO - root -   Epoch: [15/300][220/341], lr: 0.00000012 	 loss = 0.3305(0.7129)
2023/11/09 01:40:35 - INFO - root -   Epoch: [15/300][240/341], lr: 0.00000012 	 loss = 0.6671(0.7066)
2023/11/09 01:41:27 - INFO - root -   Epoch: [15/300][260/341], lr: 0.00000012 	 loss = 0.3735(0.7061)
2023/11/09 01:42:37 - INFO - root -   Epoch: [15/300][280/341], lr: 0.00000012 	 loss = 0.6588(0.7059)
2023/11/09 01:43:30 - INFO - root -   Epoch: [15/300][300/341], lr: 0.00000012 	 loss = 0.8883(0.7155)
2023/11/09 01:44:31 - INFO - root -   Epoch: [15/300][320/341], lr: 0.00000012 	 loss = 0.3607(0.7217)
2023/11/09 01:45:23 - INFO - root -   Epoch: [15/300][340/341], lr: 0.00000012 	 loss = 1.0077(0.7231)
2023/11/09 01:45:23 - INFO - root -   Epoch: [15/300] 	 loss = 0.7231
2023/11/09 01:45:23 - INFO - root -   train_accuracy = 0.5616
2023/11/09 01:45:54 - INFO - root -   Epoch: [16/300][0/341], lr: 0.00000012 	 loss = 0.8362(0.8362)
2023/11/09 01:46:59 - INFO - root -   Epoch: [16/300][20/341], lr: 0.00000012 	 loss = 0.7857(0.7441)
2023/11/09 01:48:04 - INFO - root -   Epoch: [16/300][40/341], lr: 0.00000012 	 loss = 0.6645(0.7419)
2023/11/09 01:49:18 - INFO - root -   Epoch: [16/300][60/341], lr: 0.00000012 	 loss = 0.3621(0.7423)
2023/11/09 01:50:12 - INFO - root -   Epoch: [16/300][80/341], lr: 0.00000012 	 loss = 1.3743(0.7432)
2023/11/09 01:51:25 - INFO - root -   Epoch: [16/300][100/341], lr: 0.00000012 	 loss = 0.8372(0.7450)
2023/11/09 01:52:39 - INFO - root -   Epoch: [16/300][120/341], lr: 0.00000012 	 loss = 0.8221(0.7445)
2023/11/09 01:53:50 - INFO - root -   Epoch: [16/300][140/341], lr: 0.00000012 	 loss = 0.5959(0.7355)
2023/11/09 01:54:53 - INFO - root -   Epoch: [16/300][160/341], lr: 0.00000012 	 loss = 0.5136(0.7261)
2023/11/09 01:56:12 - INFO - root -   Epoch: [16/300][180/341], lr: 0.00000012 	 loss = 1.0463(0.7291)
2023/11/09 01:57:12 - INFO - root -   Epoch: [16/300][200/341], lr: 0.00000012 	 loss = 0.7540(0.7469)
2023/11/09 01:58:18 - INFO - root -   Epoch: [16/300][220/341], lr: 0.00000012 	 loss = 0.3512(0.7459)
2023/11/09 01:59:13 - INFO - root -   Epoch: [16/300][240/341], lr: 0.00000012 	 loss = 0.5517(0.7395)
2023/11/09 02:00:39 - INFO - root -   Epoch: [16/300][260/341], lr: 0.00000012 	 loss = 0.6866(0.7407)
2023/11/09 02:01:41 - INFO - root -   Epoch: [16/300][280/341], lr: 0.00000012 	 loss = 0.6312(0.7364)
2023/11/09 02:02:55 - INFO - root -   Epoch: [16/300][300/341], lr: 0.00000012 	 loss = 0.4258(0.7310)
2023/11/09 02:03:58 - INFO - root -   Epoch: [16/300][320/341], lr: 0.00000012 	 loss = 0.4758(0.7340)
2023/11/09 02:04:45 - INFO - root -   Epoch: [16/300][340/341], lr: 0.00000012 	 loss = 1.0957(0.7342)
2023/11/09 02:04:45 - INFO - root -   Epoch: [16/300] 	 loss = 0.7342
2023/11/09 02:04:45 - INFO - root -   train_accuracy = 0.5425
2023/11/09 02:05:07 - INFO - root -   Epoch: [17/300][0/341], lr: 0.00000012 	 loss = 1.0081(1.0081)
2023/11/09 02:06:07 - INFO - root -   Epoch: [17/300][20/341], lr: 0.00000012 	 loss = 0.3384(0.6854)
2023/11/09 02:07:17 - INFO - root -   Epoch: [17/300][40/341], lr: 0.00000012 	 loss = 0.7001(0.6940)
2023/11/09 02:08:46 - INFO - root -   Epoch: [17/300][60/341], lr: 0.00000012 	 loss = 0.3859(0.6963)
2023/11/09 02:09:37 - INFO - root -   Epoch: [17/300][80/341], lr: 0.00000012 	 loss = 1.1706(0.7081)
2023/11/09 02:11:00 - INFO - root -   Epoch: [17/300][100/341], lr: 0.00000012 	 loss = 1.0527(0.7066)
2023/11/09 02:11:48 - INFO - root -   Epoch: [17/300][120/341], lr: 0.00000012 	 loss = 0.1774(0.6895)
2023/11/09 02:13:03 - INFO - root -   Epoch: [17/300][140/341], lr: 0.00000012 	 loss = 0.6713(0.7226)
2023/11/09 02:14:00 - INFO - root -   Epoch: [17/300][160/341], lr: 0.00000012 	 loss = 0.4569(0.7163)
2023/11/09 02:15:30 - INFO - root -   Epoch: [17/300][180/341], lr: 0.00000012 	 loss = 0.6595(0.7193)
2023/11/09 02:16:31 - INFO - root -   Epoch: [17/300][200/341], lr: 0.00000012 	 loss = 0.3567(0.7188)
2023/11/09 02:17:42 - INFO - root -   Epoch: [17/300][220/341], lr: 0.00000012 	 loss = 0.7969(0.7200)
2023/11/09 02:18:35 - INFO - root -   Epoch: [17/300][240/341], lr: 0.00000012 	 loss = 0.5074(0.7101)
2023/11/09 02:19:45 - INFO - root -   Epoch: [17/300][260/341], lr: 0.00000012 	 loss = 0.2736(0.7157)
2023/11/09 02:20:55 - INFO - root -   Epoch: [17/300][280/341], lr: 0.00000012 	 loss = 0.7575(0.7120)
2023/11/09 02:21:56 - INFO - root -   Epoch: [17/300][300/341], lr: 0.00000012 	 loss = 0.4363(0.7061)
2023/11/09 02:23:12 - INFO - root -   Epoch: [17/300][320/341], lr: 0.00000012 	 loss = 0.3683(0.7060)
2023/11/09 02:24:00 - INFO - root -   Epoch: [17/300][340/341], lr: 0.00000012 	 loss = 1.3618(0.7151)
2023/11/09 02:24:00 - INFO - root -   Epoch: [17/300] 	 loss = 0.7151
2023/11/09 02:24:00 - INFO - root -   train_accuracy = 0.5880
2023/11/09 02:24:30 - INFO - root -   Epoch: [18/300][0/341], lr: 0.00000013 	 loss = 0.5181(0.5181)
2023/11/09 02:25:36 - INFO - root -   Epoch: [18/300][20/341], lr: 0.00000013 	 loss = 0.5336(0.6376)
2023/11/09 02:26:48 - INFO - root -   Epoch: [18/300][40/341], lr: 0.00000013 	 loss = 0.3130(0.6957)
2023/11/09 02:27:52 - INFO - root -   Epoch: [18/300][60/341], lr: 0.00000013 	 loss = 0.8622(0.6803)
2023/11/09 02:28:56 - INFO - root -   Epoch: [18/300][80/341], lr: 0.00000013 	 loss = 1.5946(0.6825)
2023/11/09 02:29:53 - INFO - root -   Epoch: [18/300][100/341], lr: 0.00000013 	 loss = 1.0709(0.6914)
2023/11/09 02:31:05 - INFO - root -   Epoch: [18/300][120/341], lr: 0.00000013 	 loss = 0.4251(0.6855)
2023/11/09 02:32:08 - INFO - root -   Epoch: [18/300][140/341], lr: 0.00000013 	 loss = 1.0421(0.6964)
2023/11/09 02:33:38 - INFO - root -   Epoch: [18/300][160/341], lr: 0.00000013 	 loss = 0.2533(0.7070)
2023/11/09 02:34:41 - INFO - root -   Epoch: [18/300][180/341], lr: 0.00000013 	 loss = 0.8726(0.7164)
2023/11/09 02:35:52 - INFO - root -   Epoch: [18/300][200/341], lr: 0.00000013 	 loss = 0.3740(0.7127)
2023/11/09 02:37:12 - INFO - root -   Epoch: [18/300][220/341], lr: 0.00000013 	 loss = 0.3997(0.7163)
2023/11/09 02:38:02 - INFO - root -   Epoch: [18/300][240/341], lr: 0.00000013 	 loss = 0.8084(0.7091)
2023/11/09 02:39:16 - INFO - root -   Epoch: [18/300][260/341], lr: 0.00000013 	 loss = 0.4534(0.7122)
2023/11/09 02:40:18 - INFO - root -   Epoch: [18/300][280/341], lr: 0.00000013 	 loss = 0.4876(0.7147)
2023/11/09 02:41:21 - INFO - root -   Epoch: [18/300][300/341], lr: 0.00000013 	 loss = 0.6192(0.7154)
2023/11/09 02:42:27 - INFO - root -   Epoch: [18/300][320/341], lr: 0.00000013 	 loss = 0.6695(0.7165)
2023/11/09 02:43:08 - INFO - root -   Epoch: [18/300][340/341], lr: 0.00000013 	 loss = 0.9789(0.7134)
2023/11/09 02:43:08 - INFO - root -   Epoch: [18/300] 	 loss = 0.7134
2023/11/09 02:43:08 - INFO - root -   train_accuracy = 0.5528
2023/11/09 02:43:30 - INFO - root -   Epoch: [19/300][0/341], lr: 0.00000013 	 loss = 1.6879(1.6879)
2023/11/09 02:44:52 - INFO - root -   Epoch: [19/300][20/341], lr: 0.00000013 	 loss = 0.5479(0.6697)
2023/11/09 02:45:51 - INFO - root -   Epoch: [19/300][40/341], lr: 0.00000013 	 loss = 0.7956(0.7460)
2023/11/09 02:47:17 - INFO - root -   Epoch: [19/300][60/341], lr: 0.00000013 	 loss = 1.2987(0.7528)
2023/11/09 02:48:15 - INFO - root -   Epoch: [19/300][80/341], lr: 0.00000013 	 loss = 0.5492(0.7151)
2023/11/09 02:49:26 - INFO - root -   Epoch: [19/300][100/341], lr: 0.00000013 	 loss = 1.1701(0.7097)
2023/11/09 02:50:58 - INFO - root -   Epoch: [19/300][120/341], lr: 0.00000013 	 loss = 0.5555(0.7137)
2023/11/09 02:51:46 - INFO - root -   Epoch: [19/300][140/341], lr: 0.00000013 	 loss = 0.6012(0.7121)
2023/11/09 02:53:12 - INFO - root -   Epoch: [19/300][160/341], lr: 0.00000013 	 loss = 0.4249(0.7086)
2023/11/09 02:54:01 - INFO - root -   Epoch: [19/300][180/341], lr: 0.00000013 	 loss = 0.3057(0.7110)
2023/11/09 02:55:17 - INFO - root -   Epoch: [19/300][200/341], lr: 0.00000013 	 loss = 1.0132(0.7051)
2023/11/09 02:56:24 - INFO - root -   Epoch: [19/300][220/341], lr: 0.00000013 	 loss = 0.4903(0.7028)
2023/11/09 02:57:30 - INFO - root -   Epoch: [19/300][240/341], lr: 0.00000013 	 loss = 0.3799(0.6934)
2023/11/09 02:58:22 - INFO - root -   Epoch: [19/300][260/341], lr: 0.00000013 	 loss = 0.6683(0.6972)
2023/11/09 02:59:49 - INFO - root -   Epoch: [19/300][280/341], lr: 0.00000013 	 loss = 0.8586(0.6989)
2023/11/09 03:00:44 - INFO - root -   Epoch: [19/300][300/341], lr: 0.00000013 	 loss = 0.9477(0.7002)
2023/11/09 03:01:44 - INFO - root -   Epoch: [19/300][320/341], lr: 0.00000013 	 loss = 0.5329(0.7065)
2023/11/09 03:02:28 - INFO - root -   Epoch: [19/300][340/341], lr: 0.00000013 	 loss = 0.8190(0.7129)
2023/11/09 03:02:28 - INFO - root -   Epoch: [19/300] 	 loss = 0.7129
2023/11/09 03:06:12 - INFO - root -   precision = 0.7076
2023/11/09 03:06:12 - INFO - root -   eval_loss = 0.6157
2023/11/09 03:06:12 - INFO - root -   eval_acc = 0.7076
2023/11/09 03:06:13 - INFO - root -   train_accuracy = 0.5601
2023/11/09 03:06:36 - INFO - root -   Epoch: [20/300][0/341], lr: 0.00000013 	 loss = 0.5981(0.5981)
2023/11/09 03:08:08 - INFO - root -   Epoch: [20/300][20/341], lr: 0.00000013 	 loss = 0.5069(0.7630)
2023/11/09 03:09:02 - INFO - root -   Epoch: [20/300][40/341], lr: 0.00000013 	 loss = 0.8840(0.7272)
2023/11/09 03:10:02 - INFO - root -   Epoch: [20/300][60/341], lr: 0.00000013 	 loss = 0.2010(0.6789)
2023/11/09 03:10:59 - INFO - root -   Epoch: [20/300][80/341], lr: 0.00000013 	 loss = 0.5065(0.6940)
2023/11/09 03:12:09 - INFO - root -   Epoch: [20/300][100/341], lr: 0.00000013 	 loss = 0.7972(0.6777)
2023/11/09 03:13:30 - INFO - root -   Epoch: [20/300][120/341], lr: 0.00000013 	 loss = 0.2953(0.6921)
2023/11/09 03:14:19 - INFO - root -   Epoch: [20/300][140/341], lr: 0.00000013 	 loss = 0.5242(0.6894)
2023/11/09 03:15:30 - INFO - root -   Epoch: [20/300][160/341], lr: 0.00000013 	 loss = 0.4319(0.6684)
2023/11/09 03:16:30 - INFO - root -   Epoch: [20/300][180/341], lr: 0.00000013 	 loss = 0.6341(0.6734)
2023/11/09 03:17:55 - INFO - root -   Epoch: [20/300][200/341], lr: 0.00000013 	 loss = 0.3744(0.6718)
2023/11/09 03:19:00 - INFO - root -   Epoch: [20/300][220/341], lr: 0.00000013 	 loss = 0.5022(0.6743)
2023/11/09 03:20:00 - INFO - root -   Epoch: [20/300][240/341], lr: 0.00000013 	 loss = 0.4077(0.6760)
2023/11/09 03:21:18 - INFO - root -   Epoch: [20/300][260/341], lr: 0.00000013 	 loss = 0.6856(0.6724)
2023/11/09 03:22:08 - INFO - root -   Epoch: [20/300][280/341], lr: 0.00000013 	 loss = 0.2983(0.6750)
2023/11/09 03:23:34 - INFO - root -   Epoch: [20/300][300/341], lr: 0.00000013 	 loss = 1.0810(0.6768)
2023/11/09 03:24:34 - INFO - root -   Epoch: [20/300][320/341], lr: 0.00000013 	 loss = 0.5886(0.6743)
2023/11/09 03:25:23 - INFO - root -   Epoch: [20/300][340/341], lr: 0.00000013 	 loss = 1.0067(0.6769)
2023/11/09 03:25:23 - INFO - root -   Epoch: [20/300] 	 loss = 0.6769
2023/11/09 03:25:23 - INFO - root -   train_accuracy = 0.6144
2023/11/09 03:25:53 - INFO - root -   Epoch: [21/300][0/341], lr: 0.00000013 	 loss = 0.8248(0.8248)
2023/11/09 03:26:58 - INFO - root -   Epoch: [21/300][20/341], lr: 0.00000013 	 loss = 0.3774(0.6954)
2023/11/09 03:28:06 - INFO - root -   Epoch: [21/300][40/341], lr: 0.00000013 	 loss = 0.5394(0.6949)
2023/11/09 03:29:19 - INFO - root -   Epoch: [21/300][60/341], lr: 0.00000013 	 loss = 0.9013(0.6855)
2023/11/09 03:30:13 - INFO - root -   Epoch: [21/300][80/341], lr: 0.00000013 	 loss = 0.4634(0.7114)
2023/11/09 03:31:27 - INFO - root -   Epoch: [21/300][100/341], lr: 0.00000013 	 loss = 0.9563(0.6973)
2023/11/09 03:32:19 - INFO - root -   Epoch: [21/300][120/341], lr: 0.00000013 	 loss = 0.8610(0.7008)
2023/11/09 03:33:36 - INFO - root -   Epoch: [21/300][140/341], lr: 0.00000013 	 loss = 0.6420(0.7106)
2023/11/09 03:34:27 - INFO - root -   Epoch: [21/300][160/341], lr: 0.00000013 	 loss = 0.6238(0.7138)
2023/11/09 03:35:51 - INFO - root -   Epoch: [21/300][180/341], lr: 0.00000013 	 loss = 0.9449(0.7058)
2023/11/09 03:36:52 - INFO - root -   Epoch: [21/300][200/341], lr: 0.00000013 	 loss = 0.2999(0.7049)
2023/11/09 03:38:13 - INFO - root -   Epoch: [21/300][220/341], lr: 0.00000013 	 loss = 0.4785(0.6985)
2023/11/09 03:39:24 - INFO - root -   Epoch: [21/300][240/341], lr: 0.00000013 	 loss = 0.5849(0.6936)
2023/11/09 03:40:46 - INFO - root -   Epoch: [21/300][260/341], lr: 0.00000013 	 loss = 0.5117(0.6984)
2023/11/09 03:41:42 - INFO - root -   Epoch: [21/300][280/341], lr: 0.00000013 	 loss = 0.2521(0.7021)
2023/11/09 03:42:41 - INFO - root -   Epoch: [21/300][300/341], lr: 0.00000013 	 loss = 0.7912(0.7045)
2023/11/09 03:43:37 - INFO - root -   Epoch: [21/300][320/341], lr: 0.00000013 	 loss = 0.6767(0.7052)
2023/11/09 03:44:21 - INFO - root -   Epoch: [21/300][340/341], lr: 0.00000013 	 loss = 1.2579(0.7052)
2023/11/09 03:44:21 - INFO - root -   Epoch: [21/300] 	 loss = 0.7052
2023/11/09 03:44:21 - INFO - root -   train_accuracy = 0.5953
2023/11/09 03:44:58 - INFO - root -   Epoch: [22/300][0/341], lr: 0.00000013 	 loss = 0.7538(0.7538)
2023/11/09 03:45:42 - INFO - root -   Epoch: [22/300][20/341], lr: 0.00000013 	 loss = 0.6103(0.6319)
2023/11/09 03:47:00 - INFO - root -   Epoch: [22/300][40/341], lr: 0.00000013 	 loss = 0.1568(0.5886)
2023/11/09 03:48:11 - INFO - root -   Epoch: [22/300][60/341], lr: 0.00000013 	 loss = 0.4103(0.6449)
2023/11/09 03:49:10 - INFO - root -   Epoch: [22/300][80/341], lr: 0.00000013 	 loss = 0.8336(0.6463)
2023/11/09 03:50:22 - INFO - root -   Epoch: [22/300][100/341], lr: 0.00000013 	 loss = 0.8247(0.6458)
2023/11/09 03:51:26 - INFO - root -   Epoch: [22/300][120/341], lr: 0.00000013 	 loss = 0.3627(0.6505)
2023/11/09 03:52:30 - INFO - root -   Epoch: [22/300][140/341], lr: 0.00000013 	 loss = 0.4903(0.6613)
2023/11/09 03:53:44 - INFO - root -   Epoch: [22/300][160/341], lr: 0.00000013 	 loss = 0.6041(0.6667)
2023/11/09 03:54:30 - INFO - root -   Epoch: [22/300][180/341], lr: 0.00000013 	 loss = 1.1965(0.6795)
2023/11/09 03:55:43 - INFO - root -   Epoch: [22/300][200/341], lr: 0.00000013 	 loss = 0.7092(0.6848)
2023/11/09 03:56:59 - INFO - root -   Epoch: [22/300][220/341], lr: 0.00000013 	 loss = 1.0382(0.6835)
2023/11/09 03:58:07 - INFO - root -   Epoch: [22/300][240/341], lr: 0.00000013 	 loss = 0.3186(0.6797)
2023/11/09 03:59:06 - INFO - root -   Epoch: [22/300][260/341], lr: 0.00000013 	 loss = 0.2836(0.6799)
2023/11/09 04:00:09 - INFO - root -   Epoch: [22/300][280/341], lr: 0.00000013 	 loss = 0.9137(0.6875)
2023/11/09 04:01:19 - INFO - root -   Epoch: [22/300][300/341], lr: 0.00000013 	 loss = 0.2765(0.6921)
2023/11/09 04:02:25 - INFO - root -   Epoch: [22/300][320/341], lr: 0.00000013 	 loss = 0.5901(0.6960)
2023/11/09 04:03:11 - INFO - root -   Epoch: [22/300][340/341], lr: 0.00000013 	 loss = 0.9111(0.6931)
2023/11/09 04:03:11 - INFO - root -   Epoch: [22/300] 	 loss = 0.6931
2023/11/09 04:03:11 - INFO - root -   train_accuracy = 0.5924
2023/11/09 04:03:33 - INFO - root -   Epoch: [23/300][0/341], lr: 0.00000013 	 loss = 1.2032(1.2032)
2023/11/09 04:04:45 - INFO - root -   Epoch: [23/300][20/341], lr: 0.00000013 	 loss = 0.8565(0.7567)
2023/11/09 04:05:42 - INFO - root -   Epoch: [23/300][40/341], lr: 0.00000013 	 loss = 0.4624(0.7134)
2023/11/09 04:06:53 - INFO - root -   Epoch: [23/300][60/341], lr: 0.00000013 	 loss = 0.3874(0.7697)
2023/11/09 04:08:02 - INFO - root -   Epoch: [23/300][80/341], lr: 0.00000013 	 loss = 1.4140(0.7513)
2023/11/09 04:09:14 - INFO - root -   Epoch: [23/300][100/341], lr: 0.00000013 	 loss = 0.6745(0.7410)
2023/11/09 04:10:23 - INFO - root -   Epoch: [23/300][120/341], lr: 0.00000013 	 loss = 0.1701(0.7299)
2023/11/09 04:11:49 - INFO - root -   Epoch: [23/300][140/341], lr: 0.00000013 	 loss = 0.6161(0.7260)
2023/11/09 04:12:41 - INFO - root -   Epoch: [23/300][160/341], lr: 0.00000013 	 loss = 0.4262(0.7297)
2023/11/09 04:14:02 - INFO - root -   Epoch: [23/300][180/341], lr: 0.00000013 	 loss = 0.8464(0.7227)
2023/11/09 04:15:03 - INFO - root -   Epoch: [23/300][200/341], lr: 0.00000013 	 loss = 0.6238(0.7139)
2023/11/09 04:16:15 - INFO - root -   Epoch: [23/300][220/341], lr: 0.00000013 	 loss = 0.3510(0.7133)
2023/11/09 04:17:13 - INFO - root -   Epoch: [23/300][240/341], lr: 0.00000013 	 loss = 0.6632(0.7140)
2023/11/09 04:18:33 - INFO - root -   Epoch: [23/300][260/341], lr: 0.00000013 	 loss = 0.7378(0.7117)
2023/11/09 04:19:27 - INFO - root -   Epoch: [23/300][280/341], lr: 0.00000013 	 loss = 0.3412(0.7162)
2023/11/09 04:20:48 - INFO - root -   Epoch: [23/300][300/341], lr: 0.00000013 	 loss = 1.8597(0.7208)
2023/11/09 04:21:39 - INFO - root -   Epoch: [23/300][320/341], lr: 0.00000013 	 loss = 0.9403(0.7219)
2023/11/09 04:22:33 - INFO - root -   Epoch: [23/300][340/341], lr: 0.00000013 	 loss = 0.8993(0.7239)
2023/11/09 04:22:33 - INFO - root -   Epoch: [23/300] 	 loss = 0.7239
2023/11/09 04:22:33 - INFO - root -   train_accuracy = 0.5484
2023/11/09 04:23:12 - INFO - root -   Epoch: [24/300][0/341], lr: 0.00000013 	 loss = 1.2584(1.2584)
2023/11/09 04:24:12 - INFO - root -   Epoch: [24/300][20/341], lr: 0.00000013 	 loss = 0.3963(0.6506)
2023/11/09 04:25:21 - INFO - root -   Epoch: [24/300][40/341], lr: 0.00000013 	 loss = 0.3364(0.6293)
2023/11/09 04:26:17 - INFO - root -   Epoch: [24/300][60/341], lr: 0.00000013 	 loss = 0.4176(0.6576)
2023/11/09 04:27:32 - INFO - root -   Epoch: [24/300][80/341], lr: 0.00000013 	 loss = 1.1227(0.6535)
2023/11/09 04:28:27 - INFO - root -   Epoch: [24/300][100/341], lr: 0.00000013 	 loss = 0.5948(0.6583)
2023/11/09 04:29:43 - INFO - root -   Epoch: [24/300][120/341], lr: 0.00000013 	 loss = 0.6383(0.6807)
2023/11/09 04:30:40 - INFO - root -   Epoch: [24/300][140/341], lr: 0.00000013 	 loss = 0.8531(0.6896)
2023/11/09 04:32:10 - INFO - root -   Epoch: [24/300][160/341], lr: 0.00000013 	 loss = 0.9125(0.7098)
2023/11/09 04:33:16 - INFO - root -   Epoch: [24/300][180/341], lr: 0.00000013 	 loss = 1.5167(0.7220)
2023/11/09 04:34:28 - INFO - root -   Epoch: [24/300][200/341], lr: 0.00000013 	 loss = 0.1961(0.7134)
2023/11/09 04:35:20 - INFO - root -   Epoch: [24/300][220/341], lr: 0.00000013 	 loss = 0.4167(0.7120)
2023/11/09 04:36:38 - INFO - root -   Epoch: [24/300][240/341], lr: 0.00000013 	 loss = 0.4451(0.7019)
2023/11/09 04:37:25 - INFO - root -   Epoch: [24/300][260/341], lr: 0.00000013 	 loss = 0.7038(0.7017)
2023/11/09 04:38:46 - INFO - root -   Epoch: [24/300][280/341], lr: 0.00000013 	 loss = 0.6104(0.6935)
2023/11/09 04:39:40 - INFO - root -   Epoch: [24/300][300/341], lr: 0.00000013 	 loss = 0.7735(0.6933)
2023/11/09 04:40:55 - INFO - root -   Epoch: [24/300][320/341], lr: 0.00000013 	 loss = 0.1593(0.6962)
2023/11/09 04:41:27 - INFO - root -   Epoch: [24/300][340/341], lr: 0.00000013 	 loss = 0.7919(0.6984)
2023/11/09 04:41:27 - INFO - root -   Epoch: [24/300] 	 loss = 0.6984
2023/11/09 04:45:11 - INFO - root -   precision = 0.7018
2023/11/09 04:45:11 - INFO - root -   eval_loss = 0.6064
2023/11/09 04:45:11 - INFO - root -   eval_acc = 0.7018
2023/11/09 04:45:14 - INFO - root -   train_accuracy = 0.5660
2023/11/09 04:45:46 - INFO - root -   Epoch: [25/300][0/341], lr: 0.00000014 	 loss = 0.5844(0.5844)
2023/11/09 04:46:55 - INFO - root -   Epoch: [25/300][20/341], lr: 0.00000014 	 loss = 0.9799(0.8090)
2023/11/09 04:47:56 - INFO - root -   Epoch: [25/300][40/341], lr: 0.00000014 	 loss = 0.3936(0.7540)
2023/11/09 04:48:59 - INFO - root -   Epoch: [25/300][60/341], lr: 0.00000014 	 loss = 0.6096(0.7445)
2023/11/09 04:50:01 - INFO - root -   Epoch: [25/300][80/341], lr: 0.00000014 	 loss = 1.1968(0.7314)
2023/11/09 04:51:03 - INFO - root -   Epoch: [25/300][100/341], lr: 0.00000014 	 loss = 0.7307(0.7292)
2023/11/09 04:52:21 - INFO - root -   Epoch: [25/300][120/341], lr: 0.00000014 	 loss = 0.4178(0.7167)
2023/11/09 04:53:27 - INFO - root -   Epoch: [25/300][140/341], lr: 0.00000014 	 loss = 0.7889(0.7274)
2023/11/09 04:54:36 - INFO - root -   Epoch: [25/300][160/341], lr: 0.00000014 	 loss = 0.5621(0.7142)
2023/11/09 04:55:41 - INFO - root -   Epoch: [25/300][180/341], lr: 0.00000014 	 loss = 0.4129(0.7065)
2023/11/09 04:56:32 - INFO - root -   Epoch: [25/300][200/341], lr: 0.00000014 	 loss = 0.3668(0.7027)
2023/11/09 04:58:03 - INFO - root -   Epoch: [25/300][220/341], lr: 0.00000014 	 loss = 0.4963(0.7001)
2023/11/09 04:58:59 - INFO - root -   Epoch: [25/300][240/341], lr: 0.00000014 	 loss = 0.6272(0.7053)
2023/11/09 05:00:03 - INFO - root -   Epoch: [25/300][260/341], lr: 0.00000014 	 loss = 0.5080(0.7113)
2023/11/09 05:01:23 - INFO - root -   Epoch: [25/300][280/341], lr: 0.00000014 	 loss = 0.5520(0.7109)
2023/11/09 05:02:16 - INFO - root -   Epoch: [25/300][300/341], lr: 0.00000014 	 loss = 0.6281(0.7147)
2023/11/09 05:03:25 - INFO - root -   Epoch: [25/300][320/341], lr: 0.00000014 	 loss = 0.4999(0.7191)
2023/11/09 05:04:08 - INFO - root -   Epoch: [25/300][340/341], lr: 0.00000014 	 loss = 1.3969(0.7177)
2023/11/09 05:04:08 - INFO - root -   Epoch: [25/300] 	 loss = 0.7177
2023/11/09 05:04:08 - INFO - root -   train_accuracy = 0.5777
2023/11/09 05:04:30 - INFO - root -   Epoch: [26/300][0/341], lr: 0.00000014 	 loss = 0.6584(0.6584)
2023/11/09 05:05:45 - INFO - root -   Epoch: [26/300][20/341], lr: 0.00000014 	 loss = 0.3689(0.6135)
2023/11/09 05:06:55 - INFO - root -   Epoch: [26/300][40/341], lr: 0.00000014 	 loss = 0.8809(0.6358)
2023/11/09 05:08:14 - INFO - root -   Epoch: [26/300][60/341], lr: 0.00000014 	 loss = 0.5528(0.6413)
2023/11/09 05:09:08 - INFO - root -   Epoch: [26/300][80/341], lr: 0.00000014 	 loss = 0.9343(0.6635)
2023/11/09 05:10:23 - INFO - root -   Epoch: [26/300][100/341], lr: 0.00000014 	 loss = 0.3355(0.6582)
2023/11/09 05:11:22 - INFO - root -   Epoch: [26/300][120/341], lr: 0.00000014 	 loss = 1.6289(0.6802)
2023/11/09 05:12:42 - INFO - root -   Epoch: [26/300][140/341], lr: 0.00000014 	 loss = 0.4098(0.6865)
2023/11/09 05:13:33 - INFO - root -   Epoch: [26/300][160/341], lr: 0.00000014 	 loss = 0.4677(0.6862)
2023/11/09 05:14:51 - INFO - root -   Epoch: [26/300][180/341], lr: 0.00000014 	 loss = 0.3798(0.6959)
2023/11/09 05:15:52 - INFO - root -   Epoch: [26/300][200/341], lr: 0.00000014 	 loss = 0.4934(0.6960)
2023/11/09 05:17:18 - INFO - root -   Epoch: [26/300][220/341], lr: 0.00000014 	 loss = 0.6004(0.6985)
2023/11/09 05:18:14 - INFO - root -   Epoch: [26/300][240/341], lr: 0.00000014 	 loss = 0.6374(0.7011)
2023/11/09 05:19:28 - INFO - root -   Epoch: [26/300][260/341], lr: 0.00000014 	 loss = 0.4412(0.7015)
2023/11/09 05:20:26 - INFO - root -   Epoch: [26/300][280/341], lr: 0.00000014 	 loss = 0.7008(0.7053)
2023/11/09 05:21:42 - INFO - root -   Epoch: [26/300][300/341], lr: 0.00000014 	 loss = 0.9603(0.7076)
2023/11/09 05:22:39 - INFO - root -   Epoch: [26/300][320/341], lr: 0.00000014 	 loss = 0.9875(0.7095)
2023/11/09 05:23:14 - INFO - root -   Epoch: [26/300][340/341], lr: 0.00000014 	 loss = 1.4100(0.7103)
2023/11/09 05:23:14 - INFO - root -   Epoch: [26/300] 	 loss = 0.7103
2023/11/09 05:23:14 - INFO - root -   train_accuracy = 0.5601
2023/11/09 05:23:37 - INFO - root -   Epoch: [27/300][0/341], lr: 0.00000014 	 loss = 0.7981(0.7981)
2023/11/09 05:24:36 - INFO - root -   Epoch: [27/300][20/341], lr: 0.00000014 	 loss = 0.8892(0.8212)
2023/11/09 05:25:34 - INFO - root -   Epoch: [27/300][40/341], lr: 0.00000014 	 loss = 0.3640(0.7202)
2023/11/09 05:26:47 - INFO - root -   Epoch: [27/300][60/341], lr: 0.00000014 	 loss = 0.7967(0.7059)
2023/11/09 05:27:53 - INFO - root -   Epoch: [27/300][80/341], lr: 0.00000014 	 loss = 0.5644(0.7050)
2023/11/09 05:29:08 - INFO - root -   Epoch: [27/300][100/341], lr: 0.00000014 	 loss = 0.4314(0.7028)
2023/11/09 05:30:12 - INFO - root -   Epoch: [27/300][120/341], lr: 0.00000014 	 loss = 0.5581(0.6878)
2023/11/09 05:31:28 - INFO - root -   Epoch: [27/300][140/341], lr: 0.00000014 	 loss = 0.6349(0.6938)
2023/11/09 05:32:19 - INFO - root -   Epoch: [27/300][160/341], lr: 0.00000014 	 loss = 0.5036(0.6844)
2023/11/09 05:33:55 - INFO - root -   Epoch: [27/300][180/341], lr: 0.00000014 	 loss = 0.2985(0.6872)
2023/11/09 05:34:55 - INFO - root -   Epoch: [27/300][200/341], lr: 0.00000014 	 loss = 0.2982(0.6734)
2023/11/09 05:35:56 - INFO - root -   Epoch: [27/300][220/341], lr: 0.00000014 	 loss = 0.5993(0.6664)
2023/11/09 05:37:07 - INFO - root -   Epoch: [27/300][240/341], lr: 0.00000014 	 loss = 0.2338(0.6649)
2023/11/09 05:38:28 - INFO - root -   Epoch: [27/300][260/341], lr: 0.00000014 	 loss = 0.4454(0.6695)
2023/11/09 05:39:12 - INFO - root -   Epoch: [27/300][280/341], lr: 0.00000014 	 loss = 0.3708(0.6722)
2023/11/09 05:40:36 - INFO - root -   Epoch: [27/300][300/341], lr: 0.00000014 	 loss = 1.0106(0.6720)
2023/11/09 05:41:31 - INFO - root -   Epoch: [27/300][320/341], lr: 0.00000014 	 loss = 0.3117(0.6785)
2023/11/09 05:42:10 - INFO - root -   Epoch: [27/300][340/341], lr: 0.00000014 	 loss = 0.7297(0.6865)
2023/11/09 05:42:11 - INFO - root -   Epoch: [27/300] 	 loss = 0.6865
2023/11/09 05:42:11 - INFO - root -   train_accuracy = 0.5968
2023/11/09 05:42:33 - INFO - root -   Epoch: [28/300][0/341], lr: 0.00000014 	 loss = 0.5248(0.5248)
2023/11/09 05:43:41 - INFO - root -   Epoch: [28/300][20/341], lr: 0.00000014 	 loss = 0.3868(0.7587)
2023/11/09 05:44:45 - INFO - root -   Epoch: [28/300][40/341], lr: 0.00000014 	 loss = 0.2479(0.7187)
2023/11/09 05:46:00 - INFO - root -   Epoch: [28/300][60/341], lr: 0.00000014 	 loss = 0.5732(0.7307)
2023/11/09 05:47:06 - INFO - root -   Epoch: [28/300][80/341], lr: 0.00000014 	 loss = 1.3847(0.7351)
2023/11/09 05:48:24 - INFO - root -   Epoch: [28/300][100/341], lr: 0.00000014 	 loss = 1.2492(0.7329)
2023/11/09 05:49:17 - INFO - root -   Epoch: [28/300][120/341], lr: 0.00000014 	 loss = 0.6091(0.7255)
2023/11/09 05:50:41 - INFO - root -   Epoch: [28/300][140/341], lr: 0.00000014 	 loss = 0.9431(0.7255)
2023/11/09 05:51:25 - INFO - root -   Epoch: [28/300][160/341], lr: 0.00000014 	 loss = 0.3777(0.7193)
2023/11/09 05:52:42 - INFO - root -   Epoch: [28/300][180/341], lr: 0.00000014 	 loss = 0.4338(0.7149)
2023/11/09 05:53:38 - INFO - root -   Epoch: [28/300][200/341], lr: 0.00000014 	 loss = 0.6821(0.7113)
2023/11/09 05:54:48 - INFO - root -   Epoch: [28/300][220/341], lr: 0.00000014 	 loss = 0.2943(0.7092)
2023/11/09 05:55:47 - INFO - root -   Epoch: [28/300][240/341], lr: 0.00000014 	 loss = 0.6772(0.7053)
2023/11/09 05:57:14 - INFO - root -   Epoch: [28/300][260/341], lr: 0.00000014 	 loss = 0.6932(0.7189)
2023/11/09 05:58:03 - INFO - root -   Epoch: [28/300][280/341], lr: 0.00000014 	 loss = 0.6352(0.7058)
2023/11/09 05:59:06 - INFO - root -   Epoch: [28/300][300/341], lr: 0.00000014 	 loss = 0.4591(0.7091)
2023/11/09 06:00:05 - INFO - root -   Epoch: [28/300][320/341], lr: 0.00000014 	 loss = 0.4202(0.7097)
2023/11/09 06:00:58 - INFO - root -   Epoch: [28/300][340/341], lr: 0.00000014 	 loss = 1.0288(0.7072)
2023/11/09 06:00:58 - INFO - root -   Epoch: [28/300] 	 loss = 0.7072
2023/11/09 06:00:58 - INFO - root -   train_accuracy = 0.5689
2023/11/09 06:01:20 - INFO - root -   Epoch: [29/300][0/341], lr: 0.00000014 	 loss = 0.7068(0.7068)
2023/11/09 06:02:34 - INFO - root -   Epoch: [29/300][20/341], lr: 0.00000014 	 loss = 0.3027(0.6498)
2023/11/09 06:03:38 - INFO - root -   Epoch: [29/300][40/341], lr: 0.00000014 	 loss = 0.4884(0.6715)
2023/11/09 06:04:30 - INFO - root -   Epoch: [29/300][60/341], lr: 0.00000014 	 loss = 0.4193(0.7434)
2023/11/09 06:05:55 - INFO - root -   Epoch: [29/300][80/341], lr: 0.00000014 	 loss = 0.8314(0.7357)
2023/11/09 06:06:46 - INFO - root -   Epoch: [29/300][100/341], lr: 0.00000014 	 loss = 0.2210(0.7213)
2023/11/09 06:08:10 - INFO - root -   Epoch: [29/300][120/341], lr: 0.00000014 	 loss = 0.6414(0.7105)
2023/11/09 06:09:07 - INFO - root -   Epoch: [29/300][140/341], lr: 0.00000014 	 loss = 0.2671(0.7172)
2023/11/09 06:10:31 - INFO - root -   Epoch: [29/300][160/341], lr: 0.00000014 	 loss = 0.4066(0.7094)
2023/11/09 06:11:12 - INFO - root -   Epoch: [29/300][180/341], lr: 0.00000014 	 loss = 0.3132(0.7186)
2023/11/09 06:12:37 - INFO - root -   Epoch: [29/300][200/341], lr: 0.00000014 	 loss = 0.5834(0.7076)
2023/11/09 06:13:21 - INFO - root -   Epoch: [29/300][220/341], lr: 0.00000014 	 loss = 0.2776(0.6955)
2023/11/09 06:14:47 - INFO - root -   Epoch: [29/300][240/341], lr: 0.00000014 	 loss = 0.7823(0.6964)
2023/11/09 06:15:44 - INFO - root -   Epoch: [29/300][260/341], lr: 0.00000014 	 loss = 0.6936(0.7024)
2023/11/09 06:17:04 - INFO - root -   Epoch: [29/300][280/341], lr: 0.00000014 	 loss = 0.6294(0.7078)
2023/11/09 06:17:54 - INFO - root -   Epoch: [29/300][300/341], lr: 0.00000014 	 loss = 0.6851(0.7081)
2023/11/09 06:19:15 - INFO - root -   Epoch: [29/300][320/341], lr: 0.00000014 	 loss = 0.2531(0.7072)
2023/11/09 06:19:54 - INFO - root -   Epoch: [29/300][340/341], lr: 0.00000014 	 loss = 0.9037(0.7130)
2023/11/09 06:19:54 - INFO - root -   Epoch: [29/300] 	 loss = 0.7130
2023/11/09 06:23:38 - INFO - root -   precision = 0.6784
2023/11/09 06:23:38 - INFO - root -   eval_loss = 0.6089
2023/11/09 06:23:38 - INFO - root -   eval_acc = 0.6784
2023/11/09 06:23:40 - INFO - root -   train_accuracy = 0.5587
2023/11/09 06:24:18 - INFO - root -   Epoch: [30/300][0/341], lr: 0.00000014 	 loss = 1.0044(1.0044)
2023/11/09 06:25:20 - INFO - root -   Epoch: [30/300][20/341], lr: 0.00000014 	 loss = 0.3675(0.6811)
2023/11/09 06:26:35 - INFO - root -   Epoch: [30/300][40/341], lr: 0.00000014 	 loss = 0.1976(0.6645)
2023/11/09 06:27:47 - INFO - root -   Epoch: [30/300][60/341], lr: 0.00000014 	 loss = 0.9514(0.6968)
2023/11/09 06:28:54 - INFO - root -   Epoch: [30/300][80/341], lr: 0.00000014 	 loss = 0.6092(0.6793)
2023/11/09 06:29:53 - INFO - root -   Epoch: [30/300][100/341], lr: 0.00000014 	 loss = 0.7192(0.6811)
2023/11/09 06:31:10 - INFO - root -   Epoch: [30/300][120/341], lr: 0.00000014 	 loss = 0.7180(0.6775)
2023/11/09 06:32:24 - INFO - root -   Epoch: [30/300][140/341], lr: 0.00000014 	 loss = 0.4580(0.6749)
2023/11/09 06:33:20 - INFO - root -   Epoch: [30/300][160/341], lr: 0.00000014 	 loss = 0.2566(0.6647)
2023/11/09 06:34:43 - INFO - root -   Epoch: [30/300][180/341], lr: 0.00000014 	 loss = 0.5785(0.6674)
2023/11/09 06:35:32 - INFO - root -   Epoch: [30/300][200/341], lr: 0.00000014 	 loss = 0.2952(0.6726)
2023/11/09 06:36:48 - INFO - root -   Epoch: [30/300][220/341], lr: 0.00000014 	 loss = 0.5551(0.6792)
2023/11/09 06:37:52 - INFO - root -   Epoch: [30/300][240/341], lr: 0.00000014 	 loss = 1.2696(0.6818)
2023/11/09 06:39:04 - INFO - root -   Epoch: [30/300][260/341], lr: 0.00000014 	 loss = 0.5824(0.6821)
2023/11/09 06:39:59 - INFO - root -   Epoch: [30/300][280/341], lr: 0.00000014 	 loss = 0.4913(0.6805)
2023/11/09 06:41:10 - INFO - root -   Epoch: [30/300][300/341], lr: 0.00000014 	 loss = 0.6164(0.6809)
2023/11/09 06:42:05 - INFO - root -   Epoch: [30/300][320/341], lr: 0.00000014 	 loss = 0.1155(0.6776)
2023/11/09 06:42:58 - INFO - root -   Epoch: [30/300][340/341], lr: 0.00000014 	 loss = 1.4015(0.6829)
2023/11/09 06:42:58 - INFO - root -   Epoch: [30/300] 	 loss = 0.6829
2023/11/09 06:42:58 - INFO - root -   train_accuracy = 0.6012
2023/11/09 06:43:28 - INFO - root -   Epoch: [31/300][0/341], lr: 0.00000015 	 loss = 1.0148(1.0148)
2023/11/09 06:44:41 - INFO - root -   Epoch: [31/300][20/341], lr: 0.00000015 	 loss = 0.4094(0.6460)
2023/11/09 06:45:46 - INFO - root -   Epoch: [31/300][40/341], lr: 0.00000015 	 loss = 0.3921(0.6574)
2023/11/09 06:46:49 - INFO - root -   Epoch: [31/300][60/341], lr: 0.00000015 	 loss = 0.5755(0.6939)
2023/11/09 06:48:03 - INFO - root -   Epoch: [31/300][80/341], lr: 0.00000015 	 loss = 0.6804(0.6937)
2023/11/09 06:49:13 - INFO - root -   Epoch: [31/300][100/341], lr: 0.00000015 	 loss = 0.4335(0.6853)
2023/11/09 06:50:25 - INFO - root -   Epoch: [31/300][120/341], lr: 0.00000015 	 loss = 0.5465(0.6764)
2023/11/09 06:51:19 - INFO - root -   Epoch: [31/300][140/341], lr: 0.00000015 	 loss = 0.3112(0.6686)
2023/11/09 06:52:36 - INFO - root -   Epoch: [31/300][160/341], lr: 0.00000015 	 loss = 0.3195(0.6631)
2023/11/09 06:53:42 - INFO - root -   Epoch: [31/300][180/341], lr: 0.00000015 	 loss = 0.3460(0.6588)
2023/11/09 06:54:47 - INFO - root -   Epoch: [31/300][200/341], lr: 0.00000015 	 loss = 0.5722(0.6602)
2023/11/09 06:56:15 - INFO - root -   Epoch: [31/300][220/341], lr: 0.00000015 	 loss = 0.4959(0.6536)
2023/11/09 06:57:11 - INFO - root -   Epoch: [31/300][240/341], lr: 0.00000015 	 loss = 0.3165(0.6552)
2023/11/09 06:58:13 - INFO - root -   Epoch: [31/300][260/341], lr: 0.00000015 	 loss = 1.0333(0.6597)
2023/11/09 06:59:16 - INFO - root -   Epoch: [31/300][280/341], lr: 0.00000015 	 loss = 0.4273(0.6662)
2023/11/09 07:00:33 - INFO - root -   Epoch: [31/300][300/341], lr: 0.00000015 	 loss = 0.7661(0.6726)
2023/11/09 07:01:31 - INFO - root -   Epoch: [31/300][320/341], lr: 0.00000015 	 loss = 0.1920(0.6712)
2023/11/09 07:02:20 - INFO - root -   Epoch: [31/300][340/341], lr: 0.00000015 	 loss = 0.8885(0.6734)
2023/11/09 07:02:20 - INFO - root -   Epoch: [31/300] 	 loss = 0.6734
2023/11/09 07:02:20 - INFO - root -   train_accuracy = 0.6056
2023/11/09 07:02:42 - INFO - root -   Epoch: [32/300][0/341], lr: 0.00000015 	 loss = 0.5541(0.5541)
2023/11/09 07:03:50 - INFO - root -   Epoch: [32/300][20/341], lr: 0.00000015 	 loss = 0.6204(0.7897)
2023/11/09 07:04:55 - INFO - root -   Epoch: [32/300][40/341], lr: 0.00000015 	 loss = 0.9241(0.7252)
2023/11/09 07:06:05 - INFO - root -   Epoch: [32/300][60/341], lr: 0.00000015 	 loss = 0.8687(0.7531)
2023/11/09 07:07:23 - INFO - root -   Epoch: [32/300][80/341], lr: 0.00000015 	 loss = 0.8670(0.7542)
2023/11/09 07:08:19 - INFO - root -   Epoch: [32/300][100/341], lr: 0.00000015 	 loss = 0.6420(0.7654)
2023/11/09 07:09:37 - INFO - root -   Epoch: [32/300][120/341], lr: 0.00000015 	 loss = 0.6666(0.7616)
2023/11/09 07:10:34 - INFO - root -   Epoch: [32/300][140/341], lr: 0.00000015 	 loss = 0.7831(0.7550)
2023/11/09 07:11:49 - INFO - root -   Epoch: [32/300][160/341], lr: 0.00000015 	 loss = 0.3493(0.7348)
2023/11/09 07:13:07 - INFO - root -   Epoch: [32/300][180/341], lr: 0.00000015 	 loss = 0.1760(0.7414)
2023/11/09 07:13:59 - INFO - root -   Epoch: [32/300][200/341], lr: 0.00000015 	 loss = 0.4100(0.7308)
2023/11/09 07:15:15 - INFO - root -   Epoch: [32/300][220/341], lr: 0.00000015 	 loss = 0.5544(0.7275)
2023/11/09 07:16:03 - INFO - root -   Epoch: [32/300][240/341], lr: 0.00000015 	 loss = 0.4201(0.7120)
2023/11/09 07:17:20 - INFO - root -   Epoch: [32/300][260/341], lr: 0.00000015 	 loss = 0.5352(0.7115)
2023/11/09 07:18:19 - INFO - root -   Epoch: [32/300][280/341], lr: 0.00000015 	 loss = 0.5708(0.7137)
2023/11/09 07:19:36 - INFO - root -   Epoch: [32/300][300/341], lr: 0.00000015 	 loss = 1.1615(0.7166)
2023/11/09 07:21:01 - INFO - root -   Epoch: [32/300][320/341], lr: 0.00000015 	 loss = 0.2625(0.7076)
2023/11/09 07:21:33 - INFO - root -   Epoch: [32/300][340/341], lr: 0.00000015 	 loss = 1.3028(0.7079)
2023/11/09 07:21:33 - INFO - root -   Epoch: [32/300] 	 loss = 0.7079
2023/11/09 07:21:33 - INFO - root -   train_accuracy = 0.5792
2023/11/09 07:21:55 - INFO - root -   Epoch: [33/300][0/341], lr: 0.00000015 	 loss = 0.5435(0.5435)
2023/11/09 07:23:05 - INFO - root -   Epoch: [33/300][20/341], lr: 0.00000015 	 loss = 0.5411(0.5486)
2023/11/09 07:24:04 - INFO - root -   Epoch: [33/300][40/341], lr: 0.00000015 	 loss = 0.2667(0.6530)
2023/11/09 07:25:17 - INFO - root -   Epoch: [33/300][60/341], lr: 0.00000015 	 loss = 0.4307(0.6661)
2023/11/09 07:26:24 - INFO - root -   Epoch: [33/300][80/341], lr: 0.00000015 	 loss = 0.4230(0.6722)
2023/11/09 07:27:32 - INFO - root -   Epoch: [33/300][100/341], lr: 0.00000015 	 loss = 0.6994(0.6788)
2023/11/09 07:28:44 - INFO - root -   Epoch: [33/300][120/341], lr: 0.00000015 	 loss = 0.3428(0.6885)
2023/11/09 07:29:37 - INFO - root -   Epoch: [33/300][140/341], lr: 0.00000015 	 loss = 0.4226(0.6898)
2023/11/09 07:30:55 - INFO - root -   Epoch: [33/300][160/341], lr: 0.00000015 	 loss = 0.4325(0.6781)
2023/11/09 07:31:54 - INFO - root -   Epoch: [33/300][180/341], lr: 0.00000015 	 loss = 0.8814(0.6726)
2023/11/09 07:33:11 - INFO - root -   Epoch: [33/300][200/341], lr: 0.00000015 	 loss = 0.5685(0.6772)
2023/11/09 07:34:20 - INFO - root -   Epoch: [33/300][220/341], lr: 0.00000015 	 loss = 0.4367(0.6833)
2023/11/09 07:35:18 - INFO - root -   Epoch: [33/300][240/341], lr: 0.00000015 	 loss = 0.6489(0.6765)
2023/11/09 07:36:32 - INFO - root -   Epoch: [33/300][260/341], lr: 0.00000015 	 loss = 0.4258(0.6858)
2023/11/09 07:37:40 - INFO - root -   Epoch: [33/300][280/341], lr: 0.00000015 	 loss = 0.4117(0.6833)
2023/11/09 07:38:47 - INFO - root -   Epoch: [33/300][300/341], lr: 0.00000015 	 loss = 1.0197(0.6916)
2023/11/09 07:39:37 - INFO - root -   Epoch: [33/300][320/341], lr: 0.00000015 	 loss = 0.4871(0.6911)
2023/11/09 07:40:24 - INFO - root -   Epoch: [33/300][340/341], lr: 0.00000015 	 loss = 0.9249(0.6931)
2023/11/09 07:40:24 - INFO - root -   Epoch: [33/300] 	 loss = 0.6931
2023/11/09 07:40:24 - INFO - root -   train_accuracy = 0.5997
2023/11/09 07:41:02 - INFO - root -   Epoch: [34/300][0/341], lr: 0.00000015 	 loss = 0.7628(0.7628)
2023/11/09 07:41:46 - INFO - root -   Epoch: [34/300][20/341], lr: 0.00000015 	 loss = 0.4642(0.7763)
2023/11/09 07:43:10 - INFO - root -   Epoch: [34/300][40/341], lr: 0.00000015 	 loss = 0.3567(0.7154)
2023/11/09 07:44:28 - INFO - root -   Epoch: [34/300][60/341], lr: 0.00000015 	 loss = 0.9474(0.6722)
2023/11/09 07:45:12 - INFO - root -   Epoch: [34/300][80/341], lr: 0.00000015 	 loss = 0.7923(0.6643)
2023/11/09 07:46:31 - INFO - root -   Epoch: [34/300][100/341], lr: 0.00000015 	 loss = 0.7494(0.6657)
2023/11/09 07:47:24 - INFO - root -   Epoch: [34/300][120/341], lr: 0.00000015 	 loss = 0.4615(0.6543)
2023/11/09 07:48:39 - INFO - root -   Epoch: [34/300][140/341], lr: 0.00000015 	 loss = 0.5083(0.6672)
2023/11/09 07:49:36 - INFO - root -   Epoch: [34/300][160/341], lr: 0.00000015 	 loss = 0.2813(0.6703)
2023/11/09 07:50:52 - INFO - root -   Epoch: [34/300][180/341], lr: 0.00000015 	 loss = 0.5828(0.6665)
2023/11/09 07:52:04 - INFO - root -   Epoch: [34/300][200/341], lr: 0.00000015 	 loss = 0.9707(0.6608)
2023/11/09 07:53:11 - INFO - root -   Epoch: [34/300][220/341], lr: 0.00000015 	 loss = 0.3105(0.6568)
2023/11/09 07:54:05 - INFO - root -   Epoch: [34/300][240/341], lr: 0.00000015 	 loss = 0.3739(0.6564)
2023/11/09 07:55:27 - INFO - root -   Epoch: [34/300][260/341], lr: 0.00000015 	 loss = 0.4849(0.6696)
2023/11/09 07:56:21 - INFO - root -   Epoch: [34/300][280/341], lr: 0.00000015 	 loss = 0.8512(0.6828)
2023/11/09 07:57:27 - INFO - root -   Epoch: [34/300][300/341], lr: 0.00000015 	 loss = 0.3839(0.6880)
2023/11/09 07:58:17 - INFO - root -   Epoch: [34/300][320/341], lr: 0.00000015 	 loss = 0.1677(0.6859)
2023/11/09 07:59:10 - INFO - root -   Epoch: [34/300][340/341], lr: 0.00000015 	 loss = 1.0924(0.6868)
2023/11/09 07:59:10 - INFO - root -   Epoch: [34/300] 	 loss = 0.6868
2023/11/09 08:02:54 - INFO - root -   precision = 0.6901
2023/11/09 08:02:54 - INFO - root -   eval_loss = 0.5989
2023/11/09 08:02:54 - INFO - root -   eval_acc = 0.6901
2023/11/09 08:02:55 - INFO - root -   train_accuracy = 0.5924
2023/11/09 08:03:19 - INFO - root -   Epoch: [35/300][0/341], lr: 0.00000015 	 loss = 0.6839(0.6839)
2023/11/09 08:04:28 - INFO - root -   Epoch: [35/300][20/341], lr: 0.00000015 	 loss = 0.8253(0.7875)
2023/11/09 08:05:37 - INFO - root -   Epoch: [35/300][40/341], lr: 0.00000015 	 loss = 0.2245(0.7408)
2023/11/09 08:06:40 - INFO - root -   Epoch: [35/300][60/341], lr: 0.00000015 	 loss = 0.5883(0.6915)
2023/11/09 08:07:46 - INFO - root -   Epoch: [35/300][80/341], lr: 0.00000015 	 loss = 0.9122(0.7048)
2023/11/09 08:08:53 - INFO - root -   Epoch: [35/300][100/341], lr: 0.00000015 	 loss = 0.5681(0.6936)
2023/11/09 08:10:13 - INFO - root -   Epoch: [35/300][120/341], lr: 0.00000015 	 loss = 0.8797(0.7009)
2023/11/09 08:11:18 - INFO - root -   Epoch: [35/300][140/341], lr: 0.00000015 	 loss = 0.7897(0.7256)
2023/11/09 08:12:37 - INFO - root -   Epoch: [35/300][160/341], lr: 0.00000015 	 loss = 0.4976(0.7316)
2023/11/09 08:13:48 - INFO - root -   Epoch: [35/300][180/341], lr: 0.00000015 	 loss = 0.4241(0.7290)
2023/11/09 08:15:00 - INFO - root -   Epoch: [35/300][200/341], lr: 0.00000015 	 loss = 0.3734(0.7309)
2023/11/09 08:16:11 - INFO - root -   Epoch: [35/300][220/341], lr: 0.00000015 	 loss = 0.3947(0.7197)
2023/11/09 08:17:19 - INFO - root -   Epoch: [35/300][240/341], lr: 0.00000015 	 loss = 0.4963(0.7149)
2023/11/09 08:18:50 - INFO - root -   Epoch: [35/300][260/341], lr: 0.00000015 	 loss = 0.3645(0.7106)
2023/11/09 08:19:44 - INFO - root -   Epoch: [35/300][280/341], lr: 0.00000015 	 loss = 0.8728(0.7144)
2023/11/09 08:21:01 - INFO - root -   Epoch: [35/300][300/341], lr: 0.00000015 	 loss = 0.6248(0.7042)
2023/11/09 08:22:01 - INFO - root -   Epoch: [35/300][320/341], lr: 0.00000015 	 loss = 0.4856(0.7011)
2023/11/09 08:22:40 - INFO - root -   Epoch: [35/300][340/341], lr: 0.00000015 	 loss = 0.5332(0.6999)
2023/11/09 08:22:40 - INFO - root -   Epoch: [35/300] 	 loss = 0.6999
2023/11/09 08:22:40 - INFO - root -   train_accuracy = 0.5953
2023/11/09 08:23:03 - INFO - root -   Epoch: [36/300][0/341], lr: 0.00000015 	 loss = 1.0541(1.0541)
2023/11/09 08:24:20 - INFO - root -   Epoch: [36/300][20/341], lr: 0.00000015 	 loss = 0.6641(0.8034)
2023/11/09 08:25:13 - INFO - root -   Epoch: [36/300][40/341], lr: 0.00000015 	 loss = 0.1697(0.7131)
2023/11/09 08:26:32 - INFO - root -   Epoch: [36/300][60/341], lr: 0.00000015 	 loss = 1.0456(0.7202)
2023/11/09 08:27:44 - INFO - root -   Epoch: [36/300][80/341], lr: 0.00000015 	 loss = 0.4220(0.7266)
2023/11/09 08:28:53 - INFO - root -   Epoch: [36/300][100/341], lr: 0.00000015 	 loss = 0.7659(0.7079)
2023/11/09 08:30:01 - INFO - root -   Epoch: [36/300][120/341], lr: 0.00000015 	 loss = 0.7173(0.7009)
2023/11/09 08:31:16 - INFO - root -   Epoch: [36/300][140/341], lr: 0.00000015 	 loss = 0.6091(0.6973)
2023/11/09 08:32:10 - INFO - root -   Epoch: [36/300][160/341], lr: 0.00000015 	 loss = 0.5459(0.6831)
2023/11/09 08:33:16 - INFO - root -   Epoch: [36/300][180/341], lr: 0.00000015 	 loss = 1.1353(0.6833)
2023/11/09 08:34:40 - INFO - root -   Epoch: [36/300][200/341], lr: 0.00000015 	 loss = 0.4916(0.6897)
2023/11/09 08:35:39 - INFO - root -   Epoch: [36/300][220/341], lr: 0.00000015 	 loss = 0.3729(0.6901)
2023/11/09 08:37:12 - INFO - root -   Epoch: [36/300][240/341], lr: 0.00000015 	 loss = 0.4712(0.6884)
2023/11/09 08:37:58 - INFO - root -   Epoch: [36/300][260/341], lr: 0.00000015 	 loss = 0.4920(0.6917)
2023/11/09 08:39:13 - INFO - root -   Epoch: [36/300][280/341], lr: 0.00000015 	 loss = 0.1903(0.6947)
2023/11/09 08:40:11 - INFO - root -   Epoch: [36/300][300/341], lr: 0.00000015 	 loss = 0.8481(0.6914)
2023/11/09 08:41:17 - INFO - root -   Epoch: [36/300][320/341], lr: 0.00000015 	 loss = 0.3141(0.6892)
2023/11/09 08:41:50 - INFO - root -   Epoch: [36/300][340/341], lr: 0.00000015 	 loss = 0.9145(0.6900)
2023/11/09 08:41:50 - INFO - root -   Epoch: [36/300] 	 loss = 0.6900
2023/11/09 08:41:50 - INFO - root -   train_accuracy = 0.6114
2023/11/09 08:42:20 - INFO - root -   Epoch: [37/300][0/341], lr: 0.00000015 	 loss = 1.0887(1.0887)
2023/11/09 08:43:27 - INFO - root -   Epoch: [37/300][20/341], lr: 0.00000015 	 loss = 0.6837(0.6785)
2023/11/09 08:44:38 - INFO - root -   Epoch: [37/300][40/341], lr: 0.00000015 	 loss = 0.3625(0.6544)
2023/11/09 08:45:54 - INFO - root -   Epoch: [37/300][60/341], lr: 0.00000015 	 loss = 0.4132(0.6868)
2023/11/09 08:47:08 - INFO - root -   Epoch: [37/300][80/341], lr: 0.00000015 	 loss = 0.4250(0.6611)
2023/11/09 08:48:10 - INFO - root -   Epoch: [37/300][100/341], lr: 0.00000015 	 loss = 0.2164(0.6838)
2023/11/09 08:48:59 - INFO - root -   Epoch: [37/300][120/341], lr: 0.00000015 	 loss = 0.2865(0.6643)
2023/11/09 08:50:15 - INFO - root -   Epoch: [37/300][140/341], lr: 0.00000015 	 loss = 0.4627(0.6759)
2023/11/09 08:51:06 - INFO - root -   Epoch: [37/300][160/341], lr: 0.00000015 	 loss = 0.4540(0.6809)
2023/11/09 08:52:35 - INFO - root -   Epoch: [37/300][180/341], lr: 0.00000015 	 loss = 0.6901(0.6826)
2023/11/09 08:53:29 - INFO - root -   Epoch: [37/300][200/341], lr: 0.00000015 	 loss = 1.3883(0.6900)
2023/11/09 08:54:36 - INFO - root -   Epoch: [37/300][220/341], lr: 0.00000015 	 loss = 0.7336(0.6944)
2023/11/09 08:55:51 - INFO - root -   Epoch: [37/300][240/341], lr: 0.00000015 	 loss = 0.4233(0.6848)
2023/11/09 08:57:02 - INFO - root -   Epoch: [37/300][260/341], lr: 0.00000015 	 loss = 0.3401(0.6848)
2023/11/09 08:58:15 - INFO - root -   Epoch: [37/300][280/341], lr: 0.00000015 	 loss = 0.4761(0.6885)
2023/11/09 08:59:06 - INFO - root -   Epoch: [37/300][300/341], lr: 0.00000015 	 loss = 0.3529(0.6835)
2023/11/09 09:00:17 - INFO - root -   Epoch: [37/300][320/341], lr: 0.00000015 	 loss = 0.6190(0.6864)
2023/11/09 09:01:07 - INFO - root -   Epoch: [37/300][340/341], lr: 0.00000015 	 loss = 0.3718(0.6899)
2023/11/09 09:01:07 - INFO - root -   Epoch: [37/300] 	 loss = 0.6899
2023/11/09 09:01:07 - INFO - root -   train_accuracy = 0.6026
2023/11/09 09:01:37 - INFO - root -   Epoch: [38/300][0/341], lr: 0.00000016 	 loss = 0.8516(0.8516)
2023/11/09 09:02:35 - INFO - root -   Epoch: [38/300][20/341], lr: 0.00000016 	 loss = 1.0590(0.8208)
2023/11/09 09:03:57 - INFO - root -   Epoch: [38/300][40/341], lr: 0.00000016 	 loss = 0.8711(0.7676)
2023/11/09 09:04:53 - INFO - root -   Epoch: [38/300][60/341], lr: 0.00000016 	 loss = 0.5113(0.7060)
2023/11/09 09:05:59 - INFO - root -   Epoch: [38/300][80/341], lr: 0.00000016 	 loss = 0.7389(0.6989)
2023/11/09 09:07:04 - INFO - root -   Epoch: [38/300][100/341], lr: 0.00000016 	 loss = 0.3836(0.6856)
2023/11/09 09:07:59 - INFO - root -   Epoch: [38/300][120/341], lr: 0.00000016 	 loss = 0.5631(0.6818)
2023/11/09 09:09:20 - INFO - root -   Epoch: [38/300][140/341], lr: 0.00000016 	 loss = 0.3023(0.6783)
2023/11/09 09:10:09 - INFO - root -   Epoch: [38/300][160/341], lr: 0.00000016 	 loss = 0.4729(0.6815)
2023/11/09 09:11:24 - INFO - root -   Epoch: [38/300][180/341], lr: 0.00000016 	 loss = 0.5752(0.7052)
2023/11/09 09:12:31 - INFO - root -   Epoch: [38/300][200/341], lr: 0.00000016 	 loss = 0.4606(0.7051)
2023/11/09 09:13:42 - INFO - root -   Epoch: [38/300][220/341], lr: 0.00000016 	 loss = 0.4750(0.6993)
2023/11/09 09:14:37 - INFO - root -   Epoch: [38/300][240/341], lr: 0.00000016 	 loss = 0.1556(0.6955)
2023/11/09 09:16:02 - INFO - root -   Epoch: [38/300][260/341], lr: 0.00000016 	 loss = 0.3552(0.6958)
2023/11/09 09:17:01 - INFO - root -   Epoch: [38/300][280/341], lr: 0.00000016 	 loss = 0.7295(0.6957)
2023/11/09 09:18:29 - INFO - root -   Epoch: [38/300][300/341], lr: 0.00000016 	 loss = 0.5893(0.6908)
2023/11/09 09:19:27 - INFO - root -   Epoch: [38/300][320/341], lr: 0.00000016 	 loss = 0.5212(0.6838)
2023/11/09 09:20:10 - INFO - root -   Epoch: [38/300][340/341], lr: 0.00000016 	 loss = 0.9707(0.6849)
2023/11/09 09:20:11 - INFO - root -   Epoch: [38/300] 	 loss = 0.6849
2023/11/09 09:20:11 - INFO - root -   train_accuracy = 0.6232
2023/11/09 09:20:33 - INFO - root -   Epoch: [39/300][0/341], lr: 0.00000016 	 loss = 0.5466(0.5466)
2023/11/09 09:21:56 - INFO - root -   Epoch: [39/300][20/341], lr: 0.00000016 	 loss = 0.4280(0.7226)
2023/11/09 09:23:01 - INFO - root -   Epoch: [39/300][40/341], lr: 0.00000016 	 loss = 0.3100(0.6672)
2023/11/09 09:24:18 - INFO - root -   Epoch: [39/300][60/341], lr: 0.00000016 	 loss = 0.4700(0.6874)
2023/11/09 09:25:19 - INFO - root -   Epoch: [39/300][80/341], lr: 0.00000016 	 loss = 1.0840(0.6665)
2023/11/09 09:26:42 - INFO - root -   Epoch: [39/300][100/341], lr: 0.00000016 	 loss = 1.0381(0.6692)
2023/11/09 09:27:30 - INFO - root -   Epoch: [39/300][120/341], lr: 0.00000016 	 loss = 0.3492(0.6730)
2023/11/09 09:28:45 - INFO - root -   Epoch: [39/300][140/341], lr: 0.00000016 	 loss = 0.8175(0.6697)
2023/11/09 09:29:45 - INFO - root -   Epoch: [39/300][160/341], lr: 0.00000016 	 loss = 0.4979(0.6712)
2023/11/09 09:31:22 - INFO - root -   Epoch: [39/300][180/341], lr: 0.00000016 	 loss = 0.4522(0.6614)
2023/11/09 09:32:24 - INFO - root -   Epoch: [39/300][200/341], lr: 0.00000016 	 loss = 0.3416(0.6651)
2023/11/09 09:33:26 - INFO - root -   Epoch: [39/300][220/341], lr: 0.00000016 	 loss = 0.4922(0.6570)
2023/11/09 09:34:30 - INFO - root -   Epoch: [39/300][240/341], lr: 0.00000016 	 loss = 0.5189(0.6644)
2023/11/09 09:35:35 - INFO - root -   Epoch: [39/300][260/341], lr: 0.00000016 	 loss = 0.2253(0.6697)
2023/11/09 09:36:33 - INFO - root -   Epoch: [39/300][280/341], lr: 0.00000016 	 loss = 0.7964(0.6719)
2023/11/09 09:37:39 - INFO - root -   Epoch: [39/300][300/341], lr: 0.00000016 	 loss = 0.9201(0.6760)
2023/11/09 09:38:55 - INFO - root -   Epoch: [39/300][320/341], lr: 0.00000016 	 loss = 0.2763(0.6822)
2023/11/09 09:39:39 - INFO - root -   Epoch: [39/300][340/341], lr: 0.00000016 	 loss = 0.6556(0.6850)
2023/11/09 09:39:39 - INFO - root -   Epoch: [39/300] 	 loss = 0.6850
2023/11/09 09:43:23 - INFO - root -   precision = 0.6901
2023/11/09 09:43:23 - INFO - root -   eval_loss = 0.5944
2023/11/09 09:43:23 - INFO - root -   eval_acc = 0.6901
2023/11/09 09:43:25 - INFO - root -   train_accuracy = 0.6085
2023/11/09 09:43:47 - INFO - root -   Epoch: [40/300][0/341], lr: 0.00000016 	 loss = 0.7835(0.7835)
2023/11/09 09:45:01 - INFO - root -   Epoch: [40/300][20/341], lr: 0.00000016 	 loss = 0.8896(0.6847)
2023/11/09 09:46:16 - INFO - root -   Epoch: [40/300][40/341], lr: 0.00000016 	 loss = 0.4754(0.6118)
2023/11/09 09:47:15 - INFO - root -   Epoch: [40/300][60/341], lr: 0.00000016 	 loss = 0.4190(0.6265)
2023/11/09 09:48:32 - INFO - root -   Epoch: [40/300][80/341], lr: 0.00000016 	 loss = 0.3018(0.6236)
2023/11/09 09:49:24 - INFO - root -   Epoch: [40/300][100/341], lr: 0.00000016 	 loss = 0.6654(0.6195)
2023/11/09 09:50:40 - INFO - root -   Epoch: [40/300][120/341], lr: 0.00000016 	 loss = 0.6240(0.6063)
2023/11/09 09:51:56 - INFO - root -   Epoch: [40/300][140/341], lr: 0.00000016 	 loss = 0.5419(0.6348)
2023/11/09 09:53:05 - INFO - root -   Epoch: [40/300][160/341], lr: 0.00000016 	 loss = 0.3755(0.6421)
2023/11/09 09:54:16 - INFO - root -   Epoch: [40/300][180/341], lr: 0.00000016 	 loss = 0.2980(0.6438)
2023/11/09 09:55:06 - INFO - root -   Epoch: [40/300][200/341], lr: 0.00000016 	 loss = 0.7083(0.6499)
2023/11/09 09:56:32 - INFO - root -   Epoch: [40/300][220/341], lr: 0.00000016 	 loss = 0.4787(0.6473)
2023/11/09 09:57:15 - INFO - root -   Epoch: [40/300][240/341], lr: 0.00000016 	 loss = 0.3266(0.6412)
2023/11/09 09:58:31 - INFO - root -   Epoch: [40/300][260/341], lr: 0.00000016 	 loss = 0.6203(0.6431)
2023/11/09 09:59:30 - INFO - root -   Epoch: [40/300][280/341], lr: 0.00000016 	 loss = 0.2085(0.6392)
2023/11/09 10:00:55 - INFO - root -   Epoch: [40/300][300/341], lr: 0.00000016 	 loss = 0.9676(0.6392)
2023/11/09 10:01:36 - INFO - root -   Epoch: [40/300][320/341], lr: 0.00000016 	 loss = 0.3251(0.6492)
2023/11/09 10:02:25 - INFO - root -   Epoch: [40/300][340/341], lr: 0.00000016 	 loss = 1.4002(0.6497)
2023/11/09 10:02:25 - INFO - root -   Epoch: [40/300] 	 loss = 0.6497
2023/11/09 10:02:25 - INFO - root -   train_accuracy = 0.6364
2023/11/09 10:02:47 - INFO - root -   Epoch: [41/300][0/341], lr: 0.00000016 	 loss = 0.4757(0.4757)
2023/11/09 10:04:02 - INFO - root -   Epoch: [41/300][20/341], lr: 0.00000016 	 loss = 0.7399(0.6843)
2023/11/09 10:05:03 - INFO - root -   Epoch: [41/300][40/341], lr: 0.00000016 	 loss = 0.3061(0.6908)
2023/11/09 10:06:10 - INFO - root -   Epoch: [41/300][60/341], lr: 0.00000016 	 loss = 0.4006(0.6965)
2023/11/09 10:07:24 - INFO - root -   Epoch: [41/300][80/341], lr: 0.00000016 	 loss = 0.7866(0.6957)
2023/11/09 10:08:20 - INFO - root -   Epoch: [41/300][100/341], lr: 0.00000016 	 loss = 0.6439(0.6879)
2023/11/09 10:09:42 - INFO - root -   Epoch: [41/300][120/341], lr: 0.00000016 	 loss = 0.3069(0.6746)
2023/11/09 10:11:09 - INFO - root -   Epoch: [41/300][140/341], lr: 0.00000016 	 loss = 0.3315(0.6673)
2023/11/09 10:12:08 - INFO - root -   Epoch: [41/300][160/341], lr: 0.00000016 	 loss = 0.2184(0.6721)
2023/11/09 10:13:29 - INFO - root -   Epoch: [41/300][180/341], lr: 0.00000016 	 loss = 0.1377(0.6691)
2023/11/09 10:14:14 - INFO - root -   Epoch: [41/300][200/341], lr: 0.00000016 	 loss = 0.5144(0.6628)
2023/11/09 10:15:46 - INFO - root -   Epoch: [41/300][220/341], lr: 0.00000016 	 loss = 0.3378(0.6540)
2023/11/09 10:16:34 - INFO - root -   Epoch: [41/300][240/341], lr: 0.00000016 	 loss = 0.3316(0.6538)
2023/11/09 10:17:50 - INFO - root -   Epoch: [41/300][260/341], lr: 0.00000016 	 loss = 0.7333(0.6648)
2023/11/09 10:18:43 - INFO - root -   Epoch: [41/300][280/341], lr: 0.00000016 	 loss = 0.4103(0.6647)
2023/11/09 10:20:09 - INFO - root -   Epoch: [41/300][300/341], lr: 0.00000016 	 loss = 0.6446(0.6747)
2023/11/09 10:20:57 - INFO - root -   Epoch: [41/300][320/341], lr: 0.00000016 	 loss = 0.2433(0.6819)
2023/11/09 10:21:52 - INFO - root -   Epoch: [41/300][340/341], lr: 0.00000016 	 loss = 0.7379(0.6846)
2023/11/09 10:21:52 - INFO - root -   Epoch: [41/300] 	 loss = 0.6846
2023/11/09 10:21:52 - INFO - root -   train_accuracy = 0.5894
2023/11/09 10:22:14 - INFO - root -   Epoch: [42/300][0/341], lr: 0.00000016 	 loss = 0.6109(0.6109)
2023/11/09 10:23:30 - INFO - root -   Epoch: [42/300][20/341], lr: 0.00000016 	 loss = 1.1474(0.6315)
2023/11/09 10:24:30 - INFO - root -   Epoch: [42/300][40/341], lr: 0.00000016 	 loss = 0.2922(0.6428)
2023/11/09 10:25:45 - INFO - root -   Epoch: [42/300][60/341], lr: 0.00000016 	 loss = 0.3104(0.6644)
2023/11/09 10:26:53 - INFO - root -   Epoch: [42/300][80/341], lr: 0.00000016 	 loss = 0.6052(0.6535)
2023/11/09 10:28:10 - INFO - root -   Epoch: [42/300][100/341], lr: 0.00000016 	 loss = 0.1977(0.6579)
2023/11/09 10:28:57 - INFO - root -   Epoch: [42/300][120/341], lr: 0.00000016 	 loss = 0.2903(0.6605)
2023/11/09 10:29:54 - INFO - root -   Epoch: [42/300][140/341], lr: 0.00000016 	 loss = 0.8785(0.6739)
2023/11/09 10:31:18 - INFO - root -   Epoch: [42/300][160/341], lr: 0.00000016 	 loss = 0.3114(0.6628)
2023/11/09 10:32:32 - INFO - root -   Epoch: [42/300][180/341], lr: 0.00000016 	 loss = 0.3245(0.6683)
2023/11/09 10:33:35 - INFO - root -   Epoch: [42/300][200/341], lr: 0.00000016 	 loss = 0.3988(0.6796)
2023/11/09 10:34:38 - INFO - root -   Epoch: [42/300][220/341], lr: 0.00000016 	 loss = 0.5541(0.6801)
2023/11/09 10:35:55 - INFO - root -   Epoch: [42/300][240/341], lr: 0.00000016 	 loss = 0.4882(0.6803)
2023/11/09 10:37:06 - INFO - root -   Epoch: [42/300][260/341], lr: 0.00000016 	 loss = 0.6349(0.6882)
2023/11/09 10:38:02 - INFO - root -   Epoch: [42/300][280/341], lr: 0.00000016 	 loss = 0.8744(0.6855)
2023/11/09 10:39:10 - INFO - root -   Epoch: [42/300][300/341], lr: 0.00000016 	 loss = 0.8634(0.6869)
2023/11/09 10:40:14 - INFO - root -   Epoch: [42/300][320/341], lr: 0.00000016 	 loss = 0.3666(0.6895)
2023/11/09 10:41:03 - INFO - root -   Epoch: [42/300][340/341], lr: 0.00000016 	 loss = 0.8549(0.6905)
2023/11/09 10:41:04 - INFO - root -   Epoch: [42/300] 	 loss = 0.6905
2023/11/09 10:41:04 - INFO - root -   train_accuracy = 0.5982
2023/11/09 10:41:33 - INFO - root -   Epoch: [43/300][0/341], lr: 0.00000016 	 loss = 0.6952(0.6952)
2023/11/09 10:42:48 - INFO - root -   Epoch: [43/300][20/341], lr: 0.00000016 	 loss = 0.6085(0.7177)
2023/11/09 10:43:41 - INFO - root -   Epoch: [43/300][40/341], lr: 0.00000016 	 loss = 0.2142(0.6781)
2023/11/09 10:45:08 - INFO - root -   Epoch: [43/300][60/341], lr: 0.00000016 	 loss = 0.6797(0.6882)
2023/11/09 10:45:57 - INFO - root -   Epoch: [43/300][80/341], lr: 0.00000016 	 loss = 1.0999(0.6873)
2023/11/09 10:47:11 - INFO - root -   Epoch: [43/300][100/341], lr: 0.00000016 	 loss = 1.5285(0.6873)
2023/11/09 10:48:00 - INFO - root -   Epoch: [43/300][120/341], lr: 0.00000016 	 loss = 0.4000(0.6676)
2023/11/09 10:49:10 - INFO - root -   Epoch: [43/300][140/341], lr: 0.00000016 	 loss = 0.5181(0.6604)
2023/11/09 10:50:08 - INFO - root -   Epoch: [43/300][160/341], lr: 0.00000016 	 loss = 0.4096(0.6578)
2023/11/09 10:51:34 - INFO - root -   Epoch: [43/300][180/341], lr: 0.00000016 	 loss = 0.4598(0.6605)
2023/11/09 10:52:33 - INFO - root -   Epoch: [43/300][200/341], lr: 0.00000016 	 loss = 0.1242(0.6588)
2023/11/09 10:53:46 - INFO - root -   Epoch: [43/300][220/341], lr: 0.00000016 	 loss = 0.4037(0.6570)
2023/11/09 10:54:38 - INFO - root -   Epoch: [43/300][240/341], lr: 0.00000016 	 loss = 0.3312(0.6495)
2023/11/09 10:55:59 - INFO - root -   Epoch: [43/300][260/341], lr: 0.00000016 	 loss = 0.9531(0.6561)
2023/11/09 10:56:59 - INFO - root -   Epoch: [43/300][280/341], lr: 0.00000016 	 loss = 0.4082(0.6582)
2023/11/09 10:58:14 - INFO - root -   Epoch: [43/300][300/341], lr: 0.00000016 	 loss = 0.6117(0.6676)
2023/11/09 10:59:19 - INFO - root -   Epoch: [43/300][320/341], lr: 0.00000016 	 loss = 0.4824(0.6739)
2023/11/09 11:00:01 - INFO - root -   Epoch: [43/300][340/341], lr: 0.00000016 	 loss = 1.0096(0.6775)
2023/11/09 11:00:01 - INFO - root -   Epoch: [43/300] 	 loss = 0.6775
2023/11/09 11:00:01 - INFO - root -   train_accuracy = 0.6026
2023/11/09 11:00:23 - INFO - root -   Epoch: [44/300][0/341], lr: 0.00000016 	 loss = 0.5063(0.5063)
2023/11/09 11:01:38 - INFO - root -   Epoch: [44/300][20/341], lr: 0.00000016 	 loss = 0.9653(0.6424)
2023/11/09 11:02:49 - INFO - root -   Epoch: [44/300][40/341], lr: 0.00000016 	 loss = 0.2942(0.6342)
2023/11/09 11:03:49 - INFO - root -   Epoch: [44/300][60/341], lr: 0.00000016 	 loss = 0.7288(0.6346)
2023/11/09 11:04:49 - INFO - root -   Epoch: [44/300][80/341], lr: 0.00000016 	 loss = 0.6949(0.6523)
2023/11/09 11:06:29 - INFO - root -   Epoch: [44/300][100/341], lr: 0.00000016 	 loss = 0.5383(0.6622)
2023/11/09 11:07:22 - INFO - root -   Epoch: [44/300][120/341], lr: 0.00000016 	 loss = 0.5118(0.6567)
2023/11/09 11:08:24 - INFO - root -   Epoch: [44/300][140/341], lr: 0.00000016 	 loss = 0.4929(0.6635)
2023/11/09 11:09:43 - INFO - root -   Epoch: [44/300][160/341], lr: 0.00000016 	 loss = 0.2702(0.6595)
2023/11/09 11:10:52 - INFO - root -   Epoch: [44/300][180/341], lr: 0.00000016 	 loss = 0.2466(0.6696)
2023/11/09 11:11:59 - INFO - root -   Epoch: [44/300][200/341], lr: 0.00000016 	 loss = 0.3649(0.6699)
2023/11/09 11:13:06 - INFO - root -   Epoch: [44/300][220/341], lr: 0.00000016 	 loss = 0.4833(0.6648)
2023/11/09 11:14:09 - INFO - root -   Epoch: [44/300][240/341], lr: 0.00000016 	 loss = 0.5285(0.6619)
2023/11/09 11:15:28 - INFO - root -   Epoch: [44/300][260/341], lr: 0.00000016 	 loss = 0.5022(0.6692)
2023/11/09 11:16:20 - INFO - root -   Epoch: [44/300][280/341], lr: 0.00000016 	 loss = 0.8048(0.6722)
2023/11/09 11:17:42 - INFO - root -   Epoch: [44/300][300/341], lr: 0.00000016 	 loss = 1.3263(0.6801)
2023/11/09 11:18:39 - INFO - root -   Epoch: [44/300][320/341], lr: 0.00000016 	 loss = 0.5278(0.6809)
2023/11/09 11:19:22 - INFO - root -   Epoch: [44/300][340/341], lr: 0.00000016 	 loss = 1.0362(0.6799)
2023/11/09 11:19:22 - INFO - root -   Epoch: [44/300] 	 loss = 0.6799
2023/11/09 11:23:07 - INFO - root -   precision = 0.7135
2023/11/09 11:23:07 - INFO - root -   eval_loss = 0.5796
2023/11/09 11:23:07 - INFO - root -   eval_acc = 0.7135
2023/11/09 11:23:09 - INFO - root -   train_accuracy = 0.5982
2023/11/09 11:23:39 - INFO - root -   Epoch: [45/300][0/341], lr: 0.00000017 	 loss = 0.4870(0.4870)
2023/11/09 11:24:33 - INFO - root -   Epoch: [45/300][20/341], lr: 0.00000017 	 loss = 0.6102(0.7229)
2023/11/09 11:25:43 - INFO - root -   Epoch: [45/300][40/341], lr: 0.00000017 	 loss = 0.3029(0.6757)
2023/11/09 11:27:00 - INFO - root -   Epoch: [45/300][60/341], lr: 0.00000017 	 loss = 0.5304(0.6711)
2023/11/09 11:28:14 - INFO - root -   Epoch: [45/300][80/341], lr: 0.00000017 	 loss = 0.5211(0.6700)
2023/11/09 11:29:17 - INFO - root -   Epoch: [45/300][100/341], lr: 0.00000017 	 loss = 0.3845(0.6539)
2023/11/09 11:30:44 - INFO - root -   Epoch: [45/300][120/341], lr: 0.00000017 	 loss = 0.2674(0.6428)
2023/11/09 11:31:37 - INFO - root -   Epoch: [45/300][140/341], lr: 0.00000017 	 loss = 0.3567(0.6481)
2023/11/09 11:33:00 - INFO - root -   Epoch: [45/300][160/341], lr: 0.00000017 	 loss = 0.3990(0.6464)
2023/11/09 11:33:56 - INFO - root -   Epoch: [45/300][180/341], lr: 0.00000017 	 loss = 0.5566(0.6469)
2023/11/09 11:35:04 - INFO - root -   Epoch: [45/300][200/341], lr: 0.00000017 	 loss = 0.3818(0.6496)
2023/11/09 11:36:04 - INFO - root -   Epoch: [45/300][220/341], lr: 0.00000017 	 loss = 0.3699(0.6447)
2023/11/09 11:37:15 - INFO - root -   Epoch: [45/300][240/341], lr: 0.00000017 	 loss = 0.2171(0.6493)
2023/11/09 11:38:18 - INFO - root -   Epoch: [45/300][260/341], lr: 0.00000017 	 loss = 0.7876(0.6608)
2023/11/09 11:39:44 - INFO - root -   Epoch: [45/300][280/341], lr: 0.00000017 	 loss = 1.0076(0.6586)
2023/11/09 11:40:32 - INFO - root -   Epoch: [45/300][300/341], lr: 0.00000017 	 loss = 0.8171(0.6589)
2023/11/09 11:41:55 - INFO - root -   Epoch: [45/300][320/341], lr: 0.00000017 	 loss = 0.4532(0.6671)
2023/11/09 11:42:34 - INFO - root -   Epoch: [45/300][340/341], lr: 0.00000017 	 loss = 0.8270(0.6719)
2023/11/09 11:42:34 - INFO - root -   Epoch: [45/300] 	 loss = 0.6719
2023/11/09 11:42:34 - INFO - root -   train_accuracy = 0.6188
2023/11/09 11:42:57 - INFO - root -   Epoch: [46/300][0/341], lr: 0.00000017 	 loss = 0.5813(0.5813)
2023/11/09 11:44:17 - INFO - root -   Epoch: [46/300][20/341], lr: 0.00000017 	 loss = 0.5282(0.6647)
2023/11/09 11:45:15 - INFO - root -   Epoch: [46/300][40/341], lr: 0.00000017 	 loss = 0.4102(0.7013)
2023/11/09 11:46:31 - INFO - root -   Epoch: [46/300][60/341], lr: 0.00000017 	 loss = 1.5401(0.6929)
2023/11/09 11:47:42 - INFO - root -   Epoch: [46/300][80/341], lr: 0.00000017 	 loss = 0.9393(0.6798)
2023/11/09 11:49:05 - INFO - root -   Epoch: [46/300][100/341], lr: 0.00000017 	 loss = 0.2686(0.6452)
2023/11/09 11:50:01 - INFO - root -   Epoch: [46/300][120/341], lr: 0.00000017 	 loss = 0.4231(0.6349)
2023/11/09 11:51:19 - INFO - root -   Epoch: [46/300][140/341], lr: 0.00000017 	 loss = 0.3036(0.6424)
2023/11/09 11:52:07 - INFO - root -   Epoch: [46/300][160/341], lr: 0.00000017 	 loss = 0.6554(0.6554)
2023/11/09 11:53:16 - INFO - root -   Epoch: [46/300][180/341], lr: 0.00000017 	 loss = 0.7229(0.6563)
2023/11/09 11:54:34 - INFO - root -   Epoch: [46/300][200/341], lr: 0.00000017 	 loss = 0.3762(0.6515)
2023/11/09 11:55:35 - INFO - root -   Epoch: [46/300][220/341], lr: 0.00000017 	 loss = 0.6623(0.6383)
2023/11/09 11:56:43 - INFO - root -   Epoch: [46/300][240/341], lr: 0.00000017 	 loss = 0.1590(0.6389)
2023/11/09 11:57:57 - INFO - root -   Epoch: [46/300][260/341], lr: 0.00000017 	 loss = 0.4692(0.6389)
2023/11/09 11:58:49 - INFO - root -   Epoch: [46/300][280/341], lr: 0.00000017 	 loss = 0.6840(0.6388)
2023/11/09 12:00:03 - INFO - root -   Epoch: [46/300][300/341], lr: 0.00000017 	 loss = 1.1022(0.6487)
2023/11/09 12:01:02 - INFO - root -   Epoch: [46/300][320/341], lr: 0.00000017 	 loss = 0.4873(0.6513)
2023/11/09 12:01:49 - INFO - root -   Epoch: [46/300][340/341], lr: 0.00000017 	 loss = 1.0133(0.6566)
2023/11/09 12:01:49 - INFO - root -   Epoch: [46/300] 	 loss = 0.6566
2023/11/09 12:01:49 - INFO - root -   train_accuracy = 0.6173
2023/11/09 12:02:21 - INFO - root -   Epoch: [47/300][0/341], lr: 0.00000017 	 loss = 0.7804(0.7804)
2023/11/09 12:03:34 - INFO - root -   Epoch: [47/300][20/341], lr: 0.00000017 	 loss = 0.8823(0.7908)
2023/11/09 12:04:43 - INFO - root -   Epoch: [47/300][40/341], lr: 0.00000017 	 loss = 0.5048(0.7180)
2023/11/09 12:05:58 - INFO - root -   Epoch: [47/300][60/341], lr: 0.00000017 	 loss = 0.4704(0.6822)
2023/11/09 12:06:53 - INFO - root -   Epoch: [47/300][80/341], lr: 0.00000017 	 loss = 0.7754(0.6664)
2023/11/09 12:08:03 - INFO - root -   Epoch: [47/300][100/341], lr: 0.00000017 	 loss = 0.2879(0.6529)
2023/11/09 12:09:03 - INFO - root -   Epoch: [47/300][120/341], lr: 0.00000017 	 loss = 1.0514(0.6544)
2023/11/09 12:10:23 - INFO - root -   Epoch: [47/300][140/341], lr: 0.00000017 	 loss = 0.6819(0.6646)
2023/11/09 12:11:11 - INFO - root -   Epoch: [47/300][160/341], lr: 0.00000017 	 loss = 0.3557(0.6518)
2023/11/09 12:12:27 - INFO - root -   Epoch: [47/300][180/341], lr: 0.00000017 	 loss = 0.5787(0.6565)
2023/11/09 12:13:35 - INFO - root -   Epoch: [47/300][200/341], lr: 0.00000017 	 loss = 0.8863(0.6615)
2023/11/09 12:14:54 - INFO - root -   Epoch: [47/300][220/341], lr: 0.00000017 	 loss = 1.0230(0.6561)
2023/11/09 12:15:56 - INFO - root -   Epoch: [47/300][240/341], lr: 0.00000017 	 loss = 0.6034(0.6509)
2023/11/09 12:17:06 - INFO - root -   Epoch: [47/300][260/341], lr: 0.00000017 	 loss = 0.5790(0.6574)
2023/11/09 12:18:15 - INFO - root -   Epoch: [47/300][280/341], lr: 0.00000017 	 loss = 0.5049(0.6564)
2023/11/09 12:19:16 - INFO - root -   Epoch: [47/300][300/341], lr: 0.00000017 	 loss = 0.3592(0.6569)
2023/11/09 12:20:25 - INFO - root -   Epoch: [47/300][320/341], lr: 0.00000017 	 loss = 0.1181(0.6571)
2023/11/09 12:21:14 - INFO - root -   Epoch: [47/300][340/341], lr: 0.00000017 	 loss = 0.4902(0.6586)
2023/11/09 12:21:14 - INFO - root -   Epoch: [47/300] 	 loss = 0.6586
2023/11/09 12:21:14 - INFO - root -   train_accuracy = 0.5997
2023/11/09 12:21:53 - INFO - root -   Epoch: [48/300][0/341], lr: 0.00000017 	 loss = 0.7015(0.7015)
2023/11/09 12:22:54 - INFO - root -   Epoch: [48/300][20/341], lr: 0.00000017 	 loss = 0.8321(0.7211)
2023/11/09 12:24:05 - INFO - root -   Epoch: [48/300][40/341], lr: 0.00000017 	 loss = 0.2161(0.6383)
2023/11/09 12:25:05 - INFO - root -   Epoch: [48/300][60/341], lr: 0.00000017 	 loss = 1.0943(0.6341)
2023/11/09 12:26:38 - INFO - root -   Epoch: [48/300][80/341], lr: 0.00000017 	 loss = 0.6081(0.6264)
2023/11/09 12:27:39 - INFO - root -   Epoch: [48/300][100/341], lr: 0.00000017 	 loss = 0.3127(0.6241)
2023/11/09 12:28:36 - INFO - root -   Epoch: [48/300][120/341], lr: 0.00000017 	 loss = 0.3259(0.6352)
2023/11/09 12:29:56 - INFO - root -   Epoch: [48/300][140/341], lr: 0.00000017 	 loss = 0.5134(0.6427)
2023/11/09 12:31:10 - INFO - root -   Epoch: [48/300][160/341], lr: 0.00000017 	 loss = 0.3018(0.6386)
2023/11/09 12:32:00 - INFO - root -   Epoch: [48/300][180/341], lr: 0.00000017 	 loss = 0.4632(0.6402)
2023/11/09 12:33:20 - INFO - root -   Epoch: [48/300][200/341], lr: 0.00000017 	 loss = 0.7096(0.6382)
2023/11/09 12:34:21 - INFO - root -   Epoch: [48/300][220/341], lr: 0.00000017 	 loss = 0.3471(0.6381)
2023/11/09 12:35:38 - INFO - root -   Epoch: [48/300][240/341], lr: 0.00000017 	 loss = 0.3167(0.6424)
2023/11/09 12:36:49 - INFO - root -   Epoch: [48/300][260/341], lr: 0.00000017 	 loss = 0.6755(0.6548)
2023/11/09 12:37:54 - INFO - root -   Epoch: [48/300][280/341], lr: 0.00000017 	 loss = 0.5386(0.6580)
2023/11/09 12:38:56 - INFO - root -   Epoch: [48/300][300/341], lr: 0.00000017 	 loss = 0.7079(0.6617)
2023/11/09 12:39:53 - INFO - root -   Epoch: [48/300][320/341], lr: 0.00000017 	 loss = 0.3425(0.6604)
2023/11/09 12:40:40 - INFO - root -   Epoch: [48/300][340/341], lr: 0.00000017 	 loss = 0.5026(0.6560)
2023/11/09 12:40:40 - INFO - root -   Epoch: [48/300] 	 loss = 0.6560
2023/11/09 12:40:40 - INFO - root -   train_accuracy = 0.5982
2023/11/09 12:41:03 - INFO - root -   Epoch: [49/300][0/341], lr: 0.00000017 	 loss = 0.9847(0.9847)
2023/11/09 12:42:27 - INFO - root -   Epoch: [49/300][20/341], lr: 0.00000017 	 loss = 1.2618(0.7113)
2023/11/09 12:43:26 - INFO - root -   Epoch: [49/300][40/341], lr: 0.00000017 	 loss = 0.2688(0.6780)
2023/11/09 12:44:35 - INFO - root -   Epoch: [49/300][60/341], lr: 0.00000017 	 loss = 0.7524(0.6734)
2023/11/09 12:45:27 - INFO - root -   Epoch: [49/300][80/341], lr: 0.00000017 	 loss = 0.4819(0.6548)
2023/11/09 12:46:41 - INFO - root -   Epoch: [49/300][100/341], lr: 0.00000017 	 loss = 0.4457(0.6477)
2023/11/09 12:47:44 - INFO - root -   Epoch: [49/300][120/341], lr: 0.00000017 	 loss = 0.2072(0.6494)
2023/11/09 12:48:53 - INFO - root -   Epoch: [49/300][140/341], lr: 0.00000017 	 loss = 0.4577(0.6510)
2023/11/09 12:50:02 - INFO - root -   Epoch: [49/300][160/341], lr: 0.00000017 	 loss = 0.3189(0.6539)
2023/11/09 12:51:12 - INFO - root -   Epoch: [49/300][180/341], lr: 0.00000017 	 loss = 1.1949(0.6588)
2023/11/09 12:52:06 - INFO - root -   Epoch: [49/300][200/341], lr: 0.00000017 	 loss = 0.5295(0.6650)
2023/11/09 12:53:33 - INFO - root -   Epoch: [49/300][220/341], lr: 0.00000017 	 loss = 0.6765(0.6620)
2023/11/09 12:54:35 - INFO - root -   Epoch: [49/300][240/341], lr: 0.00000017 	 loss = 0.3127(0.6607)
2023/11/09 12:55:54 - INFO - root -   Epoch: [49/300][260/341], lr: 0.00000017 	 loss = 0.6384(0.6711)
2023/11/09 12:56:42 - INFO - root -   Epoch: [49/300][280/341], lr: 0.00000017 	 loss = 0.7199(0.6691)
2023/11/09 12:58:06 - INFO - root -   Epoch: [49/300][300/341], lr: 0.00000017 	 loss = 0.4964(0.6788)
2023/11/09 12:59:08 - INFO - root -   Epoch: [49/300][320/341], lr: 0.00000017 	 loss = 0.1961(0.6745)
2023/11/09 12:59:56 - INFO - root -   Epoch: [49/300][340/341], lr: 0.00000017 	 loss = 1.4423(0.6795)
2023/11/09 12:59:56 - INFO - root -   Epoch: [49/300] 	 loss = 0.6795
2023/11/09 13:03:42 - INFO - root -   precision = 0.6959
2023/11/09 13:03:42 - INFO - root -   eval_loss = 0.5898
2023/11/09 13:03:42 - INFO - root -   eval_acc = 0.6959
2023/11/09 13:03:43 - INFO - root -   train_accuracy = 0.6100
2023/11/09 13:04:22 - INFO - root -   Epoch: [50/300][0/341], lr: 0.00000017 	 loss = 0.5215(0.5215)
2023/11/09 13:05:28 - INFO - root -   Epoch: [50/300][20/341], lr: 0.00000017 	 loss = 0.3064(0.6658)
2023/11/09 13:06:38 - INFO - root -   Epoch: [50/300][40/341], lr: 0.00000017 	 loss = 0.2056(0.6247)
2023/11/09 13:07:34 - INFO - root -   Epoch: [50/300][60/341], lr: 0.00000017 	 loss = 0.9868(0.6350)
2023/11/09 13:08:39 - INFO - root -   Epoch: [50/300][80/341], lr: 0.00000017 	 loss = 0.4336(0.6481)
2023/11/09 13:10:02 - INFO - root -   Epoch: [50/300][100/341], lr: 0.00000017 	 loss = 0.7697(0.6408)
2023/11/09 13:11:00 - INFO - root -   Epoch: [50/300][120/341], lr: 0.00000017 	 loss = 0.3511(0.6632)
2023/11/09 13:12:03 - INFO - root -   Epoch: [50/300][140/341], lr: 0.00000017 	 loss = 0.4777(0.6587)
2023/11/09 13:13:13 - INFO - root -   Epoch: [50/300][160/341], lr: 0.00000017 	 loss = 0.2758(0.6707)
2023/11/09 13:14:27 - INFO - root -   Epoch: [50/300][180/341], lr: 0.00000017 	 loss = 0.3742(0.6738)
2023/11/09 13:15:39 - INFO - root -   Epoch: [50/300][200/341], lr: 0.00000017 	 loss = 0.4960(0.6721)
2023/11/09 13:16:53 - INFO - root -   Epoch: [50/300][220/341], lr: 0.00000017 	 loss = 0.8524(0.6764)
2023/11/09 13:17:57 - INFO - root -   Epoch: [50/300][240/341], lr: 0.00000017 	 loss = 0.4669(0.6660)
2023/11/09 13:19:11 - INFO - root -   Epoch: [50/300][260/341], lr: 0.00000017 	 loss = 1.1886(0.6689)
2023/11/09 13:20:02 - INFO - root -   Epoch: [50/300][280/341], lr: 0.00000017 	 loss = 0.3000(0.6725)
2023/11/09 13:21:32 - INFO - root -   Epoch: [50/300][300/341], lr: 0.00000017 	 loss = 0.8733(0.6783)
2023/11/09 13:22:27 - INFO - root -   Epoch: [50/300][320/341], lr: 0.00000017 	 loss = 0.3629(0.6821)
2023/11/09 13:23:08 - INFO - root -   Epoch: [50/300][340/341], lr: 0.00000017 	 loss = 0.9517(0.6813)
2023/11/09 13:23:08 - INFO - root -   Epoch: [50/300] 	 loss = 0.6813
2023/11/09 13:23:08 - INFO - root -   train_accuracy = 0.5894
2023/11/09 13:23:31 - INFO - root -   Epoch: [51/300][0/341], lr: 0.00000017 	 loss = 0.6789(0.6789)
2023/11/09 13:24:31 - INFO - root -   Epoch: [51/300][20/341], lr: 0.00000017 	 loss = 1.1415(0.6985)
2023/11/09 13:25:52 - INFO - root -   Epoch: [51/300][40/341], lr: 0.00000017 	 loss = 0.1994(0.6285)
2023/11/09 13:27:09 - INFO - root -   Epoch: [51/300][60/341], lr: 0.00000017 	 loss = 0.6634(0.6237)
2023/11/09 13:28:06 - INFO - root -   Epoch: [51/300][80/341], lr: 0.00000017 	 loss = 0.5753(0.6524)
2023/11/09 13:29:34 - INFO - root -   Epoch: [51/300][100/341], lr: 0.00000017 	 loss = 0.5745(0.6603)
2023/11/09 13:30:32 - INFO - root -   Epoch: [51/300][120/341], lr: 0.00000017 	 loss = 0.7381(0.6636)
2023/11/09 13:31:41 - INFO - root -   Epoch: [51/300][140/341], lr: 0.00000017 	 loss = 0.3960(0.6644)
2023/11/09 13:32:51 - INFO - root -   Epoch: [51/300][160/341], lr: 0.00000017 	 loss = 1.2180(0.6608)
2023/11/09 13:34:03 - INFO - root -   Epoch: [51/300][180/341], lr: 0.00000017 	 loss = 0.2690(0.6568)
2023/11/09 13:34:59 - INFO - root -   Epoch: [51/300][200/341], lr: 0.00000017 	 loss = 0.4910(0.6556)
2023/11/09 13:36:08 - INFO - root -   Epoch: [51/300][220/341], lr: 0.00000017 	 loss = 0.3139(0.6510)
2023/11/09 13:37:07 - INFO - root -   Epoch: [51/300][240/341], lr: 0.00000017 	 loss = 0.4550(0.6545)
2023/11/09 13:38:28 - INFO - root -   Epoch: [51/300][260/341], lr: 0.00000017 	 loss = 0.6057(0.6685)
2023/11/09 13:39:25 - INFO - root -   Epoch: [51/300][280/341], lr: 0.00000017 	 loss = 0.7944(0.6784)
2023/11/09 13:40:44 - INFO - root -   Epoch: [51/300][300/341], lr: 0.00000017 	 loss = 0.9280(0.6745)
2023/11/09 13:41:45 - INFO - root -   Epoch: [51/300][320/341], lr: 0.00000017 	 loss = 0.3497(0.6713)
2023/11/09 13:42:28 - INFO - root -   Epoch: [51/300][340/341], lr: 0.00000017 	 loss = 1.3434(0.6834)
2023/11/09 13:42:29 - INFO - root -   Epoch: [51/300] 	 loss = 0.6834
2023/11/09 13:42:29 - INFO - root -   train_accuracy = 0.5938
2023/11/09 13:42:59 - INFO - root -   Epoch: [52/300][0/341], lr: 0.00000018 	 loss = 0.7391(0.7391)
2023/11/09 13:44:08 - INFO - root -   Epoch: [52/300][20/341], lr: 0.00000018 	 loss = 0.4343(0.6284)
2023/11/09 13:45:17 - INFO - root -   Epoch: [52/300][40/341], lr: 0.00000018 	 loss = 0.1453(0.5969)
2023/11/09 13:46:24 - INFO - root -   Epoch: [52/300][60/341], lr: 0.00000018 	 loss = 0.5813(0.6533)
2023/11/09 13:47:55 - INFO - root -   Epoch: [52/300][80/341], lr: 0.00000018 	 loss = 0.7772(0.6636)
2023/11/09 13:49:05 - INFO - root -   Epoch: [52/300][100/341], lr: 0.00000018 	 loss = 0.7414(0.6626)
2023/11/09 13:50:04 - INFO - root -   Epoch: [52/300][120/341], lr: 0.00000018 	 loss = 0.4063(0.6574)
2023/11/09 13:51:09 - INFO - root -   Epoch: [52/300][140/341], lr: 0.00000018 	 loss = 0.2522(0.6729)
2023/11/09 13:52:18 - INFO - root -   Epoch: [52/300][160/341], lr: 0.00000018 	 loss = 0.2374(0.6580)
2023/11/09 13:53:43 - INFO - root -   Epoch: [52/300][180/341], lr: 0.00000018 	 loss = 0.4978(0.6441)
2023/11/09 13:54:25 - INFO - root -   Epoch: [52/300][200/341], lr: 0.00000018 	 loss = 0.2625(0.6470)
2023/11/09 13:55:35 - INFO - root -   Epoch: [52/300][220/341], lr: 0.00000018 	 loss = 0.4770(0.6464)
2023/11/09 13:56:49 - INFO - root -   Epoch: [52/300][240/341], lr: 0.00000018 	 loss = 0.2541(0.6522)
2023/11/09 13:57:59 - INFO - root -   Epoch: [52/300][260/341], lr: 0.00000018 	 loss = 0.7729(0.6581)
2023/11/09 13:59:08 - INFO - root -   Epoch: [52/300][280/341], lr: 0.00000018 	 loss = 0.3805(0.6579)
2023/11/09 14:00:12 - INFO - root -   Epoch: [52/300][300/341], lr: 0.00000018 	 loss = 0.8392(0.6563)
2023/11/09 14:01:22 - INFO - root -   Epoch: [52/300][320/341], lr: 0.00000018 	 loss = 0.1747(0.6613)
2023/11/09 14:02:03 - INFO - root -   Epoch: [52/300][340/341], lr: 0.00000018 	 loss = 1.0995(0.6651)
2023/11/09 14:02:03 - INFO - root -   Epoch: [52/300] 	 loss = 0.6651
2023/11/09 14:02:03 - INFO - root -   train_accuracy = 0.6217
2023/11/09 14:02:34 - INFO - root -   Epoch: [53/300][0/341], lr: 0.00000018 	 loss = 1.3113(1.3113)
2023/11/09 14:03:34 - INFO - root -   Epoch: [53/300][20/341], lr: 0.00000018 	 loss = 0.4191(0.7500)
2023/11/09 14:04:45 - INFO - root -   Epoch: [53/300][40/341], lr: 0.00000018 	 loss = 0.7206(0.7076)
2023/11/09 14:05:52 - INFO - root -   Epoch: [53/300][60/341], lr: 0.00000018 	 loss = 0.3614(0.6902)
2023/11/09 14:06:56 - INFO - root -   Epoch: [53/300][80/341], lr: 0.00000018 	 loss = 1.2058(0.6801)
2023/11/09 14:08:11 - INFO - root -   Epoch: [53/300][100/341], lr: 0.00000018 	 loss = 0.2979(0.6606)
2023/11/09 14:09:21 - INFO - root -   Epoch: [53/300][120/341], lr: 0.00000018 	 loss = 0.4360(0.6574)
2023/11/09 14:10:28 - INFO - root -   Epoch: [53/300][140/341], lr: 0.00000018 	 loss = 0.2587(0.6529)
2023/11/09 14:11:40 - INFO - root -   Epoch: [53/300][160/341], lr: 0.00000018 	 loss = 0.5895(0.6449)
2023/11/09 14:13:07 - INFO - root -   Epoch: [53/300][180/341], lr: 0.00000018 	 loss = 0.4372(0.6541)
2023/11/09 14:13:58 - INFO - root -   Epoch: [53/300][200/341], lr: 0.00000018 	 loss = 0.3780(0.6528)
2023/11/09 14:15:07 - INFO - root -   Epoch: [53/300][220/341], lr: 0.00000018 	 loss = 0.5145(0.6515)
2023/11/09 14:16:11 - INFO - root -   Epoch: [53/300][240/341], lr: 0.00000018 	 loss = 0.4950(0.6504)
2023/11/09 14:17:20 - INFO - root -   Epoch: [53/300][260/341], lr: 0.00000018 	 loss = 0.2625(0.6595)
2023/11/09 14:18:25 - INFO - root -   Epoch: [53/300][280/341], lr: 0.00000018 	 loss = 1.1400(0.6705)
2023/11/09 14:19:20 - INFO - root -   Epoch: [53/300][300/341], lr: 0.00000018 	 loss = 0.7721(0.6709)
2023/11/09 14:20:55 - INFO - root -   Epoch: [53/300][320/341], lr: 0.00000018 	 loss = 0.5912(0.6701)
2023/11/09 14:21:26 - INFO - root -   Epoch: [53/300][340/341], lr: 0.00000018 	 loss = 1.2518(0.6757)
2023/11/09 14:21:26 - INFO - root -   Epoch: [53/300] 	 loss = 0.6757
2023/11/09 14:21:26 - INFO - root -   train_accuracy = 0.6056
2023/11/09 14:22:11 - INFO - root -   Epoch: [54/300][0/341], lr: 0.00000018 	 loss = 0.8572(0.8572)
2023/11/09 14:23:00 - INFO - root -   Epoch: [54/300][20/341], lr: 0.00000018 	 loss = 0.5371(0.7110)
2023/11/09 14:24:15 - INFO - root -   Epoch: [54/300][40/341], lr: 0.00000018 	 loss = 0.2059(0.7315)
2023/11/09 14:25:16 - INFO - root -   Epoch: [54/300][60/341], lr: 0.00000018 	 loss = 0.6289(0.7068)
2023/11/20 21:19:54 - INFO - root -   Num train examples = 400
2023/11/20 21:19:54 - INFO - root -   Num val examples = 101
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0181818176060915
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.036363635212183
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.05454545468091965
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.072727270424366
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.09090908616781235
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.10909091681241989
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.12727272510528564
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1454545557498932
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.16363637149333954
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.1818181872367859
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.20000000298023224
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   No L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Double L_MHRA: True
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Use checkpoint: False
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Checkpoint number: [0]
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.0
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.13333334028720856
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.2666666507720947
2023/11/20 21:19:54 - INFO - models.uniformerv2_model -   Drop path rate: 0.4000000059604645
2023/11/20 21:19:55 - INFO - root -   backend = nccl
2023/11/20 21:19:55 - INFO - root -   batch_size = 2
2023/11/20 21:19:55 - INFO - root -   dropout = 0.5
2023/11/20 21:19:55 - INFO - root -   epochs = 300
2023/11/20 21:19:55 - INFO - root -   eval_freq = 5
2023/11/20 21:19:55 - INFO - root -   focal_loss = False
2023/11/20 21:19:55 - INFO - root -   input_size = 128
2023/11/20 21:19:55 - INFO - root -   is_pretrained = False
2023/11/20 21:19:55 - INFO - root -   label_smooth = False
2023/11/20 21:19:55 - INFO - root -   local_rank = -1
2023/11/20 21:19:55 - INFO - root -   lr = 1e-05
2023/11/20 21:19:55 - INFO - root -   lr_decay_rate = 0.1
2023/11/20 21:19:55 - INFO - root -   lr_steps = [50, 100]
2023/11/20 21:19:55 - INFO - root -   lr_type = cosine
2023/11/20 21:19:55 - INFO - root -   model_depth = 34
2023/11/20 21:19:55 - INFO - root -   model_name = resnet50
2023/11/20 21:19:55 - INFO - root -   momentum = 0.9
2023/11/20 21:19:55 - INFO - root -   num_classes = 3
2023/11/20 21:19:55 - INFO - root -   output = ./all_roi_expand_transform_gbm_results
2023/11/20 21:19:55 - INFO - root -   print_freq = 20
2023/11/20 21:19:55 - INFO - root -   resume = 
2023/11/20 21:19:55 - INFO - root -   start_epoch = 0
2023/11/20 21:19:55 - INFO - root -   train_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/ucsf_train_patients.txt
2023/11/20 21:19:55 - INFO - root -   tune_from = 
2023/11/20 21:19:55 - INFO - root -   val_list = /media/spgou/DATA/ZYJ/Glioma_easy/dataset/ucsf_test_patients.txt
2023/11/20 21:19:55 - INFO - root -   warmup_epoch = 20
2023/11/20 21:19:55 - INFO - root -   warmup_multiplier = 100
2023/11/20 21:19:55 - INFO - root -   weight_decay = 0.0005
2023/11/20 21:19:55 - INFO - root -   workers = 8
